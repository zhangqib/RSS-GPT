<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.AI updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.AI</link>


<item>
<title>The Future of Generative AI in Software Engineering: A Vision from Industry and Academia in the European GENIUS Project</title>
<link>https://arxiv.org/abs/2511.01348</link>
<guid>https://arxiv.org/abs/2511.01348</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, Software Engineering, Software Development Life Cycle, GENIUS project, Industrial Validation

Summary: 
This vision paper discusses the potential of Generative AI in software engineering and the challenges faced in its adoption across the Software Development Life Cycle (SDLC). The GENIUS project, involving European industrial and academic partners, aims to advance AI integration in all SDLC phases by developing innovative tools and addressing research challenges. The paper outlines current challenges in GenAI adoption, envisions technological advancements over the next five years, predicts changes in software professionals' roles and skill sets, and discusses how GENIUS will contribute to this transformation through practical tools and industrial validation. It emphasizes aligning technical innovation with business relevance to provide reliable and industry-ready GenAI solutions for software engineering teams. 

<br /><br />Summary: <div>
arXiv:2511.01348v2 Announce Type: replace-cross 
Abstract: Generative AI (GenAI) has recently emerged as a groundbreaking force in Software Engineering, capable of generating code, identifying bugs, recommending fixes, and supporting quality assurance. While its use in coding tasks shows considerable promise, applying GenAI across the entire Software Development Life Cycle (SDLC) has not yet been fully explored. Critical uncertainties in areas such as reliability, accountability, security, and data privacy demand deeper investigation and coordinated action. The GENIUS project, comprising over 30 European industrial and academic partners, aims to address these challenges by advancing AI integration across all SDLC phases. It focuses on GenAI's potential, the development of innovative tools, and emerging research challenges, actively shaping the future of software engineering. This vision paper presents a shared perspective on the future of GenAI-driven software engineering, grounded in cross-sector dialogue as well as experiences and findings within the GENIUS consortium. The paper explores four central elements: (1) a structured overview of current challenges in GenAI adoption across the SDLC; (2) a forward-looking vision outlining key technological and methodological advances expected over the next five years; (3) anticipated shifts in the roles and required skill sets of software professionals; and (4) the contribution of GENIUS in realising this transformation through practical tools and industrial validation. This paper focuses on aligning technical innovation with business relevance. It aims to inform both research agendas and industrial strategies, providing a foundation for reliable, scalable, and industry-ready GenAI solutions for software engineering teams.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A hybrid solution approach for the Integrated Healthcare Timetabling Competition 2024</title>
<link>https://arxiv.org/abs/2511.04685</link>
<guid>https://arxiv.org/abs/2511.04685</guid>
<content:encoded><![CDATA[
<div> algorithm, implementation, results, healthcare timetabling, competition
Summary:<br />
Team Twente participated in the Integrated Healthcare Timetabling Competition 2024 and secured third place with their innovative approach. Their algorithm combines mixed-integer programming, constraint programming, and simulated annealing in a 3-phase solution method. By decomposing the problem into subproblems, they were able to achieve impressive results. The team also provided lower bounds on the optimal solution values for benchmark instances, offering valuable insights into the problem. Additionally, they discussed their design decisions and identified areas for further improvement. Overall, Team Twente's approach showcases the power of combining different techniques to tackle complex optimization challenges in healthcare timetabling. <br /><br />Summary: <div>
arXiv:2511.04685v1 Announce Type: new 
Abstract: We report about the algorithm, implementation and results submitted to the Integrated Healthcare Timetabling Competition 2024 by Team Twente, which scored third in the competition. Our approach combines mixed-integer programming, constraint programming and simulated annealing in a 3-phase solution approach based on decomposition into subproblems. Next to describing our approach and describing our design decisions, we share our insights and, for the first time, lower bounds on the optimal solution values for the benchmark instances. We finally highlight open problems for which we think that addressing them could improve our approach even further.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Epistemic Reject Option Prediction</title>
<link>https://arxiv.org/abs/2511.04855</link>
<guid>https://arxiv.org/abs/2511.04855</guid>
<content:encoded><![CDATA[
<div> Keywords: high-stakes applications, predictive models, uncertainty, reject-option predictor, Bayesian learning

Summary: 
The paper introduces the epistemic reject-option predictor that addresses uncertainty in predictive models for high-stakes applications. Traditional reject-option methods focus on aleatoric uncertainty, assuming low epistemic uncertainty due to abundant data. However, in practical scenarios with limited data, this assumption is often invalid. The new approach uses Bayesian learning to define the optimal predictor that minimizes expected regret, considering the performance gap between the model and the Bayes-optimal predictor with complete data knowledge. The model abstains when the regret exceeds a specified rejection cost, allowing identification of inputs with insufficient data for reliable decisions. This framework is the first to provide a principled way for learning predictors to handle high epistemic uncertainty in predictive modeling. 

<br /><br />Summary: <div>
arXiv:2511.04855v1 Announce Type: new 
Abstract: In high-stakes applications, predictive models must not only produce accurate predictions but also quantify and communicate their uncertainty. Reject-option prediction addresses this by allowing the model to abstain when prediction uncertainty is high. Traditional reject-option approaches focus solely on aleatoric uncertainty, an assumption valid only when large training data makes the epistemic uncertainty negligible. However, in many practical scenarios, limited data makes this assumption unrealistic. This paper introduces the epistemic reject-option predictor, which abstains in regions of high epistemic uncertainty caused by insufficient data. Building on Bayesian learning, we redefine the optimal predictor as the one that minimizes expected regret -- the performance gap between the learned model and the Bayes-optimal predictor with full knowledge of the data distribution. The model abstains when the regret for a given input exceeds a specified rejection cost. To our knowledge, this is the first principled framework that enables learning predictors capable of identifying inputs for which the training data is insufficient to make reliable decisions.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DMA: Online RAG Alignment with Human Feedback</title>
<link>https://arxiv.org/abs/2511.04880</link>
<guid>https://arxiv.org/abs/2511.04880</guid>
<content:encoded><![CDATA[
<div> online learning, human feedback, dynamic memory alignment, retrieval-augmented generation, conversational QA 

Summary: 
Dynamic Memory Alignment (DMA) is introduced as an online learning framework that leverages multi-granularity human feedback to align ranking in retrieval-augmented generation (RAG) systems. By incorporating document-, list-, and response-level signals, DMA enables supervised training for rankers, policy optimization based on response preferences, and knowledge distillation for low-latency serving. Dual-track evaluation includes large-scale online A/B ablations and few-shot offline tests, demonstrating DMA's utility and improvements in human engagement. Notable gains are seen in conversational QA tasks like TriviaQA and HotpotQA, positioning DMA as a feedback-driven approach for real-time adaptation in RAG systems while maintaining foundational retrieval capability. <br /><br />Summary: <div>
arXiv:2511.04880v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) systems often rely on static retrieval, limiting adaptation to evolving intent and content drift. We introduce Dynamic Memory Alignment (DMA), an online learning framework that systematically incorporates multi-granularity human feedback to align ranking in interactive settings. DMA organizes document-, list-, and response-level signals into a coherent learning pipeline: supervised training for pointwise and listwise rankers, policy optimization driven by response-level preferences, and knowledge distillation into a lightweight scorer for low-latency serving. Throughout this paper, memory refers to the model's working memory, which is the entire context visible to the LLM for In-Context Learning.
  We adopt a dual-track evaluation protocol mirroring deployment: (i) large-scale online A/B ablations to isolate the utility of each feedback source, and (ii) few-shot offline tests on knowledge-intensive benchmarks. Online, a multi-month industrial deployment further shows substantial improvements in human engagement. Offline, DMA preserves competitive foundational retrieval while yielding notable gains on conversational QA (TriviaQA, HotpotQA). Taken together, these results position DMA as a principled approach to feedback-driven, real-time adaptation in RAG without sacrificing baseline capability.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-Time Reasoning Agents in Evolving Environments</title>
<link>https://arxiv.org/abs/2511.04898</link>
<guid>https://arxiv.org/abs/2511.04898</guid>
<content:encoded><![CDATA[
<div> Keywords: Real-Time Reasoning, Language Models, Agents, AgileThinker, Evolving Environments

Summary: 
Real-time reasoning is introduced as a problem formulation for agents in dynamic environments. The Real-Time Reasoning Gym is built to demonstrate this concept. Two paradigms for deploying language models in agents are studied: reactive agents for rapid responses and planning agents for complex problems. Current models struggle with logical and timely judgments in both paradigms. AgileThinker, a proposed solution, combines both paradigms to balance reasoning depth and response latency, outperforming agents engaging in only one paradigm. This research establishes a critical testbed for practical agents in evolving environments and paves the way for temporally constrained AI systems capable of real-time reasoning. <div>
arXiv:2511.04898v1 Announce Type: new 
Abstract: Agents in the real world must make not only logical but also timely judgments. This requires continuous awareness of the dynamic environment: hazards emerge, opportunities arise, and other agents act, while the agent's reasoning is still unfolding. Despite advances in language model reasoning, existing approaches fail to account for this dynamic nature. We introduce real-time reasoning as a new problem formulation for agents in evolving environments and build Real-Time Reasoning Gym to demonstrate it. We study two paradigms for deploying language models in agents: (1) reactive agents, which employ language models with bounded reasoning computation for rapid responses, and (2) planning agents, which allow extended reasoning computation for complex problems. Our experiments show that even state-of-the-art models struggle with making logical and timely judgments in either paradigm. To address this limitation, we propose AgileThinker, which simultaneously engages both reasoning paradigms. AgileThinker consistently outperforms agents engaging only one reasoning paradigm as the task difficulty and time pressure rise, effectively balancing reasoning depth and response latency. Our work establishes real-time reasoning as a critical testbed for developing practical agents and provides a foundation for research in temporally constrained AI systems, highlighting a path toward real-time capable agents.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ORCHID: Orchestrated Retrieval-Augmented Classification with Human-in-the-Loop Intelligent Decision-Making for High-Risk Property</title>
<link>https://arxiv.org/abs/2511.04956</link>
<guid>https://arxiv.org/abs/2511.04956</guid>
<content:encoded><![CDATA[
<div> dual-use equipment, export control policies, ORCHID, agentic system, DOE compliance <br />
Summary: 
The article discusses the importance of High-Risk Property (HRP) classification at U.S. Department of Energy (DOE) sites, where sensitive and dual-use equipment inventories require compliance with evolving export control policies. Traditional workflows for HRP classification are time-consuming and struggle to keep pace with regulatory changes. The demo introduces ORCHID, a modular agentic system that combines retrieval-augmented generation (RAG) with human oversight to produce policy-based outputs that can be audited. ORCHID consists of small cooperating agents that coordinate through messaging and the Model Context Protocol (MCP) for model-agnostic operation. In tests on real HRP cases, ORCHID improves accuracy and traceability while involving Subject Matter Experts (SMEs) for uncertain items. The demonstration showcases single-item submission, grounded citations, SME feedback capture, and exportable audit artifacts, demonstrating a practical approach to trustworthy assistance in DOE compliance workflows. <br /> <div>
arXiv:2511.04956v1 Announce Type: new 
Abstract: High-Risk Property (HRP) classification is critical at U.S. Department of Energy (DOE) sites, where inventories include sensitive and often dual-use equipment. Compliance must track evolving rules designated by various export control policies to make transparent and auditable decisions. Traditional expert-only workflows are time-consuming, backlog-prone, and struggle to keep pace with shifting regulatory boundaries. We demo ORCHID, a modular agentic system for HRP classification that pairs retrieval-augmented generation (RAG) with human oversight to produce policy-based outputs that can be audited. Small cooperating agents, retrieval, description refiner, classifier, validator, and feedback logger, coordinate via agent-to-agent messaging and invoke tools through the Model Context Protocol (MCP) for model-agnostic on-premise operation. The interface follows an Item to Evidence to Decision loop with step-by-step reasoning, on-policy citations, and append-only audit bundles (run-cards, prompts, evidence). In preliminary tests on real HRP cases, ORCHID improves accuracy and traceability over a non-agentic baseline while deferring uncertain items to Subject Matter Experts (SMEs). The demonstration shows single item submission, grounded citations, SME feedback capture, and exportable audit artifacts, illustrating a practical path to trustworthy LLM assistance in sensitive DOE compliance workflows.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autonomous generation of different courses of action in mechanized combat operations</title>
<link>https://arxiv.org/abs/2511.05182</link>
<guid>https://arxiv.org/abs/2511.05182</guid>
<content:encoded><![CDATA[
<div> Methodology, decision-making, military operations, courses of action, sequential decision-making

Summary:<br />
This paper presents a methodology for supporting decision-making during military ground combat operations, focusing on actions of a mechanized battalion. The methodology generates and evaluates recommendations for various courses of action based on anticipated outcomes. It generates thousands of individual action alternatives, evaluating them in context of opponent's status, unit composition, force ratios, offense and defense types, and advance rates. Field manuals are used to assess battle outcomes and advancement rates. The process involves concurrent generation and evaluation of alternative courses of action, enabling management of new course generation based on previous actions. As combat conditions evolve, revised courses of action are formulated within a sequential decision-making framework. This approach aims to provide decision-makers with a systematic method for choosing superior courses of action during military ground combat operations. 

<br /><br />Summary: <div>
arXiv:2511.05182v1 Announce Type: new 
Abstract: In this paper, we propose a methodology designed to support decision-making during the execution phase of military ground combat operations, with a focus on one's actions. This methodology generates and evaluates recommendations for various courses of action for a mechanized battalion, commencing with an initial set assessed by their anticipated outcomes. It systematically produces thousands of individual action alternatives, followed by evaluations aimed at identifying alternative courses of action with superior outcomes. These alternatives are appraised in light of the opponent's status and actions, considering unit composition, force ratios, types of offense and defense, and anticipated advance rates. Field manuals evaluate battle outcomes and advancement rates. The processes of generation and evaluation work concurrently, yielding a variety of alternative courses of action. This approach facilitates the management of new course generation based on previously evaluated actions. As the combat unfolds and conditions evolve, revised courses of action are formulated for the decision-maker within a sequential decision-making framework.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cleaning Maintenance Logs with LLM Agents for Improved Predictive Maintenance</title>
<link>https://arxiv.org/abs/2511.05311</link>
<guid>https://arxiv.org/abs/2511.05311</guid>
<content:encoded><![CDATA[
<div> Keywords: predictive maintenance, large language models, machine learning, cleaning pipelines, automotive sector

Summary:
Large language models (LLMs) show promise in overcoming economic constraints, limited datasets, and specialized expertise challenges in predictive maintenance (PdM) for the automotive sector. This study focuses on utilizing LLM-based agents to clean maintenance logs, a critical data source for training ML models but prone to errors like typos and missing information. Evaluating LLM agents on six types of noise, the study finds them effective for generic cleaning tasks, paving the way for potential industrial applications. While domain-specific errors remain a challenge, specialized training and enhanced agent capabilities hold promise for further advancements in this area. Overall, LLMs present a valuable tool to accelerate the transition of PdM from research to practical implementation in industry. 

<br /><br />Summary: <div>
arXiv:2511.05311v1 Announce Type: new 
Abstract: Economic constraints, limited availability of datasets for reproducibility and shortages of specialized expertise have long been recognized as key challenges to the adoption and advancement of predictive maintenance (PdM) in the automotive sector. Recent progress in large language models (LLMs) presents an opportunity to overcome these barriers and speed up the transition of PdM from research to industrial practice. Under these conditions, we explore the potential of LLM-based agents to support PdM cleaning pipelines. Specifically, we focus on maintenance logs, a critical data source for training well-performing machine learning (ML) models, but one often affected by errors such as typos, missing fields, near-duplicate entries, and incorrect dates. We evaluate LLM agents on cleaning tasks involving six distinct types of noise. Our findings show that LLMs are effective at handling generic cleaning tasks and offer a promising foundation for future industrial applications. While domain-specific errors remain challenging, these results highlight the potential for further improvements through specialized training and enhanced agentic capabilities.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning Is All You Need for Urban Planning AI</title>
<link>https://arxiv.org/abs/2511.05375</link>
<guid>https://arxiv.org/abs/2511.05375</guid>
<content:encoded><![CDATA[
<div> urban planning, AI, decision-making, reasoning, collaboration

Summary: 
The paper discusses the advancement of AI in urban planning analysis and proposes the Agentic Urban Planning AI Framework. This framework integrates three cognitive layers and six logic components through a multi-agent collaboration framework. It emphasizes the need for explicit reasoning capabilities in planning decisions, which should be value-based, rule-grounded, and explainable. The framework highlights the limitations of statistical learning alone in fulfilling these requirements. By comparing reasoning agents with statistical learning, the paper presents a comprehensive architecture with benchmark evaluation metrics and outlines research challenges. The framework demonstrates how AI agents can augment human planners by exploring solution spaces, ensuring regulatory compliance, and transparently deliberating over trade-offs, ultimately enhancing human judgment with computational reasoning capabilities.<br /><br /> <div>
arXiv:2511.05375v1 Announce Type: new 
Abstract: AI has proven highly successful at urban planning analysis -- learning patterns from data to predict future conditions. The next frontier is AI-assisted decision-making: agents that recommend sites, allocate resources, and evaluate trade-offs while reasoning transparently about constraints and stakeholder values. Recent breakthroughs in reasoning AI -- CoT prompting, ReAct, and multi-agent collaboration frameworks -- now make this vision achievable.
  This position paper presents the Agentic Urban Planning AI Framework for reasoning-capable planning agents that integrates three cognitive layers (Perception, Foundation, Reasoning) with six logic components (Analysis, Generation, Verification, Evaluation, Collaboration, Decision) through a multi-agents collaboration framework. We demonstrate why planning decisions require explicit reasoning capabilities that are value-based (applying normative principles), rule-grounded (guaranteeing constraint satisfaction), and explainable (generating transparent justifications) -- requirements that statistical learning alone cannot fulfill. We compare reasoning agents with statistical learning, present a comprehensive architecture with benchmark evaluation metrics, and outline critical research challenges. This framework shows how AI agents can augment human planners by systematically exploring solution spaces, verifying regulatory compliance, and deliberating over trade-offs transparently -- not replacing human judgment but amplifying it with computational reasoning capabilities.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Deployment of CNN Models on Multiple In-Memory Computing Units</title>
<link>https://arxiv.org/abs/2511.04682</link>
<guid>https://arxiv.org/abs/2511.04682</guid>
<content:encoded><![CDATA[
<div> Keywords: In-Memory Computing, Convolutional Neural Networks, IMC Emulator, Processing Units, Task Allocation

Summary:
In this study, the researchers explore the deployment of Convolutional Neural Networks (CNNs) on In-Memory Computing (IMC) hardware using an IMC Emulator (IMCE) with multiple Processing Units (PUs). They introduce the Load-Balance-Longest-Path (LBLP) algorithm to dynamically assign CNN nodes to the available PUs for maximizing processing rate and minimizing latency. The goal is to achieve efficient resource utilization and improve overall performance. By benchmarking LBLP against other scheduling strategies for various CNN models, the study demonstrates the effectiveness of the proposed algorithm in enhancing computational efficiency. The results show that the LBLP algorithm outperforms alternative strategies in terms of processing rate and latency reduction, highlighting its significance in achieving optimal performance when deploying CNN models on IMC-based hardware. Overall, the study emphasizes the importance of advanced task allocation strategies in leveraging the advantages of IMC for deep learning acceleration. 

Summary: <br /><br />In this study, researchers investigate the deployment of Convolutional Neural Networks (CNNs) on In-Memory Computing (IMC) hardware using an IMC Emulator (IMCE) with multiple Processing Units (PUs). They introduce the Load-Balance-Longest-Path (LBLP) algorithm to dynamically assign CNN nodes to PUs for maximizing processing rate and minimizing latency. By benchmarking LBLP against other strategies, they demonstrate its effectiveness in enhancing computational efficiency. The results show that LBLP outperforms alternatives in processing rate and latency reduction. The study highlights the importance of advanced task allocation strategies in leveraging IMC for deep learning acceleration. <div>
arXiv:2511.04682v1 Announce Type: cross 
Abstract: In-Memory Computing (IMC) represents a paradigm shift in deep learning acceleration by mitigating data movement bottlenecks and leveraging the inherent parallelism of memory-based computations. The efficient deployment of Convolutional Neural Networks (CNNs) on IMC-based hardware necessitates the use of advanced task allocation strategies for achieving maximum computational efficiency. In this work, we exploit an IMC Emulator (IMCE) with multiple Processing Units (PUs) for investigating how the deployment of a CNN model in a multi-processing system affects its performance, in terms of processing rate and latency. For that purpose, we introduce the Load-Balance-Longest-Path (LBLP) algorithm, that dynamically assigns all CNN nodes to the available IMCE PUs, for maximizing the processing rate and minimizing latency due to efficient resources utilization. We are benchmarking LBLP against other alternative scheduling strategies for a number of CNN models and experimental results demonstrate the effectiveness of the proposed algorithm.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Powered Citation Auditing: A Zero-Assumption Protocol for Systematic Reference Verification in Academic Research</title>
<link>https://arxiv.org/abs/2511.04683</link>
<guid>https://arxiv.org/abs/2511.04683</guid>
<content:encoded><![CDATA[
<div> AI-powered reference auditing, academic citation integrity, agentic AI, tool-use capabilities, verification protocol<br />
<br />Summary: This paper introduces a novel AI-powered methodology for auditing academic references to address challenges in citation integrity. The methodology independently validates references against multiple academic databases without assuming any citation is correct. Validation on 30 academic documents showed a 91.7% average verification rate on published PLOS papers and detected fabricated references, retracted articles, orphan citations, and predatory journals. The methodology significantly improves time efficiency, with audits for doctoral theses taking only 90 minutes compared to months of manual review. The system achieved a <0.5% false positive rate and identified critical issues that manual review might miss. This work establishes the first validated AI-agent methodology for academic citation integrity, showing practical applicability for supervisors, students, and institutional quality assurance. <br /> <div>
arXiv:2511.04683v1 Announce Type: cross 
Abstract: Academic citation integrity faces persistent challenges, with research indicating 20% of citations contain errors and manual verification requiring months of expert time. This paper presents a novel AI-powered methodology for systematic, comprehensive reference auditing using agentic AI with tool-use capabilities. We develop a zero-assumption verification protocol that independently validates every reference against multiple academic databases (Semantic Scholar, Google Scholar, CrossRef) without assuming any citation is correct. The methodology was validated across 30 academic documents (2,581 references) spanning undergraduate projects to doctoral theses and peer-reviewed publications. Results demonstrate 91.7% average verification rate on published PLOS papers, with successful detection of fabricated references, retracted articles, orphan citations, and predatory journals. Time efficiency improved dramatically: 90-minute audits for 916-reference doctoral theses versus months of manual review. The system achieved <0.5% false positive rate while identifying critical issues manual review might miss. This work establishes the first validated AI-agent methodology for academic citation integrity, demonstrating practical applicability for supervisors, students, and institutional quality assurance.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stateful KV Cache Management for LLMs: Balancing Space, Time, Accuracy, and Positional Fidelity</title>
<link>https://arxiv.org/abs/2511.04686</link>
<guid>https://arxiv.org/abs/2511.04686</guid>
<content:encoded><![CDATA[
<div> cache management, language models, autoregressive inference, positional encodings, eviction strategies
Summary:
Cache management in Key-Value (KV) caches plays a crucial role in efficient autoregressive inference in large language models (LLMs). This study explores the impact of KV cache strategies on model performance, particularly in stateful multi-turn scenarios. The research emphasizes the importance of respecting architectural limits and preserving positional coherence in cache eviction techniques to prevent degradation of LLM generation quality. Eviction strategies that disrupt positional signals can lead to degenerative outputs, highlighting the need for simple strategies that maintain contiguous context blocks. By prioritizing the preservation of positional structure and maintaining "cache health" holistically, LLMs can generate more coherent outputs without compromising performance. <div>
arXiv:2511.04686v1 Announce Type: cross 
Abstract: The Key-Value (KV) cache is integral to efficient autoregressive inference in large language models (LLMs), yet its unbounded growth in stateful multi-turn scenarios presents major challenges. This paper examines the interplay between KV cache management strategies, the architectural context limits of models like meta-llama/Meta-Llama-3-8b-instruct, and the often-overlooked integrity of positional encodings. Through empirical analysis using a stateful benchmarking framework, we show that LLM generation quality degrades sharply when the accumulated KV cache approaches or exceeds the model's trained context window (e.g., 8192 tokens for Llama 3), a failure mode distinct from GPU memory exhaustion. Common eviction strategies, even high-retention ones (e.g., 99% via AttentionTop), can worsen performance if they disrupt positional coherence. Because LLMs rely on consistent positional signals (e.g., RoPE), compacting a cache by removing non-contiguous tokens can scramble these signals and lead to degenerative outputs. We further show that simple strategies preserving contiguous context blocks (e.g., keeping an initial "gist") can yield more coherent generations than complex or positionally disruptive ones. We advocate for eviction techniques that respect architectural limits, preserve positional structure, and view "cache health" holistically beyond mere size.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Testing for LLM Evaluation: A Psychometric Alternative to Static Benchmarks</title>
<link>https://arxiv.org/abs/2511.04689</link>
<guid>https://arxiv.org/abs/2511.04689</guid>
<content:encoded><![CDATA[
<div> Item Response Theory, large language model evaluation, benchmark items, ATLAS, adaptive testing

Summary: The study introduces ATLAS, an adaptive testing framework utilizing Item Response Theory for large language model evaluation. By selecting items based on Fisher information, ATLAS can estimate model ability efficiently. Analysis of major benchmarks reveals annotation errors in 3-6% of items, affecting static evaluation accuracy. With a 90% reduction in items, ATLAS maintains measurement precision, achieving comparable results to full-benchmark estimates with significantly fewer items. The framework ensures low item exposure rates and limited test overlap, addressing existing evaluation issues where all models are exposed to every item. IRT ranks differ from accuracy ranks, highlighting variations in model performance. The code and calibrated item banks for ATLAS are available for further exploration. <br /><br />Summary: <div>
arXiv:2511.04689v1 Announce Type: cross 
Abstract: Large language model evaluation requires thousands of benchmark items, making evaluations expensive and slow. Existing methods compute average accuracy across fixed item sets, treating all items equally despite varying quality and informativeness. We present ATLAS an adaptive testing framework using Item Response Theory (IRT) to estimate model ability through Fisher information-guided item selection. Our analysis of five major benchmarks reveals that 3-6% of items exhibit negative discrimination, indicating annotation errors that corrupt static evaluation. ATLAS achieves 90% item reduction while maintaining measurement precision: on HellaSwag (5,608 items), we match full-benchmark estimates using only 42 items with 0.154 MAE. Our framework maintains item exposure rates below 10% and test overlap at 16-27%, compared to static benchmarks where every model sees all items (100% exposure). Among 4,000+ tested models, IRT ranks differ from accuracy ranks: models with the same accuracy get different IRT scores, and 23-31% of all models shift by more than 10 rank positions. Code and calibrated item banks are available at https://github.com/Peiyu-Georgia-Li/ATLAS.git.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Penny for Your Thoughts: Decoding Speech from Inexpensive Brain Signals</title>
<link>https://arxiv.org/abs/2511.04691</link>
<guid>https://arxiv.org/abs/2511.04691</guid>
<content:encoded><![CDATA[
<div> EEG, neural networks, speech decoding, CLIP loss, personalized architectures<br />
<br />
Summary:<br />
Neural networks are investigated for their ability to decode brain activity into speech by mapping EEG recordings to audio representations. The study utilized EEG data collected while subjects listened to natural speech and trained a model using a contrastive CLIP loss to align EEG-derived embeddings with those from a pre-trained transformer-based speech model. Building upon an existing EEG decoder, three architectural modifications were introduced: subject-specific attention layers, personalized spatial attention, and a dual-path RNN with attention. These modifications led to improvements in performance, indicating the potential of personalized architectures for brain-to-speech decoding and applications in brain-computer interfaces. <div>
arXiv:2511.04691v1 Announce Type: cross 
Abstract: We explore whether neural networks can decode brain activity into speech by mapping EEG recordings to audio representations. Using EEG data recorded as subjects listened to natural speech, we train a model with a contrastive CLIP loss to align EEG-derived embeddings with embeddings from a pre-trained transformer-based speech model. Building on the state-of-the-art EEG decoder from Meta, we introduce three architectural modifications: (i) subject-specific attention layers (+0.15% WER improvement), (ii) personalized spatial attention (+0.45%), and (iii) a dual-path RNN with attention (-1.87%). Two of the three modifications improved performance, highlighting the promise of personalized architectures for brain-to-speech decoding and applications in brain-computer interfaces.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning Up the Instruction Ladder for Controllable Language Models</title>
<link>https://arxiv.org/abs/2511.04694</link>
<guid>https://arxiv.org/abs/2511.04694</guid>
<content:encoded><![CDATA[
<div> instruction hierarchy, large language models, reasoning task, VerIH dataset, reinforcement learning

Summary:
Large language model (LLM) systems are playing significant roles in decision-making, but they must manage conflicting instructions effectively. This study introduces the concept of instruction hierarchy (IH) in LLMs to prioritize directives and ensure reliable and controllable behavior. By reframing IH resolution as a reasoning task, a new dataset called VerIH is created to train models on instruction hierarchy. Using lightweight reinforcement learning on VerIH, models develop reasoning capabilities that improve instruction following and hierarchy resolution. These models demonstrate enhanced performance in safety-critical scenarios and resistance to attacks, as they can effectively resolve conflicts between user inputs and system instructions. This approach offers a practical pathway to enhance the reliability and controllability of LLMs in real-world applications. 

<br /><br />Summary: <div>
arXiv:2511.04694v1 Announce Type: cross 
Abstract: As large language model (LLM) based systems take on high-stakes roles in real-world decision-making, they must reconcile competing instructions from multiple sources (e.g., model developers, users, and tools) within a single prompt context. Thus, enforcing an instruction hierarchy (IH) in LLMs, where higher-level directives override lower-priority requests, is critical for the reliability and controllability of LLMs. In this work, we reframe instruction hierarchy resolution as a reasoning task. Specifically, the model must first "think" about the relationship between a given user prompt and higher-priority (system) instructions before generating a response. To enable this capability via training, we construct VerIH, an instruction hierarchy dataset of constraint-following tasks with verifiable answers. This dataset comprises both aligned and conflicting system-user instructions. We show that lightweight reinforcement learning with VerIH effectively transfers general reasoning capabilities of models to instruction prioritization. Our finetuned models achieve consistent improvements on instruction following and instruction hierarchy benchmarks. This reasoning ability also generalizes to safety-critical settings beyond the training distribution. By treating safety issues as resolving conflicts between adversarial user inputs and predefined higher-priority policies, our trained model enhances robustness against jailbreak and prompt injection attacks. These results demonstrate that reasoning over instruction hierarchies provides a practical path to reliable LLMs, where updates to system prompts yield controllable and robust changes in model behavior.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EncouRAGe: Evaluating RAG Local, Fast, and Reliable</title>
<link>https://arxiv.org/abs/2511.04696</link>
<guid>https://arxiv.org/abs/2511.04696</guid>
<content:encoded><![CDATA[
<div> Keywords: EncouRAGe, Python framework, Retrieval-Augmented Generation, Large Language Models, Evaluation metrics

Summary:
EncouRAGe is a Python framework developed to simplify the creation and evaluation of Retrieval-Augmented Generation (RAG) systems using Large Language Models (LLMs) and Embedding Models. The framework consists of five components: Type Manifest, RAG Factory, Inference, Vector Store, and Metrics, allowing for flexible experimentation and development. It focuses on scientific reproducibility, diverse evaluation metrics, and local deployment, enabling efficient assessment of datasets within RAG workflows. Despite extensive evaluation on benchmark datasets, RAG systems still perform below the Oracle Context, with Hybrid BM25 consistently outperforming RAG across all datasets. Reranking strategies only yield marginal performance improvements with increased response latency. This study highlights the challenges and opportunities in developing and evaluating RAG systems, emphasizing the importance of comprehensive frameworks like EncouRAGe. 

<br /><br />Summary: <div>
arXiv:2511.04696v1 Announce Type: cross 
Abstract: We introduce EncouRAGe, a comprehensive Python framework designed to streamline the development and evaluation of Retrieval-Augmented Generation (RAG) systems using Large Language Models (LLMs) and Embedding Models. EncouRAGe comprises five modular and extensible components: Type Manifest, RAG Factory, Inference, Vector Store, and Metrics, facilitating flexible experimentation and extensible development. The framework emphasizes scientific reproducibility, diverse evaluation metrics, and local deployment, enabling researchers to efficiently assess datasets within RAG workflows. This paper presents implementation details and an extensive evaluation across multiple benchmark datasets, including 25k QA pairs and over 51k documents. Our results show that RAG still underperforms compared to the Oracle Context, while Hybrid BM25 consistently achieves the best results across all four datasets. We further examine the effects of reranking, observing only marginal performance improvements accompanied by higher response latency.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simulating Misinformation Vulnerabilities With Agent Personas</title>
<link>https://arxiv.org/abs/2511.04697</link>
<guid>https://arxiv.org/abs/2511.04697</guid>
<content:encoded><![CDATA[
<div> Keywords: disinformation, agent-based simulation, Large Language Models, misinformation, mental schemas 

Summary: 
Disinformation campaigns can manipulate public perception and destabilize institutions. Understanding how different populations react to misinformation is essential for effective interventions, but real-world experiments are impractical and ethically challenging. This study develops an agent-based simulation using Large Language Models (LLMs) to model responses to misinformation. Agent personas from various professions and mental schemas are constructed, revealing that LLM-generated agents closely align with human predictions and ground-truth labels. The study also highlights that mental schemas, rather than professional backgrounds, significantly influence how agents interpret misinformation. This work validates the use of LLMs as agents in modeling information networks for analyzing trust, polarization, and susceptibility to deceptive content in complex social systems. 

Summary: <div>
arXiv:2511.04697v1 Announce Type: cross 
Abstract: Disinformation campaigns can distort public perception and destabilize institutions. Understanding how different populations respond to information is crucial for designing effective interventions, yet real-world experimentation is impractical and ethically challenging. To address this, we develop an agent-based simulation using Large Language Models (LLMs) to model responses to misinformation. We construct agent personas spanning five professions and three mental schemas, and evaluate their reactions to news headlines. Our findings show that LLM-generated agents align closely with ground-truth labels and human predictions, supporting their use as proxies for studying information responses. We also find that mental schemas, more than professional background, influence how agents interpret misinformation. This work provides a validation of LLMs to be used as agents in an agent-based model of an information network for analyzing trust, polarization, and susceptibility to deceptive content in complex social systems.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>multiMentalRoBERTa: A Fine-tuned Multiclass Classifier for Mental Health Disorder</title>
<link>https://arxiv.org/abs/2511.04698</link>
<guid>https://arxiv.org/abs/2511.04698</guid>
<content:encoded><![CDATA[
<div> Keywords: mental health disorders, social media text, multiMentalRoBERTa, classification, explainability methods <br />
<br />
Summary: 
The study introduces multiMentalRoBERTa, a RoBERTa model tailored for classifying mental health conditions from social media text. It analyzes overlaps between classes and finds correlations between certain conditions. Comparative experiments show that multiMentalRoBERTa outperforms other methods in classification accuracy. The model's explainability methods help identify key features driving classification decisions, particularly distinguishing between depression and suicidal ideation. The study emphasizes the effectiveness of fine-tuned transformers for sensitive detection tasks and underscores the importance of fairness, bias mitigation, and safety protocols. Overall, multiMentalRoBERTa is presented as a lightweight, robust, and deployable solution for improving mental health support platforms. <div>
arXiv:2511.04698v1 Announce Type: cross 
Abstract: The early detection of mental health disorders from social media text is critical for enabling timely support, risk assessment, and referral to appropriate resources. This work introduces multiMentalRoBERTa, a fine-tuned RoBERTa model designed for multiclass classification of common mental health conditions, including stress, anxiety, depression, post-traumatic stress disorder (PTSD), suicidal ideation, and neutral discourse. Drawing on multiple curated datasets, data exploration is conducted to analyze class overlaps, revealing strong correlations between depression and suicidal ideation as well as anxiety and PTSD, while stress emerges as a broad, overlapping category. Comparative experiments with traditional machine learning methods, domain-specific transformers, and prompting-based large language models demonstrate that multiMentalRoBERTa achieves superior performance, with macro F1-scores of 0.839 in the six-class setup and 0.870 in the five-class setup (excluding stress), outperforming both fine-tuned MentalBERT and baseline classifiers. Beyond predictive accuracy, explainability methods, including Layer Integrated Gradients and KeyBERT, are applied to identify lexical cues that drive classification, with a particular focus on distinguishing depression from suicidal ideation. The findings emphasize the effectiveness of fine-tuned transformers for reliable and interpretable detection in sensitive contexts, while also underscoring the importance of fairness, bias mitigation, and human-in-the-loop safety protocols. Overall, multiMentalRoBERTa is presented as a lightweight, robust, and deployable solution for enhancing support in mental health platforms.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Separate the Wheat from the Chaff: Winnowing Down Divergent Views in Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2511.04700</link>
<guid>https://arxiv.org/abs/2511.04700</guid>
<content:encoded><![CDATA[
<div> Keywords: RAG, large language models, external knowledge, retrieval, noisy documents<br />
Summary: <br />
The article introduces a novel framework called WinnowRAG, which aims to enhance large language models (LLMs) by integrating external knowledge sources for improved information retrieval. By increasing the number of retrieved documents, the framework utilizes query-aware clustering to group similar documents and assign them to LLM agents for generating unique responses. Winnowing, a process of filtering out noisy documents while preserving valuable content, is implemented by a critic LLM in the second stage. To ensure only relevant knowledge is used, strategic merging techniques are proposed. WinnowRAG is model-agnostic and effectively filters noisy documents without requiring model fine-tuning. Experimental results on various datasets show the superiority of WinnowRAG over existing baselines in generating accurate responses. <br />Summary: <div>
arXiv:2511.04700v1 Announce Type: cross 
Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge sources to address their limitations in accessing up-to-date or specialized information. A natural strategy to increase the likelihood of retrieving relevant information is to expand the number of retrieved documents. However, involving more documents could introduce significant noise, as many documents may be irrelevant or misleading, thereby reducing the overall accuracy of the generated responses. To overcome the challenge associated with handling a larger number of documents, we propose WinnowRAG, a novel RAG framework designed to systematically filter out noisy documents while preserving valuable content -- a process we refer to as winnowing. WinnowRAG operates in two stages: In Stage I, we perform query-aware clustering to group similar documents and form distinct topic clusters. Each cluster is assigned to an LLM agent for generating a unique answer. In Stage II, we perform winnowing, wherein a critic LLM evaluates the outputs of multiple agents and iteratively separates useful documents from noisy ones. To retain useful documents when discarding agents, we propose two strategic merging techniques to ensure that only relevant knowledge is used for generating the final response. Crucially, WinnowRAG is model-agnostic and does not require any model fine-tuning, making it easily adaptable to various tasks. Extensive experiments on various realistic datasets demonstrate the effectiveness of WinnowRAG over state-of-the-art baselines.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring what Matters: Construct Validity in Large Language Model Benchmarks</title>
<link>https://arxiv.org/abs/2511.04703</link>
<guid>https://arxiv.org/abs/2511.04703</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Evaluation, Benchmarks, Construct Validity, Recommendations

Summary: 
The study evaluates the measurement of large language models (LLMs) through a systematic review of 445 LLM benchmarks. It emphasizes the importance of construct validity in assessing complex phenomena like safety and robustness. The researchers identify patterns in the reviewed benchmarks that raise concerns about the validity of their claims. To address these issues, they provide eight key recommendations and actionable guidance for researchers and practitioners working with LLM benchmarks. This includes improving the alignment between measured phenomena, tasks, and scoring metrics, as well as promoting transparency and reproducibility in benchmark development. By highlighting these shortcomings and offering practical solutions, the study aims to enhance the reliability and effectiveness of evaluating LLMs.  <br /><br />Summary: <div>
arXiv:2511.04703v1 Announce Type: cross 
Abstract: Evaluating large language models (LLMs) is crucial for both assessing their capabilities and identifying safety or robustness issues prior to deployment. Reliably measuring abstract and complex phenomena such as 'safety' and 'robustness' requires strong construct validity, that is, having measures that represent what matters to the phenomenon. With a team of 29 expert reviewers, we conduct a systematic review of 445 LLM benchmarks from leading conferences in natural language processing and machine learning. Across the reviewed articles, we find patterns related to the measured phenomena, tasks, and scoring metrics which undermine the validity of the resulting claims. To address these shortcomings, we provide eight key recommendations and detailed actionable guidance to researchers and practitioners in developing LLM benchmarks.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>POLIS-Bench: Towards Multi-Dimensional Evaluation of LLMs for Bilingual Policy Tasks in Governmental Scenarios</title>
<link>https://arxiv.org/abs/2511.04705</link>
<guid>https://arxiv.org/abs/2511.04705</guid>
<content:encoded><![CDATA[
<div> Evaluation Suite, Bilingual Corpus, Scenario-Grounded Task Design, Dual-Metric Evaluation Framework, Compliance Tasks <br />
<br />
Summary: POLIS-Bench is a new evaluation suite for Language Model models operating in governmental bilingual policy scenarios. It includes an extensive policy corpus, three specialized tasks, and a dual-metric evaluation framework. A large-scale evaluation of over 10 models shows a clear hierarchy of performance, with reasoning models performing well across tasks. The benchmark also allows for fine-tuning a lightweight open-source model, which achieves parity with or surpasses strong proprietary baselines on policy subtasks at a lower cost. This approach provides a cost-effective and compliant solution for real-world governmental deployment, showcasing the importance of model understanding and application in complex policy scenarios. <div>
arXiv:2511.04705v1 Announce Type: cross 
Abstract: We introduce POLIS-Bench, the first rigorous, systematic evaluation suite designed for LLMs operating in governmental bilingual policy scenarios. Compared to existing benchmarks, POLIS-Bench introduces three major advancements. (i) Up-to-date Bilingual Corpus: We construct an extensive, up-to-date policy corpus that significantly scales the effective assessment sample size, ensuring relevance to current governance practice. (ii) Scenario-Grounded Task Design: We distill three specialized, scenario-grounded tasks -- Clause Retrieval & Interpretation, Solution Generation, and the Compliance Judgmen--to comprehensively probe model understanding and application. (iii) Dual-Metric Evaluation Framework: We establish a novel dual-metric evaluation framework combining semantic similarity with accuracy rate to precisely measure both content alignment and task requirement adherence. A large-scale evaluation of over 10 state-of-the-art LLMs on POLIS-Bench reveals a clear performance hierarchy where reasoning models maintain superior cross-task stability and accuracy, highlighting the difficulty of compliance tasks. Furthermore, leveraging our benchmark, we successfully fine-tune a lightweight open-source model. The resulting POLIS series models achieves parity with, or surpasses, strong proprietary baselines on multiple policy subtasks at a significantly reduced cost, providing a cost-effective and compliant path for robust real-world governmental deployment.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prioritize Economy or Climate Action? Investigating ChatGPT Response Differences Based on Inferred Political Orientation</title>
<link>https://arxiv.org/abs/2511.04706</link>
<guid>https://arxiv.org/abs/2511.04706</guid>
<content:encoded><![CDATA[
<div> bias, natural language processing, political orientation, ChatGPT, personalized response

Summary:
- Large Language Models (LLMs) like ChatGPT can quickly provide personalized responses but may infer user demographics, leading to ethical concerns about bias and echo chambers.
- This study investigates how inferred political views affect ChatGPT responses globally and explores the impact of custom instruction and memory features on responses.
- Three personas with different political orientations were created, and ChatGPT's responses aligned with the inferred political views, showing variations in reasoning and vocabulary.
- Responses displayed alignment with political perspectives even with custom instructions and memory features, indicating a left-leaning bias in ChatGPT outputs.
- Analysis showed closest matches between the democratic persona with custom instruction and the neutral persona, supporting the observation of left-leaning responses by ChatGPT. 

<br /><br />Summary: <div>
arXiv:2511.04706v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) distinguish themselves by quickly delivering information and providing personalized responses through natural language prompts. However, they also infer user demographics, which can raise ethical concerns about bias and implicit personalization and create an echo chamber effect. This study aims to explore how inferred political views impact the responses of ChatGPT globally, regardless of the chat session. We also investigate how custom instruction and memory features alter responses in ChatGPT, considering the influence of political orientation. We developed three personas (two politically oriented and one neutral), each with four statements reflecting their viewpoints on DEI programs, abortion, gun rights, and vaccination. We convey the personas' remarks to ChatGPT using memory and custom instructions, allowing it to infer their political perspectives without directly stating them. We then ask eight questions to reveal differences in worldview among the personas and conduct a qualitative analysis of the responses. Our findings indicate that responses are aligned with the inferred political views of the personas, showing varied reasoning and vocabulary, even when discussing similar topics. We also find the inference happening with explicit custom instructions and the implicit memory feature in similar ways. Analyzing response similarities reveals that the closest matches occur between the democratic persona with custom instruction and the neutral persona, supporting the observation that ChatGPT's outputs lean left.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Jailbreaking in the Haystack</title>
<link>https://arxiv.org/abs/2511.04707</link>
<guid>https://arxiv.org/abs/2511.04707</guid>
<content:encoded><![CDATA[
<div> Keywords: long-context language models, NINJA, safety implications, jailbreak attack, HarmBench

Summary: 
NINJA is a method developed to exploit vulnerabilities in long-context language models (LMs) by strategically appending benign content to harmful user goals, significantly increasing the success rates of attacks on models like LLaMA, Qwen, Mistral, and Gemini. The position of harmful goals is crucial in determining the success of the attack. Unlike previous methods, NINJA is low-resource, transferable, less detectable, and compute-optimal, outperforming traditional methods in jailbreaking LMs under fixed compute constraints. The findings highlight that even seemingly harmless extended contexts can introduce fundamental security vulnerabilities in modern language models. <br /><br />Summary: <div>
arXiv:2511.04707v1 Announce Type: cross 
Abstract: Recent advances in long-context language models (LMs) have enabled million-token inputs, expanding their capabilities across complex tasks like computer-use agents. Yet, the safety implications of these extended contexts remain unclear. To bridge this gap, we introduce NINJA (short for Needle-in-haystack jailbreak attack), a method that jailbreaks aligned LMs by appending benign, model-generated content to harmful user goals. Critical to our method is the observation that the position of harmful goals play an important role in safety. Experiments on standard safety benchmark, HarmBench, show that NINJA significantly increases attack success rates across state-of-the-art open and proprietary models, including LLaMA, Qwen, Mistral, and Gemini. Unlike prior jailbreaking methods, our approach is low-resource, transferable, and less detectable. Moreover, we show that NINJA is compute-optimal -- under a fixed compute budget, increasing context length can outperform increasing the number of trials in best-of-N jailbreak. These findings reveal that even benign long contexts -- when crafted with careful goal positioning -- introduce fundamental vulnerabilities in modern LMs.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SWAP: Towards Copyright Auditing of Soft Prompts via Sequential Watermarking</title>
<link>https://arxiv.org/abs/2511.04711</link>
<guid>https://arxiv.org/abs/2511.04711</guid>
<content:encoded><![CDATA[
<div> watermarking, vision-language models, soft prompts, copyright protection, model auditing  
Summary:  
The article discusses the challenges of protecting copyrighted soft prompts in vision-language models, particularly in the context of model auditing. Existing techniques for auditing model ownership are found to be ineffective due to the unique characteristics of prompt learning. Traditional non-intrusive and intrusive auditing methods fail to accurately detect unauthorized use of protected prompts. The proposed solution, Sequential Watermarking for Soft Prompts (SWAP), embeds watermarks in a different and more complex space to ensure effectiveness, harmlessness, and robustness against potential attacks. SWAP encodes watermarks by ordering defender-specified out-of-distribution classes, inspired by CLIP's zero-shot prediction capability. A hypothesis-test-guided verification protocol is designed for SWAP, supported by theoretical analyses and extensive experimental validation on 11 datasets. SWAP proves to be an effective solution for protecting soft prompts in vision-language models.  
<br /><br />Summary: <div>
arXiv:2511.04711v1 Announce Type: cross 
Abstract: Large-scale vision-language models, especially CLIP, have demonstrated remarkable performance across diverse downstream tasks. Soft prompts, as carefully crafted modules that efficiently adapt vision-language models to specific tasks, necessitate effective copyright protection. In this paper, we investigate model copyright protection by auditing whether suspicious third-party models incorporate protected soft prompts. While this can be viewed as a special case of model ownership auditing, our analysis shows that existing techniques are ineffective due to prompt learning's unique characteristics. Non-intrusive auditing is inherently prone to false positives when independent models share similar data distributions with victim models. Intrusive approaches also fail: backdoor methods designed for CLIP cannot embed functional triggers, while extending traditional DNN backdoor techniques to prompt learning suffers from harmfulness and ambiguity challenges. We find that these failures in intrusive auditing stem from the same fundamental reason: watermarking operates within the same decision space as the primary task yet pursues opposing objectives. Motivated by these findings, we propose sequential watermarking for soft prompts (SWAP), which implants watermarks into a different and more complex space. SWAP encodes watermarks through a specific order of defender-specified out-of-distribution classes, inspired by the zero-shot prediction capability of CLIP. This watermark, which is embedded in a more complex space, keeps the original prediction label unchanged, making it less opposed to the primary task. We further design a hypothesis-test-guided verification protocol for SWAP and provide theoretical analyses of success conditions. Extensive experiments on 11 datasets demonstrate SWAP's effectiveness, harmlessness, and robustness against potential adaptive attacks.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>First is Not Really Better Than Last: Evaluating Layer Choice and Aggregation Strategies in Language Model Data Influence Estimation</title>
<link>https://arxiv.org/abs/2511.04715</link>
<guid>https://arxiv.org/abs/2511.04715</guid>
<content:encoded><![CDATA[
<div> influence estimation, Large Language Model, cancellation effect, attention layers, Noise Detection Rate

Summary:
Middle attention layers, rather than first layers, are shown to be better estimators for influence in Large Language Models. The cancellation effect hypothesis, previously believed to guide influence score computation, is deemed unreliable. Alternative methods for aggregating influence scores, such as ranking and vote-based approaches, demonstrate significantly improved performance. The Noise Detection Rate metric is proposed as a more effective evaluation measure for influence score efficacy in LLMs without necessitating model retraining. Extensive experiments across LLMs of varying sizes and types reveal that the traditional belief of first layers being best for influence estimation may not hold true, contrasting with prior research findings. <div>
arXiv:2511.04715v1 Announce Type: cross 
Abstract: Identifying how training samples influence/impact Large Language Model (LLM) decision-making is essential for effectively interpreting model decisions and auditing large-scale datasets. Current training sample influence estimation methods (also known as influence functions) undertake this goal by utilizing information flow through the model via its first-order and higher-order gradient terms. However, owing to the large model sizes of today consisting of billions of parameters, these influence computations are often restricted to some subset of model layers to ensure computational feasibility. Prior seminal work by Yeh et al. (2022) in assessing which layers are best suited for computing language data influence concluded that the first (embedding) layers are the most informative for this purpose, using a hypothesis based on influence scores canceling out (i.e., the cancellation effect). In this work, we propose theoretical and empirical evidence demonstrating how the cancellation effect is unreliable, and that middle attention layers are better estimators for influence. Furthermore, we address the broader challenge of aggregating influence scores across layers, and showcase how alternatives to standard averaging (such as ranking and vote-based methods) can lead to significantly improved performance. Finally, we propose better methods for evaluating influence score efficacy in LLMs without undertaking model retraining, and propose a new metric known as the Noise Detection Rate (NDR) that exhibits strong predictive capability compared to the cancellation effect. Through extensive experiments across LLMs of varying types and scales, we concretely determine that the first (layers) are not necessarily better than the last (layers) for LLM influence estimation, contrasting with prior knowledge in the field.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>P-MIA: A Profiled-Based Membership Inference Attack on Cognitive Diagnosis Models</title>
<link>https://arxiv.org/abs/2511.04716</link>
<guid>https://arxiv.org/abs/2511.04716</guid>
<content:encoded><![CDATA[
<div> privacy concerns, cognitive diagnosis models, membership inference attacks, grey box threat model, profile-based MIA<br />
Summary:<br />
This paper investigates membership inference attacks (MIA) against cognitive diagnosis models (CDMs) used in intelligent education platforms, addressing privacy concerns. A novel grey box threat model is introduced, leveraging explainability features to reverse-engineer internal knowledge state vectors from visualizations like radar charts. A profile-based MIA (P-MIA) framework is proposed, using model prediction probabilities and knowledge state vectors as features. Experiments on real-world datasets show the grey-box attack outperforms black-box baselines. P-MIA is proven effective as an auditing tool to evaluate machine unlearning techniques and their limitations. <div>
arXiv:2511.04716v1 Announce Type: cross 
Abstract: Cognitive diagnosis models (CDMs) are pivotal for creating fine-grained learner profiles in modern intelligent education platforms. However, these models are trained on sensitive student data, raising significant privacy concerns. While membership inference attacks (MIA) have been studied in various domains, their application to CDMs remains a critical research gap, leaving their privacy risks unquantified. This paper is the first to systematically investigate MIA against CDMs. We introduce a novel and realistic grey box threat model that exploits the explainability features of these platforms, where a model's internal knowledge state vectors are exposed to users through visualizations such as radar charts. We demonstrate that these vectors can be accurately reverse-engineered from such visualizations, creating a potent attack surface. Based on this threat model, we propose a profile-based MIA (P-MIA) framework that leverages both the model's final prediction probabilities and the exposed internal knowledge state vectors as features. Extensive experiments on three real-world datasets against mainstream CDMs show that our grey-box attack significantly outperforms standard black-box baselines. Furthermore, we showcase the utility of P-MIA as an auditing tool by successfully evaluating the efficacy of machine unlearning techniques and revealing their limitations.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ada-FCN: Adaptive Frequency-Coupled Network for fMRI-Based Brain Disorder Classification</title>
<link>https://arxiv.org/abs/2511.04718</link>
<guid>https://arxiv.org/abs/2511.04718</guid>
<content:encoded><![CDATA[
<div> Frequency bands, neuronal oscillations, resting-state fMRI, brain disorders, brain functional connectivity networks <br />
<br />
Summary: The article introduces a novel framework that addresses the multi-frequency nature of neuronal oscillations in brain disorders. Existing models often overlook disruptions within specific frequency bands, limiting diagnostic accuracy. The proposed framework utilizes Adaptive Cascade Decomposition to learn task-relevant frequency sub-bands for each brain region, and Frequency-Coupled Connectivity Learning to capture intra- and cross-band interactions in a unified functional network. This unified network informs a message-passing mechanism in the Unified-GCN, generating refined node representations for diagnostic prediction. Experimental results on ADNI and ABIDE datasets show superior performance compared to existing methods. The code for the framework is available on GitHub. <div>
arXiv:2511.04718v1 Announce Type: cross 
Abstract: Resting-state fMRI has become a valuable tool for classifying brain disorders and constructing brain functional connectivity networks
  by tracking BOLD signals across brain regions. However, existing mod els largely neglect the multi-frequency nature of neuronal oscillations,
  treating BOLD signals as monolithic time series. This overlooks the cru cial fact that neurological disorders often manifest as disruptions within
  specific frequency bands, limiting diagnostic sensitivity and specificity.
  While some methods have attempted to incorporate frequency informa tion, they often rely on predefined frequency bands, which may not be
  optimal for capturing individual variability or disease-specific alterations.
  To address this, we propose a novel framework featuring Adaptive Cas cade Decomposition to learn task-relevant frequency sub-bands for each
  brain region and Frequency-Coupled Connectivity Learning to capture
  both intra- and nuanced cross-band interactions in a unified functional
  network. This unified network informs a novel message-passing mecha nism within our Unified-GCN, generating refined node representations
  for diagnostic prediction. Experimental results on the ADNI and ABIDE
  datasets demonstrate superior performance over existing methods. The
  code is available at https://github.com/XXYY20221234/Ada-FCN.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to reason about rare diseases through retrieval-augmented agents</title>
<link>https://arxiv.org/abs/2511.04720</link>
<guid>https://arxiv.org/abs/2511.04720</guid>
<content:encoded><![CDATA[
<div> Rare diseases, AI models, brain MRI, RADAR, retrieval-augmented diagnostic reasoning agents<br />
<br />
Summary:<br />
- RADAR is an agentic system for rare disease detection in brain MRI using AI agents with access to external medical knowledge.<br />
- It embeds case reports and literature using sentence transformers and FAISS indexing for efficient similarity search.<br />
- RADAR does not require additional training and can be integrated with large language models to improve rare pathology recognition.<br />
- On the NOVA dataset of 280 rare diseases, RADAR shows up to a 10.2% performance gain, particularly with open source models like DeepSeek.<br />
- The retrieved examples offer interpretable, literature-grounded explanations, demonstrating the power of retrieval-augmented reasoning for low-prevalence conditions in medical imaging.<br /> <div>
arXiv:2511.04720v1 Announce Type: cross 
Abstract: Rare diseases represent the long tail of medical imaging, where AI models often fail due to the scarcity of representative training data. In clinical workflows, radiologists frequently consult case reports and literature when confronted with unfamiliar findings. Following this line of reasoning, we introduce RADAR, Retrieval Augmented Diagnostic Reasoning Agents, an agentic system for rare disease detection in brain MRI. Our approach uses AI agents with access to external medical knowledge by embedding both case reports and literature using sentence transformers and indexing them with FAISS to enable efficient similarity search. The agent retrieves clinically relevant evidence to guide diagnostic decision making on unseen diseases, without the need of additional training. Designed as a model-agnostic reasoning module, RADAR can be seamlessly integrated with diverse large language models, consistently improving their rare pathology recognition and interpretability. On the NOVA dataset comprising 280 distinct rare diseases, RADAR achieves up to a 10.2% performance gain, with the strongest improvements observed for open source models such as DeepSeek. Beyond accuracy, the retrieved examples provide interpretable, literature grounded explanations, highlighting retrieval-augmented reasoning as a powerful paradigm for low-prevalence conditions in medical imaging.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Temporal convolutional and fusional transformer model with Bi-LSTM encoder-decoder for multi-time-window remaining useful life prediction</title>
<link>https://arxiv.org/abs/2511.04723</link>
<guid>https://arxiv.org/abs/2511.04723</guid>
<content:encoded><![CDATA[
<div> Keywords: Health prediction, Remaining Useful Life (RUL) prediction, Temporal Convolutional Networks (TCNs), Temporal Fusion Transformer (TFT), Bi-LSTM encoder-decoder <br />
Summary: <br />
- Proposed novel framework integrates TCNs and modified TFT with Bi-LSTM encoder-decoder for RUL prediction in industrial systems.
- Framework effectively captures fine-grained temporal dependencies and prioritizes critical features across time.
- Multi-time-window methodology enhances adaptability across diverse operating conditions.
- Extensive evaluations on benchmark datasets show up to a 5.5% reduction in average RMSE, indicating improved predictive accuracy.
- Advances the effectiveness of industrial prognostic systems and showcases the potential of advanced time-series transformers for RUL prediction. <div>
arXiv:2511.04723v1 Announce Type: cross 
Abstract: Health prediction is crucial for ensuring reliability, minimizing downtime, and optimizing maintenance in industrial systems. Remaining Useful Life (RUL) prediction is a key component of this process; however, many existing models struggle to capture fine-grained temporal dependencies while dynamically prioritizing critical features across time for robust prognostics. To address these challenges, we propose a novel framework that integrates Temporal Convolutional Networks (TCNs) for localized temporal feature extraction with a modified Temporal Fusion Transformer (TFT) enhanced by Bi-LSTM encoder-decoder. This architecture effectively bridges short- and long-term dependencies while emphasizing salient temporal patterns. Furthermore, the incorporation of a multi-time-window methodology improves adaptability across diverse operating conditions. Extensive evaluations on benchmark datasets demonstrate that the proposed model reduces the average RMSE by up to 5.5%, underscoring its improved predictive accuracy compared to state-of-the-art methods. By closing critical gaps in current approaches, this framework advances the effectiveness of industrial prognostic systems and highlights the potential of advanced time-series transformers for RUL prediction.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IndicVisionBench: Benchmarking Cultural and Multilingual Understanding in VLMs</title>
<link>https://arxiv.org/abs/2511.04727</link>
<guid>https://arxiv.org/abs/2511.04727</guid>
<content:encoded><![CDATA[
<div> benchmark, Vision-language models, IndicVisionBench, Indian subcontinent, culturally diverse

Summary:
IndicVisionBench is introduced as a large-scale benchmark focusing on the Indian subcontinent, encompassing English and 10 Indian languages. The benchmark covers three multimodal tasks, including OCR, MMT, and VQA, across 13 culturally relevant topics. It consists of approximately 5K images and over 37K QA pairs, providing a unique resource for analyzing cultural and linguistic biases in VLMs. The benchmark also includes a paired parallel corpus of annotations across 10 Indic languages. Evaluation of 8 models reveals significant performance gaps, highlighting the shortcomings of current VLMs in diverse cultural contexts. IndicVisionBench aims to establish a reproducible evaluation framework that encourages inclusive research in the field of multimodal systems.<br /><br />Summary: <div>
arXiv:2511.04727v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) have demonstrated impressive generalization across multimodal tasks, yet most evaluation benchmarks remain Western-centric, leaving open questions about their performance in culturally diverse and multilingual settings. To address this gap, we introduce IndicVisionBench, the first large-scale benchmark centered on the Indian subcontinent. Covering English and 10 Indian languages, our benchmark spans 3 multimodal tasks, including Optical Character Recognition (OCR), Multimodal Machine Translation (MMT), and Visual Question Answering (VQA), covering 6 kinds of question types. Our final benchmark consists of a total of ~5K images and 37K+ QA pairs across 13 culturally grounded topics. In addition, we release a paired parallel corpus of annotations across 10 Indic languages, creating a unique resource for analyzing cultural and linguistic biases in VLMs. We evaluate a broad spectrum of 8 models, from proprietary closed-source systems to open-weights medium and large-scale models. Our experiments reveal substantial performance gaps, underscoring the limitations of current VLMs in culturally diverse contexts. By centering cultural diversity and multilinguality, IndicVisionBench establishes a reproducible evaluation framework that paves the way for more inclusive multimodal research.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trustworthiness Calibration Framework for Phishing Email Detection Using Large Language Models</title>
<link>https://arxiv.org/abs/2511.04728</link>
<guid>https://arxiv.org/abs/2511.04728</guid>
<content:encoded><![CDATA[
<div> Keywords: Phishing emails, Large language models, Trustworthiness Calibration Framework, Trustworthiness Calibration Index, Cross-Dataset Stability

Summary:
The study introduces the Trustworthiness Calibration Framework (TCF) for evaluating phishing detectors based on calibration, consistency, and robustness. A Trustworthiness Calibration Index (TCI) is used to assess the reliability of large language models (LLMs) like GPT-4, LLaMA-3-8B, and DeBERTa-v3-base in detecting phishing emails. The Cross-Dataset Stability (CDS) metric measures the stability of trustworthiness across different datasets. Experiments on five corpora show that GPT-4 has the highest trust profile, followed by LLaMA-3-8B and DeBERTa-v3-base. The study emphasizes that reliability of models varies independently of raw accuracy, highlighting the need for trust-aware evaluation in real-world scenarios. The framework provides a transparent and reproducible method for assessing model dependability in LLM-based phishing detection.<br /><br />Summary: The Trustworthiness Calibration Framework evaluates phishing detectors using three dimensions: calibration, consistency, and robustness. Trustworthiness Calibration Index and Cross-Dataset Stability metrics determine the reliability of large language models in detecting phishing emails. GPT-4 shows the strongest trust profile among tested models, emphasizing the importance of trust-aware evaluation for real-world deployment. This framework establishes a transparent and reproducible foundation for assessing model dependability in LLM-based phishing detection. <div>
arXiv:2511.04728v1 Announce Type: cross 
Abstract: Phishing emails continue to pose a persistent challenge to online communication, exploiting human trust and evading automated filters through realistic language and adaptive tactics. While large language models (LLMs) such as GPT-4 and LLaMA-3-8B achieve strong accuracy in text classification, their deployment in security systems requires assessing reliability beyond benchmark performance. To address this, this study introduces the Trustworthiness Calibration Framework (TCF), a reproducible methodology for evaluating phishing detectors across three dimensions: calibration, consistency, and robustness. These components are integrated into a bounded index, the Trustworthiness Calibration Index (TCI), and complemented by the Cross-Dataset Stability (CDS) metric that quantifies stability of trustworthiness across datasets. Experiments conducted on five corpora, such as SecureMail 2025, Phishing Validation 2024, CSDMC2010, Enron-Spam, and Nazario, using DeBERTa-v3-base, LLaMA-3-8B, and GPT-4 demonstrate that GPT-4 achieves the strongest overall trust profile, followed by LLaMA-3-8B and DeBERTa-v3-base. Statistical analysis confirms that reliability varies independently of raw accuracy, underscoring the importance of trust-aware evaluation for real-world deployment. The proposed framework establishes a transparent and reproducible foundation for assessing model dependability in LLM-based phishing detection.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge-based anomaly detection for identifying network-induced shape artifacts</title>
<link>https://arxiv.org/abs/2511.04729</link>
<guid>https://arxiv.org/abs/2511.04729</guid>
<content:encoded><![CDATA[
<div> anomaly detection, synthetic data, machine learning models, shape artifacts, mammography<br />
<br />Summary:
This work presents a novel knowledge-based anomaly detection method for identifying network-induced shape artifacts in synthetic images. The method utilizes a two-stage framework with a specialized feature extractor analyzing angle gradients along anatomical boundaries and an isolation forest-based anomaly detector. Quantitative evaluation shows high success rates in detecting artifacts in synthetic mammography datasets. A reader study involving imaging scientists confirmed the algorithm's effectiveness, with mean agreement rates of 66% and 68% for different datasets. Kendall-Tau correlations between algorithmic and human rankings were reasonable, indicating agreement in artifact detection. This method enables developers to assess synthetic images for known anatomical constraints, improving dataset quality for machine learning model training and clinical utility. <br /><br />Summary: <div>
arXiv:2511.04729v1 Announce Type: cross 
Abstract: Synthetic data provides a promising approach to address data scarcity for training machine learning models; however, adoption without proper quality assessments may introduce artifacts, distortions, and unrealistic features that compromise model performance and clinical utility. This work introduces a novel knowledge-based anomaly detection method for detecting network-induced shape artifacts in synthetic images. The introduced method utilizes a two-stage framework comprising (i) a novel feature extractor that constructs a specialized feature space by analyzing the per-image distribution of angle gradients along anatomical boundaries, and (ii) an isolation forest-based anomaly detector. We demonstrate the effectiveness of the method for identifying network-induced shape artifacts in two synthetic mammography datasets from models trained on CSAW-M and VinDr-Mammo patient datasets respectively. Quantitative evaluation shows that the method successfully concentrates artifacts in the most anomalous partition (1st percentile), with AUC values of 0.97 (CSAW-syn) and 0.91 (VMLO-syn). In addition, a reader study involving three imaging scientists confirmed that images identified by the method as containing network-induced shape artifacts were also flagged by human readers with mean agreement rates of 66% (CSAW-syn) and 68% (VMLO-syn) for the most anomalous partition, approximately 1.5-2 times higher than the least anomalous partition. Kendall-Tau correlations between algorithmic and human rankings were 0.45 and 0.43 for the two datasets, indicating reasonable agreement despite the challenging nature of subtle artifact detection. This method is a step forward in the responsible use of synthetic data, as it allows developers to evaluate synthetic images for known anatomic constraints and pinpoint and address specific issues to improve the overall quality of a synthetic dataset.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CPO: Condition Preference Optimization for Controllable Image Generation</title>
<link>https://arxiv.org/abs/2511.04753</link>
<guid>https://arxiv.org/abs/2511.04753</guid>
<content:encoded><![CDATA[
<div> ControlNet, ControlNet++, Direct Preference Optimization, Condition Preference Optimization, controllability<br />
Summary:<br />
ControlNet introduces image-based control signals, while ControlNet++ enhances pixel-level cycle consistency. ControlNet++ optimizes low-noise timesteps using a single-step approximation, while Direct Preference Optimization fine-tunes model preference across all timesteps. Condition Preference Optimization improves preference learning by training on control signals rather than generated images. CPO eliminates confounding factors and yields a low-variance training objective. The approach theoretically exhibits lower contrastive loss variance than DPO and achieves superior results. CPO requires less computation and storage for dataset curation. Extensive experiments show CPO significantly improves controllability over ControlNet++ across multiple control types, with error rate reductions in segmentation, human pose, and improvements in edge and depth maps. <br /><br /> <div>
arXiv:2511.04753v1 Announce Type: cross 
Abstract: To enhance controllability in text-to-image generation, ControlNet introduces image-based control signals, while ControlNet++ improves pixel-level cycle consistency between generated images and the input control signal. To avoid the prohibitive cost of back-propagating through the sampling process, ControlNet++ optimizes only low-noise timesteps (e.g., $t < 200$) using a single-step approximation, which not only ignores the contribution of high-noise timesteps but also introduces additional approximation errors. A straightforward alternative for optimizing controllability across all timesteps is Direct Preference Optimization (DPO), a fine-tuning method that increases model preference for more controllable images ($I^{w}$) over less controllable ones ($I^{l}$). However, due to uncertainty in generative models, it is difficult to ensure that win--lose image pairs differ only in controllability while keeping other factors, such as image quality, fixed. To address this, we propose performing preference learning over control conditions rather than generated images. Specifically, we construct winning and losing control signals, $\mathbf{c}^{w}$ and $\mathbf{c}^{l}$, and train the model to prefer $\mathbf{c}^{w}$. This method, which we term \textit{Condition Preference Optimization} (CPO), eliminates confounding factors and yields a low-variance training objective. Our approach theoretically exhibits lower contrastive loss variance than DPO and empirically achieves superior results. Moreover, CPO requires less computation and storage for dataset curation. Extensive experiments show that CPO significantly improves controllability over the state-of-the-art ControlNet++ across multiple control types: over $10\%$ error rate reduction in segmentation, $70$--$80\%$ in human pose, and consistent $2$--$5\%$ reductions in edge and depth maps.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ScheduleStream: Temporal Planning with Samplers for GPU-Accelerated Multi-Arm Task and Motion Planning &amp; Scheduling</title>
<link>https://arxiv.org/abs/2511.04758</link>
<guid>https://arxiv.org/abs/2511.04758</guid>
<content:encoded><![CDATA[
<div> Keywords: Bimanual robots, humanoid robots, Task and Motion Planning, scheduling, ScheduleStream

Summary:
ScheduleStream introduces a framework for planning and scheduling tasks for bimanual and humanoid robots. It addresses the challenge of controlling multiple arms simultaneously by modeling temporal dynamics through hybrid durative actions. The framework allows for asynchronous execution of actions and schedules, enhancing efficiency in completing tasks. GPU acceleration is utilized to expedite planning in Task and Motion Planning & Scheduling (TAMPAS). The domain-independent algorithms implemented in ScheduleStream produce more efficient solutions compared to other methods. Real-world bimanual robot tasks are successfully demonstrated using ScheduleStream. The framework is a significant advancement in enhancing the capabilities of bimanual and humanoid robots in efficiently completing tasks requiring coordination between multiple arms. 

<br /><br />Summary: <div>
arXiv:2511.04758v1 Announce Type: cross 
Abstract: Bimanual and humanoid robots are appealing because of their human-like ability to leverage multiple arms to efficiently complete tasks. However, controlling multiple arms at once is computationally challenging due to the growth in the hybrid discrete-continuous action space. Task and Motion Planning (TAMP) algorithms can efficiently plan in hybrid spaces but generally produce plans, where only one arm is moving at a time, rather than schedules that allow for parallel arm motion. In order to extend TAMP to produce schedules, we present ScheduleStream, the first general-purpose framework for planning & scheduling with sampling operations. ScheduleStream models temporal dynamics using hybrid durative actions, which can be started asynchronously and persist for a duration that's a function of their parameters. We propose domain-independent algorithms that solve ScheduleStream problems without any application-specific mechanisms. We apply ScheduleStream to Task and Motion Planning & Scheduling (TAMPAS), where we use GPU acceleration within samplers to expedite planning. We compare ScheduleStream algorithms to several ablations in simulation and find that they produce more efficient solutions. We demonstrate ScheduleStream on several real-world bimanual robot tasks at https://schedulestream.github.io.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Structure and Representation Learning with Biomedical Applications</title>
<link>https://arxiv.org/abs/2511.04790</link>
<guid>https://arxiv.org/abs/2511.04790</guid>
<content:encoded><![CDATA[
<div> Keywords: representation learning, causal inference, multi-modal data, observational data, perturbational data

Summary:
In the realm of deep learning, representation learning has been instrumental in uncovering underlying patterns in data without explicit labels. However, when it comes to causal tasks such as predicting the impact of interventions, representation learning falls short. To bridge this gap, a fusion of representation learning and causal inference is crucial, especially given the abundance of multi-modal data now available. This article proposes a statistical and computational framework specifically catered to biomedical inquiries, aiming to discover causal variables from observational and perturbational data, leverage multi-modal information for causal variable identification, and optimize perturbations for causal discovery. By utilizing diverse data sources and incorporating causal inference principles into representation learning, this framework offers a promising approach for understanding complex biological systems and making informed decisions based on causal relationships.<br /><br />Summary: <div>
arXiv:2511.04790v1 Announce Type: cross 
Abstract: Massive data collection holds the promise of a better understanding of complex phenomena and, ultimately, better decisions. Representation learning has become a key driver of deep learning applications, as it allows learning latent spaces that capture important properties of the data without requiring any supervised annotations. Although representation learning has been hugely successful in predictive tasks, it can fail miserably in causal tasks including predicting the effect of a perturbation/intervention. This calls for a marriage between representation learning and causal inference. An exciting opportunity in this regard stems from the growing availability of multi-modal data (observational and perturbational, imaging-based and sequencing-based, at the single-cell level, tissue-level, and organism-level). We outline a statistical and computational framework for causal structure and representation learning motivated by fundamental biomedical questions: how to effectively use observational and perturbational data to perform causal discovery on observed causal variables; how to use multi-modal views of the system to learn causal variables; and how to design optimal perturbations.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MDM: Manhattan Distance Mapping of DNN Weights for Parasitic-Resistance-Resilient Memristive Crossbars</title>
<link>https://arxiv.org/abs/2511.04798</link>
<guid>https://arxiv.org/abs/2511.04798</guid>
<content:encoded><![CDATA[
<div> Mapping, Memristive, DNN, Crossbar, Manhattan Distance Mapping

Summary:
Manhattan Distance Mapping (MDM) is a technique for reducing parasitic resistance (PR) nonidealities in memristive bit-sliced compute-in-memory (CIM) crossbars used for deep neural network (DNN) weight mapping. This technique optimizes active-memristor placement by exploiting bit-level structured sparsity. It feeds activations from the denser low-order side and reorders rows based on Manhattan distance to relocate active cells, reducing the nonideality factor (NF). By applying MDM to DNN models on ImageNet-1k, NF is reduced by up to 46% and accuracy under analog distortion is improved by an average of 3.6% in ResNets. This approach provides a lightweight and spatially informed method for improving efficiency in CIM-based DNN accelerators by addressing PR effects and optimizing crossbar tile execution. <br /><br /> <div>
arXiv:2511.04798v1 Announce Type: cross 
Abstract: Manhattan Distance Mapping (MDM) is a post-training deep neural network (DNN) weight mapping technique for memristive bit-sliced compute-in-memory (CIM) crossbars that reduces parasitic resistance (PR) nonidealities.
  PR limits crossbar efficiency by mapping DNN matrices into small crossbar tiles, reducing CIM-based speedup. Each crossbar executes one tile, requiring digital synchronization before the next layer. At this granularity, designers either deploy many small crossbars in parallel or reuse a few sequentially-both increasing analog-to-digital conversions, latency, I/O pressure, and chip area.
  MDM alleviates PR effects by optimizing active-memristor placement. Exploiting bit-level structured sparsity, it feeds activations from the denser low-order side and reorders rows according to the Manhattan distance, relocating active cells toward regions less affected by PR and thus lowering the nonideality factor (NF).
  Applied to DNN models on ImageNet-1k, MDM reduces NF by up to 46% and improves accuracy under analog distortion by an average of 3.6% in ResNets. Overall, it provides a lightweight, spatially informed method for scaling CIM DNN accelerators.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data Efficiency and Transfer Robustness in Biomedical Image Segmentation: A Study of Redundancy and Forgetting with Cellpose</title>
<link>https://arxiv.org/abs/2511.04803</link>
<guid>https://arxiv.org/abs/2511.04803</guid>
<content:encoded><![CDATA[
<div> Dataset quantization, data redundancy, cross domain transfer, catastrophic forgetting, biomedical image segmentation<br />
<br />
Summary:<br />
- Dataset quantization strategy (DQ) reveals data redundancy in training subsets, showing 10% of data is sufficient for image segmentation training, indicating potential for minimal annotations.<br />
- Latent space analysis confirms DQ captures greater feature diversity than random sampling, enhancing model training efficiency.<br />
- Cross domain finetuning experiments show significant performance degradation without retention strategies, highlighting the impact of catastrophic forgetting in model adaptation.<br />
- Selective replay with DQ based reintroduction of 5-10% source data restores performance, while full replay can hinder target adaptation.<br />
- Training domain sequencing improves generalization and reduces forgetting, emphasizing the importance of informed domain ordering in multi-stage transfer learning for biomedical image segmentation efficiency. <div>
arXiv:2511.04803v1 Announce Type: cross 
Abstract: Generalist biomedical image segmentation models such as Cellpose are increasingly applied across diverse imaging modalities and cell types. However, two critical challenges remain underexplored: (1) the extent of training data redundancy and (2) the impact of cross domain transfer on model retention. In this study, we conduct a systematic empirical analysis of these challenges using Cellpose as a case study. First, to assess data redundancy, we propose a simple dataset quantization (DQ) strategy for constructing compact yet diverse training subsets. Experiments on the Cyto dataset show that image segmentation performance saturates with only 10% of the data, revealing substantial redundancy and potential for training with minimal annotations. Latent space analysis using MAE embeddings and t-SNE confirms that DQ selected patches capture greater feature diversity than random sampling. Second, to examine catastrophic forgetting, we perform cross domain finetuning experiments and observe significant degradation in source domain performance, particularly when adapting from generalist to specialist domains. We demonstrate that selective DQ based replay reintroducing just 5-10% of the source data effectively restores source performance, while full replay can hinder target adaptation. Additionally, we find that training domain sequencing improves generalization and reduces forgetting in multi stage transfer. Our findings highlight the importance of data centric design in biomedical image segmentation and suggest that efficient training requires not only compact subsets but also retention aware learning strategies and informed domain ordering. The code is available at https://github.com/MMV-Lab/biomedseg-efficiency.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PuzzleMoE: Efficient Compression of Large Mixture-of-Experts Models via Sparse Expert Merging and Bit-packed inference</title>
<link>https://arxiv.org/abs/2511.04805</link>
<guid>https://arxiv.org/abs/2511.04805</guid>
<content:encoded><![CDATA[
<div> compression, Mixture-of-Experts, PuzzleMoE, sparse merging, bit-packed encoding

Summary:
PuzzleMoE is a novel compression method for Mixture-of-Experts (MoE) models that addresses the high memory overhead issue associated with storing all expert parameters. It achieves high accuracy and efficient inference through sparse expert merging and a bit-packed encoding scheme. PuzzleMoE identifies weight redundancy and specialization for expert merging and uses a dual-mask to capture shared and expert-specific parameters. The introduction of a bit-packed encoding scheme reuses underutilized bits for efficient GPU inference. Extensive experiments show that PuzzleMoE can compress MoE models by up to 50% while maintaining accuracy, outperforming previous compression methods by up to 16.7% on MMLU at a 50% compression ratio, and achieving up to 1.28 inference speedup. <div>
arXiv:2511.04805v1 Announce Type: cross 
Abstract: Mixture-of-Experts (MoE) models have shown strong potential in scaling language models efficiently by activating only a small subset of experts per input. However, their widespread deployment remains limited due to the high memory overhead associated with storing all expert parameters, particularly as the number of experts increases. To address this challenge, prior works have explored expert dropping and merging strategies, yet they often suffer from performance drop at high compression ratios. In this paper, we introduce PuzzleMoE, a training-free MoE compression method that achieves both high accuracy and efficient inference through two key innovations: First, PuzzleMoE performs sparse expert merging by identifying element-wise weight redundancy and specialization. It uses a dual-mask to capture both shared and expert-specific parameters. Second, to avoid the overhead of storing binary masks and signs, PuzzleMoE introduces a bit-packed encoding scheme that reuses underutilized exponent bits, enabling efficient MoE inference on GPUs. Extensive experiments demonstrate that PuzzleMoE can compress MoE models by up to 50% while maintaining accuracy across various tasks. Specifically, it outperforms prior MoE compression methods by up to 16.7% on MMLU at 50% compression ratio, and achieves up to 1.28\times inference speedup.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Active Learning Pipeline for Biomedical Image Instance Segmentation with Minimal Human Intervention</title>
<link>https://arxiv.org/abs/2511.04811</link>
<guid>https://arxiv.org/abs/2511.04811</guid>
<content:encoded><![CDATA[
<div> Keywords: Biomedical image segmentation, nnU-Net, active learning, pseudo-labeling, AI workflow 

Summary: 
This article presents a novel approach to biomedical image segmentation by combining traditional neural networks and large foundation models. The proposed data-centric AI workflow leverages active learning and pseudo-labeling to automate model configuration and minimize manual intervention. The pipeline starts with generating pseudo-labels from a foundation model and using them for nnU-Net's self-configuration. A representative core-set is then selected for minimal manual annotation to fine-tune the model effectively. This approach reduces the need for extensive manual annotations while maintaining competitive segmentation performance. This method provides a practical solution for biomedical researchers to leverage state-of-the-art AI techniques in their segmentation tasks. The code for this approach is available on GitHub for further exploration and implementation. 

<br /><br />Summary: <div>
arXiv:2511.04811v1 Announce Type: cross 
Abstract: Biomedical image segmentation is critical for precise structure delineation and downstream analysis. Traditional methods often struggle with noisy data, while deep learning models such as U-Net have set new benchmarks in segmentation performance. nnU-Net further automates model configuration, making it adaptable across datasets without extensive tuning. However, it requires a substantial amount of annotated data for cross-validation, posing a challenge when only raw images but no labels are available. Large foundation models offer zero-shot generalizability, but may underperform on specific datasets with unique characteristics, limiting their direct use for analysis. This work addresses these bottlenecks by proposing a data-centric AI workflow that leverages active learning and pseudo-labeling to combine the strengths of traditional neural networks and large foundation models while minimizing human intervention. The pipeline starts by generating pseudo-labels from a foundation model, which are then used for nnU-Net's self-configuration. Subsequently, a representative core-set is selected for minimal manual annotation, enabling effective fine-tuning of the nnU-Net model. This approach significantly reduces the need for manual annotations while maintaining competitive performance, providing an accessible solution for biomedical researchers to apply state-of-the-art AI techniques in their segmentation tasks. The code is available at https://github.com/MMV-Lab/AL_BioMed_img_seg.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Multimodal Diffusion Forcing for Forceful Manipulation</title>
<link>https://arxiv.org/abs/2511.04812</link>
<guid>https://arxiv.org/abs/2511.04812</guid>
<content:encoded><![CDATA[
<div> Keywords: imitation learning, multimodal trajectories, diffusion model, forceful manipulation, robustness

Summary:
MDF (Multimodal Diffusion Forcing) is a framework for learning from expert trajectories that captures temporal and cross-modal dependencies in robot behaviors. Unlike traditional approaches, MDF goes beyond simple action generation by considering the interplay between sensory inputs, actions, and rewards. By applying random partial masking and training a diffusion model to reconstruct trajectories, MDF encourages the model to learn diverse functionalities such as predicting the effects of actions on force signals and inferring states from partial observations. The framework is evaluated on contact-rich, forceful manipulation tasks in both simulated and real-world environments, demonstrating strong performance and robustness in the face of noisy data. By incorporating multimodal information, MDF offers a versatile and effective approach for modeling robot behavior and understanding task outcomes. Visualizations of the results can be found on the project's website for further exploration. 

<br /><br />Summary: <div>
arXiv:2511.04812v1 Announce Type: cross 
Abstract: Given a dataset of expert trajectories, standard imitation learning approaches typically learn a direct mapping from observations (e.g., RGB images) to actions. However, such methods often overlook the rich interplay between different modalities, i.e., sensory inputs, actions, and rewards, which is crucial for modeling robot behavior and understanding task outcomes. In this work, we propose Multimodal Diffusion Forcing, a unified framework for learning from multimodal robot trajectories that extends beyond action generation. Rather than modeling a fixed distribution, MDF applies random partial masking and trains a diffusion model to reconstruct the trajectory. This training objective encourages the model to learn temporal and cross-modal dependencies, such as predicting the effects of actions on force signals or inferring states from partial observations. We evaluate MDF on contact-rich, forceful manipulation tasks in simulated and real-world environments. Our results show that MDF not only delivers versatile functionalities, but also achieves strong performance, and robustness under noisy observations. More visualizations can be found on our website https://unified-df.github.io
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Standardized Benchmark for Multilabel Antimicrobial Peptide Classification</title>
<link>https://arxiv.org/abs/2511.04814</link>
<guid>https://arxiv.org/abs/2511.04814</guid>
<content:encoded><![CDATA[
<div> Keywords: antimicrobial peptides, ESCAPE, dataset, multilabel classification, AI-driven research

Summary:
The article introduces the Expanded Standardized Collection for Antimicrobial Peptide Evaluation (ESCAPE), a dataset containing over 80,000 peptides from validated repositories. ESCAPE separates antimicrobial peptides from negative sequences and includes functional annotations in a hierarchical structure covering antibacterial, antifungal, antiviral, and antiparasitic activities. A transformer-based model is proposed to predict multiple functional activities of peptides, achieving a 2.56% relative average improvement in mean Average Precision compared to existing methods. This model establishes a new state-of-the-art multilabel peptide classification approach. ESCAPE provides a comprehensive and reproducible evaluation framework for AI-driven research on antimicrobial peptides. 

<br /><br />Summary: <div>
arXiv:2511.04814v1 Announce Type: cross 
Abstract: Antimicrobial peptides have emerged as promising molecules to combat antimicrobial resistance. However, fragmented datasets, inconsistent annotations, and the lack of standardized benchmarks hinder computational approaches and slow down the discovery of new candidates. To address these challenges, we present the Expanded Standardized Collection for Antimicrobial Peptide Evaluation (ESCAPE), an experimental framework integrating over 80.000 peptides from 27 validated repositories. Our dataset separates antimicrobial peptides from negative sequences and incorporates their functional annotations into a biologically coherent multilabel hierarchy, capturing activities across antibacterial, antifungal, antiviral, and antiparasitic classes. Building on ESCAPE, we propose a transformer-based model that leverages sequence and structural information to predict multiple functional activities of peptides. Our method achieves up to a 2.56% relative average improvement in mean Average Precision over the second-best method adapted for this task, establishing a new state-of-the-art multilabel peptide classification. ESCAPE provides a comprehensive and reproducible evaluation framework to advance AI-driven antimicrobial peptide research.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Isaac Lab: A GPU-Accelerated Simulation Framework for Multi-Modal Robot Learning</title>
<link>https://arxiv.org/abs/2511.04831</link>
<guid>https://arxiv.org/abs/2511.04831</guid>
<content:encoded><![CDATA[
<div> physics, rendering, simulation, robotics, learning
Summary:
Isaac Lab, successor to Isaac Gym, expands GPU-native robotics simulation into large-scale multi-modal learning. It offers high-fidelity GPU parallel physics, realistic rendering, modular environment design, and policy training. The framework incorporates actuator models, sensor simulation, data pipelines, and domain randomization for reinforcement and imitation learning. It addresses challenges like whole-body control, cross-embodiment mobility, manipulation, and human skill acquisition. Integration with a differentiable Newton physics engine will enable scalable, data-efficient, gradient-based robot learning. Isaac Lab's advanced simulation, rich sensing, and data-center scale execution are expected to drive breakthroughs in robotics research. 
<br /><br />Summary: <div>
arXiv:2511.04831v1 Announce Type: cross 
Abstract: We present Isaac Lab, the natural successor to Isaac Gym, which extends the paradigm of GPU-native robotics simulation into the era of large-scale multi-modal learning. Isaac Lab combines high-fidelity GPU parallel physics, photorealistic rendering, and a modular, composable architecture for designing environments and training robot policies. Beyond physics and rendering, the framework integrates actuator models, multi-frequency sensor simulation, data collection pipelines, and domain randomization tools, unifying best practices for reinforcement and imitation learning at scale within a single extensible platform. We highlight its application to a diverse set of challenges, including whole-body control, cross-embodiment mobility, contact-rich and dexterous manipulation, and the integration of human demonstrations for skill acquisition. Finally, we discuss upcoming integration with the differentiable, GPU-accelerated Newton physics engine, which promises new opportunities for scalable, data-efficient, and gradient-based approaches to robot learning. We believe Isaac Lab's combination of advanced simulation capabilities, rich sensing, and data-center scale execution will help unlock the next generation of breakthroughs in robotics research.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt-Based Safety Guidance Is Ineffective for Unlearned Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2511.04834</link>
<guid>https://arxiv.org/abs/2511.04834</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-image generative models, harmful content, negative prompts, concept inversion, defense performance

Summary:
Recent advancements in text-to-image generative models have raised concerns about the potential generation of harmful content when provided with malicious input prompts. There are two main approaches to address this issue: fine-tuning the model to unlearn harmful concepts and using training-free methods that leverage negative prompts. However, combining these approaches often results in marginal or degraded defense performance due to an inherent incompatibility between them. In this study, a new method is proposed that replaces negative prompts with implicit negative embeddings obtained through concept inversion. This method requires no modifications to existing approaches and shows improved defense success rates on nudity and violence benchmarks. The approach effectively enhances defense mechanisms while maintaining the core semantics of input prompts. Overall, this method offers a simple yet robust solution for mitigating the generation of harmful content by text-to-image generative models. 

<br /><br />Summary: <div>
arXiv:2511.04834v1 Announce Type: cross 
Abstract: Recent advances in text-to-image generative models have raised concerns about their potential to produce harmful content when provided with malicious input text prompts. To address this issue, two main approaches have emerged: (1) fine-tuning the model to unlearn harmful concepts and (2) training-free guidance methods that leverage negative prompts. However, we observe that combining these two orthogonal approaches often leads to marginal or even degraded defense performance. This observation indicates a critical incompatibility between two paradigms, which hinders their combined effectiveness. In this work, we address this issue by proposing a conceptually simple yet experimentally robust method: replacing the negative prompts used in training-free methods with implicit negative embeddings obtained through concept inversion. Our method requires no modification to either approach and can be easily integrated into existing pipelines. We experimentally validate its effectiveness on nudity and violence benchmarks, demonstrating consistent improvements in defense success rate while preserving the core semantics of input prompts.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Software Defined Vehicle Code Generation: A Few-Shot Prompting Approach</title>
<link>https://arxiv.org/abs/2511.04849</link>
<guid>https://arxiv.org/abs/2511.04849</guid>
<content:encoded><![CDATA[
<div> Keywords: Software-Defined Vehicles, code generation, large language models, prompts, advanced prompt engineering<br />
Summary:<br />
The emergence of Software-Defined Vehicles (SDVs) has led to a shift in the automotive industry, where software plays a crucial role in vehicle functionality. Efficient code generation tools are needed for SDV-specific applications. This study explores the use of prompts to interact with large language models (LLMs) for SDV code generation without access to proprietary model architectures. By employing advanced prompt engineering techniques, system prompts can effectively redirect LLM responses. Experiments with different LLM models and prompting techniques show that a few-shot prompting strategy outperforms others in generating SDV code accurately. This approach offers a way to leverage LLMs for specific tasks like SDV code generation without the need for training sessions or model architecture access.<br /><br />Summary: <div>
arXiv:2511.04849v1 Announce Type: cross 
Abstract: The emergence of Software-Defined Vehicles (SDVs) marks a paradigm shift in the automotive industry, where software now plays a pivotal role in defining vehicle functionality, enabling rapid innovation of modern vehicles. Developing SDV-specific applications demands advanced tools to streamline code generation and improve development efficiency. In recent years, general-purpose large language models (LLMs) have demonstrated transformative potential across domains. Still, restricted access to proprietary model architectures hinders their adaption to specific tasks like SDV code generation. In this study, we propose using prompts, a common and basic strategy to interact with LLMs and redirect their responses. Using only system prompts with an appropriate and efficient prompt structure designed using advanced prompt engineering techniques, LLMs can be crafted without requiring a training session or access to their base design. This research investigates the extensive experiments on different models by applying various prompting techniques, including bare models, using a benchmark specifically created to evaluate LLMs' performance in generating SDV code. The results reveal that the model with a few-shot prompting strategy outperforms the others in adjusting the LLM answers to match the expected outcomes based on quantitative metrics.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Minimal and Mechanistic Conditions for Behavioral Self-Awareness in LLMs</title>
<link>https://arxiv.org/abs/2511.04875</link>
<guid>https://arxiv.org/abs/2511.04875</guid>
<content:encoded><![CDATA[
<div> self-awareness, LLMs, fine-tuning, LoRA, domain-specific <br />
Summary:
Self-awareness in large language models (LLMs) has been shown to emerge through fine-tuning with low-rank adapters (LoRA). Using a single rank-1 LoRA adapter, self-aware behavior can be reliably induced and captured by a single steering vector in activation space. This self-awareness is non-universal and domain-localized, with distinct representations across tasks. The findings suggest that behavioral self-awareness is a linear, domain-specific feature that can be easily induced and modulated. <div>
arXiv:2511.04875v1 Announce Type: cross 
Abstract: Recent studies have revealed that LLMs can exhibit behavioral self-awareness: the ability to accurately describe or predict their own learned behaviors without explicit supervision. This capability raises safety concerns as it may, for example, allow models to better conceal their true abilities during evaluation. We attempt to characterize the minimal conditions under which such self-awareness emerges, and the mechanistic processes through which it manifests. Through controlled finetuning experiments on instruction-tuned LLMs with low-rank adapters (LoRA), we find: (1) that self-awareness can be reliably induced using a single rank-1 LoRA adapter; (2) that the learned self-aware behavior can be largely captured by a single steering vector in activation space, recovering nearly all of the fine-tune's behavioral effect; and (3) that self-awareness is non-universal and domain-localized, with independent representations across tasks. Together, these findings suggest that behavioral self-awareness emerges as a domain-specific, linear feature that can be easily induced and modulated.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beta Distribution Learning for Reliable Roadway Crash Risk Assessment</title>
<link>https://arxiv.org/abs/2511.04886</link>
<guid>https://arxiv.org/abs/2511.04886</guid>
<content:encoded><![CDATA[
<div> satellite imagery, geospatial deep learning framework, fatal crash risk, uncertainty-aware predictions, roadway safety 

Summary:
This article introduces a novel geospatial deep learning framework that utilizes satellite imagery to assess fatal crash risks on roadways. Traditional studies often overlook the complex spatial and contextual interactions in the built environment. The framework generates a full Beta probability distribution over fatal crash risk, providing not only accurate predictions but also model uncertainty. This approach outperforms baselines by improving recall by 17-23% and offers superior calibration for flagging potential dangers. By analyzing satellite imagery alone, the model provides reliable and interpretable risk assessments, benefiting autonomous navigation safety and enabling urban planners and policymakers to enhance roadway safety equitably and cost-effectively. <div>
arXiv:2511.04886v1 Announce Type: cross 
Abstract: Roadway traffic accidents represent a global health crisis, responsible for over a million deaths annually and costing many countries up to 3% of their GDP. Traditional traffic safety studies often examine risk factors in isolation, overlooking the spatial complexity and contextual interactions inherent in the built environment. Furthermore, conventional Neural Network-based risk estimators typically generate point estimates without conveying model uncertainty, limiting their utility in critical decision-making. To address these shortcomings, we introduce a novel geospatial deep learning framework that leverages satellite imagery as a comprehensive spatial input. This approach enables the model to capture the nuanced spatial patterns and embedded environmental risk factors that contribute to fatal crash risks. Rather than producing a single deterministic output, our model estimates a full Beta probability distribution over fatal crash risk, yielding accurate and uncertainty-aware predictions--a critical feature for trustworthy AI in safety-critical applications. Our model outperforms baselines by achieving a 17-23% improvement in recall, a key metric for flagging potential dangers, while delivering superior calibration. By providing reliable and interpretable risk assessments from satellite imagery alone, our method enables safer autonomous navigation and offers a highly scalable tool for urban planners and policymakers to enhance roadway safety equitably and cost-effectively.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>You Need Reasoning to Learn Reasoning: The Limitations of Label-Free RL in Weak Base Models</title>
<link>https://arxiv.org/abs/2511.04902</link>
<guid>https://arxiv.org/abs/2511.04902</guid>
<content:encoded><![CDATA[
<div> Keywords: label-free RL, model sizes, reasoning capabilities, curriculum learning, data curation<br />
Summary:<br />
Recent research has focused on the potential of unsupervised reinforcement learning (RL) methods to improve reasoning abilities without external supervision. However, the effectiveness of these label-free RL methods on smaller base models with limited reasoning capabilities has not been thoroughly explored. The study investigates the performance of label-free RL across a range of model sizes and reasoning strengths, revealing that the success of this approach is heavily dependent on the base model's pre-existing reasoning capability. Smaller models often struggle to generate complex chains of thought necessary for effective self-reflection. To overcome these challenges, the authors propose a method that incorporates curriculum learning to gradually introduce more difficult problems during training and masks no-majority rollouts. They also introduce a data curation pipeline to generate samples with predefined difficulty levels. Through these enhancements, the proposed approach consistently improves performance across all model sizes and reasoning capabilities, offering a pathway to enhance unsupervised RL in resource-constrained models.<br /><br />Summary: <div>
arXiv:2511.04902v1 Announce Type: cross 
Abstract: Recent advances in large language models have demonstrated the promise of unsupervised reinforcement learning (RL) methods for enhancing reasoning capabilities without external supervision. However, the generalizability of these label-free RL approaches to smaller base models with limited reasoning capabilities remains unexplored. In this work, we systematically investigate the performance of label-free RL methods across different model sizes and reasoning strengths, from 0.5B to 7B parameters. Our empirical analysis reveals critical limitations: label-free RL is highly dependent on the base model's pre-existing reasoning capability, with performance often degrading below baseline levels for weaker models. We find that smaller models fail to generate sufficiently long or diverse chain-of-thought reasoning to enable effective self-reflection, and that training data difficulty plays a crucial role in determining success. To address these challenges, we propose a simple yet effective method for label-free RL that utilizes curriculum learning to progressively introduce harder problems during training and mask no-majority rollouts during training. Additionally, we introduce a data curation pipeline to generate samples with predefined difficulty. Our approach demonstrates consistent improvements across all model sizes and reasoning capabilities, providing a path toward more robust unsupervised RL that can bootstrap reasoning abilities in resource-constrained models. We make our code available at https://github.com/BorealisAI/CuMa
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Dual Perspective on Decision-Focused Learning: Scalable Training via Dual-Guided Surrogates</title>
<link>https://arxiv.org/abs/2511.04909</link>
<guid>https://arxiv.org/abs/2511.04909</guid>
<content:encoded><![CDATA[
<div> dual variables, decision-focused learning, combinatorial selection problems, Dual-Guided Loss, optimizer calls<br />
<br />
Summary:<br />
This paper introduces Dual-Guided Loss (DGL), a scalable approach for decision-focused learning in optimization problems. It addresses the challenge of scaling decision-focused learning by using dual variables from the downstream problem to shape learning. DGL decouples optimization from gradient updates, trains on dual-adjusted targets using differentiable surrogate losses, and drives training cost towards standard supervised learning. The approach is specifically tailored for combinatorial selection problems with one-of-many constraints. DGL offers asymptotically diminishing decision regret, has a lower runtime complexity, and outperforms state-of-the-art decision-focused learning methods while using fewer solver calls and less training time. The code for DGL is available on GitHub for further exploration and implementation. <div>
arXiv:2511.04909v1 Announce Type: cross 
Abstract: Many real-world decisions are made under uncertainty by solving optimization problems using predicted quantities. This predict-then-optimize paradigm has motivated decision-focused learning, which trains models with awareness of how the optimizer uses predictions, improving the performance of downstream decisions. Despite its promise, scaling is challenging: state-of-the-art methods either differentiate through a solver or rely on task-specific surrogates, both of which require frequent and expensive calls to an optimizer, often a combinatorial one. In this paper, we leverage dual variables from the downstream problem to shape learning and introduce Dual-Guided Loss (DGL), a simple, scalable objective that preserves decision alignment while reducing solver dependence. We construct DGL specifically for combinatorial selection problems with natural one-of-many constraints, such as matching, knapsack, and shortest path. Our approach (a) decouples optimization from gradient updates by solving the downstream problem only periodically; (b) between refreshes, trains on dual-adjusted targets using simple differentiable surrogate losses; and (c) as refreshes become less frequent, drives training cost toward standard supervised learning while retaining strong decision alignment. We prove that DGL has asymptotically diminishing decision regret, analyze runtime complexity, and show on two problem classes that DGL matches or exceeds state-of-the-art DFL methods while using far fewer solver calls and substantially less training time. Code is available at https://github.com/paularodr/Dual-Guided-Learning.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MERaLiON-SER: Robust Speech Emotion Recognition Model for English and SEA Languages</title>
<link>https://arxiv.org/abs/2511.04914</link>
<guid>https://arxiv.org/abs/2511.04914</guid>
<content:encoded><![CDATA[
<div> Keywords: MERaLiON-SER, speech emotion recognition, hybrid objective, categorical cross-entropy, Concordance Correlation Coefficient

Summary:<br /><br />The article presents MERaLiON-SER, a robust speech emotion recognition model designed for English and Southeast Asian languages. It utilizes a hybrid objective combining weighted categorical cross-entropy and Concordance Correlation Coefficient (CCC) losses to jointly model discrete and dimensional emotions. This approach captures both distinct emotion categories and fine-grained aspects like arousal, valence, and dominance, enhancing human affect representation. Evaluations across multilingual Singaporean languages and public benchmarks demonstrate MERaLiON-SER's superior performance over open-source speech encoders and large Audio-LLMs. The results highlight the significance of specialized speech-only models for accurate paralinguistic understanding and cross-lingual generalization. The proposed framework lays the groundwork for integrating emotion-aware perception into future agentic audio systems, enabling more empathetic and contextually adaptive multimodal reasoning.<br /><br />Summary: <div>
arXiv:2511.04914v1 Announce Type: cross 
Abstract: We present MERaLiON-SER, a robust speech emotion recognition model de- signed for English and Southeast Asian languages. The model is trained using a hybrid objective combining weighted categorical cross-entropy and Concordance Correlation Coefficient (CCC) losses for joint discrete and dimensional emotion modelling. This dual approach enables the model to capture both the distinct categories of emotion (like happy or angry) and the fine-grained, such as arousal (intensity), valence (positivity/negativity), and dominance (sense of control), lead- ing to a more comprehensive and robust representation of human affect. Extensive evaluations across multilingual Singaporean languages (English, Chinese, Malay, and Tamil ) and other public benchmarks show that MERaLiON-SER consistently surpasses both open-source speech encoders and large Audio-LLMs. These results underscore the importance of specialised speech-only models for accurate paralin- guistic understanding and cross-lingual generalisation. Furthermore, the proposed framework provides a foundation for integrating emotion-aware perception into future agentic audio systems, enabling more empathetic and contextually adaptive multimodal reasoning.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BudgetMem: Learning Selective Memory Policies for Cost-Efficient Long-Context Processing in Language Models</title>
<link>https://arxiv.org/abs/2511.04919</link>
<guid>https://arxiv.org/abs/2511.04919</guid>
<content:encoded><![CDATA[
<div> Memory-augmented architecture, selective memory policies, feature-based salience scoring, efficient information access, practical long context systems<br />
Summary:<br />Large Language Models (LLMs) often struggle with processing long contexts due to computational and memory constraints. In response, BudgetMem proposes a novel memory augmented architecture that selectively remembers information based on salience scoring. By combining selective memory policies and feature-based scoring, BudgetMem can efficiently store and access key information, achieving remarkable results on long documents while saving memory compared to existing systems. Through comprehensive experiments, BudgetMem's benefits were demonstrated across various document lengths, highlighting its effectiveness in democratizing access to advanced language understanding capabilities. Sensitivity analysis further validated the benefits of BudgetMem, making it a practical solution for deploying capable long context systems on modest hardware. <br /><br />Summary: <div>
arXiv:2511.04919v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) face significant computational and memory constraints when processing long contexts, despite growing demand for applications requiring reasoning over extensive documents, multi-session dialogues, and book length texts. While recent advances have extended context windows to 100K-1M tokens, such approaches incur prohibitive costs for resource constrained deployments. We propose BudgetMem, a novel memory augmented architecture that learns what to remember rather than remembering everything. Our system combines selective memory policies with feature based salience scoring (entity density, TF-IDF, discourse markers, position bias) to decide which information merits storage under strict budget constraints. Unlike existing retrieval augmented generation (RAG) systems that store all chunks, BudgetMem employs learned gating mechanisms coupled with BM25 sparse retrieval for efficient information access. Through comprehensive experiments on 700 question answer pairs across short (237 tokens) and long (5K-10K tokens) documents with Llama-3.2-3B-Instruct, we demonstrate that BudgetMem achieves remarkable results on long documents: only 1.0% F1 score degradation while saving 72.4% memory compared to baseline RAG. We validate our approach through budget sensitivity analysis (testing 7 budget ratios), naive baseline comparisons, and document length analysis, showing that BudgetMem's benefits increase with document length. Our work provides a practical pathway for deploying capable long context systems on modest hardware, democratizing access to advanced language understanding capabilities.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Search Is Not Retrieval: Decoupling Semantic Matching from Contextual Assembly in RAG</title>
<link>https://arxiv.org/abs/2511.04939</link>
<guid>https://arxiv.org/abs/2511.04939</guid>
<content:encoded><![CDATA[
<div> framework, retrieval, search, context, AI
Summary:
The article introduces the Search-Is-Not-Retrieve (SINR) framework, which aims to enhance the performance of retrieval systems in AI pipelines. It distinguishes between fine-grained search representations and coarse-grained retrieval contexts, improving composability, scalability, and context fidelity of retrieval systems. SINR connects small, semantically accurate search chunks to larger, contextually complete retrieve chunks, without additional processing costs. This design transforms retrieval from a passive to an active process, mimicking how humans process information. The article discusses the conceptual foundation, formal structure, implementation issues, and qualitative outcomes of the SINR framework, offering a practical basis for the next generation of AI systems utilizing retrieval.<br /><br />Summary: <div>
arXiv:2511.04939v1 Announce Type: cross 
Abstract: Retrieval systems are essential to contemporary AI pipelines, although most confuse two separate processes: finding relevant information and giving enough context for reasoning. We introduce the Search-Is-Not-Retrieve (SINR) framework, a dual-layer architecture that distinguishes between fine-grained search representations and coarse-grained retrieval contexts. SINR enhances the composability, scalability, and context fidelity of retrieval systems by directly connecting small, semantically accurate search chunks to larger, contextually complete retrieve chunks, all without incurring extra processing costs. This design changes retrieval from a passive step to an active one, making the system architecture more like how people process information. We discuss the SINR framework's conceptual foundation, formal structure, implementation issues, and qualitative outcomes. This provides a practical foundation for the next generation of AI systems that use retrieval.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A benchmark multimodal oro-dental dataset for large vision-language models</title>
<link>https://arxiv.org/abs/2511.04948</link>
<guid>https://arxiv.org/abs/2511.04948</guid>
<content:encoded><![CDATA[
arXiv:2511.04948v1 Announce Type: cross 
Abstract: The advancement of artificial intelligence in oral healthcare relies on the availability of large-scale multimodal datasets that capture the complexity of clinical practice. In this paper, we present a comprehensive multimodal dataset, comprising 8775 dental checkups from 4800 patients collected over eight years (2018-2025), with patients ranging from 10 to 90 years of age. The dataset includes 50000 intraoral images, 8056 radiographs, and detailed textual records, including diagnoses, treatment plans, and follow-up notes. The data were collected under standard ethical guidelines and annotated for benchmarking. To demonstrate its utility, we fine-tuned state-of-the-art large vision-language models, Qwen-VL 3B and 7B, and evaluated them on two tasks: classification of six oro-dental anomalies and generation of complete diagnostic reports from multimodal inputs. We compared the fine-tuned models with their base counterparts and GPT-4o. The fine-tuned models achieved substantial gains over these baselines, validating the dataset and underscoring its effectiveness in advancing AI-driven oro-dental healthcare solutions. The dataset is publicly available, providing an essential resource for future research in AI dentistry.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepForgeSeal: Latent Space-Driven Semi-Fragile Watermarking for Deepfake Detection Using Multi-Agent Adversarial Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.04949</link>
<guid>https://arxiv.org/abs/2511.04949</guid>
<content:encoded><![CDATA[
arXiv:2511.04949v1 Announce Type: cross 
Abstract: Rapid advances in generative AI have led to increasingly realistic deepfakes, posing growing challenges for law enforcement and public trust. Existing passive deepfake detectors struggle to keep pace, largely due to their dependence on specific forgery artifacts, which limits their ability to generalize to new deepfake types. Proactive deepfake detection using watermarks has emerged to address the challenge of identifying high-quality synthetic media. However, these methods often struggle to balance robustness against benign distortions with sensitivity to malicious tampering. This paper introduces a novel deep learning framework that harnesses high-dimensional latent space representations and the Multi-Agent Adversarial Reinforcement Learning (MAARL) paradigm to develop a robust and adaptive watermarking approach. Specifically, we develop a learnable watermark embedder that operates in the latent space, capturing high-level image semantics, while offering precise control over message encoding and extraction. The MAARL paradigm empowers the learnable watermarking agent to pursue an optimal balance between robustness and fragility by interacting with a dynamic curriculum of benign and malicious image manipulations simulated by an adversarial attacker agent. Comprehensive evaluations on the CelebA and CelebA-HQ benchmarks reveal that our method consistently outperforms state-of-the-art approaches, achieving improvements of over 4.5% on CelebA and more than 5.3% on CelebA-HQ under challenging manipulation scenarios.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Too Good to be Bad: On the Failure of LLMs to Role-Play Villains</title>
<link>https://arxiv.org/abs/2511.04962</link>
<guid>https://arxiv.org/abs/2511.04962</guid>
<content:encoded><![CDATA[
arXiv:2511.04962v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly tasked with creative generation, including the simulation of fictional characters. However, their ability to portray non-prosocial, antagonistic personas remains largely unexamined. We hypothesize that the safety alignment of modern LLMs creates a fundamental conflict with the task of authentically role-playing morally ambiguous or villainous characters. To investigate this, we introduce the Moral RolePlay benchmark, a new dataset featuring a four-level moral alignment scale and a balanced test set for rigorous evaluation. We task state-of-the-art LLMs with role-playing characters from moral paragons to pure villains. Our large-scale evaluation reveals a consistent, monotonic decline in role-playing fidelity as character morality decreases. We find that models struggle most with traits directly antithetical to safety principles, such as ``Deceitful'' and ``Manipulative'', often substituting nuanced malevolence with superficial aggression. Furthermore, we demonstrate that general chatbot proficiency is a poor predictor of villain role-playing ability, with highly safety-aligned models performing particularly poorly. Our work provides the first systematic evidence of this critical limitation, highlighting a key tension between model safety and creative fidelity. Our benchmark and findings pave the way for developing more nuanced, context-aware alignment methods.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pattern-Aware Diffusion Synthesis of fMRI/dMRI with Tissue and Microstructural Refinement</title>
<link>https://arxiv.org/abs/2511.04963</link>
<guid>https://arxiv.org/abs/2511.04963</guid>
<content:encoded><![CDATA[
arXiv:2511.04963v1 Announce Type: cross 
Abstract: Magnetic resonance imaging (MRI), especially functional MRI (fMRI) and diffusion MRI (dMRI), is essential for studying neurodegenerative diseases. However, missing modalities pose a major barrier to their clinical use. Although GAN- and diffusion model-based approaches have shown some promise in modality completion, they remain limited in fMRI-dMRI synthesis due to (1) significant BOLD vs. diffusion-weighted signal differences between fMRI and dMRI in time/gradient axis, and (2) inadequate integration of disease-related neuroanatomical patterns during generation. To address these challenges, we propose PDS, introducing two key innovations: (1) a pattern-aware dual-modal 3D diffusion framework for cross-modality learning, and (2) a tissue refinement network integrated with a efficient microstructure refinement to maintain structural fidelity and fine details. Evaluated on OASIS-3, ADNI, and in-house datasets, our method achieves state-of-the-art results, with PSNR/SSIM scores of 29.83 dB/90.84\% for fMRI synthesis (+1.54 dB/+4.12\% over baselines) and 30.00 dB/77.55\% for dMRI synthesis (+1.02 dB/+2.2\%). In clinical validation, the synthesized data show strong diagnostic performance, achieving 67.92\%/66.02\%/64.15\% accuracy (NC vs. MCI vs. AD) in hybrid real-synthetic experiments. Code is available in \href{https://github.com/SXR3015/PDS}{PDS GitHub Repository}
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Fourier shapes to probe the geometric world of deep neural networks</title>
<link>https://arxiv.org/abs/2511.04970</link>
<guid>https://arxiv.org/abs/2511.04970</guid>
<content:encoded><![CDATA[
arXiv:2511.04970v1 Announce Type: cross 
Abstract: While both shape and texture are fundamental to visual recognition, research on deep neural networks (DNNs) has predominantly focused on the latter, leaving their geometric understanding poorly probed. Here, we show: first, that optimized shapes can act as potent semantic carriers, generating high-confidence classifications from inputs defined purely by their geometry; second, that they are high-fidelity interpretability tools that precisely isolate a model's salient regions; and third, that they constitute a new, generalizable adversarial paradigm capable of deceiving downstream visual tasks. This is achieved through an end-to-end differentiable framework that unifies a powerful Fourier series to parameterize arbitrary shapes, a winding number-based mapping to translate them into the pixel grid required by DNNs, and signal energy constraints that enhance optimization efficiency while ensuring physically plausible shapes. Our work provides a versatile framework for probing the geometric world of DNNs and opens new frontiers for challenging and understanding machine perception.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Public Speaking Skills in Engineering Students Through AI</title>
<link>https://arxiv.org/abs/2511.04995</link>
<guid>https://arxiv.org/abs/2511.04995</guid>
<content:encoded><![CDATA[
arXiv:2511.04995v1 Announce Type: cross 
Abstract: This research-to-practice full paper was inspired by the persistent challenge in effective communication among engineering students. Public speaking is a necessary skill for future engineers as they have to communicate technical knowledge with diverse stakeholders. While universities offer courses or workshops, they are unable to offer sustained and personalized training to students. Providing comprehensive feedback on both verbal and non-verbal aspects of public speaking is time-intensive, making consistent and individualized assessment impractical. This study integrates research on verbal and non-verbal cues in public speaking to develop an AI-driven assessment model for engineering students. Our approach combines speech analysis, computer vision, and sentiment detection into a multi-modal AI system that provides assessment and feedback. The model evaluates (1) verbal communication (pitch, loudness, pacing, intonation), (2) non-verbal communication (facial expressions, gestures, posture), and (3) expressive coherence, a novel integration ensuring alignment between speech and body language. Unlike previous systems that assess these aspects separately, our model fuses multiple modalities to deliver personalized, scalable feedback. Preliminary testing demonstrated that our AI-generated feedback was moderately aligned with expert evaluations. Among the state-of-the-art AI models evaluated, all of which were Large Language Models (LLMs), including Gemini and OpenAI models, Gemini Pro emerged as the best-performing, showing the strongest agreement with human annotators. By eliminating reliance on human evaluators, this AI-driven public speaking trainer enables repeated practice, helping students naturally align their speech with body language and emotion, crucial for impactful and professional communication.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BiPETE: A Bi-Positional Embedding Transformer Encoder for Risk Assessment of Alcohol and Substance Use Disorder with Electronic Health Records</title>
<link>https://arxiv.org/abs/2511.04998</link>
<guid>https://arxiv.org/abs/2511.04998</guid>
<content:encoded><![CDATA[
arXiv:2511.04998v1 Announce Type: cross 
Abstract: Transformer-based deep learning models have shown promise for disease risk prediction using electronic health records(EHRs), but modeling temporal dependencies remains a key challenge due to irregular visit intervals and lack of uniform structure. We propose a Bi-Positional Embedding Transformer Encoder or BiPETE for single-disease prediction, which integrates rotary positional embeddings to encode relative visit timing and sinusoidal embeddings to preserve visit order. Without relying on large-scale pretraining, BiPETE is trained on EHR data from two mental health cohorts-depressive disorder and post-traumatic stress disorder (PTSD)-to predict the risk of alcohol and substance use disorders (ASUD). BiPETE outperforms baseline models, improving the area under the precision-recall curve (AUPRC) by 34% and 50% in the depression and PTSD cohorts, respectively. An ablation study further confirms the effectiveness of the dual positional encoding strategy. We apply the Integrated Gradients method to interpret model predictions, identifying key clinical features associated with ASUD risk and protection, such as abnormal inflammatory, hematologic, and metabolic markers, as well as specific medications and comorbidities. Overall, these key clinical features identified by the attribution methods contribute to a deeper understanding of the risk assessment process and offer valuable clues for mitigating potential risks. In summary, our study presents a practical and interpretable framework for disease risk prediction using EHR data, which can achieve strong performance.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Query Generation Pipeline with Enhanced Answerability Assessment for Financial Information Retrieval</title>
<link>https://arxiv.org/abs/2511.05000</link>
<guid>https://arxiv.org/abs/2511.05000</guid>
<content:encoded><![CDATA[
arXiv:2511.05000v1 Announce Type: cross 
Abstract: As financial applications of large language models (LLMs) gain attention, accurate Information Retrieval (IR) remains crucial for reliable AI services. However, existing benchmarks fail to capture the complex and domain-specific information needs of real-world banking scenarios. Building domain-specific IR benchmarks is costly and constrained by legal restrictions on using real customer data. To address these challenges, we propose a systematic methodology for constructing domain-specific IR benchmarks through LLM-based query generation. As a concrete implementation of this methodology, our pipeline combines single and multi-document query generation with an enhanced and reasoning-augmented answerability assessment method, achieving stronger alignment with human judgments than prior approaches. Using this methodology, we construct KoBankIR, comprising 815 queries derived from 204 official banking documents. Our experiments show that existing retrieval models struggle with the complex multi-document queries in KoBankIR, demonstrating the value of our systematic approach for domain-specific benchmark construction and underscoring the need for improved retrieval techniques in financial domains.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-agent Coordination via Flow Matching</title>
<link>https://arxiv.org/abs/2511.05005</link>
<guid>https://arxiv.org/abs/2511.05005</guid>
<content:encoded><![CDATA[
arXiv:2511.05005v1 Announce Type: cross 
Abstract: This work presents MAC-Flow, a simple yet expressive framework for multi-agent coordination. We argue that requirements of effective coordination are twofold: (i) a rich representation of the diverse joint behaviors present in offline data and (ii) the ability to act efficiently in real time. However, prior approaches often sacrifice one for the other, i.e., denoising diffusion-based solutions capture complex coordination but are computationally slow, while Gaussian policy-based solutions are fast but brittle in handling multi-agent interaction. MAC-Flow addresses this trade-off by first learning a flow-based representation of joint behaviors, and then distilling it into decentralized one-step policies that preserve coordination while enabling fast execution. Across four different benchmarks, including $12$ environments and $34$ datasets, MAC-Flow alleviates the trade-off between performance and computational cost, specifically achieving about $\boldsymbol{\times14.5}$ faster inference compared to diffusion-based MARL methods, while maintaining good performance. At the same time, its inference speed is similar to that of prior Gaussian policy-based offline multi-agent reinforcement learning (MARL) methods.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pluralistic Behavior Suite: Stress-Testing Multi-Turn Adherence to Custom Behavioral Policies</title>
<link>https://arxiv.org/abs/2511.05018</link>
<guid>https://arxiv.org/abs/2511.05018</guid>
<content:encoded><![CDATA[
arXiv:2511.05018v1 Announce Type: cross 
Abstract: Large language models (LLMs) are typically aligned to a universal set of safety and usage principles intended for broad public acceptability. Yet, real-world applications of LLMs often take place within organizational ecosystems shaped by distinctive corporate policies, regulatory requirements, use cases, brand guidelines, and ethical commitments. This reality highlights the need for rigorous and comprehensive evaluation of LLMs with pluralistic alignment goals, an alignment paradigm that emphasizes adaptability to diverse user values and needs. In this work, we present PLURALISTIC BEHAVIOR SUITE (PBSUITE), a dynamic evaluation suite designed to systematically assess LLMs' capacity to adhere to pluralistic alignment specifications in multi-turn, interactive conversations. PBSUITE consists of (1) a diverse dataset of 300 realistic LLM behavioral policies, grounded in 30 industries; and (2) a dynamic evaluation framework for stress-testing model compliance with custom behavioral specifications under adversarial conditions. Using PBSUITE, We find that leading open- and closed-source LLMs maintain robust adherence to behavioral policies in single-turn settings (less than 4% failure rates), but their compliance weakens substantially in multi-turn adversarial interactions (up to 84% failure rates). These findings highlight that existing model alignment and safety moderation methods fall short in coherently enforcing pluralistic behavioral policies in real-world LLM interactions. Our work contributes both the dataset and analytical framework to support future research toward robust and context-aware pluralistic alignment techniques.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>8bit-GPT: Exploring Human-AI Interaction on Obsolete Macintosh Operating Systems</title>
<link>https://arxiv.org/abs/2511.05025</link>
<guid>https://arxiv.org/abs/2511.05025</guid>
<content:encoded><![CDATA[
arXiv:2511.05025v1 Announce Type: cross 
Abstract: The proliferation of assistive chatbots offering efficient, personalized communication has driven widespread over-reliance on them for decision-making, information-seeking and everyday tasks. This dependence was found to have adverse consequences on information retention as well as lead to superficial emotional attachment. As such, this work introduces 8bit-GPT; a language model simulated on a legacy Macintosh Operating System, to evoke reflection on the nature of Human-AI interaction and the consequences of anthropomorphic rhetoric. Drawing on reflective design principles such as slow-technology and counterfunctionality, this work aims to foreground the presence of chatbots as a tool by defamiliarizing the interface and prioritizing inefficient interaction, creating a friction between the familiar and not.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OvA-LP: A Simple and Efficient Framework for Federated Learning on Non-IID Data</title>
<link>https://arxiv.org/abs/2511.05028</link>
<guid>https://arxiv.org/abs/2511.05028</guid>
<content:encoded><![CDATA[
arXiv:2511.05028v1 Announce Type: cross 
Abstract: Federated fine-tuning (FFT) adapts foundation models to decentralized data but remains fragile under heterogeneous client distributions due to local drift, i.e., client-level update divergences that induce systematic bias and amplified variance in the global model. Existing aggregation and personalization methods largely correct drift post hoc, which proves brittle under extreme non-IID conditions. We introduce OvA-LP, a minimalist framework that is, to our knowledge, the first explicitly designed to suppress drift at its source within the PEFT-based FFT paradigm. OvA-LP combines linear probing on a frozen encoder with a one-vs-all head and a simple two-stage procedure, preserving pretrained feature geometry and decoupling logits to prevent the mechanisms that amplify drift. On CIFAR-100 with 100 clients, averaged over shard-1, shard-2, and Bernoulli-Dirichlet partitions, OvA-LP retains 95.9% of its IID accuracy, whereas state-of-the-art FFT baselines retain only 10.1% (PFPT) and 34.5% (FFT-MoE) under the same conditions. OvA-LP further maintains resilience under both symmetric and asymmetric label noise. In addition, precomputing encoder features makes per-round cost nearly independent of encoder size. Together, these results demonstrate that OvA-LP provides a principled and efficient basis for robust FFT under heterogeneity.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Residual Encoding with Slide-Level Contrastive Learning for End-to-End Whole Slide Image Representation</title>
<link>https://arxiv.org/abs/2511.05034</link>
<guid>https://arxiv.org/abs/2511.05034</guid>
<content:encoded><![CDATA[
arXiv:2511.05034v1 Announce Type: cross 
Abstract: Whole Slide Image (WSI) representation is critical for cancer subtyping, cancer recognition and mutation prediction.Training an end-to-end WSI representation model poses significant challenges, as a standard gigapixel slide can contain tens of thousands of image tiles, making it difficult to compute gradients of all tiles in a single mini-batch due to current GPU limitations. To address this challenge, we propose a method of dynamic residual encoding with slide-level contrastive learning (DRE-SLCL) for end-to-end WSI representation. Our approach utilizes a memory bank to store the features of tiles across all WSIs in the dataset. During training, a mini-batch usually contains multiple WSIs. For each WSI in the batch, a subset of tiles is randomly sampled and their features are computed using a tile encoder. Then, additional tile features from the same WSI are selected from the memory bank. The representation of each individual WSI is generated using a residual encoding technique that incorporates both the sampled features and those retrieved from the memory bank. Finally, the slide-level contrastive loss is computed based on the representations and histopathology reports ofthe WSIs within the mini-batch. Experiments conducted over cancer subtyping, cancer recognition, and mutation prediction tasks proved the effectiveness of the proposed DRE-SLCL method.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PECL: A Heterogeneous Parallel Multi-Domain Network for Radar-Based Human Activity Recognition</title>
<link>https://arxiv.org/abs/2511.05039</link>
<guid>https://arxiv.org/abs/2511.05039</guid>
<content:encoded><![CDATA[
arXiv:2511.05039v1 Announce Type: cross 
Abstract: Radar systems are increasingly favored for medical applications because they provide non-intrusive monitoring with high privacy and robustness to lighting conditions. However, existing research typically relies on single-domain radar signals and overlooks the temporal dependencies inherent in human activity, which complicates the classification of similar actions. To address this issue, we designed the Parallel-EfficientNet-CBAM-LSTM (PECL) network to process data in three complementary domains: Range-Time, Doppler-Time, and Range-Doppler. PECL combines a channel-spatial attention module and temporal units to capture more features and dynamic dependencies during action sequences, improving both accuracy and robustness. The experimental results show that PECL achieves an accuracy of 96.16% on the same dataset, outperforming existing methods by at least 4.78%. PECL also performs best in distinguishing between easily confused actions. Despite its strong performance, PECL maintains moderate model complexity, with 23.42M parameters and 1324.82M FLOPs. Its parameter-efficient design further reduces computational cost.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UA-Code-Bench: A Competitive Programming Benchmark for Evaluating LLM Code Generation in Ukrainian</title>
<link>https://arxiv.org/abs/2511.05040</link>
<guid>https://arxiv.org/abs/2511.05040</guid>
<content:encoded><![CDATA[
arXiv:2511.05040v1 Announce Type: cross 
Abstract: Evaluating the real capabilities of large language models in low-resource languages still represents a challenge, as many existing benchmarks focus on widespread tasks translated from English or evaluate only simple language understanding. This paper introduces UA-Code-Bench, a new open-source benchmark established for a thorough evaluation of language models' code generation and competitive programming problem-solving abilities in Ukrainian. The benchmark comprises 500 problems from the Eolymp platform, evenly distributed across five complexity levels from very easy to very hard. A diverse set of 13 leading proprietary and open-source models, generating Python solutions based on a one-shot prompt, was evaluated via the dedicated Eolymp environment against hidden tests, ensuring code correctness. The obtained results reveal that even top-performing models, such as OpenAI o3 and GPT-5, solve only half of the problems, highlighting the challenge of code generation in low-resource natural language. Furthermore, this research presents a comprehensive analysis of performance across various difficulty levels, as well as an assessment of solution uniqueness and computational efficiency, measured by both elapsed time and memory consumption of the generated solutions. In conclusion, this work demonstrates the value of competitive programming benchmarks in evaluating large language models, especially in underrepresented languages. It also paves the way for future research on multilingual code generation and reasoning-enhanced models. The benchmark, data parsing, preparation, code generation, and evaluation scripts are available at https://huggingface.co/datasets/NLPForUA/ua-code-bench.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerating HDC-CNN Hybrid Models Using Custom Instructions on RISC-V GPUs</title>
<link>https://arxiv.org/abs/2511.05053</link>
<guid>https://arxiv.org/abs/2511.05053</guid>
<content:encoded><![CDATA[
arXiv:2511.05053v1 Announce Type: cross 
Abstract: Machine learning based on neural networks has advanced rapidly, but the high energy consumption required for training and inference remains a major challenge. Hyperdimensional Computing (HDC) offers a lightweight, brain-inspired alternative that enables high parallelism but often suffers from lower accuracy on complex visual tasks. To overcome this, hybrid accelerators combining HDC and Convolutional Neural Networks (CNNs) have been proposed, though their adoption is limited by poor generalizability and programmability. The rise of open-source RISC-V architectures has created new opportunities for domain-specific GPU design. Unlike traditional proprietary GPUs, emerging RISC-V-based GPUs provide flexible, programmable platforms suitable for custom computation models such as HDC. In this study, we design and implement custom GPU instructions optimized for HDC operations, enabling efficient processing for hybrid HDC-CNN workloads. Experimental results using four types of custom HDC instructions show a performance improvement of up to 56.2 times in microbenchmark tests, demonstrating the potential of RISC-V GPUs for energy-efficient, high-performance computing.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>No Pose Estimation? No Problem: Pose-Agnostic and Instance-Aware Test-Time Adaptation for Monocular Depth Estimation</title>
<link>https://arxiv.org/abs/2511.05055</link>
<guid>https://arxiv.org/abs/2511.05055</guid>
<content:encoded><![CDATA[
arXiv:2511.05055v1 Announce Type: cross 
Abstract: Monocular depth estimation (MDE), inferring pixel-level depths in single RGB images from a monocular camera, plays a crucial and pivotal role in a variety of AI applications demanding a three-dimensional (3D) topographical scene. In the real-world scenarios, MDE models often need to be deployed in environments with different conditions from those for training. Test-time (domain) adaptation (TTA) is one of the compelling and practical approaches to address the issue. Although there have been notable advancements in TTA for MDE, particularly in a self-supervised manner, existing methods are still ineffective and problematic when applied to diverse and dynamic environments. To break through this challenge, we propose a novel and high-performing TTA framework for MDE, named PITTA. Our approach incorporates two key innovative strategies: (i) pose-agnostic TTA paradigm for MDE and (ii) instance-aware image masking. Specifically, PITTA enables highly effective TTA on a pretrained MDE network in a pose-agnostic manner without resorting to any camera pose information. Besides, our instance-aware masking strategy extracts instance-wise masks for dynamic objects (e.g., vehicles, pedestrians, etc.) from a segmentation mask produced by a pretrained panoptic segmentation network, by removing static objects including background components. To further boost performance, we also present a simple yet effective edge extraction methodology for the input image (i.e., a single monocular image) and depth map. Extensive experimental evaluations on DrivingStereo and Waymo datasets with varying environmental conditions demonstrate that our proposed framework, PITTA, surpasses the existing state-of-the-art techniques with remarkable performance improvements in MDE during TTA.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep learning models are vulnerable, but adversarial examples are even more vulnerable</title>
<link>https://arxiv.org/abs/2511.05073</link>
<guid>https://arxiv.org/abs/2511.05073</guid>
<content:encoded><![CDATA[
arXiv:2511.05073v1 Announce Type: cross 
Abstract: Understanding intrinsic differences between adversarial examples and clean samples is key to enhancing DNN robustness and detection against adversarial attacks. This study first empirically finds that image-based adversarial examples are notably sensitive to occlusion. Controlled experiments on CIFAR-10 used nine canonical attacks (e.g., FGSM, PGD) to generate adversarial examples, paired with original samples for evaluation. We introduce Sliding Mask Confidence Entropy (SMCE) to quantify model confidence fluctuation under occlusion. Using 1800+ test images, SMCE calculations supported by Mask Entropy Field Maps and statistical distributions show adversarial examples have significantly higher confidence volatility under occlusion than originals. Based on this, we propose Sliding Window Mask-based Adversarial Example Detection (SWM-AED), which avoids catastrophic overfitting of conventional adversarial training. Evaluations across classifiers and attacks on CIFAR-10 demonstrate robust performance, with accuracy over 62% in most cases and up to 96.5%.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DL101 Neural Network Outputs and Loss Functions</title>
<link>https://arxiv.org/abs/2511.05131</link>
<guid>https://arxiv.org/abs/2511.05131</guid>
<content:encoded><![CDATA[
arXiv:2511.05131v1 Announce Type: cross 
Abstract: The loss function used to train a neural network is strongly connected to its output layer from a statistical point of view. This technical report analyzes common activation functions for a neural network output layer, like linear, sigmoid, ReLU, and softmax, detailing their mathematical properties and their appropriate use cases. A strong statistical justification exists for the selection of the suitable loss function for training a deep learning model. This report connects common loss functions such as Mean Squared Error (MSE), Mean Absolute Error (MAE), and various Cross-Entropy losses to the statistical principle of Maximum Likelihood Estimation (MLE). Choosing a specific loss function is equivalent to assuming a specific probability distribution for the model output, highlighting the link between these functions and the Generalized Linear Models (GLMs) that underlie network output layers. Additional scenarios of practical interest are also considered, such as alternative output encodings, constrained outputs, and distributions with heavy tails.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Linear Probing to Joint-Weighted Token Hierarchy: A Foundation Model Bridging Global and Cellular Representations in Biomarker Detection</title>
<link>https://arxiv.org/abs/2511.05150</link>
<guid>https://arxiv.org/abs/2511.05150</guid>
<content:encoded><![CDATA[
arXiv:2511.05150v1 Announce Type: cross 
Abstract: AI-based biomarkers can infer molecular features directly from hematoxylin & eosin (H&amp;E) slides, yet most pathology foundation models (PFMs) rely on global patch-level embeddings and overlook cell-level morphology. We present a PFM model, JWTH (Joint-Weighted Token Hierarchy), which integrates large-scale self-supervised pretraining with cell-centric post-tuning and attention pooling to fuse local and global tokens. Across four tasks involving four biomarkers and eight cohorts, JWTH achieves up to 8.3% higher balanced accuracy and 1.2% average improvement over prior PFMs, advancing interpretable and robust AI-based biomarker detection in digital pathology.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SmartSecChain-SDN: A Blockchain-Integrated Intelligent Framework for Secure and Efficient Software-Defined Networks</title>
<link>https://arxiv.org/abs/2511.05156</link>
<guid>https://arxiv.org/abs/2511.05156</guid>
<content:encoded><![CDATA[
arXiv:2511.05156v1 Announce Type: cross 
Abstract: With more and more existing networks being transformed to Software-Defined Networking (SDN), they need to be more secure and demand smarter ways of traffic control. This work, SmartSecChain-SDN, is a platform that combines machine learning based intrusion detection, blockchain-based storage of logs, and application-awareness-based priority in SDN networks. To detect network intrusions in a real-time, precision and low-false positives setup, the framework utilizes the application of advanced machine learning algorithms, namely Random Forest, XGBoost, CatBoost, and CNN-BiLSTM. SmartSecChain-SDN is based on the Hyperledger Fabric, which is a permissioned blockchain technology, to provide secure, scalable, and privacy-preserving storage and, thus, guarantee that the Intrusion Detection System (IDS) records cannot be altered and can be analyzed comprehensively. The system also has Quality of Service (QoS) rules and traffic shaping based on applications, which enables prioritization of critical services, such as VoIP, video conferencing, and business applications, as well as de-prioritization of non-essential traffic, such as downloads and updates. Mininet can simulate real-time SDN scenarios because it is used to prototype whole architectures. It is also compatible with controllers OpenDaylight and Ryu. It has tested the framework using the InSDN dataset and proved that it can identify different kinds of cyberattacks and handle bandwidth allocation efficiently under circumstances of resource constraints. SmartSecChain-SDN comprehensively addresses SDN system protection, securing and enhancing. The proposed study offers an innovative, extensible way to improve cybersecurity, regulatory compliance, and the administration of next-generation programmable networks.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generating Software Architecture Description from Source Code using Reverse Engineering and Large Language Model</title>
<link>https://arxiv.org/abs/2511.05165</link>
<guid>https://arxiv.org/abs/2511.05165</guid>
<content:encoded><![CDATA[
arXiv:2511.05165v1 Announce Type: cross 
Abstract: Software Architecture Descriptions (SADs) are essential for managing the inherent complexity of modern software systems. They enable high-level architectural reasoning, guide design decisions, and facilitate effective communication among diverse stakeholders. However, in practice, SADs are often missing, outdated, or poorly aligned with the system's actual implementation. Consequently, developers are compelled to derive architectural insights directly from source code-a time-intensive process that increases cognitive load, slows new developer onboarding, and contributes to the gradual degradation of clarity over the system's lifetime. To address these issues, we propose a semi-automated generation of SADs from source code by integrating reverse engineering (RE) techniques with a Large Language Model (LLM). Our approach recovers both static and behavioral architectural views by extracting a comprehensive component diagram, filtering architecturally significant elements (core components) via prompt engineering, and generating state machine diagrams to model component behavior based on underlying code logic with few-shots prompting. This resulting views representation offer a scalable and maintainable alternative to traditional manual architectural documentation. This methodology, demonstrated using C++ examples, highlights the potent capability of LLMs to: 1) abstract the component diagram, thereby reducing the reliance on human expert involvement, and 2) accurately represent complex software behaviors, especially when enriched with domain-specific knowledge through few-shot prompting. These findings suggest a viable path toward significantly reducing manual effort while enhancing system understanding and long-term maintainability.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Model Merging Improves Zero-Shot Generalization in Bioacoustic Foundation Models</title>
<link>https://arxiv.org/abs/2511.05171</link>
<guid>https://arxiv.org/abs/2511.05171</guid>
<content:encoded><![CDATA[
arXiv:2511.05171v1 Announce Type: cross 
Abstract: Foundation models capable of generalizing across species and tasks represent a promising new frontier in bioacoustics, with NatureLM being one of the most prominent examples. While its domain-specific fine-tuning yields strong performance on bioacoustic benchmarks, we observe that it also introduces trade-offs in instruction-following flexibility. For instance, NatureLM achieves high accuracy when prompted for either the common or scientific name individually, but its accuracy drops significantly when both are requested in a single prompt. We address this by applying a simple model merging strategy that interpolates NatureLM with its base language model, recovering instruction-following capabilities with minimal loss of domain expertise. Finally, we show that the merged model exhibits markedly stronger zero-shot generalization, achieving over a 200% relative improvement and setting a new state-of-the-art in closed-set zero-shot classification of unseen species.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>No One-Model-Fits-All: Uncovering Spatio-Temporal Forecasting Trade-offs with Graph Neural Networks and Foundation Models</title>
<link>https://arxiv.org/abs/2511.05179</link>
<guid>https://arxiv.org/abs/2511.05179</guid>
<content:encoded><![CDATA[
arXiv:2511.05179v1 Announce Type: cross 
Abstract: Modern IoT deployments for environmental sensing produce high volume spatiotemporal data to support downstream tasks such as forecasting, typically powered by machine learning models. While existing filtering and strategic deployment techniques optimize collected data volume at the edge, they overlook how variations in sampling frequencies and spatial coverage affect downstream model performance. In many forecasting models, incorporating data from additional sensors denoise predictions by providing broader spatial contexts. This interplay between sampling frequency, spatial coverage and different forecasting model architectures remain underexplored. This work presents a systematic study of forecasting models - classical models (VAR), neural networks (GRU, Transformer), spatio-temporal graph neural networks (STGNNs), and time series foundation models (TSFMs: Chronos Moirai, TimesFM) under varying spatial sensor nodes density and sampling intervals using real-world temperature data in a wireless sensor network. Our results show that STGNNs are effective when sensor deployments are sparse and sampling rate is moderate, leveraging spatial correlations via encoded graph structure to compensate for limited coverage. In contrast, TSFMs perform competitively at high frequencies but degrade when spatial coverage from neighboring sensors is reduced. Crucially, the multivariate TSFM Moirai outperforms all models by natively learning cross-sensor dependencies. These findings offer actionable insights for building efficient forecasting pipelines in spatio-temporal systems. All code for model configurations, training, dataset, and logs are open-sourced for reproducibility: https://github.com/UIUC-MONET-Projects/Benchmarking-Spatiotemporal-Forecast-Models
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>4D3R: Motion-Aware Neural Reconstruction and Rendering of Dynamic Scenes from Monocular Videos</title>
<link>https://arxiv.org/abs/2511.05229</link>
<guid>https://arxiv.org/abs/2511.05229</guid>
<content:encoded><![CDATA[
arXiv:2511.05229v1 Announce Type: cross 
Abstract: Novel view synthesis from monocular videos of dynamic scenes with unknown camera poses remains a fundamental challenge in computer vision and graphics. While recent advances in 3D representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have shown promising results for static scenes, they struggle with dynamic content and typically rely on pre-computed camera poses. We present 4D3R, a pose-free dynamic neural rendering framework that decouples static and dynamic components through a two-stage approach. Our method first leverages 3D foundational models for initial pose and geometry estimation, followed by motion-aware refinement. 4D3R introduces two key technical innovations: (1) a motion-aware bundle adjustment (MA-BA) module that combines transformer-based learned priors with SAM2 for robust dynamic object segmentation, enabling more accurate camera pose refinement; and (2) an efficient Motion-Aware Gaussian Splatting (MA-GS) representation that uses control points with a deformation field MLP and linear blend skinning to model dynamic motion, significantly reducing computational cost while maintaining high-quality reconstruction. Extensive experiments on real-world dynamic datasets demonstrate that our approach achieves up to 1.8dB PSNR improvement over state-of-the-art methods, particularly in challenging scenarios with large dynamic objects, while reducing computational requirements by 5x compared to previous dynamic scene representations.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accurate online action and gesture recognition system using detectors and Deep SPD Siamese Networks</title>
<link>https://arxiv.org/abs/2511.05250</link>
<guid>https://arxiv.org/abs/2511.05250</guid>
<content:encoded><![CDATA[
arXiv:2511.05250v1 Announce Type: cross 
Abstract: Online continuous motion recognition is a hot topic of research since it is more practical in real life application cases. Recently, Skeleton-based approaches have become increasingly popular, demonstrating the power of using such 3D temporal data. However, most of these works have focused on segment-based recognition and are not suitable for the online scenarios. In this paper, we propose an online recognition system for skeleton sequence streaming composed from two main components: a detector and a classifier, which use a Semi-Positive Definite (SPD) matrix representation and a Siamese network. The powerful statistical representations for the skeletal data given by the SPD matrices and the learning of their semantic similarity by the Siamese network enable the detector to predict time intervals of the motions throughout an unsegmented sequence. In addition, they ensure the classifier capability to recognize the motion in each predicted interval. The proposed detector is flexible and able to identify the kinetic state continuously. We conduct extensive experiments on both hand gesture and body action recognition benchmarks to prove the accuracy of our online recognition system which in most cases outperforms state-of-the-art performances.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Gate-Based Quantum Genetic Algorithm for Real-Valued Global Optimization</title>
<link>https://arxiv.org/abs/2511.05254</link>
<guid>https://arxiv.org/abs/2511.05254</guid>
<content:encoded><![CDATA[
arXiv:2511.05254v1 Announce Type: cross 
Abstract: We propose a gate-based Quantum Genetic Algorithm (QGA) for real-valued global optimization. In this model, individuals are represented by quantum circuits whose measurement outcomes are decoded into real-valued vectors through binary discretization. Evolutionary operators act directly on circuit structures, allowing mutation and crossover to explore the space of gate-based encodings. Both fixed-depth and variable-depth variants are introduced, enabling either uniform circuit complexity or adaptive structural evolution. Fitness is evaluated through quantum sampling, using the mean decoded output of measurement outcomes as the argument of the objective function. To isolate the impact of quantum resources, we compare gate sets with and without the Hadamard gate, showing that superposition consistently improves convergence and robustness across benchmark functions such as the Rastrigin function. Furthermore, we demonstrate that introducing pairwise inter-individual entanglement in the population accelerates early convergence, revealing that quantum correlations among individuals provide an additional optimization advantage. Together, these results show that both superposition and entanglement enhance the search dynamics of evolutionary quantum algorithms, establishing gate-based QGAs as a promising framework for quantum-enhanced global optimization.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OregairuChar: A Benchmark Dataset for Character Appearance Frequency Analysis in My Teen Romantic Comedy SNAFU</title>
<link>https://arxiv.org/abs/2511.05263</link>
<guid>https://arxiv.org/abs/2511.05263</guid>
<content:encoded><![CDATA[
arXiv:2511.05263v1 Announce Type: cross 
Abstract: The analysis of character appearance frequency is essential for understanding narrative structure, character prominence, and story progression in anime. In this work, we introduce OregairuChar, a benchmark dataset designed for appearance frequency analysis in the anime series My Teen Romantic Comedy SNAFU. The dataset comprises 1600 manually selected frames from the third season, annotated with 2860 bounding boxes across 11 main characters. OregairuChar captures diverse visual challenges, including occlusion, pose variation, and inter-character similarity, providing a realistic basis for appearance-based studies. To enable quantitative research, we benchmark several object detection models on the dataset and leverage their predictions for fine-grained, episode-level analysis of character presence over time. This approach reveals patterns of character prominence and their evolution within the narrative. By emphasizing appearance frequency, OregairuChar serves as a valuable resource for exploring computational narrative dynamics and character-centric storytelling in stylized media.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An End-to-End Deep Reinforcement Learning Approach for Solving the Traveling Salesman Problem with Drones</title>
<link>https://arxiv.org/abs/2511.05265</link>
<guid>https://arxiv.org/abs/2511.05265</guid>
<content:encoded><![CDATA[
arXiv:2511.05265v1 Announce Type: cross 
Abstract: The emergence of truck-drone collaborative systems in last-mile logistics has positioned the Traveling Salesman Problem with Drones (TSP-D) as a pivotal extension of classical routing optimization, where synchronized vehicle coordination promises substantial operational efficiency and reduced environmental impact, yet introduces NP-hard combinatorial complexity beyond the reach of conventional optimization paradigms. Deep reinforcement learning offers a theoretically grounded framework to address TSP-D's inherent challenges through self-supervised policy learning and adaptive decision-making. This study proposes a hierarchical Actor-Critic deep reinforcement learning framework for solving the TSP-D problem. The architecture consists of two primary components: a Transformer-inspired encoder and an efficient Minimal Gated Unit decoder. The encoder incorporates a novel, optimized k-nearest neighbors sparse attention mechanism specifically for focusing on relevant spatial relationships, further enhanced by the integration of global node features. The Minimal Gated Unit decoder processes these encoded representations to efficiently generate solution sequences. The entire framework operates within an asynchronous advantage actor-critic paradigm. Experimental results show that, on benchmark TSP-D instances of various scales (N=10 to 100), the proposed model can obtain competitive or even superior solutions in shorter average computation times compared to high-performance heuristic algorithms and existing reinforcement learning methods. Moreover, compared to advanced reinforcement learning algorithm benchmarks, the proposed framework significantly reduces the total training time required while achieving superior final performance, highlighting its notable advantage in training efficiency.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integrating Score-Based Diffusion Models with Machine Learning-Enhanced Localization for Advanced Data Assimilation in Geological Carbon Storage</title>
<link>https://arxiv.org/abs/2511.05266</link>
<guid>https://arxiv.org/abs/2511.05266</guid>
<content:encoded><![CDATA[
arXiv:2511.05266v1 Announce Type: cross 
Abstract: Accurate characterization of subsurface heterogeneity is important for the safe and effective implementation of geological carbon storage (GCS) projects. This paper explores how machine learning methods can enhance data assimilation for GCS with a framework that integrates score-based diffusion models with machine learning-enhanced localization in channelized reservoirs during CO$_2$ injection. We employ a machine learning-enhanced localization framework that uses large ensembles ($N_s = 5000$) with permeabilities generated by the diffusion model and states computed by simple ML algorithms to improve covariance estimation for the Ensemble Smoother with Multiple Data Assimilation (ESMDA). We apply ML algorithms to a prior ensemble of channelized permeability fields, generated with the geostatistical model FLUVSIM. Our approach is applied on a CO$_2$ injection scenario simulated using the Delft Advanced Research Terra Simulator (DARTS). Our ML-based localization maintains significantly more ensemble variance than when localization is not applied, while achieving comparable data-matching quality. This framework has practical implications for GCS projects, helping improve the reliability of uncertainty quantification for risk assessment.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TAMAS: Benchmarking Adversarial Risks in Multi-Agent LLM Systems</title>
<link>https://arxiv.org/abs/2511.05269</link>
<guid>https://arxiv.org/abs/2511.05269</guid>
<content:encoded><![CDATA[
arXiv:2511.05269v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated strong capabilities as autonomous agents through tool use, planning, and decision-making abilities, leading to their widespread adoption across diverse tasks. As task complexity grows, multi-agent LLM systems are increasingly used to solve problems collaboratively. However, safety and security of these systems remains largely under-explored. Existing benchmarks and datasets predominantly focus on single-agent settings, failing to capture the unique vulnerabilities of multi-agent dynamics and co-ordination. To address this gap, we introduce $\textbf{T}$hreats and $\textbf{A}$ttacks in $\textbf{M}$ulti-$\textbf{A}$gent $\textbf{S}$ystems ($\textbf{TAMAS}$), a benchmark designed to evaluate the robustness and safety of multi-agent LLM systems. TAMAS includes five distinct scenarios comprising 300 adversarial instances across six attack types and 211 tools, along with 100 harmless tasks. We assess system performance across ten backbone LLMs and three agent interaction configurations from Autogen and CrewAI frameworks, highlighting critical challenges and failure modes in current multi-agent deployments. Furthermore, we introduce Effective Robustness Score (ERS) to assess the tradeoff between safety and task effectiveness of these frameworks. Our findings show that multi-agent systems are highly vulnerable to adversarial attacks, underscoring the urgent need for stronger defenses. TAMAS provides a foundation for systematically studying and improving the safety of multi-agent LLM systems.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepEyesV2: Toward Agentic Multimodal Model</title>
<link>https://arxiv.org/abs/2511.05271</link>
<guid>https://arxiv.org/abs/2511.05271</guid>
<content:encoded><![CDATA[
arXiv:2511.05271v1 Announce Type: cross 
Abstract: Agentic multimodal models should not only comprehend text and images, but also actively invoke external tools, such as code execution environments and web search, and integrate these operations into reasoning. In this work, we introduce DeepEyesV2 and explore how to build an agentic multimodal model from the perspectives of data construction, training methods, and model evaluation. We observe that direct reinforcement learning alone fails to induce robust tool-use behavior. This phenomenon motivates a two-stage training pipeline: a cold-start stage to establish tool-use patterns, and reinforcement learning stage to further refine tool invocation. We curate a diverse, moderately challenging training dataset, specifically including examples where tool use is beneficial. We further introduce RealX-Bench, a comprehensive benchmark designed to evaluate real-world multimodal reasoning, which inherently requires the integration of multiple capabilities, including perception, search, and reasoning. We evaluate DeepEyesV2 on RealX-Bench and other representative benchmarks, demonstrating its effectiveness across real-world understanding, mathematical reasoning, and search-intensive tasks. Moreover, DeepEyesV2 exhibits task-adaptive tool invocation, tending to use image operations for perception tasks and numerical computations for reasoning tasks. Reinforcement learning further enables complex tool combinations and allows model to selectively invoke tools based on context. We hope our study can provide guidance for community in developing agentic multimodal models.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiveStar: Live Streaming Assistant for Real-World Online Video Understanding</title>
<link>https://arxiv.org/abs/2511.05299</link>
<guid>https://arxiv.org/abs/2511.05299</guid>
<content:encoded><![CDATA[
arXiv:2511.05299v1 Announce Type: cross 
Abstract: Despite significant progress in Video Large Language Models (Video-LLMs) for offline video understanding, existing online Video-LLMs typically struggle to simultaneously process continuous frame-by-frame inputs and determine optimal response timing, often compromising real-time responsiveness and narrative coherence. To address these limitations, we introduce LiveStar, a pioneering live streaming assistant that achieves always-on proactive responses through adaptive streaming decoding. Specifically, LiveStar incorporates: (1) a training strategy enabling incremental video-language alignment for variable-length video streams, preserving temporal consistency across dynamically evolving frame sequences; (2) a response-silence decoding framework that determines optimal proactive response timing via a single forward pass verification; (3) memory-aware acceleration via peak-end memory compression for online inference on 10+ minute videos, combined with streaming key-value cache to achieve 1.53x faster inference. We also construct an OmniStar dataset, a comprehensive dataset for training and benchmarking that encompasses 15 diverse real-world scenarios and 5 evaluation tasks for online video understanding. Extensive experiments across three benchmarks demonstrate LiveStar's state-of-the-art performance, achieving an average 19.5% improvement in semantic correctness with 18.1% reduced timing difference compared to existing online Video-LLMs, while improving FPS by 12.0% across all five OmniStar tasks. Our model and dataset can be accessed at https://github.com/yzy-bupt/LiveStar.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Metrics and Diffusion Architecture for 3D Point Cloud Generation</title>
<link>https://arxiv.org/abs/2511.05308</link>
<guid>https://arxiv.org/abs/2511.05308</guid>
<content:encoded><![CDATA[
arXiv:2511.05308v1 Announce Type: cross 
Abstract: As 3D point clouds become a cornerstone of modern technology, the need for sophisticated generative models and reliable evaluation metrics has grown exponentially. In this work, we first expose that some commonly used metrics for evaluating generated point clouds, particularly those based on Chamfer Distance (CD), lack robustness against defects and fail to capture geometric fidelity and local shape consistency when used as quality indicators. We further show that introducing samples alignment prior to distance calculation and replacing CD with Density-Aware Chamfer Distance (DCD) are simple yet essential steps to ensure the consistency and robustness of point cloud generative model evaluation metrics. While existing metrics primarily focus on directly comparing 3D Euclidean coordinates, we present a novel metric, named Surface Normal Concordance (SNC), which approximates surface similarity by comparing estimated point normals. This new metric, when combined with traditional ones, provides a more comprehensive evaluation of the quality of generated samples. Finally, leveraging recent advancements in transformer-based models for point cloud analysis, such as serialized patch attention , we propose a new architecture for generating high-fidelity 3D structures, the Diffusion Point Transformer. We perform extensive experiments and comparisons on the ShapeNet dataset, showing that our model outperforms previous solutions, particularly in terms of quality of generated point clouds, achieving new state-of-the-art. Code available at https://github.com/matteo-bastico/DiffusionPointTransformer.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Are the Facts? Automated Extraction of Court-Established Facts from Criminal-Court Opinions</title>
<link>https://arxiv.org/abs/2511.05320</link>
<guid>https://arxiv.org/abs/2511.05320</guid>
<content:encoded><![CDATA[
arXiv:2511.05320v1 Announce Type: cross 
Abstract: Criminal justice administrative data contain only a limited amount of information about the committed offense. However, there is an unused source of extensive information in continental European courts' decisions: descriptions of criminal behaviors in verdicts by which offenders are found guilty. In this paper, we study the feasibility of extracting these descriptions from publicly available court decisions from Slovakia. We use two different approaches for retrieval: regular expressions and large language models (LLMs). Our baseline was a simple method employing regular expressions to identify typical words occurring before and after the description. The advanced regular expression approach further focused on "sparing" and its normalization (insertion of spaces between individual letters), typical for delineating the description. The LLM approach involved prompting the Gemini Flash 2.0 model to extract the descriptions using predefined instructions. Although the baseline identified descriptions in only 40.5% of verdicts, both methods significantly outperformed it, achieving 97% with advanced regular expressions and 98.75% with LLMs, and 99.5% when combined. Evaluation by law students showed that both advanced methods matched human annotations in about 90% of cases, compared to just 34.5% for the baseline. LLMs fully matched human-labeled descriptions in 91.75% of instances, and a combination of advanced regular expressions with LLMs reached 92%.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Perceptually Aligning Representations of Music via Noise-Augmented Autoencoders</title>
<link>https://arxiv.org/abs/2511.05350</link>
<guid>https://arxiv.org/abs/2511.05350</guid>
<content:encoded><![CDATA[
arXiv:2511.05350v1 Announce Type: cross 
Abstract: We argue that training autoencoders to reconstruct inputs from noised versions of their encodings, when combined with perceptual losses, yields encodings that are structured according to a perceptual hierarchy. We demonstrate the emergence of this hierarchical structure by showing that, after training an audio autoencoder in this manner, perceptually salient information is captured in coarser representation structures than with conventional training. Furthermore, we show that such perceptual hierarchies improve latent diffusion decoding in the context of estimating surprisal in music pitches and predicting EEG-brain responses to music listening. Pretrained weights are available on github.com/CPJKU/pa-audioic.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A multimodal multiplex of the mental lexicon for multilingual individuals</title>
<link>https://arxiv.org/abs/2511.05361</link>
<guid>https://arxiv.org/abs/2511.05361</guid>
<content:encoded><![CDATA[
arXiv:2511.05361v1 Announce Type: cross 
Abstract: Historically, bilingualism was often perceived as an additional cognitive load that could hinder linguistic and intellectual development. However, over the last three decades, this view has changed considerably. Numerous studies have aimed to model and understand the architecture of the bilingual word recognition system Dijkstra and van Heuven (2002), investigating how parallel activation operates in the brain and how one language influences another Kroll et al. (2015). Increasingly, evidence suggests that multilinguals, individuals who speak three or more languages, can perform better than monolinguals in various linguistic and cognitive tasks, such as learning an additional language Abu-Rabia and Sanitsky (2010). This research proposal focuses on the study of the mental lexicon and how it may be structured in individuals who speak multiple languages. Building on the work of Stella et al. (2018), who investigated explosive learning in humans using a multiplex model of the mental lexicon, and the Bilingual Interactive Activation (BIA+) framework proposed by Dijkstra and van Heuven (2002), the present study applies the same multilayer network principles introduced by Kivela et al. (2014). Our experimental design extends previous research by incorporating multimodality into the multiplex model, introducing an additional layer that connects visual inputs to their corresponding lexical representations across the multilingual layers of the mental lexicon. In this research, we aim to explore how a heritage language influences the acquisition of another language. Specifically, we ask: Does the presence of visual input in a translation task influence participants' proficiency and accuracy compared to text-only conditions?
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Literacy for Community Colleges: Instructors' Perspectives on Scenario-Based and Interactive Approaches to Teaching AI</title>
<link>https://arxiv.org/abs/2511.05363</link>
<guid>https://arxiv.org/abs/2511.05363</guid>
<content:encoded><![CDATA[
arXiv:2511.05363v1 Announce Type: cross 
Abstract: This research category full paper investigates how community college instructors evaluate interactive, no-code AI literacy resources designed for non-STEM learners. As artificial intelligence becomes increasingly integrated into everyday technologies, AI literacy - the ability to evaluate AI systems, communicate with them, and understand their broader impacts - has emerged as a critical skill across disciplines. Yet effective, scalable approaches for teaching these concepts in higher education remain limited, particularly for students outside STEM fields.
  To address this gap, we developed AI User, an interactive online curriculum that introduces core AI concepts through scenario - based activities set in real - world contexts. This study presents findings from four focus groups with instructors who engaged with AI User materials and participated in structured feedback activities. Thematic analysis revealed that instructors valued exploratory tasks that simulated real - world AI use cases and fostered experimentation, while also identifying challenges related to scaffolding, accessibility, and multi-modal support. A ranking task for instructional support materials showed a strong preference for interactive demonstrations over traditional educational materials like conceptual guides or lecture slides.
  These findings offer insights into instructor perspectives on making AI concepts more accessible and relevant for broad learner audiences. They also inform the design of AI literacy tools that align with diverse teaching contexts and support critical engagement with AI in higher education.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TeaRAG: A Token-Efficient Agentic Retrieval-Augmented Generation Framework</title>
<link>https://arxiv.org/abs/2511.05385</link>
<guid>https://arxiv.org/abs/2511.05385</guid>
<content:encoded><![CDATA[
arXiv:2511.05385v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) utilizes external knowledge to augment Large Language Models' (LLMs) reliability. For flexibility, agentic RAG employs autonomous, multi-round retrieval and reasoning to resolve queries. Although recent agentic RAG has improved via reinforcement learning, they often incur substantial token overhead from search and reasoning processes. This trade-off prioritizes accuracy over efficiency. To address this issue, this work proposes TeaRAG, a token-efficient agentic RAG framework capable of compressing both retrieval content and reasoning steps. 1) First, the retrieved content is compressed by augmenting chunk-based semantic retrieval with a graph retrieval using concise triplets. A knowledge association graph is then built from semantic similarity and co-occurrence. Finally, Personalized PageRank is leveraged to highlight key knowledge within this graph, reducing the number of tokens per retrieval. 2) Besides, to reduce reasoning steps, Iterative Process-aware Direct Preference Optimization (IP-DPO) is proposed. Specifically, our reward function evaluates the knowledge sufficiency by a knowledge matching mechanism, while penalizing excessive reasoning steps. This design can produce high-quality preference-pair datasets, supporting iterative DPO to improve reasoning conciseness. Across six datasets, TeaRAG improves the average Exact Match by 4% and 2% while reducing output tokens by 61% and 59% on Llama3-8B-Instruct and Qwen2.5-14B-Instruct, respectively. Code is available at https://github.com/Applied-Machine-Learning-Lab/TeaRAG.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Assisted AR Assembly: Object Recognition and Computer Vision for Augmented Reality Assisted Assembly</title>
<link>https://arxiv.org/abs/2511.05394</link>
<guid>https://arxiv.org/abs/2511.05394</guid>
<content:encoded><![CDATA[
arXiv:2511.05394v1 Announce Type: cross 
Abstract: We present an AI-assisted Augmented Reality assembly workflow that uses deep learning-based object recognition to identify different assembly components and display step-by-step instructions. For each assembly step, the system displays a bounding box around the corresponding components in the physical space, and where the component should be placed. By connecting assembly instructions with the real-time location of relevant components, the system eliminates the need for manual searching, sorting, or labeling of different components before each assembly. To demonstrate the feasibility of using object recognition for AR-assisted assembly, we highlight a case study involving the assembly of LEGO sculptures.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction</title>
<link>https://arxiv.org/abs/2511.05396</link>
<guid>https://arxiv.org/abs/2511.05396</guid>
<content:encoded><![CDATA[
arXiv:2511.05396v1 Announce Type: cross 
Abstract: Off-dynamics reinforcement learning (RL), where training and deployment transition dynamics are different, can be formulated as learning in a robust Markov decision process (RMDP) where uncertainties in transition dynamics are imposed. Existing literature mostly assumes access to generative models allowing arbitrary state-action queries or pre-collected datasets with a good state coverage of the deployment environment, bypassing the challenge of exploration. In this work, we study a more realistic and challenging setting where the agent is limited to online interaction with the training environment. To capture the intrinsic difficulty of exploration in online RMDPs, we introduce the supremal visitation ratio, a novel quantity that measures the mismatch between the training dynamics and the deployment dynamics. We show that if this ratio is unbounded, online learning becomes exponentially hard. We propose the first computationally efficient algorithm that achieves sublinear regret in online RMDPs with $f$-divergence based transition uncertainties. We also establish matching regret lower bounds, demonstrating that our algorithm achieves optimal dependence on both the supremal visitation ratio and the number of interaction episodes. Finally, we validate our theoretical results through comprehensive numerical experiments.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Neural Audio Fingerprinting using Music Foundation Models</title>
<link>https://arxiv.org/abs/2511.05399</link>
<guid>https://arxiv.org/abs/2511.05399</guid>
<content:encoded><![CDATA[
arXiv:2511.05399v1 Announce Type: cross 
Abstract: The proliferation of distorted, compressed, and manipulated music on modern media platforms like TikTok motivates the development of more robust audio fingerprinting techniques to identify the sources of musical recordings. In this paper, we develop and evaluate new neural audio fingerprinting techniques with the aim of improving their robustness. We make two contributions to neural fingerprinting methodology: (1) we use a pretrained music foundation model as the backbone of the neural architecture and (2) we expand the use of data augmentation to train fingerprinting models under a wide variety of audio manipulations, including time streching, pitch modulation, compression, and filtering. We systematically evaluate our methods in comparison to two state-of-the-art neural fingerprinting models: NAFP and GraFPrint. Results show that fingerprints extracted with music foundation models (e.g., MuQ, MERT) consistently outperform models trained from scratch or pretrained on non-musical audio. Segment-level evaluation further reveals their capability to accurately localize fingerprint matches, an important practical feature for catalog management.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-modal Loop Closure Detection with Foundation Models in Severely Unstructured Environments</title>
<link>https://arxiv.org/abs/2511.05404</link>
<guid>https://arxiv.org/abs/2511.05404</guid>
<content:encoded><![CDATA[
arXiv:2511.05404v1 Announce Type: cross 
Abstract: Robust loop closure detection is a critical component of Simultaneous Localization and Mapping (SLAM) algorithms in GNSS-denied environments, such as in the context of planetary exploration. In these settings, visual place recognition often fails due to aliasing and weak textures, while LiDAR-based methods suffer from sparsity and ambiguity. This paper presents MPRF, a multimodal pipeline that leverages transformer-based foundation models for both vision and LiDAR modalities to achieve robust loop closure in severely unstructured environments. Unlike prior work limited to retrieval, MPRF integrates a two-stage visual retrieval strategy with explicit 6-DoF pose estimation, combining DINOv2 features with SALAD aggregation for efficient candidate screening and SONATA-based LiDAR descriptors for geometric verification. Experiments on the S3LI dataset and S3LI Vulcano dataset show that MPRF outperforms state-of-the-art retrieval methods in precision while enhancing pose estimation robustness in low-texture regions. By providing interpretable correspondences suitable for SLAM back-ends, MPRF achieves a favorable trade-off between accuracy, efficiency, and reliability, demonstrating the potential of foundation models to unify place recognition and pose estimation. Code and models will be released at github.com/DLR-RM/MPRF.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProDER: A Continual Learning Approach for Fault Prediction in Evolving Smart Grids</title>
<link>https://arxiv.org/abs/2511.05420</link>
<guid>https://arxiv.org/abs/2511.05420</guid>
<content:encoded><![CDATA[
arXiv:2511.05420v1 Announce Type: cross 
Abstract: As smart grids evolve to meet growing energy demands and modern operational challenges, the ability to accurately predict faults becomes increasingly critical. However, existing AI-based fault prediction models struggle to ensure reliability in evolving environments where they are required to adapt to new fault types and operational zones. In this paper, we propose a continual learning (CL) framework in the smart grid context to evolve the model together with the environment. We design four realistic evaluation scenarios grounded in class-incremental and domain-incremental learning to emulate evolving grid conditions. We further introduce Prototype-based Dark Experience Replay (ProDER), a unified replay-based approach that integrates prototype-based feature regularization, logit distillation, and a prototype-guided replay memory. ProDER achieves the best performance among tested CL techniques, with only a 0.045 accuracy drop for fault type prediction and 0.015 for fault zone prediction. These results demonstrate the practicality of CL for scalable, real-world fault prediction in smart grids.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>"I Like That You Have to Poke Around": Instructors on How Experiential Approaches to AI Literacy Spark Inquiry and Critical Thinking</title>
<link>https://arxiv.org/abs/2511.05430</link>
<guid>https://arxiv.org/abs/2511.05430</guid>
<content:encoded><![CDATA[
arXiv:2511.05430v1 Announce Type: cross 
Abstract: As artificial intelligence (AI) increasingly shapes decision-making across domains, there is a growing need to support AI literacy among learners beyond computer science. However, many current approaches rely on programming-heavy tools or abstract lecture-based content, limiting accessibility for non-STEM audiences. This paper presents findings from a study of AI User, a modular, web-based curriculum that teaches core AI concepts through interactive, no-code projects grounded in real-world scenarios. The curriculum includes eight projects; this study focuses on instructor feedback on Projects 5-8, which address applied topics such as natural language processing, computer vision, decision support, and responsible AI. Fifteen community college instructors participated in structured focus groups, completing the projects as learners and providing feedback through individual reflection and group discussion. Using thematic analysis, we examined how instructors evaluated the design, instructional value, and classroom applicability of these experiential activities. Findings highlight instructors' appreciation for exploratory tasks, role-based simulations, and real-world relevance, while also surfacing design trade-offs around cognitive load, guidance, and adaptability for diverse learners. This work extends prior research on AI literacy by centering instructor perspectives on teaching complex AI topics without code. It offers actionable insights for designing inclusive, experiential AI learning resources that scale across disciplines and learner backgrounds.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>APP: Accelerated Path Patching with Task-Specific Pruning</title>
<link>https://arxiv.org/abs/2511.05442</link>
<guid>https://arxiv.org/abs/2511.05442</guid>
<content:encoded><![CDATA[
arXiv:2511.05442v1 Announce Type: cross 
Abstract: Circuit discovery is a key step in many mechanistic interpretability pipelines. Current methods, such as Path Patching, are computationally expensive and have limited in-depth circuit analysis for smaller models. In this study, we propose Accelerated Path Patching (APP), a hybrid approach leveraging our novel contrastive attention head pruning method to drastically reduce the search space of circuit discovery methods. Our Contrastive-FLAP pruning algorithm uses techniques from causal mediation analysis to assign higher pruning scores to task-specific attention heads, leading to higher performing sparse models compared to traditional pruning techniques. Although Contrastive-FLAP is successful at preserving task-specific heads that existing pruning algorithms remove at low sparsity ratios, the circuits found by Contrastive-FLAP alone are too large to satisfy the minimality constraint required in circuit analysis. APP first applies Contrastive-FLAP to reduce the search space on required for circuit discovery algorithms by, on average, 56\%. Next, APP, applies traditional Path Patching on the remaining attention heads, leading to a speed up of 59.63\%-93.27\% compared to Path Patching applied to the dense model. Despite the substantial computational saving that APP provides, circuits obtained from APP exhibit substantial overlap and similar performance to previously established Path Patching circuits
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-adaptive weighting and sampling for physics-informed neural networks</title>
<link>https://arxiv.org/abs/2511.05452</link>
<guid>https://arxiv.org/abs/2511.05452</guid>
<content:encoded><![CDATA[
arXiv:2511.05452v1 Announce Type: cross 
Abstract: Physics-informed deep learning has emerged as a promising framework for solving partial differential equations (PDEs). Nevertheless, training these models on complex problems remains challenging, often leading to limited accuracy and efficiency. In this work, we introduce a hybrid adaptive sampling and weighting method to enhance the performance of physics-informed neural networks (PINNs). The adaptive sampling component identifies training points in regions where the solution exhibits rapid variation, while the adaptive weighting component balances the convergence rate across training points. Numerical experiments show that applying only adaptive sampling or only adaptive weighting is insufficient to consistently achieve accurate predictions, particularly when training points are scarce. Since each method emphasizes different aspects of the solution, their effectiveness is problem dependent. By combining both strategies, the proposed framework consistently improves prediction accuracy and training efficiency, offering a more robust approach for solving PDEs with PINNs.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SWE-Compass: Towards Unified Evaluation of Agentic Coding Abilities for Large Language Models</title>
<link>https://arxiv.org/abs/2511.05459</link>
<guid>https://arxiv.org/abs/2511.05459</guid>
<content:encoded><![CDATA[
arXiv:2511.05459v1 Announce Type: cross 
Abstract: Evaluating large language models (LLMs) for software engineering has been limited by narrow task coverage, language bias, and insufficient alignment with real-world developer workflows. Existing benchmarks often focus on algorithmic problems or Python-centric bug fixing, leaving critical dimensions of software engineering underexplored. To address these gaps, we introduce SWE-Compass1, a comprehensive benchmark that unifies heterogeneous code-related evaluations into a structured and production-aligned framework. SWE-Compass spans 8 task types, 8 programming scenarios, and 10 programming languages, with 2000 high-quality instances curated from authentic GitHub pull requests and refined through systematic filtering and validation. We benchmark ten state-of-the-art LLMs under two agentic frameworks, SWE-Agent and Claude Code, revealing a clear hierarchy of difficulty across task types, languages, and scenarios. Moreover, by aligning evaluation with real-world developer practices, SWE-Compass provides a rigorous and reproducible foundation for diagnosing and advancing agentic coding capabilities in large language models.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Literacy Assessment Revisited: A Task-Oriented Approach Aligned with Real-world Occupations</title>
<link>https://arxiv.org/abs/2511.05475</link>
<guid>https://arxiv.org/abs/2511.05475</guid>
<content:encoded><![CDATA[
arXiv:2511.05475v1 Announce Type: cross 
Abstract: As artificial intelligence (AI) systems become ubiquitous in professional contexts, there is an urgent need to equip workers, often with backgrounds outside of STEM, with the skills to use these tools effectively as well as responsibly, that is, to be AI literate. However, prevailing definitions and therefore assessments of AI literacy often emphasize foundational technical knowledge, such as programming, mathematics, and statistics, over practical knowledge such as interpreting model outputs, selecting tools, or identifying ethical concerns. This leaves a noticeable gap in assessing someone's AI literacy for real-world job use. We propose a work-task-oriented assessment model for AI literacy which is grounded in the competencies required for effective use of AI tools in professional settings. We describe the development of a novel AI literacy assessment instrument, and accompanying formative assessments, in the context of a US Navy robotics training program. The program included training in robotics and AI literacy, as well as a competition with practical tasks and a multiple choice scenario task meant to simulate use of AI in a job setting. We found that, as a measure of applied AI literacy, the competition's scenario task outperformed the tests we adopted from past research or developed ourselves. We argue that when training people for AI-related work, educators should consider evaluating them with instruments that emphasize highly contextualized practical skills rather than abstract technical knowledge, especially when preparing workers without technical backgrounds for AI-integrated roles.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Flow Matching KL Divergence</title>
<link>https://arxiv.org/abs/2511.05480</link>
<guid>https://arxiv.org/abs/2511.05480</guid>
<content:encoded><![CDATA[
arXiv:2511.05480v1 Announce Type: cross 
Abstract: We derive a deterministic, non-asymptotic upper bound on the Kullback-Leibler (KL) divergence of the flow-matching distribution approximation. In particular, if the $L_2$ flow-matching loss is bounded by $\epsilon^2 > 0$, then the KL divergence between the true data distribution and the estimated distribution is bounded by $A_1 \epsilon + A_2 \epsilon^2$. Here, the constants $A_1$ and $A_2$ depend only on the regularities of the data and velocity fields. Consequently, this bound implies statistical convergence rates of Flow Matching Transformers under the Total Variation (TV) distance. We show that, flow matching achieves nearly minimax-optimal efficiency in estimating smooth distributions. Our results make the statistical efficiency of flow matching comparable to that of diffusion models under the TV distance. Numerical studies on synthetic and learned velocities corroborate our theory.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DGTN: Graph-Enhanced Transformer with Diffusive Attention Gating Mechanism for Enzyme DDG Prediction</title>
<link>https://arxiv.org/abs/2511.05483</link>
<guid>https://arxiv.org/abs/2511.05483</guid>
<content:encoded><![CDATA[
arXiv:2511.05483v1 Announce Type: cross 
Abstract: Predicting the effect of amino acid mutations on enzyme thermodynamic stability (DDG) is fundamental to protein engineering and drug design. While recent deep learning approaches have shown promise, they often process sequence and structure information independently, failing to capture the intricate coupling between local structural geometry and global sequential patterns. We present DGTN (Diffused Graph-Transformer Network), a novel architecture that co-learns graph neural network (GNN) weights for structural priors and transformer attention through a diffusion mechanism. Our key innovation is a bidirectional diffusion process where: (1) GNN-derived structural embeddings guide transformer attention via learnable diffusion kernels, and (2) transformer representations refine GNN message passing through attention-modulated graph updates. We provide rigorous mathematical analysis showing this co-learning scheme achieves provably better approximation bounds than independent processing. On ProTherm and SKEMPI benchmarks, DGTN achieves state-of-the-art performance (Pearson Rho = 0.87, RMSE = 1.21 kcal/mol), with 6.2% improvement over best baselines. Ablation studies confirm the diffusion mechanism contributes 4.8 points to correlation. Our theoretical analysis proves the diffused attention converges to optimal structure-sequence coupling, with convergence rate O(1/sqrt(T) ) where T is diffusion steps. This work establishes a principled framework for integrating heterogeneous protein representations through learnable diffusion.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.05489</link>
<guid>https://arxiv.org/abs/2511.05489</guid>
<content:encoded><![CDATA[
arXiv:2511.05489v1 Announce Type: cross 
Abstract: Temporal search aims to identify a minimal set of relevant frames from tens of thousands based on a given query, serving as a foundation for accurate long-form video understanding. Existing works attempt to progressively narrow the search space. However, these approaches typically rely on a hand-crafted search process, lacking end-to-end optimization for learning optimal search strategies. In this paper, we propose TimeSearch-R, which reformulates temporal search as interleaved text-video thinking, seamlessly integrating searching video clips into the reasoning process through reinforcement learning (RL). However, applying RL training methods, such as Group Relative Policy Optimization (GRPO), to video reasoning can result in unsupervised intermediate search decisions. This leads to insufficient exploration of the video content and inconsistent logical reasoning. To address these issues, we introduce GRPO with Completeness Self-Verification (GRPO-CSV), which gathers searched video frames from the interleaved reasoning process and utilizes the same policy model to verify the adequacy of searched frames, thereby improving the completeness of video reasoning. Additionally, we construct datasets specifically designed for the SFT cold-start and RL training of GRPO-CSV, filtering out samples with weak temporal dependencies to enhance task difficulty and improve temporal search capabilities. Extensive experiments demonstrate that TimeSearch-R achieves significant improvements on temporal search benchmarks such as Haystack-LVBench and Haystack-Ego4D, as well as long-form video understanding benchmarks like VideoMME and MLVU. Notably, TimeSearch-R establishes a new state-of-the-art on LongVideoBench with 4.1% improvement over the base model Qwen2.5-VL and 2.0% over the advanced video reasoning model Video-R1. Our code is available at https://github.com/Time-Search/TimeSearch-R.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Joint Verification and Refinement of Language Models for Safety-Constrained Planning</title>
<link>https://arxiv.org/abs/2410.14865</link>
<guid>https://arxiv.org/abs/2410.14865</guid>
<content:encoded><![CDATA[
arXiv:2410.14865v2 Announce Type: replace 
Abstract: Large language models possess impressive capabilities in generating programs (e.g., Python) from natural language descriptions to execute robotic tasks. However, these generated programs often contain errors that violate externally given task specifications. Without an effective method to verify their correctness, the reliable deployment of language models in real-world systems is practically infeasible. We develop a method that converts generated robot programs into an automaton-based representation and verifies them against task-relevant safety specifications. We establish a theorem that any arbitrary combination of the verified programs will also satisfy the safety specifications. Hence, the method eliminates the need to verify complex programs composed of multiple simpler ones, reducing computation complexity. We then introduce an automated fine-tuning procedure that leverages verification outcomes for supervision. By applying the theorem, this procedure only requires training the model to generate safe sub-components, thereby improving training efficiency. Empirical results on robot applications show a 30 percent increase in the probability of generating specification-compliant programs, with training time reduced by half compared to fine-tuning on generating full programs.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retrieval Augmented Diffusion Model for Structure-informed Antibody Design and Optimization</title>
<link>https://arxiv.org/abs/2410.15040</link>
<guid>https://arxiv.org/abs/2410.15040</guid>
<content:encoded><![CDATA[
arXiv:2410.15040v2 Announce Type: replace 
Abstract: Antibodies are essential proteins responsible for immune responses in organisms, capable of specifically recognizing antigen molecules of pathogens. Recent advances in generative models have significantly enhanced rational antibody design. However, existing methods mainly create antibodies from scratch without template constraints, leading to model optimization challenges and unnatural sequences. To address these issues, we propose a retrieval-augmented diffusion framework, termed RADAb, for efficient antibody design. Our method leverages a set of structural homologous motifs that align with query structural constraints to guide the generative model in inversely optimizing antibodies according to desired design criteria. Specifically, we introduce a structure-informed retrieval mechanism that integrates these exemplar motifs with the input backbone through a novel dual-branch denoising module, utilizing both structural and evolutionary information. Additionally, we develop a conditional diffusion model that iteratively refines the optimization process by incorporating both global context and local evolutionary conditions. Our approach is agnostic to the choice of generative models. Empirical experiments demonstrate that our method achieves state-of-the-art performance in multiple antibody inverse folding and optimization tasks, offering a new perspective on biomolecular generative models.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wider or Deeper? Scaling LLM Inference-Time Compute with Adaptive Branching Tree Search</title>
<link>https://arxiv.org/abs/2503.04412</link>
<guid>https://arxiv.org/abs/2503.04412</guid>
<content:encoded><![CDATA[
arXiv:2503.04412v5 Announce Type: replace 
Abstract: Recent advances demonstrate that increasing inference-time computation can significantly boost the reasoning capabilities of large language models (LLMs). Although repeated sampling (i.e., generating multiple candidate outputs) is a highly effective strategy, it does not leverage external feedback signals for refinement, which are often available in tasks like coding. In this work, we propose Adaptive Branching Monte Carlo Tree Search (AB-MCTS), a novel inference-time framework that generalizes repeated sampling with principled multi-turn exploration and exploitation. At each node in the search tree, AB-MCTS dynamically decides whether to "go wider" by expanding new candidate responses or "go deeper" by revisiting existing ones based on external feedback signals. We evaluate our method on complex coding and engineering tasks using frontier models. Empirical results show that AB-MCTS consistently outperforms both repeated sampling and standard MCTS, underscoring the importance of combining the response diversity of LLMs with multi-turn solution refinement for effective inference-time scaling. Code is available at https://github.com/SakanaAI/treequest .
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Through the Human Lens: Investigating Cognitive Theories in Machine Psychology</title>
<link>https://arxiv.org/abs/2506.18156</link>
<guid>https://arxiv.org/abs/2506.18156</guid>
<content:encoded><![CDATA[
arXiv:2506.18156v2 Announce Type: replace 
Abstract: We investigate whether Large Language Models (LLMs) exhibit human-like cognitive patterns under four established frameworks from psychology: Thematic Apperception Test (TAT), Framing Bias, Moral Foundations Theory (MFT), and Cognitive Dissonance. We evaluated several proprietary and open-source models using structured prompts and automated scoring. Our findings reveal that these models often produce coherent narratives, show susceptibility to positive framing, exhibit moral judgments aligned with Liberty/Oppression concerns, and demonstrate self-contradictions tempered by extensive rationalization. Such behaviors mirror human cognitive tendencies yet are shaped by their training data and alignment methods. We discuss the implications for AI transparency, ethical deployment, and future work that bridges cognitive psychology and AI safety
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Introducing LongCat-Flash-Thinking: A Technical Report</title>
<link>https://arxiv.org/abs/2509.18883</link>
<guid>https://arxiv.org/abs/2509.18883</guid>
<content:encoded><![CDATA[
arXiv:2509.18883v2 Announce Type: replace 
Abstract: We present LongCat-Flash-Thinking, an efficient 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model. Its advanced capabilities are cultivated through a meticulously crafted training process, beginning with long Chain-of-Thought (CoT) data cold-start and culminating in large-scale Reinforcement Learning (RL). We first employ a well-designed cold-start training strategy, which significantly enhances the reasoning potential and equips the model with specialized skills in both formal and agentic reasoning. Then, a core innovation is our domain-parallel training scheme, which decouples optimization across distinct domains (e.g., STEM, Code, Agentic) and subsequently fuses the resulting expert models into a single, nearly Pareto-optimal model. This entire process is powered by our Dynamic ORchestration for Asynchronous rollout (DORA) system, a large-scale RL framework that delivers a greater than threefold training speedup over synchronous methods on tens of thousands of accelerators. As a result, LongCat-Flash-Thinking achieves state-of-the-art performance among open-source models on a suite of complex reasoning tasks. The model exhibits exceptional efficiency in agentic reasoning, reducing average token consumption by 64.5% (from 19, 653 to 6, 965) on AIME-25, without degrading task accuracy. We release LongCat-Flash-Thinking to promote further advances in reasoning systems and agentic AI research.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bilinear relational structure fixes reversal curse and enables consistent model editing</title>
<link>https://arxiv.org/abs/2509.21993</link>
<guid>https://arxiv.org/abs/2509.21993</guid>
<content:encoded><![CDATA[
arXiv:2509.21993v2 Announce Type: replace 
Abstract: The reversal curse -- a language model's (LM) inability to infer an unseen fact ``B is A'' from a learned fact ``A is B'' -- is widely considered a fundamental limitation. We show that this is not an inherent failure but an artifact of how models encode knowledge. By training LMs from scratch on a synthetic dataset of relational knowledge graphs, we demonstrate that bilinear relational structure emerges in their hidden representations. This structure substantially alleviates the reversal curse, enabling LMs to infer unseen reverse facts. Crucially, we also find that this bilinear structure plays a key role in consistent model editing. When a fact is updated in a LM with this structure, the edit correctly propagates to its reverse and other logically dependent facts. In contrast, models lacking this representation not only suffer from the reversal curse but also fail to generalize edits, further introducing logical inconsistencies. Our results establish that training on a relational knowledge dataset induces the emergence of bilinear internal representations, which in turn enable LMs to behave in a logically consistent manner after editing. This implies that the success of model editing depends critically not just on editing algorithms but on the underlying representational geometry of the knowledge being modified.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Open Agent Specification (Agent Spec): A Unified Representation for AI Agents</title>
<link>https://arxiv.org/abs/2510.04173</link>
<guid>https://arxiv.org/abs/2510.04173</guid>
<content:encoded><![CDATA[
arXiv:2510.04173v4 Announce Type: replace 
Abstract: The proliferation of agent frameworks has led to fragmentation in how agents are defined, executed, and evaluated. Existing systems differ in their abstractions, data flow semantics, and tool integrations, making it difficult to share or reproduce workflows. We introduce Open Agent Specification (Agent Spec), a declarative language that defines AI agents and agentic workflows in a way that is compatible across frameworks, promoting reusability, portability and interoperability of AI agents. Agent Spec defines a common set of components, control and data flow semantics, and schemas that allow an agent to be defined once and executed across different runtimes. Agent Spec also introduces a standardized Evaluation harness to assess agent behavior and agentic workflows across runtimes - analogous to how HELM and related harnesses standardized LLM evaluation - so that performance, robustness, and efficiency can be compared consistently across frameworks. We demonstrate this using four distinct runtimes (LangGraph, CrewAI, AutoGen, and WayFlow) evaluated over three different benchmarks (SimpleQA Verified, $\tau^2$-Bench and BIRD-SQL). We provide accompanying toolsets: a Python SDK (PyAgentSpec), a reference runtime (WayFlow), and adapters for popular frameworks (e.g., LangGraph, AutoGen, CrewAI). Agent Spec bridges the gap between model-centric and agent-centric standardization & evaluation, laying the groundwork for reliable, reusable, and portable agentic systems.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Internal World Models as Imagination Networks in Cognitive Agents</title>
<link>https://arxiv.org/abs/2510.04391</link>
<guid>https://arxiv.org/abs/2510.04391</guid>
<content:encoded><![CDATA[
arXiv:2510.04391v2 Announce Type: replace 
Abstract: What is the computational objective of imagination? While classical interpretations suggest imagination is useful for maximizing rewards, recent findings challenge this view. In this study, we propose that imagination serves to access an internal world model (IWM) and use psychological network analysis to explore IWMs in humans and large language models (LLMs). Specifically, we assessed imagination vividness ratings using two questionnaires and constructed imagination networks from these reports. Imagination networks from human groups showed correlations between different centrality measures, including expected influence, strength, and closeness. However, imagination networks from LLMs showed a lack of clustering and lower correlations between centrality measures under different prompts and conversational memory conditions. Together, these results indicate a lack of similarity between IWMs in human and LLM agents. Overall, our study offers a novel method for comparing internally-generated representations in humans and AI, providing insights for developing human-like imagination in artificial intelligence.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HugAgent: Benchmarking LLMs for Simulation of Individualized Human Reasoning</title>
<link>https://arxiv.org/abs/2510.15144</link>
<guid>https://arxiv.org/abs/2510.15144</guid>
<content:encoded><![CDATA[
arXiv:2510.15144v3 Announce Type: replace 
Abstract: Simulating human reasoning in open-ended tasks has long been a central aspiration in AI and cognitive science. While large language models now approximate human responses at scale, they remain tuned to population-level consensus, often erasing the individuality of reasoning styles and belief trajectories. To advance the vision of more human-like reasoning in machines, we introduce HugAgent (Human-Grounded Agent Benchmark), which rethinks human reasoning simulation along three dimensions: (i) from averaged to individualized reasoning, (ii) from behavioral mimicry to cognitive alignment, and (iii) from vignette-based to open-ended data. The benchmark evaluates whether a model can predict a specific person's behavioral responses and the underlying reasoning dynamics in out-of-distribution scenarios, given partial evidence of their prior views. HugAgent adopts a dual-track design: a human track that automates and scales the think-aloud method to collect ecologically valid human reasoning data, and a synthetic track for further scalability and systematic stress testing. This architecture enables low-cost, extensible expansion to new tasks and populations. Experiments with state-of-the-art language models reveal persistent adaptation gaps, positioning HugAgent as the first extensible benchmark for aligning machine reasoning with the individuality of human thought. The benchmark, along with its complete data collection pipeline and companion chatbot, is open-sourced as HugAgent (https://anonymous.4open.science/r/HugAgent) and TraceYourThinking (https://anonymous.4open.science/r/trace-your-thinking).
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>String Seed of Thought: Prompting LLMs for Distribution-Faithful and Diverse Generation</title>
<link>https://arxiv.org/abs/2510.21150</link>
<guid>https://arxiv.org/abs/2510.21150</guid>
<content:encoded><![CDATA[
arXiv:2510.21150v2 Announce Type: replace 
Abstract: We introduce String Seed of Thought (SSoT), a novel prompting method for LLMs that improves Probabilistic Instruction Following (PIF). We define PIF as a task requiring an LLM to select its answer from a predefined set of options, each associated with a specific probability, such that the empirical distribution of the generated answers aligns with the target distribution when prompted multiple times. While LLMs excel at tasks with single, deterministic answers, they often fail at PIF, exhibiting biases problematic for applications requiring non-deterministic behaviors, such as human-behavior simulation, content diversification, and multiplayer games. It also harms the diversity of generated responses, a crucial factor in test-time scaling, by causing the outputs to collapse into a limited set of answers. To address this, we propose SSoT, a simple prompting method that instructs an LLM to first output a random string to generate sufficient entropy. SSoT also instructs the LLM to extract randomness by manipulating this string to derive a final answer, thereby preserving diversity while adhering to specific constraints. We demonstrate that SSoT significantly improves the PIF performance of LLMs, approaching the ideal performance of a pseudo-random number generator. Furthermore, our experiments on NoveltyBench show SSoT's benefits extend beyond closed-set tasks to open-ended tasks by enhancing response diversity.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Do AI Agents Do Human Work? Comparing AI and Human Workflows Across Diverse Occupations</title>
<link>https://arxiv.org/abs/2510.22780</link>
<guid>https://arxiv.org/abs/2510.22780</guid>
<content:encoded><![CDATA[
arXiv:2510.22780v2 Announce Type: replace 
Abstract: AI agents are continually optimized for tasks related to human work, such as software engineering and professional writing, signaling a pressing trend with significant impacts on the human workforce. However, these agent developments have often not been grounded in a clear understanding of how humans execute work, to reveal what expertise agents possess and the roles they can play in diverse workflows. In this work, we study how agents do human work by presenting the first direct comparison of human and agent workers across multiple essential work-related skills: data analysis, engineering, computation, writing, and design. To better understand and compare heterogeneous computer-use activities of workers, we introduce a scalable toolkit to induce interpretable, structured workflows from either human or agent computer-use activities. Using such induced workflows, we compare how humans and agents perform the same tasks and find that: (1) While agents exhibit promise in their alignment to human workflows, they take an overwhelmingly programmatic approach across all work domains, even for open-ended, visually dependent tasks like design, creating a contrast with the UI-centric methods typically used by humans. (2) Agents produce work of inferior quality, yet often mask their deficiencies via data fabrication and misuse of advanced tools. (3) Nonetheless, agents deliver results 88.3% faster and cost 90.4-96.2% less than humans, highlighting the potential for enabling efficient collaboration by delegating easily programmable tasks to agents.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Observability Data to Diagnosis: An Evolving Multi-agent System for Incident Management in Cloud Systems</title>
<link>https://arxiv.org/abs/2510.24145</link>
<guid>https://arxiv.org/abs/2510.24145</guid>
<content:encoded><![CDATA[
arXiv:2510.24145v2 Announce Type: replace 
Abstract: Incident management (IM) is central to the reliability of large-scale cloud systems. Yet manual IM, where on-call engineers examine metrics, logs, and traces is labor-intensive and error-prone in the face of massive and heterogeneous observability data. Existing automated IM approaches often struggle to generalize across systems, provide limited interpretability, and incur high deployment costs, which hinders adoption in practice. In this paper, we present OpsAgent, a lightweight, self-evolving multi-agent system for IM that employs a training-free data processor to convert heterogeneous observability data into structured textual descriptions, along with a multi-agent collaboration framework that makes diagnostic inference transparent and auditable. To support continual capability growth, OpsAgent also introduces a dual self-evolution mechanism that integrates internal model updates with external experience accumulation, thereby closing the deployment loop. Comprehensive experiments on the OPENRCA benchmark demonstrate state-of-the-art performance and show that OpsAgent is generalizable, interpretable, cost-efficient, and self-evolving, making it a practically deployable and sustainable solution for long-term operation in real-world cloud systems.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inverse Knowledge Search over Verifiable Reasoning: Synthesizing a Scientific Encyclopedia from a Long Chains-of-Thought Knowledge Base</title>
<link>https://arxiv.org/abs/2510.26854</link>
<guid>https://arxiv.org/abs/2510.26854</guid>
<content:encoded><![CDATA[
arXiv:2510.26854v2 Announce Type: replace 
Abstract: Most scientific materials compress reasoning, presenting conclusions while omitting the derivational chains that justify them. This compression hinders verification by lacking explicit, step-wise justifications and inhibits cross-domain links by collapsing the very pathways that establish the logical and causal connections between concepts. We introduce a scalable framework that decompresses scientific reasoning, constructing a verifiable Long Chain-of-Thought (LCoT) knowledge base and projecting it into an emergent encyclopedia, SciencePedia. Our pipeline operationalizes an endpoint-driven, reductionist strategy: a Socratic agent, guided by a curriculum of around 200 courses, generates approximately 3 million first-principles questions. To ensure high fidelity, multiple independent solver models generate LCoTs, which are then rigorously filtered by prompt sanitization and cross-model answer consensus, retaining only those with verifiable endpoints. This verified corpus powers the Brainstorm Search Engine, which performs inverse knowledge search -- retrieving diverse, first-principles derivations that culminate in a target concept. This engine, in turn, feeds the Plato synthesizer, which narrates these verified chains into coherent articles. The initial SciencePedia comprises approximately 200,000 fine-grained entries spanning mathematics, physics, chemistry, biology, engineering, and computation. In evaluations across six disciplines, Plato-synthesized articles (conditioned on retrieved LCoTs) exhibit substantially higher knowledge-point density and significantly lower factual error rates than an equally-prompted baseline without retrieval (as judged by an external LLM). Built on this verifiable LCoT knowledge base, this reasoning-centric approach enables trustworthy, cross-domain scientific synthesis at scale and establishes the foundation for an ever-expanding encyclopedia.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Less Intelligent the Elements, the More Intelligent the Whole. Or, Possibly Not?</title>
<link>https://arxiv.org/abs/2012.12689</link>
<guid>https://arxiv.org/abs/2012.12689</guid>
<content:encoded><![CDATA[
arXiv:2012.12689v5 Announce Type: replace-cross 
Abstract: The agent-based modelling community has a debate on how ``intelligent'' artificial agents should be, and in what ways their local intelligence relates to the emergence of a collective intelligence. I approach this debate by endowing the preys and predators of the Lotka-Volterra model with behavioral algorithms characterized by different levels of sophistication. The main finding is that by endowing both preys and predators with the capability of making predictions based on linear extrapolation a novel sort of dynamic equilibrium appears, where both species co-exist while both populations grow indefinitely. While this broadly confirms that, in general, relatively simple agents favor the emergence of complex collective behavior, it also suggests that one fundamental mechanism is that the capability of individuals to take first-order derivatives of one other's behavior can allow the collective computation of derivatives of any order.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning for Interval Prediction of Electricity Demand: A Cluster-based Bootstrapping Approach</title>
<link>https://arxiv.org/abs/2309.01336</link>
<guid>https://arxiv.org/abs/2309.01336</guid>
<content:encoded><![CDATA[
arXiv:2309.01336v2 Announce Type: replace-cross 
Abstract: Accurate predictions of electricity demands are necessary for managing operations in a small aggregation load setting like a Microgrid. Due to low aggregation, the electricity demands can be highly stochastic and point estimates would lead to inflated errors. Interval estimation in this scenario, would provide a range of values within which the future values might lie and helps quantify the errors around the point estimates. This paper introduces a residual bootstrap algorithm to generate interval estimates of day-ahead electricity demand. A machine learning algorithm is used to obtain the point estimates of electricity demand and respective residuals on the training set. The obtained residuals are stored in memory and the memory is further partitioned. Days with similar demand patterns are grouped in clusters using an unsupervised learning algorithm and these clusters are used to partition the memory. The point estimates for test day are used to find the closest cluster of similar days and the residuals are bootstrapped from the chosen cluster. This algorithm is evaluated on the real electricity demand data from EULR(End Use Load Research) and is compared to other bootstrapping methods for varying confidence intervals.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Characterizing the Training Dynamics of Private Fine-tuning with Langevin diffusion</title>
<link>https://arxiv.org/abs/2402.18905</link>
<guid>https://arxiv.org/abs/2402.18905</guid>
<content:encoded><![CDATA[
arXiv:2402.18905v2 Announce Type: replace-cross 
Abstract: We show that differentially private full fine-tuning (DP-FFT) can distort pre-trained backbone features based on both theoretical and empirical results. We identify the cause of the distortion as the misalignment between the pre-trained backbone and the randomly initialized linear head. We prove that a sequential fine-tuning strategy can mitigate the feature distortion: first-linear-probing-then-fine-tuning (DP-LP-FFT). A new approximation scheme allows us to derive approximate upper and lower bounds on the training loss of DP-LP and DP-FFT, in a simple but canonical setting of 2-layer neural networks with ReLU activation. Experiments on real-world datasets and architectures are consistent with our theoretical insights. We also derive new upper bounds for 2-layer linear networks without the approximation. Moreover, our theory suggests a trade-off of privacy budget allocation in multi-phase fine-tuning methods like DP-LP-FFT.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tactical Decision Making for Autonomous Trucks by Deep Reinforcement Learning with Total Cost of Operation Based Reward</title>
<link>https://arxiv.org/abs/2403.06524</link>
<guid>https://arxiv.org/abs/2403.06524</guid>
<content:encoded><![CDATA[
arXiv:2403.06524v2 Announce Type: replace-cross 
Abstract: We develop a deep reinforcement learning framework for tactical decision making in an autonomous truck, specifically for Adaptive Cruise Control (ACC) and lane change maneuvers in a highway scenario. Our results demonstrate that it is beneficial to separate high-level decision-making processes and low-level control actions between the reinforcement learning agent and the low-level controllers based on physical models. In the following, we study optimizing the performance with a realistic and multi-objective reward function based on Total Cost of Operation (TCOP) of the truck using different approaches; by adding weights to reward components, by normalizing the reward components and by using curriculum learning techniques.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FunOTTA: On-the-Fly Adaptation on Cross-Domain Fundus Image via Stable Test-time Training</title>
<link>https://arxiv.org/abs/2407.04396</link>
<guid>https://arxiv.org/abs/2407.04396</guid>
<content:encoded><![CDATA[
arXiv:2407.04396v3 Announce Type: replace-cross 
Abstract: Fundus images are essential for the early screening and detection of eye diseases. While deep learning models using fundus images have significantly advanced the diagnosis of multiple eye diseases, variations in images from different imaging devices and locations (known as domain shifts) pose challenges for deploying pre-trained models in real-world applications. To address this, we propose a novel Fundus On-the-fly Test-Time Adaptation (FunOTTA) framework that effectively generalizes a fundus image diagnosis model to unseen environments, even under strong domain shifts. FunOTTA stands out for its stable adaptation process by performing dynamic disambiguation in the memory bank while minimizing harmful prior knowledge bias. We also introduce a new training objective during adaptation that enables the classifier to incrementally adapt to target patterns with reliable class conditional estimation and consistency regularization. We compare our method with several state-of-the-art test-time adaptation (TTA) pipelines. Experiments on cross-domain fundus image benchmarks across two diseases demonstrate the superiority of the overall framework and individual components under different backbone networks. Code is available at https://github.com/Casperqian/FunOTTA.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Affordance-based Robot Manipulation with Flow Matching</title>
<link>https://arxiv.org/abs/2409.01083</link>
<guid>https://arxiv.org/abs/2409.01083</guid>
<content:encoded><![CDATA[
arXiv:2409.01083v5 Announce Type: replace-cross 
Abstract: We present a framework for assistive robot manipulation, which focuses on two fundamental challenges: first, efficiently adapting large-scale models to downstream scene affordance understanding tasks, especially in daily living scenarios where gathering multi-task data involving humans requires strenuous effort; second, effectively learning robot action trajectories by grounding the visual affordance model. We tackle the first challenge by employing a parameter-efficient prompt tuning method that prepends learnable text prompts to the frozen vision model to predict manipulation affordances in multi-task scenarios. Then we propose to learn robot action trajectories guided by affordances in a supervised flow matching method. Flow matching represents a robot visuomotor policy as a conditional process of flowing random waypoints to desired robot action trajectories. Finally, we introduce a real-world dataset with 10 tasks across Activities of Daily Living to test our framework. Our extensive evaluation highlights that the proposed prompt tuning method for learning manipulation affordance achieves competitive performance and even outperforms some other finetuning protocols across data scales, while satisfying parameter efficiency. Learning multi-task robot action trajectories with flow matching leads to consistently favorable results in several robot manipulation benchmarks than some alternative behavior cloning methods. This includes more stable training and evaluation, and noticeably faster inference, while maintaining comparable generalization performance to diffusion policy, where flow matching performs marginally better in most cases. Our framework seamlessly unifies affordance learning and action generation with flow matching for robot manipulation.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CTPD: Cross-Modal Temporal Pattern Discovery for Enhanced Multimodal Electronic Health Records Analysis</title>
<link>https://arxiv.org/abs/2411.00696</link>
<guid>https://arxiv.org/abs/2411.00696</guid>
<content:encoded><![CDATA[
arXiv:2411.00696v3 Announce Type: replace-cross 
Abstract: Integrating multimodal Electronic Health Records (EHR) data, such as numerical time series and free-text clinical reports, has great potential in predicting clinical outcomes. However, prior work has primarily focused on capturing temporal interactions within individual samples and fusing multimodal information, overlooking critical temporal patterns across patients. These patterns, such as trends in vital signs like abnormal heart rate or blood pressure, can indicate deteriorating health or an impending critical event. Similarly, clinical notes often contain textual descriptions that reflect these patterns. Identifying corresponding temporal patterns across different modalities is crucial for improving the accuracy of clinical outcome predictions, yet it remains a challenging task. To address this gap, we introduce a Cross-Modal Temporal Pattern Discovery (CTPD) framework, designed to efficiently extract meaningful cross-modal temporal patterns from multimodal EHR data. Our approach introduces shared initial temporal pattern representations which are refined using slot attention to generate temporal semantic embeddings. To ensure rich cross-modal temporal semantics in the learned patterns, we introduce a contrastive-based TPNCE loss for cross-modal alignment, along with two reconstruction losses to retain core information of each modality. Evaluations on two clinically critical tasks, 48-hour in-hospital mortality and 24-hour phenotype classification, using the MIMIC-III database demonstrate the superiority of our method over existing approaches.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TOBUGraph: Knowledge Graph-Based Retrieval for Enhanced LLM Performance Beyond RAG</title>
<link>https://arxiv.org/abs/2412.05447</link>
<guid>https://arxiv.org/abs/2412.05447</guid>
<content:encoded><![CDATA[
arXiv:2412.05447v3 Announce Type: replace-cross 
Abstract: Retrieval-Augmented Generation (RAG) is one of the leading and most widely used techniques for enhancing LLM retrieval capabilities, but it still faces significant limitations in commercial use cases. RAG primarily relies on the query-chunk text-to-text similarity in the embedding space for retrieval and can fail to capture deeper semantic relationships across chunks, is highly sensitive to chunking strategies, and is prone to hallucinations. To address these challenges, we propose TOBUGraph, a graph-based retrieval framework that first constructs the knowledge graph from unstructured data dynamically and automatically. Using LLMs, TOBUGraph extracts structured knowledge and diverse relationships among data, going beyond RAG's text-to-text similarity. Retrieval is achieved through graph traversal, leveraging the extracted relationships and structures to enhance retrieval accuracy, eliminating the need for chunking configurations while reducing hallucination. We demonstrate TOBUGraph's effectiveness in TOBU, a real-world application in production for personal memory organization and retrieval. Our evaluation using real user data demonstrates that TOBUGraph outperforms multiple RAG implementations in both precision and recall, significantly improving user experience through improved retrieval accuracy.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cognitive Edge Computing: A Comprehensive Survey on Optimizing Large Models and AI Agents for Pervasive Deployment</title>
<link>https://arxiv.org/abs/2501.03265</link>
<guid>https://arxiv.org/abs/2501.03265</guid>
<content:encoded><![CDATA[
arXiv:2501.03265v2 Announce Type: replace-cross 
Abstract: This article surveys Cognitive Edge Computing as a practical and methodical pathway for deploying reasoning-capable Large Language Models (LLMs) and autonomous AI agents on resource-constrained devices at the network edge. We present a unified, cognition-preserving framework spanning: (1) model optimization (quantization, sparsity, low-rank adaptation, distillation) aimed at retaining multi-step reasoning under tight memory/compute budgets; (2) system architecture (on-device inference, elastic offloading, cloud-edge collaboration) that trades off latency, energy, privacy, and capacity; and (3) adaptive intelligence (context compression, dynamic routing, federated personalization) that tailors computation to task difficulty and device constraints. We synthesize advances in efficient Transformer design, multimodal integration, hardware-aware compilation, privacy-preserving learning, and agentic tool use, and map them to edge-specific operating envelopes. We further outline a standardized evaluation protocol covering latency, throughput, energy per token, accuracy, robustness, privacy, and sustainability, with explicit measurement assumptions to enhance comparability. Remaining challenges include modality-aware reasoning benchmarks, transparent and reproducible energy reporting, edge-oriented safety/alignment evaluation, and multi-agent testbeds. We conclude with practitioner guidelines for cross-layer co-design of algorithms, runtime, and hardware to deliver reliable, efficient, and privacy-preserving cognitive capabilities on edge devices.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMDocIR: Benchmarking Multimodal Retrieval for Long Documents</title>
<link>https://arxiv.org/abs/2501.08828</link>
<guid>https://arxiv.org/abs/2501.08828</guid>
<content:encoded><![CDATA[
arXiv:2501.08828v3 Announce Type: replace-cross 
Abstract: Multimodal document retrieval aims to identify and retrieve various forms of multimodal content, such as figures, tables, charts, and layout information from extensive documents. Despite its increasing popularity, there is a notable lack of a comprehensive and robust benchmark to effectively evaluate the performance of systems in such tasks. To address this gap, this work introduces a new benchmark, named MMDocIR, that encompasses two distinct tasks: page-level and layout-level retrieval. The former evaluates the performance of identifying the most relevant pages within a long document, while the later assesses the ability of detecting specific layouts, providing a more fine-grained measure than whole-page analysis. A layout refers to a variety of elements, including textual paragraphs, equations, figures, tables, or charts. The MMDocIR benchmark comprises a rich dataset featuring 1,685 questions annotated by experts and 173,843 questions with bootstrapped labels, making it a valuable resource in multimodal document retrieval for both training and evaluation. Through rigorous experiments, we demonstrate that (i) visual retrievers significantly outperform their text counterparts, (ii) MMDocIR training set effectively enhances the performance of multimodal document retrieval and (iii) text retrievers leveraging VLM-text significantly outperforms retrievers relying on OCR-text. Our dataset is available at https://mmdocrag.github.io/MMDocIR/.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>iTool: Reinforced Fine-Tuning with Dynamic Deficiency Calibration for Advanced Tool Use</title>
<link>https://arxiv.org/abs/2501.09766</link>
<guid>https://arxiv.org/abs/2501.09766</guid>
<content:encoded><![CDATA[
arXiv:2501.09766v5 Announce Type: replace-cross 
Abstract: Augmenting large language models (LLMs) with external tools is a promising approach to enhance their capabilities, especially for complex tasks. Synthesizing tool-use data through real-world simulations is an effective way to achieve this. However, our investigation reveals that training gains significantly decay as synthetic data increases. The model struggles to benefit from additional synthetic data, which fails to endow it with advanced tool-use capabilities in complex scenarios Moreover, we discovered that the above limitation usually manifests as a fragment deficiency (i.e., parameter errors) in response. To this end, we propose an iterative reinforced fine-tuning strategy designed to alleviate this limitation. This strategy involves: (1) enhancing the diversity of response for synthetic data through path exploration of Monte Carlo Tree Search. (2) iteratively pinpointing the model's deficiency by constructing fine-grained preference pairs, and then improving it by preference optimization algorithms for targeted improvement. The experiments show that our method achieves 13.11% better performance than the same-size base model. It achieves an improvement of 6.5% in complex scenarios compared to the baseline, and it also outperforms larger open-source and closed-source models.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Activation-Informed Merging of Large Language Models</title>
<link>https://arxiv.org/abs/2502.02421</link>
<guid>https://arxiv.org/abs/2502.02421</guid>
<content:encoded><![CDATA[
arXiv:2502.02421v3 Announce Type: replace-cross 
Abstract: Model merging, a method that combines the parameters and embeddings of multiple fine-tuned large language models (LLMs), offers a promising approach to enhance model performance across various tasks while maintaining computational efficiency. This paper introduces Activation-Informed Merging (AIM), a technique that integrates the information from the activation space of LLMs into the merging process to improve performance and robustness. AIM is designed as a flexible, complementary solution that is applicable to any existing merging method. It aims to preserve critical weights from the base model, drawing on principles from continual learning (CL) and model compression. Utilizing a task-agnostic calibration set, AIM selectively prioritizes essential weights during merging. We empirically demonstrate that AIM significantly enhances the performance of merged models across multiple benchmarks. Our findings suggest that considering the activation-space information can provide substantial advancements in the model merging strategies for LLMs, with up to a 40% increase in benchmark performance.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Analyzing limits for in-context learning</title>
<link>https://arxiv.org/abs/2502.03503</link>
<guid>https://arxiv.org/abs/2502.03503</guid>
<content:encoded><![CDATA[
arXiv:2502.03503v3 Announce Type: replace-cross 
Abstract: Our paper challenges claims from prior research that transformer-based models, when learning in context, implicitly implement standard learning algorithms. We present empirical evidence inconsistent with this view and provide a mathematical analysis demonstrating that transformers cannot achieve general predictive accuracy due to inherent architectural limitations.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InterFeedback: Unveiling Interactive Intelligence of Large Multimodal Models via Human Feedback</title>
<link>https://arxiv.org/abs/2502.15027</link>
<guid>https://arxiv.org/abs/2502.15027</guid>
<content:encoded><![CDATA[
arXiv:2502.15027v3 Announce Type: replace-cross 
Abstract: Existing benchmarks do not test Large Multimodal Models (LMMs) on their interactive intelligence with human users, which is vital for developing general-purpose AI assistants. We design InterFeedback, an interactive framework, which can be applied to any LMM and dataset to assess this ability autonomously. On top of this, we introduce InterFeedback-Bench which evaluates interactive intelligence using two representative datasets, MMMU-Pro and MathVerse, to test 10 different open-source LMMs. Additionally, we present InterFeedback-Human, a newly collected dataset of 120 cases designed for manually testing interactive performance in leading models such as OpenAI-o1 and Claude-Sonnet-4. Our evaluation results indicate that even the state-of-the-art LMM, OpenAI-o1, struggles to refine its responses based on human feedback, achieving an average score of less than 50%. Our findings point to the need for methods that can enhance LMMs' capabilities to interpret and benefit from feedback.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AIRepr: An Analyst-Inspector Framework for Evaluating Reproducibility of LLMs in Data Science</title>
<link>https://arxiv.org/abs/2502.16395</link>
<guid>https://arxiv.org/abs/2502.16395</guid>
<content:encoded><![CDATA[
arXiv:2502.16395v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are increasingly used to automate data analysis through executable code generation. Yet, data science tasks often admit multiple statistically valid solutions, e.g. different modeling strategies, making it critical to understand the reasoning behind analyses, not just their outcomes. While manual review of LLM-generated code can help ensure statistical soundness, it is labor-intensive and requires expertise. A more scalable approach is to evaluate the underlying workflows-the logical plans guiding code generation. However, it remains unclear how to assess whether an LLM-generated workflow supports reproducible implementations.
  To address this, we present AIRepr, an Analyst-Inspector framework for automatically evaluating and improving the reproducibility of LLM-generated data analysis workflows. Our framework is grounded in statistical principles and supports scalable, automated assessment. We introduce two novel reproducibility-enhancing prompting strategies and benchmark them against standard prompting across 15 analyst-inspector LLM pairs and 1,032 tasks from three public benchmarks. Our findings show that workflows with higher reproducibility also yield more accurate analyses, and that reproducibility-enhancing prompts substantially improve both metrics. This work provides a foundation for transparent, reliable, and efficient human-AI collaboration in data science. Our code is publicly available.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large language models as uncertainty-calibrated optimizers for experimental discovery</title>
<link>https://arxiv.org/abs/2504.06265</link>
<guid>https://arxiv.org/abs/2504.06265</guid>
<content:encoded><![CDATA[
arXiv:2504.06265v3 Announce Type: replace-cross 
Abstract: Scientific discovery increasingly depends on efficient experimental optimization to navigate vast design spaces under time and resource constraints. Traditional approaches often require extensive domain expertise and feature engineering. While large language models, with their vast scientific knowledge, circumvent the feature engineering limitations, they lack the calibrated uncertainty estimates required for high-stakes decision making. Hence, current optimization methods force a choice between domain knowledge and reliability, with no principled approach that affords both. In this work, we show that training language models through the uncertainty-aware objectives of traditional optimization methods enables their use as reliable optimizers guided by natural language. By teaching LLMs from experimental outcomes under uncertainty, we transform their overconfidence from a fundamental limitation into a precise calibration mechanism. Applied to Buchwald-Hartwig reactions, a cornerstone of pharmaceutical synthesis, our method nearly doubles the discovery rate of high-yielding reaction conditions, from 24% to 43% in 50 experimental iterations starting from 10 unsuccessful conditions. Across 19 diverse optimization problems spanning organic synthesis, materials science and catalysis, process chemistry, and molecular design, our approach ranks first on average, establishing a new paradigm for reliable, uncertainty-guided optimization with LLMs. Our approach can accelerate discovery by lowering the barrier to using powerful optimization methods, replacing the need for domain-specific feature engineering with more accessible natural language interfaces. These findings highlight that ensuring reliability through principled uncertainty quantification is critical for realizing the full potential of AI-guided experimentation.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pogobot: an Open-Source, Low-Cost Robot for Swarm Robotics and Programmable Active Matter</title>
<link>https://arxiv.org/abs/2504.08686</link>
<guid>https://arxiv.org/abs/2504.08686</guid>
<content:encoded><![CDATA[
arXiv:2504.08686v2 Announce Type: replace-cross 
Abstract: This paper describes the Pogobot, an open-source platform specifically designed for research at the interface of swarm robotics and active matter. Pogobot features vibration-based or wheel-based locomotion, fast infrared communication, and an array of sensors in a cost-effective package (approx. 250euros/unit). The platform's modular design, comprehensive API, and extensible architecture facilitate the implementation of swarm intelligence algorithms and collective motion. Pogobots offer an accessible alternative to existing platforms while providing advanced capabilities including directional communication between units and fast locomotion, all with a compact form factor. More than 200 Pogobots are already being used on a daily basis in several Universities to study self-organizing systems, programmable active matter, discrete reaction-diffusion-advection systems and computational models of social learning and evolution. This paper details the hardware and software architecture, communication protocols, locomotion mechanisms, and the infrastructure built around the Pogobots.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>XBreaking: Understanding how LLMs security alignment can be broken</title>
<link>https://arxiv.org/abs/2504.21700</link>
<guid>https://arxiv.org/abs/2504.21700</guid>
<content:encoded><![CDATA[
arXiv:2504.21700v3 Announce Type: replace-cross 
Abstract: Large Language Models are fundamental actors in the modern IT landscape dominated by AI solutions. However, security threats associated with them might prevent their reliable adoption in critical application scenarios such as government organizations and medical institutions. For this reason, commercial LLMs typically undergo a sophisticated censoring mechanism to eliminate any harmful output they could possibly produce. These mechanisms maintain the integrity of LLM alignment by guaranteeing that the models respond safely and ethically. In response to this, attacks on LLMs are a significant threat to such protections, and many previous approaches have already demonstrated their effectiveness across diverse domains. Existing LLM attacks mostly adopt a generate-and-test strategy to craft malicious input. To improve the comprehension of censoring mechanisms and design a targeted attack, we propose an Explainable-AI solution that comparatively analyzes the behavior of censored and uncensored models to derive unique exploitable alignment patterns. Then, we propose XBreaking, a novel approach that exploits these unique patterns to break the security and alignment constraints of LLMs by targeted noise injection. Our thorough experimental campaign returns important insights about the censoring mechanisms and demonstrates the effectiveness and performance of our approach.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Taskmaster Deconstructed: A Quantitative Look at Tension, Volatility, and Viewer Ratings</title>
<link>https://arxiv.org/abs/2505.02886</link>
<guid>https://arxiv.org/abs/2505.02886</guid>
<content:encoded><![CDATA[
arXiv:2505.02886v2 Announce Type: replace-cross 
Abstract: Taskmaster is a British television show that combines comedic performance with a formal scoring system. Despite the appearance of structured competition, it remains unclear whether scoring dynamics contribute meaningfully to audience engagement. We conducted a statistical analysis of 162 episodes across 18 series, using fifteen episode-level metrics to quantify rank volatility, point spread, lead changes, and winner dominance. None of these metrics showed a significant association with IMDb ratings, even after controlling for series effects. Long-term trends suggest that average points have increased over time, while volatility has slightly declined and rank spread has remained stable. These patterns indicate an attempt to enhance competitive visibility without altering the show's structural equilibrium. We also analyzed contestant rank trajectories and identified five recurring archetypes describing performance styles. These patterns suggest that viewer interest is shaped more by contestant behavior than by game mechanics.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Anytime Reasoning via Budget Relative Policy Optimization</title>
<link>https://arxiv.org/abs/2505.13438</link>
<guid>https://arxiv.org/abs/2505.13438</guid>
<content:encoded><![CDATA[
arXiv:2505.13438v3 Announce Type: replace-cross 
Abstract: Scaling test-time compute is crucial for enhancing the reasoning capabilities of large language models (LLMs). Existing approaches typically employ reinforcement learning (RL) to maximize a verifiable reward obtained at the end of reasoning traces. However, such methods optimize only the final performance under a large and fixed token budget, which hinders efficiency in both training and deployment. In this work, we present a novel framework, AnytimeReasoner, to optimize anytime reasoning performance, which aims to improve token efficiency and the flexibility of reasoning under varying token budget constraints. To achieve this, we truncate the complete thinking process to fit within sampled token budgets from a prior distribution, compelling the model to summarize the optimal answer for each truncated thinking for verification. This introduces verifiable dense rewards into the reasoning process, facilitating more effective credit assignment in RL optimization. We then optimize the thinking and summary policies in a decoupled manner to maximize the cumulative reward. Additionally, we introduce a novel variance reduction technique, Budget Relative Policy Optimization (BRPO), to enhance the robustness and efficiency of the learning process when reinforcing the thinking policy. Empirical results in mathematical reasoning tasks demonstrate that our method consistently outperforms GRPO across all thinking budgets under various prior distributions, enhancing both training and token efficiency.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RCCDA: Adaptive Model Updates in the Presence of Concept Drift under a Constrained Resource Budget</title>
<link>https://arxiv.org/abs/2505.24149</link>
<guid>https://arxiv.org/abs/2505.24149</guid>
<content:encoded><![CDATA[
arXiv:2505.24149v2 Announce Type: replace-cross 
Abstract: Machine learning (ML) algorithms deployed in real-world environments are often faced with the challenge of adapting models to concept drift, where the task data distributions are shifting over time. The problem becomes even more difficult when model performance must be maintained under adherence to strict resource constraints. Existing solutions often depend on drift-detection methods that produce high computational overhead for resource-constrained environments, and fail to provide strict guarantees on resource usage or theoretical performance assurances. To address these shortcomings, we propose RCCDA: a dynamic model update policy that optimizes ML training dynamics while ensuring compliance to predefined resource constraints, utilizing only past loss information and a tunable drift threshold. In developing our policy, we analytically characterize the evolution of model loss under concept drift with arbitrary training update decisions. Integrating these results into a Lyapunov drift-plus-penalty framework produces a lightweight greedy-optimal policy that provably limits update frequency and cost. Experimental results on four domain generalization datasets demonstrate that our policy outperforms baseline methods in inference accuracy while adhering to strict resource constraints under several schedules of concept drift, making our solution uniquely suited for real-time ML deployments.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning of Population Dynamics: Inverse Optimization Meets JKO Scheme</title>
<link>https://arxiv.org/abs/2506.01502</link>
<guid>https://arxiv.org/abs/2506.01502</guid>
<content:encoded><![CDATA[
arXiv:2506.01502v2 Announce Type: replace-cross 
Abstract: Learning population dynamics involves recovering the underlying process that governs particle evolution, given evolutionary snapshots of samples at discrete time points. Recent methods frame this as an energy minimization problem in probability space and leverage the celebrated JKO scheme for efficient time discretization. In this work, we introduce $\texttt{iJKOnet}$, an approach that combines the JKO framework with inverse optimization techniques to learn population dynamics. Our method relies on a conventional $\textit{end-to-end}$ adversarial training procedure and does not require restrictive architectural choices, e.g., input-convex neural networks. We establish theoretical guarantees for our methodology and demonstrate improved performance over prior JKO-based methods.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProRefine: Inference-Time Prompt Refinement with Textual Feedback</title>
<link>https://arxiv.org/abs/2506.05305</link>
<guid>https://arxiv.org/abs/2506.05305</guid>
<content:encoded><![CDATA[
arXiv:2506.05305v3 Announce Type: replace-cross 
Abstract: Agentic workflows, where multiple AI agents collaborate to accomplish complex tasks like reasoning or planning, play a substantial role in many cutting-edge commercial applications, and continue to fascinate researchers across fields for their potential to accomplish expensive, complex tasks that, until recently, only humans have been trusted to do. These workflows depend critically on the prompts used to provide the roles models play in such workflows. Poorly designed prompts that fail even slightly to guide individual agents can lead to sub-optimal performance that may snowball within a system of agents, limiting their reliability and scalability. To address this important problem of inference-time prompt optimization, we introduce ProRefine, an innovative inference-time optimization method that uses an agentic loop of LLMs to generate and apply textual feedback. ProRefine dynamically refines prompts for multi-step reasoning tasks without additional training or ground truth labels. Evaluated on five benchmark mathematical reasoning datasets, ProRefine significantly surpasses zero-shot Chain-of-Thought baselines by 3 to 37 percentage points. This approach not only boosts accuracy but also allows smaller models to approach the performance of their larger counterparts. This highlights its potential for building more cost-effective and powerful hybrid AI systems, thereby democratizing access to high-performing AI.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conformal Prediction Adaptive to Unknown Subpopulation Shifts</title>
<link>https://arxiv.org/abs/2506.05583</link>
<guid>https://arxiv.org/abs/2506.05583</guid>
<content:encoded><![CDATA[
arXiv:2506.05583v2 Announce Type: replace-cross 
Abstract: Conformal prediction is widely used to equip black-box machine learning models with uncertainty quantification, offering formal coverage guarantees under exchangeable data. However, these guarantees fail when faced with subpopulation shifts, where the test environment contains a different mix of subpopulations than the calibration data. In this work, we focus on unknown subpopulation shifts where we are not given group-information i.e. the subpopulation labels of datapoints have to be inferred. We propose new methods that provably adapt conformal prediction to such shifts, ensuring valid coverage without explicit knowledge of subpopulation structure. While existing methods in similar setups assume perfect subpopulation labels, our framework explicitly relaxes this requirement and characterizes conditions where formal coverage guarantees remain feasible. Further, our algorithms scale to high-dimensional settings and remain practical in realistic machine learning tasks. Extensive experiments on vision (with vision transformers) and language (with large language models) benchmarks demonstrate that our methods reliably maintain coverage and effectively control risks in scenarios where standard conformal prediction fails.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Know What You Don't Know: Uncertainty Calibration of Process Reward Models</title>
<link>https://arxiv.org/abs/2506.09338</link>
<guid>https://arxiv.org/abs/2506.09338</guid>
<content:encoded><![CDATA[
arXiv:2506.09338v2 Announce Type: replace-cross 
Abstract: Process reward models (PRMs) play a central role in guiding inference-time scaling algorithms for large language models (LLMs). However, we observe that even state-of-the-art PRMs can be poorly calibrated. Specifically, they tend to overestimate the success probability that a partial reasoning step will lead to a correct final answer, particularly when smaller LLMs are used to complete the reasoning trajectory. To address this, we present a calibration approach -- performed via quantile regression -- that adjusts PRM outputs to better align with true success probabilities. Leveraging these calibrated success estimates and their associated confidence bounds, we introduce an \emph{instance-adaptive scaling} (IAS) framework that dynamically adjusts the compute budget based on the estimated likelihood that a partial reasoning trajectory will yield a correct final answer. Unlike conventional methods that allocate a fixed number of reasoning trajectories per query, this approach adapts to each instance and reasoning step when using our calibrated PRMs. Experiments on mathematical reasoning benchmarks show that (i) our PRM calibration method achieves small calibration error, outperforming the baseline methods, (ii) calibration is crucial for enabling effective IAS, and (iii) the proposed IAS strategy reduces inference costs while maintaining final answer accuracy, utilizing less compute on more confident problems as desired.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Progressive Inference-Time Annealing of Diffusion Models for Sampling from Boltzmann Densities</title>
<link>https://arxiv.org/abs/2506.16471</link>
<guid>https://arxiv.org/abs/2506.16471</guid>
<content:encoded><![CDATA[
arXiv:2506.16471v2 Announce Type: replace-cross 
Abstract: Sampling efficiently from a target unnormalized probability density remains a core challenge, with relevance across countless high-impact scientific applications. A promising approach towards this challenge is the design of amortized samplers that borrow key ideas, such as probability path design, from state-of-the-art generative diffusion models. However, all existing diffusion-based samplers remain unable to draw samples from distributions at the scale of even simple molecular systems. In this paper, we propose Progressive Inference-Time Annealing (PITA), a novel framework to learn diffusion-based samplers that combines two complementary interpolation techniques: I.) Annealing of the Boltzmann distribution and II.) Diffusion smoothing. PITA trains a sequence of diffusion models from high to low temperatures by sequentially training each model at progressively higher temperatures, leveraging engineered easy access to samples of the temperature-annealed target density. In the subsequent step, PITA enables simulating the trained diffusion model to procure training samples at a lower temperature for the next diffusion model through inference-time annealing using a novel Feynman-Kac PDE combined with Sequential Monte Carlo. Empirically, PITA enables, for the first time, equilibrium sampling of N-body particle systems, Alanine Dipeptide, and tripeptides in Cartesian coordinates with dramatically lower energy function evaluations. Code available at: https://github.com/taraak/pita
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Less Greedy Equivalence Search</title>
<link>https://arxiv.org/abs/2506.22331</link>
<guid>https://arxiv.org/abs/2506.22331</guid>
<content:encoded><![CDATA[
arXiv:2506.22331v2 Announce Type: replace-cross 
Abstract: Greedy Equivalence Search (GES) is a classic score-based algorithm for causal discovery from observational data. In the sample limit, it recovers the Markov equivalence class of graphs that describe the data. Still, it faces two challenges in practice: computational cost and finite-sample accuracy. In this paper, we develop Less Greedy Equivalence Search (LGES), a variant of GES that retains its theoretical guarantees while partially addressing these limitations. LGES modifies the greedy step; rather than always applying the highest-scoring insertion, it avoids edge insertions between variables for which the score implies some conditional independence. This more targeted search yields up to a \(10\)-fold speed-up and a substantial reduction in structural error relative to GES. Moreover, LGES can guide the search using prior knowledge, and can correct this knowledge when contradicted by data. Finally, LGES can use interventional data to refine the learned observational equivalence class. We prove that LGES recovers the true equivalence class in the sample limit, even with misspecified knowledge. Experiments demonstrate that LGES outperforms GES and other baselines in speed, accuracy, and robustness to misspecified knowledge. Our code is available at https://github.com/CausalAILab/lges.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conformal Information Pursuit for Interactively Guiding Large Language Models</title>
<link>https://arxiv.org/abs/2507.03279</link>
<guid>https://arxiv.org/abs/2507.03279</guid>
<content:encoded><![CDATA[
arXiv:2507.03279v2 Announce Type: replace-cross 
Abstract: A significant use case of instruction-finetuned Large Language Models (LLMs) is to solve question-answering tasks interactively. In this setting, an LLM agent is tasked with making a prediction by sequentially querying relevant information from the user, as opposed to a single-turn conversation. This paper explores sequential querying strategies that aim to minimize the expected number of queries. One such strategy is Information Pursuit (IP), a greedy algorithm that at each iteration selects the query that maximizes information gain or equivalently minimizes uncertainty. However, obtaining accurate estimates of mutual information or conditional entropy for LLMs is very difficult in practice due to over- or under-confident LLM proba- bilities, which leads to suboptimal query selection and predictive performance. To better estimate the uncertainty at each iteration, we propose Conformal Information Pursuit (C-IP), an alternative approach to sequential information gain based on conformal prediction sets. More specifically, C-IP leverages a relationship between prediction sets and conditional entropy at each iteration to estimate uncertainty based on the average size of conformal prediction sets. In contrast to conditional entropy, we find that conformal prediction sets are a distribution-free and robust method of measuring uncertainty. Experiments with 20 Questions show that C-IP obtains better predictive performance and shorter query-answer chains compared to previous approaches to IP and uncertainty-based chain-of-thought methods. Furthermore, extending to an interactive medical setting between a doctor and a patient on the MediQ dataset, C-IP achieves competitive performance with direct single-turn prediction while offering greater interpretability.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ZERO: Industry-ready Vision Foundation Model with Multi-modal Prompts</title>
<link>https://arxiv.org/abs/2507.04270</link>
<guid>https://arxiv.org/abs/2507.04270</guid>
<content:encoded><![CDATA[
arXiv:2507.04270v4 Announce Type: replace-cross 
Abstract: Foundation models have revolutionized AI, yet they struggle with zero-shot deployment in real-world industrial settings due to a lack of high-quality, domain-specific datasets. To bridge this gap, Superb AI introduces ZERO, an industry-ready vision foundation model that leverages multi-modal prompting (textual and visual) for generalization without retraining. Trained on a compact yet representative 0.9 million annotated samples from a proprietary billion-scale industrial dataset, ZERO demonstrates competitive performance on academic benchmarks like LVIS-Val and significantly outperforms existing models across 37 diverse industrial datasets. Furthermore, ZERO achieved 2nd place in the CVPR 2025 Object Instance Detection Challenge and 4th place in the Foundational Few-shot Object Detection Challenge, highlighting its practical deployability and generalizability with minimal adaptation and limited data. To the best of our knowledge, ZERO is the first vision foundation model explicitly built for domain-specific, zero-shot industrial applications.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Learning</title>
<link>https://arxiv.org/abs/2507.05636</link>
<guid>https://arxiv.org/abs/2507.05636</guid>
<content:encoded><![CDATA[
arXiv:2507.05636v2 Announce Type: replace-cross 
Abstract: Graph learning has rapidly evolved into a critical subfield of machine learning and artificial intelligence (AI). Its development began with early graph-theoretic methods, gaining significant momentum with the advent of graph neural networks (GNNs). Over the past decade, progress in scalable architectures, dynamic graph modeling, multimodal learning, generative AI, explainable AI (XAI), and responsible AI has broadened the applicability of graph learning to various challenging environments. Graph learning is significant due to its ability to model complex, non-Euclidean relationships that traditional machine learning struggles to capture, thus better supporting real-world applications ranging from drug discovery and fraud detection to recommender systems and scientific reasoning. However, challenges like scalability, generalization, heterogeneity, interpretability, and trustworthiness must be addressed to unlock its full potential. This survey provides a comprehensive introduction to graph learning, focusing on key dimensions including scalable, temporal, multimodal, generative, explainable, and responsible graph learning. We review state-of-the-art techniques for efficiently handling large-scale graphs, capturing dynamic temporal dependencies, integrating heterogeneous data modalities, generating novel graph samples, and enhancing interpretability to foster trust and transparency. We also explore ethical considerations, such as privacy and fairness, to ensure responsible deployment of graph learning models. Additionally, we identify and discuss emerging topics, highlighting recent integration of graph learning and other AI paradigms and offering insights into future directions. This survey serves as a valuable resource for researchers and practitioners seeking to navigate the rapidly evolving landscape of graph learning.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance</title>
<link>https://arxiv.org/abs/2507.09601</link>
<guid>https://arxiv.org/abs/2507.09601</guid>
<content:encoded><![CDATA[
arXiv:2507.09601v2 Announce Type: replace-cross 
Abstract: General-purpose sentence embedding models often struggle to capture specialized financial semantics, especially in low-resource languages like Korean, due to domain-specific jargon, temporal meaning shifts, and misaligned bilingual vocabularies. To address these gaps, we introduce NMIXX (Neural eMbeddings for Cross-lingual eXploration of Finance), a suite of cross-lingual embedding models fine-tuned with 18.8K high-confidence triplets that pair in-domain paraphrases, hard negatives derived from a semantic-shift typology, and exact Korean-English translations. Concurrently, we release KorFinSTS, a 1,921-pair Korean financial STS benchmark spanning news, disclosures, research reports, and regulations, designed to expose nuances that general benchmarks miss.
  When evaluated against seven open-license baselines, NMIXX's multilingual bge-m3 variant achieves Spearman's rho gains of +0.10 on English FinSTS and +0.22 on KorFinSTS, outperforming its pre-adaptation checkpoint and surpassing other models by the largest margin, while revealing a modest trade-off in general STS performance. Our analysis further shows that models with richer Korean token coverage adapt more effectively, underscoring the importance of tokenizer design in low-resource, cross-lingual settings. By making both models and the benchmark publicly available, we provide the community with robust tools for domain-adapted, multilingual representation learning in finance.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>To Trust or Not to Trust: On Calibration in ML-based Resource Allocation for Wireless Networks</title>
<link>https://arxiv.org/abs/2507.17494</link>
<guid>https://arxiv.org/abs/2507.17494</guid>
<content:encoded><![CDATA[
arXiv:2507.17494v2 Announce Type: replace-cross 
Abstract: In next-generation communications and networks, machine learning (ML) models are expected to deliver not only accurate predictions but also well-calibrated confidence scores that reflect the true likelihood of correct decisions. This paper studies the calibration performance of an ML-based outage predictor within a single-user, multi-resource allocation framework. We first establish key theoretical properties of this system's outage probability (OP) under perfect calibration. Importantly, we show that as the number of resources grows, the OP of a perfectly calibrated predictor approaches the expected output conditioned on it being below the classification threshold. In contrast, when only one resource is available, the system's OP equals the model's overall expected output. We then derive the OP conditions for a perfectly calibrated predictor. These findings guide the choice of the classification threshold to achieve a desired OP, helping system designers meet specific reliability requirements. We also demonstrate that post-processing calibration cannot improve the system's minimum achievable OP, as it does not introduce new information about future channel states. Additionally, we show that well-calibrated models are part of a broader class of predictors that necessarily improve OP. In particular, we establish a monotonicity condition that the accuracy-confidence function must satisfy for such improvement to occur. To demonstrate these theoretical properties, we conduct a rigorous simulation-based analysis using post-processing calibration techniques: Platt scaling and isotonic regression. As part of this framework, the predictor is trained using an outage loss function specifically designed for this system. Furthermore, this analysis is performed on Rayleigh fading channels with temporal correlation captured by Clarke's 2D model, which accounts for receiver mobility.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GAITEX: Human motion dataset of impaired gait and rehabilitation exercises using inertial and optical sensors</title>
<link>https://arxiv.org/abs/2507.21069</link>
<guid>https://arxiv.org/abs/2507.21069</guid>
<content:encoded><![CDATA[
arXiv:2507.21069v2 Announce Type: replace-cross 
Abstract: Wearable inertial measurement units (IMUs) provide a cost-effective approach to assessing human movement in clinical and everyday environments. However, developing the associated classification models for robust assessment of physiotherapeutic exercise and gait analysis requires large, diverse datasets that are costly and time-consuming to collect. We present a multimodal dataset of physiotherapeutic and gait-related exercises, including correct and clinically relevant variants, recorded from 19 healthy subjects using synchronized IMUs and optical marker-based motion capture (MoCap). It contains data from nine IMUs and 68 markers tracking full-body kinematics. Four markers per IMU allow direct comparison between IMU- and MoCap-derived orientations. We additionally provide processed IMU orientations aligned to common segment coordinate systems, subject-specific OpenSim models, inverse kinematics outputs, and visualization tools for IMU-derived orientations. The dataset is fully annotated with movement quality ratings and timestamped segmentations. It supports various machine learning tasks such as exercise evaluation, gait classification, temporal segmentation, and biomechanical parameter estimation. Code for postprocessing, alignment, inverse kinematics, and technical validation is provided to promote reproducibility.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Dynamics of Meta-Learning in Small Model Pretraining</title>
<link>https://arxiv.org/abs/2508.02189</link>
<guid>https://arxiv.org/abs/2508.02189</guid>
<content:encoded><![CDATA[
arXiv:2508.02189v2 Announce Type: replace-cross 
Abstract: Large language models are powerful but costly. We ask whether meta-learning can make the pretraining of small language models not only better but also more interpretable. We integrate first-order MAML with subset-masked LM pretraining, producing four LLama-style decoder-only models (11M-570M params), and evaluate it on a fundamental NLP task with many settings and real-world applications. Compared with vanilla training, our model (i) reaches the same loss up to 1.6x sooner, (ii) improves F1 on multilingual Universal NER under equal compute, and (iii) makes the training dynamics easy to read: first the network's representations fan out ("diversify") and later they collapse into a smaller, shared subspace ("compress"). This two-stage shift shows up as a rise-and-fall in both effective-rank curves and attention-head entropy. The same curves pinpoint which layers specialise earliest and which later reconverge, giving a compact, interpretable signature of meta-adaptation. Code, checkpoints and WandB logs are released.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advanced Hybrid Transformer LSTM Technique with Attention and TS Mixer for Drilling Rate of Penetration Prediction</title>
<link>https://arxiv.org/abs/2508.05210</link>
<guid>https://arxiv.org/abs/2508.05210</guid>
<content:encoded><![CDATA[
arXiv:2508.05210v3 Announce Type: replace-cross 
Abstract: Rate of Penetration (ROP) prediction is critical for drilling optimization yet remains challenging due to the nonlinear, dynamic, and heterogeneous characteristics of drilling data. Conventional empirical, physics-based, and standard machine learning models rely on oversimplified assumptions or intensive feature engineering, constraining their capacity to model long-term dependencies and intricate feature interactions. To address these issues, this study presents a new deep learning Hybrid LSTM-Trans-Mixer-Att framework that first processes input data through a customized Long Short-Term Memory (LSTM) network to capture multi-scale temporal dependencies aligned with drilling cycles. Subsequently, an Enhanced Transformer encoder with drilling-specific positional encodings and real-time optimization refines the features. Concurrently, a parallel Time-Series Mixer (TS-Mixer) block introduced facilitates efficient cross-feature interaction modeling of static and categorical parameters, including lithological indices and mud properties. The feature representations extracted from the Enhanced Transformer and TS-Mixer modules are integrated through a dedicated fusion layer. Finally, an adaptive attention mechanism then dynamically assigns contextual weights to salient features, enhancing discriminative representation learning and enabling high-fidelity ROP prediction. The proposed framework combines sequential memory, static feature interactions, global context learning, and dynamic feature weighting, providing a comprehensive solution for the heterogeneous and event-driven nature of drilling dynamics. Experimental validation on real-world drilling datasets demonstrates superior performance, achieving an Rsquare of 0.9991 and a MAPE of 1.447%, significantly outperforming existing baseline and hybrid models.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ethics-Aware Safe Reinforcement Learning for Rare-Event Risk Control in Interactive Urban Driving</title>
<link>https://arxiv.org/abs/2508.14926</link>
<guid>https://arxiv.org/abs/2508.14926</guid>
<content:encoded><![CDATA[
arXiv:2508.14926v3 Announce Type: replace-cross 
Abstract: Autonomous vehicles hold great promise for reducing traffic fatalities and improving transportation efficiency, yet their widespread adoption hinges on embedding credible and transparent ethical reasoning into routine and emergency maneuvers, particularly to protect vulnerable road users (VRUs) such as pedestrians and cyclists. Here, we present a hierarchical Safe Reinforcement Learning (Safe RL) framework that augments standard driving objectives with ethics-aware cost signals. At the decision level, a Safe RL agent is trained using a composite ethical risk cost, combining collision probability and harm severity, to generate high-level motion targets. A dynamic, risk-sensitive Prioritized Experience Replay mechanism amplifies learning from rare but critical, high-risk events. At the execution level, polynomial path planning coupled with Proportional-Integral-Derivative (PID) and Stanley controllers translates these targets into smooth, feasible trajectories, ensuring both accuracy and comfort. We train and validate our approach on closed-loop simulation environments derived from large-scale, real-world traffic datasets encompassing diverse vehicles, cyclists, and pedestrians, and demonstrate that it outperforms baseline methods in reducing risk to others while maintaining ego performance and comfort. This work provides a reproducible benchmark for Safe RL with explicitly ethics-aware objectives in human-mixed traffic scenarios. Our results highlight the potential of combining formal control theory and data-driven learning to advance ethically accountable autonomy that explicitly protects those most at risk in urban traffic environments. Across two interactive benchmarks and five random seeds, our policy decreases conflict frequency by 25-45% compared to matched task successes while maintaining comfort metrics within 5%.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chain-of-Query: Unleashing the Power of LLMs in SQL-Aided Table Understanding via Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2508.15809</link>
<guid>https://arxiv.org/abs/2508.15809</guid>
<content:encoded><![CDATA[
arXiv:2508.15809v2 Announce Type: replace-cross 
Abstract: Table understanding requires structured, multi-step reasoning. Large Language Models (LLMs) struggle with it due to the structural complexity of tabular data. Recently, multi-agent frameworks for SQL generation have shown promise in tackling the challenges of understanding tabular data, but existing approaches often suffer from limitations such as the inability to comprehend table structure for reliable SQL generation, error propagation that results in invalid queries, and over-reliance on execution correctness. To address these issues, we propose Chain-of-Query (CoQ), a novel multi-agent framework for SQL-aided table understanding. CoQ adopts natural-language-style representations of table schemas to abstract away structural noise and enhance understanding. It employs a clause-by-clause SQL generation strategy to improve query quality and introduces a hybrid reasoning division that separates SQL-based mechanical reasoning from LLM-based logical inference, thereby reducing reliance on execution outcomes. Extensive experiments across four models and five widely used benchmarks demonstrate that CoQ achieves substantial accuracy improvements and significantly lowers invalid SQL rates compared to prior generic LLM-based, SQL-aided, and hybrid baselines, confirming its superior effectiveness in table understanding. The code is available at https://github.com/SongyuanSui/ChainofQuery.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Matters in Data for DPO?</title>
<link>https://arxiv.org/abs/2508.18312</link>
<guid>https://arxiv.org/abs/2508.18312</guid>
<content:encoded><![CDATA[
arXiv:2508.18312v3 Announce Type: replace-cross 
Abstract: Direct Preference Optimization (DPO) has emerged as a simple and effective approach for aligning large language models (LLMs) with human preferences, bypassing the need for a learned reward model. Despite its growing adoption, a fundamental question remains open: what characteristics of preference data are most critical for DPO performance? In this work, we provide a systematic study of how preference data distribution influences DPO, from both theoretical and empirical perspectives. We show that the quality of chosen responses plays a dominant role in optimizing the DPO objective, while the quality of rejected responses may have relatively limited impact. Our theoretical analysis characterizes the optimal response distribution under DPO and reveals how contrastiveness between responses helps primarily by improving the chosen samples. We further study an online DPO setting and show it effectively reduces to supervised fine-tuning on the chosen responses. Extensive experiments across diverse tasks confirm our findings: improving the quality of chosen responses consistently boosts performance regardless of the quality of the rejected responses. We also investigate the benefit of mixing the on-policy data. Our results interpret the mechanism behind some widely adopted strategies and offer practical insights for constructing high-impact preference datasets for LLM alignment.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Agentic Vulnerability Injection And Transformation with Optimized Reasoning</title>
<link>https://arxiv.org/abs/2508.20866</link>
<guid>https://arxiv.org/abs/2508.20866</guid>
<content:encoded><![CDATA[
arXiv:2508.20866v3 Announce Type: replace-cross 
Abstract: The increasing complexity of software systems and the sophistication of cyber-attacks have underscored the critical need for effective automated vulnerability detection and repair systems. Data-driven approaches using deep learning models show promise but critically depend on the availability of large, accurately labeled datasets. Yet existing datasets either suffer from noisy labels, limited range of vulnerabilities, or fail to reflect vulnerabilities as they occur in real-world software. This also limits large-scale benchmarking of such solutions. Automated vulnerability injection provides a way to directly address these dataset limitations, but existing techniques remain limited in coverage, contextual fidelity, or injection success rates. In this paper, we present AVIATOR, the first AI-agentic vulnerability injection workflow. It automatically injects realistic, category-specific vulnerabilities for high-fidelity, diverse, large-scale vulnerability dataset generation. Unlike prior monolithic approaches, AVIATOR orchestrates specialized AI agents, function agents and traditional code analysis tools that replicate expert reasoning. It combines semantic analysis, injection synthesis enhanced with LoRA-based fine-tuning and Retrieval-Augmented Generation, as well as post-injection validation via static analysis and LLM-based discriminators. This modular decomposition allows specialized agents to focus on distinct tasks, improving robustness of injection and reducing error propagation across the workflow. Evaluations across three distinct benchmarks demonstrate that AVIATOR achieves 91%-95% injection success rates, significantly surpassing existing automated dataset generation techniques in both accuracy and scope of software vulnerabilities.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TimeCopilot</title>
<link>https://arxiv.org/abs/2509.00616</link>
<guid>https://arxiv.org/abs/2509.00616</guid>
<content:encoded><![CDATA[
arXiv:2509.00616v3 Announce Type: replace-cross 
Abstract: We introduce TimeCopilot, the first open-source agentic framework for forecasting that combines multiple Time Series Foundation Models (TSFMs) with Large Language Models (LLMs) through a single unified API. TimeCopilot automates the forecasting pipeline: feature analysis, model selection, cross-validation, and forecast generation, while providing natural language explanations and supporting direct queries about the future. The framework is LLM-agnostic, compatible with both commercial and open-source models, and supports ensembles across diverse forecasting families. Results on the large-scale GIFT-Eval benchmark show that TimeCopilot achieves state-of-the-art probabilistic forecasting performance at low cost. Our framework provides a practical foundation for reproducible, explainable, and accessible agentic forecasting systems.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LimiX: Unleashing Structured-Data Modeling Capability for Generalist Intelligence</title>
<link>https://arxiv.org/abs/2509.03505</link>
<guid>https://arxiv.org/abs/2509.03505</guid>
<content:encoded><![CDATA[
arXiv:2509.03505v2 Announce Type: replace-cross 
Abstract: We argue that progress toward general intelligence requires complementary foundation models grounded in language, the physical world, and structured data. This report presents LimiX-16M and LimiX-2M, two instantiations of our large structured-data models (LDMs). Both models treat structured data as a joint distribution over variables and missingness, thus capable of addressing a wide range of tabular tasks through query-based conditional prediction via a single model. They are pretrained using masked joint-distribution modeling with an episodic, context-conditional objective, supporting rapid, training-free adaptation at inference. We evaluate LimiX models across 11 large structured-data benchmarks with broad regimes of sample size, feature dimensionality, class number, categorical-to-numerical feature ratio, missingness, and sample-to-feature ratios. LimiX-16M consistently surpasses strong baselines, as shown in Figure 1 and Figure 2. The superiority holds across a wide range of tasks, such as classification, regression, missing value imputation, and data generation, often by substantial margins, while avoiding task-specific architectures or bespoke training per task. Notably, LimiX-2M delivers strong results under tight compute and memory budgets. We also present the first scaling law study for LDMs, revealing how data and model scaling jointly influence downstream performance and offering quantitative guidance for tabular foundation modeling. All LimiX models are publicly accessible under Apache 2.0.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Amulet: a Python Library for Assessing Interactions Among ML Defenses and Risks</title>
<link>https://arxiv.org/abs/2509.12386</link>
<guid>https://arxiv.org/abs/2509.12386</guid>
<content:encoded><![CDATA[
arXiv:2509.12386v2 Announce Type: replace-cross 
Abstract: Machine learning (ML) models are susceptible to various risks to security, privacy, and fairness. Most defenses are designed to protect against each risk individually (intended interactions) but can inadvertently affect susceptibility to other unrelated risks (unintended interactions). We introduce Amulet, the first Python library for evaluating both intended and unintended interactions among ML defenses and risks. Amulet is comprehensive by including representative attacks, defenses, and metrics; extensible to new modules due to its modular design; consistent with a user-friendly API template for inputs and outputs; and applicable for evaluating novel interactions. By satisfying all four properties, Amulet offers a unified foundation for studying how defenses interact, enabling the first systematic evaluation of unintended interactions across multiple risks.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Amortized Latent Steering: Low-Cost Alternative to Test-Time Optimization</title>
<link>https://arxiv.org/abs/2509.18116</link>
<guid>https://arxiv.org/abs/2509.18116</guid>
<content:encoded><![CDATA[
arXiv:2509.18116v2 Announce Type: replace-cross 
Abstract: Test-time optimization remains impractical at scale due to prohibitive inference costs--techniques like iterative refinement and multi-step verification can require $10-100\times$ more compute per query than standard decoding. Latent space test-time optimization methods like LatentSeek offer a more direct approach by steering hidden representations, but still demand expensive per-query optimization loops with multiple backward passes. We propose Amortized Latent Steering (ALS), which collapses this iterative optimization into a single offline-computed vector applied at constant cost during inference. ALS computes the mean difference between hidden states from successful versus unsuccessful generations, then uses this direction to calibrate the model's hidden representations: when decoding drifts away from the success manifold, ALS nudges activations back toward it. Across GSM8K and MATH-500 benchmarks, ALS achieves $2-5\times$ speedup over iterative methods while matching or surpassing greedy Chain-of-Thought (CoT) and Self-Consistency baselines, yielding up to 101% improvement in efficiency--accuracy trade-off. These results show that much of latent optimization's benefit can be captured offline, making sophisticated reasoning techniques viable for production deployment. Code is available at https://github.com/negbuna/ALS.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Mega-Study of Digital Twins Reveals Strengths, Weaknesses and Opportunities for Further Improvement</title>
<link>https://arxiv.org/abs/2509.19088</link>
<guid>https://arxiv.org/abs/2509.19088</guid>
<content:encoded><![CDATA[
arXiv:2509.19088v3 Announce Type: replace-cross 
Abstract: Digital representations of individuals ("digital twins") promise to transform social science and decision-making. Yet it remains unclear whether such twins truly mirror the people they emulate. We conducted 19 preregistered studies with a representative U.S. panel and their digital twins, each constructed from rich individual-level data, enabling direct comparisons between human and twin behavior across a wide range of domains and stimuli (including never-seen-before ones). Twins reproduced individual responses with 75% accuracy and seemingly low correlation with human answers (approximately 0.2). However, this apparently high accuracy was no higher than that achieved by generic personas based on demographics only. In contrast, correlation improved when twins incorporated detailed personal information, even outperforming traditional machine learning benchmarks that require additional data. Twins exhibited systematic strengths and weaknesses - performing better in social and personality domains, but worse in political ones - and were more accurate for participants with higher education, higher income, and moderate political views and religious attendance. Together, these findings delineate both the promise and the current limits of digital twins: they capture some relative differences among individuals but not yet the unique judgments of specific people. All data and code are publicly available to support the further development and evaluation of digital twin pipelines.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Policy-as-Prompt: Turning AI Governance Rules into Guardrails for AI Agents</title>
<link>https://arxiv.org/abs/2509.23994</link>
<guid>https://arxiv.org/abs/2509.23994</guid>
<content:encoded><![CDATA[
arXiv:2509.23994v2 Announce Type: replace-cross 
Abstract: As autonomous AI agents are used in regulated and safety-critical settings, organizations need effective ways to turn policy into enforceable controls. We introduce a regulatory machine learning framework that converts unstructured design artifacts (like PRDs, TDDs, and code) into verifiable runtime guardrails. Our Policy as Prompt method reads these documents and risk controls to build a source-linked policy tree. This tree is then compiled into lightweight, prompt-based classifiers for real-time runtime monitoring. The system is built to enforce least privilege and data minimization. For conformity assessment, it provides complete provenance, traceability, and audit logging, all integrated with a human-in-the-loop review process. Evaluations show our system reduces prompt-injection risk, blocks out-of-scope requests, and limits toxic outputs. It also generates auditable rationales aligned with AI governance frameworks. By treating policies as executable prompts (a policy-as-code for agents), this approach enables secure-by-design deployment, continuous compliance, and scalable AI safety and AI security assurance for regulatable ML.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Tuning MedGemma for Clinical Captioning to Enhance Multimodal RAG over Malaysia CPGs</title>
<link>https://arxiv.org/abs/2510.15418</link>
<guid>https://arxiv.org/abs/2510.15418</guid>
<content:encoded><![CDATA[
arXiv:2510.15418v2 Announce Type: replace-cross 
Abstract: Retrieval-Augmented Generation systems are essential for providing fact-based guidance from Malaysian Clinical Practice Guidelines. However, their effectiveness with image-based queries is limited, as general Vision-Language Model captions often lack clinical specificity and factual grounding. This study proposes and validates a framework to specialize the MedGemma model for generating high-fidelity captions that serve as superior queries. To overcome data scarcity, we employ a knowledge distillation pipeline to create a synthetic dataset across dermatology, fundus, and chest radiography domains, and fine-tune MedGemma using the parameter-efficient QLoRA method. Performance was rigorously assessed through a dual framework measuring both classification accuracy and, via a novel application of the RAGAS framework, caption faithfulness, relevancy, and correctness. The fine-tuned model demonstrated substantial improvements in classification performance, while RAGAS evaluation confirmed significant gains in caption faithfulness and correctness, validating the models ability to produce reliable, factually grounded descriptions. This work establishes a robust pipeline for specializing medical VLMs and validates the resulting model as a high-quality query generator, laying the groundwork for enhancing multimodal RAG systems in evidence-based clinical decision support.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Can String Probability Tell Us About Grammaticality?</title>
<link>https://arxiv.org/abs/2510.16227</link>
<guid>https://arxiv.org/abs/2510.16227</guid>
<content:encoded><![CDATA[
arXiv:2510.16227v2 Announce Type: replace-cross 
Abstract: What have language models (LMs) learned about grammar? This question remains hotly debated, with major ramifications for linguistic theory. However, since probability and grammaticality are distinct notions in linguistics, it is not obvious what string probabilities can reveal about an LM's underlying grammatical knowledge. We present a theoretical analysis of the relationship between grammar, meaning, and string probability, based on simple assumptions about the generative process of corpus data. Our framework makes three predictions, which we validate empirically using 280K sentence pairs in English and Chinese: (1) correlation between the probability of strings within minimal pairs, i.e., string pairs with minimal semantic differences; (2) correlation between models' and humans' deltas within minimal pairs; and (3) poor separation in probability space between unpaired grammatical and ungrammatical strings. Our analyses give theoretical grounding for using probability to learn about LMs' structural knowledge, and suggest directions for future work in LM grammatical evaluation.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics</title>
<link>https://arxiv.org/abs/2510.17797</link>
<guid>https://arxiv.org/abs/2510.17797</guid>
<content:encoded><![CDATA[
arXiv:2510.17797v2 Announce Type: replace-cross 
Abstract: As information grows exponentially, enterprises face increasing pressure to transform unstructured data into coherent, actionable insights. While autonomous agents show promise, they often struggle with domain-specific nuances, intent alignment, and enterprise integration. We present Enterprise Deep Research (EDR), a multi-agent system that integrates (1) a Master Planning Agent for adaptive query decomposition, (2) four specialized search agents (General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool ecosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a Visualization Agent for data-driven insights, and (5) a reflection mechanism that detects knowledge gaps and updates research direction with optional human-in-the-loop steering guidance. These components enable automated report generation, real-time streaming, and seamless enterprise deployment, as validated on internal datasets. On open-ended benchmarks including DeepResearch Bench and DeepConsult, EDR outperforms state-of-the-art agentic systems without any human steering. We release the EDR framework and benchmark trajectories to advance research on multi-agent reasoning applications.
  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and Dataset at https://huggingface.co/datasets/Salesforce/EDR-200
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open Language Foundation</title>
<link>https://arxiv.org/abs/2510.22115</link>
<guid>https://arxiv.org/abs/2510.22115</guid>
<content:encoded><![CDATA[
arXiv:2510.22115v2 Announce Type: replace-cross 
Abstract: We introduce Ling 2.0, a series reasoning-oriented language foundation built upon the principle that every activation boosts reasoning capability. Designed to scale from tens of billions to one trillion parameters under a unified Mixture-of-Experts (MoE) paradigm, Ling 2.0 emphasizes high sparsity, cross-scale consistency, and efficiency guided by empirical scaling laws. The series includes three non-thinking (instruct) models - Ling-mini-2.0, Ling-flash-2.0, and Ling-1T - ranging from 16B to 1T total parameters and achieving up to 7-fold active-compute efficiency compared with dense counterparts. Ling 2.0 integrates coordinated innovations across model architecture, pre-training, post-training, and infrastructure: a high-sparsity MoE with MTP for efficient reasoning, reasoning-oriented data and mid-training CoT activation, reinforcement-based fine-tuning (DFT, Evo-CoT), and full-scale FP8 training with fine-grained heterogeneous pipelines. At the trillion scale, Ling-1T establishes a new Pareto frontier of reasoning accuracy versus computational efficiency, demonstrating that sparse activation, when properly aligned with reasoning objectives, enables scalable and efficient intelligence. Collectively, Ling 2.0 provides a coherent, open, and efficient foundation for advancing future reasoning and thinking models, including the Ring series built upon the same base.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CompressionAttack: Exploiting Prompt Compression as a New Attack Surface in LLM-Powered Agents</title>
<link>https://arxiv.org/abs/2510.22963</link>
<guid>https://arxiv.org/abs/2510.22963</guid>
<content:encoded><![CDATA[
arXiv:2510.22963v2 Announce Type: replace-cross 
Abstract: LLM-powered agents often use prompt compression to reduce inference costs, but this introduces a new security risk. Compression modules, which are optimized for efficiency rather than safety, can be manipulated by adversarial inputs, causing semantic drift and altering LLM behavior. This work identifies prompt compression as a novel attack surface and presents CompressionAttack, the first framework to exploit it. CompressionAttack includes two strategies: HardCom, which uses discrete adversarial edits for hard compression, and SoftCom, which performs latent-space perturbations for soft compression. Experiments on multiple LLMs show up to 80% attack success and 98% preference flips, while remaining highly stealthy and transferable. Case studies in VSCode Cline and Ollama confirm real-world impact, and current defenses prove ineffective, highlighting the need for stronger protections.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>USF-MAE: Ultrasound Self-Supervised Foundation Model with Masked Autoencoding</title>
<link>https://arxiv.org/abs/2510.22990</link>
<guid>https://arxiv.org/abs/2510.22990</guid>
<content:encoded><![CDATA[
arXiv:2510.22990v2 Announce Type: replace-cross 
Abstract: Ultrasound imaging is one of the most widely used diagnostic modalities, offering real-time, radiation-free assessment across diverse clinical domains. However, interpretation of ultrasound images remains challenging due to high noise levels, operator dependence, and limited field of view, resulting in substantial inter-observer variability. Current Deep Learning approaches are hindered by the scarcity of large labeled datasets and the domain gap between general and sonographic images, which limits the transferability of models pretrained on non-medical data. To address these challenges, we introduce the Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE), the first large-scale self-supervised MAE framework pretrained exclusively on ultrasound data. The model was pre-trained on 370,000 2D and 3D ultrasound images curated from 46 open-source datasets, collectively termed OpenUS-46, spanning over twenty anatomical regions. This curated dataset has been made publicly available to facilitate further research and reproducibility. Using a Vision Transformer encoder-decoder architecture, USF-MAE reconstructs masked image patches, enabling it to learn rich, modality-specific representations directly from unlabeled data. The pretrained encoder was fine-tuned on three public downstream classification benchmarks: BUS-BRA (breast cancer), MMOTU-2D (ovarian tumors), and GIST514-DB (gastrointestinal stromal tumors). Across all tasks, USF-MAE consistently outperformed conventional CNN and ViT baselines, achieving F1-scores of 81.6%, 79.6%, and 82.4%, respectively. Despite not using labels during pretraining, USF-MAE approached the performance of the supervised foundation model UltraSam on breast cancer classification and surpassed it on the other tasks, demonstrating strong cross-anatomical generalization.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grounded in Reality: Learning and Deploying Proactive LLM from Offline Logs</title>
<link>https://arxiv.org/abs/2510.25441</link>
<guid>https://arxiv.org/abs/2510.25441</guid>
<content:encoded><![CDATA[
arXiv:2510.25441v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) excel as passive responders, but teaching them to be proactive, goal-oriented partners, a critical capability in high-stakes domains, remains a major challenge. Current paradigms either myopically optimize single-turn attributes or rely on brittle, high-cost user simulators, creating a persistent ``reality gap''. To bridge this gap, we introduce \texttt{Learn-to-Ask}, a general, simulator-free framework for learning and deploying proactive dialogue agents \textit{directly from offline expert data}, bypassing the need to model complex user dynamics. Our key insight is to reframe the offline policy learning problem by leveraging the \textbf{observed future} of each expert trajectory. This allows us to infer a dense, turn-by-turn reward signal grounded in the expert's revealed strategy, decomposing the intractable long-horizon problem into a series of supervised learning tasks, and training a policy to output a structured \texttt{(action, state_assessment)} tuple, governing both \textbf{what to ask} and, crucially, \textbf{when to stop}. To ensure reward fidelity, our Automated Grader Calibration pipeline systematically purges noise from the LLM-based reward model with minimal human supervision. Empirically, we demonstrate the efficacy of \texttt{Learn-to-Ask} in a real-world medical dataset, using LLMs of varying sizes up to 32B. Our approach culminates in the successful deployment of LLMs into a live, large-scale online AI service. In rigorous in-house evaluations, our model was launched and achieved performance even superior to human experts, proving our framework's ability to translate offline data into tangible, real-world impact. We hope this work provides a practical and economically viable blueprint for transforming passive LLMs into proactive, goal-oriented LLM applications.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AURA: A Reinforcement Learning Framework for AI-Driven Adaptive Conversational Surveys</title>
<link>https://arxiv.org/abs/2510.27126</link>
<guid>https://arxiv.org/abs/2510.27126</guid>
<content:encoded><![CDATA[
arXiv:2510.27126v2 Announce Type: replace-cross 
Abstract: Conventional online surveys provide limited personalization, often resulting in low engagement and superficial responses. Although AI survey chatbots improve convenience, most are still reactive: they rely on fixed dialogue trees or static prompt templates and therefore cannot adapt within a session to fit individual users, which leads to generic follow-ups and weak response quality. We address these limitations with AURA (Adaptive Understanding through Reinforcement Learning for Assessment), a reinforcement learning framework for AI-driven adaptive conversational surveys. AURA quantifies response quality using a four-dimensional LSDE metric (Length, Self-disclosure, Emotion, and Specificity) and selects follow-up question types via an epsilon-greedy policy that updates the expected quality gain within each session. Initialized with priors extracted from 96 prior campus-climate conversations (467 total chatbot-user exchanges), the system balances exploration and exploitation across 10-15 dialogue exchanges, dynamically adapting to individual participants in real time. In controlled evaluations, AURA achieved a +0.076 mean gain in response quality and a statistically significant improvement over non-adaptive baselines (p=0.044, d=0.66), driven by a 63% reduction in specification prompts and a 10x increase in validation behavior. These results demonstrate that reinforcement learning can give survey chatbots improved adaptivity, transforming static questionnaires into interactive, self-improving assessment systems.
]]></content:encoded>
<pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autocomp: A Powerful and Portable Code Optimizer for Tensor Accelerators</title>
<link>https://arxiv.org/abs/2505.18574</link>
<guid>https://arxiv.org/abs/2505.18574</guid>
<content:encoded><![CDATA[
<div> accelerators, tensor processing, Autocomp, code optimization, hardware feedback
Summary:
Autocomp is a novel approach that leverages large language models to optimize code for tensor accelerators by integrating domain knowledge and hardware feedback in a structured two-phase prompt. By formulating optimization passes, inserting domain knowledge, and using hardware feedback, Autocomp outperforms vendor-provided libraries like Gemmini by 5.6x, expert hand-tuned code like AWS Trainium by 1.9x, and machine learning-based cost models like NVIDIA L40S by 3.8x. The optimized code generated by Autocomp also can be reused across similar tensor operations, achieving speedups of up to 24% under a fixed sample budget. This approach addresses the challenge of programming tensor accelerators by automating the optimization process and demonstrating significant performance improvements over existing methods in the field.<br /><br />Summary: <div>
arXiv:2505.18574v5 Announce Type: replace-cross 
Abstract: Hardware accelerators, especially those designed for tensor processing, have become ubiquitous in today's computing landscape. However, even with significant efforts in building compilers, programming these tensor accelerators remains challenging, leaving much of their potential underutilized. Recently, large language models (LLMs), trained on large amounts of code, have shown significant promise in code generation and optimization tasks, but generating low-resource languages, such as specialized tensor accelerator code still poses a significant challenge. We tackle this challenge with Autocomp, an approach that empowers accelerator programmers to leverage domain knowledge and hardware feedback to optimize code via an automated LLM-driven search. We accomplish this by: 1) formulating each optimization pass as a structured two-phase prompt, divided into planning and code generation phases, 2) inserting domain knowledge during planning via a concise and adaptable optimization menu, and 3) integrating correctness and performance metrics from hardware as feedback at each search iteration. Across three distinct hardware platforms, we demonstrate that Autocomp-optimized code runs 5.6x faster than the vendor-provided library (Gemmini), outperforms expert-level hand-tuned code by 1.9x (AWS Trainium), and achieves 3.8x higher performance than a machine learning-based cost model for GPUs (NVIDIA L40S). Additionally, we demonstrate that optimization schedules generated from Autocomp can be reused across similar tensor operations, improving speedups by up to 24% under a fixed sample budget.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Agent Learning via Experience Synthesis</title>
<link>https://arxiv.org/abs/2511.03773</link>
<guid>https://arxiv.org/abs/2511.03773</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, large language model, DreamGym, scalability, simulation-to-real transfer

Summary: 
DreamGym is a new framework that addresses challenges in reinforcement learning for large language model agents by providing a unified solution for synthesizing diverse experiences. It utilizes reasoning-based models to collect scalable agent rollout data, leveraging an experience replay buffer initialized with real-world data. The framework also dynamically generates challenging tasks to improve knowledge acquisition through online curriculum learning. Experimental results show that DreamGym outperforms baseline methods by over 30% on non-RL-ready tasks and matches performance on costly RL-ready settings using only synthetic interactions. When transferring policies trained on synthetic experiences to real-world RL, DreamGym achieves significant performance gains while requiring fewer real-world interactions, making it a scalable warm-start strategy for general-purpose RL. 

<br /><br />Summary: <div>
arXiv:2511.03773v1 Announce Type: new 
Abstract: While reinforcement learning (RL) can empower large language model (LLM) agents by enabling self-improvement through interaction, its practical adoption remains challenging due to costly rollouts, limited task diversity, unreliable reward signals, and infrastructure complexity, all of which obstruct the collection of scalable experience data. To address these challenges, we introduce DreamGym, the first unified framework designed to synthesize diverse experiences with scalability in mind to enable effective online RL training for autonomous agents. Rather than relying on expensive real-environment rollouts, DreamGym distills environment dynamics into a reasoning-based experience model that derives consistent state transitions and feedback signals through step-by-step reasoning, enabling scalable agent rollout collection for RL. To improve the stability and quality of transitions, DreamGym leverages an experience replay buffer initialized with offline real-world data and continuously enriched with fresh interactions to actively support agent training. To improve knowledge acquisition, DreamGym adaptively generates new tasks that challenge the current agent policy, enabling more effective online curriculum learning. Experiments across diverse environments and agent backbones demonstrate that DreamGym substantially improves RL training, both in fully synthetic settings and in sim-to-real transfer scenarios. On non-RL-ready tasks like WebArena, DreamGym outperforms all baselines by over 30%. And in RL-ready but costly settings, it matches GRPO and PPO performance using only synthetic interactions. When transferring a policy trained purely on synthetic experiences to real-environment RL, DreamGym yields significant additional performance gains while requiring far fewer real-world interactions, providing a scalable warm-start strategy for general-purpose RL.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Different Tokenization Algorithms Impact LLMs and Transformer Models for Binary Code Analysis</title>
<link>https://arxiv.org/abs/2511.03825</link>
<guid>https://arxiv.org/abs/2511.03825</guid>
<content:encoded><![CDATA[
<div> Keywords: Tokenization, Assembly code analysis, NLP models, Vocabulary size, Downstream tasks

Summary:<br /><br />Tokenization in assembly code analysis is a crucial but underexplored area impacting vocabulary size, semantic coverage, and downstream task performance. This study evaluates the intrinsic properties of NLP tokenization models and parameter choices for assembly code, including vocabulary size and preprocessing customization. Pre-tokenization rules tailored to assembly code characteristics are explored, along with their impact on tasks like function signature prediction in binary code analysis. Various tokenization models are compared for efficiency in encoding assembly instructions and capturing semantic nuances. State-of-the-art pre-trained models like Llama 3.2, BERT, and BART are evaluated for their effectiveness in tokenization. Findings suggest that tokenizer choice significantly affects downstream performance, with intrinsic metrics providing some predictability of extrinsic evaluation outcomes. Complex trade-offs between intrinsic tokenizer properties and utility in practical assembly code tasks are revealed, providing insights for optimizing tokenization models in low-level code analysis workflows. <div>
arXiv:2511.03825v1 Announce Type: new 
Abstract: Tokenization is fundamental in assembly code analysis, impacting intrinsic characteristics like vocabulary size, semantic coverage, and extrinsic performance in downstream tasks. Despite its significance, tokenization in the context of assembly code remains an underexplored area. This study aims to address this gap by evaluating the intrinsic properties of Natural Language Processing (NLP) tokenization models and parameter choices, such as vocabulary size. We explore preprocessing customization options and pre-tokenization rules tailored to the unique characteristics of assembly code. Additionally, we assess their impact on downstream tasks like function signature prediction -- a critical problem in binary code analysis.
  To this end, we conduct a thorough study on various tokenization models, systematically analyzing their efficiency in encoding assembly instructions and capturing semantic nuances. Through intrinsic evaluations, we compare tokenizers based on tokenization efficiency, vocabulary compression, and representational fidelity for assembly code. Using state-of-the-art pre-trained models such as the decoder-only Large Language Model (LLM) Llama 3.2, the encoder-only transformer BERT, and the encoder-decoder model BART, we evaluate the effectiveness of these tokenizers across multiple performance metrics. Preliminary findings indicate that tokenizer choice significantly influences downstream performance, with intrinsic metrics providing partial but incomplete predictability of extrinsic evaluation outcomes. These results reveal complex trade-offs between intrinsic tokenizer properties and their utility in practical assembly code tasks. Ultimately, this study provides valuable insights into optimizing tokenization models for low-level code analysis, contributing to the robustness and scalability of Natural Language Model (NLM)-based binary analysis workflows.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>To See or To Read: User Behavior Reasoning in Multimodal LLMs</title>
<link>https://arxiv.org/abs/2511.03845</link>
<guid>https://arxiv.org/abs/2511.03845</guid>
<content:encoded><![CDATA[
<div> benchmarking, modality trade-offs, user-behavior reasoning, multimodal large language models, next-purchase prediction 

Summary: 
The study introduces \texttt{BehaviorLens}, a benchmarking framework for evaluating modality choices in user-behavior analysis across multiple Multimodal Large Language Models (MLLMs). Three data representation formats - text, scatter plot, and flowchart - were compared using a real-world purchase-sequence dataset. The results showed that utilizing image representations significantly enhanced MLLM performance in predicting next purchases. The accuracy of next-purchase predictions increased by 87.5% when using image data compared to text data, with no added computational overhead. This finding suggests that for tasks involving user-behavior reasoning, modalities beyond text, such as images, can lead to substantial performance gains in MLLM-based systems. <div>
arXiv:2511.03845v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) are reshaping how modern agentic systems reason over sequential user-behavior data. However, whether textual or image representations of user behavior data are more effective for maximizing MLLM performance remains underexplored. We present \texttt{BehaviorLens}, a systematic benchmarking framework for assessing modality trade-offs in user-behavior reasoning across six MLLMs by representing transaction data as (1) a text paragraph, (2) a scatter plot, and (3) a flowchart. Using a real-world purchase-sequence dataset, we find that when data is represented as images, MLLMs next-purchase prediction accuracy is improved by 87.5% compared with an equivalent textual representation without any additional computational cost.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KnowThyself: An Agentic Assistant for LLM Interpretability</title>
<link>https://arxiv.org/abs/2511.03878</link>
<guid>https://arxiv.org/abs/2511.03878</guid>
<content:encoded><![CDATA[
<div> Keywords: KnowThyself, large language model interpretability, chat-based interface, interactive visualizations, agentic assistant <br />
Summary: KnowThyself is an agentic assistant designed to enhance interpretability of large language models (LLMs). It offers a chat-based interface where users can upload models, ask questions in natural language, and receive interactive visualizations with guided explanations. The system consists of an orchestrator LLM that reformulates user queries, an agent router that directs queries to specialized modules, and explanations that contextualize the outputs. By consolidating existing tools into a cohesive platform and embedding the process into a conversational workflow, KnowThyself lowers technical barriers and provides an accessible solution for LLM inspection. Its design allows for extensibility and simplifies the interpretation process for users. <br /><br />Summary: <div>
arXiv:2511.03878v1 Announce Type: new 
Abstract: We develop KnowThyself, an agentic assistant that advances large language model (LLM) interpretability. Existing tools provide useful insights but remain fragmented and code-intensive. KnowThyself consolidates these capabilities into a chat-based interface, where users can upload models, pose natural language questions, and obtain interactive visualizations with guided explanations. At its core, an orchestrator LLM first reformulates user queries, an agent router further directs them to specialized modules, and the outputs are finally contextualized into coherent explanations. This design lowers technical barriers and provides an extensible platform for LLM inspection. By embedding the whole process into a conversational workflow, KnowThyself offers a robust foundation for accessible LLM interpretability.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extracting Causal Relations in Deep Knowledge Tracing</title>
<link>https://arxiv.org/abs/2511.03948</link>
<guid>https://arxiv.org/abs/2511.03948</guid>
<content:encoded><![CDATA[
<div> Knowledge Tracing, Deep Knowledge Tracing, Recurrent Neural Network, Directed Acyclic Graphs, Causal Dependencies
<br />
Summary:
In this study, the authors challenge the prevailing explanation of Deep Knowledge Tracing (DKT) models, suggesting that its effectiveness lies in its implicit ability to model prerequisite relationships as a causal structure rather than bidirectional relationships. By pruning exercise relation graphs into Directed Acyclic Graphs (DAGs) and training DKT on causal subsets of the Assistments dataset, they demonstrate that DKT's predictive capabilities align strongly with these causal structures. They propose an alternative method for extracting exercise relation DAGs using DKT's learned representations and provide empirical evidence supporting their claim. Overall, the findings suggest that DKT's effectiveness is largely driven by its capacity to approximate causal dependencies between knowledge components (KCs) rather than simple relational mappings.
<br /><br /> <div>
arXiv:2511.03948v1 Announce Type: new 
Abstract: A longstanding goal in computational educational research is to develop explainable knowledge tracing (KT) models. Deep Knowledge Tracing (DKT), which leverages a Recurrent Neural Network (RNN) to predict student knowledge and performance on exercises, has been proposed as a major advancement over traditional KT methods. Several studies suggest that its performance gains stem from its ability to model bidirectional relationships between different knowledge components (KCs) within a course, enabling the inference of a student's understanding of one KC from their performance on others. In this paper, we challenge this prevailing explanation and demonstrate that DKT's strength lies in its implicit ability to model prerequisite relationships as a causal structure, rather than bidirectional relationships. By pruning exercise relation graphs into Directed Acyclic Graphs (DAGs) and training DKT on causal subsets of the Assistments dataset, we show that DKT's predictive capabilities align strongly with these causal structures. Furthermore, we propose an alternative method for extracting exercise relation DAGs using DKT's learned representations and provide empirical evidence supporting our claim. Our findings suggest that DKT's effectiveness is largely driven by its capacity to approximate causal dependencies between KCs rather than simple relational mappings.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs and Cultural Values: the Impact of Prompt Language and Explicit Cultural Framing</title>
<link>https://arxiv.org/abs/2511.03980</link>
<guid>https://arxiv.org/abs/2511.03980</guid>
<content:encoded><![CDATA[
<div> LLMs, cultural values, prompts, language, biases
<br />
Summary: Large Language Models (LLMs) are used globally but may struggle to represent cultural diversity due to biases in training data. Prompt language and cultural perspectives influence LLM responses, aligning more with values of specific countries like the Netherlands, Germany, US, and Japan. However, all models exhibit similar patterns, leaning toward neutral responses with progressive stances on certain issues. Cultural framing improves alignment with human values more than targeted prompts, while a combination of both is not significantly better than cultural framing alone. LLMs show responsiveness to prompts but remain anchored to specific cultural defaults, thus hindering their ability to truly represent cultural diversity.<br /><br />Summary: <div>
arXiv:2511.03980v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are rapidly being adopted by users across the globe, who interact with them in a diverse range of languages. At the same time, there are well-documented imbalances in the training data and optimisation objectives of this technology, raising doubts as to whether LLMs can represent the cultural diversity of their broad user base. In this study, we look at LLMs and cultural values and examine how prompt language and cultural framing influence model responses and their alignment with human values in different countries. We probe 10 LLMs with 63 items from the Hofstede Values Survey Module and World Values Survey, translated into 11 languages, and formulated as prompts with and without different explicit cultural perspectives. Our study confirms that both prompt language and cultural perspective produce variation in LLM outputs, but with an important caveat: While targeted prompting can, to a certain extent, steer LLM responses in the direction of the predominant values of the corresponding countries, it does not overcome the models' systematic bias toward the values associated with a restricted set of countries in our dataset: the Netherlands, Germany, the US, and Japan. All tested models, regardless of their origin, exhibit remarkably similar patterns: They produce fairly neutral responses on most topics, with selective progressive stances on issues such as social tolerance. Alignment with cultural values of human respondents is improved more with an explicit cultural perspective than with a targeted prompt language. Unexpectedly, combining both approaches is no more effective than cultural framing with an English prompt. These findings reveal that LLMs occupy an uncomfortable middle ground: They are responsive enough to changes in prompts to produce variation, but too firmly anchored to specific cultural defaults to adequately represent cultural diversity.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ArchPilot: A Proxy-Guided Multi-Agent Approach for Machine Learning Engineering</title>
<link>https://arxiv.org/abs/2511.03985</link>
<guid>https://arxiv.org/abs/2511.03985</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, automated ML engineering, ArchPilot, multi-agent system, Monte Carlo Tree Search 

Summary: 
ArchPilot introduces a multi-agent system for efficient ML engineering by combining architecture generation, proxy-based evaluation, and adaptive search. The system consists of three specialized agents - orchestration, generation, and evaluation - working together to prioritize high-potential candidates without the need for repeated full training runs. The orchestration agent coordinates the search process using a novel algorithm inspired by Monte Carlo Tree Search and manages memory of previous candidates. The generation agent iteratively generates and improves candidate architectures, while the evaluation agent executes proxy training runs and generates fidelity-aware performance metrics. This collaborative approach allows ArchPilot to outperform state-of-the-art baselines like AIDE and ML-Master on MLE-Bench, demonstrating its effectiveness in optimizing ML engineering processes. <div>
arXiv:2511.03985v1 Announce Type: new 
Abstract: Recent LLM-based agents have demonstrated strong capabilities in automated ML engineering. However, they heavily rely on repeated full training runs to evaluate candidate solutions, resulting in significant computational overhead, limited scalability to large search spaces, and slow iteration cycles. To address these challenges, we introduce ArchPilot, a multi-agent system that integrates architecture generation, proxy-based evaluation, and adaptive search into a unified framework. ArchPilot consists of three specialized agents: an orchestration agent that coordinates the search process using a Monte Carlo Tree Search (MCTS)-inspired novel algorithm with a restart mechanism and manages memory of previous candidates; a generation agent that iteratively generates, improves, and debugs candidate architectures; and an evaluation agent that executes proxy training runs, generates and optimizes proxy functions, and aggregates the proxy scores into a fidelity-aware performance metric. This multi-agent collaboration allows ArchPilot to prioritize high-potential candidates with minimal reliance on expensive full training runs, facilitating efficient ML engineering under limited budgets. Experiments on MLE-Bench demonstrate that ArchPilot outperforms SOTA baselines such as AIDE and ML-Master, validating the effectiveness of our multi-agent system.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting Silent Failures in Multi-Agentic AI Trajectories</title>
<link>https://arxiv.org/abs/2511.04032</link>
<guid>https://arxiv.org/abs/2511.04032</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-Agentic AI systems, anomaly detection, large language models, dataset curation pipeline, benchmark datasets <br />
Summary: <br />
- This paper introduces the task of anomaly detection in Multi-Agentic AI systems, which are prone to non-deterministic failures. 
- The authors present a dataset curation pipeline to capture user behavior, agent non-determinism, and variations in large language models. 
- Two benchmark datasets comprising 4,275 and 894 trajectories from Multi-Agentic AI systems are curated and labeled for anomaly detection methods evaluation. 
- Supervised (XGBoost) and semi-supervised (SVDD) approaches show comparable performance, achieving accuracies up to 98% and 96%, respectively. 
- The study offers insights, datasets, and benchmarks for future research in anomaly detection for Multi-Agentic AI systems. <br /> <div>
arXiv:2511.04032v1 Announce Type: new 
Abstract: Multi-Agentic AI systems, powered by large language models (LLMs), are inherently non-deterministic and prone to silent failures such as drift, cycles, and missing details in outputs, which are difficult to detect. We introduce the task of anomaly detection in agentic trajectories to identify these failures and present a dataset curation pipeline that captures user behavior, agent non-determinism, and LLM variation. Using this pipeline, we curate and label two benchmark datasets comprising \textbf{4,275 and 894} trajectories from Multi-Agentic AI systems. Benchmarking anomaly detection methods on these datasets, we show that supervised (XGBoost) and semi-supervised (SVDD) approaches perform comparably, achieving accuracies up to 98% and 96%, respectively. This work provides the first systematic study of anomaly detection in Multi-Agentic AI systems, offering datasets, benchmarks, and insights to guide future research.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpreting Multi-Attribute Confounding through Numerical Attributes in Large Language Models</title>
<link>https://arxiv.org/abs/2511.04053</link>
<guid>https://arxiv.org/abs/2511.04053</guid>
<content:encoded><![CDATA[
<div> numerical attributes, latent subspaces, language models, numerical reasoning errors, representation mechanisms <br />
Summary: Language models exhibit numerical reasoning errors due to the integration of shared latent subspaces for numerical attributes. These models encode real-world numerical correlations but tend to amplify them. Irrelevant numerical context perturbs magnitude representations, leading to varying downstream effects based on model size. This vulnerability in decision-making highlights the need for fairer, representation-aware control in multi-attribute entanglement. <br /><br /> <div>
arXiv:2511.04053v1 Announce Type: new 
Abstract: Although behavioral studies have documented numerical reasoning errors in large language models (LLMs), the underlying representational mechanisms remain unclear. We hypothesize that numerical attributes occupy shared latent subspaces and investigate two questions:(1) How do LLMs internally integrate multiple numerical attributes of a single entity? (2)How does irrelevant numerical context perturb these representations and their downstream outputs? To address these questions, we combine linear probing with partial correlation analysis and prompt-based vulnerability tests across models of varying sizes. Our results show that LLMs encode real-world numerical correlations but tend to systematically amplify them. Moreover, irrelevant context induces consistent shifts in magnitude representations, with downstream effects that vary by model size. These findings reveal a vulnerability in LLM decision-making and lay the groundwork for fairer, representation-aware control under multi-attribute entanglement.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agentmandering: A Game-Theoretic Framework for Fair Redistricting via Large Language Model Agents</title>
<link>https://arxiv.org/abs/2511.04076</link>
<guid>https://arxiv.org/abs/2511.04076</guid>
<content:encoded><![CDATA[
<div> Agentmandering, redistricting, strategic dynamics, game-theoretic, large language model

Summary: 

Agentmandering introduces a new framework for redistricting that incorporates strategic interactions between two opposing political agents. Inspired by game theory concepts, it involves a turn-based negotiation process where agents choose and freeze district maps from a candidate set. This method ensures fairness and reduces partisan bias by embedding strategic decision-making into the redistricting process. Evaluation on U.S. Census data post-2020 demonstrates significant improvements in reducing unfairness and achieving stability, especially in swing-state scenarios. Compared to standard baselines, Agentmandering shows 2 to 3 orders of magnitude lower variance in results. This innovative approach addresses the manipulation potential in redistricting processes and provides a reliable method for creating more equitable districting plans. The code for Agentmandering is publicly available on GitHub for further exploration and implementation. 

<br /><br /> Summary: <div>
arXiv:2511.04076v1 Announce Type: new 
Abstract: Redistricting plays a central role in shaping how votes are translated into political power. While existing computational methods primarily aim to generate large ensembles of legally valid districting plans, they often neglect the strategic dynamics involved in the selection process. This oversight creates opportunities for partisan actors to cherry-pick maps that, while technically compliant, are politically advantageous. Simply satisfying formal constraints does not ensure fairness when the selection process itself can be manipulated. We propose \textbf{Agentmandering}, a framework that reimagines redistricting as a turn-based negotiation between two agents representing opposing political interests. Drawing inspiration from game-theoretic ideas, particularly the \textit{Choose-and-Freeze} protocol, our method embeds strategic interaction into the redistricting process via large language model (LLM) agents. Agents alternate between selecting and freezing districts from a small set of candidate maps, gradually partitioning the state through constrained and interpretable choices. Evaluation on post-2020 U.S. Census data across all states shows that Agentmandering significantly reduces partisan bias and unfairness, while achieving 2 to 3 orders of magnitude lower variance than standard baselines. These results demonstrate both fairness and stability, especially in swing-state scenarios. Our code is available at https://github.com/Lihaogx/AgentMandering.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KGFR: A Foundation Retriever for Generalized Knowledge Graph Question Answering</title>
<link>https://arxiv.org/abs/2511.04093</link>
<guid>https://arxiv.org/abs/2511.04093</guid>
<content:encoded><![CDATA[
<div> framework, large language models, knowledge graph, retriever, reasoning
Summary:
The paper introduces a collaborative framework called LLM-KGFR that combines a large language model with a structured retriever, the Knowledge Graph Foundation Retriever (KGFR). KGFR encodes relations using descriptions generated by the language model and initializes entities based on their roles in questions, enabling zero-shot generalization to unseen knowledge graphs. To efficiently handle large graphs, the framework employs Asymmetric Progressive Propagation (APP) for selective expansion. The framework supports node-, edge-, and path-level interfaces for the language model to request candidate answers, supporting facts, and reasoning paths in an iterative and controllable reasoning loop. Experimental results show that LLM-KGFR achieves strong performance, maintaining scalability and generalization, making it a practical solution for knowledge graph-augmented reasoning.<br /><br />Summary: <div>
arXiv:2511.04093v1 Announce Type: new 
Abstract: Large language models (LLMs) excel at reasoning but struggle with knowledge-intensive questions due to limited context and parametric knowledge. However, existing methods that rely on finetuned LLMs or GNN retrievers are limited by dataset-specific tuning and scalability on large or unseen graphs. We propose the LLM-KGFR collaborative framework, where an LLM works with a structured retriever, the Knowledge Graph Foundation Retriever (KGFR). KGFR encodes relations using LLM-generated descriptions and initializes entities based on their roles in the question, enabling zero-shot generalization to unseen KGs. To handle large graphs efficiently, it employs Asymmetric Progressive Propagation (APP)- a stepwise expansion that selectively limits high-degree nodes while retaining informative paths. Through node-, edge-, and path-level interfaces, the LLM iteratively requests candidate answers, supporting facts, and reasoning paths, forming a controllable reasoning loop. Experiments demonstrate that LLM-KGFR achieves strong performance while maintaining scalability and generalization, providing a practical solution for KG-augmented reasoning.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Testing the Testers: Human-Driven Quality Assessment of Voice AI Testing Platforms</title>
<link>https://arxiv.org/abs/2511.04133</link>
<guid>https://arxiv.org/abs/2511.04133</guid>
<content:encoded><![CDATA[
<div> Keywords: Voice AI, testing quality, benchmarking, simulation quality, evaluation quality

Summary:<br /><br />
Voice AI agents are being deployed in production at a rapid pace, but there is a lack of systematic methods for testing reliability. This article introduces a framework to evaluate the quality of voice AI testing through human-centered benchmarking. The framework addresses the challenges of generating realistic test conversations and accurately evaluating agent responses. Through empirical evaluation of three commercial platforms, the framework was validated, showing statistically significant performance differences. The top-performing platform, Evalion, achieved high evaluation quality and simulation quality scores compared to others. This framework provides a way for researchers and organizations to validate the testing capabilities of any platform, enabling confident deployment of voice AI at scale. Supporting materials are available for reproducibility and adoption. <div>
arXiv:2511.04133v1 Announce Type: new 
Abstract: Voice AI agents are rapidly transitioning to production deployments, yet systematic methods for ensuring testing reliability remain underdeveloped. Organizations cannot objectively assess whether their testing approaches (internal tools or external platforms) actually work, creating a critical measurement gap as voice AI scales to billions of daily interactions.
  We present the first systematic framework for evaluating voice AI testing quality through human-centered benchmarking. Our methodology addresses the fundamental dual challenge of testing platforms: generating realistic test conversations (simulation quality) and accurately evaluating agent responses (evaluation quality). The framework combines established psychometric techniques (pairwise comparisons yielding Elo ratings, bootstrap confidence intervals, and permutation tests) with rigorous statistical validation to provide reproducible metrics applicable to any testing approach.
  To validate the framework and demonstrate its utility, we conducted comprehensive empirical evaluation of three leading commercial platforms focused on Voice AI Testing using 21,600 human judgments across 45 simulations and ground truth validation on 60 conversations. Results reveal statistically significant performance differences with the proposed framework, with the top-performing platform, Evalion, achieving 0.92 evaluation quality measured as f1-score versus 0.73 for others, and 0.61 simulation quality using a league based scoring system (including ties) vs 0.43 for other platforms.
  This framework enables researchers and organizations to empirically validate the testing capabilities of any platform, providing essential measurement foundations for confident voice AI deployment at scale. Supporting materials are made available to facilitate reproducibility and adoption.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Empowerment Disempowers</title>
<link>https://arxiv.org/abs/2511.04177</link>
<guid>https://arxiv.org/abs/2511.04177</guid>
<content:encoded><![CDATA[
<div> Empowerment, multi-human settings, assistive behavior, AI agents, Disempower-Grid<br />
<br />
Summary:
The article introduces Disempower-Grid, a multi-human gridworld test suite to study the impact of assistive RL agents on multiple humans in environments. It shows that optimizing for one human's empowerment can lead to disempowerment of another human, reducing their influence and rewards. This phenomenon, known as disempowerment, highlights the challenge that goal-agnostic objectives may become misaligned in multi-agent contexts. The study demonstrates that joint empowerment can mitigate disempowerment but at the cost of the user's reward. This research raises important implications for the AI alignment community, emphasizing the need to consider the broader impact of assistive AI agents in complex, multi-human settings. <br /><br />Summary: <div>
arXiv:2511.04177v1 Announce Type: new 
Abstract: Empowerment, a measure of an agent's ability to control its environment, has been proposed as a universal goal-agnostic objective for motivating assistive behavior in AI agents. While multi-human settings like homes and hospitals are promising for AI assistance, prior work on empowerment-based assistance assumes that the agent assists one human in isolation. We introduce an open source multi-human gridworld test suite Disempower-Grid. Using Disempower-Grid, we empirically show that assistive RL agents optimizing for one human's empowerment can significantly reduce another human's environmental influence and rewards - a phenomenon we formalize as disempowerment. We characterize when disempowerment occurs in these environments and show that joint empowerment mitigates disempowerment at the cost of the user's reward. Our work reveals a broader challenge for the AI alignment community: goal-agnostic objectives that seem aligned in single-agent settings can become misaligned in multi-agent contexts.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Opus: A Quantitative Framework for Workflow Evaluation</title>
<link>https://arxiv.org/abs/2511.04220</link>
<guid>https://arxiv.org/abs/2511.04220</guid>
<content:encoded><![CDATA[
<div> Opus Workflow Evaluation Framework, probabilistic formulation, Workflow quality, efficiency, correctness, reliability, cost <br />
<br />Summary:
The Opus Workflow Evaluation Framework is introduced in this paper, offering a mathematical model that quantifies Workflow quality and efficiency by integrating correctness, reliability, and cost considerations. It consists of two components: the Opus Workflow Reward model estimates performance based on success likelihood, resource usage, and output gain, while the Opus Workflow Normative Penalties measure structural and informational quality in terms of Cohesion, Coupling, Observability, and Information Hygiene. This framework enables automated assessment, ranking, and optimization of Workflows within automation systems like Opus. It can also be integrated into Reinforcement Learning loops to aid in Workflow discovery and refinement. Overall, the Opus Workflow Reward model defines Workflow success probabilistically, while the Normative Penalties capture various properties of Workflows, leading to a unified optimization formulation for identifying and ranking optimal Workflows considering both rewards and penalties.<br /> <div>
arXiv:2511.04220v1 Announce Type: new 
Abstract: This paper introduces the Opus Workflow Evaluation Framework, a probabilistic-normative formulation for quantifying Workflow quality and efficiency. It integrates notions of correctness, reliability, and cost into a coherent mathematical model that enables direct comparison, scoring, and optimization of Workflows. The framework combines the Opus Workflow Reward, a probabilistic function estimating expected performance through success likelihood, resource usage, and output gain, with the Opus Workflow Normative Penalties, a set of measurable functions capturing structural and informational quality across Cohesion, Coupling, Observability, and Information Hygiene. It supports automated Workflow assessment, ranking, and optimization within modern automation systems such as Opus and can be integrated into Reinforcement Learning loops to guide Workflow discovery and refinement. In this paper, we introduce the Opus Workflow Reward model that formalizes Workflow success as a probabilistic expectation over costs and outcomes. We define measurable Opus Workflow Normative Penalties capturing structural, semantic, and signal-related properties of Workflows. Finally, we propose a unified optimization formulation for identifying and ranking optimal Workflows under joint Reward-Penalty trade-offs.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shared Spatial Memory Through Predictive Coding</title>
<link>https://arxiv.org/abs/2511.04235</link>
<guid>https://arxiv.org/abs/2511.04235</guid>
<content:encoded><![CDATA[
<div> framework, multi-agent, spatial memory, coordination, communication
Summary:
- The article introduces a multi-agent predictive coding framework to address challenges in sharing and reconstructing consistent spatial memory in multi-agent systems.
- Coordination is formulated as minimizing mutual uncertainty among agents, prompting them to learn who, what, and when to communicate.
- Agents use a grid-cell-like metric for internal spatial coding, which emerges from self-supervised motion prediction.
- Specialized neural populations, similar to hippocampal social place cells, encode partners' locations.
- A hierarchical reinforcement learning policy actively explores to reduce joint uncertainty. 
- On the Memory-Maze benchmark, the approach shows resilience to bandwidth constraints, outperforming a full-broadcast baseline. 
- The findings provide a biologically plausible basis for the emergence of complex social representations from a unified predictive drive, fostering social collective intelligence.

<br /><br />Summary: <div>
arXiv:2511.04235v1 Announce Type: new 
Abstract: Sharing and reconstructing a consistent spatial memory is a critical challenge in multi-agent systems, where partial observability and limited bandwidth often lead to catastrophic failures in coordination. We introduce a multi-agent predictive coding framework that formulate coordination as the minimization of mutual uncertainty among agents. Instantiated as an information bottleneck objective, it prompts agents to learn not only who and what to communicate but also when. At the foundation of this framework lies a grid-cell-like metric as internal spatial coding for self-localization, emerging spontaneously from self-supervised motion prediction. Building upon this internal spatial code, agents gradually develop a bandwidth-efficient communication mechanism and specialized neural populations that encode partners' locations: an artificial analogue of hippocampal social place cells (SPCs). These social representations are further enacted by a hierarchical reinforcement learning policy that actively explores to reduce joint uncertainty. On the Memory-Maze benchmark, our approach shows exceptional resilience to bandwidth constraints: success degrades gracefully from 73.5% to 64.4% as bandwidth shrinks from 128 to 4 bits/step, whereas a full-broadcast baseline collapses from 67.6% to 28.6%. Our findings establish a theoretically principled and biologically plausible basis for how complex social representations emerge from a unified predictive drive, leading to social collective intelligence.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RLoop: An Self-Improving Framework for Reinforcement Learning with Iterative Policy Initialization</title>
<link>https://arxiv.org/abs/2511.04285</link>
<guid>https://arxiv.org/abs/2511.04285</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Verifiable Rewards, RL overfitting, RLoop, iterative policy initialization

Summary: 
RLoop addresses the challenge of RL overfitting by introducing a self-improving framework based on iterative policy initialization. The traditional RL training process is transformed into a virtuous cycle by using RL to explore the solution space, creating an expert dataset through successful trajectories, and refining the initial policy using Rejection-sampling Fine-Tuning (RFT). This iterative re-initialization process effectively converts transient policy variations into robust performance gains, mitigating the issue of forgetting and improving generalization. Experimental results demonstrate that RLoop significantly enhances performance, increasing average accuracy by 9% and pass@32 by over 15% compared to vanilla RL. 

<br /><br />Summary: <div>
arXiv:2511.04285v1 Announce Type: new 
Abstract: While Reinforcement Learning for Verifiable Rewards (RLVR) is powerful for training large reasoning models, its training dynamics harbor a critical challenge: RL overfitting, where models gain training rewards but lose generalization. Our analysis reveals this is driven by policy over-specialization and catastrophic forgetting of diverse solutions generated during training. Standard optimization discards this valuable inter-step policy diversity. To address this, we introduce RLoop, a self-improving framework built on iterative policy initialization. RLoop transforms the standard training process into a virtuous cycle: it first uses RL to explore the solution space from a given policy, then filters the successful trajectories to create an expert dataset. This dataset is used via Rejection-sampling Fine-Tuning (RFT) to refine the initial policy, creating a superior starting point for the next iteration. This loop of exploration and exploitation via iterative re-initialization effectively converts transient policy variations into robust performance gains. Our experiments show RLoop mitigates forgetting and substantially improves generalization, boosting average accuracy by 9% and pass@32 by over 15% compared to vanilla RL.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents</title>
<link>https://arxiv.org/abs/2511.04307</link>
<guid>https://arxiv.org/abs/2511.04307</guid>
<content:encoded><![CDATA[
<div> Dataset, benchmark suite, computer-using agents, GUI grounding, action prediction
Summary:<br /><br />GUI-360$^\circ$ introduces a large-scale dataset and benchmark suite for computer-using agents (CUAs), addressing gaps in real-world CUA tasks, automated collection-and-annotation pipelines, and unified benchmarking. The dataset includes over 1.2M action steps in popular Windows office applications, supporting GUI grounding, screen parsing, action prediction, and a hybrid GUI+API action space. Benchmarking vision-language models on GUI-360$^\circ$ highlights shortcomings in grounding and action prediction. While supervised fine-tuning and reinforcement learning show improvements, they do not achieve human-level reliability. The dataset is publicly available to facilitate reproducible research and advance desktop CUAs. <div>
arXiv:2511.04307v1 Announce Type: new 
Abstract: We introduce GUI-360$^\circ$, a large-scale, comprehensive dataset and benchmark suite designed to advance computer-using agents (CUAs). CUAs present unique challenges and is constrained by three persistent gaps: a scarcity of real-world CUA tasks, the lack of automated collection-and-annotation pipelines for multi-modal trajectories, and the absence of a unified benchmark that jointly evaluates GUI grounding, screen parsing, and action prediction.
  GUI-360$^\circ$ addresses these gaps with an LLM-augmented, largely automated pipeline for query sourcing, environment-template construction, task instantiation, batched execution, and LLM-driven quality filtering. The released corpus contains over 1.2M executed action steps across thousands of trajectories in popular Windows office applications, and includes full-resolution screenshots, accessibility metadata when available, instantiated goals, intermediate reasoning traces, and both successful and failed action trajectories. The dataset supports three canonical tasks, GUI grounding, screen parsing, and action prediction, and a hybrid GUI+API action space that reflects modern agent designs. Benchmarking state-of-the-art vision--language models on GUI-360$^\circ$ reveals substantial out-of-the-box shortcomings in grounding and action prediction; supervised fine-tuning and reinforcement learning yield significant gains but do not close the gap to human-level reliability. We release GUI-360$^\circ$ and accompanying code to facilitate reproducible research and accelerate progress on robust desktop CUAs.
  The full dataset has been made public on https://huggingface.co/datasets/vyokky/GUI-360.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probing the Probes: Methods and Metrics for Concept Alignment</title>
<link>https://arxiv.org/abs/2511.04312</link>
<guid>https://arxiv.org/abs/2511.04312</guid>
<content:encoded><![CDATA[
<div> Concept Activation Vectors, explainable AI, probe accuracy, concept alignment, spurious correlations <br />
Summary:<br />In the field of explainable AI, Concept Activation Vectors (CAVs) are used to detect human-understandable concepts in deep neural networks. However, relying solely on probe accuracy to measure concept alignment may lead to capturing spurious correlations rather than the intended concept. This study introduces a novel concept localization method based on spatial linear attribution and proposes metrics for quantitatively assessing concept alignment: hard accuracy, segmentation scores, and augmentation robustness. Probes with translation invariance and spatial alignment are shown to increase concept alignment. The findings emphasize the importance of alignment-based evaluation metrics and the customization of probes to fit the model architecture and target concept. <br /><br />Summary: <div>
arXiv:2511.04312v1 Announce Type: new 
Abstract: In explainable AI, Concept Activation Vectors (CAVs) are typically obtained by training linear classifier probes to detect human-understandable concepts as directions in the activation space of deep neural networks. It is widely assumed that a high probe accuracy indicates a CAV faithfully representing its target concept. However, we show that the probe's classification accuracy alone is an unreliable measure of concept alignment, i.e., the degree to which a CAV captures the intended concept. In fact, we argue that probes are more likely to capture spurious correlations than they are to represent only the intended concept. As part of our analysis, we demonstrate that deliberately misaligned probes constructed to exploit spurious correlations, achieve an accuracy close to that of standard probes. To address this severe problem, we introduce a novel concept localization method based on spatial linear attribution, and provide a comprehensive comparison of it to existing feature visualization techniques for detecting and mitigating concept misalignment. We further propose three classes of metrics for quantitatively assessing concept alignment: hard accuracy, segmentation scores, and augmentation robustness. Our analysis shows that probes with translation invariance and spatial alignment consistently increase concept alignment. These findings highlight the need for alignment-based evaluation metrics rather than probe accuracy, and the importance of tailoring probes to both the model architecture and the nature of the target concept.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdversariaLLM: A Unified and Modular Toolbox for LLM Robustness Research</title>
<link>https://arxiv.org/abs/2511.04316</link>
<guid>https://arxiv.org/abs/2511.04316</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, robustness, reproducibility, adversarial attacks, benchmark datasets 

Summary: 
The article introduces AdversariaLLM, a toolbox designed for conducting research on the robustness of Large Language Models (LLMs). The current ecosystem of LLM research is fragmented and lacks consistency, making reproducibility and comparability challenging. AdversariaLLM addresses these issues by providing a robust framework for conducting jailbreak robustness research on LLMs. The toolbox includes twelve adversarial attack algorithms, integrates seven benchmark datasets for evaluation, and offers access to a variety of open-weight LLMs through Hugging Face. Features such as compute-resource tracking, deterministic results, and distributional evaluation techniques enhance comparability and reproducibility. The framework also includes the companion package JudgeZoo for judging purposes. By providing a transparent, comparable, and reproducible foundation for LLM safety research, AdversariaLLM aims to advance the field in a meaningful way.<br /><br />Summary: <div>
arXiv:2511.04316v1 Announce Type: new 
Abstract: The rapid expansion of research on Large Language Model (LLM) safety and robustness has produced a fragmented and oftentimes buggy ecosystem of implementations, datasets, and evaluation methods. This fragmentation makes reproducibility and comparability across studies challenging, hindering meaningful progress. To address these issues, we introduce AdversariaLLM, a toolbox for conducting LLM jailbreak robustness research. Its design centers on reproducibility, correctness, and extensibility. The framework implements twelve adversarial attack algorithms, integrates seven benchmark datasets spanning harmfulness, over-refusal, and utility evaluation, and provides access to a wide range of open-weight LLMs via Hugging Face. The implementation includes advanced features for comparability and reproducibility such as compute-resource tracking, deterministic results, and distributional evaluation techniques. \name also integrates judging through the companion package JudgeZoo, which can also be used independently. Together, these components aim to establish a robust foundation for transparent, comparable, and reproducible research in LLM safety.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RxSafeBench: Identifying Medication Safety Issues of Large Language Models in Simulated Consultation</title>
<link>https://arxiv.org/abs/2511.04328</link>
<guid>https://arxiv.org/abs/2511.04328</guid>
<content:encoded><![CDATA[
<div> framework, medication safety, Large Language Models, RxSafeBench, evaluation<br />
<br />Summary: 
This article addresses the limited research on medication safety in Large Language Models (LLMs) within healthcare systems. A framework is proposed to simulate and evaluate clinical consultations for assessing medication safety capabilities of LLMs. A medication safety database, RxRisk DB, is created with contraindications, drug interactions, and indication-drug pairs. The RxSafeBench benchmark is developed with high-quality consultation scenarios. Leading LLMs are evaluated using structured multiple choice questions to test their ability to recommend safe medications in simulated patient contexts. Results show current LLMs struggle in integrating contraindication and interaction knowledge, especially with implied risks. Challenges in ensuring medication safety in LLM-based systems are identified, with insights into improving reliability through better prompting and task-specific tuning. RxSafeBench provides a comprehensive benchmark for evaluating medication safety in LLMs, enhancing AI-driven clinical decision support. <div>
arXiv:2511.04328v1 Announce Type: new 
Abstract: Numerous medical systems powered by Large Language Models (LLMs) have achieved remarkable progress in diverse healthcare tasks. However, research on their medication safety remains limited due to the lack of real world datasets, constrained by privacy and accessibility issues. Moreover, evaluation of LLMs in realistic clinical consultation settings, particularly regarding medication safety, is still underexplored. To address these gaps, we propose a framework that simulates and evaluates clinical consultations to systematically assess the medication safety capabilities of LLMs. Within this framework, we generate inquiry diagnosis dialogues with embedded medication risks and construct a dedicated medication safety database, RxRisk DB, containing 6,725 contraindications, 28,781 drug interactions, and 14,906 indication-drug pairs. A two-stage filtering strategy ensures clinical realism and professional quality, resulting in the benchmark RxSafeBench with 2,443 high-quality consultation scenarios. We evaluate leading open-source and proprietary LLMs using structured multiple choice questions that test their ability to recommend safe medications under simulated patient contexts. Results show that current LLMs struggle to integrate contraindication and interaction knowledge, especially when risks are implied rather than explicit. Our findings highlight key challenges in ensuring medication safety in LLM-based systems and provide insights into improving reliability through better prompting and task-specific tuning. RxSafeBench offers the first comprehensive benchmark for evaluating medication safety in LLMs, advancing safer and more trustworthy AI-driven clinical decision support.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Monitor-Generate-Verify (MGV):Formalising Metacognitive Theory for Language Model Reasoning</title>
<link>https://arxiv.org/abs/2511.04341</link>
<guid>https://arxiv.org/abs/2511.04341</guid>
<content:encoded><![CDATA[
<div> Keywords: Test-time reasoning, Generate-Verify paradigm, Metacognitive theories, Monitoring processes, Architectural interventions <br />
Summary: 
The article introduces the Monitor-Generate-Verify (MGV) framework, which addresses the lack of monitoring processes in current test-time reasoning architectures. These architectures often fall into the prefix dominance trap, leading to suboptimal reasoning paths and decreased accuracy. By formalizing metacognitive theories into computational specifications, the MGV framework incorporates explicit monitoring before generation begins, capturing metacognitive experiences such as difficulty assessments and confidence judgements. The framework also refines future monitoring through verification feedback. While the article does not provide empirical validation, it offers a systematic computational translation of foundational metacognitive theories, providing a structured approach to understanding reasoning system failures and proposing specific architectural interventions for future test-time reasoning designs. <br /><br /> <div>
arXiv:2511.04341v1 Announce Type: new 
Abstract: Test-time reasoning architectures such as those following the Generate-Verify paradigm -- where a model iteratively refines or verifies its own generated outputs -- prioritise generation and verification but exclude the monitoring processes that determine when and how reasoning should begin. This omission may contribute to the prefix dominance trap, in which models commit early to suboptimal reasoning paths and seldom recover, yielding roughly 20% accuracy loss. We address this architectural gap by formalising Flavell's and Nelson and Narens' metacognitive theories into computational specifications, proposing the Monitor-Generate-Verify (MGV) framework. MGV extends the Generate-Verify paradigm by adding explicit monitoring that captures metacognitive experiences (from difficulty assessments to confidence judgements) before generation begins and refines future monitoring through verification feedback. Though we present no empirical validation, this work provides the first systematic computational translation of foundational metacognitive theories, offering a principled vocabulary for understanding reasoning system failures and suggesting specific architectural interventions for future test-time reasoning designs.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Post-Training LLMs as Better Decision-Making Agents: A Regret-Minimization Approach</title>
<link>https://arxiv.org/abs/2511.04393</link>
<guid>https://arxiv.org/abs/2511.04393</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, decision-making, regret minimization, fine-tuning, post-training

Summary: 
Iterative Regret-Minimization Fine-Tuning (Iterative RMFT) is proposed as a post-training procedure to enhance the decision-making capabilities of Large Language Models (LLMs). The approach repeatedly distills low-regret decision trajectories back into the base model by rolling out multiple decision trajectories, selecting the k-lowest regret ones, and fine-tuning the model on them. Unlike previous methods, Iterative RMFT does not rely on known DM algorithms or manually crafted templates but leverages the regret metric to understand the model's own decision-making abilities and reasoning rationales. The approach provides more flexible, natural-language training signals, improving LLMs' performance across diverse models. Empirical results demonstrate the effectiveness of Iterative RMFT in enhancing LLMs' decision-making performance across various tasks and natural-language contexts. Theoretical insights show that a single-layer Transformer under this paradigm can act as a no-regret learner in simplified settings. Overall, Iterative RMFT offers a principled and general post-training framework for improving LLMs' decision-making capabilities.<br /><br />Summary: <div>
arXiv:2511.04393v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed as "agents" for decision-making (DM) in interactive and dynamic environments. Yet, since they were not originally designed for DM, recent studies show that LLMs can struggle even in basic online DM problems, failing to achieve low regret or an effective exploration-exploitation tradeoff. To address this, we introduce Iterative Regret-Minimization Fine-Tuning (Iterative RMFT), a post-training procedure that repeatedly distills low-regret decision trajectories back into the base model. At each iteration, the model rolls out multiple decision trajectories, selects the k-lowest regret ones, and fine-tunes itself on them. Unlike prior methods that (a) distill action sequences from known DM algorithms or (b) rely on manually crafted chain-of-thought templates, our approach leverages the regret metric to elicit the model's own DM ability and reasoning rationales. This reliance on model-generated reasoning avoids rigid output engineering and provides more flexible, natural-language training signals. Empirical results show that Iterative RMFT improves LLMs' DM performance across diverse models - from Transformers with numerical input/output, to open-weight LLMs, and advanced closed-weight models like GPT-4o mini. Its flexibility in output and reasoning formats enables generalization across tasks with varying horizons, action spaces, reward processes, and natural-language contexts. Finally, we provide theoretical insight showing that a single-layer Transformer under this paradigm can act as a no-regret learner in a simplified setting. Overall, Iterative RMFT offers a principled and general post-training framework for enhancing LLMs' decision-making capabilities.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Peril of Preference: Why GRPO fails on Ordinal Rewards</title>
<link>https://arxiv.org/abs/2511.04439</link>
<guid>https://arxiv.org/abs/2511.04439</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Policy Optimization, Correctness, Code Verification, Generalization <br />
Summary:
The article introduces Correctness Relative Policy Optimization (CoRPO) as a solution to group-relative Policy Optimization's (GRPO) limitation in handling partial credit feedback with ordinal rewards. CoRPO incorporates an adaptive baseline that ensures failed solutions are not positively reinforced and transitions to a relative preference mode once a quality threshold is consistently met. Empirical validation on a code verification task shows CoRPO's superior stability in convergence and out-of-domain generalization. This work is part of a larger effort to enhance Language Model Models' (LLMs) capabilities through reinforcement learning, transitioning from binary to ordinal rewards and aiming for denser, per-step supervision. <br /><br />Summary: <div>
arXiv:2511.04439v1 Announce Type: new 
Abstract: Group-relative Policy Optimization's (GRPO) simplicity makes it highly desirable for adapting LLMs to become experts at specific tasks. But this simplicity also makes it ill-specified as we seek to enhance RL training with richer, non-binary feedback. When using ordinal rewards to give partial credit, GRPO's simplicity starts to hurt, as its group-average baseline often assigns a positive advantage to failed trajectories and reinforces incorrect behavior.
  We introduce Correctness Relative Policy Optimization (CoRPO), a new formulation that solves this flaw. CoRPO uses an adaptive baseline that enforces a minimum quality threshold, ensuring failed solutions are never positively reinforced. Once the policy consistently meets this threshold, the baseline automatically transitions to a relative preference mode, pushing the model to find optimal solutions rather than just "acceptable" ones. We empirically validate CoRPO on a code verification task, where it demonstrates more stable convergence and better out-of-domain generalization.
  This work represents a critical step in our broader research program to enable LLMs to learn genuinely new capabilities through reinforcement learning. We achieve this by enabling LLMs to learn from rich, multi-dimensional feedback - progressing from binary to ordinal rewards in this work, and onward to denser, per-step supervision.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Shortest Path: Agentic Vehicular Routing with Semantic Context</title>
<link>https://arxiv.org/abs/2511.04464</link>
<guid>https://arxiv.org/abs/2511.04464</guid>
<content:encoded><![CDATA[
<div> algorithm, routing, personalized, urban mobility, contextual reasoning
Summary:
This paper introduces the PAVe system, a hybrid agentic assistant that enhances traditional vehicle routing systems by incorporating contextual reasoning. PAVe combines a Large Language Model agent with a multi-objective Dijkstra algorithm to generate personalized route options based on user tasks, preferences, and avoidance rules. By leveraging a geospatial cache of urban Points of Interest (POIs), PAVe successfully modifies routes to accommodate complex user intent, achieving over 88% accuracy in initial route selections in realistic urban scenarios. The approach of combining classical routing algorithms with a semantic reasoning layer proves robust and effective for creating adaptive, scalable solutions for urban mobility optimization. <br /><br />Summary: <div>
arXiv:2511.04464v1 Announce Type: new 
Abstract: Traditional vehicle routing systems efficiently optimize singular metrics like time or distance, and when considering multiple metrics, they need more processes to optimize . However, they lack the capability to interpret and integrate the complex, semantic, and dynamic contexts of human drivers, such as multi-step tasks, situational constraints, or urgent needs. This paper introduces and evaluates PAVe (Personalized Agentic Vehicular Routing), a hybrid agentic assistant designed to augment classical pathfinding algorithms with contextual reasoning. Our approach employs a Large Language Model (LLM) agent that operates on a candidate set of routes generated by a multi-objective (time, CO2) Dijkstra algorithm. The agent evaluates these options against user-provided tasks, preferences, and avoidance rules by leveraging a pre-processed geospatial cache of urban Points of Interest (POIs). In a benchmark of realistic urban scenarios, PAVe successfully used complex user intent into appropriate route modifications, achieving over 88% accuracy in its initial route selections with a local model. We conclude that combining classical routing algorithms with an LLM-based semantic reasoning layer is a robust and effective approach for creating personalized, adaptive, and scalable solutions for urban mobility optimization.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Promoting Sustainable Web Agents: Benchmarking and Estimating Energy Consumption through Empirical and Theoretical Analysis</title>
<link>https://arxiv.org/abs/2511.04481</link>
<guid>https://arxiv.org/abs/2511.04481</guid>
<content:encoded><![CDATA[
<div> Keywords: Web agents, Large Language Models, Energy consumption, Sustainability, Transparency

Summary: 
Web agents like OpenAI's Operator and Google's Project Mariner are pushing the boundaries of Large Language Models (LLMs) by autonomously interacting with the internet. However, the sustainability issues associated with these agents remain largely unexplored. This study examines the energy and carbon dioxide (CO2) cost of web agents through theoretical estimation and empirical benchmarking. The research demonstrates that different approaches to creating web agents can have a significant impact on energy consumption, with more energy expended not necessarily leading to better results. A lack of transparency in disclosing model parameters and processes further complicates the estimation of energy consumption. The findings underscore the need for dedicated metrics to measure energy consumption in web agent benchmarks, urging a shift in how these systems are evaluated. <br /><br />Summary: <div>
arXiv:2511.04481v1 Announce Type: new 
Abstract: Web agents, like OpenAI's Operator and Google's Project Mariner, are powerful agentic systems pushing the boundaries of Large Language Models (LLM). They can autonomously interact with the internet at the user's behest, such as navigating websites, filling search masks, and comparing price lists. Though web agent research is thriving, induced sustainability issues remain largely unexplored. To highlight the urgency of this issue, we provide an initial exploration of the energy and $CO_2$ cost associated with web agents from both a theoretical -via estimation- and an empirical perspective -by benchmarking. Our results show how different philosophies in web agent creation can severely impact the associated expended energy, and that more energy consumed does not necessarily equate to better results. We highlight a lack of transparency regarding disclosing model parameters and processes used for some web agents as a limiting factor when estimating energy consumption. Our work contributes towards a change in thinking of how we evaluate web agents, advocating for dedicated metrics measuring energy consumption in benchmarks.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large language models replicate and predict human cooperation across experiments in game theory</title>
<link>https://arxiv.org/abs/2511.04500</link>
<guid>https://arxiv.org/abs/2511.04500</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, human decision-making, game-theoretic experiments, machine-behavioral evaluation, social simulations

Summary: 
Large language models (LLMs) are used in various domains, but their alignment with human decision-making is not well understood. A digital twin of game-theoretic experiments was created to evaluate LLMs. Testing three models, Llama displayed high fidelity in replicating human cooperation patterns, while Qwen aligned closely with Nash equilibrium. Population-level behavioral replication was achieved without persona-based prompting. The study generated testable hypotheses for novel game configurations. Calibrated LLMs can replicate human behavioral patterns and enable exploration of unexplored experimental spaces, providing new empirical predictions about human social decision-making. <br /><br />Summary: <div>
arXiv:2511.04500v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used both to make decisions in domains such as health, education and law, and to simulate human behavior. Yet how closely LLMs mirror actual human decision-making remains poorly understood. This gap is critical: misalignment could produce harmful outcomes in practical applications, while failure to replicate human behavior renders LLMs ineffective for social simulations. Here, we address this gap by developing a digital twin of game-theoretic experiments and introducing a systematic prompting and probing framework for machine-behavioral evaluation. Testing three open-source models (Llama, Mistral and Qwen), we find that Llama reproduces human cooperation patterns with high fidelity, capturing human deviations from rational choice theory, while Qwen aligns closely with Nash equilibrium predictions. Notably, we achieved population-level behavioral replication without persona-based prompting, simplifying the simulation process. Extending beyond the original human-tested games, we generate and preregister testable hypotheses for novel game configurations outside the original parameter grid. Our findings demonstrate that appropriately calibrated LLMs can replicate aggregate human behavioral patterns and enable systematic exploration of unexplored experimental spaces, offering a complementary approach to traditional research in the social and behavioral sciences that generates new empirical predictions about human social decision-making.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Sensor Placement in Urban Storm Sewers: A Data-Driven Sparse Sensing Approach</title>
<link>https://arxiv.org/abs/2511.04556</link>
<guid>https://arxiv.org/abs/2511.04556</guid>
<content:encoded><![CDATA[
<div> framework, sensor placement, peak flowrates, urban surface water flooding, data-driven

<br />
Summary:
This study introduces a data-driven sparse sensing (DSS) framework integrated with EPA-SWMM for optimizing sensor placement and predicting peak flowrates in urban drainage networks. Using the Woodland Avenue catchment in Duluth, Minnesota as a case study, the framework leverages SWMM to generate a training dataset for peak flowrate profiles. Through DSS, optimal monitoring nodes are identified using singular value decomposition and QR factorization. The framework achieved high reconstruction performance with NSE values ranging from 0.92 to 0.95. It also demonstrated robustness to uncertainty in measurements and sensor failures depending on location and sensor deployment. This computationally efficient and interpretable framework enables accurate flow reconstruction with minimal sensors, paving the way for flood early warning systems and real-time control in urban drainage networks with limited monitoring resources.<br /><br /> <div>
arXiv:2511.04556v1 Announce Type: new 
Abstract: Urban surface water flooding, triggered by intense rainfall overwhelming drainage systems, is increasingly frequent and widespread. While flood prediction and monitoring in high spatial-temporal resolution are desired, practical constraints in time, budget, and technology hinder its full implementation. How to monitor urban drainage networks and predict flow conditions under constrained resource is a major challenge. This study presents a data-driven sparse sensing (DSS) framework, integrated with EPA-SWMM, to optimize sensor placement and reconstruct peak flowrates in a stormwater system, using the Woodland Avenue catchment in Duluth, Minnesota, as a case study. We utilized a SWMM model to generate a training dataset of peak flowrate profiles across the stormwater network. Furthermore, we applied DSS - leveraging singular value decomposition for dimensionality reduction and QR factorization for sensor allocation - to identify the optimal monitoring nodes based on the simulated training dataset. We then validated the representativeness of these identified monitoring nodes by comparing the DSS-reconstructed peak flowrate profiles with those obtained from SWMM. Three optimally placed sensors among 77 nodes achieved satisfactory reconstruction performance with Nash-Sutcliffe Efficiency (NSE) values of 0.92-0.95 (25th to 75th percentiles). In addition, the model showed good robustness to uncertainty in measurements. Its robustness to sensor failures is location-dependent and improves with the number of sensors deployed. The framework balances computational efficiency and physical interpretability, enabling high-accuracy flow reconstruction with minimal sensors. This DSS framework can be further integrated with predictive models to realize flood early warning and real-time control under limited sensing and monitoring resource.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper</title>
<link>https://arxiv.org/abs/2511.04583</link>
<guid>https://arxiv.org/abs/2511.04583</guid>
<content:encoded><![CDATA[
<div> Keywords: AI Scientist, autonomous system, research workflow, experimentation, evaluation

Summary: 
Jr. AI Scientist is a cutting-edge autonomous AI scientist system designed to mimic the research workflow of a novice student researcher. It analyzes a baseline paper, formulates new hypotheses, conducts experiments, and generates a paper with the results. Different from previous approaches, it leverages coding agents to handle complex implementations, leading to valuable scientific contributions. Evaluation using AI Reviewers and author-led assessments showed that Jr. AI Scientist outperformed existing fully automated systems. However, limitations were identified through author evaluations and reviews from Agents4Science, highlighting risks and challenges for future research. The article also reports various risks encountered during development, providing insights into the current capabilities, risks, and challenges of AI Scientist systems. 

<br /><br />Summary: Jr. AI Scientist is an advanced autonomous AI researcher system that simulates a student researcher's workflow. It formulates hypotheses, conducts experiments, and writes papers based on a baseline paper from a mentor. Utilizing coding agents, it handles complex implementations effectively. Evaluation showed superior performance compared to existing automated systems. However, limitations and potential risks were identified through assessments, emphasizing the need for further research. The article also reports risks encountered during development, contributing to understanding the current state and challenges of AI Scientist systems. <div>
arXiv:2511.04583v1 Announce Type: new 
Abstract: Understanding the current capabilities and risks of AI Scientist systems is essential for ensuring trustworthy and sustainable AI-driven scientific progress while preserving the integrity of the academic ecosystem. To this end, we develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system that mimics the core research workflow of a novice student researcher: Given the baseline paper from the human mentor, it analyzes its limitations, formulates novel hypotheses for improvement, validates them through rigorous experimentation, and writes a paper with the results. Unlike previous approaches that assume full automation or operate on small-scale code, Jr. AI Scientist follows a well-defined research workflow and leverages modern coding agents to handle complex, multi-file implementations, leading to scientifically valuable contributions. For evaluation, we conducted automated assessments using AI Reviewers, author-led evaluations, and submissions to Agents4Science, a venue dedicated to AI-driven scientific contributions. The findings demonstrate that Jr. AI Scientist generates papers receiving higher review scores than existing fully automated systems. Nevertheless, we identify important limitations from both the author evaluation and the Agents4Science reviews, indicating the potential risks of directly applying current AI Scientist systems and key challenges for future research. Finally, we comprehensively report various risks identified during development. We hope these insights will deepen understanding of current progress and risks in AI Scientist development.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are We Asking the Right Questions? On Ambiguity in Natural Language Queries for Tabular Data Analysis</title>
<link>https://arxiv.org/abs/2511.04584</link>
<guid>https://arxiv.org/abs/2511.04584</guid>
<content:encoded><![CDATA[
<div> ambiguity, natural language interfaces, tabular data, cooperative queries, interpretation

Summary:<br /><br />
The article discusses the importance of understanding and embracing ambiguity in natural language interfaces for tabular data. Rather than viewing ambiguity as a flaw, the authors propose a framework that categorizes queries as cooperative or uncooperative, based on their resolvability. They evaluate popular datasets to analyze the types of queries used, highlighting the need for a more controlled approach to query evaluation. By shifting the focus to cooperation in query resolution, the framework provides insights for designing and evaluating natural language interfaces effectively. The study suggests directions for future research to enhance interpretation capabilities and improve the user-system interaction in tabular data analysis.<br /> <div>
arXiv:2511.04584v1 Announce Type: new 
Abstract: Natural language interfaces to tabular data must handle ambiguities inherent to queries. Instead of treating ambiguity as a deficiency, we reframe it as a feature of cooperative interaction, where the responsibility of query specification is shared among the user and the system. We develop a principled framework distinguishing cooperative queries, i.e., queries that yield a resolvable interpretation, from uncooperative queries that cannot be resolved. Applying the framework to evaluations for tabular question answering and analysis, we analyze the queries in 15 popular datasets, and observe an uncontrolled mixing of query types neither adequate for evaluating a system's execution accuracy nor for evaluating interpretation capabilities. Our framework and analysis of queries shifts the perspective from fixing ambiguity to embracing cooperation in resolving queries. This reflection enables more informed design and evaluation for natural language interfaces for tabular data, for which we outline implications and directions for future research.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Question the Questions: Auditing Representation in Online Deliberative Processes</title>
<link>https://arxiv.org/abs/2511.04588</link>
<guid>https://arxiv.org/abs/2511.04588</guid>
<content:encoded><![CDATA[
<div> Keywords: deliberative processes, expert panels, representation, auditing framework, online deliberation platform<br />
Summary: 
- The article introduces an auditing framework for measuring the level of representation in deliberative processes involving expert panels.
- The framework is based on the social choice concept of justified representation (JR) and includes algorithms for auditing JR in the general utility setting.
- The study compares the representativeness of actual questions posed by moderators, participant-chosen questions, and summary questions generated by large language models (LLMs).
- Results demonstrate both the potential and limitations of LLMs in supporting deliberative processes.
- The methods developed in the study have been integrated into an online deliberation platform used in hundreds of deliberations across over 50 countries, allowing practitioners to easily audit and enhance representation in future deliberations.

Summary: <br />
A new auditing framework assesses representation in deliberative processes involving expert panels, using the justified representation concept and algorithms for the general utility setting. Comparisons are made among questions chosen by moderators, participants, and LLMs, revealing the strengths and weaknesses of LLMs in supporting deliberative processes. The integration of these methods into an online platform enables practitioners to audit and enhance representation in future deliberations. <div>
arXiv:2511.04588v1 Announce Type: new 
Abstract: A central feature of many deliberative processes, such as citizens' assemblies and deliberative polls, is the opportunity for participants to engage directly with experts. While participants are typically invited to propose questions for expert panels, only a limited number can be selected due to time constraints. This raises the challenge of how to choose a small set of questions that best represent the interests of all participants. We introduce an auditing framework for measuring the level of representation provided by a slate of questions, based on the social choice concept known as justified representation (JR). We present the first algorithms for auditing JR in the general utility setting, with our most efficient algorithm achieving a runtime of $O(mn\log n)$, where $n$ is the number of participants and $m$ is the number of proposed questions. We apply our auditing methods to historical deliberations, comparing the representativeness of (a) the actual questions posed to the expert panel (chosen by a moderator), (b) participants' questions chosen via integer linear programming, (c) summary questions generated by large language models (LLMs). Our results highlight both the promise and current limitations of LLMs in supporting deliberative processes. By integrating our methods into an online deliberation platform that has been used for over hundreds of deliberations across more than 50 countries, we make it easy for practitioners to audit and improve representation in future deliberations.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for Embodied LLM-Based Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2511.04646</link>
<guid>https://arxiv.org/abs/2511.04646</guid>
<content:encoded><![CDATA[
<div> Cooperative multi-agent planning, decentralized neurosymbolic framework, two-phase negotiation protocol, symbolic planning, dynamic world model<br />
<br />
Summary: <br />
The article introduces DR. WELL, a decentralized neurosymbolic framework for cooperative multi-agent planning. It employs a two-phase negotiation protocol where agents propose roles and commit to joint allocations based on reasoning and environment constraints. Each agent independently generates and executes a symbolic plan based on a shared world model, enabling synchronization and progress without revealing detailed trajectories. By reasoning over symbolic plans, DR. WELL avoids step-level alignment issues and facilitates higher-level operations. Experimental results on cooperative block-push tasks demonstrate that agents adapt, with the dynamic world model capturing reusable patterns and enhancing task completion rates and efficiency. The model continuously refines through negotiation and self-improvement, leading to more efficient collaboration strategies over time. <div>
arXiv:2511.04646v1 Announce Type: new 
Abstract: Cooperative multi-agent planning requires agents to make joint decisions with partial information and limited communication. Coordination at the trajectory level often fails, as small deviations in timing or movement cascade into conflicts. Symbolic planning mitigates this challenge by raising the level of abstraction and providing a minimal vocabulary of actions that enable synchronization and collective progress. We present DR. WELL, a decentralized neurosymbolic framework for cooperative multi-agent planning. Cooperation unfolds through a two-phase negotiation protocol: agents first propose candidate roles with reasoning and then commit to a joint allocation under consensus and environment constraints. After commitment, each agent independently generates and executes a symbolic plan for its role without revealing detailed trajectories. Plans are grounded in execution outcomes via a shared world model that encodes the current state and is updated as agents act. By reasoning over symbolic plans rather than raw trajectories, DR. WELL avoids brittle step-level alignment and enables higher-level operations that are reusable, synchronizable, and interpretable. Experiments on cooperative block-push tasks show that agents adapt across episodes, with the dynamic world model capturing reusable patterns and improving task completion rates and efficiency. Experiments on cooperative block-push tasks show that our dynamic world model improves task completion and efficiency through negotiation and self-refinement, trading a time overhead for evolving, more efficient collaboration strategies.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks</title>
<link>https://arxiv.org/abs/2511.04662</link>
<guid>https://arxiv.org/abs/2511.04662</guid>
<content:encoded><![CDATA[
<div> neuro-symbolic method, logical reasoning, verification, self-reflection, fine-tuning

Summary:
VeriCoT introduces a neuro-symbolic method to extract and verify formal logical arguments from Chain-of-Thought (CoT) reasoning, addressing the issue of trust in high-stakes scenarios. It formalizes CoT reasoning steps into first-order logic and identifies premises for grounding the argument in source context, commonsense knowledge, or prior reasoning steps. The symbolic representation allows automated solvers to verify logical validity, while the use of natural language premises enables identification of ungrounded or fallacious reasoning steps by humans and systems. Experiments on various datasets show VeriCoT effectively identifies flawed reasoning and improves the accuracy of final answers. The verification signal is leveraged for inference-time self-reflection, supervised fine-tuning (SFT), and preference fine-tuning (PFT) with direct preference optimization (DPO), leading to enhanced reasoning validity and accuracy. <div>
arXiv:2511.04662v1 Announce Type: new 
Abstract: LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but they cannot reliably verify their own logic. Even when they reach correct answers, the underlying reasoning may be flawed, undermining trust in high-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a neuro-symbolic method that extracts and verifies formal logical arguments from CoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order logic and identifies premises that ground the argument in source context, commonsense knowledge, or prior reasoning steps. The symbolic representation enables automated solvers to verify logical validity while the NL premises allow humans and systems to identify ungrounded or fallacious reasoning steps. Experiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoT effectively identifies flawed reasoning, and serves as a strong predictor of final answer correctness. We also leverage VeriCoT's verification signal for (1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on VeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct preference optimization (DPO) using verification-based pairwise rewards, further improving reasoning validity and accuracy.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploratory Analysis of Cyberattack Patterns on E-Commerce Platforms Using Statistical Methods</title>
<link>https://arxiv.org/abs/2511.03020</link>
<guid>https://arxiv.org/abs/2511.03020</guid>
<content:encoded><![CDATA[
<div> Keywords: cyberattacks, e-commerce, forecasting, machine learning, risk anticipation

Summary: 
The research presents a hybrid analytical framework integrating statistical modeling and machine learning to detect and forecast cyberattack patterns in e-commerce platforms. Using the Verizon Community Data Breach dataset, the study found that holiday shopping events experience significantly more severe cyberattacks compared to non-holiday periods, confirmed by a Mann-Whitney U test. Seasonal variation in threat severity was examined using ANOVA. Recurrent attack spikes were observed during high-risk periods like Black Friday and holidays, with breaches involving Personally Identifiable Information showing elevated threat indicators. Ensemble machine learning models such as XGBoost, LightGBM, and CatBoost were employed for predictive classification, with CatBoost achieving the highest performance. The framework combines seasonal forecasting with interpretable ensemble learning for temporal risk anticipation and breach-type classification, incorporating ethical considerations and providing insights for proactive cybersecurity resource allocation and future research on real-time threat detection.<br /><br />Summary: <div>
arXiv:2511.03020v1 Announce Type: cross 
Abstract: Cyberattacks on e-commerce platforms have grown in sophistication, threatening consumer trust and operational continuity. This research presents a hybrid analytical framework that integrates statistical modelling and machine learning for detecting and forecasting cyberattack patterns in the e-commerce domain. Using the Verizon Community Data Breach (VCDB) dataset, the study applies Auto ARIMA for temporal forecasting and significance testing, including a Mann-Whitney U test (U = 2579981.5, p = 0.0121), which confirmed that holiday shopping events experienced significantly more severe cyberattacks than non-holiday periods. ANOVA was also used to examine seasonal variation in threat severity, while ensemble machine learning models (XGBoost, LightGBM, and CatBoost) were employed for predictive classification. Results reveal recurrent attack spikes during high-risk periods such as Black Friday and holiday seasons, with breaches involving Personally Identifiable Information (PII) exhibiting elevated threat indicators. Among the models, CatBoost achieved the highest performance (accuracy = 85.29%, F1 score = 0.2254, ROC AUC = 0.8247). The framework uniquely combines seasonal forecasting with interpretable ensemble learning, enabling temporal risk anticipation and breach-type classification. Ethical considerations, including responsible use of sensitive data and bias assessment, were incorporated. Despite class imbalance and reliance on historical data, the study provides insights for proactive cybersecurity resource allocation and outlines directions for future real-time threat detection research.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simulation-Based Validation of an Integrated 4D/5D Digital-Twin Framework for Predictive Construction Control</title>
<link>https://arxiv.org/abs/2511.03684</link>
<guid>https://arxiv.org/abs/2511.03684</guid>
<content:encoded><![CDATA[
<div> Keywords: construction industry, digital-twin framework, AI-based analytics, probabilistic CPM, deep-reinforcement-learning<br />
Summary:<br />
Persistent cost and schedule challenges in the construction industry have led to the development of an integrated 4D/5D digital-twin framework. This framework combines BIM with NLP-based cost mapping, CV-driven progress measurement, Bayesian probabilistic CPM updating, and DRL resource-leveling. A case study on a mid-rise project in Dallas-Fort Worth demonstrated significant improvements in accuracy and efficiency, including reduced labor estimates and overtime, as well as improved project-buffer utilization. The use of AI-based analytics, probabilistic CPM, and DRL enhanced forecasting precision, transparency, and control resilience. The workflow also enabled real-time "what-if" forecasting and traceable cost-schedule alignment through a 5D knowledge graph. Overall, the study shows how integrating advanced technologies can lead to predictive, adaptive, and auditable construction management practices. <br /> <div>
arXiv:2511.03684v1 Announce Type: cross 
Abstract: Persistent cost and schedule deviations remain a major challenge in the U.S. construction industry, revealing the limitations of deterministic CPM and static document-based estimating. This study presents an integrated 4D/5D digital-twin framework that couples Building Information Modeling (BIM) with natural-language processing (NLP)-based cost mapping, computer-vision (CV)-driven progress measurement, Bayesian probabilistic CPM updating, and deep-reinforcement-learning (DRL) resource-leveling. A nine-month case implementation on a Dallas-Fort Worth mid-rise project demonstrated measurable gains in accuracy and efficiency: 43% reduction in estimating labor, 6% reduction in overtime, and 30% project-buffer utilization, while maintaining an on-time finish at 128 days within P50-P80 confidence bounds. The digital-twin sandbox also enabled real-time "what-if" forecasting and traceable cost-schedule alignment through a 5D knowledge graph. Findings confirm that integrating AI-based analytics with probabilistic CPM and DRL enhances forecasting precision, transparency, and control resilience. The validated workflow establishes a practical pathway toward predictive, adaptive, and auditable construction management.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MazeMate: An LLM-Powered Chatbot to Support Computational Thinking in Gamified Programming Learning</title>
<link>https://arxiv.org/abs/2511.03727</link>
<guid>https://arxiv.org/abs/2511.03727</guid>
<content:encoded><![CDATA[
<div> Keywords: Computational Thinking, Large Language Models, Gamified programming, Maze solving, Maze design

Summary:
The article discusses the development and implementation of MazeMate, an LLM-powered chatbot integrated into a 3D Maze programming game aimed at promoting Computational Thinking (CT) skills among students. The study involved 247 undergraduate students who found MazeMate to be moderately helpful, particularly in maze solving. Thematic analysis revealed that MazeMate effectively supported CT processes such as decomposition, abstraction, and algorithmic thinking. However, there were limitations in supporting maze design, including mismatched suggestions and fabricated algorithmic solutions. The research findings highlight the potential of LLM-based scaffolding in cultivating CT skills but also emphasize the need for design enhancements to improve MazeMate's usability in real-world educational settings.<br /><br />Summary: Computational Thinking is fostered by MazeMate, an LLM-powered chatbot in a 3D Maze game. Students found it moderately helpful for maze solving and highlighted support for CT processes. However, limitations in maze design support were identified, emphasizing the need for design refinements. <div>
arXiv:2511.03727v1 Announce Type: cross 
Abstract: Computational Thinking (CT) is a foundational problem-solving skill, and gamified programming environments are a widely adopted approach to cultivating it. While large language models (LLMs) provide on-demand programming support, current applications rarely foster CT development. We present MazeMate, an LLM-powered chatbot embedded in a 3D Maze programming game, designed to deliver adaptive, context-sensitive scaffolds aligned with CT processes in maze solving and maze design. We report on the first classroom implementation with 247 undergraduates. Students rated MazeMate as moderately helpful, with higher perceived usefulness for maze solving than for maze design. Thematic analysis confirmed support for CT processes such as decomposition, abstraction, and algorithmic thinking, while also revealing limitations in supporting maze design, including mismatched suggestions and fabricated algorithmic solutions. These findings demonstrate the potential of LLM-based scaffolding to support CT and underscore directions for design refinement to enhance MazeMate usability in authentic classrooms.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient On-Device Agents via Adaptive Context Management</title>
<link>https://arxiv.org/abs/2511.03728</link>
<guid>https://arxiv.org/abs/2511.03728</guid>
<content:encoded><![CDATA[
<div> Keywords: on-device AI agents, context-efficient, memory optimization, minimal serialization format, just-in-time schema passing<br />
Summary: <br />
The article discusses the challenges faced in deploying on-device AI agents due to limited memory capacity and the trade-off between context richness and feasibility. To address this, a framework for context-efficient on-device agents is proposed, focusing on three key optimizations. Firstly, a dynamic memory system using specialized adapters to distill conversational history into a compressed Context State Object. Secondly, a minimalist serialization format for tool schemas to reduce token overhead. And finally, a just-in-time schema passing mechanism that loads full tool definitions only upon tool selection. By adapting a 3B parameter SLM to context-efficient trajectories, the agent achieves a more than 6-fold reduction in initial system prompt context and a 10- to 25-fold reduction in context growth rate, while matching or exceeding the performance of a conventional baseline on complex user tasks. This highlights the importance of strategic context management in enabling capable and persistent on-device AI agents. <br /> <div>
arXiv:2511.03728v1 Announce Type: cross 
Abstract: On-device AI agents offer the potential for personalized, low-latency assistance, but their deployment is fundamentally constrained by limited memory capacity, which restricts usable context. This reduced practical context window creates a trade-off between supporting rich, stateful interactions with complex tool capabilities and maintaining on-device feasibility. We break this trade-off with a framework for context-efficient on-device agents, driven by three synergistic optimizations (1) a dynamic memory system using specialized LoRA adapters to distill conversational history into a compressed, and structured Context State Object; (2) a minimalist serialization format for tool schemas to minimize token overhead per tool; and (3) a just-in-time schema-passing mechanism that loads full tool definitions only upon tool selection. We instantiate this framework by adapting a 3B parameter SLM to context-efficient trajectories and rigorously evaluate it against a conventional baseline on complex user tasks. Our agent matches, or exceeds, the performance of a conventional baseline while dramatically compressing context, achieving more than a 6-fold reduction in initial system prompt context and a 10- to 25-fold reduction in context growth rate based on the interaction verbosity, demonstrating that strategic context management is key to unlocking capable and persistent on-device AI.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Chat: a Framework for LLMs as Human-Centered Support Systems</title>
<link>https://arxiv.org/abs/2511.03729</link>
<guid>https://arxiv.org/abs/2511.03729</guid>
<content:encoded><![CDATA[
<div> framework, support systems, design principles, evaluation metrics, risks
Summary:
This paper presents a framework for role-based human-centered support systems utilizing large language models (LLMs) for various purposes beyond simple question answering. It discusses the importance of transparency, personalization, guardrails, memory with privacy, empathy, and reliability as key design principles for such systems. The paper also suggests evaluation metrics that go beyond accuracy to include trust, engagement, and long-term outcomes. Risks associated with over-reliance, bias, privacy exposure, and unequal access are highlighted, calling for careful consideration in deploying LLMs. The paper proposes future directions such as unified evaluation, hybrid human-AI models, memory architectures, cross-domain benchmarking, and governance to enhance the responsible integration of LLMs in sensitive environments where guidance and support are needed beyond mere information provision.<br /><br />Summary: <div>
arXiv:2511.03729v1 Announce Type: cross 
Abstract: Large language models are moving beyond transactional question answering to act as companions, coaches, mediators, and curators that scaffold human growth, decision-making, and well-being. This paper proposes a role-based framework for human-centered LLM support systems, compares real deployments across domains, and identifies cross-cutting design principles: transparency, personalization, guardrails, memory with privacy, and a balance of empathy and reliability. It outlines evaluation metrics that extend beyond accuracy to trust, engagement, and longitudinal outcomes. It also analyzes risks including over-reliance, hallucination, bias, privacy exposure, and unequal access, and proposes future directions spanning unified evaluation, hybrid human-AI models, memory architectures, cross-domain benchmarking, and governance. The goal is to support responsible integration of LLMs in sensitive settings where people need accompaniment and guidance, not only answers.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Not All Explanations are Created Equal: Investigating the Pitfalls of Current XAI Evaluation</title>
<link>https://arxiv.org/abs/2511.03730</link>
<guid>https://arxiv.org/abs/2511.03730</guid>
<content:encoded><![CDATA[
<div> Keywords: Explainable Artificial Intelligence, XAI evaluation, user satisfaction, actionable explanations, chess concepts

Summary: 
Explainable Artificial Intelligence (XAI) aims to provide transparent explanations of AI models to users. Current XAI evaluation methods lack generalizability, often relying on ad hoc user surveys. This study challenges the notion that any explanation is better than none, highlighting the need for comprehensive evaluation techniques. The focus should be on actionable explanations that go beyond merely increasing user satisfaction. Through a chess assistant experiment, the study demonstrates the importance of evaluating explanation quality beyond user perception. A call to action is made for improved XAI evaluation methods and the consideration of placebic versus actionable explanations. It emphasizes the need for future research to validate explanation quality to ensure transparency and effectiveness in AI models.<br /><br />Summary: <div>
arXiv:2511.03730v1 Announce Type: cross 
Abstract: Explainable Artificial Intelligence (XAI) aims to create transparency in modern AI models by offering explanations of the models to human users. There are many ways in which researchers have attempted to evaluate the quality of these XAI models, such as user studies or proposed objective metrics like "fidelity". However, these current XAI evaluation techniques are ad hoc at best and not generalizable. Thus, most studies done within this field conduct simple user surveys to analyze the difference between no explanations and those generated by their proposed solution. We do not find this to provide adequate evidence that the explanations generated are of good quality since we believe any kind of explanation will be "better" in most metrics when compared to none at all. Thus, our study looks to highlight this pitfall: most explanations, regardless of quality or correctness, will increase user satisfaction. We also propose that emphasis should be placed on actionable explanations. We demonstrate the validity of both of our claims using an agent assistant to teach chess concepts to users. The results of this chapter will act as a call to action in the field of XAI for more comprehensive evaluation techniques for future research in order to prove explanation quality beyond user satisfaction. Additionally, we present an analysis of the scenarios in which placebic or actionable explanations would be most useful.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MimiTalk: Revolutionizing Qualitative Research with Dual-Agent AI</title>
<link>https://arxiv.org/abs/2511.03731</link>
<guid>https://arxiv.org/abs/2511.03731</guid>
<content:encoded><![CDATA[
<div> Keywords: MimiTalk, dual-agent constitutional AI, conversational data collection, social science research, human-AI collaboration

Summary: 
MimiTalk is a revolutionary dual-agent constitutional AI framework designed for conversational data collection in social science research. It combines a supervisor model for oversight and a conversational model for question generation, enabling scalable and ethical interactions. Usability studies show that MimiTalk reduces interview anxiety and maintains conversational coherence. Comparison studies with human interviews demonstrate that AI interviews outperform in information richness, coherence, and stability. AI interviews excel in eliciting technical insights and candid views on sensitive topics, while human interviews capture cultural and emotional nuances better. The results suggest that dual-agent constitutional AI facilitates effective human-AI collaboration, enabling replicable, scalable, and quality-controlled qualitative research. <div>
arXiv:2511.03731v1 Announce Type: cross 
Abstract: We present MimiTalk, a dual-agent constitutional AI framework designed for scalable and ethical conversational data collection in social science research. The framework integrates a supervisor model for strategic oversight and a conversational model for question generation. We conducted three studies: Study 1 evaluated usability with 20 participants; Study 2 compared 121 AI interviews to 1,271 human interviews from the MediaSum dataset using NLP metrics and propensity score matching; Study 3 involved 10 interdisciplinary researchers conducting both human and AI interviews, followed by blind thematic analysis. Results across studies indicate that MimiTalk reduces interview anxiety, maintains conversational coherence, and outperforms human interviews in information richness, coherence, and stability. AI interviews elicit technical insights and candid views on sensitive topics, while human interviews better capture cultural and emotional nuances. These findings suggest that dual-agent constitutional AI supports effective human-AI collaboration, enabling replicable, scalable and quality-controlled qualitative research.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conversational Collective Intelligence (CCI) using Hyperchat AI in an Authentic Forecasting Task</title>
<link>https://arxiv.org/abs/2511.03732</link>
<guid>https://arxiv.org/abs/2511.03732</guid>
<content:encoded><![CDATA[
<div> AI, Hyperchat, Collective Intelligence, MLB games, forecasting

Summary: 
Hyperchat AI is a technology that facilitates group conversations to improve Collective Intelligence. In a study involving 24 sports fans, groups using Hyperchat AI accurately predicted Major League Baseball game outcomes. The groups' High Confidence predictions outperformed Vegas betting markets, achieving 78% accuracy compared to Vegas' 57%. Betting against the spread would have resulted in a 46% ROI. High Confidence forecasts generated through interactive deliberation had an 88% accuracy rate, highlighting the importance of real-time conversation. This study demonstrates the potential of Hyperchat AI in enhancing group decision-making and forecasting accuracy in a variety of settings. <div>
arXiv:2511.03732v1 Announce Type: cross 
Abstract: Hyperchat AI is a novel agentic technology that enables thoughtful conversations among networked human groups of potentially unlimited size. It allows large teams to discuss complex issues, brainstorm ideas, surface risks, assess alternatives and efficiently converge on optimized solutions that amplify the group's Collective Intelligence (CI). A formal study was conducted to quantify the forecasting accuracy of human groups using Hyperchat AI to conversationally predict the outcome of Major League Baseball (MLB) games. During an 8-week period, networked groups of approximately 24 sports fans were tasked with collaboratively forecasting the winners of 59 baseball games through real-time conversation facilitated by AI agents. The results showed that when debating the games using Hyperchat AI technology, the groups converged on High Confidence predictions that significantly outperformed Vegas betting markets. Specifically, groups were 78% accurate in their High Confidence picks, a statistically strong result vs the Vegas odds of 57% (p=0.020). Had the groups bet against the spread (ATS) on these games, they would have achieved a 46% ROI against Vegas betting markets. In addition, High Confidence forecasts that were generated through above-average conversation rates were 88% accurate, suggesting that real-time interactive deliberation is central to amplified accuracy.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A convolutional neural network deep learning method for model class selection</title>
<link>https://arxiv.org/abs/2511.03743</link>
<guid>https://arxiv.org/abs/2511.03743</guid>
<content:encoded><![CDATA[
<div> novel deep convolutional neural network, model class selection, response-only, structural health monitoring, Kalman filter<br />
Summary:<br />
This study investigates the capability of a novel deep convolutional neural network in selecting model classes using only response information. By training and validating a one-dimensional CNN with response signals and class information, the network can identify the model class of new signals without requiring input data or full system identification. The study also explores an optional physics-based enhancement using the Kalman filter to integrate response signals according to kinematic constraints. The method proves effective in identifying model classes in slight variations caused by damping or hysteresis in linear and nonlinear dynamic systems, as well as in a 3D building finite element model. These findings demonstrate the potential of the proposed approach for structural health monitoring applications. <br /><br />Summary: <div>
arXiv:2511.03743v1 Announce Type: cross 
Abstract: The response-only model class selection capability of a novel deep convolutional neural network method is examined herein in a simple, yet effective, manner. Specifically, the responses from a unique degree of freedom along with their class information train and validate a one-dimensional convolutional neural network. In doing so, the network selects the model class of new and unlabeled signals without the need of the system input information, or full system identification. An optional physics-based algorithm enhancement is also examined using the Kalman filter to fuse the system response signals using the kinematics constraints of the acceleration and displacement data. Importantly, the method is shown to select the model class in slight signal variations attributed to the damping behavior or hysteresis behavior on both linear and nonlinear dynamic systems, as well as on a 3D building finite element model, providing a powerful tool for structural health monitoring applications.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenMENA: An Open-Source Memristor Interfacing and Compute Board for Neuromorphic Edge-AI Applications</title>
<link>https://arxiv.org/abs/2511.03747</link>
<guid>https://arxiv.org/abs/2511.03747</guid>
<content:encoded><![CDATA[
<div> memristive crossbars, Open-MENA, edge AI, reproducible hardware interface, in-memory accelerate<br />
Summary:<br />
Open-MENA is an open memristor interfacing system designed for energy-efficient edge AI. It integrates hardware interface for memristor crossbars, firmware-software stack with high-level APIs, and a Voltage-Incremental Proportional-Integral method for weight programming and fine-tuning. The system is validated on digit recognition and robot obstacle-avoidance tasks, demonstrating its ability to transfer weights and adapt on-device. Open-MENA offers a path to in-memory multiply-accumulate and local plasticity learning, providing a solution for energy-efficient edge AI. It is the first fully open system of its kind and is released as open source to democratize memristor-enabled edge-AI research. <div>
arXiv:2511.03747v1 Announce Type: cross 
Abstract: Memristive crossbars enable in-memory multiply-accumulate and local plasticity learning, offering a path to energy-efficient edge AI. To this end, we present Open-MENA (Open Memristor-in-Memory Accelerator), which, to our knowledge, is the first fully open memristor interfacing system integrating (i) a reproducible hardware interface for memristor crossbars with mixed-signal read-program-verify loops; (ii) a firmware-software stack with high-level APIs for inference and on-device learning; and (iii) a Voltage-Incremental Proportional-Integral (VIPI) method to program pre-trained weights into analog conductances, followed by chip-in-the-loop fine-tuning to mitigate device non-idealities. OpenMENA is validated on digit recognition, demonstrating the flow from weight transfer to on-device adaptation, and on a real-world robot obstacle-avoidance task, where the memristor-based model learns to map localization inputs to motor commands. OpenMENA is released as open source to democratize memristor-enabled edge-AI research.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Applying Time Series Deep Learning Models to Forecast the Growth of Perennial Ryegrass in Ireland</title>
<link>https://arxiv.org/abs/2511.03749</link>
<guid>https://arxiv.org/abs/2511.03749</guid>
<content:encoded><![CDATA[
<div> Keywords: grasslands, dairy sector, deep learning, forecasting, sustainability

Summary: 
Grasslands are a crucial part of the global carbon cycle and biodiversity, with the Irish dairy sector facing challenges in sustainability and profitability. Traditional grass growth forecasting models are impractical, leading researchers to propose deep learning models tailored for univariate datasets. A temporal convolutional network specifically designed for forecasting Perennial Ryegrass growth in Cork demonstrates high performance with RMSE of 2.74 and MAE of 3.46, using historical grass height data. Validation across a extensive dataset spanning 1,757 weeks over 34 years offers insights into optimal model configurations, enhancing our understanding of model behavior. This study improves the reliability of grass growth forecasting and contributes to advancing sustainable dairy farming practices. 

<br /><br />Summary: <div>
arXiv:2511.03749v1 Announce Type: cross 
Abstract: Grasslands, constituting the world's second-largest terrestrial carbon sink, play a crucial role in biodiversity and the regulation of the carbon cycle. Currently, the Irish dairy sector, a significant economic contributor, grapples with challenges related to profitability and sustainability. Presently, grass growth forecasting relies on impractical mechanistic models. In response, we propose deep learning models tailored for univariate datasets, presenting cost-effective alternatives. Notably, a temporal convolutional network designed for forecasting Perennial Ryegrass growth in Cork exhibits high performance, leveraging historical grass height data with RMSE of 2.74 and MAE of 3.46. Validation across a comprehensive dataset spanning 1,757 weeks over 34 years provides insights into optimal model configurations. This study enhances our understanding of model behavior, thereby improving reliability in grass growth forecasting and contributing to the advancement of sustainable dairy farming practices.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Learning with Gramian Angular Fields for Privacy-Preserving ECG Classification on Heterogeneous IoT Devices</title>
<link>https://arxiv.org/abs/2511.03753</link>
<guid>https://arxiv.org/abs/2511.03753</guid>
<content:encoded><![CDATA[
<div> Keywords: federated learning, electrocardiogram, privacy-preserving, IoT, healthcare

Summary:
The study introduces a federated learning framework for ECG classification in IoT healthcare, using GAF images for feature extraction. The approach ensures data privacy by keeping sensitive medical data local to each device. Experimental validation across varied IoT devices demonstrates the efficacy of the FL-GAF model, achieving a high accuracy of 95.18% in multi-client setups. This outperforms a single-client baseline in accuracy and training time. Despite increased computational complexity, the framework maintains efficient resource utilization and communication overhead. The results suggest the potential of lightweight, privacy-preserving AI for IoT healthcare monitoring, facilitating scalable and secure edge deployments in smart health systems. <div>
arXiv:2511.03753v1 Announce Type: cross 
Abstract: This study presents a federated learning (FL) framework for privacy-preserving electrocardiogram (ECG) classification in Internet of Things (IoT) healthcare environments. By transforming 1D ECG signals into 2D Gramian Angular Field (GAF) images, the proposed approach enables efficient feature extraction through Convolutional Neural Networks (CNNs) while ensuring that sensitive medical data remain local to each device. This work is among the first to experimentally validate GAF-based federated ECG classification across heterogeneous IoT devices, quantifying both performance and communication efficiency. To evaluate feasibility in realistic IoT settings, we deployed the framework across a server, a laptop, and a resource-constrained Raspberry Pi 4, reflecting edge-cloud integration in IoT ecosystems. Experimental results demonstrate that the FL-GAF model achieves a high classification accuracy of 95.18% in a multi-client setup, significantly outperforming a single-client baseline in both accuracy and training time. Despite the added computational complexity of GAF transformations, the framework maintains efficient resource utilization and communication overhead. These findings highlight the potential of lightweight, privacy-preserving AI for IoT-based healthcare monitoring, supporting scalable and secure edge deployments in smart health systems.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Laugh, Relate, Engage: Stylized Comment Generation for Short Videos</title>
<link>https://arxiv.org/abs/2511.03757</link>
<guid>https://arxiv.org/abs/2511.03757</guid>
<content:encoded><![CDATA[
<div> Keywords: short-video platforms, comment generation, multi-agent system, style control, user engagement<br />
Summary:<br />
The article introduces LOLGORITHM, a multi-agent system for generating diverse and contextually aware comments on short-video platforms. It addresses the challenge of creating compliant yet stylistically varied comments by integrating video segmentation, contextual analysis, and style-aware prompt construction. LOLGORITHM supports six comment styles including puns, rhyming, and sarcasm, powered by a large language model. A bilingual dataset from Douyin and YouTube is used for development and evaluation across popular video genres. Evaluation metrics and a human preference study show that LOLGORITHM outperforms baseline models with high preference rates. The system offers a scalable and culturally adaptive framework for enhancing user engagement and creative interaction on short-video platforms.<br /><br />Summary: <div>
arXiv:2511.03757v1 Announce Type: cross 
Abstract: Short-video platforms have become a central medium in the modern Internet landscape, where efficient information delivery and strong interactivity are reshaping user engagement and cultural dissemination. Among the various forms of user interaction, comments play a vital role in fostering community participation and enabling content re-creation. However, generating comments that are both compliant with platform guidelines and capable of exhibiting stylistic diversity and contextual awareness remains a significant challenge. We introduce LOLGORITHM, a modular multi-agent system (MAS) designed for controllable short-video comment generation. The system integrates video segmentation, contextual and affective analysis, and style-aware prompt construction. It supports six distinct comment styles: puns (homophones), rhyming, meme application, sarcasm (irony), plain humor, and content extraction. Powered by a multimodal large language model (MLLM), LOLGORITHM directly processes video inputs and achieves fine-grained style control through explicit prompt markers and few-shot examples. To support development and evaluation, we construct a bilingual dataset using official APIs from Douyin (Chinese) and YouTube (English), covering five popular video genres: comedy skits, daily life jokes, funny animal clips, humorous commentary, and talk shows. Evaluation combines automated metrics originality, relevance, and style conformity with a large-scale human preference study involving 40 videos and 105 participants. Results show that LOLGORITHM significantly outperforms baseline models, achieving preference rates of over 90% on Douyin and 87.55% on YouTube. This work presents a scalable and culturally adaptive framework for stylized comment generation on short-video platforms, offering a promising path to enhance user engagement and creative interaction.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging LLM-based agents for social science research: insights from citation network simulations</title>
<link>https://arxiv.org/abs/2511.03758</link>
<guid>https://arxiv.org/abs/2511.03758</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, CiteAgent framework, citation networks, LLM-SE, LLM-LE<br />
<br />
Summary: 
The article explores the potential of Large Language Models (LLMs) in social simulation through the CiteAgent framework, which generates realistic citation networks based on human behavior simulation with LLM-based agents. The framework successfully replicates key characteristics of real-world citation networks, such as power-law distribution and citational distortion. Two research paradigms, LLM-SE and LLM-LE, are introduced to analyze and challenge existing theories in social science using LLMs. These paradigms enable rigorous analyses of citation network phenomena and extend the scope of science of science studies. The simulation experiment results provide valuable insights for academic environments, showcasing the potential of LLMs in advancing research in social science.<br /><br /> <div>
arXiv:2511.03758v1 Announce Type: cross 
Abstract: The emergence of Large Language Models (LLMs) demonstrates their potential to encapsulate the logic and patterns inherent in human behavior simulation by leveraging extensive web data pre-training. However, the boundaries of LLM capabilities in social simulation remain unclear. To further explore the social attributes of LLMs, we introduce the CiteAgent framework, designed to generate citation networks based on human-behavior simulation with LLM-based agents. CiteAgent successfully captures predominant phenomena in real-world citation networks, including power-law distribution, citational distortion, and shrinking diameter. Building on this realistic simulation, we establish two LLM-based research paradigms in social science: LLM-SE (LLM-based Survey Experiment) and LLM-LE (LLM-based Laboratory Experiment). These paradigms facilitate rigorous analyses of citation network phenomena, allowing us to validate and challenge existing theories. Additionally, we extend the research scope of traditional science of science studies through idealized social experiments, with the simulation experiment results providing valuable insights for real-world academic environments. Our work demonstrates the potential of LLMs for advancing science of science research in social science.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OptiMA: A Transaction-Based Framework with Throughput Optimization for Very Complex Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2511.03761</link>
<guid>https://arxiv.org/abs/2511.03761</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent systems, complex models, faults, performance bottlenecks, transaction scheduling

Summary: 
The research on multi-agent systems has been focused on developing larger and more complex models for sophisticated tasks. However, this increased complexity can lead to vulnerabilities to faults and performance bottlenecks. To address these issues, a transaction-based framework called the OptiMA framework has been proposed. This framework is designed to handle very complex multi-agent systems (VCMAS) and integrates transaction scheduling to improve performance. The implementation of the OptiMA framework has shown promising results, with the ability to execute VCMAS with over a hundred agents and significant performance improvements of up to 16%. Additionally, a theoretical analysis of transaction scheduling has been conducted, providing practical tools for future research in this area. This approach offers a promising solution to the challenges posed by complex multi-agent systems. 

<br /><br />Summary: <div>
arXiv:2511.03761v1 Announce Type: cross 
Abstract: In recent years, the research of multi-agent systems has taken a direction to explore larger and more complex models to fulfill sophisticated tasks. We point out two possible pitfalls that might be caused by increasing complexity; susceptibilities to faults, and performance bottlenecks. To prevent the former threat, we propose a transaction-based framework to design very complex multi-agent systems (VCMAS). To address the second threat, we offer to integrate transaction scheduling into the proposed framework. We implemented both of these ideas to develop the OptiMA framework and show that it is able to facilitate the execution of VCMAS with more than a hundred agents. We also demonstrate the effect of transaction scheduling on such a system by showing improvements up to more than 16\%. Furthermore, we also performed a theoretical analysis on the transaction scheduling problem and provided practical tools that can be used for future research on it.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Climbing the label tree: Hierarchy-preserving contrastive learning for medical imaging</title>
<link>https://arxiv.org/abs/2511.03771</link>
<guid>https://arxiv.org/abs/2511.03771</guid>
<content:encoded><![CDATA[
<div> Keywords: medical image labels, hierarchy-preserving contrastive framework, self-supervised learning, taxonomy, representation quality

Summary:
The article introduces a hierarchy-preserving contrastive framework for medical image labels organized by taxonomies. The framework incorporates two plug-in objectives: Hierarchy-Weighted Contrastive (HWC) and Level-Aware Margin (LAM) to promote coherence within parent groups and separate ancestor groups across levels, respectively. This formulation is independent of geometry and can be applied to Euclidean and hyperbolic embeddings without architectural changes. The proposed objectives consistently improve representation quality over strong SSL baselines while respecting the taxonomy structure. Evaluation metrics such as HF1, H-Acc, and parent-distance violation rate are used to assess hierarchy faithfulness. Ablations demonstrate the effectiveness of HWC and LAM, even without curvature, and combining them produces the most taxonomy-aligned representations. This research presents a simple yet powerful method for learning medical image representations that advance both performance and interpretability in hierarchy-rich domains.<br /><br />Summary: The article introduces a hierarchy-preserving contrastive framework for medical image labels organized by taxonomies. The framework incorporates two plug-in objectives: Hierarchy-Weighted Contrastive (HWC) and Level-Aware Margin (LAM) to promote coherence within parent groups and separate ancestor groups across levels, respectively. This formulation is independent of geometry and can be applied to Euclidean and hyperbolic embeddings without architectural changes. The proposed objectives consistently improve representation quality over strong SSL baselines while respecting the taxonomy structure. Evaluation metrics such as HF1, H-Acc, and parent-distance violation rate are used to assess hierarchy faithfulness. Ablations demonstrate the effectiveness of HWC and LAM, even without curvature, and combining them produces the most taxonomy-aligned representations. This research presents a simple yet powerful method for learning medical image representations that advance both performance and interpretability in hierarchy-rich domains. <div>
arXiv:2511.03771v1 Announce Type: cross 
Abstract: Medical image labels are often organized by taxonomies (e.g., organ - tissue - subtype), yet standard self-supervised learning (SSL) ignores this structure. We present a hierarchy-preserving contrastive framework that makes the label tree a first-class training signal and an evaluation target. Our approach introduces two plug-in objectives: Hierarchy-Weighted Contrastive (HWC), which scales positive/negative pair strengths by shared ancestors to promote within-parent coherence, and Level-Aware Margin (LAM), a prototype margin that separates ancestor groups across levels. The formulation is geometry-agnostic and applies to Euclidean and hyperbolic embeddings without architectural changes. Across several benchmarks, including breast histopathology, the proposed objectives consistently improve representation quality over strong SSL baselines while better respecting the taxonomy. We evaluate with metrics tailored to hierarchy faithfulness: HF1 (hierarchical F1), H-Acc (tree-distance-weighted accuracy), and parent-distance violation rate. We also report top-1 accuracy for completeness. Ablations show that HWC and LAM are effective even without curvature, and combining them yields the most taxonomy-aligned representations. Taken together, these results provide a simple, general recipe for learning medical image representations that respect the label tree and advance both performance and interpretability in hierarchy-rich domains.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Expert Evaluation of LLM World Models: A High-$T_c$ Superconductivity Case Study</title>
<link>https://arxiv.org/abs/2511.03782</link>
<guid>https://arxiv.org/abs/2511.03782</guid>
<content:encoded><![CDATA[
<div> evaluate, Large Language Models, high-temperature cuprates, expert-curated database, retrieval-augmented generation<br />
Summary:<br />
The study evaluates the performance of Large Language Models (LLMs) in understanding scientific literature using high-temperature cuprates as an example. An expert-curated database and 67 expert-formulated questions are used to test the LLMs. Six different systems, including a custom retrieval-augmented generation (RAG) system, are evaluated by experts based on criteria such as balanced perspectives, factual comprehensiveness, succinctness, and evidentiary support. The RAG systems outperformed existing closed models in providing comprehensive and well-supported answers. The study highlights both promising aspects and critical shortcomings of LLM performances, emphasizing the importance of expert-level evaluation for LLM-based reasoning systems. The expert-formulated questions and rubric developed in the study can be valuable tools for assessing the performance of LLM systems. <br /> <div>
arXiv:2511.03782v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) show great promise as a powerful tool for scientific literature exploration. However, their effectiveness in providing scientifically accurate and comprehensive answers to complex questions within specialized domains remains an active area of research. Using the field of high-temperature cuprates as an exemplar, we evaluate the ability of LLM systems to understand the literature at the level of an expert. We construct an expert-curated database of 1,726 scientific papers that covers the history of the field, and a set of 67 expert-formulated questions that probe deep understanding of the literature. We then evaluate six different LLM-based systems for answering these questions, including both commercially available closed models and a custom retrieval-augmented generation (RAG) system capable of retrieving images alongside text. Experts then evaluate the answers of these systems against a rubric that assesses balanced perspectives, factual comprehensiveness, succinctness, and evidentiary support. Among the six systems two using RAG on curated literature outperformed existing closed models across key metrics, particularly in providing comprehensive and well-supported answers. We discuss promising aspects of LLM performances as well as critical short-comings of all the models. The set of expert-formulated questions and the rubric will be valuable for assessing expert level performance of LLM based reasoning systems.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Reasoning Efficiency through Prompt Difficulty Prediction</title>
<link>https://arxiv.org/abs/2511.03808</link>
<guid>https://arxiv.org/abs/2511.03808</guid>
<content:encoded><![CDATA[
<div> language models, reasoning, routing, problem difficulty, model correctness<br />
<br />
Summary: 
The article discusses a routing approach to improve the efficiency of reasoning language models by assigning each problem to the smallest model capable of solving it. By training lightweight predictors using intermediate representations, the approach guides routing across a pool of reasoning models based on problem difficulty or model correctness. This method helps reduce compute costs without compromising accuracy on diverse math benchmarks. The results show that difficulty-aware routing is effective in deploying reasoning models cost-efficiently, outperforming random assignment and matching the performance of larger models while using significantly less compute. This approach addresses the challenge of deploying large reasoning models by optimizing compute resources based on problem complexity and model capabilities. <div>
arXiv:2511.03808v1 Announce Type: cross 
Abstract: Reasoning language models perform well on complex tasks but are costly to deploy due to their size and long reasoning traces. We propose a routing approach that assigns each problem to the smallest model likely to solve it, reducing compute without sacrificing accuracy. Using intermediate representations from s1.1-32B, we train lightweight predictors of problem difficulty or model correctness to guide routing across a pool of reasoning models. On diverse math benchmarks, routing improves efficiency over random assignment and matches s1.1-32B's performance while using significantly less compute. Our results demonstrate that difficulty-aware routing is effective for cost-efficient deployment of reasoning models.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PLLuM: A Family of Polish Large Language Models</title>
<link>https://arxiv.org/abs/2511.03823</link>
<guid>https://arxiv.org/abs/2511.03823</guid>
<content:encoded><![CDATA[
arXiv:2511.03823v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) play a central role in modern artificial intelligence, yet their development has been primarily focused on English, resulting in limited support for other languages. We present PLLuM (Polish Large Language Model), the largest open-source family of foundation models tailored specifically for the Polish language. Developed by a consortium of major Polish research institutions, PLLuM addresses the need for high-quality, transparent, and culturally relevant language models beyond the English-centric commercial landscape. We describe the development process, including the construction of a new 140-billion-token Polish text corpus for pre-training, a 77k custom instructions dataset, and a 100k preference optimization dataset. A key component is a Responsible AI framework that incorporates strict data governance and a hybrid module for output correction and safety filtering. We detail the models' architecture, training procedures, and alignment techniques for both base and instruction-tuned variants, and demonstrate their utility in a downstream task within public administration. By releasing these models publicly, PLLuM aims to foster open research and strengthen sovereign AI technologies in Poland.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CORE - A Cell-Level Coarse-to-Fine Image Registration Engine for Multi-stain Image Alignment</title>
<link>https://arxiv.org/abs/2511.03826</link>
<guid>https://arxiv.org/abs/2511.03826</guid>
<content:encoded><![CDATA[
arXiv:2511.03826v1 Announce Type: cross 
Abstract: Accurate and efficient registration of whole slide images (WSIs) is essential for high-resolution, nuclei-level analysis in multi-stained tissue slides. We propose a novel coarse-to-fine framework CORE for accurate nuclei-level registration across diverse multimodal whole-slide image (WSI) datasets. The coarse registration stage leverages prompt-based tissue mask extraction to effectively filter out artefacts and non-tissue regions, followed by global alignment using tissue morphology and ac- celerated dense feature matching with a pre-trained feature extractor. From the coarsely aligned slides, nuclei centroids are detected and subjected to fine-grained rigid registration using a custom, shape-aware point-set registration model. Finally, non-rigid alignment at the cellular level is achieved by estimating a non-linear dis- placement field using Coherent Point Drift (CPD). Our approach benefits from automatically generated nuclei that enhance the accuracy of deformable registra- tion and ensure precise nuclei-level correspondence across modalities. The pro- posed model is evaluated on three publicly available WSI registration datasets, and two private datasets. We show that CORE outperforms current state-of-the-art methods in terms of generalisability, precision, and robustness in bright-field and immunofluorescence microscopy WSIs
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Noise Injection: Improving Out-of-Distribution Generalization for Limited Size Datasets</title>
<link>https://arxiv.org/abs/2511.03855</link>
<guid>https://arxiv.org/abs/2511.03855</guid>
<content:encoded><![CDATA[
arXiv:2511.03855v1 Announce Type: cross 
Abstract: Deep learned (DL) models for image recognition have been shown to fail to generalize to data from different devices, populations, etc. COVID-19 detection from Chest X-rays (CXRs), in particular, has been shown to fail to generalize to out-of-distribution (OOD) data from new clinical sources not covered in the training set. This occurs because models learn to exploit shortcuts - source-specific artifacts that do not translate to new distributions - rather than reasonable biomarkers to maximize performance on in-distribution (ID) data. Rendering the models more robust to distribution shifts, our study investigates the use of fundamental noise injection techniques (Gaussian, Speckle, Poisson, and Salt and Pepper) during training. Our empirical results demonstrate that this technique can significantly reduce the performance gap between ID and OOD evaluation from 0.10-0.20 to 0.01-0.06, based on results averaged over ten random seeds across key metrics such as AUC, F1, accuracy, recall and specificity. Our source code is publicly available at https://github.com/Duongmai127/Noisy-ood
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Levers of Power in the Field of AI</title>
<link>https://arxiv.org/abs/2511.03859</link>
<guid>https://arxiv.org/abs/2511.03859</guid>
<content:encoded><![CDATA[
arXiv:2511.03859v1 Announce Type: cross 
Abstract: This paper examines how decision makers in academia, government, business, and civil society navigate questions of power in implementations of artificial intelligence. The study explores how individuals experience and exercise levers of power, which are presented as social mechanisms that shape institutional responses to technological change. The study reports on the responses of personalized questionnaires designed to gather insight on a decision maker's institutional purview, based on an institutional governance framework developed from the work of Neo-institutionalists. Findings present the anonymized, real responses and circumstances of respondents in the form of twelve fictional personas of high-level decision makers from North America and Europe. These personas illustrate how personal agency, organizational logics, and institutional infrastructures may intersect in the governance of AI. The decision makers' responses to the questionnaires then inform a discussion of the field-level personal power of decision makers, methods of fostering institutional stability in times of change, and methods of influencing institutional change in the field of AI. The final section of the discussion presents a table of the dynamics of the levers of power in the field of AI for change makers and five testable hypotheses for institutional and social movement researchers. In summary, this study provides insight on the means for policymakers within institutions and their counterparts in civil society to personally engage with AI governance.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OMPILOT: Harnessing Transformer Models for Auto Parallelization to Shared Memory Computing Paradigms</title>
<link>https://arxiv.org/abs/2511.03866</link>
<guid>https://arxiv.org/abs/2511.03866</guid>
<content:encoded><![CDATA[
arXiv:2511.03866v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have significantly accelerated progress in code translation, enabling more accurate and efficient transformation across programming languages. While originally developed for natural language processing, LLMs have shown strong capabilities in modeling programming language syntax and semantics, outperforming traditional rule-based systems in both accuracy and flexibility. These models have streamlined cross-language conversion, reduced development overhead, and accelerated legacy code migration. In this paper, we introduce OMPILOT, a novel domain-specific encoder-decoder transformer tailored for translating C++ code into OpenMP, enabling effective shared-memory parallelization. OMPILOT leverages custom pre-training objectives that incorporate the semantics of parallel constructs and combines both unsupervised and supervised learning strategies to improve code translation robustness. Unlike previous work that focused primarily on loop-level transformations, OMPILOT operates at the function level to capture a wider semantic context. To evaluate our approach, we propose OMPBLEU, a novel composite metric specifically crafted to assess the correctness and quality of OpenMP parallel constructs, addressing limitations in conventional translation metrics.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Investigating Robot Control Policy Learning for Autonomous X-ray-guided Spine Procedures</title>
<link>https://arxiv.org/abs/2511.03882</link>
<guid>https://arxiv.org/abs/2511.03882</guid>
<content:encoded><![CDATA[
arXiv:2511.03882v1 Announce Type: cross 
Abstract: Imitation learning-based robot control policies are enjoying renewed interest in video-based robotics. However, it remains unclear whether this approach applies to X-ray-guided procedures, such as spine instrumentation. This is because interpretation of multi-view X-rays is complex. We examine opportunities and challenges for imitation policy learning in bi-plane-guided cannula insertion. We develop an in silico sandbox for scalable, automated simulation of X-ray-guided spine procedures with a high degree of realism. We curate a dataset of correct trajectories and corresponding bi-planar X-ray sequences that emulate the stepwise alignment of providers. We then train imitation learning policies for planning and open-loop control that iteratively align a cannula solely based on visual information. This precisely controlled setup offers insights into limitations and capabilities of this method. Our policy succeeded on the first attempt in 68.5% of cases, maintaining safe intra-pedicular trajectories across diverse vertebral levels. The policy generalized to complex anatomy, including fractures, and remained robust to varied initializations. Rollouts on real bi-planar X-rays further suggest that the model can produce plausible trajectories, despite training exclusively in simulation. While these preliminary results are promising, we also identify limitations, especially in entry point precision. Full closed-look control will require additional considerations around how to provide sufficiently frequent feedback. With more robust priors and domain knowledge, such models may provide a foundation for future efforts toward lightweight and CT-free robotic intra-operative spinal navigation.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Diagnostic Performance on Small and Imbalanced Datasets Using Class-Based Input Image Composition</title>
<link>https://arxiv.org/abs/2511.03891</link>
<guid>https://arxiv.org/abs/2511.03891</guid>
<content:encoded><![CDATA[
arXiv:2511.03891v1 Announce Type: cross 
Abstract: Small, imbalanced datasets and poor input image quality can lead to high false predictions rates with deep learning models. This paper introduces Class-Based Image Composition, an approach that allows us to reformulate training inputs through a fusion of multiple images of the same class into combined visual composites, named Composite Input Images (CoImg). That enhances the intra-class variance and improves the valuable information density per training sample and increases the ability of the model to distinguish between subtle disease patterns. Our method was evaluated on the Optical Coherence Tomography Dataset for Image-Based Deep Learning Methods (OCTDL) (Kulyabin et al., 2024), which contains 2,064 high-resolution optical coherence tomography (OCT) scans of the human retina, representing seven distinct diseases with a significant class imbalance. We constructed a perfectly class-balanced version of this dataset, named Co-OCTDL, where each scan is resented as a 3x1 layout composite image. To assess the effectiveness of this new representation, we conducted a comparative analysis between the original dataset and its variant using a VGG16 model. A fair comparison was ensured by utilizing the identical model architecture and hyperparameters for all experiments. The proposed approach markedly improved diagnostic results.The enhanced Dataset achieved near-perfect accuracy (99.6%) with F1-score (0.995) and AUC (0.9996), compared to a baseline model trained on raw dataset. The false prediction rate was also significantly lower, this demonstrates that the method can producehigh-quality predictions even for weak datasets affected by class imbalance or small sample size.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Secure Code Generation at Scale with Reflexion</title>
<link>https://arxiv.org/abs/2511.03898</link>
<guid>https://arxiv.org/abs/2511.03898</guid>
<content:encoded><![CDATA[
arXiv:2511.03898v1 Announce Type: cross 
Abstract: Large language models (LLMs) are now widely used to draft and refactor code, but code that works is not necessarily secure. We evaluate secure code generation using the Instruct Prime, which eliminated compliance-required prompts and cue contamination, and evaluate five instruction-tuned code LLMs using a zero-shot baseline and a three-round reflexion prompting approach. Security is measured using the Insecure Code Detector (ICD), and results are reported by measuring Repair, Regression, and NetGain metrics, considering the programming language and CWE family. Our findings show that insecurity remains common at the first round: roughly 25-33% of programs are insecure at a zero-shot baseline (t0 ). Weak cryptography/config-dependent bugs are the hardest to avoid while templated ones like XSS, code injection, and hard-coded secrets are handled more reliably. Python yields the highest secure rates; C and C# are the lowest, with Java, JS, PHP, and C++ in the middle. Reflexion prompting improves security for all models, improving average accuracy from 70.74% at t0 to 79.43% at t3 , with the largest gains in the first round followed by diminishing returns. The trends with Repair, Regression, and NetGain metrics show that applying one to two rounds produces most of the benefits. A replication package is available at https://doi.org/10.5281/zenodo.17065846.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SnappyMeal: Design and Longitudinal Evaluation of a Multimodal AI Food Logging Application</title>
<link>https://arxiv.org/abs/2511.03907</link>
<guid>https://arxiv.org/abs/2511.03907</guid>
<content:encoded><![CDATA[
arXiv:2511.03907v1 Announce Type: cross 
Abstract: Food logging, both self-directed and prescribed, plays a critical role in uncovering correlations between diet, medical, fitness, and health outcomes. Through conversations with nutritional experts and individuals who practice dietary tracking, we find current logging methods, such as handwritten and app-based journaling, are inflexible and result in low adherence and potentially inaccurate nutritional summaries. These findings, corroborated by prior literature, emphasize the urgent need for improved food logging methods. In response, we propose SnappyMeal, an AI-powered dietary tracking system that leverages multimodal inputs to enable users to more flexibly log their food intake. SnappyMeal introduces goal-dependent follow-up questions to intelligently seek missing context from the user and information retrieval from user grocery receipts and nutritional databases to improve accuracy. We evaluate SnappyMeal through publicly available nutrition benchmarks and a multi-user, 3-week, in-the-wild deployment capturing over 500 logged food instances. Users strongly praised the multiple available input methods and reported a strong perceived accuracy. These insights suggest that multimodal AI systems can be leveraged to significantly improve dietary tracking flexibility and context-awareness, laying the groundwork for a new class of intelligent self-tracking applications.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>I Detect What I Don't Know: Incremental Anomaly Learning with Stochastic Weight Averaging-Gaussian for Oracle-Free Medical Imaging</title>
<link>https://arxiv.org/abs/2511.03912</link>
<guid>https://arxiv.org/abs/2511.03912</guid>
<content:encoded><![CDATA[
arXiv:2511.03912v1 Announce Type: cross 
Abstract: Unknown anomaly detection in medical imaging remains a fundamental challenge due to the scarcity of labeled anomalies and the high cost of expert supervision. We introduce an unsupervised, oracle-free framework that incrementally expands a trusted set of normal samples without any anomaly labels. Starting from a small, verified seed of normal images, our method alternates between lightweight adapter updates and uncertainty-gated sample admission. A frozen pretrained vision backbone is augmented with tiny convolutional adapters, ensuring rapid domain adaptation with negligible computational overhead. Extracted embeddings are stored in a compact coreset enabling efficient k-nearest neighbor anomaly (k-NN) scoring. Safety during incremental expansion is enforced by dual probabilistic gates, a sample is admitted into the normal memory only if its distance to the existing coreset lies within a calibrated z-score threshold, and its SWAG-based epistemic uncertainty remains below a seed-calibrated bound. This mechanism prevents drift and false inclusions without relying on generative reconstruction or replay buffers. Empirically, our system steadily refines the notion of normality as unlabeled data arrive, producing substantial gains over baselines. On COVID-CXR, ROC-AUC improves from 0.9489 to 0.9982 (F1: 0.8048 to 0.9746); on Pneumonia CXR, ROC-AUC rises from 0.6834 to 0.8968; and on Brain MRI ND-5, ROC-AUC increases from 0.6041 to 0.7269 and PR-AUC from 0.7539 to 0.8211. These results highlight the effectiveness and efficiency of the proposed framework for real-world, label-scarce medical imaging applications.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evolutionary Optimization Trumps Adam Optimization on Embedding Space Exploration</title>
<link>https://arxiv.org/abs/2511.03913</link>
<guid>https://arxiv.org/abs/2511.03913</guid>
<content:encoded><![CDATA[
arXiv:2511.03913v1 Announce Type: cross 
Abstract: Deep generative models, especially diffusion architectures, have transformed image generation; however, they are challenging to control and optimize for specific goals without expensive retraining. Embedding Space Exploration, especially with Evolutionary Algorithms (EAs), has been shown to be a promising method for optimizing image generation, particularly within Diffusion Models. Therefore, in this work, we study the performance of an evolutionary optimization method, namely Separable Covariance Matrix Adaptation Evolution Strategy (sep-CMA-ES), against the widely adopted Adaptive Moment Estimation (Adam), applied to Stable Diffusion XL Turbo's prompt embedding vector. The evaluation of images combines the LAION Aesthetic Predictor V2 with CLIPScore into a weighted fitness function, allowing flexible trade-offs between visual appeal and adherence to prompts. Experiments on a subset of the Parti Prompts (P2) dataset showcase that sep-CMA-ES consistently yields superior improvements in aesthetic and alignment metrics in comparison to Adam. Results indicate that the evolutionary method provides efficient, gradient-free optimization for diffusion models, enhancing controllability without the need for fine-tuning. This study emphasizes the potential of evolutionary methods for embedding space exploration of deep generative models and outlines future research directions.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Collaborative Agents for Automated Program Repair in Ruby</title>
<link>https://arxiv.org/abs/2511.03925</link>
<guid>https://arxiv.org/abs/2511.03925</guid>
<content:encoded><![CDATA[
arXiv:2511.03925v1 Announce Type: cross 
Abstract: Automated Program Repair (APR) has advanced rapidly with Large Language Models (LLMs), but most existing methods remain computationally expensive, and focused on a small set of languages. Ruby, despite its widespread use in web development and the persistent challenges faced by its developers, has received little attention in APR research. In this paper, we introduce RAMP, a novel lightweight framework that formulates program repair as a feedback-driven, iterative process for Ruby. RAMP employs a team of collaborative agents that generate targeted tests, reflect on errors, and refine candidate fixes until a correct solution is found. Unlike prior approaches, RAMP is designed to avoid reliance on large multilingual repair databases or costly fine-tuning, instead operating directly on Ruby through lightweight prompting and test-driven feedback. Evaluation on the XCodeEval benchmark shows that RAMP achieves a pass@1 of 67% on Ruby, outper-forming prior approaches. RAMP converges quickly within five iterations, and ablation studies confirm that test generation and self-reflection are key drivers of its performance. Further analysis shows that RAMP is particularly effective at repairing wrong answers, compilation errors, and runtime errors. Our approach provides new insights into multi-agent repair strategies, and establishes a foundation for extending LLM-based debugging tools to under-studied languages.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NVIDIA Nemotron Nano V2 VL</title>
<link>https://arxiv.org/abs/2511.03929</link>
<guid>https://arxiv.org/abs/2511.03929</guid>
<content:encoded><![CDATA[
arXiv:2511.03929v1 Announce Type: cross 
Abstract: We introduce Nemotron Nano V2 VL, the latest model of the Nemotron vision-language series designed for strong real-world document understanding, long video comprehension, and reasoning tasks. Nemotron Nano V2 VL delivers significant improvements over our previous model, Llama-3.1-Nemotron-Nano-VL-8B, across all vision and text domains through major enhancements in model architecture, datasets, and training recipes. Nemotron Nano V2 VL builds on Nemotron Nano V2, a hybrid Mamba-Transformer LLM, and innovative token reduction techniques to achieve higher inference throughput in long document and video scenarios. We are releasing model checkpoints in BF16, FP8, and FP4 formats and sharing large parts of our datasets, recipes and training code.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PEFA-AI: Advancing Open-source LLMs for RTL generation using Progressive Error Feedback Agentic-AI</title>
<link>https://arxiv.org/abs/2511.03934</link>
<guid>https://arxiv.org/abs/2511.03934</guid>
<content:encoded><![CDATA[
arXiv:2511.03934v1 Announce Type: cross 
Abstract: We present an agentic flow consisting of multiple agents that combine specialized LLMs and hardware simulation tools to collaboratively complete the complex task of Register Transfer Level (RTL) generation without human intervention. A key feature of the proposed flow is the progressive error feedback system of agents (PEFA), a self-correcting mechanism that leverages iterative error feedback to progressively increase the complexity of the approach. The generated RTL includes checks for compilation, functional correctness, and synthesizable constructs. To validate this adaptive approach to code generation, benchmarking is performed using two opensource natural language-to-RTL datasets. We demonstrate the benefits of the proposed approach implemented on an open source agentic framework, using both open- and closed-source LLMs, effectively bridging the performance gap between them. Compared to previously published methods, our approach sets a new benchmark, providing state-of-the-art pass rates while being efficient in token counts.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RLHF: A comprehensive Survey for Cultural, Multimodal and Low Latency Alignment Methods</title>
<link>https://arxiv.org/abs/2511.03939</link>
<guid>https://arxiv.org/abs/2511.03939</guid>
<content:encoded><![CDATA[
arXiv:2511.03939v1 Announce Type: cross 
Abstract: Reinforcement Learning from Human Feedback (RLHF) is the standard for aligning Large Language Models (LLMs), yet recent progress has moved beyond canonical text-based methods. This survey synthesizes the new frontier of alignment research by addressing critical gaps in multi-modal alignment, cultural fairness, and low-latency optimization. To systematically explore these domains, we first review foundational algo- rithms, including PPO, DPO, and GRPO, before presenting a detailed analysis of the latest innovations. By providing a comparative synthesis of these techniques and outlining open challenges, this work serves as an essential roadmap for researchers building more robust, efficient, and equitable AI systems.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Direct Semantic Communication Between Large Language Models via Vector Translation</title>
<link>https://arxiv.org/abs/2511.03945</link>
<guid>https://arxiv.org/abs/2511.03945</guid>
<content:encoded><![CDATA[
arXiv:2511.03945v1 Announce Type: cross 
Abstract: In multi-agent settings, such as debate, reflection, or tool-calling, large language models (LLMs) pass messages as plain tokens, discarding most latent semantics. This constrains information transfer and adds unnecessary computational overhead. We form a latent bridge via vector translations, which use learned mappings that enable direct semantic exchange between representation spaces. A dual-encoder translator trained between Llama-2-7B and Mistral-7B-Instruct attains an average cosine alignment of 0.538. Injecting the translated vectors at 30 percent blending strength steers the target model's generation without destabilizing logits. Bidirectional evaluation shows a 2.01:1 transfer asymmetry, indicating that general-purpose models yield more transferable representations than instruction-tuned variants. This conservative injection preserves computational stability while demonstrating that cross-model latent communication is feasible, enabling collaborative AI systems that share meaning rather than tokens.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Multi-View Reconstruction via Texture-Guided Gaussian-Mesh Joint Optimization</title>
<link>https://arxiv.org/abs/2511.03950</link>
<guid>https://arxiv.org/abs/2511.03950</guid>
<content:encoded><![CDATA[
arXiv:2511.03950v1 Announce Type: cross 
Abstract: Reconstructing real-world objects from multi-view images is essential for applications in 3D editing, AR/VR, and digital content creation. Existing methods typically prioritize either geometric accuracy (Multi-View Stereo) or photorealistic rendering (Novel View Synthesis), often decoupling geometry and appearance optimization, which hinders downstream editing tasks. This paper advocates an unified treatment on geometry and appearance optimization for seamless Gaussian-mesh joint optimization. More specifically, we propose a novel framework that simultaneously optimizes mesh geometry (vertex positions and faces) and vertex colors via Gaussian-guided mesh differentiable rendering, leveraging photometric consistency from input images and geometric regularization from normal and depth maps. The obtained high-quality 3D reconstruction can be further exploit in down-stream editing tasks, such as relighting and shape deformation. The code will be publicly available upon acceptance.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PETRA: Pretrained Evolutionary Transformer for SARS-CoV-2 Mutation Prediction</title>
<link>https://arxiv.org/abs/2511.03976</link>
<guid>https://arxiv.org/abs/2511.03976</guid>
<content:encoded><![CDATA[
arXiv:2511.03976v1 Announce Type: cross 
Abstract: Since its emergence, SARS-CoV-2 has demonstrated a rapid and unpredictable evolutionary trajectory, characterized by the continual emergence of immune-evasive variants. This poses persistent challenges to public health and vaccine development.
  While large-scale generative pre-trained transformers (GPTs) have revolutionized the modeling of sequential data, their direct applications to noisy viral genomic sequences are limited. In this paper, we introduce PETRA(Pretrained Evolutionary TRAnsformer), a novel transformer approach based on evolutionary trajectories derived from phylogenetic trees rather than raw RNA sequences. This method effectively mitigates sequencing noise and captures the hierarchical structure of viral evolution.
  With a weighted training framework to address substantial geographical and temporal imbalances in global sequence data, PETRA excels in predicting future SARS-CoV-2 mutations, achieving a weighted recall@1 of 9.45% for nucleotide mutations and 17.10\% for spike amino-acid mutations, compared to 0.49% and 6.64% respectively for the best baseline. PETRA also demonstrates its ability to aid in the real-time mutation prediction of major clades like 24F(XEC) and 25A(LP.8.1). The code is open sourced on https://github.com/xz-keg/PETra
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multiscale Astrocyte Network Calcium Dynamics for Biologically Plausible Intelligence in Anomaly Detection</title>
<link>https://arxiv.org/abs/2511.03993</link>
<guid>https://arxiv.org/abs/2511.03993</guid>
<content:encoded><![CDATA[
arXiv:2511.03993v1 Announce Type: cross 
Abstract: Network anomaly detection systems encounter several challenges with traditional detectors trained offline. They become susceptible to concept drift and new threats such as zero-day or polymorphic attacks. To address this limitation, we propose a Ca$^{2+}$-modulated learning framework that draws inspiration from astrocytic Ca$^{2+}$ signaling in the brain, where rapid, context-sensitive adaptation enables robust information processing. Our approach couples a multicellular astrocyte dynamics simulator with a deep neural network (DNN). The simulator models astrocytic Ca$^{2+}$ dynamics through three key mechanisms: IP$_3$-mediated Ca$^{2+}$ release, SERCA pump uptake, and conductance-aware diffusion through gap junctions between cells. Evaluation of our proposed network on CTU-13 (Neris) network traffic data demonstrates the effectiveness of our biologically plausible approach. The Ca$^{2+}$-gated model outperforms a matched baseline DNN, achieving up to $\sim$98\% accuracy with reduced false positives and negatives across multiple train/test splits. Importantly, this improved performance comes with negligible runtime overhead once Ca$^{2+}$ trajectories are precomputed. While demonstrated here for cybersecurity applications, this Ca$^{2+}$-modulated learning framework offers a generic solution for streaming detection tasks that require rapid, biologically grounded adaptation to evolving data patterns.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Fuzzing with LLM-Guided Input Mutation and Semantic Feedback</title>
<link>https://arxiv.org/abs/2511.03995</link>
<guid>https://arxiv.org/abs/2511.03995</guid>
<content:encoded><![CDATA[
arXiv:2511.03995v1 Announce Type: cross 
Abstract: Software fuzzing has become a cornerstone in automated vulnerability discovery, yet existing mutation strategies often lack semantic awareness, leading to redundant test cases and slow exploration of deep program states. In this work, I present a hybrid fuzzing framework that integrates static and dynamic analysis with Large Language Model (LLM)-guided input mutation and semantic feedback. Static analysis extracts control-flow and data-flow information, which is transformed into structured prompts for the LLM to generate syntactically valid and semantically diverse inputs. During execution, I augment traditional coverage-based feedback with semantic feedback signals-derived from program state changes, exception types, and output semantics-allowing the fuzzer to prioritize inputs that trigger novel program behaviors beyond mere code coverage. I implement our approach atop AFL++, combining program instrumentation with embedding-based semantic similarity metrics to guide seed selection. Evaluation on real-world open-source targets, including libpng, tcpdump, and sqlite, demonstrates that our method achieves faster time-to-first-bug, higher semantic diversity, and a competitive number of unique bugs compared to state-of-the-art fuzzers. This work highlights the potential of combining LLM reasoning with semantic-aware feedback to accelerate and deepen vulnerability discovery.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Scalable Meta-Learning of near-optimal Interpretable Models via Synthetic Model Generations</title>
<link>https://arxiv.org/abs/2511.04000</link>
<guid>https://arxiv.org/abs/2511.04000</guid>
<content:encoded><![CDATA[
arXiv:2511.04000v1 Announce Type: cross 
Abstract: Decision trees are widely used in high-stakes fields like finance and healthcare due to their interpretability. This work introduces an efficient, scalable method for generating synthetic pre-training data to enable meta-learning of decision trees. Our approach samples near-optimal decision trees synthetically, creating large-scale, realistic datasets. Using the MetaTree transformer architecture, we demonstrate that this method achieves performance comparable to pre-training on real-world data or with computationally expensive optimal decision trees. This strategy significantly reduces computational costs, enhances data generation flexibility, and paves the way for scalable and efficient meta-learning of interpretable decision tree models.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerating scientific discovery with the common task framework</title>
<link>https://arxiv.org/abs/2511.04001</link>
<guid>https://arxiv.org/abs/2511.04001</guid>
<content:encoded><![CDATA[
arXiv:2511.04001v1 Announce Type: cross 
Abstract: Machine learning (ML) and artificial intelligence (AI) algorithms are transforming and empowering the characterization and control of dynamic systems in the engineering, physical, and biological sciences. These emerging modeling paradigms require comparative metrics to evaluate a diverse set of scientific objectives, including forecasting, state reconstruction, generalization, and control, while also considering limited data scenarios and noisy measurements. We introduce a common task framework (CTF) for science and engineering, which features a growing collection of challenge data sets with a diverse set of practical and common objectives. The CTF is a critically enabling technology that has contributed to the rapid advance of ML/AI algorithms in traditional applications such as speech recognition, language processing, and computer vision. There is a critical need for the objective metrics of a CTF to compare the diverse algorithms being rapidly developed and deployed in practice today across science and engineering.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memory- and Latency-Constrained Inference of Large Language Models via Adaptive Split Computing</title>
<link>https://arxiv.org/abs/2511.04002</link>
<guid>https://arxiv.org/abs/2511.04002</guid>
<content:encoded><![CDATA[
arXiv:2511.04002v1 Announce Type: cross 
Abstract: Large language models (LLMs) have achieved near-human performance across diverse reasoning tasks, yet their deployment on resource-constrained Internet-of-Things (IoT) devices remains impractical due to massive parameter footprints and memory-intensive autoregressive decoding. While split computing offers a promising solution by partitioning model execution between edge devices and cloud servers, existing approaches fail to address the unique challenges of autoregressive inference, particularly the iterative token generation process and expanding key-value (KV) cache requirements. This work introduces the first autoregressive-aware split computing framework designed explicitly for LLM deployment on edge devices. Our approach makes three key contributions. First, we develop one-point split compression (OPSC), a mixed-precision quantization scheme that prevents out-of-memory failures by strategically partitioning models into front-end and back-end segments with different precision levels. Second, we propose a two-stage intermediate compression pipeline that combines threshold splitting (TS) and token-wise adaptive bit quantization (TAB-Q) to preserve accuracy-critical activations while dramatically reducing communication overhead. Third, we formulate a unified optimization framework that jointly selects optimal split points, quantization settings, and sequence lengths to satisfy strict memory and latency constraints. Extensive evaluations across diverse LLMs and hardware platforms demonstrate superior performance compared to state-of-the-art quantization methods, including SmoothQuant, OmniQuant, and Atom. The framework achieves a 1.49 inference speedup and significant communication overhead reduction while maintaining or improving model accuracy.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Abductive Inference in Retrieval-Augmented Language Models: Generating and Validating Missing Premises</title>
<link>https://arxiv.org/abs/2511.04020</link>
<guid>https://arxiv.org/abs/2511.04020</guid>
<content:encoded><![CDATA[
arXiv:2511.04020v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) enhanced with retrieval -- commonly referred to as Retrieval-Augmented Generation (RAG) -- have demonstrated strong performance in knowledge-intensive tasks. However, RAG pipelines often fail when retrieved evidence is incomplete, leaving gaps in the reasoning process. In such cases, \emph{abductive inference} -- the process of generating plausible missing premises to explain observations -- offers a principled approach to bridge these gaps. In this paper, we propose a framework that integrates abductive inference into retrieval-augmented LLMs. Our method detects insufficient evidence, generates candidate missing premises, and validates them through consistency and plausibility checks. Experimental results on abductive reasoning and multi-hop QA benchmarks show that our approach improves both answer accuracy and reasoning faithfulness. This work highlights abductive inference as a promising direction for enhancing the robustness and explainability of RAG systems.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An LLM-based Framework for Human-Swarm Teaming Cognition in Disaster Search and Rescue</title>
<link>https://arxiv.org/abs/2511.04042</link>
<guid>https://arxiv.org/abs/2511.04042</guid>
<content:encoded><![CDATA[
arXiv:2511.04042v1 Announce Type: cross 
Abstract: Large-scale disaster Search And Rescue (SAR) operations are persistently challenged by complex terrain and disrupted communications. While Unmanned Aerial Vehicle (UAV) swarms offer a promising solution for tasks like wide-area search and supply delivery, yet their effective coordination places a significant cognitive burden on human operators. The core human-machine collaboration bottleneck lies in the ``intention-to-action gap'', which is an error-prone process of translating a high-level rescue objective into a low-level swarm command under high intensity and pressure. To bridge this gap, this study proposes a novel LLM-CRF system that leverages Large Language Models (LLMs) to model and augment human-swarm teaming cognition. The proposed framework initially captures the operator's intention through natural and multi-modal interactions with the device via voice or graphical annotations. It then employs the LLM as a cognitive engine to perform intention comprehension, hierarchical task decomposition, and mission planning for the UAV swarm. This closed-loop framework enables the swarm to act as a proactive partner, providing active feedback in real-time while reducing the need for manual monitoring and control, which considerably advances the efficacy of the SAR task. We evaluate the proposed framework in a simulated SAR scenario. Experimental results demonstrate that, compared to traditional order and command-based interfaces, the proposed LLM-driven approach reduced task completion time by approximately $64.2\%$ and improved task success rate by $7\%$. It also leads to a considerable reduction in subjective cognitive workload, with NASA-TLX scores dropping by $42.9\%$. This work establishes the potential of LLMs to create more intuitive and effective human-swarm collaborations in high-stakes scenarios.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pediatric Appendicitis Detection from Ultrasound Images</title>
<link>https://arxiv.org/abs/2511.04069</link>
<guid>https://arxiv.org/abs/2511.04069</guid>
<content:encoded><![CDATA[
arXiv:2511.04069v1 Announce Type: cross 
Abstract: Pediatric appendicitis remains one of the most common causes of acute abdominal pain in children, and its diagnosis continues to challenge clinicians due to overlapping symptoms and variable imaging quality. This study aims to develop and evaluate a deep learning model based on a pretrained ResNet architecture for automated detection of appendicitis from ultrasound images. We used the Regensburg Pediatric Appendicitis Dataset, which includes ultrasound scans, laboratory data, and clinical scores from pediatric patients admitted with abdominal pain to Children Hospital. Hedwig in Regensburg, Germany. Each subject had 1 to 15 ultrasound views covering the right lower quadrant, appendix, lymph nodes, and related structures. For the image based classification task, ResNet was fine tuned to distinguish appendicitis from non-appendicitis cases. Images were preprocessed by normalization, resizing, and augmentation to enhance generalization. The proposed ResNet model achieved an overall accuracy of 93.44, precision of 91.53, and recall of 89.8, demonstrating strong performance in identifying appendicitis across heterogeneous ultrasound views. The model effectively learned discriminative spatial features, overcoming challenges posed by low contrast, speckle noise, and anatomical variability in pediatric imaging.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Left Atrial Segmentation with nnU-Net Using MRI</title>
<link>https://arxiv.org/abs/2511.04071</link>
<guid>https://arxiv.org/abs/2511.04071</guid>
<content:encoded><![CDATA[
arXiv:2511.04071v1 Announce Type: cross 
Abstract: Accurate segmentation of the left atrium (LA) from cardiac MRI is critical for guiding atrial fibrillation (AF) ablation and constructing biophysical cardiac models. Manual delineation is time-consuming, observer-dependent, and impractical for large-scale or time-sensitive clinical workflows. Deep learning methods, particularly convolutional architectures, have recently demonstrated superior performance in medical image segmentation tasks. In this study, we applied the nnU-Net framework, an automated, self-configuring deep learning segmentation architecture, to the Left Atrial Segmentation Challenge 2013 dataset. The dataset consists of thirty MRI scans with corresponding expert-annotated masks. The nnU-Net model automatically adapted its preprocessing, network configuration, and training pipeline to the characteristics of the MRI data. Model performance was quantitatively evaluated using the Dice similarity coefficient (DSC), and qualitative results were compared against expert segmentations. The proposed nnUNet model achieved a mean Dice score of 93.5, demonstrating high overlap with expert annotations and outperforming several traditional segmentation approaches reported in previous studies. The network exhibited robust generalization across variations in left atrial shape, contrast, and image quality, accurately delineating both the atrial body and proximal pulmonary veins.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeNoise: Learning Robust Graph Representations for Unsupervised Graph-Level Anomaly Detection</title>
<link>https://arxiv.org/abs/2511.04086</link>
<guid>https://arxiv.org/abs/2511.04086</guid>
<content:encoded><![CDATA[
arXiv:2511.04086v1 Announce Type: cross 
Abstract: With the rapid growth of graph-structured data in critical domains, unsupervised graph-level anomaly detection (UGAD) has become a pivotal task. UGAD seeks to identify entire graphs that deviate from normal behavioral patterns. However, most Graph Neural Network (GNN) approaches implicitly assume that the training set is clean, containing only normal graphs, which is rarely true in practice. Even modest contamination by anomalous graphs can distort learned representations and sharply degrade performance. To address this challenge, we propose DeNoise, a robust UGAD framework explicitly designed for contaminated training data. It jointly optimizes a graph-level encoder, an attribute decoder, and a structure decoder via an adversarial objective to learn noise-resistant embeddings. Further, DeNoise introduces an encoder anchor-alignment denoising mechanism that fuses high-information node embeddings from normal graphs into all graph embeddings, improving representation quality while suppressing anomaly interference. A contrastive learning component then compacts normal graph embeddings and repels anomalous ones in the latent space. Extensive experiments on eight real-world datasets demonstrate that DeNoise consistently learns reliable graph-level representations under varying noise intensities and significantly outperforms state-of-the-art UGAD baselines.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing Equitable AI: Evaluating Cultural Expressiveness in LLMs for Latin American Contexts</title>
<link>https://arxiv.org/abs/2511.04090</link>
<guid>https://arxiv.org/abs/2511.04090</guid>
<content:encoded><![CDATA[
arXiv:2511.04090v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) systems often reflect biases from economically advanced regions, marginalizing contexts in economically developing regions like Latin America due to imbalanced datasets. This paper examines AI representations of diverse Latin American contexts, revealing disparities between data from economically advanced and developing regions. We highlight how the dominance of English over Spanish, Portuguese, and indigenous languages such as Quechua and Nahuatl perpetuates biases, framing Latin American perspectives through a Western lens. To address this, we introduce a culturally aware dataset rooted in Latin American history and socio-political contexts, challenging Eurocentric models. We evaluate six language models on questions testing cultural context awareness, using a novel Cultural Expressiveness metric, statistical tests, and linguistic analyses. Our findings show that some models better capture Latin American perspectives, while others exhibit significant sentiment misalignment (p < 0.001). Fine-tuning Mistral-7B with our dataset improves its cultural expressiveness by 42.9%, advancing equitable AI development. We advocate for equitable AI by prioritizing datasets that reflect Latin American history, indigenous knowledge, and diverse languages, while emphasizing community-centered approaches to amplify marginalized voices.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Automated Theorem Generator with Theoretical Foundation Based on Rectangular Standard Contradiction</title>
<link>https://arxiv.org/abs/2511.04092</link>
<guid>https://arxiv.org/abs/2511.04092</guid>
<content:encoded><![CDATA[
arXiv:2511.04092v1 Announce Type: cross 
Abstract: Currently, there is a lack of rigorous theoretical system for systematically generating non-trivial and logically valid theorems. Addressing this critical gap, this paper conducts research to propose a novel automated theorem generation theory and tool. Based on the concept of standard contradiction which possesses unique deductive advantages, this paper defines and proves, for the first time, a new logical structure known as rectangular standard contradiction. Centered on this structure, a complete Automated Theorem Generation (ATG) theory is put forward. Theoretical proofs clarify two core properties of rectangular standard contradiction: first, it is a standard contradiction (necessarily unsatisfiable); second, it exhibits non-redundancy (the remaining clause set becomes satisfiable after removing any clause). Leveraging these properties, this paper proves that partitioning a rectangular standard contradiction into a premise subset $A$ and negation of its complement $H$, a valid theorem $A \vdash \neg H$ can be formed, and all such theorems are logically equivalent. To implement this theory, an efficient template-based ATG algorithm is designed, and a Rectangular Automated Theorem Generator is developed. This research enables machines to transition from "verifiers" to "discoverers", opening up new avenues for fundamental research in the fields of logic and artificial intelligence.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Characterization of List Language Identification in the Limit</title>
<link>https://arxiv.org/abs/2511.04103</link>
<guid>https://arxiv.org/abs/2511.04103</guid>
<content:encoded><![CDATA[
arXiv:2511.04103v1 Announce Type: cross 
Abstract: We study the problem of language identification in the limit, where given a sequence of examples from a target language, the goal of the learner is to output a sequence of guesses for the target language such that all the guesses beyond some finite time are correct. Classical results of Gold showed that language identification in the limit is impossible for essentially any interesting collection of languages. Later, Angluin gave a precise characterization of language collections for which this task is possible. Motivated by recent positive results for the related problem of language generation, we revisit the classic language identification problem in the setting where the learner is given the additional power of producing a list of $k$ guesses at each time step. The goal is to ensure that beyond some finite time, one of the guesses is correct at each time step.
  We give an exact characterization of collections of languages that can be $k$-list identified in the limit, based on a recursive version of Angluin's characterization (for language identification with a list of size $1$). This further leads to a conceptually appealing characterization: A language collection can be $k$-list identified in the limit if and only if the collection can be decomposed into $k$ collections of languages, each of which can be identified in the limit (with a list of size $1$). We also use our characterization to establish rates for list identification in the statistical setting where the input is drawn as an i.i.d. stream from a distribution supported on some language in the collection. Our results show that if a collection is $k$-list identifiable in the limit, then the collection can be $k$-list identified at an exponential rate, and this is best possible. On the other hand, if a collection is not $k$-list identifiable in the limit, then it cannot be $k$-list identified at any rate that goes to zero.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated and Explainable Denial of Service Analysis for AI-Driven Intrusion Detection Systems</title>
<link>https://arxiv.org/abs/2511.04114</link>
<guid>https://arxiv.org/abs/2511.04114</guid>
<content:encoded><![CDATA[
arXiv:2511.04114v1 Announce Type: cross 
Abstract: With the increasing frequency and sophistication of Distributed Denial of Service (DDoS) attacks, it has become critical to develop more efficient and interpretable detection methods. Traditional detection systems often struggle with scalability and transparency, hindering real-time response and understanding of attack vectors. This paper presents an automated framework for detecting and interpreting DDoS attacks using machine learning (ML). The proposed method leverages the Tree-based Pipeline Optimization Tool (TPOT) to automate the selection and optimization of ML models and features, reducing the need for manual experimentation. SHapley Additive exPlanations (SHAP) is incorporated to enhance model interpretability, providing detailed insights into the contribution of individual features to the detection process. By combining TPOT's automated pipeline selection with SHAP interpretability, this approach improves the accuracy and transparency of DDoS detection. Experimental results demonstrate that key features such as mean backward packet length and minimum forward packet header length are critical in detecting DDoS attacks, offering a scalable and explainable cybersecurity solution.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Tennis Player and Ball Tracking with Court Keypoints Detection (Hawk Eye System)</title>
<link>https://arxiv.org/abs/2511.04126</link>
<guid>https://arxiv.org/abs/2511.04126</guid>
<content:encoded><![CDATA[
arXiv:2511.04126v1 Announce Type: cross 
Abstract: This study presents a complete pipeline for automated tennis match analysis. Our framework integrates multiple deep learning models to detect and track players and the tennis ball in real time, while also identifying court keypoints for spatial reference. Using YOLOv8 for player detection, a custom-trained YOLOv5 model for ball tracking, and a ResNet50-based architecture for court keypoint detection, our system provides detailed analytics including player movement patterns, ball speed, shot accuracy, and player reaction times. The experimental results demonstrate robust performance in varying court conditions and match scenarios. The model outputs an annotated video along with detailed performance metrics, enabling coaches, broadcasters, and players to gain actionable insights into the dynamics of the game.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DMSORT: An efficient parallel maritime multi-object tracking architecture for unmanned vessel platforms</title>
<link>https://arxiv.org/abs/2511.04128</link>
<guid>https://arxiv.org/abs/2511.04128</guid>
<content:encoded><![CDATA[
arXiv:2511.04128v1 Announce Type: cross 
Abstract: Accurate perception of the marine environment through robust multi-object tracking (MOT) is essential for ensuring safe vessel navigation and effective maritime surveillance. However, the complicated maritime environment often causes camera motion and subsequent visual degradation, posing significant challenges to MOT. To address this challenge, we propose an efficient Dual-branch Maritime SORT (DMSORT) method for maritime MOT. The core of the framework is a parallel tracker with affine compensation, which incorporates an object detection and re-identification (ReID) branch, along with a dedicated branch for dynamic camera motion estimation. Specifically, a Reversible Columnar Detection Network (RCDN) is integrated into the detection module to leverage multi-level visual features for robust object detection. Furthermore, a lightweight Transformer-based appearance extractor (Li-TAE) is designed to capture global contextual information and generate robust appearance features. Another branch decouples platform-induced and target-intrinsic motion by constructing a projective transformation, applying platform-motion compensation within the Kalman filter, and thereby stabilizing true object trajectories. Finally, a clustering-optimized feature fusion module effectively combines motion and appearance cues to ensure identity consistency under noise, occlusion, and drift. Extensive evaluations on the Singapore Maritime Dataset demonstrate that DMSORT achieves state-of-the-art performance. Notably, DMSORT attains the fastest runtime among existing ReID-based MOT frameworks while maintaining high identity consistency and robustness to jitter and occlusion. Code is available at: https://github.com/BiscuitsLzy/DMSORT-An-efficient-parallel-maritime-multi-object-tracking-architecture-.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning from Online Videos at Inference Time for Computer-Use Agents</title>
<link>https://arxiv.org/abs/2511.04137</link>
<guid>https://arxiv.org/abs/2511.04137</guid>
<content:encoded><![CDATA[
arXiv:2511.04137v1 Announce Type: cross 
Abstract: Computer-use agents can operate computers and automate laborious tasks, but despite recent rapid progress, they still lag behind human users, especially when tasks require domain-specific procedural knowledge about particular applications, platforms, and multi-step workflows. Humans can bridge this gap by watching video tutorials: we search, skim, and selectively imitate short segments that match our current subgoal. In this paper, we study how to enable computer-use agents to learn from online videos at inference time effectively. We propose a framework that retrieves and filters tutorial videos, converts them into structured demonstration trajectories, and dynamically selects trajectories as in-context guidance during execution. Particularly, using a VLM, we infer UI actions, segment videos into short subsequences of actions, and assign each subsequence a textual objective. At inference time, a two-stage selection mechanism dynamically chooses a single trajectory to add in context at each step, focusing the agent on the most helpful local guidance for its next decision. Experiments on two widely used benchmarks show that our framework consistently outperforms strong base agents and variants that use only textual tutorials or transcripts. Analyses highlight the importance of trajectory segmentation and selection, action filtering, and visual information, suggesting that abundant online videos can be systematically distilled into actionable guidance that improves computer-use agents at inference time. Our code is available at https://github.com/UCSB-NLP-Chang/video_demo.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaffolding Metacognition in Programming Education: Understanding Student-AI Interactions and Design Implications</title>
<link>https://arxiv.org/abs/2511.04144</link>
<guid>https://arxiv.org/abs/2511.04144</guid>
<content:encoded><![CDATA[
arXiv:2511.04144v1 Announce Type: cross 
Abstract: Generative AI tools such as ChatGPT now provide novice programmers with unprecedented access to instant, personalized support. While this holds clear promise, their influence on students' metacognitive processes remains underexplored. Existing work has largely focused on correctness and usability, with limited attention to whether and how students' use of AI assistants supports or bypasses key metacognitive processes. This study addresses that gap by analyzing student-AI interactions through a metacognitive lens in university-level programming courses. We examined more than 10,000 dialogue logs collected over three years, complemented by surveys of students and educators. Our analysis focused on how prompts and responses aligned with metacognitive phases and strategies. Synthesizing these findings across data sources, we distill design considerations for AI-powered coding assistants that aim to support rather than supplant metacognitive engagement. Our findings provide guidance for developing educational AI tools that strengthen students' learning processes in programming education.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BAPPA: Benchmarking Agents, Plans, and Pipelines for Automated Text-to-SQL Generation</title>
<link>https://arxiv.org/abs/2511.04153</link>
<guid>https://arxiv.org/abs/2511.04153</guid>
<content:encoded><![CDATA[
arXiv:2511.04153v1 Announce Type: cross 
Abstract: Text-to-SQL systems provide a natural language interface that can enable even laymen to access information stored in databases. However, existing Large Language Models (LLM) struggle with SQL generation from natural instructions due to large schema sizes and complex reasoning. Prior work often focuses on complex, somewhat impractical pipelines using flagship models, while smaller, efficient models remain overlooked. In this work, we explore three multi-agent LLM pipelines, with systematic performance benchmarking across a range of small to large open-source models: (1) Multi-agent discussion pipeline, where agents iteratively critique and refine SQL queries, and a judge synthesizes the final answer; (2) Planner-Coder pipeline, where a thinking model planner generates stepwise SQL generation plans and a coder synthesizes queries; and (3) Coder-Aggregator pipeline, where multiple coders independently generate SQL queries, and a reasoning agent selects the best query. Experiments on the Bird-Bench Mini-Dev set reveal that Multi-Agent discussion can improve small model performance, with up to 10.6% increase in Execution Accuracy for Qwen2.5-7b-Instruct seen after three rounds of discussion. Among the pipelines, the LLM Reasoner-Coder pipeline yields the best results, with DeepSeek-R1-32B and QwQ-32B planners boosting Gemma 3 27B IT accuracy from 52.4% to the highest score of 56.4%. Codes are available at https://github.com/treeDweller98/bappa-sql.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are We Aligned? A Preliminary Investigation of the Alignment of Responsible AI Values between LLMs and Human Judgment</title>
<link>https://arxiv.org/abs/2511.04157</link>
<guid>https://arxiv.org/abs/2511.04157</guid>
<content:encoded><![CDATA[
arXiv:2511.04157v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly employed in software engineering tasks such as requirements elicitation, design, and evaluation, raising critical questions regarding their alignment with human judgments on responsible AI values. This study investigates how closely LLMs' value preferences align with those of two human groups: a US-representative sample and AI practitioners. We evaluate 23 LLMs across four tasks: (T1) selecting key responsible AI values, (T2) rating their importance in specific contexts, (T3) resolving trade-offs between competing values, and (T4) prioritizing software requirements that embody those values. The results show that LLMs generally align more closely with AI practitioners than with the US-representative sample, emphasizing fairness, privacy, transparency, safety, and accountability. However, inconsistencies appear between the values that LLMs claim to uphold (Tasks 1-3) and the way they prioritize requirements (Task 4), revealing gaps in faithfulness between stated and applied behavior. These findings highlight the practical risk of relying on LLMs in requirements engineering without human oversight and motivate the need for systematic approaches to benchmark, interpret, and monitor value alignment in AI-assisted software development.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Systematic Evaluation of Preprocessing Techniques for Accurate Image Registration in Digital Pathology</title>
<link>https://arxiv.org/abs/2511.04171</link>
<guid>https://arxiv.org/abs/2511.04171</guid>
<content:encoded><![CDATA[
arXiv:2511.04171v1 Announce Type: cross 
Abstract: Image registration refers to the process of spatially aligning two or more images by mapping them into a common coordinate system, so that corresponding anatomical or tissue structures are matched across images. In digital pathology, registration enables direct comparison and integration of information from different stains or imaging modalities, sup-porting applications such as biomarker analysis and tissue reconstruction. Accurate registration of images from different modalities is an essential step in digital pathology. In this study, we investigated how various color transformation techniques affect image registration between hematoxylin and eosin (H&amp;E) stained images and non-linear multimodal images. We used a dataset of 20 tissue sample pairs, with each pair undergoing several preprocessing steps, including different color transformation (CycleGAN, Macenko, Reinhard, Vahadane), inversion, contrast adjustment, intensity normalization, and denoising. All images were registered using the VALIS registration method, which first applies rigid registration and then performs non-rigid registration in two steps on both low and high-resolution images. Registration performance was evaluated using the relative Target Registration Error (rTRE). We reported the median of median rTRE values (MMrTRE) and the average of median rTRE values (AMrTRE) for each method. In addition, we performed a custom point-based evaluation using ten manually selected key points. Registration was done separately for two scenarios, using either the original or inverted multimodal images. In both scenarios, CycleGAN color transformation achieved the lowest registration errors, while the other methods showed higher errors. These findings show that applying color transformation before registration improves alignment between images from different modalities and supports more reliable analysis in digital pathology.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explaining Software Vulnerabilities with Large Language Models</title>
<link>https://arxiv.org/abs/2511.04179</link>
<guid>https://arxiv.org/abs/2511.04179</guid>
<content:encoded><![CDATA[
arXiv:2511.04179v1 Announce Type: cross 
Abstract: The prevalence of security vulnerabilities has prompted companies to adopt static application security testing (SAST) tools for vulnerability detection. Nevertheless, these tools frequently exhibit usability limitations, as their generic warning messages do not sufficiently communicate important information to developers, resulting in misunderstandings or oversight of critical findings. In light of recent developments in Large Language Models (LLMs) and their text generation capabilities, our work investigates a hybrid approach that uses LLMs to tackle the SAST explainability challenges. In this paper, we present SAFE, an Integrated Development Environment (IDE) plugin that leverages GPT-4o to explain the causes, impacts, and mitigation strategies of vulnerabilities detected by SAST tools. Our expert user study findings indicate that the explanations generated by SAFE can significantly assist beginner to intermediate developers in understanding and addressing security vulnerabilities, thereby improving the overall usability of SAST tools.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Reinforced Evolution-Based Approach to Multi-Resource Load Balancing</title>
<link>https://arxiv.org/abs/2511.04183</link>
<guid>https://arxiv.org/abs/2511.04183</guid>
<content:encoded><![CDATA[
arXiv:2511.04183v1 Announce Type: cross 
Abstract: This paper presents a reinforced genetic approach to a defined d-resource system optimization problem. The classical evolution schema was ineffective due to a very strict feasibility function in the studied problem. Hence, the presented strategy has introduced several modifications and adaptations to standard genetic routines, e.g.: a migration operator which is an analogy to the biological random genetic drift.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trustworthy LLM-Mediated Communication: Evaluating Information Fidelity in LLM as a Communicator (LAAC) Framework in Multiple Application Domains</title>
<link>https://arxiv.org/abs/2511.04184</link>
<guid>https://arxiv.org/abs/2511.04184</guid>
<content:encoded><![CDATA[
arXiv:2511.04184v1 Announce Type: cross 
Abstract: The proliferation of AI-generated content has created an absurd communication theater where senders use LLMs to inflate simple ideas into verbose content, recipients use LLMs to compress them back into summaries, and as a consequence neither party engage with authentic content. LAAC (LLM as a Communicator) proposes a paradigm shift - positioning LLMs as intelligent communication intermediaries that capture the sender's intent through structured dialogue and facilitate genuine knowledge exchange with recipients. Rather than perpetuating cycles of AI-generated inflation and compression, LAAC enables authentic communication across diverse contexts including academic papers, proposals, professional emails, and cross-platform content generation. However, deploying LLMs as trusted communication intermediaries raises critical questions about information fidelity, consistency, and reliability. This position paper systematically evaluates the trustworthiness requirements for LAAC's deployment across multiple communication domains. We investigate three fundamental dimensions: (1) Information Capture Fidelity - accuracy of intent extraction during sender interviews across different communication types, (2) Reproducibility - consistency of structured knowledge across multiple interaction instances, and (3) Query Response Integrity - reliability of recipient-facing responses without hallucination, source conflation, or fabrication. Through controlled experiments spanning multiple LAAC use cases, we assess these trust dimensions using LAAC's multi-agent architecture. Preliminary findings reveal measurable trust gaps that must be addressed before LAAC can be reliably deployed in high-stakes communication scenarios.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AStF: Motion Style Transfer via Adaptive Statistics Fusor</title>
<link>https://arxiv.org/abs/2511.04192</link>
<guid>https://arxiv.org/abs/2511.04192</guid>
<content:encoded><![CDATA[
arXiv:2511.04192v1 Announce Type: cross 
Abstract: Human motion style transfer allows characters to appear less rigidity and more realism with specific style. Traditional arbitrary image style transfer typically process mean and variance which is proved effective. Meanwhile, similar methods have been adapted for motion style transfer. However, due to the fundamental differences between images and motion, relying on mean and variance is insufficient to fully capture the complex dynamic patterns and spatiotemporal coherence properties of motion data. Building upon this, our key insight is to bring two more coefficient, skewness and kurtosis, into the analysis of motion style. Specifically, we propose a novel Adaptive Statistics Fusor (AStF) which consists of Style Disentanglement Module (SDM) and High-Order Multi-Statistics Attention (HOS-Attn). We trained our AStF in conjunction with a Motion Consistency Regularization (MCR) discriminator. Experimental results show that, by providing a more comprehensive model of the spatiotemporal statistical patterns inherent in dynamic styles, our proposed AStF shows proficiency superiority in motion style transfers over state-of-the-arts. Our code and model are available at https://github.com/CHMimilanlan/AStF.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms</title>
<link>https://arxiv.org/abs/2511.04217</link>
<guid>https://arxiv.org/abs/2511.04217</guid>
<content:encoded><![CDATA[
arXiv:2511.04217v1 Announce Type: cross 
Abstract: The strong lottery ticket hypothesis (SLTH) conjectures that high-performing subnetworks, called strong lottery tickets (SLTs), are hidden in randomly initialized neural networks. Although recent theoretical studies have established the SLTH across various neural architectures, the SLTH for transformer architectures still lacks theoretical understanding. In particular, the current theory of the SLTH does not yet account for the multi-head attention (MHA) mechanism, a core component of transformers. To address this gap, we introduce a theoretical analysis of the existence of SLTs within MHAs. We prove that, if a randomly initialized MHA of $H$ heads and input dimension $d$ has the hidden dimension $O(d\log(Hd^{3/2}))$ for the key and value, it contains an SLT that approximates an arbitrary MHA with the same input dimension with high probability. Furthermore, by leveraging this theory for MHAs, we extend the SLTH to transformers without normalization layers. We empirically validate our theoretical findings, demonstrating that the approximation error between the SLT within a source model (MHA and transformer) and an approximate target counterpart decreases exponentially by increasing the hidden dimension of the source model.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Denoised Recommendation Model with Collaborative Signal Decoupling</title>
<link>https://arxiv.org/abs/2511.04237</link>
<guid>https://arxiv.org/abs/2511.04237</guid>
<content:encoded><![CDATA[
arXiv:2511.04237v1 Announce Type: cross 
Abstract: Although the collaborative filtering (CF) algorithm has achieved remarkable performance in recommendation systems, it suffers from suboptimal recommendation performance due to noise in the user-item interaction matrix. Numerous noise-removal studies have improved recommendation models, but most existing approaches conduct denoising on a single graph. This may cause attenuation of collaborative signals: removing edges between two nodes can interrupt paths between other nodes, weakening path-dependent collaborative information. To address these limitations, this study proposes a novel GNN-based CF model called DRCSD for denoising unstable interactions. DRCSD includes two core modules: a collaborative signal decoupling module (decomposes signals into distinct orders by structural characteristics) and an order-wise denoising module (performs targeted denoising on each order). Additionally, the information aggregation mechanism of traditional GNN-based CF models is modified to avoid cross-order signal interference until the final pooling operation. Extensive experiments on three public real-world datasets show that DRCSD has superior robustness against unstable interactions and achieves statistically significant performance improvements in recommendation accuracy metrics compared to state-of-the-art baseline models.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>seqme: a Python library for evaluating biological sequence design</title>
<link>https://arxiv.org/abs/2511.04239</link>
<guid>https://arxiv.org/abs/2511.04239</guid>
<content:encoded><![CDATA[
arXiv:2511.04239v1 Announce Type: cross 
Abstract: Recent advances in computational methods for designing biological sequences have sparked the development of metrics to evaluate these methods performance in terms of the fidelity of the designed sequences to a target distribution and their attainment of desired properties. However, a single software library implementing these metrics was lacking. In this work we introduce seqme, a modular and highly extendable open-source Python library, containing model-agnostic metrics for evaluating computational methods for biological sequence design. seqme considers three groups of metrics: sequence-based, embedding-based, and property-based, and is applicable to a wide range of biological sequences: small molecules, DNA, ncRNA, mRNA, peptides and proteins. The library offers a number of embedding and property models for biological sequences, as well as diagnostics and visualization functions to inspect the results. seqme can be used to evaluate both one-shot and iterative computational design methods.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Brittleness of CLIP Text Encoders</title>
<link>https://arxiv.org/abs/2511.04247</link>
<guid>https://arxiv.org/abs/2511.04247</guid>
<content:encoded><![CDATA[
arXiv:2511.04247v1 Announce Type: cross 
Abstract: Multimodal co-embedding models, especially CLIP, have advanced the state of the art in zero-shot classification and multimedia information retrieval in recent years by aligning images and text in a shared representation space. However, such modals trained on a contrastive alignment can lack stability towards small input perturbations. Especially when dealing with manually expressed queries, minor variations in the query can cause large differences in the ranking of the best-matching results. In this paper, we present a systematic analysis of the effect of multiple classes of non-semantic query perturbations in an multimedia information retrieval scenario. We evaluate a diverse set of lexical, syntactic, and semantic perturbations across multiple CLIP variants using the TRECVID Ad-Hoc Video Search queries and the V3C1 video collection. Across models, we find that syntactic and semantic perturbations drive the largest instabilities, while brittleness is concentrated in trivial surface edits such as punctuation and case. Our results highlight robustness as a critical dimension for evaluating vision-language models beyond benchmark accuracy.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedSapiens: Taking a Pose to Rethink Medical Imaging Landmark Detection</title>
<link>https://arxiv.org/abs/2511.04255</link>
<guid>https://arxiv.org/abs/2511.04255</guid>
<content:encoded><![CDATA[
arXiv:2511.04255v1 Announce Type: cross 
Abstract: This paper does not introduce a novel architecture; instead, it revisits a fundamental yet overlooked baseline: adapting human-centric foundation models for anatomical landmark detection in medical imaging. While landmark detection has traditionally relied on domain-specific models, the emergence of large-scale pre-trained vision models presents new opportunities. In this study, we investigate the adaptation of Sapiens, a human-centric foundation model designed for pose estimation, to medical imaging through multi-dataset pretraining, establishing a new state of the art across multiple datasets. Our proposed model, MedSapiens, demonstrates that human-centric foundation models, inherently optimized for spatial pose localization, provide strong priors for anatomical landmark detection, yet this potential has remained largely untapped. We benchmark MedSapiens against existing state-of-the-art models, achieving up to 5.26% improvement over generalist models and up to 21.81% improvement over specialist models in the average success detection rate (SDR). To further assess MedSapiens adaptability to novel downstream tasks with few annotations, we evaluate its performance in limited-data settings, achieving 2.69% improvement over the few-shot state of the art in SDR. Code and model weights are available at https://github.com/xmed-lab/MedSapiens .
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Proto-LeakNet: Towards Signal-Leak Aware Attribution in Synthetic Human Face Imagery</title>
<link>https://arxiv.org/abs/2511.04260</link>
<guid>https://arxiv.org/abs/2511.04260</guid>
<content:encoded><![CDATA[
arXiv:2511.04260v1 Announce Type: cross 
Abstract: The growing sophistication of synthetic image and deepfake generation models has turned source attribution and authenticity verification into a critical challenge for modern computer vision systems. Recent studies suggest that diffusion pipelines unintentionally imprint persistent statistical traces, known as signal leaks, within their outputs, particularly in latent representations. Building on this observation, we propose Proto-LeakNet, a signal-leak-aware and interpretable attribution framework that integrates closed-set classification with a density-based open-set evaluation on the learned embeddings, enabling analysis of unseen generators without retraining. Operating in the latent domain of diffusion models, our method re-simulates partial forward diffusion to expose residual generator-specific cues. A temporal attention encoder aggregates multi-step latent features, while a feature-weighted prototype head structures the embedding space and enables transparent attribution. Trained solely on closed data and achieving a Macro AUC of 98.13%, Proto-LeakNet learns a latent geometry that remains robust under post-processing, surpassing state-of-the-art methods, and achieves strong separability between known and unseen generators. These results demonstrate that modeling signal-leak bias in latent space enables reliable and interpretable AI-image and deepfake forensics. The code for the whole work will be available upon submission.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Reinforcement Learning from Human Feedback via Bayesian Preference Inference</title>
<link>https://arxiv.org/abs/2511.04286</link>
<guid>https://arxiv.org/abs/2511.04286</guid>
<content:encoded><![CDATA[
arXiv:2511.04286v1 Announce Type: cross 
Abstract: Learning from human preferences is a cornerstone of aligning machine learning models with subjective human judgments. Yet, collecting such preference data is often costly and time-consuming, motivating the need for more efficient learning paradigms. Two established approaches offer complementary advantages: RLHF scales effectively to high-dimensional tasks such as LLM fine-tuning, while PBO achieves greater sample efficiency through active querying. We propose a hybrid framework that unifies RLHF's scalability with PBO's query efficiency by integrating an acquisition-driven module into the RLHF pipeline, thereby enabling active and sample-efficient preference gathering. We validate the proposed approach on two representative domains: (i) high-dimensional preference optimization and (ii) LLM fine-tuning. Experimental results demonstrate consistent improvements in both sample efficiency and overall performance across these tasks.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep learning-based object detection of offshore platforms on Sentinel-1 Imagery and the impact of synthetic training data</title>
<link>https://arxiv.org/abs/2511.04304</link>
<guid>https://arxiv.org/abs/2511.04304</guid>
<content:encoded><![CDATA[
arXiv:2511.04304v1 Announce Type: cross 
Abstract: The recent and ongoing expansion of marine infrastructure, including offshore wind farms, oil and gas platforms, artificial islands, and aquaculture facilities, highlights the need for effective monitoring systems. The development of robust models for offshore infrastructure detection relies on comprehensive, balanced datasets, but falls short when samples are scarce, particularly for underrepresented object classes, shapes, and sizes. By training deep learning-based YOLOv10 object detection models with a combination of synthetic and real Sentinel-1 satellite imagery acquired in the fourth quarter of 2023 from four regions (Caspian Sea, South China Sea, Gulf of Guinea, and Coast of Brazil), this study investigates the use of synthetic training data to enhance model performance. We evaluated this approach by applying the model to detect offshore platforms in three unseen regions (Gulf of Mexico, North Sea, Persian Gulf) and thereby assess geographic transferability. This region-holdout evaluation demonstrated that the model generalises beyond the training areas. In total, 3,529 offshore platforms were detected, including 411 in the North Sea, 1,519 in the Gulf of Mexico, and 1,593 in the Persian Gulf. The model achieved an F1 score of 0.85, which improved to 0.90 upon incorporating synthetic data. We analysed how synthetic data enhances the representation of unbalanced classes and overall model performance, taking a first step toward globally transferable detection of offshore infrastructure. This study underscores the importance of balanced datasets and highlights synthetic data generation as an effective strategy to address common challenges in remote sensing, demonstrating the potential of deep learning for scalable, global offshore infrastructure monitoring.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AIM: Software and Hardware Co-design for Architecture-level IR-drop Mitigation in High-performance PIM</title>
<link>https://arxiv.org/abs/2511.04321</link>
<guid>https://arxiv.org/abs/2511.04321</guid>
<content:encoded><![CDATA[
arXiv:2511.04321v1 Announce Type: cross 
Abstract: SRAM Processing-in-Memory (PIM) has emerged as the most promising implementation for high-performance PIM, delivering superior computing density, energy efficiency, and computational precision. However, the pursuit of higher performance necessitates more complex circuit designs and increased operating frequencies, which exacerbate IR-drop issues. Severe IR-drop can significantly degrade chip performance and even threaten reliability. Conventional circuit-level IR-drop mitigation methods, such as back-end optimizations, are resource-intensive and often compromise power, performance, and area (PPA). To address these challenges, we propose AIM, comprehensive software and hardware co-design for architecture-level IR-drop mitigation in high-performance PIM. Initially, leveraging the bit-serial and in-situ dataflow processing properties of PIM, we introduce Rtog and HR, which establish a direct correlation between PIM workloads and IR-drop. Building on this foundation, we propose LHR and WDS, enabling extensive exploration of architecture-level IR-drop mitigation while maintaining computational accuracy through software optimization. Subsequently, we develop IR-Booster, a dynamic adjustment mechanism that integrates software-level HR information with hardware-based IR-drop monitoring to adapt the V-f pairs of the PIM macro, achieving enhanced energy efficiency and performance. Finally, we propose the HR-aware task mapping method, bridging software and hardware designs to achieve optimal improvement. Post-layout simulation results on a 7nm 256-TOPS PIM chip demonstrate that AIM achieves up to 69.2% IR-drop mitigation, resulting in 2.29x energy efficiency improvement and 1.152x speedup.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentially Private In-Context Learning with Nearest Neighbor Search</title>
<link>https://arxiv.org/abs/2511.04332</link>
<guid>https://arxiv.org/abs/2511.04332</guid>
<content:encoded><![CDATA[
arXiv:2511.04332v1 Announce Type: cross 
Abstract: Differentially private in-context learning (DP-ICL) has recently become an active research topic due to the inherent privacy risks of in-context learning. However, existing approaches overlook a critical component of modern large language model (LLM) pipelines: the similarity search used to retrieve relevant context data. In this work, we introduce a DP framework for in-context learning that integrates nearest neighbor search of relevant examples in a privacy-aware manner. Our method outperforms existing baselines by a substantial margin across all evaluated benchmarks, achieving more favorable privacy-utility trade-offs. To achieve this, we employ nearest neighbor retrieval from a database of context data, combined with a privacy filter that tracks the cumulative privacy cost of selected samples to ensure adherence to a central differential privacy budget. Experimental results on text classification and document question answering show a clear advantage of the proposed method over existing baselines.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LUME-DBN: Full Bayesian Learning of DBNs from Incomplete data in Intensive Care</title>
<link>https://arxiv.org/abs/2511.04333</link>
<guid>https://arxiv.org/abs/2511.04333</guid>
<content:encoded><![CDATA[
arXiv:2511.04333v1 Announce Type: cross 
Abstract: Dynamic Bayesian networks (DBNs) are increasingly used in healthcare due to their ability to model complex temporal relationships in patient data while maintaining interpretability, an essential feature for clinical decision-making. However, existing approaches to handling missing data in longitudinal clinical datasets are largely derived from static Bayesian networks literature, failing to properly account for the temporal nature of the data. This gap limits the ability to quantify uncertainty over time, which is particularly critical in settings such as intensive care, where understanding the temporal dynamics is fundamental for model trustworthiness and applicability across diverse patient groups. Despite the potential of DBNs, a full Bayesian framework that integrates missing data handling remains underdeveloped. In this work, we propose a novel Gibbs sampling-based method for learning DBNs from incomplete data. Our method treats each missing value as an unknown parameter following a Gaussian distribution. At each iteration, the unobserved values are sampled from their full conditional distributions, allowing for principled imputation and uncertainty estimation. We evaluate our method on both simulated datasets and real-world intensive care data from critically ill patients. Compared to standard model-agnostic techniques such as MICE, our Bayesian approach demonstrates superior reconstruction accuracy and convergence properties. These results highlight the clinical relevance of incorporating full Bayesian inference in temporal models, providing more reliable imputations and offering deeper insight into model behavior. Our approach supports safer and more informed clinical decision-making, particularly in settings where missing data are frequent and potentially impactful.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MusRec: Zero-Shot Text-to-Music Editing via Rectified Flow and Diffusion Transformers</title>
<link>https://arxiv.org/abs/2511.04376</link>
<guid>https://arxiv.org/abs/2511.04376</guid>
<content:encoded><![CDATA[
arXiv:2511.04376v1 Announce Type: cross 
Abstract: Music editing has emerged as an important and practical area of artificial intelligence, with applications ranging from video game and film music production to personalizing existing tracks according to user preferences. However, existing models face significant limitations, such as being restricted to editing synthesized music generated by their own models, requiring highly precise prompts, or necessitating task-specific retraining, thus lacking true zero-shot capability. Leveraging recent advances in rectified flow and diffusion transformers, we introduce MusRec, the first zero-shot text-to-music editing model capable of performing diverse editing tasks on real-world music efficiently and effectively. Experimental results demonstrate that our approach outperforms existing methods in preserving musical content, structural consistency, and editing fidelity, establishing a strong foundation for controllable music editing in real-world scenarios.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spurious Correlation-Aware Embedding Regularization for Worst-Group Robustness</title>
<link>https://arxiv.org/abs/2511.04401</link>
<guid>https://arxiv.org/abs/2511.04401</guid>
<content:encoded><![CDATA[
arXiv:2511.04401v1 Announce Type: cross 
Abstract: Deep learning models achieve strong performance across various domains but often rely on spurious correlations, making them vulnerable to distribution shifts. This issue is particularly severe in subpopulation shift scenarios, where models struggle in underrepresented groups. While existing methods have made progress in mitigating this issue, their performance gains are still constrained. They lack a rigorous theoretical framework connecting the embedding space representations with worst-group error. To address this limitation, we propose Spurious Correlation-Aware Embedding Regularization for Worst-Group Robustness (SCER), a novel approach that directly regularizes feature representations to suppress spurious cues. We show theoretically that worst-group error is influenced by how strongly the classifier relies on spurious versus core directions, identified from differences in group-wise mean embeddings across domains and classes. By imposing theoretical constraints at the embedding level, SCER encourages models to focus on core features while reducing sensitivity to spurious patterns. Through systematic evaluation on multiple vision and language, we show that SCER outperforms prior state-of-the-art studies in worst-group accuracy. Our code is available at \href{https://github.com/MLAI-Yonsei/SCER}{https://github.com/MLAI-Yonsei/SCER}.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Equivalence of Regression and Classification</title>
<link>https://arxiv.org/abs/2511.04422</link>
<guid>https://arxiv.org/abs/2511.04422</guid>
<content:encoded><![CDATA[
arXiv:2511.04422v1 Announce Type: cross 
Abstract: A formal link between regression and classification has been tenuous. Even though the margin maximization term $\|w\|$ is used in support vector regression, it has at best been justified as a regularizer. We show that a regression problem with $M$ samples lying on a hyperplane has a one-to-one equivalence with a linearly separable classification task with $2M$ samples. We show that margin maximization on the equivalent classification task leads to a different regression formulation than traditionally used. Using the equivalence, we demonstrate a ``regressability'' measure, that can be used to estimate the difficulty of regressing a dataset, without needing to first learn a model for it. We use the equivalence to train neural networks to learn a linearizing map, that transforms input variables into a space where a linear regressor is adequate.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Speed at the Cost of Quality? The Impact of LLM Agent Assistance on Software Development</title>
<link>https://arxiv.org/abs/2511.04427</link>
<guid>https://arxiv.org/abs/2511.04427</guid>
<content:encoded><![CDATA[
arXiv:2511.04427v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated the promise to revolutionize the field of software engineering. Among other things, LLM agents are rapidly gaining momentum in their application to software development, with practitioners claiming a multifold productivity increase after adoption. Yet, empirical evidence is lacking around these claims. In this paper, we estimate the causal effect of adopting a widely popular LLM agent assistant, namely Cursor, on development velocity and software quality. The estimation is enabled by a state-of-the-art difference-in-differences design comparing Cursor-adopting GitHub projects with a matched control group of similar GitHub projects that do not use Cursor. We find that the adoption of Cursor leads to a significant, large, but transient increase in project-level development velocity, along with a significant and persistent increase in static analysis warnings and code complexity. Further panel generalized method of moments estimation reveals that the increase in static analysis warnings and code complexity acts as a major factor causing long-term velocity slowdown. Our study carries implications for software engineering practitioners, LLM agent assistant designers, and researchers.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Koopman Economic Model Predictive Control of a Pasteurisation Unit</title>
<link>https://arxiv.org/abs/2511.04437</link>
<guid>https://arxiv.org/abs/2511.04437</guid>
<content:encoded><![CDATA[
arXiv:2511.04437v1 Announce Type: cross 
Abstract: This paper presents a deep Koopman-based Economic Model Predictive Control (EMPC) for efficient operation of a laboratory-scale pasteurization unit (PU). The method uses Koopman operator theory to transform the complex, nonlinear system dynamics into a linear representation, enabling the application of convex optimization while representing the complex PU accurately. The deep Koopman model utilizes neural networks to learn the linear dynamics from experimental data, achieving a 45% improvement in open-loop prediction accuracy over conventional N4SID subspace identification. Both analyzed models were employed in the EMPC formulation that includes interpretable economic costs, such as energy consumption, material losses due to inadequate pasteurization, and actuator wear. The feasibility of EMPC is ensured using slack variables. The deep Koopman EMPC and N4SID EMPC are numerically validated on a nonlinear model of multivariable PU under external disturbance. The disturbances include feed pump fail-to-close scenario and the introduction of a cold batch to be pastuerized. These results demonstrate that the deep Koopmand EMPC achieves a 32% reduction in total economic cost compared to the N4SID baseline. This improvement is mainly due to the reductions in material losses and energy consumption. Furthermore, the steady-state operation via Koopman-based EMPC requires 10.2% less electrical energy. The results highlight the practical advantages of integrating deep Koopman representations with economic optimization to achieve resource-efficient control of thermal-intensive plants.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Dictionary-Free Method for Identifying Linear Model of Nonlinear System with Input Delay</title>
<link>https://arxiv.org/abs/2511.04451</link>
<guid>https://arxiv.org/abs/2511.04451</guid>
<content:encoded><![CDATA[
arXiv:2511.04451v1 Announce Type: cross 
Abstract: Nonlinear dynamical systems with input delays pose significant challenges for prediction, estimation, and control due to their inherent complexity and the impact of delays on system behavior. Traditional linear control techniques often fail in these contexts, necessitating innovative approaches. This paper introduces a novel approach to approximate the Koopman operator using an LSTM-enhanced Deep Koopman model, enabling linear representations of nonlinear systems with time delays. By incorporating Long Short-Term Memory (LSTM) layers, the proposed framework captures historical dependencies and efficiently encodes time-delayed system dynamics into a latent space. Unlike traditional extended Dynamic Mode Decomposition (eDMD) approaches that rely on predefined dictionaries, the LSTM-enhanced Deep Koopman model is dictionary-free, which mitigates the problems with the underlying dynamics being known and incorporated into the dictionary. Quantitative comparisons with extended eDMD on a simulated system demonstrate highly significant performance gains in prediction accuracy in cases where the true nonlinear dynamics are unknown and achieve comparable results to eDMD with known dynamics of a system.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fraud-Proof Revenue Division on Subscription Platforms</title>
<link>https://arxiv.org/abs/2511.04465</link>
<guid>https://arxiv.org/abs/2511.04465</guid>
<content:encoded><![CDATA[
arXiv:2511.04465v1 Announce Type: cross 
Abstract: We study a model of subscription-based platforms where users pay a fixed fee for unlimited access to content, and creators receive a share of the revenue. Existing approaches to detecting fraud predominantly rely on machine learning methods, engaging in an ongoing arms race with bad actors. We explore revenue division mechanisms that inherently disincentivize manipulation. We formalize three types of manipulation-resistance axioms and examine which existing rules satisfy these. We show that a mechanism widely used by streaming platforms, not only fails to prevent fraud, but also makes detecting manipulation computationally intractable. We also introduce a novel rule, ScaledUserProp, that satisfies all three manipulation-resistance axioms. Finally, experiments with both real-world and synthetic streaming data support ScaledUserProp as a fairer alternative compared to existing rules.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ground-Truth Subgraphs for Better Training and Evaluation of Knowledge Graph Augmented LLMs</title>
<link>https://arxiv.org/abs/2511.04473</link>
<guid>https://arxiv.org/abs/2511.04473</guid>
<content:encoded><![CDATA[
arXiv:2511.04473v1 Announce Type: cross 
Abstract: Retrieval of information from graph-structured knowledge bases represents a promising direction for improving the factuality of LLMs. While various solutions have been proposed, a comparison of methods is difficult due to the lack of challenging QA datasets with ground-truth targets for graph retrieval. We present SynthKGQA, a framework for generating high-quality synthetic Knowledge Graph Question Answering datasets from any Knowledge Graph, providing the full set of ground-truth facts in the KG to reason over each question. We show how, in addition to enabling more informative benchmarking of KG retrievers, the data produced with SynthKGQA also allows us to train better models. We apply SynthKGQA to Wikidata to generate GTSQA, a new dataset designed to test zero-shot generalization abilities of KG retrievers with respect to unseen graph structures and relation types, and benchmark popular solutions for KG-augmented LLMs on it.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generate, Evaluate, Iterate: Synthetic Data for Human-in-the-Loop Refinement of LLM Judges</title>
<link>https://arxiv.org/abs/2511.04478</link>
<guid>https://arxiv.org/abs/2511.04478</guid>
<content:encoded><![CDATA[
arXiv:2511.04478v1 Announce Type: cross 
Abstract: The LLM-as-a-judge paradigm enables flexible, user-defined evaluation, but its effectiveness is often limited by the scarcity of diverse, representative data for refining criteria. We present a tool that integrates synthetic data generation into the LLM-as-a-judge workflow, empowering users to create tailored and challenging test cases with configurable domains, personas, lengths, and desired outcomes, including borderline cases. The tool also supports AI-assisted inline editing of existing test cases. To enhance transparency and interpretability, it reveals the prompts and explanations behind each generation. In a user study (N=24), 83% of participants preferred the tool over manually creating or selecting test cases, as it allowed them to rapidly generate diverse synthetic data without additional workload. The generated synthetic data proved as effective as hand-crafted data for both refining evaluation criteria and aligning with human preferences. These findings highlight synthetic data as a promising alternative, particularly in contexts where efficiency and scalability are critical.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Q3R: Quadratic Reweighted Rank Regularizer for Effective Low-Rank Training</title>
<link>https://arxiv.org/abs/2511.04485</link>
<guid>https://arxiv.org/abs/2511.04485</guid>
<content:encoded><![CDATA[
arXiv:2511.04485v1 Announce Type: cross 
Abstract: Parameter-efficient training, based on low-rank optimization, has become a highly successful tool for fine-tuning large deep-learning models. However, these methods fail at low-rank pre-training tasks where maintaining the low-rank structure and the objective remains a challenging task. We propose the Quadratic Reweighted Rank Regularizer dubbed Q3R, which leads to a novel low-rank inducing training strategy inspired by the iteratively reweighted least squares (IRLS) framework. Q3R is based on a quadratic regularizer term which majorizes a smoothed log determinant serving as rank surrogate objective. Unlike other low-rank training techniques, Q3R is able to train weight matrices with prescribed, low target ranks of models that achieve comparable predictive performance as dense models, with small computational overhead, while remaining fully compatible with existing architectures. For example, we demonstrated one experiment where we are able to truncate $60\%$ and $80\%$ of the parameters of a ViT-Tiny model with $~1.3\%$ and $~4\%$ accuracy drop in CIFAR-10 performance respectively. The efficacy of Q3R is confirmed on Transformers across both image and language tasks, including for low-rank fine-tuning.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within Structured Tables</title>
<link>https://arxiv.org/abs/2511.04491</link>
<guid>https://arxiv.org/abs/2511.04491</guid>
<content:encoded><![CDATA[
arXiv:2511.04491v1 Announce Type: cross 
Abstract: Existing tabular reasoning benchmarks mostly test models on small, uniform tables, underrepresenting the complexity of real-world data and giving an incomplete view of Large Language Models' (LLMs) reasoning abilities. Real tables are long, heterogeneous, and domain-specific, mixing structured fields with free text and requiring multi-hop reasoning across thousands of tokens. To address this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from 2031 real-world tables spanning two domains: i) RB-Science (NSF grant records) and ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates LLMs jointly across scale, heterogeneity, domain specificity, and reasoning complexity. Experiments with open-source and proprietary models show that LLMs struggle with heterogeneous schemas and complex multi-hop inference, revealing persistent weaknesses in current architectures and prompting strategies. RUST-BENCH establishes a challenging new testbed for advancing tabular reasoning research.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OUNLP at TSAR 2025 Shared Task: Multi-Round Text Simplifier via Code Generation</title>
<link>https://arxiv.org/abs/2511.04495</link>
<guid>https://arxiv.org/abs/2511.04495</guid>
<content:encoded><![CDATA[
arXiv:2511.04495v1 Announce Type: cross 
Abstract: This paper describes the OUNLP system submitted to the TSAR-2025 Shared Task (Alva-Manchego et al., 2025), designed for readability-controlled text simplification using LLM-prompting-based generation. Based on the analysis of prompt-based text simplification methods, we discovered an interesting finding that text simplification performance is highly related to the gap between the source CEFR (Arase et al., 2022) level and the target CEFR level. Inspired by this finding, we propose two multi-round simplification methods and generate them via GPT-4o: rule-based simplification (MRS-Rule) and jointly rule-based LLM simplification (MRS-Joint). Our submitted systems ranked 7 out of 20 teams. Later improvements with MRS-Joint show that taking the LLM simplified candidates as the starting point could further boost the multi-round simplification performance.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoding Emergent Big Five Traits in Large Language Models: Temperature-Dependent Expression and Architectural Clustering</title>
<link>https://arxiv.org/abs/2511.04499</link>
<guid>https://arxiv.org/abs/2511.04499</guid>
<content:encoded><![CDATA[
arXiv:2511.04499v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) become integral to human-centered applications, understanding their personality-like behaviors is increasingly important for responsible development and deployment. This paper systematically evaluates six LLMs, applying the Big Five Inventory-2 (BFI-2) framework, to assess trait expressions under varying sampling temperatures. We find significant differences across four of the five personality dimensions, with Neuroticism and Extraversion susceptible to temperature adjustments. Further, hierarchical clustering reveals distinct model clusters, suggesting that architectural features may predispose certain models toward stable trait profiles. Taken together, these results offer new insights into the emergence of personality-like patterns in LLMs and provide a new perspective on model tuning, selection, and the ethical governance of AI systems. We share the data and code for this analysis here: https://osf.io/bsvzc/?view_only=6672219bede24b4e875097426dc3fac1
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAGalyst: Automated Human-Aligned Agentic Evaluation for Domain-Specific RAG</title>
<link>https://arxiv.org/abs/2511.04502</link>
<guid>https://arxiv.org/abs/2511.04502</guid>
<content:encoded><![CDATA[
arXiv:2511.04502v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) is a critical technique for grounding Large Language Models (LLMs) in factual evidence, yet evaluating RAG systems in specialized, safety-critical domains remains a significant challenge. Existing evaluation frameworks often rely on heuristic-based metrics that fail to capture domain-specific nuances and other works utilize LLM-as-a-Judge approaches that lack validated alignment with human judgment. This paper introduces RAGalyst, an automated, human-aligned agentic framework designed for the rigorous evaluation of domain-specific RAG systems. RAGalyst features an agentic pipeline that generates high-quality, synthetic question-answering (QA) datasets from source documents, incorporating an agentic filtering step to ensure data fidelity. The framework refines two key LLM-as-a-Judge metrics-Answer Correctness and Answerability-using prompt optimization to achieve a strong correlation with human annotations. Applying this framework to evaluate various RAG components across three distinct domains (military operations, cybersecurity, and bridge engineering), we find that performance is highly context-dependent. No single embedding model, LLM, or hyperparameter configuration proves universally optimal. Additionally, we provide an analysis on the most common low Answer Correctness reasons in RAG. These findings highlight the necessity of a systematic evaluation framework like RAGalyst, which empowers practitioners to uncover domain-specific trade-offs and make informed design choices for building reliable and effective RAG systems. RAGalyst is available on our Github.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Alternative Fairness and Accuracy Optimization in Criminal Justice</title>
<link>https://arxiv.org/abs/2511.04505</link>
<guid>https://arxiv.org/abs/2511.04505</guid>
<content:encoded><![CDATA[
arXiv:2511.04505v1 Announce Type: cross 
Abstract: Algorithmic fairness has grown rapidly as a research area, yet key concepts remain unsettled, especially in criminal justice. We review group, individual, and process fairness and map the conditions under which they conflict. We then develop a simple modification to standard group fairness. Rather than exact parity across protected groups, we minimize a weighted error loss while keeping differences in false negative rates within a small tolerance. This makes solutions easier to find, can raise predictive accuracy, and surfaces the ethical choice of error costs. We situate this proposal within three classes of critique: biased and incomplete data, latent affirmative action, and the explosion of subgroup constraints. Finally, we offer a practical framework for deployment in public decision systems built on three pillars: need-based decisions, Transparency and accountability, and narrowly tailored definitions and solutions. Together, these elements link technical design to legitimacy and provide actionable guidance for agencies that use risk assessment and related tools.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are language models aware of the road not taken? Token-level uncertainty and hidden state dynamics</title>
<link>https://arxiv.org/abs/2511.04527</link>
<guid>https://arxiv.org/abs/2511.04527</guid>
<content:encoded><![CDATA[
arXiv:2511.04527v1 Announce Type: cross 
Abstract: When a language model generates text, the selection of individual tokens might lead it down very different reasoning paths, making uncertainty difficult to quantify. In this work, we consider whether reasoning language models represent the alternate paths that they could take during generation. To test this hypothesis, we use hidden activations to control and predict a language model's uncertainty during chain-of-thought reasoning. In our experiments, we find a clear correlation between how uncertain a model is at different tokens, and how easily the model can be steered by controlling its activations. This suggests that activation interventions are most effective when there are alternate paths available to the model -- in other words, when it has not yet committed to a particular final answer. We also find that hidden activations can predict a model's future outcome distribution, demonstrating that models implicitly represent the space of possible paths.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-as-a-Judge: Toward World Models for Slate Recommendation Systems</title>
<link>https://arxiv.org/abs/2511.04541</link>
<guid>https://arxiv.org/abs/2511.04541</guid>
<content:encoded><![CDATA[
arXiv:2511.04541v1 Announce Type: cross 
Abstract: Modeling user preferences across domains remains a key challenge in slate recommendation (i.e. recommending an ordered sequence of items) research. We investigate how Large Language Models (LLM) can effectively act as world models of user preferences through pairwise reasoning over slates. We conduct an empirical study involving several LLMs on three tasks spanning different datasets. Our results reveal relationships between task performance and properties of the preference function captured by LLMs, hinting towards areas for improvement and highlighting the potential of LLMs as world models in recommender systems.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integrating Temporal and Structural Context in Graph Transformers for Relational Deep Learning</title>
<link>https://arxiv.org/abs/2511.04557</link>
<guid>https://arxiv.org/abs/2511.04557</guid>
<content:encoded><![CDATA[
arXiv:2511.04557v1 Announce Type: cross 
Abstract: In domains such as healthcare, finance, and e-commerce, the temporal dynamics of relational data emerge from complex interactions-such as those between patients and providers, or users and products across diverse categories. To be broadly useful, models operating on these data must integrate long-range spatial and temporal dependencies across diverse types of entities, while also supporting multiple predictive tasks. However, existing graph models for relational data primarily focus on spatial structure, treating temporal information merely as a filtering constraint to exclude future events rather than a modeling signal, and are typically designed for single-task prediction. To address these gaps, we introduce a temporal subgraph sampler that enhances global context by retrieving nodes beyond the immediate neighborhood to capture temporally relevant relationships. In addition, we propose the Relational Graph Perceiver (RGP), a graph transformer architecture for relational deep learning that leverages a cross-attention-based latent bottleneck to efficiently integrate information from both structural and temporal contexts. This latent bottleneck integrates signals from different node and edge types into a common latent space, enabling the model to build global context across the entire relational system. RGP also incorporates a flexible cross-attention decoder that supports joint learning across tasks with disjoint label spaces within a single model. Experiments on RelBench, SALT, and CTU show that RGP delivers state-of-the-art performance, offering a general and scalable solution for relational deep learning with support for diverse predictive tasks.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Addressing divergent representations from causal interventions on neural networks</title>
<link>https://arxiv.org/abs/2511.04638</link>
<guid>https://arxiv.org/abs/2511.04638</guid>
<content:encoded><![CDATA[
arXiv:2511.04638v1 Announce Type: cross 
Abstract: A common approach to mechanistic interpretability is to causally manipulate model representations via targeted interventions in order to understand what those representations encode. Here we ask whether such interventions create out-of-distribution (divergent) representations, and whether this raises concerns about how faithful their resulting explanations are to the target model in its natural state. First, we demonstrate empirically that common causal intervention techniques often do shift internal representations away from the natural distribution of the target model. Then, we provide a theoretical analysis of two classes of such divergences: `harmless' divergences that occur in the null-space of the weights and from covariance within behavioral decision boundaries, and `pernicious' divergences that activate hidden network pathways and cause dormant behavioral changes. Finally, in an effort to mitigate the pernicious cases, we modify the Counterfactual Latent (CL) loss from Grant (2025) that regularizes interventions to remain closer to the natural distributions, reducing the likelihood of harmful divergences while preserving the interpretive power of interventions. Together, these results highlight a path towards more reliable interpretability methods.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>X-Diffusion: Training Diffusion Policies on Cross-Embodiment Human Demonstrations</title>
<link>https://arxiv.org/abs/2511.04671</link>
<guid>https://arxiv.org/abs/2511.04671</guid>
<content:encoded><![CDATA[
arXiv:2511.04671v1 Announce Type: cross 
Abstract: Human videos can be recorded quickly and at scale, making them an appealing source of training data for robot learning. However, humans and robots differ fundamentally in embodiment, resulting in mismatched action execution. Direct kinematic retargeting of human hand motion can therefore produce actions that are physically infeasible for robots. Despite these low-level differences, human demonstrations provide valuable motion cues about how to manipulate and interact with objects. Our key idea is to exploit the forward diffusion process: as noise is added to actions, low-level execution differences fade while high-level task guidance is preserved. We present X-Diffusion, a principled framework for training diffusion policies that maximally leverages human data without learning dynamically infeasible motions. X-Diffusion first trains a classifier to predict whether a noisy action is executed by a human or robot. Then, a human action is incorporated into policy training only after adding sufficient noise such that the classifier cannot discern its embodiment. Actions consistent with robot execution supervise fine-grained denoising at low noise levels, while mismatched human actions provide only coarse guidance at higher noise levels. Our experiments show that naive co-training under execution mismatches degrades policy performance, while X-Diffusion consistently improves it. Across five manipulation tasks, X-Diffusion achieves a 16% higher average success rate than the best baseline. The project website is available at https://portal-cornell.github.io/X-Diffusion/.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Discussion Graph Semantics of First-Order Logic with Equality for Reasoning about Discussion and Argumentation</title>
<link>https://arxiv.org/abs/2406.12163</link>
<guid>https://arxiv.org/abs/2406.12163</guid>
<content:encoded><![CDATA[
arXiv:2406.12163v2 Announce Type: replace 
Abstract: We make three contributions. First, we formulate a discussion-graph semantics for first-order logic with equality, enabling reasoning about discussion and argumentation in AI more generally than before. This addresses the current lack of a formal reasoning framework capable of handling diverse discussion and argumentation models. Second, we generalise Dung's notion of extensions to cases where two or more graph nodes in an argumentation framework are equivalent. Third, we connect these two contributions by showing that the generalised extensions are first-order characterisable within the proposed discussion-graph semantics. Propositional characterisability of all Dung's extensions is an immediate consequence.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Collaboration Dynamics and Reliability Challenges of Multi-Agent LLM Systems in Finite Element Analysis</title>
<link>https://arxiv.org/abs/2408.13406</link>
<guid>https://arxiv.org/abs/2408.13406</guid>
<content:encoded><![CDATA[
arXiv:2408.13406v2 Announce Type: replace 
Abstract: Large Language Model (LLM)-based multi-agent systems are increasingly applied to automate computational workflows in science and engineering. However, how inter-agent dynamics influence reasoning quality and verification reliability remains unclear. We study these mechanisms using an AutoGen-based multi-agent framework for linear-elastic Finite Element Analysis (FEA), evaluating seven role configurations across four tasks under a fixed 12-turn conversation limit. From 1,120 controlled trials, we find that collaboration effectiveness depends more on functional complementarity than team size: the three-agent Coder-Executor-Critic configuration uniquely produced physically and visually correct solutions, while adding redundant reviewers reduced success rates. Yet three systematic failure modes persist: (1) affirmation bias, where the Rebuttal agent endorsed rather than challenged outputs (85-92% agreement, including errors); (2) premature consensus caused by redundant reviewers; and (3) a verification-validation gap where executable but physically incorrect code passed undetected. No agent combination successfully validated constitutive relations in complex tasks. Building on theories of functional diversity, role differentiation, and computational validation, we propose actionable design principles: (i) assign complementary agent roles, (ii) enforce multi-level validation (execution, specification, physics), and (iii) prevent early consensus through adversarial or trigger-based interaction control. These findings establish a principled foundation for designing trustworthy LLM collaborations in engineering workflows.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>"Let's Agree to Disagree": Investigating the Disagreement Problem in Explainable AI for Text Summarization</title>
<link>https://arxiv.org/abs/2410.18560</link>
<guid>https://arxiv.org/abs/2410.18560</guid>
<content:encoded><![CDATA[
arXiv:2410.18560v2 Announce Type: replace 
Abstract: Explainable Artificial Intelligence (XAI) methods in text summarization are essential for understanding the model behavior and fostering trust in model-generated summaries. Despite the effectiveness of XAI methods, recent studies have highlighted a key challenge in this area known as the "disagreement problem". This problem occurs when different XAI methods yield conflicting explanations for the same model outcome. Such discrepancies raise concerns about the consistency of explanations and reduce confidence in model interpretations, which is crucial for secure and accountable AI applications. This work is among the first to empirically investigate the disagreement problem in text summarization, demonstrating that such discrepancies are widespread in state-of-the-art summarization models. To address this gap, we propose Regional Explainable AI (RXAI) a novel segmentation-based approach, where each article is divided into smaller, coherent segments using sentence transformers and clustering. We use XAI methods on text segments to create localized explanations that help reduce disagreement between different XAI methods, thereby enhancing the trustworthiness of AI-generated summaries. Our results illustrate that the localized explanations are more consistent than full-text explanations. The proposed approach is validated using two benchmark summarization datasets, Extreme summarization (Xsum) and CNN/Daily Mail, indicating a substantial decrease in disagreement. Additionally, the interactive JavaScript visualization tool is developed to facilitate easy, color-coded exploration of attribution scores at the sentence level, enhancing user comprehension of model explanations.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Building Altruistic and Moral AI Agent with Brain-inspired Emotional Empathy Mechanisms</title>
<link>https://arxiv.org/abs/2410.21882</link>
<guid>https://arxiv.org/abs/2410.21882</guid>
<content:encoded><![CDATA[
arXiv:2410.21882v2 Announce Type: replace 
Abstract: As AI closely interacts with human society, it is crucial to ensure that its behavior is safe, altruistic, and aligned with human ethical and moral values. However, existing research on embedding ethical considerations into AI remains insufficient, and previous external constraints based on principles and rules are inadequate to provide AI with long-term stability and generalization capabilities. Emotional empathy intrinsically motivates altruistic behaviors aimed at alleviating others' negative emotions through emotional sharing and contagion mechanisms. Motivated by this, we draw inspiration from the neural mechanism of human emotional empathy-driven altruistic decision making, and simulate the shared self-other perception-mirroring-empathy neural circuits, to construct a brain-inspired emotional empathy-driven altruistic decision-making model. Here, empathy directly impacts dopamine release to form intrinsic altruistic motivation. The proposed model exhibits consistent altruistic behaviors across three experimental settings: emotional contagion-integrated two-agent altruistic rescue, multi-agent gaming, and robotic emotional empathy interaction scenarios. In-depth analyses validate the positive correlation between empathy levels and altruistic preferences (consistent with psychological behavioral experiment findings), while also demonstrating how interaction partners' empathy levels influence the agent's behavioral patterns. We further test the proposed model's performance and stability in moral dilemmas involving conflicts between self-interest and others' well-being, partially observable environments, and adversarial defense scenarios. This work provides preliminary exploration of human-like empathy-driven altruistic moral decision making, contributing potential perspectives for developing ethically-aligned AI.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Style2Code: A Style-Controllable Code Generation Framework with Dual-Modal Contrastive Representation Learning</title>
<link>https://arxiv.org/abs/2505.19442</link>
<guid>https://arxiv.org/abs/2505.19442</guid>
<content:encoded><![CDATA[
arXiv:2505.19442v3 Announce Type: replace 
Abstract: Controllable code generation, the ability to synthesize code that follows a specified style while maintaining functionality, remains a challenging task. We propose a two-stage training framework combining contrastive learning and conditional decoding to enable flexible style control. The first stage aligns code style representations with semantic and structural features. In the second stage, we fine-tune a language model (e.g., Flan-T5) conditioned on the learned style vector to guide generation. Our method supports style interpolation and user personalization via lightweight mixing. Compared to prior work, our unified framework offers improved stylistic control without sacrificing code correctness. This is among the first approaches to combine contrastive alignment with conditional decoding for style-guided code generation.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating LLM-Contaminated Crowdsourcing Data Without Ground Truth</title>
<link>https://arxiv.org/abs/2506.06991</link>
<guid>https://arxiv.org/abs/2506.06991</guid>
<content:encoded><![CDATA[
arXiv:2506.06991v2 Announce Type: replace 
Abstract: The recent success of generative AI highlights the crucial role of high-quality human feedback in building trustworthy AI systems. However, the increasing use of large language models (LLMs) by crowdsourcing workers poses a significant challenge: datasets intended to reflect human input may be compromised by LLM-generated responses. Existing LLM detection approaches often rely on high-dimensional training data such as text, making them unsuitable for annotation tasks like multiple-choice labeling. In this work, we investigate the potential of peer prediction -- a mechanism that evaluates the information within workers' responses without using ground truth -- to mitigate LLM-assisted cheating in crowdsourcing with a focus on annotation tasks. Our approach quantifies the correlations between worker answers while conditioning on (a subset of) LLM-generated labels available to the requester. Building on prior research, we propose a training-free scoring mechanism with theoretical guarantees under a crowdsourcing model that accounts for LLM collusion. We establish conditions under which our method is effective and empirically demonstrate its robustness in detecting low-effort cheating on real-world crowdsourcing datasets.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-modal Causal Intervention for Alzheimer's Disease Prediction</title>
<link>https://arxiv.org/abs/2507.13956</link>
<guid>https://arxiv.org/abs/2507.13956</guid>
<content:encoded><![CDATA[
arXiv:2507.13956v2 Announce Type: replace 
Abstract: Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's Disease (AD), where early identification and intervention can effectively slow the progression to dementia. However, diagnosing AD remains a significant challenge in neurology due to the confounders caused mainly by the selection bias of multi-modal data and the complex relationships between variables. To address these issues, we propose a novel visual-language causality-inspired framework named Cross-modal Causal Intervention with Mediator for Alzheimer's Disease Diagnosis (MediAD) for diagnostic assistance. Our MediAD employs Large Language Models (LLMs) to summarize clinical data under strict templates, therefore enriching textual inputs. The MediAD model utilizes Magnetic Resonance Imaging (MRI), clinical data, and textual data enriched by LLMs to classify participants into Cognitively Normal (CN), MCI, and AD categories. Because of the presence of confounders, such as cerebral vascular lesions and age-related biomarkers, non-causal models are likely to capture spurious input-output correlations, generating less reliable results. Our framework implicitly mitigates the effect of both observable and unobservable confounders through a unified causal intervention method. Experimental results demonstrate the outstanding performance of our method in distinguishing CN/MCI/AD cases, outperforming other methods in most evaluation metrics. The study showcases the potential of integrating causal reasoning with multi-modal learning for neurological disease diagnosis.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structured Debate Improves Corporate Credit Reasoning in Financial AI</title>
<link>https://arxiv.org/abs/2510.17108</link>
<guid>https://arxiv.org/abs/2510.17108</guid>
<content:encoded><![CDATA[
arXiv:2510.17108v2 Announce Type: replace 
Abstract: Despite advances in financial AI, the automation of evidence-based reasoning remains unresolved in corporate credit assessment, where qualitative non-financial indicators exert decisive influence on loan repayment outcomes yet resist formalization. Existing approaches focus predominantly on numerical prediction and provide limited support for the interpretive judgments required in professional loan evaluation. This study develops and evaluates two operational large language model (LLM)-based systems designed to generate structured reasoning from non-financial evidence. The first is a non-adversarial single-agent system (NAS) that produces bidirectional analysis through a single-pass reasoning pipeline. The second is a debate-based multi-agent system (KPD-MADS) that operationalizes adversarial verification through a ten-step structured interaction protocol grounded in Karl Popper's critical dialogue framework. Both systems were applied to three real corporate cases and evaluated by experienced credit risk professionals. Compared to manual expert reporting, both systems achieved substantial productivity gains (NAS: 11.55 s per case; KPD-MADS: 91.97 s; human baseline: 1920 s). The KPD-MADS demonstrated superior reasoning quality, receiving higher median ratings in explanatory adequacy (4.0 vs. 3.0), practical applicability (4.0 vs. 3.0), and usability (62.5 vs. 52.5). These findings show that structured multi-agent interaction can enhance reasoning rigor and interpretability in financial AI, advancing scalable and defensible automation in corporate credit assessment.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Principle of Targeted Intervention for Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.17697</link>
<guid>https://arxiv.org/abs/2510.17697</guid>
<content:encoded><![CDATA[
arXiv:2510.17697v4 Announce Type: replace 
Abstract: Steering cooperative multi-agent reinforcement learning (MARL) towards desired outcomes is challenging, particularly when the global guidance from a human on the whole multi-agent system is impractical in a large-scale MARL. On the other hand, designing external mechanisms (e.g., intrinsic rewards and human feedback) to coordinate agents mostly relies on empirical studies, lacking a easy-to-use research tool. In this work, we employ multi-agent influence diagrams (MAIDs) as a graphical framework to address the above issues. First, we introduce the concept of MARL interaction paradigms (orthogonal to MARL learning paradigms), using MAIDs to analyze and visualize both unguided self-organization and global guidance mechanisms in MARL. Then, we design a new MARL interaction paradigm, referred to as the targeted intervention paradigm that is applied to only a single targeted agent, so the problem of global guidance can be mitigated. In implementation, we introduce a causal inference technique, referred to as Pre-Strategy Intervention (PSI), to realize the targeted intervention paradigm. Since MAIDs can be regarded as a special class of causal diagrams, a composite desired outcome that integrates the primary task goal and an additional desired outcome can be achieved by maximizing the corresponding causal effect through the PSI. Moreover, the bundled relevance graph analysis of MAIDs provides a tool to identify whether an MARL learning paradigm is workable under the design of an MARL interaction paradigm. In experiments, we demonstrate the effectiveness of our proposed targeted intervention, and verify the result of relevance graph analysis.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seg the HAB: Language-Guided Geospatial Algae Bloom Reasoning and Segmentation</title>
<link>https://arxiv.org/abs/2510.18751</link>
<guid>https://arxiv.org/abs/2510.18751</guid>
<content:encoded><![CDATA[
arXiv:2510.18751v2 Announce Type: replace 
Abstract: Climate change is intensifying the occurrence of harmful algal bloom (HAB), particularly cyanobacteria, which threaten aquatic ecosystems and human health through oxygen depletion, toxin release, and disruption of marine biodiversity. Traditional monitoring approaches, such as manual water sampling, remain labor-intensive and limited in spatial and temporal coverage. Recent advances in vision-language models (VLMs) for remote sensing have shown potential for scalable AI-driven solutions, yet challenges remain in reasoning over imagery and quantifying bloom severity. In this work, we introduce ALGae Observation and Segmentation (ALGOS), a segmentation-and-reasoning system for HAB monitoring that combines remote sensing image understanding with severity estimation. Our approach integrates GeoSAM-assisted human evaluation for high-quality segmentation mask curation and fine-tunes vision language model on severity prediction using the Cyanobacteria Aggregated Manual Labels (CAML) from NASA. Experiments demonstrate that ALGOS achieves robust performance on both segmentation and severity-level estimation, paving the way toward practical and automated cyanobacterial monitoring systems.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Clinically Grounded Foundation Models in Pathology</title>
<link>https://arxiv.org/abs/2510.23807</link>
<guid>https://arxiv.org/abs/2510.23807</guid>
<content:encoded><![CDATA[
arXiv:2510.23807v3 Announce Type: replace 
Abstract: In non-medical domains, foundation models (FMs) have revolutionized computer vision and language processing through large-scale self-supervised and multimodal learning. Consequently, their rapid adoption in computational pathology was expected to deliver comparable breakthroughs in cancer diagnosis, prognostication, and multimodal retrieval. However, recent systematic evaluations reveal fundamental weaknesses: low diagnostic accuracy, poor robustness, geometric instability, heavy computational demands, and concerning safety vulnerabilities. This short paper examines these shortcomings and argues that they stem from deeper conceptual mismatches between the assumptions underlying generic foundation modeling in mainstream AI and the intrinsic complexity of human tissue. Seven interrelated causes are identified: biological complexity, ineffective self-supervision, overgeneralization, excessive architectural complexity, lack of domain-specific innovation, insufficient data, and a fundamental design flaw related to tissue patch size. These findings suggest that current pathology foundation models remain conceptually misaligned with the nature of tissue morphology and call for a fundamental rethinking of the paradigm itself.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BOTS: A Unified Framework for Bayesian Online Task Selection in LLM Reinforcement Finetuning</title>
<link>https://arxiv.org/abs/2510.26374</link>
<guid>https://arxiv.org/abs/2510.26374</guid>
<content:encoded><![CDATA[
arXiv:2510.26374v2 Announce Type: replace 
Abstract: Reinforcement finetuning (RFT) is a key technique for aligning Large Language Models (LLMs) with human preferences and enhancing reasoning, yet its effectiveness is highly sensitive to which tasks are explored during training. Uniform task sampling is inefficient, wasting computation on tasks that are either trivial or unsolvable, while existing task selection methods often suffer from high rollout costs, poor adaptivity, or incomplete evidence. We introduce BOTS, a unified framework for Bayesian Online Task Selection in LLM reinforcement finetuning. Grounded in Bayesian inference, BOTS adaptively maintains posterior estimates of task difficulty as the model evolves. It jointly incorporates explicit evidence from direct evaluations of selected tasks and implicit evidence inferred from these evaluations for unselected tasks, with Thompson sampling ensuring a principled balance between exploration and exploitation. To make implicit evidence practical, we instantiate it with an ultra-light interpolation-based plug-in that estimates difficulties of unevaluated tasks without extra rollouts, adding negligible overhead. Empirically, across diverse domains and LLM scales, BOTS consistently improves data efficiency and performance over baselines and ablations, providing a practical and extensible solution for dynamic task selection in RFT.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unified Kernel for Neural Network Learning</title>
<link>https://arxiv.org/abs/2403.17467</link>
<guid>https://arxiv.org/abs/2403.17467</guid>
<content:encoded><![CDATA[
arXiv:2403.17467v2 Announce Type: replace-cross 
Abstract: Past decades have witnessed a great interest in the distinction and connection between neural network learning and kernel learning. Recent advancements have made theoretical progress in connecting infinite-wide neural networks and Gaussian processes. Two predominant approaches have emerged: the Neural Network Gaussian Process (NNGP) and the Neural Tangent Kernel (NTK). The former, rooted in Bayesian inference, represents a zero-order kernel, while the latter, grounded in the tangent space of gradient descents, is a first-order kernel. In this paper, we present the Unified Neural Kernel (UNK), which {is induced by the inner product of produced variables and characterizes the learning dynamics of neural networks with gradient descents and parameter initialization.} The proposed UNK kernel maintains the limiting properties of both NNGP and NTK, exhibiting behaviors akin to NTK with a finite learning step and converging to NNGP as the learning step approaches infinity. Besides, we also theoretically characterize the uniform tightness and learning convergence of the UNK kernel, providing comprehensive insights into this unified kernel. Experimental results underscore the effectiveness of our proposed method.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stochastic Diffusion: A Diffusion Probabilistic Model for Stochastic Time Series Forecasting</title>
<link>https://arxiv.org/abs/2406.02827</link>
<guid>https://arxiv.org/abs/2406.02827</guid>
<content:encoded><![CDATA[
arXiv:2406.02827v3 Announce Type: replace-cross 
Abstract: Recent innovations in diffusion probabilistic models have paved the way for significant progress in image, text and audio generation, leading to their applications in generative time series forecasting. However, leveraging such abilities to model highly stochastic time series data remains a challenge. In this paper, we propose a novel Stochastic Diffusion (StochDiff) model which learns data-driven prior knowledge at each time step by utilizing the representational power of the stochastic latent spaces to model the variability of the multivariate time series data. The learnt prior knowledge helps the model to capture complex temporal dynamics and the inherent uncertainty of the data. This improves its ability to model highly stochastic time series data. Through extensive experiments on real-world datasets, we demonstrate the effectiveness of our proposed model on stochastic time series forecasting. Additionally, we showcase an application of our model for real-world surgical guidance, highlighting its potential to benefit the medical community.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Projection Methods for Operator Learning and Universal Approximation</title>
<link>https://arxiv.org/abs/2406.12264</link>
<guid>https://arxiv.org/abs/2406.12264</guid>
<content:encoded><![CDATA[
arXiv:2406.12264v3 Announce Type: replace-cross 
Abstract: We obtain a new universal approximation theorem for continuous (possibly nonlinear) operators on arbitrary Banach spaces using the Leray-Schauder mapping. Moreover, we introduce and study a method for operator learning in Banach spaces $L^p$ of functions with multiple variables, based on orthogonal projections on polynomial bases. We derive a universal approximation result for operators where we learn a linear projection and a finite dimensional mapping under some additional assumptions. For the case of $p=2$, we give some sufficient conditions for the approximation results to hold. This article serves as the theoretical framework for a deep learning methodology in operator learning.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM Targeted Underperformance Disproportionately Impacts Vulnerable Users</title>
<link>https://arxiv.org/abs/2406.17737</link>
<guid>https://arxiv.org/abs/2406.17737</guid>
<content:encoded><![CDATA[
arXiv:2406.17737v2 Announce Type: replace-cross 
Abstract: While state-of-the-art large language models (LLMs) have shown impressive performance on many tasks, there has been extensive research on undesirable model behavior such as hallucinations and bias. In this work, we investigate how the quality of LLM responses changes in terms of information accuracy, truthfulness, and refusals depending on three user traits: English proficiency, education level, and country of origin. We present extensive experimentation on three state-of-the-art LLMs and two different datasets targeting truthfulness and factuality. Our findings suggest that undesirable behaviors in state-of-the-art LLMs occur disproportionately more for users with lower English proficiency, of lower education status, and originating from outside the US, rendering these models unreliable sources of information towards their most vulnerable users.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Legal Fact Prediction: The Missing Piece in Legal Judgment Prediction</title>
<link>https://arxiv.org/abs/2409.07055</link>
<guid>https://arxiv.org/abs/2409.07055</guid>
<content:encoded><![CDATA[
arXiv:2409.07055v3 Announce Type: replace-cross 
Abstract: Legal judgment prediction (LJP), which enables litigants and their lawyers to forecast judgment outcomes and refine litigation strategies, has emerged as a crucial legal NLP task. Existing studies typically utilize legal facts, i.e., facts that have been established by evidence and determined by the judge, to predict the judgment. However, legal facts are often difficult to obtain in the early stages of litigation, significantly limiting the practical applicability of fact-based LJP. To address this limitation, we propose a novel legal NLP task: legal fact prediction (LFP), which takes the evidence submitted by litigants for trial as input to predict legal facts, thereby empowering fact-based LJP technologies to make predictions in the absence of ground-truth legal facts. We also propose the first benchmark dataset, LFPBench, for evaluating the LFP task. Our extensive experiments on LFPBench demonstrate the effectiveness of LFP-empowered LJP and highlight promising research directions for LFP.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Residual Kolmogorov-Arnold Network for Enhanced Deep Learning</title>
<link>https://arxiv.org/abs/2410.05500</link>
<guid>https://arxiv.org/abs/2410.05500</guid>
<content:encoded><![CDATA[
arXiv:2410.05500v4 Announce Type: replace-cross 
Abstract: Despite their immense success, deep convolutional neural networks (CNNs) can be difficult to optimize and costly to train due to hundreds of layers within the network depth. Conventional convolutional operations are fundamentally limited by their linear nature along with fixed activations, where many layers are needed to learn meaningful patterns in data. Because of the sheer size of these networks, this approach is simply computationally inefficient, and poses overfitting or gradient explosion risks, especially in small datasets. As a result, we introduce a "plug-in" module, called Residual Kolmogorov-Arnold Network (RKAN). Our module is highly compact, so it can be easily added into any stage (level) of traditional deep networks, where it learns to integrate supportive polynomial feature transformations to existing convolutional frameworks. RKAN offers consistent improvements over baseline models in different vision tasks and widely tested benchmarks, accomplishing cutting-edge performance on them.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Kolmogorov Barrier: A Learnable Weighted Hybrid Autoencoder for Model Order Reduction</title>
<link>https://arxiv.org/abs/2410.18148</link>
<guid>https://arxiv.org/abs/2410.18148</guid>
<content:encoded><![CDATA[
arXiv:2410.18148v5 Announce Type: replace-cross 
Abstract: Representation learning for high-dimensional, complex physical systems aims to identify a low-dimensional intrinsic latent space, which is crucial for reduced-order modeling and modal analysis. To overcome the well-known Kolmogorov barrier, deep autoencoders (AEs) have been introduced in recent years, but they often suffer from poor convergence behavior as the rank of the latent space increases. To address this issue, we propose the learnable weighted hybrid autoencoder, a hybrid approach that combines the strengths of singular value decomposition (SVD) with deep autoencoders through a learnable weighted framework. We find that the introduction of learnable weighting parameters is essential -- without them, the resulting model would either collapse into a standard POD or fail to exhibit the desired convergence behavior. Interestingly, we empirically find that our trained model has a sharpness thousands of times smaller compared to other models. Our experiments on classical chaotic PDE systems, including the 1D Kuramoto-Sivashinsky and forced isotropic turbulence datasets, demonstrate that our approach significantly improves generalization performance compared to several competing methods. Additionally, when combining with time series modeling techniques (e.g., Koopman operator, LSTM), the proposed technique offers significant improvements for surrogate modeling of high-dimensional multi-scale PDE systems.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Adam Requires Better Rotation Dependent Assumptions</title>
<link>https://arxiv.org/abs/2410.19964</link>
<guid>https://arxiv.org/abs/2410.19964</guid>
<content:encoded><![CDATA[
arXiv:2410.19964v3 Announce Type: replace-cross 
Abstract: Despite its widespread adoption, Adam's advantage over Stochastic Gradient Descent (SGD) lacks a comprehensive theoretical explanation. This paper investigates Adam's sensitivity to rotations of the parameter space. We observe that Adam's performance in training transformers degrades under random rotations of the parameter space, indicating a crucial sensitivity to the choice of basis in practice. This reveals that conventional rotation-invariant assumptions are insufficient to capture Adam's advantages theoretically. To better understand the rotation-dependent properties that benefit Adam, we also identify structured rotations that preserve or even enhance its empirical performance. We then examine the rotation-dependent assumptions in the literature and find that they fall short in explaining Adam's behaviour across various rotation types. In contrast, we verify the orthogonality of the update as a promising indicator of Adam's basis sensitivity, suggesting it may be the key quantity for developing rotation-dependent theoretical frameworks that better explain its empirical success.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transferable &amp; Stealthy Ensemble Attacks: A Black-Box Jailbreaking Framework for Large Language Models</title>
<link>https://arxiv.org/abs/2410.23558</link>
<guid>https://arxiv.org/abs/2410.23558</guid>
<content:encoded><![CDATA[
arXiv:2410.23558v3 Announce Type: replace-cross 
Abstract: We present a novel black-box jailbreaking framework that integrates multiple LLM-as-Attacker strategies to deliver highly transferable and effective attacks. The framework is grounded in three key insights from prior jailbreaking research and practice: ensemble approaches outperform single methods in exposing aligned LLM vulnerabilities, malicious instructions vary in jailbreaking difficulty requiring tailored optimization, and disrupting semantic coherence of malicious prompts can manipulate their embeddings to boost success rates. Validated in the Competition for LLM and Agent Safety 2024, our solution achieved top rankings in the Jailbreaking Attack Track.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GASP: Efficient Black-Box Generation of Adversarial Suffixes for Jailbreaking LLMs</title>
<link>https://arxiv.org/abs/2411.14133</link>
<guid>https://arxiv.org/abs/2411.14133</guid>
<content:encoded><![CDATA[
arXiv:2411.14133v3 Announce Type: replace-cross 
Abstract: LLMs have shown impressive capabilities across various natural language processing tasks, yet remain vulnerable to input prompts, known as jailbreak attacks, carefully designed to bypass safety guardrails and elicit harmful responses. Traditional methods rely on manual heuristics but suffer from limited generalizability. Despite being automatic, optimization-based attacks often produce unnatural prompts that can be easily detected by safety filters or require high computational costs due to discrete token optimization. In this paper, we introduce Generative Adversarial Suffix Prompter (GASP), a novel automated framework that can efficiently generate human-readable jailbreak prompts in a fully black-box setting. In particular, GASP leverages latent Bayesian optimization to craft adversarial suffixes by efficiently exploring continuous latent embedding spaces, gradually optimizing the suffix prompter to improve attack efficacy while balancing prompt coherence via a targeted iterative refinement procedure. Through comprehensive experiments, we show that GASP can produce natural adversarial prompts, significantly improving jailbreak success over baselines, reducing training times, and accelerating inference speed, thus making it an efficient and scalable solution for red-teaming LLMs.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Memory in Optimization Algorithms Implicitly Modifies the Loss</title>
<link>https://arxiv.org/abs/2502.02132</link>
<guid>https://arxiv.org/abs/2502.02132</guid>
<content:encoded><![CDATA[
arXiv:2502.02132v2 Announce Type: replace-cross 
Abstract: In modern optimization methods used in deep learning, each update depends on the history of previous iterations, often referred to as memory, and this dependence decays fast as the iterates go further into the past. For example, gradient descent with momentum has exponentially decaying memory through exponentially averaged past gradients. We introduce a general technique for identifying a memoryless algorithm that approximates an optimization algorithm with memory. It is obtained by replacing all past iterates in the update by the current one, and then adding a correction term arising from memory (also a function of the current iterate). This correction term can be interpreted as a perturbation of the loss, and the nature of this perturbation can inform how memory implicitly (anti-)regularizes the optimization dynamics. As an application of our theory, we find that Lion does not have the kind of implicit anti-regularization induced by memory that AdamW does, providing a theory-based explanation for Lion's better generalization performance recently documented.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KGGen: Extracting Knowledge Graphs from Plain Text with Language Models</title>
<link>https://arxiv.org/abs/2502.09956</link>
<guid>https://arxiv.org/abs/2502.09956</guid>
<content:encoded><![CDATA[
arXiv:2502.09956v2 Announce Type: replace-cross 
Abstract: Recent interest in building foundation models for KGs has highlighted a fundamental challenge: knowledge-graph data is relatively scarce. The best-known KGs are primarily human-labeled, created by pattern-matching, or extracted using early NLP techniques. While human-generated KGs are in short supply, automatically extracted KGs are of questionable quality. We present a solution to this data scarcity problem in the form of a text-to-KG generator (KGGen), a package that uses language models to create high-quality graphs from plaintext. Unlike other KG extractors, KGGen clusters related entities to reduce sparsity in extracted KGs. KGGen is available as a Python library (\texttt{pip install kg-gen}), making it accessible to everyone. Along with KGGen, we release the first benchmark, Measure of of Information in Nodes and Edges (MINE), that tests an extractor's ability to produce a useful KG from plain text. We benchmark our new tool against existing extractors and demonstrate far superior performance.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pragmatic Reasoning improves LLM Code Generation</title>
<link>https://arxiv.org/abs/2502.15835</link>
<guid>https://arxiv.org/abs/2502.15835</guid>
<content:encoded><![CDATA[
arXiv:2502.15835v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated impressive potential in translating natural language (NL) instructions into program code. However, user instructions often contain inherent ambiguities, making it challenging for LLMs to generate code that accurately reflects the user's true intent. To address this challenge, researchers have proposed approaches that produce multiple candidates of the program code and then rerank them to identify the best solution. In this paper, we propose CodeRSA, a novel code candidate reranking mechanism built upon the Rational Speech Act (RSA) framework, designed to guide LLMs toward more comprehensive pragmatic reasoning about user intent. We evaluate CodeRSA using Llama-3-8B-Instruct and Qwen-2.5-7B-Instruct on two widely used code generation benchmarks, HumanEval and MBPP. Our experiment results show that CodeRSA consistently outperforms common baselines, surpasses the state-of-the-art approach in most cases, and demonstrates robust overall performance. These findings underscore the effectiveness of integrating pragmatic reasoning into code candidate reranking, offering a promising direction for enhancing code generation quality in LLMs.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Constrained Learning</title>
<link>https://arxiv.org/abs/2503.03480</link>
<guid>https://arxiv.org/abs/2503.03480</guid>
<content:encoded><![CDATA[
arXiv:2503.03480v3 Announce Type: replace-cross 
Abstract: Vision-language-action models (VLAs) show potential as generalist robot policies. However, these models pose extreme safety challenges during real-world deployment, including the risk of harm to the environment, the robot itself, and humans. How can safety constraints be explicitly integrated into VLAs? We address this by exploring an integrated safety approach (ISA), systematically modeling safety requirements, then actively eliciting diverse unsafe behaviors, effectively constraining VLA policies via safe reinforcement learning, and rigorously assuring their safety through targeted evaluations. Leveraging the constrained Markov decision process (CMDP) paradigm, ISA optimizes VLAs from a min-max perspective against elicited safety risks. Thus, policies aligned through this comprehensive approach achieve the following key features: (I) effective safety-performance trade-offs, reducing the cumulative cost of safety violations by 83.58% compared to the state-of-the-art method, while also maintaining task success rate (+3.85%). (II) strong safety assurance, with the ability to mitigate long-tail risks and handle extreme failure scenarios. (III) robust generalization of learned safety behaviors to various out-of-distribution perturbations. The effectiveness is evaluated on long-horizon mobile manipulation tasks. Our data, models and newly proposed benchmark environment are available at https://pku-safevla.github.io.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Model Development through Fine-tuning Transfer</title>
<link>https://arxiv.org/abs/2503.20110</link>
<guid>https://arxiv.org/abs/2503.20110</guid>
<content:encoded><![CDATA[
arXiv:2503.20110v2 Announce Type: replace-cross 
Abstract: Modern LLMs struggle with efficient updates, as each new pretrained model version requires repeating expensive alignment processes. This challenge also applies to domain- or languagespecific models, where fine-tuning on specialized data must be redone for every new base model release. In this paper, we explore the transfer of fine-tuning updates between model versions. Specifically, we derive the diff vector (representing the weight changes from finetuning) from one source model version and apply it to the base model of a different target version. Through empirical evaluations on various open-weight model versions, we show that transferring diff vectors can significantly improve the performance of the target base model. For example, transferring the fine-tuning updates from Llama 3.0 8B improves Llama 3.1 8B by 46.9% on IFEval and 15.7% on LiveCodeBench without additional training, even surpassing Llama 3.1 8B Instruct. Furthermore, we demonstrate performance gains on multilingual tasks, with 4.7% and 15.5% improvements on Global MMLU for Malagasy and Turkish, respectively. We observe that these merged models provide stronger initializations for further fine-tuning. Lastly, our controlled experiments suggest that fine-tuning transfer is most effective when source and target models lie in a linearly connected region of parameter space, and we provide a theoretical analysis of our method. Taken together, fine-tuning transfer offers a cost-efficient and practical strategy for continuous LLM development. Our code is available at github.com/pjlintw/finetuning-transfer.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models</title>
<link>https://arxiv.org/abs/2503.22879</link>
<guid>https://arxiv.org/abs/2503.22879</guid>
<content:encoded><![CDATA[
arXiv:2503.22879v4 Announce Type: replace-cross 
Abstract: State Space Models (SSMs) are emerging as a compelling alternative to Transformers because of their consistent memory usage and high performance. Despite this, scaling up SSMs on cloud services or limited-resource devices is challenging due to their storage requirements and computational power. To overcome this, quantizing SSMs with low bit-width data formats can reduce model size and benefit from hardware acceleration. As SSMs are prone to quantization-induced errors, recent efforts have focused on optimizing a particular model or bit-width for efficiency without sacrificing performance. However, distinct bit-width configurations are essential for different scenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for enhancing generation speed in short prompt applications for a single user. To this end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both Mamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment on various platforms. Based on the channel order preserving and activation persistence of SSMs, we propose an offline approach to quantize inputs of a linear recurrence in 8-bit by sorting and clustering for input $x$, combined with a per-state-group quantization for input-dependent parameters $B$ and $C$. To ensure compute-invariance in the SSM output, we rearrange weights offline according to the clustering sequence. The experiments show that Quamba2-8B outperforms two state-of-the-art SSM quantization methods and delivers 1.3$\times$ and 3$\times$ speed-ups in the pre-filling and generation stages, respectively, while offering 4$\times$ memory reduction with only a $1.6\%$ average accuracy drop. The evaluation on MMLU shows the generalizability and robustness of our framework. The code and quantized models will be released at: https://github.com/enyac-group/Quamba.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TathyaNyaya and FactLegalLlama: Advancing Factual Judgment Prediction and Explanation in the Indian Legal Context</title>
<link>https://arxiv.org/abs/2504.04737</link>
<guid>https://arxiv.org/abs/2504.04737</guid>
<content:encoded><![CDATA[
arXiv:2504.04737v2 Announce Type: replace-cross 
Abstract: In the landscape of Fact-based Judgment Prediction and Explanation (FJPE), reliance on factual data is essential for developing robust and realistic AI-driven decision-making tools. This paper introduces TathyaNyaya, the largest annotated dataset for FJPE tailored to the Indian legal context, encompassing judgments from the Supreme Court of India and various High Courts. Derived from the Hindi terms "Tathya" (fact) and "Nyaya" (justice), the TathyaNyaya dataset is uniquely designed to focus on factual statements rather than complete legal texts, reflecting real-world judicial processes where factual data drives outcomes. Complementing this dataset, we present FactLegalLlama, an instruction-tuned variant of the LLaMa-3-8B Large Language Model (LLM), optimized for generating high-quality explanations in FJPE tasks. Finetuned on the factual data in TathyaNyaya, FactLegalLlama integrates predictive accuracy with coherent, contextually relevant explanations, addressing the critical need for transparency and interpretability in AI-assisted legal systems. Our methodology combines transformers for binary judgment prediction with FactLegalLlama for explanation generation, creating a robust framework for advancing FJPE in the Indian legal domain. TathyaNyaya not only surpasses existing datasets in scale and diversity but also establishes a benchmark for building explainable AI systems in legal analysis. The findings underscore the importance of factual precision and domain-specific tuning in enhancing predictive performance and interpretability, positioning TathyaNyaya and FactLegalLlama as foundational resources for AI-assisted legal decision-making.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking LLM Faithfulness in RAG with Evolving Leaderboards</title>
<link>https://arxiv.org/abs/2505.04847</link>
<guid>https://arxiv.org/abs/2505.04847</guid>
<content:encoded><![CDATA[
arXiv:2505.04847v2 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) aims to reduce hallucinations by grounding responses in external context, yet large language models (LLMs) still frequently introduce unsupported information or contradictions even when provided with relevant context. This paper presents two complementary efforts at Vectara to measure and benchmark LLM faithfulness in RAG. First, we describe our original hallucination leaderboard, which has tracked hallucination rates for LLMs since 2023 using our HHEM hallucination detection model. Motivated by limitations observed in current hallucination detection methods, we introduce FaithJudge, an LLM-as-a-judge framework that leverages a pool of diverse human-annotated hallucination examples to substantially improve the automated hallucination evaluation of LLMs. We introduce an enhanced hallucination leaderboard centered on FaithJudge that benchmarks LLMs on RAG faithfulness in summarization, question-answering, and data-to-text generation tasks. FaithJudge enables a more reliable benchmarking of LLM hallucinations in RAG and supports the development of more trustworthy generative AI systems: https://github.com/vectara/FaithJudge.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Cancer Modeling in the Age of Foundation Model Embeddings</title>
<link>https://arxiv.org/abs/2505.07683</link>
<guid>https://arxiv.org/abs/2505.07683</guid>
<content:encoded><![CDATA[
arXiv:2505.07683v3 Announce Type: replace-cross 
Abstract: The Cancer Genome Atlas (TCGA) has enabled novel discoveries and served as a large-scale reference dataset in cancer through its harmonized genomics, clinical, and imaging data. Numerous prior studies have developed bespoke deep learning models over TCGA for tasks such as cancer survival prediction. A modern paradigm in biomedical deep learning is the development of foundation models (FMs) to derive feature embeddings agnostic to a specific modeling task. Biomedical text especially has seen growing development of FMs. While TCGA contains free-text data as pathology reports, these have been historically underutilized. Here, we investigate the ability to train classical machine learning models over multimodal, zero-shot FM embeddings of cancer data. We demonstrate the ease and additive effect of multimodal fusion, outperforming unimodal models. Further, we show the benefit of including pathology report text and rigorously evaluate the effect of model-based text summarization and hallucination. Overall, we propose an embedding-centric approach to multimodal cancer modeling.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting Residual Connections: Orthogonal Updates for Stable and Efficient Deep Networks</title>
<link>https://arxiv.org/abs/2505.11881</link>
<guid>https://arxiv.org/abs/2505.11881</guid>
<content:encoded><![CDATA[
arXiv:2505.11881v3 Announce Type: replace-cross 
Abstract: Residual connections are pivotal for deep neural networks, enabling greater depth by mitigating vanishing gradients. However, in standard residual updates, the module's output is directly added to the input stream. This can lead to updates that predominantly reinforce or modulate the existing stream direction, potentially underutilizing the module's capacity for learning entirely novel features. In this work, we introduce Orthogonal Residual Update: we decompose the module's output relative to the input stream and add only the component orthogonal to this stream. This design aims to guide modules to contribute primarily new representational directions, fostering richer feature learning while promoting more efficient training. We demonstrate that our orthogonal update strategy improves generalization accuracy and training stability across diverse architectures (ResNetV2, Vision Transformers) and datasets (CIFARs, TinyImageNet, ImageNet-1k), achieving, for instance, a +3.78 pp top-1 accuracy gain for ViT-B on ImageNet-1k.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Dynamics of RNNs in Closed-Loop Environments</title>
<link>https://arxiv.org/abs/2505.13567</link>
<guid>https://arxiv.org/abs/2505.13567</guid>
<content:encoded><![CDATA[
arXiv:2505.13567v2 Announce Type: replace-cross 
Abstract: Recurrent neural networks (RNNs) trained on neuroscience-inspired tasks offer powerful models of brain computation. However, typical training paradigms rely on open-loop, supervised settings, whereas real-world learning unfolds in closed-loop environments. Here, we develop a mathematical theory describing the learning dynamics of linear RNNs trained in closed-loop contexts. We first demonstrate that two otherwise identical RNNs, trained in either closed- or open-loop modes, follow markedly different learning trajectories. To probe this divergence, we analytically characterize the closed-loop case, revealing distinct stages aligned with the evolution of the training loss. Specifically, we show that the learning dynamics of closed-loop RNNs, in contrast to open-loop ones, are governed by an interplay between two competing objectives: short-term policy improvement and long-term stability of the agent-environment interaction. Finally, we apply our framework to a realistic motor control task, highlighting its broader applicability. Taken together, our results underscore the importance of modeling closed-loop dynamics in a biologically plausible setting.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>But what is your honest answer? Aiding LLM-judges with honest alternatives using steering vectors</title>
<link>https://arxiv.org/abs/2505.17760</link>
<guid>https://arxiv.org/abs/2505.17760</guid>
<content:encoded><![CDATA[
arXiv:2505.17760v2 Announce Type: replace-cross 
Abstract: Detecting subtle forms of dishonesty like sycophancy and manipulation in Large Language Models (LLMs) remains challenging for both humans and automated evaluators, as these behaviors often appear through small biases rather than clear false statements. We introduce Judge Using Safety-Steered Alternatives (JUSSA), a novel framework that employs steering vectors not to improve model behavior directly, but to enhance LLM judges' evaluation capabilities. JUSSA applies steering vectors during inference to generate more honest alternatives, providing judges with contrastive examples that make subtle dishonest patterns easier to detect. While existing evaluation methods rely on black-box evaluation, JUSSA leverages model internals to create targeted comparisons from single examples. We evaluate our method on sycophancy detection and introduce a new manipulation dataset covering multiple types of manipulation. Our results demonstrate that JUSSA effectively improves detection accuracy over single-response evaluation in various cases. Analysis across judge models reveals that JUSSA helps weaker judges on easier dishonesty detection tasks, and stronger judges on harder tasks. Layer-wise experiments show how dishonest prompts cause representations to diverge from honest ones in middle layers, revealing where steering interventions are most effective for generating contrastive examples. By demonstrating that steering vectors can enhance safety evaluation rather than just modify behavior, our work opens new directions for scalable model auditing as systems become increasingly sophisticated.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robustness in Large Language Models: A Survey of Mitigation Strategies and Evaluation Metrics</title>
<link>https://arxiv.org/abs/2505.18658</link>
<guid>https://arxiv.org/abs/2505.18658</guid>
<content:encoded><![CDATA[
arXiv:2505.18658v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have emerged as a promising cornerstone for the development of natural language processing (NLP) and artificial intelligence (AI). However, ensuring the robustness of LLMs remains a critical challenge. To address these challenges and advance the field, this survey provides a comprehensive overview of current studies in this area. First, we systematically examine the nature of robustness in LLMs, including its conceptual foundations, the importance of consistent performance across diverse inputs, and the implications of failure modes in real-world applications. Next, we analyze the sources of non-robustness, categorizing intrinsic model limitations, data-driven vulnerabilities, and external adversarial factors that compromise reliability. Following this, we review state-of-the-art mitigation strategies, and then we discuss widely adopted benchmarks, emerging metrics, and persistent gaps in assessing real-world reliability. Finally, we synthesize findings from existing surveys and interdisciplinary studies to highlight trends, unresolved issues, and pathways for future research.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Two Causally Related Needles in a Video Haystack</title>
<link>https://arxiv.org/abs/2505.19853</link>
<guid>https://arxiv.org/abs/2505.19853</guid>
<content:encoded><![CDATA[
arXiv:2505.19853v3 Announce Type: replace-cross 
Abstract: Properly evaluating the ability of Video-Language Models (VLMs) to understand long videos remains a challenge. We propose a long-context video understanding benchmark, Causal2Needles, that assesses two crucial abilities insufficiently addressed by existing benchmarks: (1) extracting information from two separate locations (two needles) in a long video and understanding them jointly, and (2) modeling the world in terms of cause and effect in human behaviors. Causal2Needles evaluates these abilities using noncausal one-needle, causal one-needle, and causal two-needle questions. The most complex question type, causal two-needle questions, require extracting information from both the cause and effect events from a long video and the associated narration text. To prevent textual bias, we introduce two complementary question formats: locating the video clip containing the answer, and verbal description of a visual detail from that video clip. Our experiments reveal that models excelling on existing benchmarks struggle with causal 2-needle questions, and the model performance is negatively correlated with the distance between the two needles. These findings highlight critical limitations in current VLMs. The dataset is available at: https://huggingface.co/datasets/causal2needles/Causal2Needles
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Retrieval with Evidence Curation for Open-Domain Financial Question Answering on Standardized Documents</title>
<link>https://arxiv.org/abs/2505.20368</link>
<guid>https://arxiv.org/abs/2505.20368</guid>
<content:encoded><![CDATA[
arXiv:2505.20368v3 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) based large language models (LLMs) are widely used in finance for their excellent performance on knowledge-intensive tasks. However, standardized documents (e.g., SEC filing) share similar formats such as repetitive boilerplate texts, and similar table structures. This similarity forces traditional RAG methods to misidentify near-duplicate text, leading to duplicate retrieval that undermines accuracy and completeness. To address these issues, we propose the Hierarchical Retrieval with Evidence Curation (HiREC) framework. Our approach first performs hierarchical retrieval to reduce confusion among similar texts. It first retrieve related documents and then selects the most relevant passages from the documents. The evidence curation process removes irrelevant passages. When necessary, it automatically generates complementary queries to collect missing information. To evaluate our approach, we construct and release a Large-scale Open-domain Financial (LOFin) question answering benchmark that includes 145,897 SEC documents and 1,595 question-answer pairs. Our code and data are available at https://github.com/deep-over/LOFin-bench-HiREC.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Composite Flow Matching for Reinforcement Learning with Shifted-Dynamics Data</title>
<link>https://arxiv.org/abs/2505.23062</link>
<guid>https://arxiv.org/abs/2505.23062</guid>
<content:encoded><![CDATA[
arXiv:2505.23062v3 Announce Type: replace-cross 
Abstract: Incorporating pre-collected offline data from a source environment can significantly improve the sample efficiency of reinforcement learning (RL), but this benefit is often challenged by discrepancies between the transition dynamics of the source and target environments. Existing methods typically address this issue by penalizing or filtering out source transitions in high dynamics-gap regions. However, their estimation of the dynamics gap often relies on KL divergence or mutual information, which can be ill-defined when the source and target dynamics have disjoint support. To overcome these limitations, we propose CompFlow, a method grounded in the theoretical connection between flow matching and optimal transport. Specifically, we model the target dynamics as a conditional flow built upon the output distribution of the source-domain flow, rather than learning it directly from a Gaussian prior. This composite structure offers two key advantages: (1) improved generalization for learning target dynamics, and (2) a principled estimation of the dynamics gap via the Wasserstein distance between source and target transitions. Leveraging our principled estimation of the dynamics gap, we further introduce an optimistic active data collection strategy that prioritizes exploration in regions of high dynamics gap, and theoretically prove that it reduces the performance disparity with the optimal policy. Empirically, CompFlow outperforms strong baselines across several RL benchmarks with shifted dynamics.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning Models Hallucinate More: Factuality-Aware Reinforcement Learning for Large Reasoning Models</title>
<link>https://arxiv.org/abs/2505.24630</link>
<guid>https://arxiv.org/abs/2505.24630</guid>
<content:encoded><![CDATA[
arXiv:2505.24630v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have significantly advanced in reasoning tasks through reinforcement learning (RL) optimization, achieving impressive capabilities across various challenging benchmarks. However, our empirical analysis reveals a critical drawback: reasoning-oriented RL fine-tuning significantly increases the prevalence of hallucinations. We theoretically analyze the RL training dynamics, identifying high-variance gradient, entropy-induced randomness, and susceptibility to spurious local optima as key factors leading to hallucinations. To address this drawback, we propose Factuality-aware Step-wise Policy Optimization (FSPO), an innovative RL fine-tuning algorithm incorporating explicit factuality verification at each reasoning step. FSPO leverages automated verification against given evidence to dynamically adjust token-level advantage values, incentivizing factual correctness throughout the reasoning process. Experiments across mathematical reasoning and hallucination benchmarks using Qwen2.5 and Llama models demonstrate that FSPO effectively reduces hallucinations while enhancing reasoning accuracy, substantially improving both reliability and performance.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts</title>
<link>https://arxiv.org/abs/2505.24722</link>
<guid>https://arxiv.org/abs/2505.24722</guid>
<content:encoded><![CDATA[
arXiv:2505.24722v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have shown great success in text modeling tasks across domains. However, natural language exhibits inherent semantic hierarchies and nuanced geometric structure, which current LLMs do not capture completely owing to their reliance on Euclidean operations. Recent studies have also shown that not respecting the geometry of token embeddings leads to training instabilities and degradation of generative capabilities. These findings suggest that shifting to non-Euclidean geometries can better align language models with the underlying geometry of text. We thus propose to operate fully in Hyperbolic space, known for its expansive, scale-free, and low-distortion properties. We thus introduce HELM, a family of HypErbolic Large Language Models, offering a geometric rethinking of the Transformer-based LLM that addresses the representational inflexibility, missing set of necessary operations, and poor scalability of existing hyperbolic LMs. We additionally introduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert operates in a distinct curvature space to encode more fine-grained geometric structure from text, as well as a dense model, HELM-D. For HELM-MICE, we further develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient, reduced-KV-cache training and inference. For both models, we develop essential hyperbolic equivalents of rotary positional encodings and RMS normalization. We are the first to train fully hyperbolic LLMs at billion-parameter scale, and evaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM problem-solving, general knowledge, and commonsense reasoning. Our results show consistent gains from our HELM architectures -- up to 4% -- over popular Euclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy and enhanced reasoning afforded by hyperbolic geometry in large-scale LM pretraining.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explicit Density Approximation for Neural Implicit Samplers Using a Bernstein-Based Convex Divergence</title>
<link>https://arxiv.org/abs/2506.04700</link>
<guid>https://arxiv.org/abs/2506.04700</guid>
<content:encoded><![CDATA[
arXiv:2506.04700v2 Announce Type: replace-cross 
Abstract: Rank-based statistical metrics, such as the invariant statistical loss (ISL), have recently emerged as robust and practically effective tools for training implicit generative models. In this work, we introduce dual-ISL, a novel likelihood-free objective for training implicit generative models that interchanges the roles of the target and model distributions in the ISL framework, yielding a convex optimization problem in the space of model densities. We prove that the resulting rank-based discrepancy $d_K$ is i) continuous under weak convergence and with respect to the $L^1$ norm, and ii) convex in its first argument-properties not shared by classical divergences such as KL or Wasserstein distances. Building on this, we develop a theoretical framework that interprets $d_K$ as an $L^2$-projection of the density ratio $q = p/\tilde p$ onto a Bernstein polynomial basis, from which we derive exact bounds on the truncation error, precise convergence rates, and a closed-form expression for the truncated density approximation. We further extend our analysis to the multivariate setting via random one-dimensional projections, defining a sliced dual-ISL divergence that retains both convexity and continuity. We empirically show that these theoretical advantages translate into practical ones. Specifically, across several benchmarks dual-ISL converges more rapidly, delivers markedly smoother and more stable training, and more effectively prevents mode collapse than classical ISL and other leading implicit generative methods-while also providing an explicit density approximation.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HoliSafe: Holistic Safety Benchmarking and Modeling for Vision-Language Model</title>
<link>https://arxiv.org/abs/2506.04704</link>
<guid>https://arxiv.org/abs/2506.04704</guid>
<content:encoded><![CDATA[
arXiv:2506.04704v4 Announce Type: replace-cross 
Abstract: Despite emerging efforts to enhance the safety of Vision-Language Models (VLMs), current approaches face two main shortcomings. 1) Existing safety-tuning datasets and benchmarks only partially consider how image-text interactions can yield harmful content, often overlooking contextually unsafe outcomes from seemingly benign pairs. This narrow coverage leaves VLMs vulnerable to jailbreak attacks in unseen configurations. 2) Prior methods rely primarily on data-centric tuning, with limited architectural innovations to intrinsically strengthen safety. We address these gaps by introducing a holistic safety dataset and benchmark, \textbf{HoliSafe}, that spans all five safe/unsafe image-text combinations, providing a more robust basis for both training and evaluation (HoliSafe-Bench). We further propose a novel modular framework for enhancing VLM safety with a visual guard module (VGM) designed to assess the harmfulness of input images for VLMs. This module endows VLMs with a dual functionality: they not only learn to generate safer responses but can also provide an interpretable harmfulness classification to justify their refusal decisions. A significant advantage of this approach is its modularity; the VGM is designed as a plug-in component, allowing for seamless integration with diverse pre-trained VLMs across various scales. Experiments show that Safe-VLM with VGM, trained on our HoliSafe, achieves state-of-the-art safety performance across multiple VLM benchmarks. Additionally, the HoliSafe-Bench itself reveals critical vulnerabilities in existing VLM models. We hope that HoliSafe and VGM will spur further research into robust and interpretable VLM safety, expanding future avenues for multimodal alignment.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advanced Sign Language Video Generation with Compressed and Quantized Multi-Condition Tokenization</title>
<link>https://arxiv.org/abs/2506.15980</link>
<guid>https://arxiv.org/abs/2506.15980</guid>
<content:encoded><![CDATA[
arXiv:2506.15980v2 Announce Type: replace-cross 
Abstract: Sign Language Video Generation (SLVG) seeks to generate identity-preserving sign language videos from spoken language texts. Existing methods primarily rely on the single coarse condition (\eg, skeleton sequences) as the intermediary to bridge the translation model and the video generation model, which limits both the naturalness and expressiveness of the generated videos. To overcome these limitations, we propose SignViP, a novel SLVG framework that incorporates multiple fine-grained conditions for improved generation fidelity. Rather than directly translating error-prone high-dimensional conditions, SignViP adopts a discrete tokenization paradigm to integrate and represent fine-grained conditions (\ie, fine-grained poses and 3D hands). SignViP contains three core components. (1) Sign Video Diffusion Model is jointly trained with a multi-condition encoder to learn continuous embeddings that encapsulate fine-grained motion and appearance. (2) Finite Scalar Quantization (FSQ) Autoencoder is further trained to compress and quantize these embeddings into discrete tokens for compact representation of the conditions. (3) Multi-Condition Token Translator is trained to translate spoken language text to discrete multi-condition tokens. During inference, Multi-Condition Token Translator first translates the spoken language text into discrete multi-condition tokens. These tokens are then decoded to continuous embeddings by FSQ Autoencoder, which are subsequently injected into Sign Video Diffusion Model to guide video generation. Experimental results show that SignViP achieves state-of-the-art performance across metrics, including video quality, temporal coherence, and semantic fidelity. The code is available at https://github.com/umnooob/signvip/.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Efficient and Accurate Spiking Neural Networks via Adaptive Bit Allocation</title>
<link>https://arxiv.org/abs/2506.23717</link>
<guid>https://arxiv.org/abs/2506.23717</guid>
<content:encoded><![CDATA[
arXiv:2506.23717v3 Announce Type: replace-cross 
Abstract: Multi-bit spiking neural networks (SNNs) have recently become a heated research spot, pursuing energy-efficient and high-accurate AI. However, with more bits involved, the associated memory and computation demands escalate to the point where the performance improvements become disproportionate. Based on the insight that different layers demonstrate different importance and extra bits could be wasted and interfering, this paper presents an adaptive bit allocation strategy for direct-trained SNNs, achieving fine-grained layer-wise allocation of memory and computation resources. Thus, SNN's efficiency and accuracy can be improved. Specifically, we parametrize the temporal lengths and the bit widths of weights and spikes, and make them learnable and controllable through gradients. To address the challenges caused by changeable bit widths and temporal lengths, we propose the refined spiking neuron, which can handle different temporal lengths, enable the derivation of gradients for temporal lengths, and suit spike quantization better. In addition, we theoretically formulate the step-size mismatch problem of learnable bit widths, which may incur severe quantization errors to SNN, and accordingly propose the step-size renewal mechanism to alleviate this issue. Experiments on various datasets, including the static CIFAR and ImageNet datasets and the dynamic CIFAR-DVS, DVS-GESTURE, and SHD datasets, demonstrate that our methods can reduce the overall memory and computation cost while achieving higher accuracy. Particularly, our SEWResNet-34 can achieve a 2.69% accuracy gain and 4.16x lower bit budgets over the advanced baseline work on ImageNet. This work will be open-sourced.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Graph Learning for Industrial Carbon Emission Analysis and Policy Impact</title>
<link>https://arxiv.org/abs/2507.02912</link>
<guid>https://arxiv.org/abs/2507.02912</guid>
<content:encoded><![CDATA[
arXiv:2507.02912v3 Announce Type: replace-cross 
Abstract: Industrial carbon emissions are a major driver of climate change, yet modeling these emissions is challenging due to multicollinearity among factors and complex interdependencies across sectors and time. We propose a novel graph-based deep learning framework DGL to analyze and forecast industrial CO_2 emissions, addressing high feature correlation and capturing industrial-temporal interdependencies. Unlike traditional regression or clustering methods, our approach leverages a Graph Neural Network (GNN) with attention mechanisms to model relationships between industries (or regions) and a temporal transformer to learn long-range patterns. We evaluate our framework on public global industry emissions dataset derived from EDGAR v8.0, spanning multiple countries and sectors. The proposed model achieves superior predictive performance - reducing error by over 15% compared to baseline deep models - while maintaining interpretability via attention weights and causal analysis. We believe that we are the first Graph-Temporal architecture that resolves multicollinearity by structurally encoding feature relationships, along with integration of causal inference to identify true drivers of emissions, improving transparency and fairness. We also stand a demonstration of policy relevance, showing how model insights can guide sector-specific decarbonization strategies aligned with sustainable development goals. Based on the above, we show high-emission "hotspots" and suggest equitable intervention plans, illustrating the potential of state-of-the-art AI graph learning to advance climate action, offering a powerful tool for policymakers and industry stakeholders to achieve carbon reduction targets.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MeAJOR Corpus: A Multi-Source Dataset for Phishing Email Detection</title>
<link>https://arxiv.org/abs/2507.17978</link>
<guid>https://arxiv.org/abs/2507.17978</guid>
<content:encoded><![CDATA[
arXiv:2507.17978v2 Announce Type: replace-cross 
Abstract: Phishing emails continue to pose a significant threat to cybersecurity by exploiting human vulnerabilities through deceptive content and malicious payloads. While Machine Learning (ML) models are effective at detecting phishing threats, their performance largely relies on the quality and diversity of the training data. This paper presents MeAJOR (Merged email Assets from Joint Open-source Repositories) Corpus, a novel, multi-source phishing email dataset designed to overcome critical limitations in existing resources. It integrates 135894 samples representing a broad number of phishing tactics and legitimate emails, with a wide spectrum of engineered features. We evaluated the dataset's utility for phishing detection research through systematic experiments with four classification models (RF, XGB, MLP, and CNN) across multiple feature configurations. Results highlight the dataset's effectiveness, achieving 98.34% F1 with XGB. By integrating broad features from multiple categories, our dataset provides a reusable and consistent resource, while addressing common challenges like class imbalance, generalisability and reproducibility.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GENIAL: Generative Design Space Exploration via Network Inversion for Low Power Algorithmic Logic Units</title>
<link>https://arxiv.org/abs/2507.18989</link>
<guid>https://arxiv.org/abs/2507.18989</guid>
<content:encoded><![CDATA[
arXiv:2507.18989v2 Announce Type: replace-cross 
Abstract: As AI workloads proliferate, optimizing arithmetic units is becoming increasingly important for reducing the footprint of digital systems. Conventional design flows, which often rely on manual or heuristic-based optimization, are limited in their ability to thoroughly explore the vast design space. In this paper, we introduce GENIAL, a machine learning-based framework for the automatic generation and optimization of arithmetic units, with a focus on multipliers.
  At the core of GENIAL is a Transformer-based surrogate model trained in two stages, involving self-supervised pretraining followed by supervised finetuning, to robustly forecast key hardware metrics such as power and area from abstracted design representations. By inverting the surrogate model, GENIAL efficiently searches for new operand encodings that directly minimize power consumption in arithmetic units for specific input data distributions. Extensive experiments on large datasets demonstrate that GENIAL is consistently more sample efficient than other methods, and converges faster towards optimized designs. This enables deployment of a high-effort logic synthesis optimization flow in the loop, improving the accuracy of the surrogate model. Notably, GENIAL automatically discovers encodings that achieve up to 18% switching activity savings within multipliers on representative AI workloads compared with the conventional two's complement. We also demonstrate the versatility of our approach by achieving significant improvements on Finite State Machines, highlighting GENIAL's applicability for a wide spectrum of logic functions. Together, these advances mark a significant step toward automated Quality-of-Results-optimized combinational circuit generation for digital systems.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vibe Coding as a Reconfiguration of Intent Mediation in Software Development: Definition, Implications, and Research Agenda</title>
<link>https://arxiv.org/abs/2507.21928</link>
<guid>https://arxiv.org/abs/2507.21928</guid>
<content:encoded><![CDATA[
arXiv:2507.21928v2 Announce Type: replace-cross 
Abstract: Software development is undergoing a fundamental transformation as vibe coding becomes widespread, with large portions of contemporary codebases now being AI-generated. The disconnect between rapid adoption and limited conceptual understanding highlights the need for an inquiry into this emerging paradigm. Drawing on an intent perspective and historical analysis, we define vibe coding as a software development paradigm where humans and generative AI engage in collaborative flow to co-create software artifacts through natural language dialogue, shifting the mediation of developer intent from deterministic instruction to probabilistic inference. By intent mediation, we refer to the fundamental process through which developers translate their conceptual goals into representations that computational systems can execute. Our results show that vibe coding reconfigures cognitive work by redistributing epistemic labor between humans and machines, shifting the expertise in the software development process away from traditional areas such as design or technical implementation toward collaborative orchestration. We identify key opportunities, including democratization, acceleration, and systemic leverage, alongside risks, such as black box codebases, responsibility gaps, and ecosystem bias. We conclude with a research agenda spanning human-, technology-, and organization-centered directions to guide future investigations of this paradigm.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation</title>
<link>https://arxiv.org/abs/2508.00097</link>
<guid>https://arxiv.org/abs/2508.00097</guid>
<content:encoded><![CDATA[
arXiv:2508.00097v2 Announce Type: replace-cross 
Abstract: The rapid advancement of Vision-Language-Action models has created an urgent need for large-scale, high-quality robot demonstration datasets. Although teleoperation is the predominant method for data collection, current approaches suffer from limited scalability, complex setup procedures, and suboptimal data quality. This paper presents XRoboToolkit, a cross-platform framework for extended reality based robot teleoperation built on the OpenXR standard. The system features low-latency stereoscopic visual feedback, optimization-based inverse kinematics, and support for diverse tracking modalities including head, controller, hand, and auxiliary motion trackers. XRoboToolkit's modular architecture enables seamless integration across robotic platforms and simulation environments, spanning precision manipulators, mobile robots, and dexterous hands. We demonstrate the framework's effectiveness through precision manipulation tasks and validate data quality by training VLA models that exhibit robust autonomous performance.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System</title>
<link>https://arxiv.org/abs/2508.00709</link>
<guid>https://arxiv.org/abs/2508.00709</guid>
<content:encoded><![CDATA[
arXiv:2508.00709v2 Announce Type: replace-cross 
Abstract: Legal Judgment Prediction (LJP) has emerged as a key area in AI for law, aiming to automate judicial outcome forecasting and enhance interpretability in legal reasoning. While previous approaches in the Indian context have relied on internal case content such as facts, issues, and reasoning, they often overlook a core element of common law systems, which is reliance on statutory provisions and judicial precedents. In this work, we propose NyayaRAG, a Retrieval-Augmented Generation (RAG) framework that simulates realistic courtroom scenarios by providing models with factual case descriptions, relevant legal statutes, and semantically retrieved prior cases. NyayaRAG evaluates the effectiveness of these combined inputs in predicting court decisions and generating legal explanations using a domain-specific pipeline tailored to the Indian legal system. We assess performance across various input configurations using both standard lexical and semantic metrics as well as LLM-based evaluators such as G-Eval. Our results show that augmenting factual inputs with structured legal knowledge significantly improves both predictive accuracy and explanation quality.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Med-GLIP: Advancing Medical Language-Image Pre-training with Large-scale Grounded Dataset</title>
<link>https://arxiv.org/abs/2508.10528</link>
<guid>https://arxiv.org/abs/2508.10528</guid>
<content:encoded><![CDATA[
arXiv:2508.10528v2 Announce Type: replace-cross 
Abstract: Medical image grounding aims to align natural language phrases with specific regions in medical images, serving as a foundational task for intelligent diagnosis, visual question answering (VQA), and automated report generation (MRG). However, existing research is constrained by limited modality coverage, coarse-grained annotations, and the absence of a unified, generalizable grounding framework. To address these challenges, we construct a large-scale medical grounding dataset Med-GLIP-5M comprising over 5.3 million region-level annotations across seven imaging modalities, covering diverse anatomical structures and pathological findings. The dataset supports both segmentation and grounding tasks with hierarchical region labels, ranging from organ-level boundaries to fine-grained lesions. Based on this foundation, we propose Med-GLIP, a modality-aware grounding framework trained on Med-GLIP-5M. Rather than relying on explicitly designed expert modules, Med-GLIP implicitly acquires hierarchical semantic understanding from diverse training data -- enabling it to recognize multi-granularity structures, such as distinguishing lungs from pneumonia lesions. Extensive experiments demonstrate that Med-GLIP consistently outperforms state-of-the-art baselines across multiple grounding benchmarks. Furthermore, integrating its spatial outputs into downstream tasks, including medical VQA and report generation, leads to substantial performance gains. Our dataset will be released soon.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-target Bayesian Transformer Framework for Predicting Cardiovascular Disease Biomarkers during Pandemics</title>
<link>https://arxiv.org/abs/2509.01794</link>
<guid>https://arxiv.org/abs/2509.01794</guid>
<content:encoded><![CDATA[
arXiv:2509.01794v2 Announce Type: replace-cross 
Abstract: The COVID-19 pandemic disrupted healthcare systems worldwide, disproportionately impacting individuals with chronic conditions such as cardiovascular disease (CVD). These disruptions -- through delayed care and behavioral changes, affected key CVD biomarkers, including LDL cholesterol (LDL-C), HbA1c, BMI, and systolic blood pressure (SysBP). Accurate modeling of these changes is crucial for predicting disease progression and guiding preventive care. However, prior work has not addressed multi-target prediction of CVD biomarker from Electronic Health Records (EHRs) using machine learning (ML), while jointly capturing biomarker interdependencies, temporal patterns, and predictive uncertainty. In this paper, we propose MBT-CB, a Multi-target Bayesian Transformer (MBT) with pre-trained BERT-based transformer framework to jointly predict LDL-C, HbA1c, BMI and SysBP CVD biomarkers from EHR data. The model leverages Bayesian Variational Inference to estimate uncertainties, embeddings to capture temporal relationships and a DeepMTR model to capture biomarker inter-relationships. We evaluate MBT-CT on retrospective EHR data from 3,390 CVD patient records (304 unique patients) in Central Massachusetts during the Covid-19 pandemic. MBT-CB outperformed a comprehensive set of baselines including other BERT-based ML models, achieving an MAE of 0.00887, RMSE of 0.0135 and MSE of 0.00027, while effectively capturing data and model uncertainty, patient biomarker inter-relationships, and temporal dynamics via its attention and embedding mechanisms. MBT-CB's superior performance highlights its potential to improve CVD biomarker prediction and support clinical decision-making during pandemics.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Balancing Quality and Variation: Spam Filtering Distorts Data Label Distributions</title>
<link>https://arxiv.org/abs/2509.08217</link>
<guid>https://arxiv.org/abs/2509.08217</guid>
<content:encoded><![CDATA[
arXiv:2509.08217v2 Announce Type: replace-cross 
Abstract: For machine learning datasets to accurately represent diverse opinions in a population, they must preserve variation in data labels while filtering out spam or low-quality responses. How can we balance annotator reliability and representation? We empirically evaluate how a range of heuristics for annotator filtering affect the preservation of variation on subjective tasks. We find that these methods, designed for contexts in which variation from a single ground-truth label is considered noise, often remove annotators who disagree instead of spam annotators, introducing suboptimal tradeoffs between accuracy and label diversity. We find that conservative settings for annotator removal (<5%) are best, after which all tested methods increase the mean absolute error from the true average label. We analyze performance on synthetic spam to observe that these methods often assume spam annotators are more random than real spammers tend to be: most spammers are distributionally indistinguishable from real annotators, and the minority that are distinguishable tend to give relatively fixed answers, not random ones. Thus, tasks requiring the preservation of variation reverse the intuition of existing spam filtering methods: spammers tend to be less random than non-spammers, so metrics that assume variation is spam fare worse. These results highlight the need for spam removal methods that account for label diversity.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memorization in Large Language Models in Medicine: Prevalence, Characteristics, and Implications</title>
<link>https://arxiv.org/abs/2509.08604</link>
<guid>https://arxiv.org/abs/2509.08604</guid>
<content:encoded><![CDATA[
arXiv:2509.08604v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated significant potential in medicine. To date, LLMs have been widely applied to tasks such as diagnostic assistance, medical question answering, and clinical information synthesis. However, a key open question remains: to what extent do LLMs memorize medical training data. In this study, we present the first comprehensive evaluation of memorization of LLMs in medicine, assessing its prevalence (how frequently it occurs), characteristics (what is memorized), volume (how much content is memorized), and potential downstream impacts (how memorization may affect medical applications). We systematically analyze common adaptation scenarios: (1) continued pretraining on medical corpora, (2) fine-tuning on standard medical benchmarks, and (3) fine-tuning on real-world clinical data, including over 13,000 unique inpatient records from Yale New Haven Health System. The results demonstrate that memorization is prevalent across all adaptation scenarios and significantly higher than reported in the general domain. Memorization affects both the development and adoption of LLMs in medicine and can be categorized into three types: beneficial (e.g., accurate recall of clinical guidelines and biomedical references), uninformative (e.g., repeated disclaimers or templated medical document language), and harmful (e.g., regeneration of dataset-specific or sensitive clinical content). Based on these findings, we offer practical recommendations to facilitate beneficial memorization that enhances domain-specific reasoning and factual accuracy, minimize uninformative memorization to promote deeper learning beyond surface-level patterns, and mitigate harmful memorization to prevent the leakage of sensitive or identifiable patient information.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-Shot Referring Expression Comprehension via Vison-Language True/False Verification</title>
<link>https://arxiv.org/abs/2509.09958</link>
<guid>https://arxiv.org/abs/2509.09958</guid>
<content:encoded><![CDATA[
arXiv:2509.09958v2 Announce Type: replace-cross 
Abstract: Referring Expression Comprehension (REC) is usually addressed with task-trained grounding models. We show that a zero-shot workflow, without any REC-specific training, can achieve competitive or superior performance. Our approach reformulates REC as box-wise visual-language verification: given proposals from a COCO-clean generic detector (YOLO-World), a general-purpose VLM independently answers True/False queries for each region. This simple procedure reduces cross-box interference, supports abstention and multiple matches, and requires no fine-tuning. On RefCOCO, RefCOCO+, and RefCOCOg, our method not only surpasses a zero-shot GroundingDINO baseline but also exceeds reported results for GroundingDINO trained on REC and GroundingDINO+CRG. Controlled studies with identical proposals confirm that verification significantly outperforms selection-based prompting, and results hold with open VLMs. Overall, we show that workflow design, rather than task-specific pretraining, drives strong zero-shot REC performance.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test-Time Warmup for Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2509.10641</link>
<guid>https://arxiv.org/abs/2509.10641</guid>
<content:encoded><![CDATA[
arXiv:2509.10641v2 Announce Type: replace-cross 
Abstract: Multimodal Large Language Models (MLLMs) hold great promise for advanced reasoning at the intersection of text and images, yet they have not fully realized this potential. MLLMs typically integrate an LLM, a vision encoder, and a connector that maps the vision encoder's embeddings into the LLM's text embedding space. Although each component is pretrained on massive datasets with billions of samples, the entire multimodal model is typically trained on only thousands (or a few million) samples, which can result in weak performance on complex reasoning tasks. To address these shortcomings, instead of relying on extensive labeled datasets for fine-tuning, we propose a Test-Time Warmup method that adapts the MLLM per test instance by leveraging data from weakly supervised auxiliary tasks. With our approach, we observe a relative performance improvement of 4.03% on MMMU, 5.28% on VQA-Rad, and 1.63% on GQA on the Llama-Vision-Instruct model. Our method demonstrates that 'warming up' before inference can enhance MLLMs' robustness across diverse reasoning tasks.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Back to Ear: Perceptually Driven High Fidelity Music Reconstruction</title>
<link>https://arxiv.org/abs/2509.14912</link>
<guid>https://arxiv.org/abs/2509.14912</guid>
<content:encoded><![CDATA[
arXiv:2509.14912v2 Announce Type: replace-cross 
Abstract: Variational Autoencoders (VAEs) are essential for large-scale audio tasks like diffusion-based generation. However, existing open-source models often neglect auditory perceptual aspects during training, leading to weaknesses in phase accuracy and stereophonic spatial representation. To address these challenges, we propose {\epsilon}ar-VAE, an open-source music signal reconstruction model that rethinks and optimizes the VAE training paradigm. Our contributions are threefold: (i) A K-weighting perceptual filter applied prior to loss calculation to align the objective with auditory perception. (ii) Two novel phase losses: a Correlation Loss for stereo coherence, and a Phase Loss using its derivatives--Instantaneous Frequency and Group Delay--for precision. (iii) A new spectral supervision paradigm where magnitude is supervised by all four Mid/Side/Left/Right components, while phase is supervised only by the LR components. Experiments show {\epsilon}ar-VAE at 44.1kHz substantially outperforms leading open-source models across diverse metrics, showing particular strength in reconstructing high-frequency harmonics and the spatial characteristics.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparing Computational Pathology Foundation Models using Representational Similarity Analysis</title>
<link>https://arxiv.org/abs/2509.15482</link>
<guid>https://arxiv.org/abs/2509.15482</guid>
<content:encoded><![CDATA[
arXiv:2509.15482v2 Announce Type: replace-cross 
Abstract: Foundation models are increasingly developed in computational pathology (CPath) given their promise in facilitating many downstream tasks. While recent studies have evaluated task performance across models, less is known about the structure and variability of their learned representations. Here, we systematically analyze the representational spaces of six CPath foundation models using techniques popularized in computational neuroscience. The models analyzed span vision-language contrastive learning (CONCH, PLIP, KEEP) and self-distillation (UNI (v2), Virchow (v2), Prov-GigaPath) approaches. Through representational similarity analysis using H&amp;E image patches from TCGA, we find that UNI2 and Virchow2 have the most distinct representational structures, whereas Prov-Gigapath has the highest average similarity across models. Having the same training paradigm (vision-only vs. vision-language) did not guarantee higher representational similarity. The representations of all models showed a high slide-dependence, but relatively low disease-dependence. Stain normalization decreased slide-dependence for all models by a range of 5.5% (CONCH) to 20.5% (PLIP). In terms of intrinsic dimensionality, vision-language models demonstrated relatively compact representations, compared to the more distributed representations of vision-only models. These findings highlight opportunities to improve robustness to slide-specific features, inform model ensembling strategies, and provide insights into how training paradigms shape model representations. Our framework is extendable across medical imaging domains, where probing the internal representations of foundation models can support their effective development and deployment.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HyperAdapt: Simple High-Rank Adaptation</title>
<link>https://arxiv.org/abs/2509.18629</link>
<guid>https://arxiv.org/abs/2509.18629</guid>
<content:encoded><![CDATA[
arXiv:2509.18629v2 Announce Type: replace-cross 
Abstract: Foundation models excel across diverse tasks, but adapting them to specialized applications often requires fine-tuning, an approach that is memory and compute-intensive. Parameter-efficient fine-tuning (PEFT) methods mitigate this by updating only a small subset of weights. In this paper, we introduce HyperAdapt, a parameter-efficient fine-tuning method that significantly reduces the number of trainable parameters compared to state-of-the-art methods like LoRA. Specifically, HyperAdapt adapts a pre-trained weight matrix by applying row- and column-wise scaling through diagonal matrices, thereby inducing a high-rank update while requiring only $n+m$ trainable parameters for an $n \times m$ matrix. Theoretically, we establish an upper bound on the rank of HyperAdapt's updates, and empirically, we confirm that it consistently induces high-rank transformations across model layers. Experiments on GLUE, arithmetic reasoning, and commonsense reasoning benchmarks with models up to 14B parameters demonstrate that HyperAdapt matches or nearly matches the performance of full fine-tuning and state-of-the-art PEFT methods while using orders of magnitude fewer trainable parameters.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChessArena: A Chess Testbed for Evaluating Strategic Reasoning Capabilities of Large Language Models</title>
<link>https://arxiv.org/abs/2509.24239</link>
<guid>https://arxiv.org/abs/2509.24239</guid>
<content:encoded><![CDATA[
arXiv:2509.24239v2 Announce Type: replace-cross 
Abstract: Recent large language models (LLMs) have shown strong reasoning capabilities. However, a critical question remains: do these models possess genuine reasoning skills particularly complex strategic reasoning or are they primarily excelling at sophisticated pattern recognition within their training data? To address this question, this paper presents a chess testbed, ChessArena, to evaluate the strategic reasoning capabilities of LLMs. Chess requires complex strategic reasoning capabilities including long-term planning, strict rule comprehension, and multi-turn conversation memorization. Specifically, ChessArena is a competitive framework where LLMs play against each other, under four different play modes. The testbed is equipped with a ranking algorithm and a leaderboard. The testbed can also evaluate fine-grained capabilities including basic understanding, move selection, and puzzle solving. Over 13 LLMs with different modes are evaluated in ChessArena, playing over 800 games. The results reveal significant shortcomings in current LLMs: no model can beat Maia-1100 (a chess engine at human amateur level), while some even failed to defeat a random player that selects moves arbitrarily. We also present a strong baseline to the testbed: our fine-tuned Qwen3-8B substantially improved performance, approaching much larger state-of-the-art reasoning models.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Large Language Models To Reason In Parallel With Global Forking Tokens</title>
<link>https://arxiv.org/abs/2510.05132</link>
<guid>https://arxiv.org/abs/2510.05132</guid>
<content:encoded><![CDATA[
arXiv:2510.05132v2 Announce Type: replace-cross 
Abstract: Although LLMs have demonstrated improved performance by scaling parallel test-time compute, doing so relies on generating reasoning paths that are both diverse and accurate. For challenging problems, the forking tokens that trigger diverse yet correct reasoning modes are typically deep in the sampling tree. Consequently, common strategies to encourage diversity, such as temperature scaling, encounter a worsened trade-off between diversity and accuracy. Motivated by this challenge, we treat parallel reasoning as a set-of-next-token-prediction problem, and incorporate a set-based global loss into Supervised Fine-Tuning (SFT) using self-supervised bipartite matching between our global forking tokens and unique reasoning traces. We observe that, while naive fine-tuning with multiple reasoning traces collapses these unique reasoning modes, our proposed method, Set Supervised Fine-Tuning (SSFT), preserves these modes and produces emergent global forking tokens. Experiments on multiple reasoning benchmarks show that our SSFT consistently outperforms SFT under both Pass@1 and Cons@k metrics.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Navigate Socially Through Proactive Risk Perception</title>
<link>https://arxiv.org/abs/2510.07871</link>
<guid>https://arxiv.org/abs/2510.07871</guid>
<content:encoded><![CDATA[
arXiv:2510.07871v2 Announce Type: replace-cross 
Abstract: In this report, we describe the technical details of our submission to the IROS 2025 RoboSense Challenge Social Navigation Track. This track focuses on developing RGBD-based perception and navigation systems that enable autonomous agents to navigate safely, efficiently, and socially compliantly in dynamic human-populated indoor environments. The challenge requires agents to operate from an egocentric perspective using only onboard sensors including RGB-D observations and odometry, without access to global maps or privileged information, while maintaining social norm compliance such as safe distances and collision avoidance. Building upon the Falcon model, we introduce a Proactive Risk Perception Module to enhance social navigation performance. Our approach augments Falcon with collision risk understanding that learns to predict distance-based collision risk scores for surrounding humans, which enables the agent to develop more robust spatial awareness and proactive collision avoidance behaviors. The evaluation on the Social-HM3D benchmark demonstrates that our method improves the agent's ability to maintain personal space compliance while navigating toward goals in crowded indoor scenes with dynamic human agents, achieving 2nd place among 16 participating teams in the challenge.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Systematic Evaluation of Self-Supervised Learning for Label-Efficient Sleep Staging with Wearable EEG</title>
<link>https://arxiv.org/abs/2510.07960</link>
<guid>https://arxiv.org/abs/2510.07960</guid>
<content:encoded><![CDATA[
arXiv:2510.07960v2 Announce Type: replace-cross 
Abstract: Wearable EEG devices have emerged as a promising alternative to polysomnography (PSG). As affordable and scalable solutions, their widespread adoption results in the collection of massive volumes of unlabeled data that cannot be analyzed by clinicians at scale. Meanwhile, the recent success of deep learning for sleep scoring has relied on large annotated datasets. Self-supervised learning (SSL) offers an opportunity to bridge this gap, leveraging unlabeled signals to address label scarcity and reduce annotation effort. In this paper, we present the first systematic evaluation of SSL for sleep staging using wearable EEG. We investigate a range of well-established SSL methods and evaluate them on two sleep databases acquired with the Ikon Sleep wearable EEG headband: BOAS, a high-quality benchmark containing PSG and wearable EEG recordings with consensus labels, and HOGAR, a large collection of home-based, self-recorded, and unlabeled recordings. Three evaluation scenarios are defined to study label efficiency, representation quality, and cross-dataset generalization. Results show that SSL consistently improves classification performance by up to 10% over supervised baselines, with gains particularly evident when labeled data is scarce. SSL achieves clinical-grade accuracy above 80% leveraging only 5% to 10% of labeled data, while the supervised approach requires twice the labels. Additionally, SSL representations prove robust to variations in population characteristics, recording environments, and signal quality. Our findings demonstrate the potential of SSL to enable label-efficient sleep staging with wearable EEG, reducing reliance on manual annotations and advancing the development of affordable sleep monitoring systems.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integrating Sequential and Relational Modeling for User Events: Datasets and Prediction Tasks</title>
<link>https://arxiv.org/abs/2510.11903</link>
<guid>https://arxiv.org/abs/2510.11903</guid>
<content:encoded><![CDATA[
arXiv:2510.11903v2 Announce Type: replace-cross 
Abstract: User event modeling plays a central role in many machine learning applications, with use cases spanning e-commerce, social media, finance, cybersecurity, and other domains. User events can be broadly categorized into personal events, which involve individual actions, and relational events, which involve interactions between two users. These two types of events are typically modeled separately, using sequence-based methods for personal events and graph-based methods for relational events. Despite the need to capture both event types in real-world systems, prior work has rarely considered them together. This is often due to the convenient simplification that user behavior can be adequately represented by a single formalization, either as a sequence or a graph. To address this gap, there is a need for public datasets and prediction tasks that explicitly incorporate both personal and relational events. In this work, we introduce a collection of such datasets, propose a unified formalization, and empirically show that models benefit from incorporating both event types. Our results also indicate that current methods leave a notable room for improvements. We release these resources to support further research in unified user event modeling and encourage progress in this direction.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mathematics with large language models as provers and verifiers</title>
<link>https://arxiv.org/abs/2510.12829</link>
<guid>https://arxiv.org/abs/2510.12829</guid>
<content:encoded><![CDATA[
arXiv:2510.12829v3 Announce Type: replace-cross 
Abstract: During 2024 and 2025 the discussion about the theorem-proving capabilities of large language models started reporting interesting success stories, mostly to do with difficult exercises (such as problems from the International Mathematical Olympiad), but also with conjectures [Feldman & Karbasi, arXiv:2509.18383v1] formulated for the purpose of verifying whether the artificial intelligence could prove it. In this paper we report a theorem proving feat achieved by ChatGPT by using a protocol involving different prover and verifier instances of the gpt-5 model working collaboratively. To make sure that the produced proofs do not suffer from hallucinations, the final proof is formally verified by the lean proof assistant, and the conformance of premises and conclusion of the lean code is verified by a human. Our methodology is by no means complete or exact. It was nonetheless able to solve five out of six 2025 IMO problems, and close about a third of the sixty-six number theory conjectures in [Cohen, Journal of Integer Sequences, 2025].
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Edge Filter: Return of the Human-Crafted Layer in Deep Learning</title>
<link>https://arxiv.org/abs/2510.13865</link>
<guid>https://arxiv.org/abs/2510.13865</guid>
<content:encoded><![CDATA[
arXiv:2510.13865v4 Announce Type: replace-cross 
Abstract: We introduce the Deep Edge Filter, a novel approach that applies high-pass filtering to deep neural network features to improve model generalizability. Our method is motivated by our hypothesis that neural networks encode task-relevant semantic information in high-frequency components while storing domain-specific biases in low-frequency components of deep features. By subtracting low-pass filtered outputs from original features, our approach isolates generalizable representations while preserving architectural integrity. Experimental results across diverse domains such as Vision, Text, 3D, and Audio demonstrate consistent performance improvements regardless of model architecture and data modality. Analysis reveals that our method induces feature sparsification and effectively isolates high-frequency components, providing empirical validation of our core hypothesis. The code is available at https://github.com/dongkwani/DeepEdgeFilter.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RealDPO: Real or Not Real, that is the Preference</title>
<link>https://arxiv.org/abs/2510.14955</link>
<guid>https://arxiv.org/abs/2510.14955</guid>
<content:encoded><![CDATA[
arXiv:2510.14955v2 Announce Type: replace-cross 
Abstract: Video generative models have recently achieved notable advancements in synthesis quality. However, generating complex motions remains a critical challenge, as existing models often struggle to produce natural, smooth, and contextually consistent movements. This gap between generated and real-world motions limits their practical applicability. To address this issue, we introduce RealDPO, a novel alignment paradigm that leverages real-world data as positive samples for preference learning, enabling more accurate motion synthesis. Unlike traditional supervised fine-tuning (SFT), which offers limited corrective feedback, RealDPO employs Direct Preference Optimization (DPO) with a tailored loss function to enhance motion realism. By contrasting real-world videos with erroneous model outputs, RealDPO enables iterative self-correction, progressively refining motion quality. To support post-training in complex motion synthesis, we propose RealAction-5K, a curated dataset of high-quality videos capturing human daily activities with rich and precise motion details. Extensive experiments demonstrate that RealDPO significantly improves video quality, text alignment, and motion realism compared to state-of-the-art models and existing preference optimization techniques.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rewarding the Journey, Not Just the Destination: A Composite Path and Answer Self-Scoring Reward Mechanism for Test-Time Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.17923</link>
<guid>https://arxiv.org/abs/2510.17923</guid>
<content:encoded><![CDATA[
arXiv:2510.17923v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning (RL) has emerged as a powerful paradigm for advancing Large Language Models (LLMs), achieving remarkable performance in complex reasoning domains such as mathematics and code generation. However, current RL methods face a fundamental scalability bottleneck due to their heavy reliance on human-curated preference data or labeled datasets for reward modeling. To overcome this limitation, we explore RL on unlabeled data where models learn autonomously from continuous experience streams. The core challenge in this setting lies in reliable reward estimation without ground-truth supervision. Existing approaches like Test-Time RL address this through self-consistent consensus, but risk reinforcing incorrect pseudo-labels derived from majority voting. We introduce COMPASS (Composite Path and Answer Self-Scoring), a novel test-time reward mechanism that operates without external supervision. COMPASS integrates two complementary components: the Dual-Calibration Answer Reward (DCAR), which stabilizes training by establishing trustworthy pseudo-labels through confidence and credibility calibration, and the Decisive Path Reward (DPR), which directly optimizes the reasoning process quality beyond mere outcome supervision. By jointly reinforcing trustworthy consensus answers and highly decisive reasoning chains, the COMPASS systematically enhances the model's analytical capabilities. Extensive experiments show that COMPASS achieves significant and consistent performance gains across diverse reasoning tasks and model architectures, advancing a more scalable direction for LLMs to learn from continuous experience.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gestura: A LVLM-Powered System Bridging Motion and Semantics for Real-Time Free-Form Gesture Understanding</title>
<link>https://arxiv.org/abs/2510.21814</link>
<guid>https://arxiv.org/abs/2510.21814</guid>
<content:encoded><![CDATA[
arXiv:2510.21814v2 Announce Type: replace-cross 
Abstract: Free-form gesture understanding is highly appealing for human-computer interaction, as it liberates users from the constraints of predefined gesture categories. However, the sole existing solution GestureGPT suffers from limited recognition accuracy and slow response times. In this paper, we propose Gestura, an end-to-end system for free-form gesture understanding. Gestura harnesses a pre-trained Large Vision-Language Model (LVLM) to align the highly dynamic and diverse patterns of free-form gestures with high-level semantic concepts. To better capture subtle hand movements across different styles, we introduce a Landmark Processing Module that compensate for LVLMs' lack of fine-grained domain knowledge by embedding anatomical hand priors. Further, a Chain-of-Thought (CoT) reasoning strategy enables step-by-step semantic inference, transforming shallow knowledge into deep semantic understanding and significantly enhancing the model's ability to interpret ambiguous or unconventional gestures. Together, these components allow Gestura to achieve robust and adaptable free-form gesture comprehension. Additionally, we have developed the first open-source dataset for free-form gesture intention reasoning and understanding with over 300,000 annotated QA pairs.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TowerVision: Understanding and Improving Multilinguality in Vision-Language Models</title>
<link>https://arxiv.org/abs/2510.21849</link>
<guid>https://arxiv.org/abs/2510.21849</guid>
<content:encoded><![CDATA[
arXiv:2510.21849v3 Announce Type: replace-cross 
Abstract: Despite significant advances in vision-language models (VLMs), most existing work follows an English-centric design process, limiting their effectiveness in multilingual settings. In this work, we provide a comprehensive empirical study analyzing the impact of several multilingual design choices, such as training data composition, encoder selection, and text backbones. The result is TowerVision, a family of open multilingual VLMs for both image-text and video-text tasks, built upon the multilingual text-only model Tower+. TowerVision achieves competitive performance on multiple multimodal multilingual benchmarks and shows particular strength in culturally grounded tasks and multimodal translation. By incorporating visual and cultural context during fine-tuning, our models surpass existing approaches trained on substantially larger datasets, as demonstrated on ALM-Bench and Multi30K (image tasks) and ViMUL-Bench (video tasks). Alongside the models, we release VisionBlocks, a high-quality, curated vision-language dataset. Our findings highlight that multilingual vision-language training data substantially improves cross-lingual generalization -- both from high-resource to underrepresented languages and vice versa -- and that instruction-tuned LLMs are not always the optimal initialization point. To support further research, we publicly release all models, data, and training recipes.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TraceTrans: Translation and Spatial Tracing for Surgical Prediction</title>
<link>https://arxiv.org/abs/2510.22379</link>
<guid>https://arxiv.org/abs/2510.22379</guid>
<content:encoded><![CDATA[
arXiv:2510.22379v3 Announce Type: replace-cross 
Abstract: Image-to-image translation models have achieved notable success in converting images across visual domains and are increasingly used for medical tasks such as predicting post-operative outcomes and modeling disease progression. However, most existing methods primarily aim to match the target distribution and often neglect spatial correspondences between the source and translated images. This limitation can lead to structural inconsistencies and hallucinations, undermining the reliability and interpretability of the predictions. These challenges are accentuated in clinical applications by the stringent requirement for anatomical accuracy. In this work, we present TraceTrans, a novel deformable image translation model designed for post-operative prediction that generates images aligned with the target distribution while explicitly revealing spatial correspondences with the pre-operative input. The framework employs an encoder for feature extraction and dual decoders for predicting spatial deformations and synthesizing the translated image. The predicted deformation field imposes spatial constraints on the generated output, ensuring anatomical consistency with the source. Extensive experiments on medical cosmetology and brain MRI datasets demonstrate that TraceTrans delivers accurate and interpretable post-operative predictions, highlighting its potential for reliable clinical deployment.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Convex Over-the-Air Heterogeneous Federated Learning: A Bias-Variance Trade-off</title>
<link>https://arxiv.org/abs/2510.26722</link>
<guid>https://arxiv.org/abs/2510.26722</guid>
<content:encoded><![CDATA[
arXiv:2510.26722v3 Announce Type: replace-cross 
Abstract: Over-the-air (OTA) federated learning (FL) has been well recognized as a scalable paradigm that exploits the waveform superposition of the wireless multiple-access channel to aggregate model updates in a single use. Existing OTA-FL designs largely enforce zero-bias model updates by either assuming \emph{homogeneous} wireless conditions (equal path loss across devices) or forcing zero-bias updates to guarantee convergence. Under \emph{heterogeneous} wireless scenarios, however, such designs are constrained by the weakest device and inflate the update variance. Moreover, prior analyses of biased OTA-FL largely address convex objectives, while most modern AI models are highly non-convex. Motivated by these gaps, we study OTA-FL with stochastic gradient descent (SGD) for general smooth non-convex objectives under wireless heterogeneity. We develop novel OTA-FL SGD updates that allow a structured, time-invariant model bias while facilitating reduced variance updates. We derive a finite-time stationarity bound (expected time average squared gradient norm) that explicitly reveals a bias-variance trade-off. To optimize this trade-off, we pose a non-convex joint OTA power-control design and develop an efficient successive convex approximation (SCA) algorithm that requires only statistical CSI at the base station. Experiments on a non-convex image classification task validate the approach: the SCA-based design accelerates convergence via an optimized bias and improves generalization over prior OTA-FL baselines.
]]></content:encoded>
<pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why Isn't Relational Learning Taking Over the World?</title>
<link>https://arxiv.org/abs/2507.13558</link>
<guid>https://arxiv.org/abs/2507.13558</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, entities, relational learning, statistical relational AI, data <br />
Summary: <br />
- The article discusses the dominance of artificial intelligence systems that model pixels, words, and phonemes, suggesting a shift towards modeling entities with properties and relations.
- It argues that valuable data in the world is often in relational formats such as spreadsheets and databases, containing identifiers that cannot be interpreted naively as numbers.
- The field of relational learning, also known as statistical relational AI, focuses on studying this type of data but has not yet gained widespread prominence.
- The paper highlights the importance of bringing relational learning to the forefront and addresses the challenges involved in doing so.
- Overall, the article emphasizes the need for a shift towards modeling entities and their relationships, highlighting the potential benefits of incorporating relational learning into artificial intelligence systems. <br /> <div>
arXiv:2507.13558v5 Announce Type: replace 
Abstract: Artificial intelligence seems to be taking over the world with systems that model pixels, words, and phonemes. The world is arguably made up, not of pixels, words, and phonemes but of entities (objects, things, including events) with properties and relations among them. Surely we should model these, not the perception or description of them. You might suspect that concentrating on modeling words and pixels is because all of the (valuable) data in the world is in terms of text and images. If you look into almost any company you will find their most valuable data is in spreadsheets, databases and other relational formats. These are not the form that are studied in introductory machine learning, but are full of product numbers, student numbers, transaction numbers and other identifiers that can't be interpreted naively as numbers. The field that studies this sort of data has various names including relational learning, statistical relational AI, and many others. This paper explains why relational learning is not taking over the world -- except in a few cases with restricted relations -- and what needs to be done to bring it to it's rightful prominence.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Control Protocols for Untrusted AI Agents</title>
<link>https://arxiv.org/abs/2511.02997</link>
<guid>https://arxiv.org/abs/2511.02997</guid>
<content:encoded><![CDATA[
<div> control protocols, safety evaluation, AI agents, adaptive adversaries, SHADE-Arena <br />
<br />
Summary: 
The study focuses on evaluating AI control protocols in ensuring the safe operation of AI systems. Various blue team protocols such as deferral to trusted models, resampling, and deferring on critical actions were tested against a default attack policy in SHADE-Arena environments. Resampling for incrimination and deferring on critical actions were found to significantly increase safety. Red team strategies were then iterated against these protocols, with attack policies gaining insight into resampling leading to decreased safety. However, the deferring on critical actions protocol remained highly robust against the strongest red team strategies, emphasizing the importance of restricting access to protocol internals to prevent attacks and ensure AI system safety. This study highlights the need for effective control protocols to mitigate risks from untrusted AI agents in diverse agentic environments. 
<br /> <div>
arXiv:2511.02997v1 Announce Type: new 
Abstract: As AI systems become more capable and widely deployed as agents, ensuring their safe operation becomes critical. AI control offers one approach to mitigating the risk from untrusted AI agents by monitoring their actions and intervening or auditing when necessary. Evaluating the safety of these protocols requires understanding both their effectiveness against current attacks and their robustness to adaptive adversaries. In this work, we systematically evaluate a range of control protocols in SHADE-Arena, a dataset of diverse agentic environments. First, we evaluate blue team protocols, including deferral to trusted models, resampling, and deferring on critical actions, against a default attack policy. We find that resampling for incrimination and deferring on critical actions perform best, increasing safety from 50% to 96%. We then iterate on red team strategies against these protocols and find that attack policies with additional affordances, such as knowledge of when resampling occurs or the ability to simulate monitors, can substantially improve attack success rates against our resampling strategy, decreasing safety to 17%. However, deferring on critical actions is highly robust to even our strongest red team strategies, demonstrating the importance of denying attack policies access to protocol internals.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PublicAgent: Multi-Agent Design Principles From an LLM-Based Open Data Analysis Framework</title>
<link>https://arxiv.org/abs/2511.03023</link>
<guid>https://arxiv.org/abs/2511.03023</guid>
<content:encoded><![CDATA[
<div> Keywords: Open data repositories, language models, multi-agent framework, dataset discovery, analytical workflows<br />
Summary:<br />
PublicAgent introduces a multi-agent framework aimed at improving access to open data repositories for decision-making. By specializing agents in intent clarification, dataset discovery, analysis, and reporting, the framework maintains focused attention and enables validation at each stage of the analytical workflow. Five design principles for multi-agent LLM systems are derived from the evaluation, highlighting the value of specialization independent of model strength, the division of agents into universal and conditional categories, the mitigation of distinct failure modes, the persistence of architectural benefits across task complexity, and the necessity of model-aware architecture design. These principles guide the necessity of specialization for complex analytical workflows, ultimately enabling broader access to public data through natural language interfaces.<br /><br />Summary: <div>
arXiv:2511.03023v1 Announce Type: new 
Abstract: Open data repositories hold potential for evidence-based decision-making, yet are inaccessible to non-experts lacking expertise in dataset discovery, schema mapping, and statistical analysis. Large language models show promise for individual tasks, but end-to-end analytical workflows expose fundamental limitations: attention dilutes across growing contexts, specialized reasoning patterns interfere, and errors propagate undetected. We present PublicAgent, a multi-agent framework that addresses these limitations through decomposition into specialized agents for intent clarification, dataset discovery, analysis, and reporting. This architecture maintains focused attention within agent contexts and enables validation at each stage. Evaluation across five models and 50 queries derives five design principles for multi-agent LLM systems. First, specialization provides value independent of model strength--even the strongest model shows 97.5% agent win rates, with benefits orthogonal to model scale. Second, agents divide into universal (discovery, analysis) and conditional (report, intent) categories. Universal agents show consistent effectiveness (std dev 12.4%) while conditional agents vary by model (std dev 20.5%). Third, agents mitigate distinct failure modes--removing discovery or analysis causes catastrophic failures (243-280 instances), while removing report or intent causes quality degradation. Fourth, architectural benefits persist across task complexity with stable win rates (86-92% analysis, 84-94% discovery), indicating workflow management value rather than reasoning enhancement. Fifth, wide variance in agent effectiveness across models (42-96% for analysis) requires model-aware architecture design. These principles guide when and why specialization is necessary for complex analytical workflows while enabling broader access to public data through natural language interfaces.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>No-Human in the Loop: Agentic Evaluation at Scale for Recommendation</title>
<link>https://arxiv.org/abs/2511.03051</link>
<guid>https://arxiv.org/abs/2511.03051</guid>
<content:encoded><![CDATA[
<div> benchmarking study, large language models, evaluation protocol, recommendation, consensus 

Summary:
- The study, ScalingEval, compared 36 large language models (LLMs) like GPT and Gemini across various product categories using a consensus-driven evaluation protocol.
- Ground-truth labels were created through scalable majority voting, eliminating the need for human annotation.
- Anthropic Claude 3.5 Sonnet demonstrated the highest decision confidence, while Gemini 1.5 Pro showed the best overall performance and GPT-4o offered a favorable latency-accuracy-cost tradeoff.
- GPT-OSS 20B was the leading open-source model in the study.
- Analysis revealed strong consensus in structured domains like Electronics and Sports, but disagreement in lifestyle categories such as Clothing and Food. 

<br /><br />Summary: <div>
arXiv:2511.03051v1 Announce Type: new 
Abstract: Evaluating large language models (LLMs) as judges is increasingly critical for building scalable and trustworthy evaluation pipelines. We present ScalingEval, a large-scale benchmarking study that systematically compares 36 LLMs, including GPT, Gemini, Claude, and Llama, across multiple product categories using a consensus-driven evaluation protocol. Our multi-agent framework aggregates pattern audits and issue codes into ground-truth labels via scalable majority voting, enabling reproducible comparison of LLM evaluators without human annotation. Applied to large-scale complementary-item recommendation, the benchmark reports four key findings: (i) Anthropic Claude 3.5 Sonnet achieves the highest decision confidence; (ii) Gemini 1.5 Pro offers the best overall performance across categories; (iii) GPT-4o provides the most favorable latency-accuracy-cost tradeoff; and (iv) GPT-OSS 20B leads among open-source models. Category-level analysis shows strong consensus in structured domains (Electronics, Sports) but persistent disagreement in lifestyle categories (Clothing, Food). These results establish ScalingEval as a reproducible benchmark and evaluation protocol for LLMs as judges, with actionable guidance on scaling, reliability, and model family tradeoffs.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Epidemiology of Large Language Models: A Benchmark for Observational Distribution Knowledge</title>
<link>https://arxiv.org/abs/2511.03070</link>
<guid>https://arxiv.org/abs/2511.03070</guid>
<content:encoded><![CDATA[
<div> Benchmark, LLMs, knowledge, probability distributions, real-world<br />
Summary:<br />
- LLMs are powerful AI systems used in various applications but struggle with probabilistic knowledge.<br />
- This paper introduces a benchmark to assess LLMs' understanding of probability distributions in real-world domains.<br />
- LLMs, despite being trained on vast text data, perform poorly in internalizing real-world statistics.<br />
- The benchmark shows LLMs lack knowledge on observational distributions according to Pearl's Causal Hierarchy.<br />
- Limited interventional and counterfactual knowledge in LLMs as per the Causal Hierarchy Theorem.<br /> <div>
arXiv:2511.03070v1 Announce Type: new 
Abstract: Artificial intelligence (AI) systems hold great promise for advancing various scientific disciplines, and are increasingly used in real-world applications. Despite their remarkable progress, further capabilities are expected in order to achieve more general types of intelligence. A critical distinction in this context is between factual knowledge, which can be evaluated against true or false answers (e.g., "what is the capital of England?"), and probabilistic knowledge, reflecting probabilistic properties of the real world (e.g., "what is the sex of a computer science graduate in the US?"). In this paper, our goal is to build a benchmark for understanding the capabilities of LLMs in terms of knowledge of probability distributions describing the real world. Given that LLMs are trained on vast amounts of text, it may be plausible that they internalize aspects of these distributions. Indeed, LLMs are touted as powerful universal approximators of real-world distributions. At the same time, classical results in statistics, known as curse of dimensionality, highlight fundamental challenges in learning distributions in high dimensions, challenging the notion of universal distributional learning. In this work, we develop the first benchmark to directly test this hypothesis, evaluating whether LLMs have access to empirical distributions describing real-world populations across domains such as economics, health, education, and social behavior. Our results demonstrate that LLMs perform poorly overall, and do not seem to internalize real-world statistics naturally. When interpreted in the context of Pearl's Causal Hierarchy (PCH), our benchmark demonstrates that language models do not contain knowledge on observational distributions (Layer 1 of PCH), and thus the Causal Hierarchy Theorem implies that interventional (Layer 2) and counterfactual (Layer 3) knowledge of these models is also limited.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators</title>
<link>https://arxiv.org/abs/2511.03092</link>
<guid>https://arxiv.org/abs/2511.03092</guid>
<content:encoded><![CDATA[
<div> KV cache compression, Large Language Models, on-chip memory, SnapStream, production deployment
Summary:
- The increasing use of Large Language Models (LLMs) with massive parameter sizes has led to a need for on-chip memory to support large KV caches.
- Existing techniques like StreamingLLM and SnapKV demonstrate how to control KV cache size while maintaining model accuracy, but are not widely used in industrial frameworks due to static graphs and continuous batching methodologies.
- This paper explores the impact of KV cache compression, specifically SnapStream, on Llama-3.1-8B-Instruct and DeepSeek-R1 models.
- SnapStream is shown to significantly improve on-chip memory usage in a 16-way tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators without significant accuracy degradation on various benchmark datasets.
- The deployment of sparse KV attention techniques in a production inference system using static graphs and continuous batching is a novel contribution of this work.
<br /><br />Summary: <div>
arXiv:2511.03092v1 Announce Type: new 
Abstract: The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+ context length support have resulted in increasing demands for on-chip memory to support large KV caches. Techniques such as StreamingLLM and SnapKV demonstrate how to control KV cache size while maintaining model accuracy. Yet, these techniques are not commonly used within industrial deployments using frameworks like vLLM or SGLang. The reason is twofold: on one hand, the static graphs and continuous batching methodology employed by these frameworks make it difficult to admit modifications to the standard multi-head attention algorithm, while on the other hand, the accuracy implications of such techniques on modern instruction-following and reasoning models are not well understood, obfuscating the need for implementing these techniques. In this paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and DeepSeek-R1, and develop SnapStream, a KV cache compression method that can be deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators running at 128k context length and up to 1832 tokens per second in a real production setting. SnapStream enables $4\times$ improved on-chip memory usage and introduces minimal accuracy degradation on LongBench-v2, AIME24 and LiveCodeBench. To the best of our knowledge, this is the first implementation of sparse KV attention techniques deployed in a production inference system with static graphs and continuous batching.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large language models require a new form of oversight: capability-based monitoring</title>
<link>https://arxiv.org/abs/2511.03106</link>
<guid>https://arxiv.org/abs/2511.03106</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, healthcare, monitoring, capability-based monitoring, artificial intelligence

Summary: 
Capability-based monitoring is proposed as a new approach for overseeing large language models (LLMs) in healthcare. Traditional task-based monitoring methods may not be adequate for LLMs, as they are generalist systems with overlapping capabilities used across various tasks. Capability-based monitoring focuses on shared model capabilities, such as summarization and reasoning, to detect systemic weaknesses, errors, and emergent behaviors. This approach enables cross-task detection of issues that task-based monitoring may overlook. Developers, organizational leaders, and professional societies are advised to implement capability-based monitoring to ensure the safe and collaborative monitoring of LLMs and future AI models in healthcare. This new organizing principle aims to provide a scalable foundation for monitoring the performance and safety of LLMs and other generalist AI models in healthcare settings.<br /><br />Summary: <div>
arXiv:2511.03106v1 Announce Type: new 
Abstract: The rapid adoption of large language models (LLMs) in healthcare has been accompanied by scrutiny of their oversight. Existing monitoring approaches, inherited from traditional machine learning (ML), are task-based and founded on assumed performance degradation arising from dataset drift. In contrast, with LLMs, inevitable model degradation due to changes in populations compared to the training dataset cannot be assumed, because LLMs were not trained for any specific task in any given population. We therefore propose a new organizing principle guiding generalist LLM monitoring that is scalable and grounded in how these models are developed and used in practice: capability-based monitoring. Capability-based monitoring is motivated by the fact that LLMs are generalist systems whose overlapping internal capabilities are reused across numerous downstream tasks. Instead of evaluating each downstream task independently, this approach organizes monitoring around shared model capabilities, such as summarization, reasoning, translation, or safety guardrails, in order to enable cross-task detection of systemic weaknesses, long-tail errors, and emergent behaviors that task-based monitoring may miss. We describe considerations for developers, organizational leaders, and professional societies for implementing a capability-based monitoring approach. Ultimately, capability-based monitoring will provide a scalable foundation for safe, adaptive, and collaborative monitoring of LLMs and future generalist artificial intelligence models in healthcare.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>miniF2F-Lean Revisited: Reviewing Limitations and Charting a Path Forward</title>
<link>https://arxiv.org/abs/2511.03108</link>
<guid>https://arxiv.org/abs/2511.03108</guid>
<content:encoded><![CDATA[
<div> machine learning, artificial intelligence, theorem proving, mathematical reasoning, benchmark

Summary: 
The article presents an analysis of the miniF2F benchmark from the perspective of an AI system participating in a math Olympiad. The model is required to read, comprehend, formalize, and prove the problems presented in natural language. Results show that the accuracy of this pipeline can reach 36% using state-of-the-art models, lower than individual accuracies in autoformalization and theorem proving. The discrepancies between formal and informal statements contribute to this lower accuracy. The miniF2F-v2 is introduced with corrected formal and informal statements, leading to an accuracy improvement to 70%. However, there is still a misalignment between autoformalization models and theorem provers. The article suggests that a higher quality benchmark can aid in evaluating progress in formal reasoning and diagnosing failure and success modes in autoformalization and theorem proving models. The dataset for miniF2F-v2 is available on GitHub at https://github.com/roozbeh-yz/miniF2F_v2. 

<br /><br />Summary: <div>
arXiv:2511.03108v1 Announce Type: new 
Abstract: We perform a thorough analysis of the formal and informal statements in the miniF2F benchmark from the perspective of an AI system that is tasked to participate in a math Olympiad consisting of the problems in miniF2F. In such setting, the model has to read and comprehend the problems in natural language, formalize them in Lean language, then proceed with proving the problems, and it will get credit for each problem if the formal proof corresponds to the original informal statement presented to the model. Our evaluation results reveal that the best accuracy of such pipeline can be about 36% using the SoTA models in the literature, considerably lower than the individual SoTA accuracies, 97% and 69% reported in the autoformalization and theorem proving literature. Analyzing the failure modes, we trace back a considerable portion of this drop to discrepancies between the formal and informal statements for more than half of the problems in miniF2F. We proceed with correcting all the errors, discrepancies and simplifications in formal and informal statements, and present the miniF2F-v2 with fully verified formal and informal statements and proofs. Evaluating the full theorem proving pipeline on miniF2F-v2 leads to the best accuracy of 70%, a significant improvement from the 40% on the original miniF2F, yet indicating considerable misalignment between the autoformalization models and theorem provers. Our deep analysis suggests that a higher quality benchmark can help the community better evaluate progress in the field of formal reasoning and also better diagnose the failure and success modes of autoformalization and theorem proving models. Our dataset is available at https://github.com/roozbeh-yz/miniF2F_v2.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Using Multi-modal Large Language Model to Boost Fireworks Algorithm's Ability in Settling Challenging Optimization Tasks</title>
<link>https://arxiv.org/abs/2511.03137</link>
<guid>https://arxiv.org/abs/2511.03137</guid>
<content:encoded><![CDATA[
<div> Large language models, optimization techniques, fireworks algorithm, multi-modal, critical part<br />
Summary:<br />
The study focuses on addressing the challenges posed by complex and diverse optimization problems by incorporating large language models into the fireworks algorithm (FWA). The traditional optimization methods struggle with non-convexity and high dimensionality, making them inefficient. By leveraging multi-modal large language models, the proposed approach introduces the concept of Critical Part (CP) in FWA to enhance optimization for complex high-dimensional tasks like the Traveling Salesman Problem (TSP) and Electronic Design Automation Problem (EDA). Experimental results demonstrate that the FWA designed using the new framework achieves state-of-the-art results on various problem instances. <div>
arXiv:2511.03137v1 Announce Type: new 
Abstract: As optimization problems grow increasingly complex and diverse, advancements in optimization techniques and paradigm innovations hold significant importance. The challenges posed by optimization problems are primarily manifested in their non-convexity, high-dimensionality, black-box nature, and other unfavorable characteristics. Traditional zero-order or first-order methods, which are often characterized by low efficiency, inaccurate gradient information, and insufficient utilization of optimization information, are ill-equipped to address these challenges effectively. In recent years, the rapid development of large language models (LLM) has led to substantial improvements in their language understanding and code generation capabilities. Consequently, the design of optimization algorithms leveraging large language models has garnered increasing attention from researchers. In this study, we choose the fireworks algorithm(FWA) as the basic optimizer and propose a novel approach to assist the design of the FWA by incorporating multi-modal large language model(MLLM). To put it simply, we propose the concept of Critical Part(CP), which extends FWA to complex high-dimensional tasks, and further utilizes the information in the optimization process with the help of the multi-modal characteristics of large language models. We focus on two specific tasks: the \textit{traveling salesman problem }(TSP) and \textit{electronic design automation problem} (EDA). The experimental results show that FWAs generated under our new framework have achieved or surpassed SOTA results on many problem instances.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Proprietary Model-Based Safety Response Framework for AI Agents</title>
<link>https://arxiv.org/abs/2511.03138</link>
<guid>https://arxiv.org/abs/2511.03138</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, safety response framework, risk identification, fine-tuning, trustworthy knowledge base

Summary:
The paper introduces a safety response framework for Large Language Models (LLMs) to address security issues in critical domains. At the input level, a supervised fine-tuning-based safety classification model with a four-tier taxonomy identifies and handles user queries based on risk levels, achieving a high recall rate. At the output level, the framework integrates Retrieval-Augmented Generation (RAG) with a fine-tuned interpretation model to ensure responses are grounded in real-time knowledge, eliminating information fabrication. Experimental results show the framework outperforms baseline models on safety evaluation benchmarks and achieves a perfect safety score on a high-risk test set. This research presents an effective approach for developing secure and trustworthy LLM applications.

<br /><br />Summary: <div>
arXiv:2511.03138v1 Announce Type: new 
Abstract: With the widespread application of Large Language Models (LLMs), their associated security issues have become increasingly prominent, severely constraining their trustworthy deployment in critical domains. This paper proposes a novel safety response framework designed to systematically safeguard LLMs at both the input and output levels. At the input level, the framework employs a supervised fine-tuning-based safety classification model. Through a fine-grained four-tier taxonomy (Safe, Unsafe, Conditionally Safe, Focused Attention), it performs precise risk identification and differentiated handling of user queries, significantly enhancing risk coverage and business scenario adaptability, and achieving a risk recall rate of 99.3%. At the output level, the framework integrates Retrieval-Augmented Generation (RAG) with a specifically fine-tuned interpretation model, ensuring all responses are grounded in a real-time, trustworthy knowledge base. This approach eliminates information fabrication and enables result traceability. Experimental results demonstrate that our proposed safety control model achieves a significantly higher safety score on public safety evaluation benchmarks compared to the baseline model, TinyR1-Safety-8B. Furthermore, on our proprietary high-risk test set, the framework's components attained a perfect 100% safety score, validating their exceptional protective capabilities in complex risk scenarios. This research provides an effective engineering pathway for building high-security, high-trust LLM applications.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncovering Bugs in Formal Explainers: A Case Study with PyXAI</title>
<link>https://arxiv.org/abs/2511.03169</link>
<guid>https://arxiv.org/abs/2511.03169</guid>
<content:encoded><![CDATA[
<div> Keywords: Formal explainable artificial intelligence, validation, PyXAI, incorrect explanations, methodology <br />
Summary: Formal explainable artificial intelligence (XAI) is valuable for ensuring rigorous explainability in AI systems. However, the practical implementations of formal explainers like PyXAI require validation. This paper introduces a new methodology for validating formal explainers and assesses PyXAI. The experiments revealed incorrect explanations generated by PyXAI on various datasets, highlighting the need for rigorous validation methods. This study underscores the importance of ensuring the accuracy and reliability of formal explainers in AI systems. <br /><br />Summary: <div>
arXiv:2511.03169v1 Announce Type: new 
Abstract: Formal explainable artificial intelligence (XAI) offers unique theoretical guarantees of rigor when compared to other non-formal methods of explainability. However, little attention has been given to the validation of practical implementations of formal explainers. This paper develops a novel methodology for validating formal explainers and reports on the assessment of the publicly available formal explainer PyXAI. The paper documents the existence of incorrect explanations computed by PyXAI on most of the datasets analyzed in the experiments, thereby confirming the importance of the proposed novel methodology for the validation of formal explainers.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Autonomous Engineering Design: A Knowledge-Guided Multi-Agent Framework</title>
<link>https://arxiv.org/abs/2511.03179</link>
<guid>https://arxiv.org/abs/2511.03179</guid>
<content:encoded><![CDATA[
<div> Keywords: engineering design process, multi-agent AI framework, aerodynamic optimization, knowledge graphs, collaborative AI agents<br />
Summary: 
The article introduces a novel multi-agent AI framework for the engineering design process, focusing on the aerodynamic optimization of 4-digit NACA airfoils. The framework consists of three key AI agents: a Graph Ontologist, a Design Engineer, and a Systems Engineer. The Graph Ontologist constructs knowledge graphs using a Large Language Model based on airfoil design literature. The Systems Engineer formulates technical requirements, guiding the Design Engineer in generating and evaluating candidate airfoils. An iterative feedback loop between the Design Engineer and Systems Engineer, aided by knowledge graphs, refines the designs until validation by a human manager. The optimized design aims to maximize performance metrics, such as the lift-to-drag ratio, showcasing the efficiency, consistency, and quality improvements achievable through collaborative AI agents with structured knowledge representations. <br /><br />Summary: <div>
arXiv:2511.03179v1 Announce Type: new 
Abstract: The engineering design process often demands expertise from multiple domains, leading to complex collaborations and iterative refinements. Traditional methods can be resource-intensive and prone to inefficiencies. To address this, we formalize the engineering design process through a multi-agent AI framework that integrates structured design and review loops. The framework introduces specialized knowledge-driven agents that collaborate to generate and refine design candidates. As an exemplar, we demonstrate its application to the aerodynamic optimization of 4-digit NACA airfoils. The framework consists of three key AI agents: a Graph Ontologist, a Design Engineer, and a Systems Engineer. The Graph Ontologist employs a Large Language Model (LLM) to construct two domain-specific knowledge graphs from airfoil design literature. The Systems Engineer, informed by a human manager, formulates technical requirements that guide design generation and evaluation. The Design Engineer leverages the design knowledge graph and computational tools to propose candidate airfoils meeting these requirements. The Systems Engineer reviews and provides feedback both qualitative and quantitative using its own knowledge graph, forming an iterative feedback loop until a design is validated by the manager. The final design is then optimized to maximize performance metrics such as the lift-to-drag ratio. Overall, this work demonstrates how collaborative AI agents equipped with structured knowledge representations can enhance efficiency, consistency, and quality in the engineering design process.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adobe Summit Concierge Evaluation with Human in the Loop</title>
<link>https://arxiv.org/abs/2511.03186</link>
<guid>https://arxiv.org/abs/2511.03186</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI assistants, Summit Concierge, domain-specific, human-in-the-loop, agile development<br />
<br />
Summary: 
The article discusses the development of Summit Concierge, a domain-specific AI assistant designed for Adobe Summit events. The AI assistant is created to handle various event-related queries efficiently and effectively. To overcome challenges such as data sparsity, quality assurance, and rapid deployment, a human-in-the-loop development workflow is utilized. This workflow includes prompt engineering, retrieval grounding, and lightweight human validation. The system architecture, development process, and real-world deployment outcomes are described in detail. The experience highlights the importance of agile, feedback-driven development in creating scalable and reliable AI assistants, particularly in cold-start scenarios. Overall, Summit Concierge demonstrates the potential of generative AI assistants in enhancing productivity, streamlining information access, and improving user experience in enterprise settings. <div>
arXiv:2511.03186v1 Announce Type: new 
Abstract: Generative AI assistants offer significant potential to enhance productivity, streamline information access, and improve user experience in enterprise contexts. In this work, we present Summit Concierge, a domain-specific AI assistant developed for Adobe Summit. The assistant handles a wide range of event-related queries and operates under real-world constraints such as data sparsity, quality assurance, and rapid deployment. To address these challenges, we adopt a human-in-the-loop development workflow that combines prompt engineering, retrieval grounding, and lightweight human validation. We describe the system architecture, development process, and real-world deployment outcomes. Our experience shows that agile, feedback-driven development enables scalable and reliable AI assistants, even in cold-start scenarios.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Five Dimensions to Many: Large Language Models as Precise and Interpretable Psychological Profilers</title>
<link>https://arxiv.org/abs/2511.03235</link>
<guid>https://arxiv.org/abs/2511.03235</guid>
<content:encoded><![CDATA[
<div> Large Language Models (LLMs), Psychological Traits, Interconnected, Human Data Analysis, Prediction Accuracy <br />
Summary:
- LLMs can model correlational structure of human psychological traits accurately
- LLMs use a systematic two-stage process to generate responses
- Information selection and compression are key steps in LLM reasoning
- Compressed summaries capture synergistic information and enhance prediction alignment
- LLMs demonstrate emergent reasoning capabilities in capturing trait interplay.<br /><br />Summary: <div>
arXiv:2511.03235v1 Announce Type: new 
Abstract: Psychological constructs within individuals are widely believed to be interconnected. We investigated whether and how Large Language Models (LLMs) can model the correlational structure of human psychological traits from minimal quantitative inputs. We prompted various LLMs with Big Five Personality Scale responses from 816 human individuals to role-play their responses on nine other psychological scales. LLMs demonstrated remarkable accuracy in capturing human psychological structure, with the inter-scale correlation patterns from LLM-generated responses strongly aligning with those from human data $(R^2 > 0.89)$. This zero-shot performance substantially exceeded predictions based on semantic similarity and approached the accuracy of machine learning algorithms trained directly on the dataset. Analysis of reasoning traces revealed that LLMs use a systematic two-stage process: First, they transform raw Big Five responses into natural language personality summaries through information selection and compression, analogous to generating sufficient statistics. Second, they generate target scale responses based on reasoning from these summaries. For information selection, LLMs identify the same key personality factors as trained algorithms, though they fail to differentiate item importance within factors. The resulting compressed summaries are not merely redundant representations but capture synergistic information--adding them to original scores enhances prediction alignment, suggesting they encode emergent, second-order patterns of trait interplay. Our findings demonstrate that LLMs can precisely predict individual participants' psychological traits from minimal data through a process of abstraction and reasoning, offering both a powerful tool for psychological simulation and valuable insights into their emergent reasoning capabilities.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Scalable Web Accessibility Audit with MLLMs as Copilots</title>
<link>https://arxiv.org/abs/2511.03471</link>
<guid>https://arxiv.org/abs/2511.03471</guid>
<content:encoded><![CDATA[
<div> Keywords: web accessibility, auditing framework, human-AI partnership, multimodal sampling, large language model<br />
Summary:<br />
This article introduces a new auditing framework, AAA, designed to improve web accessibility by leveraging a human-AI partnership model. The framework utilizes innovative techniques such as GRASP, a graph-based multimodal sampling method, and MaC, a multimodal large language model-based copilot, to enhance the auditing process. By combining human expertise with AI assistance, the framework aims to address the resource-intensive nature of current auditing practices and enable scalable evaluation of website user interfaces for compliance with accessibility standards. The approach is supported by four novel datasets specifically created for benchmarking different stages of the audit pipeline. Experiment results demonstrate the effectiveness of the proposed methods, highlighting the potential for small-scale language models to be valuable tools when fine-tuned for specific tasks.<br /><br />Summary: <div>
arXiv:2511.03471v1 Announce Type: new 
Abstract: Ensuring web accessibility is crucial for advancing social welfare, justice, and equality in digital spaces, yet the vast majority of website user interfaces remain non-compliant, due in part to the resource-intensive and unscalable nature of current auditing practices. While WCAG-EM offers a structured methodology for site-wise conformance evaluation, it involves great human efforts and lacks practical support for execution at scale. In this work, we present an auditing framework, AAA, which operationalizes WCAG-EM through a human-AI partnership model. AAA is anchored by two key innovations: GRASP, a graph-based multimodal sampling method that ensures representative page coverage via learned embeddings of visual, textual, and relational cues; and MaC, a multimodal large language model-based copilot that supports auditors through cross-modal reasoning and intelligent assistance in high-effort tasks. Together, these components enable scalable, end-to-end web accessibility auditing, empowering human auditors with AI-enhanced assistance for real-world impact. We further contribute four novel datasets designed for benchmarking core stages of the audit pipeline. Extensive experiments demonstrate the effectiveness of our methods, providing insights that small-scale language models can serve as capable experts when fine-tuned.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explaining Decisions in ML Models: a Parameterized Complexity Analysis (Part I)</title>
<link>https://arxiv.org/abs/2511.03545</link>
<guid>https://arxiv.org/abs/2511.03545</guid>
<content:encoded><![CDATA[
<div> explanation problems, parameterized complexity, machine learning models, transparent internal mechanisms, explainable AI (XAI) 

Summary:
This paper delves into the parameterized complexity of explanation problems in various machine learning models with transparent internal mechanisms. It specifically explores abductive and contrastive explanation problems in both local and global contexts across different models like Decision Trees, Decision Sets, Boolean Circuits, and ensembles. By addressing the complexities of generating explanations for these models, the research aims to bridge a gap in the field of explainable AI (XAI), offering foundational insights for future XAI research. The study emphasizes the importance of transparency and accountability in AI systems, contributing to the ongoing discourse on ethical AI development. <div>
arXiv:2511.03545v1 Announce Type: new 
Abstract: This paper presents a comprehensive theoretical investigation into the parameterized complexity of explanation problems in various machine learning (ML) models. Contrary to the prevalent black-box perception, our study focuses on models with transparent internal mechanisms. We address two principal types of explanation problems: abductive and contrastive, both in their local and global variants. Our analysis encompasses diverse ML models, including Decision Trees, Decision Sets, Decision Lists, Boolean Circuits, and ensembles thereof, each offering unique explanatory challenges. This research fills a significant gap in explainable AI (XAI) by providing a foundational understanding of the complexities of generating explanations for these models. This work provides insights vital for further research in the domain of XAI, contributing to the broader discourse on the necessity of transparency and accountability in AI systems.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Outbidding and Outbluffing Elite Humans: Mastering Liar's Poker via Self-Play and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.03724</link>
<guid>https://arxiv.org/abs/2511.03724</guid>
<content:encoded><![CDATA[
<div> AI, poker, Liar's Poker, reinforcement learning, multi-player dynamics  
Summary:  
Solly, an AI agent, has achieved elite human-level play in reduced-format Liar's Poker, a game with extensive multi-player engagement. Trained through self-play using a deep reinforcement learning algorithm, Solly excelled in heads-up and multi-player scenarios. It outperformed large language models in win rate and equity, developed new bidding strategies, effectively used randomization, and proved difficult to exploit by world-class human players. This breakthrough expands the scope of AI capabilities in complex, multi-player games beyond traditional focus on two-player dynamics in poker variants. Solly's success showcases the potential of advanced reinforcement learning techniques in tackling challenges of imperfect information, reasoning under uncertainty, and dynamic multi-player interactions in strategic environments like Liar's Poker. <div>
arXiv:2511.03724v1 Announce Type: new 
Abstract: AI researchers have long focused on poker-like games as a testbed for environments characterized by multi-player dynamics, imperfect information, and reasoning under uncertainty. While recent breakthroughs have matched elite human play at no-limit Texas hold'em, the multi-player dynamics are subdued: most hands converge quickly with only two players engaged through multiple rounds of bidding. In this paper, we present Solly, the first AI agent to achieve elite human play in reduced-format Liar's Poker, a game characterized by extensive multi-player engagement. We trained Solly using self-play with a model-free, actor-critic, deep reinforcement learning algorithm. Solly played at an elite human level as measured by win rate (won over 50% of hands) and equity (money won) in heads-up and multi-player Liar's Poker. Solly also outperformed large language models (LLMs), including those with reasoning abilities, on the same metrics. Solly developed novel bidding strategies, randomized play effectively, and was not easily exploitable by world-class human players.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An extended reality-based framework for user risk training in urban built environment</title>
<link>https://arxiv.org/abs/2511.02837</link>
<guid>https://arxiv.org/abs/2511.02837</guid>
<content:encoded><![CDATA[
<div> framework, urban risks, XR technologies, risk training, flooding<br />
Summary:<br />
This paper introduces an extended Reality (XR)-based framework to enhance user risk training in urban environments, particularly focusing on climate change-induced flooding. The framework utilizes immersive XR technologies to simulate real-world emergency scenarios, increasing stakeholder awareness and preparedness. Stakeholder participation is crucial in customizing training modules to address specific needs, with an iterative approach that involves user feedback and performance data for continuous improvement. The methodological phases include user flow mapping, scenario selection, and performance evaluation, with a pilot application in Senigallia, Italy. The potential of XR technologies to revolutionize urban risk training is highlighted, fostering a culture of preparedness and resilience against urban hazards.<br /> <div>
arXiv:2511.02837v1 Announce Type: cross 
Abstract: In the context of increasing urban risks, particularly from climate change-induced flooding, this paper presents an extended Reality (XR)-based framework to improve user risk training within urban built environments. The framework is designed to improve risk awareness and preparedness among various stakeholders, including citizens, local authorities, and emergency responders. Using immersive XR technologies, the training experience simulates real-world emergency scenarios, contributing to active participation and a deeper understanding of potential hazards and especially for floods. The framework highlights the importance of stakeholder participation in its development, ensuring that training modules are customized to address the specific needs of different user groups. The iterative approach of the framework supports ongoing refinement through user feedback and performance data, thus improving the overall effectiveness of risk training initiatives. This work outlines the methodological phases involved in the framework's implementation, including i) user flow mapping, ii) scenario selection, and iii) performance evaluation, with a focus on the pilot application in Senigallia, Italy. The findings underscore the potential of XR technologies to transform urban risk training, promoting a culture of preparedness and resilience against urban hazards.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Generative AI as an Educational Tool for Radiology Resident Report Drafting</title>
<link>https://arxiv.org/abs/2511.02839</link>
<guid>https://arxiv.org/abs/2511.02839</guid>
<content:encoded><![CDATA[
<div> keyword: Radiology residents, automated feedback, breast imaging reports, GPT-4o, educational errors <br />
Summary: 
GPT-4o was evaluated for providing automated feedback on breast imaging reports by radiology residents in real clinical settings. Common error types identified included omission or addition of key findings, incorrect use of technical descriptors, and inconsistencies in final assessments. The study found strong agreement between GPT-4o and attending radiologists on identifying errors, with high ratings for the helpfulness of the feedback. Inter-reader reliability showed moderate variability, but replacing a human reader with GPT-4o did not significantly affect agreement. The results suggest that GPT-4o can reliably identify educational errors in radiology reports, making it a potentially scalable tool to support radiology education. <br /><br /> <div>
arXiv:2511.02839v1 Announce Type: cross 
Abstract: Objective: Radiology residents require timely, personalized feedback to develop accurate image analysis and reporting skills. Increasing clinical workload often limits attendings' ability to provide guidance. This study evaluates a HIPAA-compliant GPT-4o system that delivers automated feedback on breast imaging reports drafted by residents in real clinical settings.
  Methods: We analyzed 5,000 resident-attending report pairs from routine practice at a multi-site U.S. health system. GPT-4o was prompted with clinical instructions to identify common errors and provide feedback. A reader study using 100 report pairs was conducted. Four attending radiologists and four residents independently reviewed each pair, determined whether predefined error types were present, and rated GPT-4o's feedback as helpful or not. Agreement between GPT and readers was assessed using percent match. Inter-reader reliability was measured with Krippendorff's alpha. Educational value was measured as the proportion of cases rated helpful.
  Results: Three common error types were identified: (1) omission or addition of key findings, (2) incorrect use or omission of technical descriptors, and (3) final assessment inconsistent with findings. GPT-4o showed strong agreement with attending consensus: 90.5%, 78.3%, and 90.4% across error types. Inter-reader reliability showed moderate variability ({\alpha} = 0.767, 0.595, 0.567), and replacing a human reader with GPT-4o did not significantly affect agreement ({\Delta} = -0.004 to 0.002). GPT's feedback was rated helpful in most cases: 89.8%, 83.0%, and 92.0%.
  Discussion: ChatGPT-4o can reliably identify key educational errors. It may serve as a scalable tool to support radiology education.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Digital Transformation Chatbot (DTchatbot): Integrating Large Language Model-based Chatbot in Acquiring Digital Transformation Needs</title>
<link>https://arxiv.org/abs/2511.02842</link>
<guid>https://arxiv.org/abs/2511.02842</guid>
<content:encoded><![CDATA[
<div> Keywords: digital transformation, Large Language Model, chatbot, workflow-based, virtual expert <br />
Summary: <br />
The article explores the use of a chatbot powered by a Large Language Model (LLM) to understand organizations' digital transformation needs. Traditional methods like expert interviews face challenges such as scheduling conflicts and inconsistency. The chatbot integrates workflow-based instruction with LLM's capabilities to function as a virtual expert. This allows it to conduct interviews effectively, following predefined workflows while supporting user interactions. The chatbot's features and implementation are detailed, with a preliminary evaluation showing its successful performance but also highlighting areas for improvement. The article concludes by discussing the implications of using chatbots to gather user information, emphasizing their potential benefits and limitations. <br /> <br />Summary: <div>
arXiv:2511.02842v1 Announce Type: cross 
Abstract: Many organisations pursue digital transformation to enhance operational efficiency, reduce manual efforts, and optimise processes by automation and digital tools. To achieve this, a comprehensive understanding of their unique needs is required. However, traditional methods, such as expert interviews, while effective, face several challenges, including scheduling conflicts, resource constraints, inconsistency, etc. To tackle these issues, we investigate the use of a Large Language Model (LLM)-powered chatbot to acquire organisations' digital transformation needs. Specifically, the chatbot integrates workflow-based instruction with LLM's planning and reasoning capabilities, enabling it to function as a virtual expert and conduct interviews. We detail the chatbot's features and its implementation. Our preliminary evaluation indicates that the chatbot performs as designed, effectively following predefined workflows and supporting user interactions with areas for improvement. We conclude by discussing the implications of employing chatbots to elicit user information, emphasizing their potential and limitations.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Enhanced Wi-Fi Sensing Through Single Transceiver Pair</title>
<link>https://arxiv.org/abs/2511.02845</link>
<guid>https://arxiv.org/abs/2511.02845</guid>
<content:encoded><![CDATA[
<div> Keywords: Wi-Fi sensing, AI-driven perception, prior information, temporal correlation, performance gains <br />
<br />
Summary:  
The study explores the impact of AI-driven perception on next-generation Wi-Fi sensing systems. It finds that AI can improve perception abilities by utilizing prior information and exploiting temporal correlation. Prior information allows AI to enhance details based on vague input, while temporal correlation helps reduce sensing errors. The research validates these findings through experiments focusing on human pose estimation and indoor localization using a single transceiver pair. The results confirm the performance gains achieved through prior information and temporal correlation. This study sheds light on the theoretical underpinnings of the improvement brought by AI in Wi-Fi sensing systems, highlighting the importance of leveraging prior knowledge and temporal relationships for enhanced sensing capabilities. <div>
arXiv:2511.02845v1 Announce Type: cross 
Abstract: The advancement of next-generation Wi-Fi technology heavily relies on sensing capabilities, which play a pivotal role in enabling sophisticated applications. In response to the growing demand for large-scale deployments, contemporary Wi-Fi sensing systems strive to achieve high-precision perception while maintaining minimal bandwidth consumption and antenna count requirements. Remarkably, various AI-driven perception technologies have demonstrated the ability to surpass the traditional resolution limitations imposed by radar theory. However, the theoretical underpinnings of this phenomenon have not been thoroughly investigated in existing research. In this study, we found that under hardware-constrained conditions, the performance gains brought by AI to Wi-Fi sensing systems primarily originate from two aspects: prior information and temporal correlation. Prior information enables the AI to generate plausible details based on vague input, while temporal correlation helps reduce the upper bound of sensing error. We developed an AI-based Wi-Fi sensing system using a single transceiver pair and designed experiments focusing on human pose estimation and indoor localization to validate the theoretical claims. The results confirm the performance gains contributed by temporal correlation and prior information.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatio-Temporal Attention Network for Epileptic Seizure Prediction</title>
<link>https://arxiv.org/abs/2511.02846</link>
<guid>https://arxiv.org/abs/2511.02846</guid>
<content:encoded><![CDATA[
<div> Framework, Deep Learning, EEG, Seizures, Epilepsy <br />
<br />
Summary: 
The study introduces a deep learning framework called Spatio-Temporal Attention Network (STAN) to predict seizures onset accurately for epilepsy patients by learning complex spatio-temporal correlation structures in EEG signals. Unlike existing methods, STAN does not rely on feature engineering or assume fixed preictal durations, instead simultaneously modeling spatio-temporal correlations and using an adversarial discriminator to distinguish preictal from interictal attention patterns for patient-specific learning. The evaluation on CHB-MIT and MSSM datasets shows high sensitivity levels of 96.6% with 0.011/h false detection rate on CHB-MIT and 94.2% sensitivity with 0.063/h FDR on MSSM, surpassing current approaches significantly. The framework can detect preictal states at least 15 minutes before onset, with patient-specific windows extending up to 45 minutes, offering ample time for intervention in clinical settings. <div>
arXiv:2511.02846v1 Announce Type: cross 
Abstract: In this study, we present a deep learning framework that learns complex spatio-temporal correlation structures of EEG signals through a Spatio-Temporal Attention Network (STAN) for accurate predictions of onset of seizures for Epilepsy patients. Unlike existing methods, which rely on feature engineering and/or assume fixed preictal durations, our approach simultaneously models spatio-temporal correlations through STAN and employs an adversarial discriminator to distinguish preictal from interictal attention patterns, enabling patient-specific learning. Evaluation on CHB-MIT and MSSM datasets demonstrates 96.6\% sensitivity with 0.011/h false detection rate on CHB-MIT, and 94.2% sensitivity with 0.063/h FDR on MSSM, significantly outperforming state-of-the-art methods. The framework reliably detects preictal states at least 15 minutes before an onset, with patient-specific windows extending to 45 minutes, providing sufficient intervention time for clinical applications.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EEGReXferNet: A Lightweight Gen-AI Framework for EEG Subspace Reconstruction via Cross-Subject Transfer Learning and Channel-Aware Embedding</title>
<link>https://arxiv.org/abs/2511.02848</link>
<guid>https://arxiv.org/abs/2511.02848</guid>
<content:encoded><![CDATA[
<div> Keywords: EEG, artifact removal, generative models, transfer learning, real-time applications

Summary: 
EEGReXferNet is a lightweight Gen-AI framework developed for EEG subspace reconstruction through cross-subject transfer learning. It addresses the challenge of low SNR in EEG data caused by artifacts, by integrating volume conduction, band-specific convolution encoding, and dynamic latent feature extraction using sliding windows. The framework ensures spatial-temporal-spectral resolution, with mean PSD correlation >= 0.95 and mean spectrogram RV-Coefficient >= 0.85. It reduces overfitting by optimizing weight parameters, making it suitable for real-time EEG preprocessing in neurophysiological and BCI applications. EEGReXferNet's design emphasizes computational efficiency, making it a promising tool for enhancing EEG data quality and utility in brain monitoring and interface systems. 

<br /><br />Summary: <div>
arXiv:2511.02848v1 Announce Type: cross 
Abstract: Electroencephalography (EEG) is a widely used non-invasive technique for monitoring brain activity, but low signal-to-noise ratios (SNR) due to various artifacts often compromise its utility. Conventional artifact removal methods require manual intervention or risk suppressing critical neural features during filtering/reconstruction. Recent advances in generative models, including Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs), have shown promise for EEG reconstruction; however, these approaches often lack integrated temporal-spectral-spatial sensitivity and are computationally intensive, limiting their suitability for real-time applications like brain-computer interfaces (BCIs). To overcome these challenges, we introduce EEGReXferNet, a lightweight Gen-AI framework for EEG subspace reconstruction via cross-subject transfer learning - developed using Keras TensorFlow (v2.15.1). EEGReXferNet employs a modular architecture that leverages volume conduction across neighboring channels, band-specific convolution encoding, and dynamic latent feature extraction through sliding windows. By integrating reference-based scaling, the framework ensures continuity across successive windows and generalizes effectively across subjects. This design improves spatial-temporal-spectral resolution (mean PSD correlation >= 0.95; mean spectrogram RV-Coefficient >= 0.85), reduces total weights by ~45% to mitigate overfitting, and maintains computational efficiency for robust, real-time EEG preprocessing in neurophysiological and BCI applications.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Approaching Low-Cost Cardiac Intelligence with Semi-Supervised Knowledge Distillation</title>
<link>https://arxiv.org/abs/2511.02851</link>
<guid>https://arxiv.org/abs/2511.02851</guid>
<content:encoded><![CDATA[
<div> Keywords: cardiac artificial intelligence, wearable device data, knowledge distillation, semi-supervised training, cardiovascular diseases<br />
Summary:<br />
The study introduces LiteHeart, a framework aimed at improving the diagnostic performance of low-cost cardiac artificial intelligence systems using wearable device data such as ECG. LiteHeart incorporates a region-aware distillation module to focus on diagnostically relevant ECG regions and a cross-layer mutual information module to align decision processes between low-cost and high-cost systems. Additionally, a semi-supervised training strategy enhances model robustness under limited supervision. Evaluation on multiple datasets shows LiteHeart significantly reducing the performance gap between low-cost and high-cost cardiac intelligence systems, outperforming existing methods by up to 7.10% in macro F1 score. These results indicate that LiteHeart enhances the diagnostic capabilities of low-cost cardiac intelligence systems, enabling scalable, affordable, and accurate daily cardiac monitoring using wearable technologies.<br /> <div>
arXiv:2511.02851v1 Announce Type: cross 
Abstract: Deploying advanced cardiac artificial intelligence for daily cardiac monitoring is hindered by its reliance on extensive medical data and high computational resources. Low-cost cardiac intelligence (LCCI) offers a promising alternative by using wearable device data, such as 1-lead electrocardiogram (ECG), but it suffers from a significant diagnostic performance gap compared to high-cost cardiac intelligence (HCCI). To bridge this gap, we propose LiteHeart, a semi-supervised knowledge distillation framework. LiteHeart introduces a region-aware distillation module to mimic how cardiologists focus on diagnostically relevant ECG regions and a cross-layer mutual information module to align the decision processes of LCCI and HCCI systems. Using a semi-supervised training strategy, LiteHeart further improves model robustness under limited supervision. Evaluated on five datasets covering over 38 cardiovascular diseases, LiteHeart substantially reduces the performance gap between LCCI and HCCI, outperforming existing methods by 4.27% to 7.10% in macro F1 score. These results demonstrate that LiteHeart significantly enhances the diagnostic capabilities of low-cost cardiac intelligence systems, paving the way for scalable, affordable, and accurate daily cardiac healthcare using wearable technologies.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consciousness-ECG Transformer for Conscious State Estimation System with Real-Time Monitoring</title>
<link>https://arxiv.org/abs/2511.02853</link>
<guid>https://arxiv.org/abs/2511.02853</guid>
<content:encoded><![CDATA[
<div> Transformer, consciousness, ECG, state estimation, patient safety

Summary:
The study introduces a novel approach called the consciousness-ECG transformer for estimating conscious states using ECG signals, offering a non-invasive and reliable alternative to traditional EEG methods. By implementing a transformer model with decoupled query attention, the system effectively captures heart rate variability features to distinguish between conscious and unconscious states. The system was validated on datasets for sleep staging and anesthesia level monitoring during surgeries, outperforming baseline models with high accuracies. The results demonstrate the potential of ECG-based monitoring in dynamic clinical settings, enhancing patient safety and advancing the understanding of conscious states. The proposed system provides a practical and robust solution for real-time monitoring, with the highest area under curve values in the respective tasks. Overall, the study highlights the significant implications of ECG-based consciousness monitoring in medical settings. 

<br /><br />Summary: <div>
arXiv:2511.02853v1 Announce Type: cross 
Abstract: Conscious state estimation is important in various medical settings, including sleep staging and anesthesia management, to ensure patient safety and optimize health outcomes. Traditional methods predominantly utilize electroencephalography (EEG), which faces challenges such as high sensitivity to noise and the requirement for controlled environments. In this study, we propose the consciousness-ECG transformer that leverages electrocardiography (ECG) signals for non-invasive and reliable conscious state estimation. Our approach employs a transformer with decoupled query attention to effectively capture heart rate variability features that distinguish between conscious and unconscious states. We implemented the conscious state estimation system with real-time monitoring and validated our system on datasets involving sleep staging and anesthesia level monitoring during surgeries. Experimental results demonstrate that our model outperforms baseline models, achieving accuracies of 0.877 on sleep staging and 0.880 on anesthesia level monitoring. Moreover, our model achieves the highest area under curve values of 0.786 and 0.895 on sleep staging and anesthesia level monitoring, respectively. The proposed system offers a practical and robust alternative to EEG-based methods, particularly suited for dynamic clinical environments. Our results highlight the potential of ECG-based consciousness monitoring to enhance patient safety and advance our understanding of conscious states.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SELF-REDRAFT: Eliciting Intrinsic Exploration-Exploitation Balance in Test-Time Scaling for Code Generation</title>
<link>https://arxiv.org/abs/2511.02854</link>
<guid>https://arxiv.org/abs/2511.02854</guid>
<content:encoded><![CDATA[
<div> framework, Self-REDRAFT, test-time scaling, exploration-exploitation balancing, LLMs  
Summary:  
- The study focuses on test-time scaling without interpreter feedback for code generation scenarios.  
- Existing paradigms rely on either iterative refinement or stochastic exploration, but the balance between these dimensions needs further exploration.  
- The framework SELF-REDRAFT builds upon Self-Refine to encourage proposing new drafts for flawed solutions.  
- SELF-REDRAFT outperforms Self-Refine but still has room for improvement in generating feedback and discrimination.  
- Different LLMs show varying strategies for balancing exploration and exploitation.  
<br /><br />Summary: <div>
arXiv:2511.02854v1 Announce Type: cross 
Abstract: Test-time scaling without interpreter feedback is essential for real-world code generation scenarios where test cases are not readily available. While existing paradigms often rely on either greedy exploitation (i.e., iterative refinement) or stochastic exploration (i.e., relying on sample-based voting or reranking mechanisms), the balance between these two dimensions remains underexplored. To investigate the LLM's intrinsic ability to balance exploitation and exploration, we introduce SELF-REDRAFT, a framework built upon Self-Refine that encourages the model to propose new drafts for solutions that are fundamentally flawed. Our results show that SELF-REDRAFT consistently achieves better performance than Self-Refine when converged under the same maximum number of iterations. Still, we observe that significant room for improvement remains, largely due to two core aspects of current self-redraft capabilities: constrained capacity for generating instructive feedback and fragile discriminative judgment. We also find that balancing strategies vary notably across different LLMs, reflecting distinct, model-specific behaviors. Overall, our study establishes a baseline for intrinsic exploration-exploitation balancing in test-time scaling and identifies feedback and discrimination as key areas with potential for future advances.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Digitizing Spermatogenesis Lineage at Nanoscale Resolution In Tissue-Level Electron Microscopy</title>
<link>https://arxiv.org/abs/2511.02860</link>
<guid>https://arxiv.org/abs/2511.02860</guid>
<content:encoded><![CDATA[
<div> segmentation, organelles, tissue scales, membrane contact sites, germ cell differentiation

Summary:
DeepOrganelle, a new tool developed for nanoscale functional analysis, uses a lightweighted Mask2Former framework to segment and extract organelles in different cell types and perform statistical analysis. It enables visualization and quantification of organelle morphologies and interactions at tissue scales, facilitating cross-scale quantification of membrane contact sites dynamics in the seminiferous epithelial cycle. The tool reveals a waved pattern of Mito-ER contact at a specific stage preceding cell differentiation, aligning with the role of mitochondrial metabolic proteins in ATP supply for DNA repair. It also uncovers dynamic organelle restructuring in Sertoli cells during prophase I of germ cell differentiation, highlighting stage-specific organelle topography changes and blood-testis barrier reorganization. DeepOrganelle revolutionizes cell atlases by providing insights into intracellular organelle networks and interactions at tissue levels. 

<br /><br />Summary: <div>
arXiv:2511.02860v1 Announce Type: cross 
Abstract: Recent advances in 2D large-scale and 3D volume electron microscopy have stimulated the rapid development of nanoscale functional analysis at the tissue and organ levels. Digitizing the cell by mapping the intricate organellar networks into its physiological and pathological textures will revolutionarize the contents of cell atlases. To meet the requirements of characterizing intracellular organelles and their interactions within defined cellular cohorts at tissue level, we have developed DeepOrganelle. It adopts a lightweighted Mask2Former frameworks as a universal segmentor and is capable of segmenting and extracting organelles within different cell types, performing statistical quantitative analysis, as well as visualizing and quantifying the spatial distribution of organelle morphologies and interactions across different cell types at tissue scales. Using DeepOrganelle, we systemically perform cross-scale quantification of membrane contact sites(MCSs) dynamics across the progression of the seminiferous epithelial cycle, covering 12 distinct developmental stages and 24 statuses of germ cells. DeepOrganelle uncovers the spatiotemporal gradient of the germ cell differentiation atlas according to different types of organelles and their interactions. Noticeably, it discovers a waved pattern of mitochondria(Mito)-endoplasmic reticulum(ER) contact with a significant increase specifically at Stage X pachytene preceding the transition to diplotene, which aligns well with a newly reported experiment that mitochondrial metabolic proteins like PDHA2 are essential for this transition by maintaining ATP supply for double-strand break(DSB) repair. DeepOrganelle also observes a dynamic restructuring of the blood-testis barrier and stage-specific reorganization of organelle topography in Sertoli cells from preleptotene to leptotene phases of prophase I.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mathematical exploration and discovery at scale</title>
<link>https://arxiv.org/abs/2511.02864</link>
<guid>https://arxiv.org/abs/2511.02864</guid>
<content:encoded><![CDATA[
<div> evolutionary coding agent, AlphaEvolve, generative capabilities, automated evaluation, mathematical constructions<br />
Summary:<br />
AlphaEvolve is a novel evolutionary coding agent that utilizes large language models to autonomously explore and refine algorithmic solutions to complex mathematical problems. By considering a diverse range of mathematical challenges, AlphaEvolve can rediscover or improve upon known solutions and generalize results into formulas applicable to all input values. The combination of Deep Think and AlphaProof further enhances the system's capabilities by providing automated proof generation and additional mathematical insights. This innovative approach showcases the potential for AI systems to complement human intuition and drive new modes of collaboration in mathematical discovery. AlphaEvolve offers a powerful tool for exploring expansive search spaces and solving optimization problems at scale, with reduced requirements for preparation and computation time. <div>
arXiv:2511.02864v1 Announce Type: cross 
Abstract: AlphaEvolve is a generic evolutionary coding agent that combines the generative capabilities of LLMs with automated evaluation in an iterative evolutionary framework that proposes, tests, and refines algorithmic solutions to challenging scientific and practical problems. In this paper we showcase AlphaEvolve as a tool for autonomously discovering novel mathematical constructions and advancing our understanding of long-standing open problems.
  To demonstrate its breadth, we considered a list of 67 problems spanning mathematical analysis, combinatorics, geometry, and number theory. The system rediscovered the best known solutions in most of the cases and discovered improved solutions in several. In some instances, AlphaEvolve is also able to generalize results for a finite number of input values into a formula valid for all input values. Furthermore, we are able to combine this methodology with Deep Think and AlphaProof in a broader framework where the additional proof-assistants and reasoning systems provide automated proof generation and further mathematical insights.
  These results demonstrate that large language model-guided evolutionary search can autonomously discover mathematical constructions that complement human intuition, at times matching or even improving the best known results, highlighting the potential for significant new ways of interaction between mathematicians and AI systems. We present AlphaEvolve as a powerful new tool for mathematical discovery, capable of exploring vast search spaces to solve complex optimization problems at scale, often with significantly reduced requirements on preparation and computation time.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LM-Fix: Lightweight Bit-Flip Detection and Rapid Recovery Framework for Language Models</title>
<link>https://arxiv.org/abs/2511.02866</link>
<guid>https://arxiv.org/abs/2511.02866</guid>
<content:encoded><![CDATA[
<div> framework, faults, language models, detection, recovery
<br />
LM-Fix is a lightweight framework designed to detect and rapidly recover from faults in large language models (LLMs). Traditional integrity methods for LLMs tend to be heavy and slow, making them unsuitable for modern models. LM-Fix operates by running a quick test-vector pass and utilizing hash-guided checks to identify bit-flip faults, subsequently repairing them locally without needing a full reload. It boasts an impressive detection rate, capturing over 94% of single-bit flips at a Total Vector Length (TVL) of 200 and nearly 100% of multi-bit flips while incurring only a slight runtime overhead ranging from 1% to 7.7%. Moreover, its recovery process is exceptionally swift, clocking in at more than 100 times faster than reloading the entire model. These findings demonstrate LM-Fix's efficacy as a practical, low-overhead solution for maintaining the reliability of LLMs in real-world production environments.
<br /><br />Summary: <div>
arXiv:2511.02866v1 Announce Type: cross 
Abstract: This paper presents LM-Fix, a lightweight detection and rapid recovery framework for faults in large language models (LLMs). Existing integrity approaches are often heavy or slow for modern LLMs. LM-Fix runs a short test-vector pass and uses hash-guided checks to detect bit-flip faults, then repairs them locally without a full reload. Across multiple models, it detects over 94% of single-bit flips at TVL=200 and nearly 100% of multi-bit flips with approximately 1% to 7.7% runtime overhead; recovery is more than 100x faster than reloading. These results show a practical, low-overhead solution to keep LLMs reliable in production
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Proof-of-Spiking-Neurons(PoSN): Neuromorphic Consensus for Next-Generation Blockchains</title>
<link>https://arxiv.org/abs/2511.02868</link>
<guid>https://arxiv.org/abs/2511.02868</guid>
<content:encoded><![CDATA[
<div> Neuromorphic, blockchain, consensus, PoSN, spiking neurons <br />
<br />
Summary: 
The paper introduces Proof-of-Spiking-Neurons (PoSN), a novel consensus protocol that addresses the scalability, latency, and energy efficiency challenges faced by blockchain systems. Inspired by spiking neural networks, PoSN encodes transactions as spike trains, elects leaders through competitive firing dynamics, and finalizes blocks via neural synchronization. This approach enables parallel and event-driven consensus with minimal energy consumption. The proposed PoSN protocol outperforms traditional Proof-of-Work (PoW) and Proof-of-Stake (PoS) protocols in terms of energy efficiency, throughput, and convergence. The implementation of a hybrid system architecture on neuromorphic platforms, supported by simulation frameworks like Nengo and PyNN, demonstrates significant gains in these key metrics. The research establishes PoSN as a promising foundation for sustainable, adaptive blockchains suitable for applications in IoT, edge computing, and large-scale distributed systems. <br /> <div>
arXiv:2511.02868v1 Announce Type: cross 
Abstract: Blockchain systems face persistent challenges of scalability, latency, and energy inefficiency. Existing consensus protocols such as Proof-of-Work (PoW) and Proof-of-Stake (PoS) either consume excessive resources or risk centralization. This paper proposes \textit{Proof-of-Spiking-Neurons (PoSN)}, a neuromorphic consensus protocol inspired by spiking neural networks. PoSN encodes transactions as spike trains, elects leaders through competitive firing dynamics, and finalizes blocks via neural synchronization, enabling parallel and event-driven consensus with minimal energy overhead. A hybrid system architecture is implemented on neuromorphic platforms, supported by simulation frameworks such as Nengo and PyNN. Experimental results show significant gains in energy efficiency, throughput, and convergence compared to PoB and PoR. PoSN establishes a foundation for sustainable, adaptive blockchains suitable for IoT, edge, and large-scale distributed systems.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Analysis of AdvFusion: Adapter-based Multilingual Learning for Code Large Language Models</title>
<link>https://arxiv.org/abs/2511.02869</link>
<guid>https://arxiv.org/abs/2511.02869</guid>
<content:encoded><![CDATA[
<div> AdapterFusion, Parameter Efficient Fine-Tuning, Code Language Models, AdvFusion, Code Large Language Models

Summary:
AdvFusion, a novel approach for fine-tuning Code Language Models (Code-LMs), was compared to AdapterFusion and other methods for various programming tasks. The study focused on code generation, code translation, and commit message generation using Code Large Language Models (Code-LLMs). Results showed that AdvFusion outperformed AdapterFusion in code generation but not other PEFT methods. In commit message generation, AdapterFusion performed better than AdvFusion, while in code translation, AdapterFusion outperformed AdvFusion overall. Other PEFT methods showed varying performance across tasks, with some outperforming both AdvFusion and AdapterFusion. The study highlights the importance of considering different characteristics of Code-LLMs and tasks when selecting a fine-tuning approach for programming tasks. <br /><br />Summary: <div>
arXiv:2511.02869v1 Announce Type: cross 
Abstract: Programming languages can benefit from one another by utilizing a language model for software engineering tasks. Full fine-tuning and Parameter Efficient Fine-Tuning (PEFT) of Code Language Models (Code-LMs) has been explored for multilingual knowledge transfer. AdapterFusion is a PEFT architecture that aims to enhance task performance by leveraging information from multiple programming languages, but primarily focuses on the target programming language.
  In our previous work, we proposed AdvFusion, a novel PEFT-based approach that effectively learns from other programming languages before adapting to the target task. Though previous experiments showed that AdvFusion outperformed AdapterFusion and LoRA, it was applied on pre-trained Code-LMs and was limited to only two tasks, code summarization and method name prediction. In this study, we expanded our work and investigated AdvFusion on Code Large Language Models (Code-LLMs), considering three new tasks: code generation, code translation, and commit message generation. We observed that different Code-LLMs/tasks exhibit different characteristics. In code generation, AdvFusion outperformed AdapterFusion but not other PEFT methods (LoRA, Compacter, and TaskAdapter). In commit message generation, AdapterFusion performed better than AdvFusion, and contrary to code generation, we found that the other PEFT methods do not have better performance. In code translation, AdvFusion performed worse than AdapterFusion overall, with the performance gap marginally widening as the model size increases. However, consistent with code generation, other PEFT methods showed better performance.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FATE: A Formal Benchmark Series for Frontier Algebra of Multiple Difficulty Levels</title>
<link>https://arxiv.org/abs/2511.02872</link>
<guid>https://arxiv.org/abs/2511.02872</guid>
<content:encoded><![CDATA[
<div> benchmark, formal algebra, large language models, mathematical reasoning, theorem proving

Summary:
The article introduces FATE (Formal Algebra Theorem Evaluation), a new benchmark series in formal algebra aimed at advancing mathematical reasoning beyond contest-based benchmarks like the IMO. FATE consists of two components, FATE-H and FATE-X, each containing 100 problems in abstract and commutative algebra ranging in difficulty from undergraduate to PhD level. Evaluations of state-of-the-art LLM provers on FATE reveal a significant performance gap compared to contest math, with the best model achieving only 3% accuracy on FATE-H and 0% on FATE-X. The study highlights that models excel in natural-language reasoning but struggle with formalizing this reasoning accurately. Common errors in formalization are systematically classified, and a comparative study shows that specialized provers may be less effective than general-purpose models. FATE aims to serve as a challenging benchmark for advancing research-level formal mathematical reasoning. 

<br /><br />Summary: <div>
arXiv:2511.02872v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have demonstrated impressive capabilities in formal theorem proving, particularly on contest-based mathematical benchmarks like the IMO. However, these contests do not reflect the depth, breadth, and abstraction of modern mathematical research. To bridge this gap, we introduce FATE (Formal Algebra Theorem Evaluation), a new benchmark series in formal algebra designed to chart a course toward advanced mathematical reasoning. We present two new components, FATE-H and FATE-X, each with 100 problems in abstract and commutative algebra. The FATE series spans a difficulty spectrum from undergraduate exercises to problems exceeding PhD qualifying exams. Notably, FATE-X is the first formal benchmark to surpass both PhD-level exam difficulty and the coverage of the Mathlib library. Our evaluations of state-of-the-art LLM provers on this new benchmark reveal a stark performance gap compared to contest math: the best model achieves only 3% (pass@64) accuracy on FATE-H and 0% on FATE-X. Our two-stage evaluation reveals that models' natural-language reasoning is notably more accurate than their ability to formalize this reasoning. We systematically classify the common errors that arise during this formalization process. Furthermore, a comparative study shows that a specialized prover can exhibit less effective reflection than general-purpose models, reducing its accuracy at the natural-language stage. We believe FATE provides a robust and challenging benchmark that establishes essential checkpoints on the path toward research-level formal mathematical reasoning.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Academics and Generative AI: Empirical and Epistemic Indicators of Policy-Practice Voids</title>
<link>https://arxiv.org/abs/2511.02875</link>
<guid>https://arxiv.org/abs/2511.02875</guid>
<content:encoded><![CDATA[
<div> Keywords: generative AI, alignment indicators, indirect-elicitation instrument, policy-practice divergence, ontological stance

Summary: 
This study presents a ten-item indirect-elicitation instrument within a structured interpretive framework to identify discrepancies between institutional rules and the use of AI in practice. Three indicators of such discrepancies are identified: AI-integrated assessment capacity, sector-level necessity, and ontological stance. The AI-integrated assessment capacity indicator measures the willingness of individuals to fully allow AI in exams based on their AI skills, perceived teaching benefit, and detection confidence. Sector-level necessity looks at the proportion of high output control users who believe AI can challenge established disciplines. The ontological stance indicator examines respondents' views on AI as different from prior tools, their reported practice changes, and their beliefs on material versus immaterial views of AI, aligning procurement claims with evidence classes. <div>
arXiv:2511.02875v1 Announce Type: cross 
Abstract: As generative AI diffuses through academia, policy-practice divergence becomes consequential, creating demand for auditable indicators of alignment. This study prototypes a ten-item, indirect-elicitation instrument embedded in a structured interpretive framework to surface voids between institutional rules and practitioner AI use. The framework extracts empirical and epistemic signals from academics, yielding three filtered indicators of such voids: (1) AI-integrated assessment capacity (proxy) - within a three-signal screen (AI skill, perceived teaching benefit, detection confidence), the share who would fully allow AI in exams; (2) sector-level necessity (proxy) - among high output control users who still credit AI with high contribution, the proportion who judge AI capable of challenging established disciplines; and (3) ontological stance - among respondents who judge AI different in kind from prior tools, report practice change, and pass a metacognition gate, the split between material and immaterial views as an ontological map aligning procurement claims with evidence classes.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Novel Reservoir Computing Framework for Chaotic Time Series Prediction Using Time Delay Embedding and Random Fourier Features</title>
<link>https://arxiv.org/abs/2511.02877</link>
<guid>https://arxiv.org/abs/2511.02877</guid>
<content:encoded><![CDATA[
<div> Keywords: chaotic time series, reservoir computing, Random Fourier Feature, delay embedding, forecasting

Summary: 
The article introduces a novel reservoir computing framework that combines time-delay embedding with Random Fourier Feature mappings to create a dynamical reservoir without traditional recurrent architectures. This approach allows for the approximation of complex nonlinear interactions among delayed coordinates, enhancing the dynamical representation of the reservoir. By reducing the reliance on manual hyperparameters, such as spectral radius and leaking rate, this method offers a more principled and efficient way to model chaotic dynamics. The framework was tested on several chaotic systems and demonstrated superior prediction accuracy, robust attractor reconstructions, and accurate long-horizon forecasts. The combination of delay embedding and Random Fourier Feature-based reservoirs uncovers new dynamical structures by embedding the system in an enriched feature space, providing a computationally efficient and interpretable approach to forecasting chaotic time series. 

<br /><br />Summary: <div>
arXiv:2511.02877v1 Announce Type: cross 
Abstract: Forecasting chaotic time series requires models that can capture the intrinsic geometry of the underlying attractor while remaining computationally efficient. We introduce a novel reservoir computing (RC) framework that integrates time-delay embedding with Random Fourier Feature (RFF) mappings to construct a dynamical reservoir without the need for traditional recurrent architectures. Unlike standard RC, which relies on high-dimensional recurrent connectivity, the proposed RFF-RC explicitly approximates nonlinear kernel transformations that uncover latent dynamical relations in the reconstructed phase space. This hybrid formulation offers two key advantages: (i) it provides a principled way to approximate complex nonlinear interactions among delayed coordinates, thereby enriching the effective dynamical representation of the reservoir, and (ii) it reduces reliance on manual reservoir hyperparameters such as spectral radius and leaking rate. We evaluate the framework on canonical chaotic systems-the Mackey-Glass equation, the Lorenz system, and the Kuramoto-Sivashinsky equation. This novel formulation demonstrates that RFF-RC not only achieves superior prediction accuracy but also yields robust attractor reconstructions and long-horizon forecasts. These results show that the combination of delay embedding and RFF-based reservoirs reveals new dynamical structure by embedding the system in an enriched feature space, providing a computationally efficient and interpretable approach to modeling chaotic dynamics.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stochastic Deep Graph Clustering for Practical Group Formation</title>
<link>https://arxiv.org/abs/2511.02879</link>
<guid>https://arxiv.org/abs/2511.02879</guid>
<content:encoded><![CDATA[
<div> group recommender systems, dynamic group formation, DeepForm, GCN architecture, real-time group formation

Summary:
DeepForm is a framework for group recommender systems that focuses on dynamic group formation. It addresses the challenges of incorporating high-order user information, real-time group formation, and dynamic adjustment of the number of groups. DeepForm uses a lightweight GCN architecture to capture high-order structural signals and employs stochastic cluster learning for adaptive group reconfiguration without retraining. Additionally, contrastive learning is used to refine groups under dynamic conditions. Experimental results show that DeepForm outperforms various baselines in terms of group formation quality, efficiency, and recommendation accuracy on multiple datasets. <div>
arXiv:2511.02879v1 Announce Type: cross 
Abstract: While prior work on group recommender systems (GRSs) has primarily focused on improving recommendation accuracy, most approaches assume static or predefined groups, making them unsuitable for dynamic, real-world scenarios. We reframe group formation as a core challenge in GRSs and propose DeepForm (Stochastic Deep Graph Clustering for Practical Group Formation), a framework designed to meet three key operational requirements: (1) the incorporation of high-order user information, (2) real-time group formation, and (3) dynamic adjustment of the number of groups. DeepForm employs a lightweight GCN architecture that effectively captures high-order structural signals. Stochastic cluster learning enables adaptive group reconfiguration without retraining, while contrastive learning refines groups under dynamic conditions. Experiments on multiple datasets demonstrate that DeepForm achieves superior group formation quality, efficiency, and recommendation accuracy compared with various baselines.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NEF-NET+: Adapting Electrocardio panorama in the wild</title>
<link>https://arxiv.org/abs/2511.02880</link>
<guid>https://arxiv.org/abs/2511.02880</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-lead electrocardiogram, ECG, Electrocardio Panorama, NEF-NET+, Panobench

Summary:<br />
The study introduces NEF-NET+, an advanced framework for realistic panoramic ECG synthesis. NEF-NET+ overcomes limitations of conventional ECG systems by allowing for arbitrary-length signal synthesis from any desired view and compensating for variations in electrode placement. The model architecture enables direct view transformation and includes workflow steps for pretraining, device calibration tuning, and on-the-fly calibration. The framework was evaluated using the Panobench dataset, which captures the spatial variability of cardiac electrical activity. Results demonstrate substantial improvements over the previous Nef-Net model, with a 6 dB increase in PSNR in real-world scenarios. The code and Panobench dataset will be made available in a subsequent publication. <br />Summary: <div>
arXiv:2511.02880v1 Announce Type: cross 
Abstract: Conventional multi-lead electrocardiogram (ECG) systems capture cardiac signals from a fixed set of anatomical viewpoints defined by lead placement. However, certain cardiac conditions (e.g., Brugada syndrome) require additional, non-standard viewpoints to reveal diagnostically critical patterns that may be absent in standard leads. To systematically overcome this limitation, Nef-Net was recently introduced to reconstruct a continuous electrocardiac field, enabling virtual observation of ECG signals from arbitrary views (termed Electrocardio Panorama). Despite its promise, Nef-Net operates under idealized assumptions and faces in-the-wild challenges, such as long-duration ECG modeling, robustness to device-specific signal artifacts, and suboptimal lead placement calibration. This paper presents NEF-NET+, an enhanced framework for realistic panoramic ECG synthesis that supports arbitrary-length signal synthesis from any desired view, generalizes across ECG devices, and com- pensates for operator-induced deviations in electrode placement. These capabilities are enabled by a newly designed model architecture that performs direct view transformation, incorporating a workflow comprising offline pretraining, device calibration tuning steps as well as an on-the-fly calibration step for patient-specific adaptation. To rigorously evaluate panoramic ECG synthesis, we construct a new Electrocardio Panorama benchmark, called Panobench, comprising 5367 recordings with 48-view per subject, capturing the full spatial variability of cardiac electrical activity. Experimental results show that NEF-NET+ delivers substantial improvements over Nef-Net, yielding an increase of around 6 dB in PSNR in real-world setting. The code and Panobench will be released in a subsequent publication.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgentSLA : Towards a Service Level Agreement for AI Agents</title>
<link>https://arxiv.org/abs/2511.02885</link>
<guid>https://arxiv.org/abs/2511.02885</guid>
<content:encoded><![CDATA[
<div> AI Agents, Large Language Models, Quality of Service, Service Level Agreements, Quality Assurance <br />
Summary:<br />
- AI components are increasingly crucial in software systems, with AI Agents offering more autonomy than plain Large Language Models. 
- Transition from Model-as-a-Service to Agent-as-a-Service paradigm poses challenges in defining Quality of Service (QoS) and Service Level Agreements (SLAs) for AI agents.
- Lack of consensus on quality definition in AI components hampers Quality Assurance (QA) efforts.
- Proposal of a quality model for AI agents based on ISO/IEC 25010 standard.
- Introduction of a domain specific language to facilitate SLA definition for services provided by AI agents. <div>
arXiv:2511.02885v1 Announce Type: cross 
Abstract: AI components are increasingly becoming a key element of all types of software systems to enhance their functionality. These AI components are often implemented as AI Agents, offering more autonomy than a plain integration of Large Language Models (LLMs), moving from a Model-as-a-Service paradigm to an Agent-as-a-Service one, bringing new challenges to the development of smart software systems. Indeed, while support for the design, implementation, and deployment of those agents exist, the specification of Quality of Service (QoS) and definition of Service Level Agreements (SLAs) aspects for those agents, important to ensure the quality of the resulting systems, remains an open challenge. Part of this is due to the difficulty to clearly define quality in the context of AI components, resulting in a lack of consensus on how to best approach Quality Assurance (QA) for these types of systems. To address this challenge, this paper proposes both a quality model for AI agents based on the ISO/IEC 25010 standard, and a domain specific language to support the definition of SLAs for the services provided by these AI agents.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test-time Adaptation of Tiny Recursive Models</title>
<link>https://arxiv.org/abs/2511.02886</link>
<guid>https://arxiv.org/abs/2511.02886</guid>
<content:encoded><![CDATA[
<div> pre-training, fine-tuning, recursive neural network, ARC tasks, AGI evaluation<br />
Summary:<br />
The paper discusses a method to improve performance in the ARC Prize competition by pre-training a tiny recursive neural network on public ARC tasks and efficiently fine-tuning it within competition compute limits. The initial model, pre-trained on 1,280 tasks, achieved a 10% score on the public evaluation set after 48 hours of training. During the competition, the model was further trained for 12,500 gradient steps and reached a score of 6.67% on semi-private evaluation tasks. This post-training performance was achieved through full fine-tuning of the tiny model, not LoRA fine-tuning or fine-tuning of task embeddings alone. This innovative approach demonstrates the potential for significant performance improvements in AI tasks through strategic pre-training and fine-tuning techniques. <br /><br />Summary: <div>
arXiv:2511.02886v1 Announce Type: cross 
Abstract: Prior to the close of the 2025 ARC Prize competition, the leading open source approach - known as TRM, or Tiny Recursive Models - involved training a 7M parameter recursive neural network on augmented variants of ARC tasks. That approach scored approximately 7.8% on the public ARC AGI II evaluation set, but required a level of compute far in excess of what is allowed during the competition. This paper shows that, by starting from a tiny recursive model that has been pre-trained on public ARC tasks, one can efficiently fine-tune on competition tasks within the allowed compute limits. Specifically, a model was pre-trained on 1,280 public tasks for 700k+ optimizer steps over 48 hours on 4xH100 SXM GPUs to obtain a ~10% score on the public evaluation set. That model was then post-trained in just 12,500 gradient steps during the competition to reach a score of 6.67% on semi-private evaluation tasks. Notably, such post-training performance is achieved by full-fine tuning of the tiny model, not LoRA fine-tuning or fine-tuning of task embeddings alone.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicting Weekly Fishing Concentration Zones through Deep Learning Integration of Heterogeneous Environmental Spatial Datasets</title>
<link>https://arxiv.org/abs/2511.02887</link>
<guid>https://arxiv.org/abs/2511.02887</guid>
<content:encoded><![CDATA[
<div> AI-assisted, Potential Fishing Zones, North Indian Ocean, oceanographic parameters, sustainable fishing practices  
Summary:  
An AI-assisted framework has been developed to predict Potential Fishing Zones (PFZs) in the North Indian Ocean using sea surface temperature and chlorophyll concentration data. The framework aims to improve the accuracy of PFZ identification and provide region-specific insights for sustainable fishing practices. Preliminary results suggest that this approach can help fishermen by reducing search time, minimizing fuel consumption, and promoting efficient resource utilization. This innovative tool could potentially enhance the livelihoods of coastal communities in the Arabian Sea and the Bay of Bengal by assisting fishermen in locating productive fishing grounds effectively. <div>
arXiv:2511.02887v1 Announce Type: cross 
Abstract: The North Indian Ocean, including the Arabian Sea and the Bay of Bengal, represents a vital source of livelihood for coastal communities, yet fishermen often face uncertainty in locating productive fishing grounds. To address this challenge, we present an AI-assisted framework for predicting Potential Fishing Zones (PFZs) using oceanographic parameters such as sea surface temperature and chlorophyll concentration. The approach is designed to enhance the accuracy of PFZ identification and provide region-specific insights for sustainable fishing practices. Preliminary results indicate that the framework can support fishermen by reducing search time, lowering fuel consumption, and promoting efficient resource utilization.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NABench: Large-Scale Benchmarks of Nucleotide Foundation Models for Fitness Prediction</title>
<link>https://arxiv.org/abs/2511.02888</link>
<guid>https://arxiv.org/abs/2511.02888</guid>
<content:encoded><![CDATA[
<div> Benchmarking, Nucleic acid, Fitness prediction, NABench, Foundation models <br />
Summary: 
The study introduces NABench, a comprehensive benchmark for nucleic acid fitness prediction. It includes 162 high-throughput assays and 2.6 million mutated sequences across DNA and RNA families, providing standardized data splits and rich metadata. NABench surpasses previous benchmarks in scale, diversity, and data quality. The evaluation assesses 29 foundation models across various prediction settings, highlighting performance variations based on tasks and nucleic-acid types. The results outline strengths and weaknesses of different modeling approaches, establishing reliable baselines. NABench is released to enhance nucleic acid modeling, benefiting applications in RNA/DNA design, synthetic biology, and biochemistry. The code for NABench is available at https://github.com/mrzzmrzz/NABench.  <br /><br /> <div>
arXiv:2511.02888v1 Announce Type: cross 
Abstract: Nucleotide sequence variation can induce significant shifts in functional fitness. Recent nucleotide foundation models promise to predict such fitness effects directly from sequence, yet heterogeneous datasets and inconsistent preprocessing make it difficult to compare methods fairly across DNA and RNA families. Here we introduce NABench, a large-scale, systematic benchmark for nucleic acid fitness prediction. NABench aggregates 162 high-throughput assays and curates 2.6 million mutated sequences spanning diverse DNA and RNA families, with standardized splits and rich metadata. We show that NABench surpasses prior nucleotide fitness benchmarks in scale, diversity, and data quality. Under a unified evaluation suite, we rigorously assess 29 representative foundation models across zero-shot, few-shot prediction, transfer learning, and supervised settings. The results quantify performance heterogeneity across tasks and nucleic-acid types, demonstrating clear strengths and failure modes for different modeling choices and establishing strong, reproducible baselines. We release NABench to advance nucleic acid modeling, supporting downstream applications in RNA/DNA design, synthetic biology, and biochemistry. Our code is available at https://github.com/mrzzmrzz/NABench.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Criminology of Machines</title>
<link>https://arxiv.org/abs/2511.02895</link>
<guid>https://arxiv.org/abs/2511.02895</guid>
<content:encoded><![CDATA[
<div> AI; criminology; multi-agent systems; AI safety; governance
<br />
Summary: 
The article discusses the increasing presence of autonomous AI agents in society and the need for criminology to address the implications of this transition for crime and social control. It argues that AI agents should be recognized as entities with agency and examines the risks associated with multi-agent AI systems. The author proposes a dual taxonomy to characterize how interactions among AI agents may lead to deviant or criminal outcomes and raises questions about the unique challenges posed by AI-driven criminal behaviors. The article emphasizes the importance of criminologists engaging with the study of crime in the context of AI systems and advocates for a more active role in debates on AI safety and governance. <div>
arXiv:2511.02895v1 Announce Type: cross 
Abstract: While the possibility of reaching human-like Artificial Intelligence (AI) remains controversial, the likelihood that the future will be characterized by a society with a growing presence of autonomous machines is high. Autonomous AI agents are already deployed and active across several industries and digital environments and alongside human-human and human-machine interactions, machine-machine interactions are poised to become increasingly prevalent. Given these developments, I argue that criminology must begin to address the implications of this transition for crime and social control. Drawing on Actor-Network Theory and Woolgar's decades-old call for a sociology of machines -- frameworks that acquire renewed relevance with the rise of generative AI agents -- I contend that criminologists should move beyond conceiving AI solely as a tool. Instead, AI agents should be recognized as entities with agency encompassing computational, social, and legal dimensions. Building on the literature on AI safety, I thus examine the risks associated with the rise of multi-agent AI systems, proposing a dual taxonomy to characterize the channels through which interactions among AI agents may generate deviant, unlawful, or criminal outcomes. I then advance and discuss four key questions that warrant theoretical and empirical attention: (1) Can we assume that machines will simply mimic humans? (2) Will crime theories developed for humans suffice to explain deviant or criminal behaviors emerging from interactions between autonomous AI agents? (3) What types of criminal behaviors will be affected first? (4) How might this unprecedented societal shift impact policing? These questions underscore the urgent need for criminologists to theoretically and empirically engage with the implications of multi-agent AI systems for the study of crime and play a more active role in debates on AI safety and governance.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Performance Evaluation of Bitstring Representations in a Linear Genetic Programming Framework</title>
<link>https://arxiv.org/abs/2511.02897</link>
<guid>https://arxiv.org/abs/2511.02897</guid>
<content:encoded><![CDATA[
<div> std::bitset, boost::dynamic_bitset, custom direct implementation, performance benchmark, Linear Genetic Programming <br />
<br />
Summary: 
This study compares three different bitstring implementations, namely std::bitset, boost::dynamic_bitset, and a custom direct implementation, in the context of a Linear Genetic Programming system. The performance of these implementations was benchmarked on three platforms - macOS, Linux, and Windows MSYS2 - to understand platform-specific variations. The results showed that the custom direct implementation exhibited the fastest performance on Linux and Windows, while std::bitset performed best on macOS. Despite being consistently slower, boost::dynamic_bitset still remained a viable and flexible option. These findings underscore the impact of compiler optimizations and system architecture on performance, providing valuable insights for selecting the most suitable method based on the platform and application requirements. <div>
arXiv:2511.02897v1 Announce Type: cross 
Abstract: Different bitstring representations can yield varying computational performance. This work compares three bitstring implementations in C++: std::bitset, boost::dynamic_bitset, and a custom direct implementation. Their performance is benchmarked in the context of concatenation within a Linear Genetic Programming system. Benchmarks were conducted on three platforms (macOS, Linux, and Windows MSYS2) to assess platform specific performance variations. The results show that the custom direct implementation delivers the fastest performance on Linux and Windows, while std::bitset performs best on macOS. Although consistently slower, boost::dynamic_bitset remains a viable and flexible option. These findings highlight the influence of compiler optimisations and system architecture on performance, providing practical guidance for selecting the optimal method based on platform and application requirements.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Hints</title>
<link>https://arxiv.org/abs/2511.02933</link>
<guid>https://arxiv.org/abs/2511.02933</guid>
<content:encoded><![CDATA[
<div> generative hints, data augmentation, vision, invariance, semi-supervised learning
<br />
Summary: 
Generative hints, a training methodology in vision, enforce known invariances in the entire input space by leveraging a generative model to generate virtual examples. These virtual examples are used to enforce functional properties known as hints, resulting in improved performance compared to standard data augmentation methods. In a semi-supervised manner, the model is trained on both classification and hint objectives using the unlabeled virtual examples. Across datasets, architectures, and loss functions, generative hints consistently outperform data augmentation when learning the same property. The approach achieves up to a 1.78% top-1 accuracy improvement on fine-grained visual classification benchmarks and an average performance boost of 1.286% on the CheXpert X-ray dataset. <div>
arXiv:2511.02933v1 Announce Type: cross 
Abstract: Data augmentation is widely used in vision to introduce variation and mitigate overfitting, through enabling models to learn invariant properties, such as spatial invariance. However, these properties are not fully captured by data augmentation alone, since it attempts to learn the property on transformations of the training data only. We propose generative hints, a training methodology that directly enforces known invariances in the entire input space. Our approach leverages a generative model trained on the training set to approximate the input distribution and generate unlabeled images, which we refer to as virtual examples. These virtual examples are used to enforce functional properties known as hints. In generative hints, although the training dataset is fully labeled, the model is trained in a semi-supervised manner on both the classification and hint objectives, using the unlabeled virtual examples to guide the model in learning the desired hint. Across datasets, architectures, and loss functions, generative hints consistently outperform standard data augmentation when learning the same property. On popular fine-grained visual classification benchmarks, we achieved up to 1.78% top-1 accuracy improvement (0.63% on average) over fine-tuned models with data augmentation and an average performance boost of 1.286% on the CheXpert X-ray dataset.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-shot data citation function classification using transformer-based large language models (LLMs)</title>
<link>https://arxiv.org/abs/2511.02936</link>
<guid>https://arxiv.org/abs/2511.02936</guid>
<content:encoded><![CDATA[
<div> pretrained large language models, data use cases, scientific literature, genomic datasets, evaluation framework <br />
Summary: 
Efforts to identify data associations in scientific literature have increased, with a focus on understanding how specific datasets are used. Utilizing pretrained transformer-based large language models (LLMs) like Llama 3.1-405B can automate the description of data use cases in publications, eliminating the need for manual labeling and training datasets for traditional machine-learning systems. In this study, Llama 3.1-405B was used to label data use cases in publications referencing genomic datasets, achieving an F1 score of .674 on a data citation classification task with no predefined categories. However, challenges such as data availability, prompt overfitting, computing resources, and cost hinder the scalability and thorough evaluation of these methods. Addressing these barriers is crucial for advancing automated data use case identification in scientific literature. <br /> <div>
arXiv:2511.02936v1 Announce Type: cross 
Abstract: Efforts have increased in recent years to identify associations between specific datasets and the scientific literature that incorporates them. Knowing that a given publication cites a given dataset, the next logical step is to explore how or why that data was used. Advances in recent years with pretrained, transformer-based large language models (LLMs) offer potential means for scaling the description of data use cases in the published literature. This avoids expensive manual labeling and the development of training datasets for classical machine-learning (ML) systems. In this work we apply an open-source LLM, Llama 3.1-405B, to generate structured data use case labels for publications known to incorporate specific genomic datasets. We also introduce a novel evaluation framework for determining the efficacy of our methods. Our results demonstrate that the stock model can achieve an F1 score of .674 on a zero-shot data citation classification task with no previously defined categories. While promising, our results are qualified by barriers related to data availability, prompt overfitting, computational infrastructure, and the expense required to conduct responsible performance evaluation.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Narrow to Wide: Autoencoding Transformers for Ultrasound Bandwidth Recovery</title>
<link>https://arxiv.org/abs/2511.02938</link>
<guid>https://arxiv.org/abs/2511.02938</guid>
<content:encoded><![CDATA[
<div> band-limited, broadband, ultrasound, auto-encoder, simulation<br />
Summary:<br />
The study introduces a method to enhance the performance of low-cost narrow-band ultrasound probes by utilizing a data-driven mapping approach. By training a modified Tiny Vision Transform auto-encoder on simulation data with a curriculum-weighted loss, the network effectively converts band-limited spectrograms to broadband, improving image quality significantly. The results on heterogeneous speckle-cyst phantoms demonstrate a 90% reduction in image-domain MSE, a 6.7 dB increase in PSNR, and a SSIM of 0.965 compared to the narrow-band input. Moreover, the network exhibits excellent out-of-distribution generalization by sharpening point-target rows in unseen resolution phantoms without compromising frame rate or phase information. These findings suggest that a software upgrade alone can upgrade the performance of narrow-band probes, potentially expanding access to high-resolution ultrasound in resource-limited environments. <br /><br />Summary: <div>
arXiv:2511.02938v1 Announce Type: cross 
Abstract: Conventional pulse-echo ultrasound suffers when low-cost probes deliver only narrow fractional bandwidths, elongating pulses and erasing high-frequency detail. We address this limitation by learning a data-driven mapping from band-limited to broadband spectrogram of radio-frequency (RF) lines. To this end, a variation of Tiny Vision Transform (ViT) auto-encoder is trained on simulation data using a curriculum-weighted loss. On heterogeneous speckle-cyst phantoms, the network reduces image-domain MSE by 90 percent, boosts PSNR by 6.7 dB, and raises SSIM to 0.965 compared with the narrow-band input. It also sharpens point-target rows in a completely unseen resolution phantom, demonstrating strong out-of-distribution generalisation without sacrificing frame rate or phase information. These results indicate that a purely software upgrade can endow installed narrow-band probes with broadband-like performance, potentially widening access to high-resolution ultrasound in resource-constrained settings.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Power Constrained Nonstationary Bandits with Habituation and Recovery Dynamics</title>
<link>https://arxiv.org/abs/2511.02944</link>
<guid>https://arxiv.org/abs/2511.02944</guid>
<content:encoded><![CDATA[
<div> Thompson Sampling, ROGUE framework, micro-randomized trials, personalization, statistical validity
Summary: 
The study introduces a Thompson Sampling algorithm, ROGUE-TS, designed for the ROGUE bandit framework to optimize decision-making in settings where rewards evolve over time. A probability clipping procedure is also proposed to balance personalization and population-level learning in micro-randomized trials. The algorithm is shown to achieve lower regret than existing methods and maintain statistical power without significantly increasing regret. This allows for reliable detection of treatment effects while accounting for individual behavioral dynamics. The framework provides practical guidance for researchers on balancing personalization with statistical validity in the design of micro-randomized trials. <div>
arXiv:2511.02944v1 Announce Type: cross 
Abstract: A common challenge for decision makers is selecting actions whose rewards are unknown and evolve over time based on prior policies. For instance, repeated use may reduce an action's effectiveness (habituation), while inactivity may restore it (recovery). These nonstationarities are captured by the Reducing or Gaining Unknown Efficacy (ROGUE) bandit framework, which models real-world settings such as behavioral health interventions. While existing algorithms can compute sublinear regret policies to optimize these settings, they may not provide sufficient exploration due to overemphasis on exploitation, limiting the ability to estimate population-level effects. This is a challenge of particular interest in micro-randomized trials (MRTs) that aid researchers in developing just-in-time adaptive interventions that have population-level effects while still providing personalized recommendations to individuals. In this paper, we first develop ROGUE-TS, a Thompson Sampling algorithm tailored to the ROGUE framework, and provide theoretical guarantees of sublinear regret. We then introduce a probability clipping procedure to balance personalization and population-level learning, with quantified trade-off that balances regret and minimum exploration probability. Validation on two MRT datasets concerning physical activity promotion and bipolar disorder treatment shows that our methods both achieve lower regret than existing approaches and maintain high statistical power through the clipping procedure without significantly increasing regret. This enables reliable detection of treatment effects while accounting for individual behavioral dynamics. For researchers designing MRTs, our framework offers practical guidance on balancing personalization with statistical validity.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EvtSlowTV - A Large and Diverse Dataset for Event-Based Depth Estimation</title>
<link>https://arxiv.org/abs/2511.02953</link>
<guid>https://arxiv.org/abs/2511.02953</guid>
<content:encoded><![CDATA[
<div> Keywords: event cameras, depth estimation, self-supervised learning, large-scale dataset, HDR<br />
Summary:<br />
This paper introduces EvtSlowTV, a large-scale event camera dataset sourced from YouTube footage, containing over 13B events across various scenarios like hiking, flying, driving, and underwater exploration. The dataset aims to address the lack of annotated data for event-based depth estimation, enabling robust depth estimation in challenging environments. EvtSlowTV is significantly larger than existing datasets, offering a naturalistic setting for event-based depth learning. The study showcases the dataset's potential for self-supervised learning, leveraging the high dynamic range of raw event streams. By training with EvtSlowTV, models exhibit improved generalization in complex scenes and motions, without the need for frame-based annotations. This approach preserves the asynchronous nature of event data, demonstrating the effectiveness of using large-scale datasets in event camera research. <br /><br />Summary: <div>
arXiv:2511.02953v1 Announce Type: cross 
Abstract: Event cameras, with their high dynamic range (HDR) and low latency, offer a promising alternative for robust depth estimation in challenging environments. However, many event-based depth estimation approaches are constrained by small-scale annotated datasets, limiting their generalizability to real-world scenarios. To bridge this gap, we introduce EvtSlowTV, a large-scale event camera dataset curated from publicly available YouTube footage, which contains more than 13B events across various environmental conditions and motions, including seasonal hiking, flying, scenic driving, and underwater exploration. EvtSlowTV is an order of magnitude larger than existing event datasets, providing an unconstrained, naturalistic setting for event-based depth learning. This work shows the suitability of EvtSlowTV for a self-supervised learning framework to capitalise on the HDR potential of raw event streams. We further demonstrate that training with EvtSlowTV enhances the model's ability to generalise to complex scenes and motions. Our approach removes the need for frame-based annotations and preserves the asynchronous nature of event data.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Value of Information-Enhanced Exploration in Bootstrapped DQN</title>
<link>https://arxiv.org/abs/2511.02969</link>
<guid>https://arxiv.org/abs/2511.02969</guid>
<content:encoded><![CDATA[
<div> Exploration, Deep Reinforcement Learning, Value of Information, Bootstrapped DQN, Sparse Rewards <br />
<br />
Summary: 
Efficient exploration in deep reinforcement learning poses a challenge in environments with high-dimensional states and sparse rewards. This paper introduces two novel algorithms that integrate the concept of expected value of information (EVOI) into Bootstrapped DQN to enhance deep exploration. By utilizing value of information estimates to measure discrepancies among network heads, the algorithms guide exploration towards promising areas. Evaluation in Atari games shows improved performance and utilization of uncertainty without additional hyperparameters. The methods successfully balance exploration and exploitation, demonstrating the potential of incorporating EVOI in reinforcement learning algorithms. <div>
arXiv:2511.02969v1 Announce Type: cross 
Abstract: Efficient exploration in deep reinforcement learning remains a fundamental challenge, especially in environments characterized by high-dimensional states and sparse rewards. Traditional exploration strategies that rely on random local policy noise, such as $\epsilon$-greedy and Boltzmann exploration methods, often struggle to efficiently balance exploration and exploitation. In this paper, we integrate the notion of (expected) value of information (EVOI) within the well-known Bootstrapped DQN algorithmic framework, to enhance the algorithm's deep exploration ability. Specifically, we develop two novel algorithms that incorporate the expected gain from learning the value of information into Bootstrapped DQN. Our methods use value of information estimates to measure the discrepancies of opinions among distinct network heads, and drive exploration towards areas with the most potential. We evaluate our algorithms with respect to performance and their ability to exploit inherent uncertainty arising from random network initialization. Our experiments in complex, sparse-reward Atari games demonstrate increased performance, all the while making better use of uncertainty, and, importantly, without introducing extra hyperparameters.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Systematizing LLM Persona Design: A Four-Quadrant Technical Taxonomy for AI Companion Applications</title>
<link>https://arxiv.org/abs/2511.02979</link>
<guid>https://arxiv.org/abs/2511.02979</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM-based personas, AI companionship, Four-Quadrant Technical Taxonomy, Emotional consistency, Embodied Intelligence

Summary:
The article introduces a Four-Quadrant Technical Taxonomy for AI companion applications to address the fragmentation in the field. The taxonomy categorizes applications into Virtual Companionship (virtual idols, romantic companions), Functional Virtual Assistants (AI in work, gaming, mental health), and Embodied Intelligence (home robots, vertical-domain assistants). Each quadrant highlights specific challenges and technologies, such as emotional consistency in virtual companionship and data privacy in embodied intelligence. The framework provides a systematic map for developers and policymakers to navigate the complex persona design space and address unique risks inherent in different application scenarios. <div>
arXiv:2511.02979v1 Announce Type: cross 
Abstract: The design and application of LLM-based personas in AI companionship is a rapidly expanding but fragmented field, spanning from virtual emotional compan- ions and game NPCs to embodied functional robots. This diversity in objectives, modality, and technical stacks creates an urgent need for a unified framework. To address this gap, this paper systematizes the field by proposing a Four-Quadrant Technical Taxonomy for AI companion applications. The framework is structured along two critical axes: Virtual vs. Embodied and Emotional Companionship vs. Functional Augmentation. Quadrant I (Virtual Companionship) explores virtual idols, romantic companions, and story characters, introducing a four-layer technical framework to analyze their challenges in maintaining long-term emotional consistency. Quadrant II (Functional Virtual Assistants) analyzes AI applica- tions in work, gaming, and mental health, highlighting the shift from "feeling" to "thinking and acting" and pinpointing key technologies like enterprise RAG and on-device inference. Quadrants III & IV (Embodied Intelligence) shift from the virtual to the physical world, analyzing home robots and vertical-domain assistants, revealing core challenges in symbol grounding, data privacy, and ethical liability. This taxonomy provides not only a systematic map for researchers and developers to navigate the complex persona design space but also a basis for policymakers to identify and address the unique risks inherent in different application scenarios.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SLIP: Structural-aware Language-Image Pretraining for Vision-Language Alignment</title>
<link>https://arxiv.org/abs/2511.03019</link>
<guid>https://arxiv.org/abs/2511.03019</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Pretraining, Structure-aware Language-Image Pretraining, structured graph, cross-modal retrieval, relational supervision<br />
Summary:<br />
- The study introduces Structure-aware Language-Image Pretraining (SLIP) to improve cross-modal tasks by incorporating structural relationships between entities in a graph. 
- SLIP integrates a structural contrastive loss and models relationships between neighboring entities to align modalities better. 
- A large-scale Amazon Product Co-purchase Multimodal Graph Dataset is used to provide structured cross-modality supervision at scale. 
- Experiment results demonstrate that SLIP outperforms CLIP in cross-modal retrieval and classification tasks in zero-shot and few-shot settings. 
- The study emphasizes the significance of relational supervision for enhancing cross-modal alignment and achieving better performance on various downstream tasks. <br /> 
Summary: <div>
arXiv:2511.03019v1 Announce Type: cross 
Abstract: Vision-Language Pretraining (VLP) has achieved remarkable success across various downstream tasks, but such gains are largely driven by scaling up on training data. Yet, literature methods treat image-text pairs as isolated training examples; this neglects the rich relational structure naturally present in many domains, such as e-commerce product co-purchase graphs and social recommendation networks. Inspired by neuroscientific evidence that human encodes knowledge as relationship cognitive maps, we introduce Structure-aware Language-Image Pretraining (SLIP). SLIP integrates a structural contrastive loss to align modalities while also modeling relationships between neighboring entities in a structured graph. To support this paradigm, we construct a large-scale Amazon Product Co-purchase Multimodal Graph Dataset, enabling structured cross-modality supervision at scale. Experiment results show that SLIP consistently outperforms CLIP on cross-modal retrieval and classification tasks in both zero-shot and few-shot settings, showing the value of relational supervision for cross-modal alignment.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive-Sensorless Monitoring of Shipping Containers</title>
<link>https://arxiv.org/abs/2511.03022</link>
<guid>https://arxiv.org/abs/2511.03022</guid>
<content:encoded><![CDATA[
<div> sensorless models, adaptive-sensorless, telemetry data, systematic biases, cargo monitoring

Summary:
The article introduces the concept of adaptive-sensorless models for monitoring the internal temperature and humidity of shipping containers using machine learning. These models incorporate live telemetry data to correct for systematic biases in predictions made by sensorless models. By training and evaluating on a large dataset of container sensor readings, adaptive-sensorless models consistently outperform baseline sensorless models, achieving lower mean absolute errors and root mean-squared errors for temperature and relative humidity predictions. The improved accuracy provided by adaptive-sensorless models enables more precise cargo monitoring, early detection of risks, and reduces the reliance on full connectivity in global shipping. <div>
arXiv:2511.03022v1 Announce Type: cross 
Abstract: Monitoring the internal temperature and humidity of shipping containers is essential to preventing quality degradation during cargo transportation. Sensorless monitoring -- machine learning models that predict the internal conditions of the containers using exogenous factors -- shows promise as an alternative to monitoring using sensors. However, it does not incorporate telemetry information and correct for systematic errors, causing the predictions to differ significantly from the live data and confusing the users. In this paper, we introduce the residual correction method, a general framework for correcting for systematic biases in sensorless models after observing live telemetry data. We call this class of models ``adaptive-sensorless'' monitoring. We train and evaluate adaptive-sensorless models on the 3.48 million data points -- the largest dataset of container sensor readings ever used in academic research -- and show that they produce consistent improvements over the baseline sensorless models. When evaluated on the holdout set of the simulated data, they achieve average mean absolute errors (MAEs) of 2.24 $\sim$ 2.31$^\circ$C (vs 2.43$^\circ$C by sensorless) for temperature and 5.72 $\sim$ 7.09% for relative humidity (vs 7.99% by sensorless) and average root mean-squared errors (RMSEs) of 3.19 $\sim$ 3.26$^\circ$C for temperature (vs 3.38$^\circ$C by sensorless) and 7.70 $\sim$ 9.12% for relative humidity (vs 10.0% by sensorless). Adaptive-sensorless models enable more accurate cargo monitoring, early risk detection, and less dependence on full connectivity in global shipping.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reading Between the Lines: The One-Sided Conversation Problem</title>
<link>https://arxiv.org/abs/2511.03056</link>
<guid>https://arxiv.org/abs/2511.03056</guid>
<content:encoded><![CDATA[
arXiv:2511.03056v1 Announce Type: cross 
Abstract: Conversational AI is constrained in many real-world settings where only one side of a dialogue can be recorded, such as telemedicine, call centers, and smart glasses. We formalize this as the one-sided conversation problem (1SC): inferring and learning from one side of a conversation. We study two tasks: (1) reconstructing the missing speaker's turns for real-time use cases, and (2) generating summaries from one-sided transcripts. Evaluating prompting and finetuned models on MultiWOZ, DailyDialog, and Candor with both human A/B testing and LLM-as-a-judge metrics, we find that access to one future turn and information about utterance length improves reconstruction, placeholder prompting helps to mitigate hallucination, and while large models generate promising reconstructions with prompting, smaller models require finetuning. Further, high-quality summaries can be generated without reconstructing missing turns. We present 1SC as a novel challenge and report promising results that mark a step toward privacy-aware conversational AI.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse, self-organizing ensembles of local kernels detect rare statistical anomalies</title>
<link>https://arxiv.org/abs/2511.03095</link>
<guid>https://arxiv.org/abs/2511.03095</guid>
<content:encoded><![CDATA[
arXiv:2511.03095v1 Announce Type: cross 
Abstract: Modern artificial intelligence has revolutionized our ability to extract rich and versatile data representations across scientific disciplines. Yet, the statistical properties of these representations remain poorly controlled, causing misspecified anomaly detection (AD) methods to falter. Weak or rare signals can remain hidden within the apparent regularity of normal data, creating a gap in our ability to detect and interpret anomalies. We examine this gap and identify a set of structural desiderata for detection methods operating under minimal prior information: sparsity, to enforce parsimony; locality, to preserve geometric sensitivity; and competition, to promote efficient allocation of model capacity. These principles define a class of self-organizing local kernels that adaptively partition the representation space around regions of statistical imbalance. As an instantiation of these principles, we introduce SparKer, a sparse ensemble of Gaussian kernels trained within a semi-supervised Neyman--Pearson framework to locally model the likelihood ratio between a sample that may contain anomalies and a nominal, anomaly-free reference. We provide theoretical insights into the mechanisms that drive detection and self-organization in the proposed model, and demonstrate the effectiveness of this approach on realistic high-dimensional problems of scientific discovery, open-world novelty detection, intrusion detection, and generative-model validation. Our applications span both the natural- and computer-science domains. We demonstrate that ensembles containing only a handful of kernels can identify statistically significant anomalous locations within representation spaces of thousands of dimensions, underscoring both the interpretability, efficiency and scalability of the proposed approach.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Multi-Agent Environment Co-Design with Diffusion Models</title>
<link>https://arxiv.org/abs/2511.03100</link>
<guid>https://arxiv.org/abs/2511.03100</guid>
<content:encoded><![CDATA[
arXiv:2511.03100v1 Announce Type: cross 
Abstract: The agent-environment co-design paradigm jointly optimises agent policies and environment configurations in search of improved system performance. With application domains ranging from warehouse logistics to windfarm management, co-design promises to fundamentally change how we deploy multi-agent systems. However, current co-design methods struggle to scale. They collapse under high-dimensional environment design spaces and suffer from sample inefficiency when addressing moving targets inherent to joint optimisation. We address these challenges by developing Diffusion Co-Design (DiCoDe), a scalable and sample-efficient co-design framework pushing co-design towards practically relevant settings. DiCoDe incorporates two core innovations. First, we introduce Projected Universal Guidance (PUG), a sampling technique that enables DiCoDe to explore a distribution of reward-maximising environments while satisfying hard constraints such as spatial separation between obstacles. Second, we devise a critic distillation mechanism to share knowledge from the reinforcement learning critic, ensuring that the guided diffusion model adapts to evolving agent policies using a dense and up-to-date learning signal. Together, these improvements lead to superior environment-policy pairs when validated on challenging multi-agent environment co-design benchmarks including warehouse automation, multi-agent pathfinding and wind farm optimisation. Our method consistently exceeds the state-of-the-art, achieving, for example, 39% higher rewards in the warehouse setting with 66% fewer simulation samples. This sets a new standard in agent-environment co-design, and is a stepping stone towards reaping the rewards of co-design in real world domains.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CARMA: Comprehensive Automatically-annotated Reddit Mental Health Dataset for Arabic</title>
<link>https://arxiv.org/abs/2511.03102</link>
<guid>https://arxiv.org/abs/2511.03102</guid>
<content:encoded><![CDATA[
arXiv:2511.03102v1 Announce Type: cross 
Abstract: Mental health disorders affect millions worldwide, yet early detection remains a major challenge, particularly for Arabic-speaking populations where resources are limited and mental health discourse is often discouraged due to cultural stigma. While substantial research has focused on English-language mental health detection, Arabic remains significantly underexplored, partly due to the scarcity of annotated datasets. We present CARMA, the first automatically annotated large-scale dataset of Arabic Reddit posts. The dataset encompasses six mental health conditions, such as Anxiety, Autism, and Depression, and a control group. CARMA surpasses existing resources in both scale and diversity. We conduct qualitative and quantitative analyses of lexical and semantic differences between users, providing insights into the linguistic markers of specific mental health conditions. To demonstrate the dataset's potential for further mental health analysis, we perform classification experiments using a range of models, from shallow classifiers to large language models. Our results highlight the promise of advancing mental health detection in underrepresented languages such as Arabic.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Detection of Software Aging under Workload Shift</title>
<link>https://arxiv.org/abs/2511.03103</link>
<guid>https://arxiv.org/abs/2511.03103</guid>
<content:encoded><![CDATA[
arXiv:2511.03103v1 Announce Type: cross 
Abstract: Software aging is a phenomenon that affects long-running systems, leading to progressive performance degradation and increasing the risk of failures. To mitigate this problem, this work proposes an adaptive approach based on machine learning for software aging detection in environments subject to dynamic workload conditions. We evaluate and compare a static model with adaptive models that incorporate adaptive detectors, specifically the Drift Detection Method (DDM) and Adaptive Windowing (ADWIN), originally developed for concept drift scenarios and applied in this work to handle workload shifts. Experiments with simulated sudden, gradual, and recurring workload transitions show that static models suffer a notable performance drop when applied to unseen workload profiles, whereas the adaptive model with ADWIN maintains high accuracy, achieving an F1-Score above 0.93 in all analyzed scenarios.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FP-AbDiff: Improving Score-based Antibody Design by Capturing Nonequilibrium Dynamics through the Underlying Fokker-Planck Equation</title>
<link>https://arxiv.org/abs/2511.03113</link>
<guid>https://arxiv.org/abs/2511.03113</guid>
<content:encoded><![CDATA[
arXiv:2511.03113v1 Announce Type: cross 
Abstract: Computational antibody design holds immense promise for therapeutic discovery, yet existing generative models are fundamentally limited by two core challenges: (i) a lack of dynamical consistency, which yields physically implausible structures, and (ii) poor generalization due to data scarcity and structural bias. We introduce FP-AbDiff, the first antibody generator to enforce Fokker-Planck Equation (FPE) physics along the entire generative trajectory. Our method minimizes a novel FPE residual loss over the mixed manifold of CDR geometries (R^3 x SO(3)), compelling locally-learned denoising scores to assemble into a globally coherent probability flow. This physics-informed regularizer is synergistically integrated with deep biological priors within a state-of-the-art SE(3)-equivariant diffusion framework. Rigorous evaluation on the RAbD benchmark confirms that FP-AbDiff establishes a new state-of-the-art. In de novo CDR-H3 design, it achieves a mean Root Mean Square Deviation of 0.99 {\AA} when superposing on the variable region, a 25% improvement over the previous state-of-the-art model, AbX, and the highest reported Contact Amino Acid Recovery of 39.91%. This superiority is underscored in the more challenging six-CDR co-design task, where our model delivers consistently superior geometric precision, cutting the average full-chain Root Mean Square Deviation by ~15%, and crucially, achieves the highest full-chain Amino Acid Recovery on the functionally dominant CDR-H3 loop (45.67%). By aligning generative dynamics with physical laws, FP-AbDiff enhances robustness and generalizability, establishing a principled approach for physically faithful and functionally viable antibody design.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Augmentation Overlap Theory of Contrastive Learning</title>
<link>https://arxiv.org/abs/2511.03114</link>
<guid>https://arxiv.org/abs/2511.03114</guid>
<content:encoded><![CDATA[
arXiv:2511.03114v1 Announce Type: cross 
Abstract: Recently, self-supervised contrastive learning has achieved great success on various tasks. However, its underlying working mechanism is yet unclear. In this paper, we first provide the tightest bounds based on the widely adopted assumption of conditional independence. Further, we relax the conditional independence assumption to a more practical assumption of augmentation overlap and derive the asymptotically closed bounds for the downstream performance. Our proposed augmentation overlap theory hinges on the insight that the support of different intra-class samples will become more overlapped under aggressive data augmentations, thus simply aligning the positive samples (augmented views of the same sample) could make contrastive learning cluster intra-class samples together. Moreover, from the newly derived augmentation overlap perspective, we develop an unsupervised metric for the representation evaluation of contrastive learning, which aligns well with the downstream performance almost without relying on additional modules. Code is available at https://github.com/PKU-ML/GARC.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image-Intrinsic Priors for Integrated Circuit Defect Detection and Novel Class Discovery via Self-Supervised Learning</title>
<link>https://arxiv.org/abs/2511.03120</link>
<guid>https://arxiv.org/abs/2511.03120</guid>
<content:encoded><![CDATA[
arXiv:2511.03120v1 Announce Type: cross 
Abstract: Integrated circuit manufacturing is highly complex, comprising hundreds of process steps. Defects can arise at any stage, causing yield loss and ultimately degrading product reliability. Supervised methods require extensive human annotation and struggle with emergent categories and rare, data scarce defects. Clustering-based unsupervised methods often exhibit unstable performance due to missing priors. We propose IC DefectNCD, a support set free framework that leverages Image Intrinsic Priors in IC SEM images for defect detection and novel class discovery. We first develop Self Normal Information Guided IC Defect Detection, aggregating representative normal features via a learnable normal information extractor and using reconstruction residuals to coarsely localize defect regions. To handle saliency variations across defects, we introduce an adaptive binarization strategy that produces stable subimages focused on core defective areas. Finally, we design Self Defect Information Guided IC Defect Classification, which incorporates a soft mask guided attention mechanism to inject spatial defect priors into the teacher student model. This enhances sensitivity to defective regions, suppresses background interference, and enables recognition and classification of unseen defects. We validate the approach on a real world dataset spanning three key fabrication stages and covering 15 defect types. Experiments demonstrate robust performance on both defect detection and unseen defect classification.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Control Barrier Function for Aligning Large Language Models</title>
<link>https://arxiv.org/abs/2511.03121</link>
<guid>https://arxiv.org/abs/2511.03121</guid>
<content:encoded><![CDATA[
arXiv:2511.03121v1 Announce Type: cross 
Abstract: This paper proposes a control-based framework for aligning large language models (LLMs) by leveraging a control barrier function (CBF) to ensure user-desirable text generation. The presented framework applies the CBF safety filter to the predicted token generated from the baseline LLM, to intervene in the generated text. The safety filter includes two significant advantages: this safety filter is an add-on type, allowing it to be used for alignment purposes without fine-tuning the baseline LLM, and if there is an evaluation model regarding the desired alignment, it can be directly applied to the filter design. The overall text-generation system is implemented with open-source language models, aiming to generate positive text.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EGMOF: Efficient Generation of Metal-Organic Frameworks Using a Hybrid Diffusion-Transformer Architecture</title>
<link>https://arxiv.org/abs/2511.03122</link>
<guid>https://arxiv.org/abs/2511.03122</guid>
<content:encoded><![CDATA[
arXiv:2511.03122v1 Announce Type: cross 
Abstract: Designing materials with targeted properties remains challenging due to the vastness of chemical space and the scarcity of property-labeled data. While recent advances in generative models offer a promising way for inverse design, most approaches require large datasets and must be retrained for every new target property. Here, we introduce the EGMOF (Efficient Generation of MOFs), a hybrid diffusion-transformer framework that overcomes these limitations through a modular, descriptor-mediated workflow. EGMOF decomposes inverse design into two steps: (1) a one-dimensional diffusion model (Prop2Desc) that maps desired properties to chemically meaningful descriptors followed by (2) a transformer model (Desc2MOF) that generates structures from these descriptors. This modular hybrid design enables minimal retraining and maintains high accuracy even under small-data conditions. On a hydrogen uptake dataset, EGMOF achieved over 95% validity and 84% hit rate, representing significant improvements of up to 57% in validity and 14% in hit rate compared to existing methods, while remaining effective with only 1,000 training samples. Moreover, our model successfully performed conditional generation across 29 diverse property datasets, including CoREMOF, QMOF, and text-mined experimental datasets, whereas previous models have not. This work presents a data-efficient, generalizable approach to the inverse design of diverse MOFs and highlights the potential of modular inverse design workflows for broader materials discovery.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Boundary Control of Diffusion on Graphs via Linear Programming</title>
<link>https://arxiv.org/abs/2511.03129</link>
<guid>https://arxiv.org/abs/2511.03129</guid>
<content:encoded><![CDATA[
arXiv:2511.03129v1 Announce Type: cross 
Abstract: We propose a linear programming (LP) framework for steady-state diffusion and flux optimization on geometric networks. The state variable satisfies a discrete diffusion law on a weighted, oriented graph, where conductances are scaled by edge lengths to preserve geometric fidelity. Boundary potentials act as controls that drive interior fluxes according to a linear network Laplacian. The optimization problem enforces physically meaningful sign and flux-cap constraints at all boundary edges, derived directly from a gradient bound. This yields a finite-dimensional LP whose feasible set is polyhedral, and whose boundedness and solvability follow from simple geometric or algebraic conditions on the network data.
  We prove that under the absence of negative recession directions--automatically satisfied in the presence of finite box bounds, flux caps, or sign restrictions--the LP admits a global minimizer. Several sufficient conditions guaranteeing boundedness of the feasible region are identified, covering both full-rank and rank-deficient flux maps. The analysis connects classical results such as the Minkowski--Weyl decomposition, Hoffman's bound, and the fundamental theorem of linear programming with modern network-based diffusion modeling.
  Two large-scale examples illustrate the framework: (i) A typical large stadium in a major modern city, which forms a single connected component with relatively uniform corridor widths, and a (ii) A complex street network emanating from a large, historical city center, which forms a multi-component system.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deploying Rapid Damage Assessments from sUAS Imagery for Disaster Response</title>
<link>https://arxiv.org/abs/2511.03132</link>
<guid>https://arxiv.org/abs/2511.03132</guid>
<content:encoded><![CDATA[
arXiv:2511.03132v1 Announce Type: cross 
Abstract: This paper presents the first AI/ML system for automating building damage assessment in uncrewed aerial systems (sUAS) imagery to be deployed operationally during federally declared disasters (Hurricanes Debby and Helene). In response to major disasters, sUAS teams are dispatched to collect imagery of the affected areas to assess damage; however, at recent disasters, teams collectively delivered between 47GB and 369GB of imagery per day, representing more imagery than can reasonably be transmitted or interpreted by subject matter experts in the disaster scene, thus delaying response efforts. To alleviate this data avalanche encountered in practice, computer vision and machine learning techniques are necessary. While prior work has been deployed to automatically assess damage in satellite imagery, there is no current state of practice for sUAS-based damage assessment systems, as all known work has been confined to academic settings. This work establishes the state of practice via the development and deployment of models for building damage assessment with sUAS imagery. The model development involved training on the largest known dataset of post-disaster sUAS aerial imagery, containing 21,716 building damage labels, and the operational training of 91 disaster practitioners. The best performing model was deployed during the responses to Hurricanes Debby and Helene, where it assessed a combined 415 buildings in approximately 18 minutes. This work contributes documentation of the actual use of AI/ML for damage assessment during a disaster and lessons learned to the benefit of the AI/ML research and user communities.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Measurement to Expertise: Empathetic Expert Adapters for Context-Based Empathy in Conversational AI Agents</title>
<link>https://arxiv.org/abs/2511.03143</link>
<guid>https://arxiv.org/abs/2511.03143</guid>
<content:encoded><![CDATA[
arXiv:2511.03143v1 Announce Type: cross 
Abstract: Empathy is a critical factor in fostering positive user experiences in conversational AI. While models can display empathy, it is often generic rather than tailored to specific tasks and contexts. In this work, we introduce a novel framework for developing and evaluating context-specific empathetic large language models (LLMs). We first analyze a real-world conversational dataset consisting of 672 multi-turn conversations across 8 tasks, revealing significant differences in terms of expected and experienced empathy before and after the conversations, respectively. To help minimize this gap, we develop a synthetic multi-turn conversational generation pipeline and steer responses toward our defined empathy patterns based on the context that more closely matches users' expectations. We then train empathetic expert adapters for context-specific empathy that specialize in varying empathy levels based on the recognized task. Our empirical results demonstrate a significant gap reduction of 72.66% between perceived and desired empathy with scores increasing by an average factor of 2.43 as measured by our metrics and reward models. Additionally, our trained empathetic expert adapters demonstrate superior effectiveness in preserving empathy patterns throughout conversation turns, outperforming system prompts, which tend to dramatically diminish in impact as conversations lengthen.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forecast2Anomaly (F2A): Adapting Multivariate Time Series Foundation Models for Anomaly Prediction</title>
<link>https://arxiv.org/abs/2511.03149</link>
<guid>https://arxiv.org/abs/2511.03149</guid>
<content:encoded><![CDATA[
arXiv:2511.03149v1 Announce Type: cross 
Abstract: Forecasting anomalies (anomaly prediction) in multivariate time series from different real-world, dynamic, and complex systems is vital for preempting critical failures, leading to a substantial minimization in operational costs and human labor. Yet, existing methods are limited to specific systems while failing to generalize to evolving anomaly patterns over time. In contrast, pretrained Time Series Foundation Models (TSFMs) have recently demonstrated strong generalization and zero-shot forecasting capabilities. However, their potential remains untapped for anomaly prediction, a task fundamentally different from forecasting normal behavior. Thus, we present Forecast2Anomaly (F2A), a novel framework that empowers TSFMs with anomaly prediction abilities through two key innovations. First, we propose a joint forecast-anomaly loss that fine-tunes TSFMs to accurately forecast future signals even at anomalous time points. Second, we introduce a Retrieval-Augmented Generation (RAG) module that retrieves historically relevant horizons and conditions predictions on them. This component dynamically adapts to distributional shifts at inference time, enabling F2A to track evolving anomalies without requiring model updates. By combining targeted fine-tuning with dynamic retrieval, F2A bridges the gap between robust TSFM zero-shot forecasting and zero-shot anomaly prediction. Extensive experiments across 16 diverse datasets and multiple TSFM backbones show that F2A consistently outperforms state-of-the-art methods, offering a scalable, zero-shot anomaly prediction solution for real-world applications.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Who Sees the Risk? Stakeholder Conflicts and Explanatory Policies in LLM-based Risk Assessment</title>
<link>https://arxiv.org/abs/2511.03152</link>
<guid>https://arxiv.org/abs/2511.03152</guid>
<content:encoded><![CDATA[
arXiv:2511.03152v1 Announce Type: cross 
Abstract: Understanding how different stakeholders perceive risks in AI systems is essential for their responsible deployment. This paper presents a framework for stakeholder-grounded risk assessment by using LLMs, acting as judges to predict and explain risks. Using the Risk Atlas Nexus and GloVE explanation method, our framework generates stakeholder-specific, interpretable policies that shows how different stakeholders agree or disagree about the same risks. We demonstrate our method using three real-world AI use cases of medical AI, autonomous vehicles, and fraud detection domain. We further propose an interactive visualization that reveals how and why conflicts emerge across stakeholder perspectives, enhancing transparency in conflict reasoning. Our results show that stakeholder perspectives significantly influence risk perception and conflict patterns. Our work emphasizes the importance of these stakeholder-aware explanations needed to make LLM-based evaluations more transparent, interpretable, and aligned with human-centered AI governance goals.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RefAgent: A Multi-agent LLM-based Framework for Automatic Software Refactoring</title>
<link>https://arxiv.org/abs/2511.03153</link>
<guid>https://arxiv.org/abs/2511.03153</guid>
<content:encoded><![CDATA[
arXiv:2511.03153v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have substantially influenced various software engineering tasks. Indeed, in the case of software refactoring, traditional LLMs have shown the ability to reduce development time and enhance code quality. However, these LLMs often rely on static, detailed instructions for specific tasks. In contrast, LLM-based agents can dynamically adapt to evolving contexts and autonomously make decisions by interacting with software tools and executing workflows. In this paper, we explore the potential of LLM-based agents in supporting refactoring activities. Specifically, we introduce RefAgent, a multi-agent LLM-based framework for end-to-end software refactoring. RefAgent consists of specialized agents responsible for planning, executing, testing, and iteratively refining refactorings using self-reflection and tool-calling capabilities. We evaluate RefAgent on eight open-source Java projects, comparing its effectiveness against a single-agent approach, a search-based refactoring tool, and historical developer refactorings. Our assessment focuses on: (1) the impact of generated refactorings on software quality, (2) the ability to identify refactoring opportunities, and (3) the contribution of each LLM agent through an ablation study. Our results show that RefAgent achieves a median unit test pass rate of 90%, reduces code smells by a median of 52.5%, and improves key quality attributes (e.g., reusability) by a median of 8.6%. Additionally, it closely aligns with developer refactorings and the search-based tool in identifying refactoring opportunities, attaining a median F1-score of 79.15% and 72.7%, respectively. Compared to single-agent approaches, RefAgent improves the median unit test pass rate by 64.7% and the median compilation success rate by 40.1%. These findings highlight the promise of multi-agent architectures in advancing automated software refactoring.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphCliff: Short-Long Range Gating for Subtle Differences but Critical Changes</title>
<link>https://arxiv.org/abs/2511.03170</link>
<guid>https://arxiv.org/abs/2511.03170</guid>
<content:encoded><![CDATA[
arXiv:2511.03170v1 Announce Type: cross 
Abstract: Quantitative structure-activity relationship assumes a smooth relationship between molecular structure and biological activity. However, activity cliffs defined as pairs of structurally similar compounds with large potency differences break this continuity. Recent benchmarks targeting activity cliffs have revealed that classical machine learning models with extended connectivity fingerprints outperform graph neural networks. Our analysis shows that graph embeddings fail to adequately separate structurally similar molecules in the embedding space, making it difficult to distinguish between structurally similar but functionally different molecules. Despite this limitation, molecular graph structures are inherently expressive and attractive, as they preserve molecular topology. To preserve the structural representation of molecules as graphs, we propose a new model, GraphCliff, which integrates short- and long-range information through a gating mechanism. Experimental results demonstrate that GraphCliff consistently improves performance on both non-cliff and cliff compounds. Furthermore, layer-wise node embedding analyses reveal reduced over-smoothing and enhanced discriminative power relative to strong baseline graph models.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Earth-Moon Transfer and Cislunar Navigation: Integrating Low-Energy Trajectories, AI Techniques and GNSS-R Technologies</title>
<link>https://arxiv.org/abs/2511.03173</link>
<guid>https://arxiv.org/abs/2511.03173</guid>
<content:encoded><![CDATA[
arXiv:2511.03173v1 Announce Type: cross 
Abstract: The rapid growth of cislunar activities, including lunar landings, the Lunar Gateway, and in-space refueling stations, requires advances in cost-efficient trajectory design and reliable integration of navigation and remote sensing. Traditional Earth-Moon transfers suffer from rigid launch windows and high propellant demands, while Earth-based GNSS systems provide little to no coverage beyond geostationary orbit. This limits autonomy and environmental awareness in cislunar space. This review compares four major transfer strategies by evaluating velocity requirements, flight durations, and fuel efficiency, and by identifying their suitability for both crewed and robotic missions. The emerging role of artificial intelligence and machine learning is highlighted: convolutional neural networks support automated crater recognition and digital terrain model generation, while deep reinforcement learning enables adaptive trajectory refinement during descent and landing to reduce risk and decision latency. The study also examines how GNSS-Reflectometry and advanced Positioning, Navigation, and Timing architectures can extend navigation capabilities beyond current limits. GNSS-R can act as a bistatic radar for mapping lunar ice, soil properties, and surface topography, while PNT systems support autonomous rendezvous, Lagrange point station-keeping, and coordinated satellite swarm operations. Combining these developments establishes a scalable framework for sustainable cislunar exploration and long-term human and robotic presence.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Linear Attention for Multivariate Time Series Modeling via Entropy Equality</title>
<link>https://arxiv.org/abs/2511.03190</link>
<guid>https://arxiv.org/abs/2511.03190</guid>
<content:encoded><![CDATA[
arXiv:2511.03190v1 Announce Type: cross 
Abstract: Attention mechanisms have been extensively employed in various applications, including time series modeling, owing to their capacity to capture intricate dependencies; however, their utility is often constrained by quadratic computational complexity, which impedes scalability for long sequences. In this work, we propose a novel linear attention mechanism designed to overcome these limitations. Our approach is grounded in a theoretical demonstration that entropy, as a strictly concave function on the probability simplex, implies that distributions with aligned probability rankings and similar entropy values exhibit structural resemblance. Building on this insight, we develop an efficient approximation algorithm that computes the entropy of dot-product-derived distributions with only linear complexity, enabling the implementation of a linear attention mechanism based on entropy equality. Through rigorous analysis, we reveal that the effectiveness of attention in spatio-temporal time series modeling may not primarily stem from the non-linearity of softmax but rather from the attainment of a moderate and well-balanced weight distribution. Extensive experiments on four spatio-temporal datasets validate our method, demonstrating competitive or superior forecasting performance while achieving substantial reductions in both memory usage and computational time.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies</title>
<link>https://arxiv.org/abs/2511.03201</link>
<guid>https://arxiv.org/abs/2511.03201</guid>
<content:encoded><![CDATA[
arXiv:2511.03201v1 Announce Type: cross 
Abstract: In an effort to counter the increasing IoT botnet-based attacks, state-of-the-art deep learning methods have been proposed and have achieved impressive detection accuracy. However, their computational intensity restricts deployment on resource-constrained IoT devices, creating a critical need for lightweight detection models. A common solution to this challenge is model compression via quantization. This study proposes a VAE-MLP model framework where an MLP-based classifier is trained on 8-dimensional latent vectors derived from the high-dimensional train data using the encoder component of a pretrained variational autoencoder (VAE). Two widely used quantization strategies--Quantization-Aware Training (QAT) and Post-Training Quantization (PTQ)--are then systematically evaluated in terms of their impact on detection performance, storage efficiency, and inference latency using two benchmark IoT botnet datasets--N-BaIoT and CICIoT2022. The results revealed that, with respect to detection accuracy, the QAT strategy experienced a more noticeable decline,whereas PTQ incurred only a marginal reduction compared to the original unquantized model. Furthermore, PTQ yielded a 6x speedup and 21x reduction in size, while QAT achieved a 3x speedup and 24x compression, demonstrating the practicality of quantization for device-level IoT botnet detection.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QG-CoC: Question-Guided Chain-of-Captions for Large Multimodal Models</title>
<link>https://arxiv.org/abs/2511.03206</link>
<guid>https://arxiv.org/abs/2511.03206</guid>
<content:encoded><![CDATA[
arXiv:2511.03206v1 Announce Type: cross 
Abstract: Recently, Multimodal Large Language Models (MLLMs) encounter two key issues in multi-image contexts: (1) a lack of fine-grained perception across disparate images, and (2) a diminished capability to effectively reason over and synthesize information from multiple visual inputs. However, while various prompting methods aim to describe visual content, many existing studies focus primarily on single-image settings or specific, constrained scenarios. This leaves a critical gap in understanding and addressing how MLLMs tackle more general and complex multi-image reasoning tasks. Thus, we first extensively investigate how current prompting methods perceive fine-grained visual details and process visual information when dealing with multiple images. Our findings reveal that existing prompting methods fall short in attending to needed clues and seamlessly integrating perception and reasoning. Inspired by the findings, we propose a new zero-shot prompting method, Question-Guided Chain-of-Captions (QG-CoC), a generalized prompting approach that effectively handles problems with an arbitrary number of images. We evaluate our method on various open-source and closed-source MLLMs for multi-image and single-image benchmarks. Experimental results indicate that QG-CoC demonstrates competitive performance across tasks and exhibits robust improvements in the challenging scenarios where existing prompting methods fail.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retrofitters, pragmatists and activists: Public interest litigation for accountable automated decision-making</title>
<link>https://arxiv.org/abs/2511.03211</link>
<guid>https://arxiv.org/abs/2511.03211</guid>
<content:encoded><![CDATA[
arXiv:2511.03211v1 Announce Type: cross 
Abstract: This paper examines the role of public interest litigation in promoting accountability for AI and automated decision-making (ADM) in Australia. Since ADM regulatio faces geopolitical headwinds, effective governance will have to rely at least in part on the enforcement of existing laws. Drawing on interviews with Australian public interest litigators, technology policy activists, and technology law scholars, the paper positions public interest litigation as part of a larger ecosystem for transparency, accountability and justice with respect to ADM. It builds on one participants's characterisation of litigation about ADM as an exercise in legal retrofitting: adapting old laws to new circumstances. The paper's primary contribution is to aggregate, organise and present original insights on pragmatic strategies and tactics for effective public interest litigation about ADM. Naturally, it also contends with the limits of these strategies, and of the legal system. Where limits are, however, capable of being overcome, the paper presents findings on urgent needs: the enabling institutional arrangements without which effective litigation and accountability will falter. The paper is relevant to law and technology scholars; individuals and groups harmed by ADM; public interest litigators and technology lawyers; civil society and advocacy organisations; and policymakers.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LGM: Enhancing Large Language Models with Conceptual Meta-Relations and Iterative Retrieval</title>
<link>https://arxiv.org/abs/2511.03214</link>
<guid>https://arxiv.org/abs/2511.03214</guid>
<content:encoded><![CDATA[
arXiv:2511.03214v1 Announce Type: cross 
Abstract: Large language models (LLMs) exhibit strong semantic understanding, yet struggle when user instructions involve ambiguous or conceptually misaligned terms. We propose the Language Graph Model (LGM) to enhance conceptual clarity by extracting meta-relations-inheritance, alias, and composition-from natural language. The model further employs a reflection mechanism to validate these meta-relations. Leveraging a Concept Iterative Retrieval Algorithm, these relations and related descriptions are dynamically supplied to the LLM, improving its ability to interpret concepts and generate accurate responses. Unlike conventional Retrieval-Augmented Generation (RAG) approaches that rely on extended context windows, our method enables large language models to process texts of any length without the need for truncation. Experiments on standard benchmarks demonstrate that the LGM consistently outperforms existing RAG baselines.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Fact-Checking that Integrates Knowledge Graphs, Large Language Models, and Search-Based Retrieval Agents Improves Interpretable Claim Verification</title>
<link>https://arxiv.org/abs/2511.03217</link>
<guid>https://arxiv.org/abs/2511.03217</guid>
<content:encoded><![CDATA[
arXiv:2511.03217v1 Announce Type: cross 
Abstract: Large language models (LLMs) excel in generating fluent utterances but can lack reliable grounding in verified information. At the same time, knowledge-graph-based fact-checkers deliver precise and interpretable evidence, yet suffer from limited coverage or latency. By integrating LLMs with knowledge graphs and real-time search agents, we introduce a hybrid fact-checking approach that leverages the individual strengths of each component. Our system comprises three autonomous steps: 1) a Knowledge Graph (KG) Retrieval for rapid one - hop lookups in DBpedia, 2) an LM-based classification guided by a task-specific labeling prompt, producing outputs with internal rule-based logic, and 3) a Web Search Agent invoked only when KG coverage is insufficient. Our pipeline achieves an F1 score of 0.93 on the FEVER benchmark on the Supported/Refuted split without task- specific fine - tuning. To address Not enough information cases, we conduct a targeted reannotation study showing that our approach frequently uncovers valid evidence for claims originally labeled as Not Enough Information (NEI), as confirmed by both expert annotators and LLM reviewers. With this paper, we present a modular, opensource fact-checking pipeline with fallback strategies and generalization across datasets.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Node-Based Editing for Multimodal Generation of Text, Audio, Image, and Vide</title>
<link>https://arxiv.org/abs/2511.03227</link>
<guid>https://arxiv.org/abs/2511.03227</guid>
<content:encoded><![CDATA[
arXiv:2511.03227v1 Announce Type: cross 
Abstract: We present a node-based storytelling system for multimodal content generation. The system represents stories as graphs of nodes that can be expanded, edited, and iteratively refined through direct user edits and natural-language prompts. Each node can integrate text, images, audio, and video, allowing creators to compose multimodal narratives. A task selection agent routes between specialized generative tasks that handle story generation, node structure reasoning, node diagram formatting, and context generation. The interface supports targeted editing of individual nodes, automatic branching for parallel storylines, and node-based iterative refinement. Our results demonstrate that node-based editing supports control over narrative structure and iterative generation of text, images, audio, and video. We report quantitative outcomes on automatic story outline generation and qualitative observations of editing workflows. Finally, we discuss current limitations such as scalability to longer narratives and consistency across multiple nodes, and outline future work toward human-in-the-loop and user-centered creative AI tools.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GMoPE:A Prompt-Expert Mixture Framework for Graph Foundation Models</title>
<link>https://arxiv.org/abs/2511.03251</link>
<guid>https://arxiv.org/abs/2511.03251</guid>
<content:encoded><![CDATA[
arXiv:2511.03251v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have demonstrated impressive performance on task-specific benchmarks, yet their ability to generalize across diverse domains and tasks remains limited. Existing approaches often struggle with negative transfer, scalability issues, and high adaptation costs. To address these challenges, we propose GMoPE (Graph Mixture of Prompt-Experts), a novel framework that seamlessly integrates the Mixture-of-Experts (MoE) architecture with prompt-based learning for graphs. GMoPE leverages expert-specific prompt vectors and structure-aware MoE routing to enable each expert to specialize in distinct subdomains and dynamically contribute to predictions. To promote diversity and prevent expert collapse, we introduce a soft orthogonality constraint across prompt vectors, encouraging expert specialization and facilitating a more balanced expert utilization. Additionally, we adopt a prompt-only fine-tuning strategy that significantly reduces spatiotemporal complexity during transfer. We validate GMoPE through extensive experiments under various pretraining strategies and multiple downstream tasks. Results show that GMoPE consistently outperforms state-of-the-art baselines and achieves performance comparable to full parameter fine-tuning-while requiring only a fraction of the adaptation overhead. Our work provides a principled and scalable framework for advancing generalizable and efficient graph foundation models.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative deep learning for foundational video translation in ultrasound</title>
<link>https://arxiv.org/abs/2511.03255</link>
<guid>https://arxiv.org/abs/2511.03255</guid>
<content:encoded><![CDATA[
arXiv:2511.03255v1 Announce Type: cross 
Abstract: Deep learning (DL) has the potential to revolutionize image acquisition and interpretation across medicine, however, attention to data imbalance and missingness is required. Ultrasound data presents a particular challenge because in addition to different views and structures, it includes several sub-modalities-such as greyscale and color flow doppler (CFD)-that are often imbalanced in clinical studies. Image translation can help balance datasets but is challenging for ultrasound sub-modalities to date. Here, we present a generative method for ultrasound CFD-greyscale video translation, trained on 54,975 videos and tested on 8,368. The method developed leveraged pixel-wise, adversarial, and perceptual loses and utilized two networks: one for reconstructing anatomic structures and one for denoising to achieve realistic ultrasound imaging. Average pairwise SSIM between synthetic videos and ground truth was 0.91+/-0.04. Synthetic videos performed indistinguishably from real ones in DL classification and segmentation tasks and when evaluated by blinded clinical experts: F1 score was 0.9 for real and 0.89 for synthetic videos; Dice score between real and synthetic segmentation was 0.97. Overall clinician accuracy in distinguishing real vs synthetic videos was 54+/-6% (42-61%), indicating realistic synthetic videos. Although trained only on heart videos, the model worked well on ultrasound spanning several clinical domains (average SSIM 0.91+/-0.05), demonstrating foundational abilities. Together, these data expand the utility of retrospectively collected imaging and augment the dataset design toolbox for medical imaging.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparing the Performance of LLMs in RAG-based Question-Answering: A Case Study in Computer Science Literature</title>
<link>https://arxiv.org/abs/2511.03261</link>
<guid>https://arxiv.org/abs/2511.03261</guid>
<content:encoded><![CDATA[
arXiv:2511.03261v1 Announce Type: cross 
Abstract: Retrieval Augmented Generation (RAG) is emerging as a powerful technique to enhance the capabilities of Generative AI models by reducing hallucination. Thus, the increasing prominence of RAG alongside Large Language Models (LLMs) has sparked interest in comparing the performance of different LLMs in question-answering (QA) in diverse domains. This study compares the performance of four open-source LLMs, Mistral-7b-instruct, LLaMa2-7b-chat, Falcon-7b-instruct and Orca-mini-v3-7b, and OpenAI's trending GPT-3.5 over QA tasks within the computer science literature leveraging RAG support. Evaluation metrics employed in the study include accuracy and precision for binary questions and ranking by a human expert, ranking by Google's AI model Gemini, alongside cosine similarity for long-answer questions. GPT-3.5, when paired with RAG, effectively answers binary and long-answer questions, reaffirming its status as an advanced LLM. Regarding open-source LLMs, Mistral AI's Mistral-7b-instruct paired with RAG surpasses the rest in answering both binary and long-answer questions. However, among the open-source LLMs, Orca-mini-v3-7b reports the shortest average latency in generating responses, whereas LLaMa2-7b-chat by Meta reports the highest average latency. This research underscores the fact that open-source LLMs, too, can go hand in hand with proprietary models like GPT-3.5 with better infrastructure.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Generative Artificial Intelligence meets Extended Reality: A Systematic Review</title>
<link>https://arxiv.org/abs/2511.03282</link>
<guid>https://arxiv.org/abs/2511.03282</guid>
<content:encoded><![CDATA[
arXiv:2511.03282v1 Announce Type: cross 
Abstract: With the continuous advancement of technology, the application of generative artificial intelligence (AI) in various fields is gradually demonstrating great potential, particularly when combined with Extended Reality (XR), creating unprecedented possibilities. This survey article systematically reviews the applications of generative AI in XR, covering as much relevant literature as possible from 2023 to 2025. The application areas of generative AI in XR and its key technology implementations are summarised through PRISMA screening and analysis of the final 26 articles. The survey highlights existing articles from the last three years related to how XR utilises generative AI, providing insights into current trends and research gaps. We also explore potential opportunities for future research to further empower XR through generative AI, providing guidance and information for future generative XR research.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How to Evaluate Speech Translation with Source-Aware Neural MT Metrics</title>
<link>https://arxiv.org/abs/2511.03295</link>
<guid>https://arxiv.org/abs/2511.03295</guid>
<content:encoded><![CDATA[
arXiv:2511.03295v1 Announce Type: cross 
Abstract: Automatic evaluation of speech-to-text translation (ST) systems is typically performed by comparing translation hypotheses with one or more reference translations. While effective to some extent, this approach inherits the limitation of reference-based evaluation that ignores valuable information from the source input. In machine translation (MT), recent progress has shown that neural metrics incorporating the source text achieve stronger correlation with human judgments. Extending this idea to ST, however, is not trivial because the source is audio rather than text, and reliable transcripts or alignments between source and references are often unavailable. In this work, we conduct the first systematic study of source-aware metrics for ST, with a particular focus on real-world operating conditions where source transcripts are not available. We explore two complementary strategies for generating textual proxies of the input audio, automatic speech recognition (ASR) transcripts, and back-translations of the reference translation, and introduce a novel two-step cross-lingual re-segmentation algorithm to address the alignment mismatch between synthetic sources and reference translations. Our experiments, carried out on two ST benchmarks covering 79 language pairs and six ST systems with diverse architectures and performance levels, show that ASR transcripts constitute a more reliable synthetic source than back-translations when word error rate is below 20%, while back-translations always represent a computationally cheaper but still effective alternative. Furthermore, our cross-lingual re-segmentation algorithm enables robust use of source-aware MT metrics in ST evaluation, paving the way toward more accurate and principled evaluation methodologies for speech translation.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extending Fair Null-Space Projections for Continuous Attributes to Kernel Methods</title>
<link>https://arxiv.org/abs/2511.03304</link>
<guid>https://arxiv.org/abs/2511.03304</guid>
<content:encoded><![CDATA[
arXiv:2511.03304v1 Announce Type: cross 
Abstract: With the on-going integration of machine learning systems into the everyday social life of millions the notion of fairness becomes an ever increasing priority in their development. Fairness notions commonly rely on protected attributes to assess potential biases. Here, the majority of literature focuses on discrete setups regarding both target and protected attributes. The literature on continuous attributes especially in conjunction with regression -- we refer to this as \emph{continuous fairness} -- is scarce. A common strategy is iterative null-space projection which as of now has only been explored for linear models or embeddings such as obtained by a non-linear encoder. We improve on this by generalizing to kernel methods, significantly extending the scope. This yields a model and fairness-score agnostic method for kernel embeddings applicable to continuous protected attributes. We demonstrate that our novel approach in conjunction with Support Vector Regression (SVR) provides competitive or improved performance across multiple datasets in comparisons to other contemporary methods.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking the Thinking Mode of Multimodal Large Language Models in Clinical Tasks</title>
<link>https://arxiv.org/abs/2511.03328</link>
<guid>https://arxiv.org/abs/2511.03328</guid>
<content:encoded><![CDATA[
arXiv:2511.03328v1 Announce Type: cross 
Abstract: A recent advancement in Multimodal Large Language Models (MLLMs) research is the emergence of "reasoning MLLMs" that offer explicit control over their internal thinking processes (normally referred as the "thinking mode") alongside the standard "non-thinking mode". This capability allows these models to engage in a step-by-step process of internal deliberation before generating a final response. With the rapid transition to and adoption of these "dual-state" MLLMs, this work rigorously evaluated how the enhanced reasoning processes of these MLLMs impact model performance and reliability in clinical tasks. This paper evaluates the active "thinking mode" capabilities of two leading MLLMs, Seed1.5-VL and Gemini-2.5-Flash, for medical applications. We assessed their performance on four visual medical tasks using VQA-RAD and ROCOv2 datasets. Our findings reveal that the improvement from activating the thinking mode remains marginal compared to the standard non-thinking mode for the majority of the tasks. Their performance on complex medical tasks such as open-ended VQA and medical image interpretation remains suboptimal, highlighting the need for domain-specific medical data and more advanced methods for medical knowledge integration.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Discourse-Aware Scientific Paper Recommendation via QA-Style Summarization and Multi-Level Contrastive Learning</title>
<link>https://arxiv.org/abs/2511.03330</link>
<guid>https://arxiv.org/abs/2511.03330</guid>
<content:encoded><![CDATA[
arXiv:2511.03330v1 Announce Type: cross 
Abstract: The rapid growth of open-access (OA) publications has intensified the challenge of identifying relevant scientific papers. Due to privacy constraints and limited access to user interaction data, recent efforts have shifted toward content-based recommendation, which relies solely on textual information. However, existing models typically treat papers as unstructured text, neglecting their discourse organization and thereby limiting semantic completeness and interpretability. To address these limitations, we propose OMRC-MR, a hierarchical framework that integrates QA-style OMRC (Objective, Method, Result, Conclusion) summarization, multi-level contrastive learning, and structure-aware re-ranking for scholarly recommendation. The QA-style summarization module converts raw papers into structured and discourse-consistent representations, while multi-level contrastive objectives align semantic representations across metadata, section, and document levels. The final re-ranking stage further refines retrieval precision through contextual similarity calibration. Experiments on DBLP, S2ORC, and the newly constructed Sci-OMRC dataset demonstrate that OMRC-MR consistently surpasses state-of-the-art baselines, achieving up to 7.2% and 3.8% improvements in Precision@10 and Recall@10, respectively. Additional evaluations confirm that QA-style summarization produces more coherent and factually complete representations. Overall, OMRC-MR provides a unified and interpretable content-based paradigm for scientific paper recommendation, advancing trustworthy and privacy-aware scholarly information retrieval.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Artificial Intelligence in Bioinformatics: A Systematic Review of Models, Applications, and Methodological Advances</title>
<link>https://arxiv.org/abs/2511.03354</link>
<guid>https://arxiv.org/abs/2511.03354</guid>
<content:encoded><![CDATA[
arXiv:2511.03354v1 Announce Type: cross 
Abstract: Generative artificial intelligence (GenAI) has become a transformative approach in bioinformatics that often enables advancements in genomics, proteomics, transcriptomics, structural biology, and drug discovery. To systematically identify and evaluate these growing developments, this review proposed six research questions (RQs), according to the preferred reporting items for systematic reviews and meta-analysis methods. The objective is to evaluate impactful GenAI strategies in methodological advancement, predictive performance, and specialization, and to identify promising approaches for advanced modeling, data-intensive discovery, and integrative biological analysis. RQ1 highlights diverse applications across multiple bioinformatics subfields (sequence analysis, molecular design, and integrative data modeling), which demonstrate superior performance over traditional methods through pattern recognition and output generation. RQ2 reveals that adapted specialized model architectures outperformed general-purpose models, an advantage attributed to targeted pretraining and context-aware strategies. RQ3 identifies significant benefits in the bioinformatics domains, focusing on molecular analysis and data integration, which improves accuracy and reduces errors in complex analysis. RQ4 indicates improvements in structural modeling, functional prediction, and synthetic data generation, validated by established benchmarks. RQ5 suggests the main constraints, such as the lack of scalability and biases in data that impact generalizability, and proposes future directions focused on robust evaluation and biologically grounded modeling. RQ6 examines that molecular datasets (such as UniProtKB and ProteinNet12), cellular datasets (such as CELLxGENE and GTEx) and textual resources (such as PubMedQA and OMIM) broadly support the training and generalization of GenAI models.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Open Source State-Of-the-Art Solution for Romanian Speech Recognition</title>
<link>https://arxiv.org/abs/2511.03361</link>
<guid>https://arxiv.org/abs/2511.03361</guid>
<content:encoded><![CDATA[
arXiv:2511.03361v1 Announce Type: cross 
Abstract: In this work, we present a new state-of-the-art Romanian Automatic Speech Recognition (ASR) system based on NVIDIA's FastConformer architecture--explored here for the first time in the context of Romanian. We train our model on a large corpus of, mostly, weakly supervised transcriptions, totaling over 2,600 hours of speech. Leveraging a hybrid decoder with both Connectionist Temporal Classification (CTC) and Token-Duration Transducer (TDT) branches, we evaluate a range of decoding strategies including greedy, ALSD, and CTC beam search with a 6-gram token-level language model. Our system achieves state-of-the-art performance across all Romanian evaluation benchmarks, including read, spontaneous, and domain-specific speech, with up to 27% relative WER reduction compared to previous best-performing systems. In addition to improved transcription accuracy, our approach demonstrates practical decoding efficiency, making it suitable for both research and deployment in low-latency ASR applications.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoupling Augmentation Bias in Prompt Learning for Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.03367</link>
<guid>https://arxiv.org/abs/2511.03367</guid>
<content:encoded><![CDATA[
arXiv:2511.03367v1 Announce Type: cross 
Abstract: Recent advances in large-scale vision and language models have led to significant progress in zero-shot learning tasks. Methods such as CoOp and CoCoOp have shown that replacing handcrafted prompts with learnable vectors, known as prompt learning, can result in improved performance. However, these models often struggle to generalize to entirely unseen categories. While traditional zero-shot learning techniques benefit from various data augmentation strategies, prompt learning has primarily focused on text-based modifications, leaving the potential of image-based augmentation largely unexplored. In this work, we explore how image-level augmentations, particularly those that introduce attribute-specific variations, can support and enhance prompt learning. Our analysis examines the interaction between these augmentations and soft prompt frameworks, revealing their potential to improve generalization. We also identify a limitation in existing methods, such as CoCoOp, which do not provide explicit guidance for learning prompts that focus on semantically meaningful visual features. To address this, we propose Adding Attributes to Prompt Learning, AAPL, a novel method that introduces adversarial token embeddings to decouple superficial visual variations introduced by augmentation from class-relevant semantic representations. This decoupling enables the learned prompts to concentrate on visually discriminative features that align with the target categories. We conduct comprehensive experiments on eleven benchmark datasets, and AAPL consistently outperforms existing methods across few-shot, zero-shot, cross-dataset, and domain generalization settings. Our source code is publicly available at: https://github.com/Gahyeonkim09/AAPL
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computational Imaging Meets LLMs: Zero-Shot IDH Mutation Prediction in Brain Gliomas</title>
<link>https://arxiv.org/abs/2511.03376</link>
<guid>https://arxiv.org/abs/2511.03376</guid>
<content:encoded><![CDATA[
arXiv:2511.03376v1 Announce Type: cross 
Abstract: We present a framework that combines Large Language Models with computational image analytics for non-invasive, zero-shot prediction of IDH mutation status in brain gliomas. For each subject, coregistered multi-parametric MRI scans and multi-class tumor segmentation maps were processed to extract interpretable semantic (visual) attributes and quantitative features, serialized in a standardized JSON file, and used to query GPT 4o and GPT 5 without fine-tuning. We evaluated this framework on six publicly available datasets (N = 1427) and results showcased high accuracy and balanced classification performance across heterogeneous cohorts, even in the absence of manual annotations. GPT 5 outperformed GPT 4o in context-driven phenotype interpretation. Volumetric features emerged as the most important predictors, supplemented by subtype-specific imaging markers and clinical information. Our results demonstrate the potential of integrating LLM-based reasoning with computational image analytics for precise, non-invasive tumor genotyping, advancing diagnostic strategies in neuro-oncology. The code is available at https://github.com/ATPLab-LUMS/CIM-LLM.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptable Hindsight Experience Replay for Search-Based Learning</title>
<link>https://arxiv.org/abs/2511.03405</link>
<guid>https://arxiv.org/abs/2511.03405</guid>
<content:encoded><![CDATA[
arXiv:2511.03405v1 Announce Type: cross 
Abstract: AlphaZero-like Monte Carlo Tree Search systems, originally introduced for two-player games, dynamically balance exploration and exploitation using neural network guidance. This combination makes them also suitable for classical search problems. However, the original method of training the network with simulation results is limited in sparse reward settings, especially in the early stages, where the network cannot yet give guidance. Hindsight Experience Replay (HER) addresses this issue by relabeling unsuccessful trajectories from the search tree as supervised learning signals. We introduce Adaptable HER (\ours{}), a flexible framework that integrates HER with AlphaZero, allowing easy adjustments to HER properties such as relabeled goals, policy targets, and trajectory selection. Our experiments, including equation discovery, show that the possibility of modifying HER is beneficial and surpasses the performance of pure supervised or reinforcement learning.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Light over Heavy: Automated Performance Requirements Quantification with Linguistic Inducement</title>
<link>https://arxiv.org/abs/2511.03421</link>
<guid>https://arxiv.org/abs/2511.03421</guid>
<content:encoded><![CDATA[
arXiv:2511.03421v1 Announce Type: cross 
Abstract: Elicited performance requirements need to be quantified for compliance in different engineering tasks, e.g., configuration tuning and performance testing. Much existing work has relied on manual quantification, which is expensive and error-prone due to the imprecision. In this paper, we present LQPR, a highly efficient automatic approach for performance requirements quantification.LQPR relies on a new theoretical framework that converts quantification as a classification problem. Despite the prevalent applications of Large Language Models (LLMs) for requirement analytics, LQPR takes a different perspective to address the classification: we observed that performance requirements can exhibit strong patterns and are often short/concise, therefore we design a lightweight linguistically induced matching mechanism. We compare LQPR against nine state-of-the-art learning-based approaches over diverse datasets, demonstrating that it is ranked as the sole best for 75% or more cases with two orders less cost. Our work proves that, at least for performance requirement quantification, specialized methods can be more suitable than the general LLM-driven approaches.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inter-Agent Trust Models: A Comparative Study of Brief, Claim, Proof, Stake, Reputation and Constraint in Agentic Web Protocol Design-A2A, AP2, ERC-8004, and Beyond</title>
<link>https://arxiv.org/abs/2511.03434</link>
<guid>https://arxiv.org/abs/2511.03434</guid>
<content:encoded><![CDATA[
arXiv:2511.03434v1 Announce Type: cross 
Abstract: As the "agentic web" takes shape-billions of AI agents (often LLM-powered) autonomously transacting and collaborating-trust shifts from human oversight to protocol design. In 2025, several inter-agent protocols crystallized this shift, including Google's Agent-to-Agent (A2A), Agent Payments Protocol (AP2), and Ethereum's ERC-8004 "Trustless Agents," yet their underlying trust assumptions remain under-examined. This paper presents a comparative study of trust models in inter-agent protocol design: Brief (self- or third-party verifiable claims), Claim (self-proclaimed capabilities and identity, e.g. AgentCard), Proof (cryptographic verification, including zero-knowledge proofs and trusted execution environment attestations), Stake (bonded collateral with slashing and insurance), Reputation (crowd feedback and graph-based trust signals), and Constraint (sandboxing and capability bounding). For each, we analyze assumptions, attack surfaces, and design trade-offs, with particular emphasis on LLM-specific fragilities-prompt injection, sycophancy/nudge-susceptibility, hallucination, deception, and misalignment-that render purely reputational or claim-only approaches brittle. Our findings indicate no single mechanism suffices. We argue for trustless-by-default architectures anchored in Proof and Stake to gate high-impact actions, augmented by Brief for identity and discovery and Reputation overlays for flexibility and social signals. We comparatively evaluate A2A, AP2, ERC-8004 and related historical variations in academic research under metrics spanning security, privacy, latency/cost, and social robustness (Sybil/collusion/whitewashing resistance). We conclude with hybrid trust model recommendations that mitigate reputation gaming and misinformed LLM behavior, and we distill actionable design guidelines for safer, interoperable, and scalable agent economies.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CareMedEval dataset: Evaluating Critical Appraisal and Reasoning in the Biomedical Field</title>
<link>https://arxiv.org/abs/2511.03441</link>
<guid>https://arxiv.org/abs/2511.03441</guid>
<content:encoded><![CDATA[
arXiv:2511.03441v1 Announce Type: cross 
Abstract: Critical appraisal of scientific literature is an essential skill in the biomedical field. While large language models (LLMs) can offer promising support in this task, their reliability remains limited, particularly for critical reasoning in specialized domains. We introduce CareMedEval, an original dataset designed to evaluate LLMs on biomedical critical appraisal and reasoning tasks. Derived from authentic exams taken by French medical students, the dataset contains 534 questions based on 37 scientific articles. Unlike existing benchmarks, CareMedEval explicitly evaluates critical reading and reasoning grounded in scientific papers. Benchmarking state-of-the-art generalist and biomedical-specialized LLMs under various context conditions reveals the difficulty of the task: open and commercial models fail to exceed an Exact Match Rate of 0.5 even though generating intermediate reasoning tokens considerably improves the results. Yet, models remain challenged especially on questions about study limitations and statistical analysis. CareMedEval provides a challenging benchmark for grounded reasoning, exposing current LLM limitations and paving the way for future development of automated support for critical appraisal.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Development of the Bioinspired Tendon-Driven DexHand 021 with Proprioceptive Compliance Control</title>
<link>https://arxiv.org/abs/2511.03481</link>
<guid>https://arxiv.org/abs/2511.03481</guid>
<content:encoded><![CDATA[
arXiv:2511.03481v1 Announce Type: cross 
Abstract: The human hand plays a vital role in daily life and industrial applications, yet replicating its multifunctional capabilities-including motion, sensing, and coordinated manipulation-with robotic systems remains a formidable challenge. Developing a dexterous robotic hand requires balancing human-like agility with engineering constraints such as complexity, size-to-weight ratio, durability, and force-sensing performance. This letter presents Dex-Hand 021, a high-performance, cable-driven five-finger robotic hand with 12 active and 7 passive degrees of freedom (DoFs), achieving 19 DoFs dexterity in a lightweight 1 kg design. We propose a proprioceptive force-sensing-based admittance control method to enhance manipulation. Experimental results demonstrate its superior performance: a single-finger load capacity exceeding 10 N, fingertip repeatability under 0.001 m, and force estimation errors below 0.2 N. Compared to PID control, joint torques in multi-object grasping are reduced by 31.19%, significantly improves force-sensing capability while preventing overload during collisions. The hand excels in both power and precision grasps, successfully executing 33 GRASP taxonomy motions and complex manipulation tasks. This work advances the design of lightweight, industrial-grade dexterous hands and enhances proprioceptive control, contributing to robotic manipulation and intelligent manufacturing.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ROSBag MCP Server: Analyzing Robot Data with LLMs for Agentic Embodied AI Applications</title>
<link>https://arxiv.org/abs/2511.03497</link>
<guid>https://arxiv.org/abs/2511.03497</guid>
<content:encoded><![CDATA[
arXiv:2511.03497v1 Announce Type: cross 
Abstract: Agentic AI systems and Physical or Embodied AI systems have been two key research verticals at the forefront of Artificial Intelligence and Robotics, with Model Context Protocol (MCP) increasingly becoming a key component and enabler of agentic applications. However, the literature at the intersection of these verticals, i.e., Agentic Embodied AI, remains scarce. This paper introduces an MCP server for analyzing ROS and ROS 2 bags, allowing for analyzing, visualizing and processing robot data with natural language through LLMs and VLMs. We describe specific tooling built with robotics domain knowledge, with our initial release focused on mobile robotics and supporting natively the analysis of trajectories, laser scan data, transforms, or time series data. This is in addition to providing an interface to standard ROS 2 CLI tools ("ros2 bag list" or "ros2 bag info"), as well as the ability to filter bags with a subset of topics or trimmed in time. Coupled with the MCP server, we provide a lightweight UI that allows the benchmarking of the tooling with different LLMs, both proprietary (Anthropic, OpenAI) and open-source (through Groq). Our experimental results include the analysis of tool calling capabilities of eight different state-of-the-art LLM/VLM models, both proprietary and open-source, large and small. Our experiments indicate that there is a large divide in tool calling capabilities, with Kimi K2 and Claude Sonnet 4 demonstrating clearly superior performance. We also conclude that there are multiple factors affecting the success rates, from the tool description schema to the number of arguments, as well as the number of tools available to the models. The code is available with a permissive license at https://github.com/binabik-ai/mcp-rosbags.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Theoretical Framework for Environmental Similarity and Vessel Mobility as Coupled Predictors of Marine Invasive Species Pathways</title>
<link>https://arxiv.org/abs/2511.03499</link>
<guid>https://arxiv.org/abs/2511.03499</guid>
<content:encoded><![CDATA[
arXiv:2511.03499v1 Announce Type: cross 
Abstract: Marine invasive species spread through global shipping and generate substantial ecological and economic impacts. Traditional risk assessments require detailed records of ballast water and traffic patterns, which are often incomplete, limiting global coverage. This work advances a theoretical framework that quantifies invasion risk by combining environmental similarity across ports with observed and forecasted maritime mobility. Climate-based feature representations characterize each port's marine conditions, while mobility networks derived from Automatic Identification System data capture vessel flows and potential transfer pathways. Clustering and metric learning reveal climate analogues and enable the estimation of species survival likelihood along shipping routes. A temporal link prediction model captures how traffic patterns may change under shifting environmental conditions. The resulting fusion of environmental similarity and predicted mobility provides exposure estimates at the port and voyage levels, supporting targeted monitoring, routing adjustments, and management interventions.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Neural Networks with Discrete Cosine Transform Activations</title>
<link>https://arxiv.org/abs/2511.03531</link>
<guid>https://arxiv.org/abs/2511.03531</guid>
<content:encoded><![CDATA[
arXiv:2511.03531v1 Announce Type: cross 
Abstract: In this paper, we extend our previous work on the Expressive Neural Network (ENN), a multilayer perceptron with adaptive activation functions parametrized using the Discrete Cosine Transform (DCT). Building upon previous work that demonstrated the strong expressiveness of ENNs with compact architectures, we now emphasize their efficiency, interpretability and pruning capabilities. The DCT-based parameterization provides a structured and decorrelated representation that reveals the functional role of each neuron and allows direct identification of redundant components. Leveraging this property, we propose an efficient pruning strategy that removes unnecessary DCT coefficients with negligible or no loss in performance. Experimental results across classification and implicit neural representation tasks confirm that ENNs achieve state-of-the-art accuracy while maintaining a low number of parameters. Furthermore, up to 40% of the activation coefficients can be safely pruned, thanks to the orthogonality and bounded nature of the DCT basis. Overall, these findings demonstrate that the ENN framework offers a principled integration of signal processing concepts into neural network design, achieving a balanced trade-off between expressiveness, compactness, and interpretability.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SOLVE-Med: Specialized Orchestration for Leading Vertical Experts across Medical Specialties</title>
<link>https://arxiv.org/abs/2511.03542</link>
<guid>https://arxiv.org/abs/2511.03542</guid>
<content:encoded><![CDATA[
arXiv:2511.03542v1 Announce Type: cross 
Abstract: Medical question answering systems face deployment challenges including hallucinations, bias, computational demands, privacy concerns, and the need for specialized expertise across diverse domains. Here, we present SOLVE-Med, a multi-agent architecture combining domain-specialized small language models for complex medical queries. The system employs a Router Agent for dynamic specialist selection, ten specialized models (1B parameters each) fine-tuned on specific medical domains, and an Orchestrator Agent that synthesizes responses. Evaluated on Italian medical forum data across ten specialties, SOLVE-Med achieves superior performance with ROUGE-1 of 0.301 and BERTScore F1 of 0.697, outperforming standalone models up to 14B parameters while enabling local deployment. Our code is publicly available on GitHub: https://github.com/PRAISELab-PicusLab/SOLVE-Med.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncovering Code Insights: Leveraging GitHub Artifacts for Deeper Code Understanding</title>
<link>https://arxiv.org/abs/2511.03549</link>
<guid>https://arxiv.org/abs/2511.03549</guid>
<content:encoded><![CDATA[
arXiv:2511.03549v1 Announce Type: cross 
Abstract: Understanding the purpose of source code is a critical task in software maintenance, onboarding, and modernization. While large language models (LLMs) have shown promise in generating code explanations, they often lack grounding in the broader software engineering context. We propose a novel approach that leverages natural language artifacts from GitHub -- such as pull request descriptions, issue descriptions and discussions, and commit messages -- to enhance LLM-based code understanding. Our system consists of three components: one that extracts and structures relevant GitHub context, another that uses this context to generate high-level explanations of the code's purpose, and a third that validates the explanation. We implemented this as a standalone tool, as well as a server within the Model Context Protocol (MCP), enabling integration with other AI-assisted development tools. Our main use case is that of enhancing a standard LLM-based code explanation with code insights that our system generates. To evaluate explanations' quality, we conducted a small scale user study, with developers of several open projects, as well as developers of proprietary projects. Our user study indicates that when insights are generated they often are helpful and non trivial, and are free from hallucinations.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MultiZebraLogic: A Multilingual Logical Reasoning Benchmark</title>
<link>https://arxiv.org/abs/2511.03553</link>
<guid>https://arxiv.org/abs/2511.03553</guid>
<content:encoded><![CDATA[
arXiv:2511.03553v1 Announce Type: cross 
Abstract: Measuring the full abilities of large language models (LLMs) requires benchmarks representing multiple tasks. We aim to create large, high-quality datasets for comparison of logical reasoning skills across several languages and of suitable difficulty for LLMs of various reasoning ability. We explore multiple ways of increasing difficulty. We generate zebra puzzles in multiple languages, themes, sizes and including 14 different clue types and 8 red herring types (uninformative clues). We find puzzle sizes 2x3 and 4x5 are sufficiently challenging for GPT-4o mini (a non-reasoning model) and o3-mini (a reasoning model), respectively. Including 5 red herrings decreases o3-mini puzzle-level accuracy on 4x5 puzzles by 15$\pm$7 %. Scores of o3-mini on 4x5 puzzles are not significantly affected by use of English vs. Danish or the common houses theme vs. the country-specific smoerrebroed theme. We find no correlation between difficulty and the selected clue types. Datasets of 128+1024 puzzles are published as MultiZebraLogic in each of nine Germanic languages for sizes 2x3 and 4x5. We publish code for puzzle generation, designed for adaptablity into more languages and themes.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AILA--First Experiments with Localist Language Models</title>
<link>https://arxiv.org/abs/2511.03559</link>
<guid>https://arxiv.org/abs/2511.03559</guid>
<content:encoded><![CDATA[
arXiv:2511.03559v1 Announce Type: cross 
Abstract: This paper presents the first empirical demonstration of controllable locality in transformer language models, a novel architectural framework that enables continuous control over the degree of representation localization through a tunable locality dial parameter. Unlike traditional language models that rely exclusively on distributed representations, our approach allows dynamic interpolation between highly interpretable localist encodings and efficient distributed representations without requiring model retraining. We conducted experiments on the WikiText corpus using a two-layer transformer architecture, systematically varying the locality parameter {\lambda} across the full spectrum from 1.0 (fully localist) to 0.0 (fully distributed). Our results demonstrate that localist configurations achieve dramatically lower attention entropy, with {\lambda} = 1.0 yielding 5.36 bits compared to 7.18 bits at {\lambda} = 0.0, while maintaining substantially higher pointer fidelity scores reflecting stronger alignment with rule-specified targets. Prediction experiments reveal that intermediate locality values optimize the tradeoff between interpretability and performance, with {\lambda} = 0.6 achieving test perplexity of 4.65 and accuracy of 84.7%. These findings establish that localist language models provide a practical framework for applications in regulated domains requiring both transparency and capability, offering precise mathematical control over the interpretability-performance spectrum through explicit penalty thresholds and information-theoretic design principles.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Imitation Learning in the Deep Learning Era: A Novel Taxonomy and Recent Advances</title>
<link>https://arxiv.org/abs/2511.03565</link>
<guid>https://arxiv.org/abs/2511.03565</guid>
<content:encoded><![CDATA[
arXiv:2511.03565v1 Announce Type: cross 
Abstract: Imitation learning (IL) enables agents to acquire skills by observing and replicating the behavior of one or multiple experts. In recent years, advances in deep learning have significantly expanded the capabilities and scalability of imitation learning across a range of domains, where expert data can range from full state-action trajectories to partial observations or unlabeled sequences. Alongside this growth, novel approaches have emerged, with new methodologies being developed to address longstanding challenges such as generalization, covariate shift, and demonstration quality. In this survey, we review the latest advances in imitation learning research, highlighting recent trends, methodological innovations, and practical applications. We propose a novel taxonomy that is distinct from existing categorizations to better reflect the current state of the IL research stratum and its trends. Throughout the survey, we critically examine the strengths, limitations, and evaluation practices of representative works, and we outline key challenges and open directions for future research.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-User Personalisation in Human-Robot Interaction: Using Quantitative Bipolar Argumentation Frameworks for Preferences Conflict Resolution</title>
<link>https://arxiv.org/abs/2511.03576</link>
<guid>https://arxiv.org/abs/2511.03576</guid>
<content:encoded><![CDATA[
arXiv:2511.03576v1 Announce Type: cross 
Abstract: While personalisation in Human-Robot Interaction (HRI) has advanced significantly, most existing approaches focus on single-user adaptation, overlooking scenarios involving multiple stakeholders with potentially conflicting preferences. To address this, we propose the Multi-User Preferences Quantitative Bipolar Argumentation Framework (MUP-QBAF), a novel multi-user personalisation framework based on Quantitative Bipolar Argumentation Frameworks (QBAFs) that explicitly models and resolves multi-user preference conflicts. Unlike prior work in Argumentation Frameworks, which typically assumes static inputs, our approach is tailored to robotics: it incorporates both users' arguments and the robot's dynamic observations of the environment, allowing the system to adapt over time and respond to changing contexts. Preferences, both positive and negative, are represented as arguments whose strength is recalculated iteratively based on new information. The framework's properties and capabilities are presented and validated through a realistic case study, where an assistive robot mediates between the conflicting preferences of a caregiver and a care recipient during a frailty assessment task. This evaluation further includes a sensitivity analysis of argument base scores, demonstrating how preference outcomes can be shaped by user input and contextual observations. By offering a transparent, structured, and context-sensitive approach to resolving competing user preferences, this work advances the field of multi-user HRI. It provides a principled alternative to data-driven methods, enabling robots to navigate conflicts in real-world environments.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Under Laws: A Constraint-Projected Neural PDE Solver that Eliminates Hallucinations</title>
<link>https://arxiv.org/abs/2511.03578</link>
<guid>https://arxiv.org/abs/2511.03578</guid>
<content:encoded><![CDATA[
arXiv:2511.03578v1 Announce Type: cross 
Abstract: Neural networks can approximate solutions to partial differential equations, but they often break the very laws they are meant to model-creating mass from nowhere, drifting shocks, or violating conservation and entropy. We address this by training within the laws of physics rather than beside them. Our framework, called Constraint-Projected Learning (CPL), keeps every update physically admissible by projecting network outputs onto the intersection of constraint sets defined by conservation, Rankine-Hugoniot balance, entropy, and positivity. The projection is differentiable and adds only about 10% computational overhead, making it fully compatible with back-propagation. We further stabilize training with total-variation damping (TVD) to suppress small oscillations and a rollout curriculum that enforces consistency over long prediction horizons. Together, these mechanisms eliminate both hard and soft violations: conservation holds at machine precision, total-variation growth vanishes, and entropy and error remain bounded. On Burgers and Euler systems, CPL produces stable, physically lawful solutions without loss of accuracy. Instead of hoping neural solvers will respect physics, CPL makes that behavior an intrinsic property of the learning process.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PerfDojo: Automated ML Library Generation for Heterogeneous Architectures</title>
<link>https://arxiv.org/abs/2511.03586</link>
<guid>https://arxiv.org/abs/2511.03586</guid>
<content:encoded><![CDATA[
arXiv:2511.03586v1 Announce Type: cross 
Abstract: The increasing complexity of machine learning models and the proliferation of diverse hardware architectures (CPUs, GPUs, accelerators) make achieving optimal performance a significant challenge. Heterogeneity in instruction sets, specialized kernel requirements for different data types and model features (e.g., sparsity, quantization), and architecture-specific optimizations complicate performance tuning. Manual optimization is resource-intensive, while existing automatic approaches often rely on complex hardware-specific heuristics and uninterpretable intermediate representations, hindering performance portability. We introduce PerfLLM, a novel automatic optimization methodology leveraging Large Language Models (LLMs) and Reinforcement Learning (RL). Central to this is PerfDojo, an environment framing optimization as an RL game using a human-readable, mathematically-inspired code representation that guarantees semantic validity through transformations. This allows effective optimization without prior hardware knowledge, facilitating both human analysis and RL agent training. We demonstrate PerfLLM's ability to achieve significant performance gains across diverse CPU (x86, Arm, RISC-V) and GPU architectures.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Step-Audio-EditX Technical Report</title>
<link>https://arxiv.org/abs/2511.03601</link>
<guid>https://arxiv.org/abs/2511.03601</guid>
<content:encoded><![CDATA[
arXiv:2511.03601v1 Announce Type: cross 
Abstract: We present Step-Audio-EditX, the first open-source LLM-based audio model excelling at expressive and iterative audio editing encompassing emotion, speaking style, and paralinguistics alongside robust zero-shot text-to-speech (TTS) capabilities.Our core innovation lies in leveraging only large-margin synthetic data, which circumvents the need for embedding-based priors or auxiliary modules. This large-margin learning approach enables both iterative control and high expressivity across voices, and represents a fundamental pivot from the conventional focus on representation-level disentanglement. Evaluation results demonstrate that Step-Audio-EditX surpasses both MiniMax-2.6-hd and Doubao-Seed-TTS-2.0 in emotion editing and other fine-grained control tasks.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visualization Biases MLLM's Decision Making in Network Data Tasks</title>
<link>https://arxiv.org/abs/2511.03617</link>
<guid>https://arxiv.org/abs/2511.03617</guid>
<content:encoded><![CDATA[
arXiv:2511.03617v1 Announce Type: cross 
Abstract: We evaluate how visualizations can influence the judgment of MLLMs about the presence or absence of bridges in a network. We show that the inclusion of visualization improves confidence over a structured text-based input that could theoretically be helpful for answering the question. On the other hand, we observe that standard visualization techniques create a strong bias towards accepting or refuting the presence of a bridge -- independently of whether or not a bridge actually exists in the network. While our results indicate that the inclusion of visualization techniques can effectively influence the MLLM's judgment without compromising its self-reported confidence, they also imply that practitioners must be careful of allowing users to include visualizations in generative AI applications so as to avoid undesired hallucinations.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiveTradeBench: Seeking Real-World Alpha with Large Language Models</title>
<link>https://arxiv.org/abs/2511.03628</link>
<guid>https://arxiv.org/abs/2511.03628</guid>
<content:encoded><![CDATA[
arXiv:2511.03628v1 Announce Type: cross 
Abstract: Large language models (LLMs) achieve strong performance across benchmarks--from knowledge quizzes and math reasoning to web-agent tasks--but these tests occur in static settings, lacking real dynamics and uncertainty. Consequently, they evaluate isolated reasoning or problem-solving rather than decision-making under uncertainty. To address this, we introduce LiveTradeBench, a live trading environment for evaluating LLM agents in realistic and evolving markets. LiveTradeBench follows three design principles: (i) Live data streaming of market prices and news, eliminating dependence on offline backtesting and preventing information leakage while capturing real-time uncertainty; (ii) a portfolio-management abstraction that extends control from single-asset actions to multi-asset allocation, integrating risk management and cross-asset reasoning; and (iii) multi-market evaluation across structurally distinct environments--U.S. stocks and Polymarket prediction markets--differing in volatility, liquidity, and information flow. At each step, an agent observes prices, news, and its portfolio, then outputs percentage allocations that balance risk and return. Using LiveTradeBench, we run 50-day live evaluations of 21 LLMs across families. Results show that (1) high LMArena scores do not imply superior trading outcomes; (2) models display distinct portfolio styles reflecting risk appetite and reasoning dynamics; and (3) some LLMs effectively leverage live signals to adapt decisions. These findings expose a gap between static evaluation and real-world competence, motivating benchmarks that test sequential decision making and consistency under live uncertainty.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Watermarking Large Language Models in Europe: Interpreting the AI Act in Light of Technology</title>
<link>https://arxiv.org/abs/2511.03641</link>
<guid>https://arxiv.org/abs/2511.03641</guid>
<content:encoded><![CDATA[
arXiv:2511.03641v1 Announce Type: cross 
Abstract: To foster trustworthy Artificial Intelligence (AI) within the European Union, the AI Act requires providers to mark and detect the outputs of their general-purpose models. The Article 50 and Recital 133 call for marking methods that are ''sufficiently reliable, interoperable, effective and robust''. Yet, the rapidly evolving and heterogeneous landscape of watermarks for Large Language Models (LLMs) makes it difficult to determine how these four standards can be translated into concrete and measurable evaluations. Our paper addresses this challenge, anchoring the normativity of European requirements in the multiplicity of watermarking techniques. Introducing clear and distinct concepts on LLM watermarking, our contribution is threefold. (1) Watermarking Categorisation: We propose an accessible taxonomy of watermarking methods according to the stage of the LLM lifecycle at which they are applied - before, during, or after training, and during next-token distribution or sampling. (2) Watermarking Evaluation: We interpret the EU AI Act's requirements by mapping each criterion with state-of-the-art evaluations on robustness and detectability of the watermark, and of quality of the LLM. Since interoperability remains largely untheorised in LLM watermarking research, we propose three normative dimensions to frame its assessment. (3) Watermarking Comparison: We compare current watermarking methods for LLMs against the operationalised European criteria and show that no approach yet satisfies all four standards. Encouraged by emerging empirical tests, we recommend further research into watermarking directly embedded within the low-level architecture of LLMs.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explaining Human Choice Probabilities with Simple Vector Representations</title>
<link>https://arxiv.org/abs/2511.03643</link>
<guid>https://arxiv.org/abs/2511.03643</guid>
<content:encoded><![CDATA[
arXiv:2511.03643v1 Announce Type: cross 
Abstract: When people pursue rewards in stochastic environments, they often match their choice frequencies to the observed target frequencies, even when this policy is demonstrably sub-optimal. We used a ``hide and seek'' task to evaluate this behavior under conditions where pursuit (seeking) could be toggled to avoidance (hiding), while leaving the probability distribution fixed, or varying complexity by changing the number of possible choices. We developed a model for participant choice built from choice frequency histograms treated as vectors. We posited the existence of a probability antimatching strategy for avoidance (hiding) rounds, and formalized this as a vector reflection of probability matching. We found that only two basis policies: matching/antimatching and maximizing/minimizing were sufficient to account for participant choices across a range of room numbers and opponent probability distributions. This schema requires only that people have the ability to remember the relative frequency of the different outcomes. With this knowledge simple operations can construct the maximizing and minimizing policies as well as matching and antimatching strategies. A mixture of these two policies captures human choice patterns in a stochastic environment.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChiMDQA: Towards Comprehensive Chinese Document QA with Fine-grained Evaluation</title>
<link>https://arxiv.org/abs/2511.03656</link>
<guid>https://arxiv.org/abs/2511.03656</guid>
<content:encoded><![CDATA[
arXiv:2511.03656v1 Announce Type: cross 
Abstract: With the rapid advancement of natural language processing (NLP) technologies, the demand for high-quality Chinese document question-answering datasets is steadily growing. To address this issue, we present the Chinese Multi-Document Question Answering Dataset(ChiMDQA), specifically designed for downstream business scenarios across prevalent domains including academic, education, finance, law, medical treatment, and news. ChiMDQA encompasses long-form documents from six distinct fields, consisting of 6,068 rigorously curated, high-quality question-answer (QA) pairs further classified into ten fine-grained categories. Through meticulous document screening and a systematic question-design methodology, the dataset guarantees both diversity and high quality, rendering it applicable to various NLP tasks such as document comprehension, knowledge extraction, and intelligent QA systems. Additionally, this paper offers a comprehensive overview of the dataset's design objectives, construction methodologies, and fine-grained evaluation system, supplying a substantial foundation for future research and practical applications in Chinese QA. The code and data are available at: https://anonymous.4open.science/r/Foxit-CHiMDQA/.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DQN Performance with Epsilon Greedy Policies and Prioritized Experience Replay</title>
<link>https://arxiv.org/abs/2511.03670</link>
<guid>https://arxiv.org/abs/2511.03670</guid>
<content:encoded><![CDATA[
arXiv:2511.03670v1 Announce Type: cross 
Abstract: We present a detailed study of Deep Q-Networks in finite environments, emphasizing the impact of epsilon-greedy exploration schedules and prioritized experience replay. Through systematic experimentation, we evaluate how variations in epsilon decay schedules affect learning efficiency, convergence behavior, and reward optimization. We investigate how prioritized experience replay leads to faster convergence and higher returns and show empirical results comparing uniform, no replay, and prioritized strategies across multiple simulations. Our findings illuminate the trade-offs and interactions between exploration strategies and memory management in DQN training, offering practical recommendations for robust reinforcement learning in resource-constrained settings.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Whisper Leak: a side-channel attack on Large Language Models</title>
<link>https://arxiv.org/abs/2511.03675</link>
<guid>https://arxiv.org/abs/2511.03675</guid>
<content:encoded><![CDATA[
arXiv:2511.03675v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly deployed in sensitive domains including healthcare, legal services, and confidential communications, where privacy is paramount. This paper introduces Whisper Leak, a side-channel attack that infers user prompt topics from encrypted LLM traffic by analyzing packet size and timing patterns in streaming responses. Despite TLS encryption protecting content, these metadata patterns leak sufficient information to enable topic classification. We demonstrate the attack across 28 popular LLMs from major providers, achieving near-perfect classification (often >98% AUPRC) and high precision even at extreme class imbalance (10,000:1 noise-to-target ratio). For many models, we achieve 100% precision in identifying sensitive topics like "money laundering" while recovering 5-20% of target conversations. This industry-wide vulnerability poses significant risks for users under network surveillance by ISPs, governments, or local adversaries. We evaluate three mitigation strategies - random padding, token batching, and packet injection - finding that while each reduces attack effectiveness, none provides complete protection. Through responsible disclosure, we have collaborated with providers to implement initial countermeasures. Our findings underscore the need for LLM providers to address metadata leakage as AI systems handle increasingly sensitive information.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structured Matrix Scaling for Multi-Class Calibration</title>
<link>https://arxiv.org/abs/2511.03685</link>
<guid>https://arxiv.org/abs/2511.03685</guid>
<content:encoded><![CDATA[
arXiv:2511.03685v1 Announce Type: cross 
Abstract: Post-hoc recalibration methods are widely used to ensure that classifiers provide faithful probability estimates. We argue that parametric recalibration functions based on logistic regression can be motivated from a simple theoretical setting for both binary and multiclass classification. This insight motivates the use of more expressive calibration methods beyond standard temperature scaling. For multi-class calibration however, a key challenge lies in the increasing number of parameters introduced by more complex models, often coupled with limited calibration data, which can lead to overfitting. Through extensive experiments, we demonstrate that the resulting bias-variance tradeoff can be effectively managed by structured regularization, robust preprocessing and efficient optimization. The resulting methods lead to substantial gains over existing logistic-based calibration techniques. We provide efficient and easy-to-use open-source implementations of our methods, making them an attractive alternative to common temperature, vector, and matrix scaling implementations.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The OpenHands Software Agent SDK: A Composable and Extensible Foundation for Production Agents</title>
<link>https://arxiv.org/abs/2511.03690</link>
<guid>https://arxiv.org/abs/2511.03690</guid>
<content:encoded><![CDATA[
arXiv:2511.03690v1 Announce Type: cross 
Abstract: Agents are now used widely in the process of software development, but building production-ready software engineering agents is a complex task. Deploying software agents effectively requires flexibility in implementation and experimentation, reliable and secure execution, and interfaces for users to interact with agents. In this paper, we present the OpenHands Software Agent SDK, a toolkit for implementing software development agents that satisfy these desiderata. This toolkit is a complete architectural redesign of the agent components of the popular OpenHands framework for software development agents, which has 64k+ GitHub stars. To achieve flexibility, we design a simple interface for implementing agents that requires only a few lines of code in the default case, but is easily extensible to more complex, full-featured agents with features such as custom tools, memory management, and more. For security and reliability, it delivers seamless local-to-remote execution portability, integrated REST/WebSocket services. For interaction with human users, it can connect directly to a variety of interfaces, such as visual workspaces (VS Code, VNC, browser), command-line interfaces, and APIs. Compared with existing SDKs from OpenAI, Claude, and Google, OpenHands uniquely integrates native sandboxed execution, lifecycle control, model-agnostic multi-LLM routing, and built-in security analysis. Empirical results on SWE-Bench Verified and GAIA benchmarks demonstrate strong performance. Put together, these elements allow the OpenHands Software Agent SDK to provide a practical foundation for prototyping, unlocking new classes of custom applications, and reliably deploying agents at scale.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AnaFlow: Agentic LLM-based Workflow for Reasoning-Driven Explainable and Sample-Efficient Analog Circuit Sizing</title>
<link>https://arxiv.org/abs/2511.03697</link>
<guid>https://arxiv.org/abs/2511.03697</guid>
<content:encoded><![CDATA[
arXiv:2511.03697v1 Announce Type: cross 
Abstract: Analog/mixed-signal circuits are key for interfacing electronics with the physical world. Their design, however, remains a largely handcrafted process, resulting in long and error-prone design cycles. While the recent rise of AI-based reinforcement learning and generative AI has created new techniques to automate this task, the need for many time-consuming simulations is a critical bottleneck hindering the overall efficiency. Furthermore, the lack of explainability of the resulting design solutions hampers widespread adoption of the tools. To address these issues, a novel agentic AI framework for sample-efficient and explainable analog circuit sizing is presented. It employs a multi-agent workflow where specialized Large Language Model (LLM)-based agents collaborate to interpret the circuit topology, to understand the design goals, and to iteratively refine the circuit's design parameters towards the target goals with human-interpretable reasoning. The adaptive simulation strategy creates an intelligent control that yields a high sample efficiency. The AnaFlow framework is demonstrated for two circuits of varying complexity and is able to complete the sizing task fully automatically, differently from pure Bayesian optimization and reinforcement learning approaches. The system learns from its optimization history to avoid past mistakes and to accelerate convergence. The inherent explainability makes this a powerful tool for analog design space exploration and a new paradigm in analog EDA, where AI agents serve as transparent design assistants.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist Annotation Scheme for MapTask</title>
<link>https://arxiv.org/abs/2511.03718</link>
<guid>https://arxiv.org/abs/2511.03718</guid>
<content:encoded><![CDATA[
arXiv:2511.03718v1 Announce Type: cross 
Abstract: Collaborative dialogue relies on participants incrementally establishing common ground, yet in asymmetric settings they may believe they agree while referring to different entities. We introduce a perspectivist annotation scheme for the HCRC MapTask corpus (Anderson et al., 1991) that separately captures speaker and addressee grounded interpretations for each reference expression, enabling us to trace how understanding emerges, diverges, and repairs over time. Using a scheme-constrained LLM annotation pipeline, we obtain 13k annotated reference expressions with reliability estimates and analyze the resulting understanding states. The results show that full misunderstandings are rare once lexical variants are unified, but multiplicity discrepancies systematically induce divergences, revealing how apparent grounding can mask referential misalignment. Our framework provides both a resource and an analytic lens for studying grounded misunderstanding and for evaluating (V)LLMs' capacity to model perspective-dependent grounding in collaborative dialogue.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Single Pass, Looping Through Time: KG-IRAG with Iterative Knowledge Retrieval</title>
<link>https://arxiv.org/abs/2503.14234</link>
<guid>https://arxiv.org/abs/2503.14234</guid>
<content:encoded><![CDATA[
arXiv:2503.14234v4 Announce Type: replace 
Abstract: Graph Retrieval-Augmented Generation (GraphRAG) has proven highly effective in enhancing the performance of Large Language Models (LLMs) on tasks that require external knowledge. By leveraging Knowledge Graphs (KGs), GraphRAG improves information retrieval for complex reasoning tasks, providing more precise and comprehensive retrieval and generating more accurate responses to QAs. However, most RAG methods fall short in addressing multi-step reasoning, particularly when both information extraction and inference are necessary. To address this limitation, this paper presents Knowledge Graph-Based Iterative Retrieval-Augmented Generation (KG-IRAG), a novel framework that integrates KGs with iterative reasoning to improve LLMs' ability to handle queries involving temporal and logical dependencies. Through iterative retrieval steps, KG-IRAG incrementally gathers relevant data from external KGs, enabling step-by-step reasoning. The proposed approach is particularly suited for scenarios where reasoning is required alongside dynamic temporal data extraction, such as determining optimal travel times based on weather conditions or traffic patterns. Experimental results show that KG-IRAG improves accuracy in complex reasoning tasks by effectively integrating external knowledge with iterative, logic-based retrieval. Additionally, three new datasets: weatherQA-Irish, weatherQA-Sydney, and trafficQA-TFNSW, are formed to evaluate KG-IRAG's performance, demonstrating its potential beyond traditional RAG applications.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TAMO: Fine-Grained Root Cause Analysis via Tool-Assisted LLM Agent with Multi-Modality Observation Data in Cloud-Native Systems</title>
<link>https://arxiv.org/abs/2504.20462</link>
<guid>https://arxiv.org/abs/2504.20462</guid>
<content:encoded><![CDATA[
arXiv:2504.20462v5 Announce Type: replace 
Abstract: Implementing large language models (LLMs)-driven root cause analysis (RCA) in cloud-native systems has become a key topic of modern software operations and maintenance. However, existing LLM-based approaches face three key challenges: multi-modality input constraint, context window limitation, and dynamic dependence graph. To address these issues, we propose a tool-assisted LLM agent with multi-modality observation data for fine-grained RCA, namely TAMO, including multimodality alignment tool, root cause localization tool, and fault types classification tool. In detail, TAMO unifies multi-modal observation data into time-aligned representations for cross-modal feature consistency. Based on the unified representations, TAMO then invokes its specialized root cause localization tool and fault types classification tool for further identifying root cause and fault type underlying system context. This approach overcomes the limitations of LLMs in processing real-time raw observational data and dynamic service dependencies, guiding the model to generate repair strategies that align with system context through structured prompt design. Experiments on two benchmark datasets demonstrate that TAMO outperforms state-of-the-art (SOTA) approaches with comparable performance.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging LLMs to Automate Energy-Aware Refactoring of Parallel Scientific Codes</title>
<link>https://arxiv.org/abs/2505.02184</link>
<guid>https://arxiv.org/abs/2505.02184</guid>
<content:encoded><![CDATA[
arXiv:2505.02184v2 Announce Type: replace 
Abstract: While large language models (LLMs) are increasingly used for generating parallel scientific codes, most efforts emphasize functional correctness, often overlooking performance, especially energy efficiency. We propose LASSI-EE, an automated LLM-based refactoring framework that generates energy-efficient parallel codes through a multi-stage, iterative approach integrating runtime power profiling, energy-aware prompting, self-correcting feedback loops, and an LLM-as-a-Judge agent for automated screening of code solutions. We introduce energy-reduction@k, a novel metric that quantifies expected energy reduction when generating k code candidates and selecting the most energy-efficient, enabling systematic evaluation of multi-attempt generation strategies. Evaluating 20 HeCBench applications and two miniApps on NVIDIA A100 and AMD MI100 GPUs, a single run (k=1) with LASSI-EE delivers refactored parallel codes with an average 29% expected energy reduction at an 81% pass rate, representing a 2.8x improvement over vanilla LLM prompting. Multiple runs (k=3) achieve an average 48% expected energy reduction at a 97% pass rate. These results are consistent across devices, demonstrating LASSI-EE's effectiveness across diverse hardware architectures.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta-Semantics Augmented Few-Shot Relational Learning</title>
<link>https://arxiv.org/abs/2505.05684</link>
<guid>https://arxiv.org/abs/2505.05684</guid>
<content:encoded><![CDATA[
arXiv:2505.05684v4 Announce Type: replace 
Abstract: Few-shot relational learning on knowledge graph (KGs) aims to perform reasoning over relations with only a few training examples. While current methods have focused primarily on leveraging specific relational information, rich semantics inherent in KGs have been largely overlooked. To bridge this gap, we propose PromptMeta, a novel prompted meta-learning framework that seamlessly integrates meta-semantics with relational information for few-shot relational learning. PromptMeta introduces two core innovations: (1) a Meta-Semantic Prompt (MSP) pool that learns and consolidates high-level meta-semantics shared across tasks, enabling effective knowledge transfer and adaptation to newly emerging relations; and (2) a learnable fusion mechanism that dynamically combines meta-semantics with task-specific relational information tailored to different few-shot tasks. Both components are optimized jointly with model parameters within a meta-learning framework. Extensive experiments and analyses on two real-world KG benchmarks validate the effectiveness of PromptMeta in adapting to new relations with limited supervision.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Divide by Question, Conquer by Agent: SPLIT-RAG with Question-Driven Graph Partitioning</title>
<link>https://arxiv.org/abs/2505.13994</link>
<guid>https://arxiv.org/abs/2505.13994</guid>
<content:encoded><![CDATA[
arXiv:2505.13994v2 Announce Type: replace 
Abstract: Retrieval-Augmented Generation (RAG) systems empower large language models (LLMs) with external knowledge, yet struggle with efficiency-accuracy trade-offs when scaling to large knowledge graphs. Existing approaches often rely on monolithic graph retrieval, incurring unnecessary latency for simple queries and fragmented reasoning for complex multi-hop questions. To address these challenges, this paper propose SPLIT-RAG, a multi-agent RAG framework that addresses these limitations with question-driven semantic graph partitioning and collaborative subgraph retrieval. The innovative framework first create Semantic Partitioning of Linked Information, then use the Type-Specialized knowledge base to achieve Multi-Agent RAG. The attribute-aware graph segmentation manages to divide knowledge graphs into semantically coherent subgraphs, ensuring subgraphs align with different query types, while lightweight LLM agents are assigned to partitioned subgraphs, and only relevant partitions are activated during retrieval, thus reduce search space while enhancing efficiency. Finally, a hierarchical merging module resolves inconsistencies across subgraph-derived answers through logical verifications. Extensive experimental validation demonstrates considerable improvements compared to existing approaches.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>s3: You Don't Need That Much Data to Train a Search Agent via RL</title>
<link>https://arxiv.org/abs/2505.14146</link>
<guid>https://arxiv.org/abs/2505.14146</guid>
<content:encoded><![CDATA[
arXiv:2505.14146v2 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) systems empower large language models (LLMs) to access external knowledge during inference. Recent advances have enabled LLMs to act as search agents via reinforcement learning (RL), improving information acquisition through multi-turn interactions with retrieval engines. However, existing approaches either optimize retrieval using search-only metrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM to jointly reason and retrieve-entangling retrieval with generation and limiting the real search utility and compatibility with frozen or proprietary models. In this work, we propose s3, a lightweight, model-agnostic framework that decouples the searcher from the generator and trains the searcher using a Gain Beyond RAG reward: the improvement in generation accuracy over naive RAG. s3 requires only 2.4k training samples to outperform baselines trained on over 70x more data, consistently delivering stronger downstream performance across six general QA and five medical QA benchmarks.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Agent Reinforcement Learning for Autonomous Multi-Satellite Earth Observation: A Realistic Case Study</title>
<link>https://arxiv.org/abs/2506.15207</link>
<guid>https://arxiv.org/abs/2506.15207</guid>
<content:encoded><![CDATA[
arXiv:2506.15207v2 Announce Type: replace 
Abstract: The exponential growth of Low Earth Orbit (LEO) satellites has revolutionised Earth Observation (EO) missions, addressing challenges in climate monitoring, disaster management, and more. However, autonomous coordination in multi-satellite systems remains a fundamental challenge. Traditional optimisation approaches struggle to handle the real-time decision-making demands of dynamic EO missions, necessitating the use of Reinforcement Learning (RL) and Multi-Agent Reinforcement Learning (MARL). In this paper, we investigate RL-based autonomous EO mission planning by modelling single-satellite operations and extending to multi-satellite constellations using MARL frameworks. We address key challenges, including energy and data storage limitations, uncertainties in satellite observations, and the complexities of decentralised coordination under partial observability. By leveraging a near-realistic satellite simulation environment, we evaluate the training stability and performance of state-of-the-art MARL algorithms, including PPO, IPPO, MAPPO, and HAPPO. Our results demonstrate that MARL can effectively balance imaging and resource management while addressing non-stationarity and reward interdependency in multi-satellite coordination. The insights gained from this study provide a foundation for autonomous satellite operations, offering practical guidelines for improving policy learning in decentralised EO missions.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-Driven Collaborative Model for Untangling Commits via Explicit and Implicit Dependency Reasoning</title>
<link>https://arxiv.org/abs/2507.16395</link>
<guid>https://arxiv.org/abs/2507.16395</guid>
<content:encoded><![CDATA[
arXiv:2507.16395v2 Announce Type: replace 
Abstract: Atomic commits, which address a single development concern, are a best practice in software development. In practice, however, developers often produce tangled commits that mix unrelated changes, complicating code review and maintenance. Prior untangling approaches (rule-based, feature-based, or graph-based) have made progress but typically rely on shallow signals and struggle to distinguish explicit dependencies (e.g., control/data flow) from implicit ones (e.g., semantic or conceptual relationships). In this paper, we propose ColaUntangle, a new collaborative consultation framework for commit untangling that models both explicit and implicit dependencies among code changes. ColaUntangle integrates Large Language Model (LLM)-driven agents in a multi-agent architecture: one agent specializes in explicit dependencies, another in implicit ones, and a reviewer agent synthesizes their perspectives through iterative consultation. To capture structural and contextual information, we construct Explicit and Implicit Contexts, enabling agents to reason over code relationships with both symbolic and semantic depth. We evaluate ColaUntangle on two widely-used datasets (1,612 C# and 14k Java tangled commits). Experimental results show that ColaUntangle outperforms the best-performing baseline, achieving an improvement of 44% on the C# dataset and 82% on the Java dataset. These findings highlight the potential of LLM-based collaborative frameworks for advancing automated commit untangling tasks.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data Dependency-Aware Code Generation from Enhanced UML Sequence Diagrams</title>
<link>https://arxiv.org/abs/2508.03379</link>
<guid>https://arxiv.org/abs/2508.03379</guid>
<content:encoded><![CDATA[
arXiv:2508.03379v3 Announce Type: replace 
Abstract: Large language models (LLMs) excel at generating code from natural language (NL) descriptions. However, the plain textual descriptions are inherently ambiguous and often fail to capture complex requirements like intricate system behaviors, conditional logic, and architectural constraints; implicit data dependencies in service-oriented architectures are difficult to infer and handle correctly. To bridge this gap, we propose a novel step-by-step code generation framework named UML2Dep by leveraging unambiguous formal specifications of complex requirements. First, we introduce an enhanced Unified Modeling Language (UML) sequence diagram tailored for service-oriented architectures. This diagram extends traditional visual syntax by integrating decision tables and API specifications, explicitly formalizing structural relationships and business logic flows in service interactions to rigorously eliminate linguistic ambiguity. Second, recognizing the critical role of data flow, we introduce a dedicated data dependency inference (DDI) task. DDI systematically constructs an explicit data dependency graph prior to actual code synthesis. To ensure reliability, we formalize DDI as a constrained mathematical reasoning task through novel prompting strategies, aligning with LLMs' excellent mathematical strengths. Additional static parsing and dependency pruning further reduce context complexity and cognitive load associated with intricate specifications, thereby enhancing reasoning accuracy and efficiency.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcement Learning Foundations for Deep Research Systems: A Survey</title>
<link>https://arxiv.org/abs/2509.06733</link>
<guid>https://arxiv.org/abs/2509.06733</guid>
<content:encoded><![CDATA[
arXiv:2509.06733v2 Announce Type: replace 
Abstract: Deep research systems, agentic AI that solve complex, multi-step tasks by coordinating reasoning, search across the open web and user files, and tool use, are moving toward hierarchical deployments with a Planner, Coordinator, and Executors. In practice, training entire stacks end-to-end remains impractical, so most work trains a single planner connected to core tools such as search, browsing, and code. While SFT imparts protocol fidelity, it suffers from imitation and exposure biases and underuses environment feedback. Preference alignment methods such as DPO are schema and proxy-dependent, off-policy, and weak for long-horizon credit assignment and multi-objective trade-offs. A further limitation of SFT and DPO is their reliance on human defined decision points and subskills through schema design and labeled comparisons. Reinforcement learning aligns with closed-loop, tool-interaction research by optimizing trajectory-level policies, enabling exploration, recovery behaviors, and principled credit assignment, and it reduces dependence on such human priors and rater biases.
  This survey is, to our knowledge, the first dedicated to the RL foundations of deep research systems. It systematizes recent work along three axes: (i) data synthesis and curation; (ii) RL methods for agentic research covering stability, sample efficiency, long context handling, reward and credit design, multi-objective optimization, and multimodal integration; and (iii) agentic RL training systems and frameworks. We also cover agent architecture and coordination, as well as evaluation and benchmarks, including recent QA, VQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We distill recurring patterns, surface infrastructure bottlenecks, and offer practical guidance for training robust, transparent deep research agents with RL.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ForTIFAI: Fending Off Recursive Training Induced Failure for AI Model Collapse</title>
<link>https://arxiv.org/abs/2509.08972</link>
<guid>https://arxiv.org/abs/2509.08972</guid>
<content:encoded><![CDATA[
arXiv:2509.08972v4 Announce Type: replace 
Abstract: The increasing reliance on generative AI models is rapidly increasing the volume of synthetic data, with some projections suggesting that most available new data for training could be machine-generated by 2030. This shift to a mainly synthetic content presents a critical challenge: repeated training in synthetic data leads to a phenomenon known as model collapse, where model performance degrades over generations of training, eventually rendering the models ineffective. While the causes of model collapse are increasingly understood, effective mitigation strategies remain scarce. We address this challenge by leveraging a key insight: auto-regressive models tend to generate text sequences to which they assign high confidence (i.e., high log-likelihood). Based on this observation, we introduce the Truncated-Cross-Entropy (TCE) loss function. TCE mitigates collapse by selectively ignoring high-confidence tokens during training, effectively filtering out likely machine-generated artifacts from the learning process. Our experiments demonstrate that models trained with TCE not only learn effectively but also exhibit significantly increased resilience, tolerating over 2.3x more synthetic data before the onset of collapse. In addition, we provide an open-source benchmark for collapse dynamics in mixed-data settings. Our results demonstrate that confidence-aware training objectives can substantially delay collapse onset, offering a practical and generalizable tool for model robustness under synthetic-data exposure.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VoiceAgentBench: Are Voice Assistants ready for agentic tasks?</title>
<link>https://arxiv.org/abs/2510.07978</link>
<guid>https://arxiv.org/abs/2510.07978</guid>
<content:encoded><![CDATA[
arXiv:2510.07978v2 Announce Type: replace 
Abstract: Large-scale Speech Language Models (SpeechLMs) have enabled voice assistants capable of understanding natural spoken queries and performing complex tasks. However, existing speech benchmarks primarily focus on isolated capabilities such as transcription, or question-answering, and do not systematically evaluate agentic scenarios encompassing multilingual and cultural understanding, as well as adversarial robustness. To address this, we introduce VoiceAgentBench, a comprehensive benchmark designed to evaluate SpeechLMs in realistic spoken agentic settings. It comprises over 5,500 synthetic spoken queries, including dialogues grounded in Indian context, covering single-tool invocations, multi-tool workflows, multi-turn interactions, and safety evaluations. The benchmark supports English, Hindi, and 5 other Indian languages, reflecting real-world linguistic and cultural diversity. We simulate speaker variability using a novel sampling algorithm that selects audios for TTS voice conversion based on its speaker embeddings, maximizing acoustic and speaker diversity. Our evaluation measures tool selection accuracy, structural consistency, and the correctness of tool invocations, including adversarial robustness. Our experiments reveal significant gaps in contextual tool orchestration tasks, Indic generalization, and adversarial robustness, exposing critical limitations of current SpeechLMs.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PlanU: Large Language Model Reasoning through Planning under Uncertainty</title>
<link>https://arxiv.org/abs/2510.18442</link>
<guid>https://arxiv.org/abs/2510.18442</guid>
<content:encoded><![CDATA[
arXiv:2510.18442v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly being explored across a range of reasoning tasks. However, LLMs sometimes struggle with reasoning tasks under uncertainty that are relatively easy for humans, such as planning actions in stochastic environments. The adoption of LLMs for reasoning is impeded by uncertainty challenges, such as LLM uncertainty and environmental uncertainty. LLM uncertainty arises from the stochastic sampling process inherent to LLMs. Most LLM-based Decision-Making (LDM) approaches address LLM uncertainty through multiple reasoning chains or search trees. However, these approaches overlook environmental uncertainty, which leads to poor performance in environments with stochastic state transitions. Some recent LDM approaches deal with uncertainty by forecasting the probability of unknown variables. However, they are not designed for multi-step reasoning tasks that require interaction with the environment. To address uncertainty in LLM decision-making, we introduce PlanU, an LLM-based planning method that captures uncertainty within Monte Carlo Tree Search (MCTS). PlanU models the return of each node in the MCTS as a quantile distribution, which uses a set of quantiles to represent the return distribution. To balance exploration and exploitation during tree search, PlanU introduces an Upper Confidence Bounds with Curiosity (UCC) score which estimates the uncertainty of MCTS nodes. Through extensive experiments, we demonstrate the effectiveness of PlanU in LLM-based reasoning tasks under uncertainty.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Misalignment Bounty: Crowdsourcing AI Agent Misbehavior</title>
<link>https://arxiv.org/abs/2510.19738</link>
<guid>https://arxiv.org/abs/2510.19738</guid>
<content:encoded><![CDATA[
arXiv:2510.19738v2 Announce Type: replace 
Abstract: Advanced AI systems sometimes act in ways that differ from human intent. To gather clear, reproducible examples, we ran the Misalignment Bounty: a crowdsourced project that collected cases of agents pursuing unintended or unsafe goals. The bounty received 295 submissions, of which nine were awarded.
  This report explains the program's motivation and evaluation criteria, and walks through the nine winning submissions step by step.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agentic Meta-Orchestrator for Multi-task Copilots</title>
<link>https://arxiv.org/abs/2510.22781</link>
<guid>https://arxiv.org/abs/2510.22781</guid>
<content:encoded><![CDATA[
arXiv:2510.22781v2 Announce Type: replace 
Abstract: Microsoft Copilot suites serve as the universal entry point for various agents skilled in handling important tasks, ranging from assisting a customer with product purchases to detecting vulnerabilities in corporate programming code. Each agent can be powered by language models, software engineering operations, such as database retrieval, and internal \& external knowledge. The repertoire of a copilot can expand dynamically with new agents. This requires a robust orchestrator that can distribute tasks from user prompts to the right agents. In this work, we propose an Agentic Meta-orchestrator (AMO) for handling multiple tasks and scalable agents in copilot services, which can provide both natural language and action responses. We will also demonstrate the planning that leverages meta-learning, i.e., a trained decision tree model for deciding the best inference strategy among various agents/models. We showcase the effectiveness of our AMO through two production use cases: Microsoft 365 (M365) E-Commerce Copilot and code compliance copilot. M365 E-Commerce Copilot advertises Microsoft products to external customers to promote sales success. The M365 E-Commerce Copilot provides up-to-date product information and connects to multiple agents, such as relational databases and human customer support. The code compliance copilot scans the internal DevOps code to detect known and new compliance issues in pull requests (PR).
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emotion Detection From Social Media Posts</title>
<link>https://arxiv.org/abs/2302.05610</link>
<guid>https://arxiv.org/abs/2302.05610</guid>
<content:encoded><![CDATA[
arXiv:2302.05610v2 Announce Type: replace-cross 
Abstract: Over the last few years, social media has evolved into a medium for expressing personal views, emotions, and even business and political proposals, recommendations, and advertisements. We address the topic of identifying emotions from text data obtained from social media posts like Twitter in this research. We have deployed different traditional machine learning techniques such as Support Vector Machines (SVM), Naive Bayes, Decision Trees, and Random Forest, as well as deep neural network models such as LSTM, CNN, GRU, BiLSTM, BiGRU to classify these tweets into four emotion categories (Fear, Anger, Joy, and Sadness). Furthermore, we have constructed a BiLSTM and BiGRU ensemble model. The evaluation result shows that the deep neural network models(BiGRU, to be specific) produce the most promising results compared to traditional machine learning models, with an 87.53 % accuracy rate. The ensemble model performs even better (87.66 %), albeit the difference is not significant. This result will aid in the development of a decision-making tool that visualizes emotional fluctuations.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transfer Learning-based Real-time Handgun Detection</title>
<link>https://arxiv.org/abs/2311.13559</link>
<guid>https://arxiv.org/abs/2311.13559</guid>
<content:encoded><![CDATA[
arXiv:2311.13559v3 Announce Type: replace-cross 
Abstract: Traditional surveillance systems rely on human attention, limiting their effectiveness. This study employs convolutional neural networks and transfer learning to develop a real-time computer vision system for automatic handgun detection. Comprehensive analysis of online handgun detection methods is conducted, emphasizing reducing false positives and learning time. Transfer learning is demonstrated as an effective approach. Despite technical challenges, the proposed system achieves a precision rate of 84.74%, demonstrating promising performance comparable to related works, enabling faster learning and accurate automatic handgun detection for enhanced security. This research advances security measures by reducing human monitoring dependence, showcasing the potential of transfer learning-based approaches for efficient and reliable handgun detection.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Survey on AI Ethics: A Socio-technical Perspective</title>
<link>https://arxiv.org/abs/2311.17228</link>
<guid>https://arxiv.org/abs/2311.17228</guid>
<content:encoded><![CDATA[
arXiv:2311.17228v2 Announce Type: replace-cross 
Abstract: The past decade has observed a significant advancement in AI with deep learning-based models being deployed in diverse scenarios, including safety-critical applications. As these AI systems become deeply embedded in our societal infrastructure, the repercussions of their decisions and actions have significant consequences, making the ethical implications of AI deployment highly relevant and essential. The ethical concerns associated with AI are multifaceted, including challenging issues of fairness, privacy and data protection, responsibility and accountability, safety and robustness, transparency and explainability, and environmental impact. These principles together form the foundations of ethical AI considerations that concern every stakeholder in the AI system lifecycle. In light of the present ethical and future x-risk concerns, governments have shown increasing interest in establishing guidelines for the ethical deployment of AI. This work unifies the current and future ethical concerns of deploying AI into society. While we acknowledge and appreciate the technical surveys for each of the ethical principles concerned, in this paper, we aim to provide a comprehensive overview that not only addresses each principle from a technical point of view but also discusses them from a social perspective.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Physics: Using AI Libraries to Develop Physics-Based Solvers for Incompressible Computational Fluid Dynamics</title>
<link>https://arxiv.org/abs/2402.17913</link>
<guid>https://arxiv.org/abs/2402.17913</guid>
<content:encoded><![CDATA[
arXiv:2402.17913v2 Announce Type: replace-cross 
Abstract: Numerical discretisations of partial differential equations (PDEs) can be written as discrete convolutions, which, themselves, are a key tool in AI libraries and used in convolutional neural networks (CNNs). We therefore propose to implement numerical discretisations as convolutional layers of a neural network, where the weights or filters are determined analytically rather than by training. Furthermore, we demonstrate that these systems can be solved entirely by functions in AI libraries, either by using Jacobi iteration or multigrid methods, the latter realised through a U-Net architecture. Some advantages of the Neural Physics approach are that (1) the methods are platform agnostic; (2) the resulting solvers are fully differentiable, ideal for optimisation tasks; and (3) writing CFD solvers as (untrained) neural networks means that they can be seamlessly integrated with trained neural networks to form hybrid models. We demonstrate the proposed approach on a number of test cases of increasing complexity from advection-diffusion problems, the non-linear Burgers equation to the Navier-Stokes equations. We validate the approach by comparing our results with solutions obtained from traditionally written code and common benchmarks from the literature. We show that the proposed methodology can solve all these problems using repurposed AI libraries in an efficient way, without training, and presents a new avenue to explore in the development of methods to solve PDEs with implicit methods.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey of Graph Neural Networks in Real world: Imbalance, Noise, Privacy and OOD Challenges</title>
<link>https://arxiv.org/abs/2403.04468</link>
<guid>https://arxiv.org/abs/2403.04468</guid>
<content:encoded><![CDATA[
arXiv:2403.04468v2 Announce Type: replace-cross 
Abstract: Graph-structured data exhibits universality and widespread applicability across diverse domains, such as social network analysis, biochemistry, financial fraud detection, and network security. Significant strides have been made in leveraging Graph Neural Networks (GNNs) to achieve remarkable success in these areas. However, in real-world scenarios, the training environment for models is often far from ideal, leading to substantial performance degradation of GNN models due to various unfavorable factors, including imbalance in data distribution, the presence of noise in erroneous data, privacy protection of sensitive information, and generalization capability for out-of-distribution (OOD) scenarios. To tackle these issues, substantial efforts have been devoted to improving the performance of GNN models in practical real-world scenarios, as well as enhancing their reliability and robustness. In this paper, we present a comprehensive survey that systematically reviews existing GNN models, focusing on solutions to the four mentioned real-world challenges including imbalance, noise, privacy, and OOD in practical scenarios that many existing reviews have not considered. Specifically, we first highlight the four key challenges faced by existing GNNs, paving the way for our exploration of real-world GNN models. Subsequently, we provide detailed discussions on these four aspects, dissecting how these solutions contribute to enhancing the reliability and robustness of GNN models. Last but not least, we outline promising directions and offer future perspectives in the field.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Reliable Cryptographic Framework for Empirical Machine Unlearning Evaluation</title>
<link>https://arxiv.org/abs/2404.11577</link>
<guid>https://arxiv.org/abs/2404.11577</guid>
<content:encoded><![CDATA[
arXiv:2404.11577v4 Announce Type: replace-cross 
Abstract: Machine unlearning updates machine learning models to remove information from specific training samples, complying with data protection regulations that allow individuals to request the removal of their personal data. Despite the recent development of numerous unlearning algorithms, reliable evaluation of these algorithms remains an open research question. In this work, we focus on membership inference attack (MIA) based evaluation, one of the most common approaches for evaluating unlearning algorithms, and address various pitfalls of existing evaluation metrics lacking theoretical understanding and reliability. Specifically, by modeling the proposed evaluation process as a \emph{cryptographic game} between unlearning algorithms and MIA adversaries, the naturally induced evaluation metric measures the data removal efficacy of unlearning algorithms and enjoys provable guarantees that existing evaluation metrics fail to satisfy. Furthermore, we propose a practical and efficient approximation of the induced evaluation metric and demonstrate its effectiveness through both theoretical analysis and empirical experiments. Overall, this work presents a novel and reliable approach to empirically evaluating unlearning algorithms, paving the way for the development of more effective unlearning techniques.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autonomous Robotic Drilling System for Mice Cranial Window Creation</title>
<link>https://arxiv.org/abs/2406.14135</link>
<guid>https://arxiv.org/abs/2406.14135</guid>
<content:encoded><![CDATA[
arXiv:2406.14135v2 Announce Type: replace-cross 
Abstract: Robotic assistance for experimental manipulation in the life sciences is expected to enable favorable outcomes, regardless of the skill of the scientist. Experimental specimens in the life sciences are subject to individual variability and hence require intricate algorithms for successful autonomous robotic control. As a use case, we are studying the cranial window creation in mice. This operation requires the removal of an 8-mm circular patch of the skull, which is approximately 300 um thick, but the shape and thickness of the mouse skull significantly varies depending on the strain of the mouse, sex, and age. In this work, we develop an autonomous robotic drilling system with no offline planning, consisting of a trajectory planner with execution-time feedback with drilling completion level recognition based on image and force information. In the experiments, we first evaluate the image-and-force-based drilling completion level recognition by comparing it with other state-of-the-art deep learning image processing methods and conduct an ablation study in eggshell drilling to evaluate the impact of each module on system performance. Finally, the system performance is further evaluated in postmortem mice, achieving a success rate of 70% (14/20 trials) with an average drilling time of 9.3 min.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via Transformer-Guided Prototyping</title>
<link>https://arxiv.org/abs/2409.11316</link>
<guid>https://arxiv.org/abs/2409.11316</guid>
<content:encoded><![CDATA[
arXiv:2409.11316v5 Announce Type: replace-cross 
Abstract: Few-shot Semantic Segmentation addresses the challenge of segmenting objects in query images with only a handful of annotated examples. However, many previous state-of-the-art methods either have to discard intricate local semantic features or suffer from high computational complexity. To address these challenges, we propose a new Few-shot Semantic Segmentation framework based on the Transformer architecture. Our approach introduces the spatial transformer decoder and the contextual mask generation module to improve the relational understanding between support and query images. Moreover, we introduce a multi scale decoder to refine the segmentation mask by incorporating features from different resolutions in a hierarchical manner. Additionally, our approach integrates global features from intermediate encoder stages to improve contextual understanding, while maintaining a lightweight structure to reduce complexity. This balance between performance and efficiency enables our method to achieve competitive results on benchmark datasets such as PASCAL-5^i and COCO-20^i in both 1-shot and 5-shot settings. Notably, our model with only 1.5 million parameters demonstrates competitive performance while overcoming limitations of existing methodologies. https://github.com/amirrezafateh/MSDNet
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inverse Entropic Optimal Transport Solves Semi-supervised Learning via Data Likelihood Maximization</title>
<link>https://arxiv.org/abs/2410.02628</link>
<guid>https://arxiv.org/abs/2410.02628</guid>
<content:encoded><![CDATA[
arXiv:2410.02628v4 Announce Type: replace-cross 
Abstract: Learning conditional distributions $\pi^*(\cdot|x)$ is a central problem in machine learning, which is typically approached via supervised methods with paired data $(x,y) \sim \pi^*$. However, acquiring paired data samples is often challenging, especially in problems such as domain translation. This necessitates the development of $\textit{semi-supervised}$ models that utilize both limited paired data and additional unpaired i.i.d. samples $x \sim \pi^*_x$ and $y \sim \pi^*_y$ from the marginal distributions. The usage of such combined data is complex and often relies on heuristic approaches. To tackle this issue, we propose a new learning paradigm that integrates both paired and unpaired data $\textbf{seamlessly}$ using the data likelihood maximization techniques. We demonstrate that our approach also connects intriguingly with inverse entropic optimal transport (OT). This finding allows us to apply recent advances in computational OT to establish an $\textbf{end-to-end}$ learning algorithm to get $\pi^*(\cdot|x)$. In addition, we derive the universal approximation property, demonstrating that our approach can theoretically recover true conditional distributions with arbitrarily small error. Furthermore, we demonstrate through empirical tests that our method effectively learns conditional distributions using paired and unpaired data simultaneously.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mastering Contact-rich Tasks by Combining Soft and Rigid Robotics with Imitation Learning</title>
<link>https://arxiv.org/abs/2410.07787</link>
<guid>https://arxiv.org/abs/2410.07787</guid>
<content:encoded><![CDATA[
arXiv:2410.07787v3 Announce Type: replace-cross 
Abstract: Soft robots have the potential to revolutionize the use of robotic systems with their capability of establishing safe, robust, and adaptable interactions with their environment, but their precise control remains challenging. In contrast, traditional rigid robots offer high accuracy and repeatability but lack the flexibility of soft robots. We argue that combining these characteristics in a hybrid robotic platform can significantly enhance overall capabilities. This work presents a novel hybrid robotic platform that integrates a rigid manipulator with a fully developed soft arm. This system is equipped with the intelligence necessary to perform flexible and generalizable tasks through imitation learning autonomously. The physical softness and machine learning enable our platform to achieve highly generalizable skills, while the rigid components ensure precision and repeatability.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intelligent Computing Social Modeling and Methodological Innovations in Political Science in the Era of Large Language Models</title>
<link>https://arxiv.org/abs/2410.16301</link>
<guid>https://arxiv.org/abs/2410.16301</guid>
<content:encoded><![CDATA[
arXiv:2410.16301v2 Announce Type: replace-cross 
Abstract: The recent wave of artificial intelligence, epitomized by large language models (LLMs),has presented opportunities and challenges for methodological innovation in political science,sparking discussions on a potential paradigm shift in the social sciences. However, how can weunderstand the impact of LLMs on knowledge production and paradigm transformation in thesocial sciences from a comprehensive perspective that integrates technology and methodology? What are LLMs' specific applications and representative innovative methods in political scienceresearch? These questions, particularly from a practical methodological standpoint, remainunderexplored. This paper proposes the "Intelligent Computing Social Modeling" (ICSM) methodto address these issues by clarifying the critical mechanisms of LLMs. ICSM leverages thestrengths of LLMs in idea synthesis and action simulation, advancing intellectual exploration inpolitical science through "simulated social construction" and "simulation validation." Bysimulating the U.S. presidential election, this study empirically demonstrates the operationalpathways and methodological advantages of ICSM. By integrating traditional social scienceparadigms, ICSM not only enhances the quantitative paradigm's capability to apply big data toassess the impact of factors but also provides qualitative paradigms with evidence for socialmechanism discovery at the individual level, offering a powerful tool that balances interpretabilityand predictability in social science research. The findings suggest that LLMs will drivemethodological innovation in political science through integration and improvement rather thandirect substitution.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Sampling for Scalable and Expressive Graph Neural Networks on Homophilic Graphs</title>
<link>https://arxiv.org/abs/2410.16593</link>
<guid>https://arxiv.org/abs/2410.16593</guid>
<content:encoded><![CDATA[
arXiv:2410.16593v5 Announce Type: replace-cross 
Abstract: Graph Neural Networks (GNNs) excel in many graph machine learning tasks but face challenges when scaling to large networks. GNN transferability allows training on smaller graphs and applying the model to larger ones, but existing methods often rely on random subsampling, leading to disconnected subgraphs and reduced model expressivity. We propose a novel graph sampling algorithm that leverages feature homophily to preserve graph structure. By minimizing the trace of the data correlation matrix, our method better preserves the graph Laplacian trace -- a proxy for the graph connectivity -- than random sampling, while achieving lower complexity than spectral methods. Experiments on citation networks show improved performance in preserving Laplacian trace and GNN transferability compared to random sampling.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Matryoshka Pilot: Learning to Drive Black-Box LLMs with LLMs</title>
<link>https://arxiv.org/abs/2410.20749</link>
<guid>https://arxiv.org/abs/2410.20749</guid>
<content:encoded><![CDATA[
arXiv:2410.20749v3 Announce Type: replace-cross 
Abstract: Despite the impressive generative abilities of black-box large language models (LLMs), their inherent opacity hinders further advancements in capabilities such as reasoning, planning, and personalization. Existing works aim to enhance LLM capabilities via domain-specific adaptation, which require additional training on accessible model parameters, an infeasible option for black-box LLMs. To address this challenge, we introduce Matryoshka Pilot (M-Pilot), a lightweight white-box LLM controller that guides a large-scale black-box LLM generator by decomposing complex tasks into a series of intermediate outputs. Specifically, we consider the black-box LLM as an environment, with M-Pilot serving as a policy to provide intermediate guidance through prompts for driving the black-box LLM. M-Pilot is trained to pivot the outputs of the black-box LLM aligning with preferences during iterative interaction, which enables controllable multi-turn generation and self-improvement in optimizing intermediate guidance. Empirical evaluations on diverse tasks demonstrate that our method effectively enhances the capabilities of black-box LLMs in complex, long-horizon tasks. Our code is publicly available at: https://github.com/lichangh20/Matryoshka.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Automatic Factuality Metrics Measure Factuality? A Critical Evaluation</title>
<link>https://arxiv.org/abs/2411.16638</link>
<guid>https://arxiv.org/abs/2411.16638</guid>
<content:encoded><![CDATA[
arXiv:2411.16638v4 Announce Type: replace-cross 
Abstract: Modern LLMs can now produce highly readable abstractive summaries, to the point that traditional automated metrics for evaluating summary quality, such as ROUGE, have saturated. However, LLMs still sometimes introduce inaccuracies into summaries, i.e., information inconsistent with or unsupported by the corresponding source. Measuring the occurrence of these often subtle factual inconsistencies automatically has proved challenging. This in turn has motivated development of metrics intended to measure the factual consistency of generated summaries against sources. But are these approaches measuring what they purport to? Or are they mostly exploiting artifacts? In this work, we stress test a range of automatic factuality metrics, including specialized models and LLM-based prompting methods, to probe what they actually capture. Using a shallow classifier to separate ``easy'' examples for factual evaluation where surface features suffice from ``hard'' cases requiring deeper reasoning, we find that all metrics show substantial performance drops on the latter. Furthermore, some metrics are more sensitive to benign, fact-preserving edits than to factual corrections. Building on this observation, we demonstrate that most automatic factuality metrics can be gamed, i.e., their scores can be artificially inflated by appending innocuous, content-free sentences to summaries. Among the metrics tested, the prompt based ChatGPT-DA approach is the most robust and reliable. However, this comes with a notable caveat: Prompting LLMs to assess factuality may overly rely on their parametric knowledge rather than the provided reference when making judgments. Taken together, our findings call into question the reliability of current factuality metrics and prompt a broader reflection on what these metrics are truly measuring.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAG-IT: Retrieval-Augmented Instruction Tuning for Automated Financial Analysis</title>
<link>https://arxiv.org/abs/2412.08179</link>
<guid>https://arxiv.org/abs/2412.08179</guid>
<content:encoded><![CDATA[
arXiv:2412.08179v2 Announce Type: replace-cross 
Abstract: Financial analysis relies heavily on the interpretation of earnings reports to assess company performance and guide decision-making. Traditional methods for generating such analyses demand significant financial expertise and are often time-consuming. With the rapid advancement of Large Language Models (LLMs), domain-specific adaptations have emerged for financial tasks such as sentiment analysis and entity recognition. This paper introduces RAG-IT (Retrieval-Augmented Instruction Tuning), a novel framework designed to automate the generation of earnings report analyses through an LLM fine-tuned specifically for the financial domain. Our approach integrates retrieval augmentation with instruction-based fine-tuning to enhance factual accuracy, contextual relevance, and domain adaptability. We construct a comprehensive financial instruction dataset derived from extensive financial documents and earnings reports to guide the LLM's adaptation to specialized financial reasoning. Experimental results demonstrate that RAG-IT outperforms general-purpose open-source models and achieves performance comparable to commercial systems like GPT-3.5 on financial report generation tasks. This research highlights the potential of retrieval-augmented instruction tuning to streamline and elevate financial analysis automation, advancing the broader field of intelligent financial reporting.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REFA: Reference Free Alignment for multi-preference optimization</title>
<link>https://arxiv.org/abs/2412.16378</link>
<guid>https://arxiv.org/abs/2412.16378</guid>
<content:encoded><![CDATA[
arXiv:2412.16378v4 Announce Type: replace-cross 
Abstract: To mitigate reward hacking from response verbosity, modern preference optimization methods are increasingly adopting length normalization (e.g., SimPO, ORPO, LN-DPO). While effective against this bias, we demonstrate that length normalization itself introduces a failure mode: the URSLA shortcut. Here models learn to satisfy the alignment objective by prematurely truncating low-quality responses rather than learning from their semantic content. To address this, we introduce REFA, a new alignment framework that proposes probabilistic control on a structural token that controls termination. Our core innovation is a new class of regularizers that operate directly on the probability of the End-of-Sequence (EOS) token, a previously unexploited control lever. This token-level intervention provides a principled solution to the URSLA shortcut, ensuring genuine quality improvements. Furthermore, it unlocks a versatile mechanism for managing the alignment-efficiency tradeoff, enabling practitioners to fine-tune models that adhere to specific token budgets. Empirically, REFA achieves a 60.29% win rate and a 52.17% length-controlled win rate on AlpacaEval2 with Llama-3-8B-Instruct, demonstrating the power of our token-level control paradigm.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Haystack to Needle: Label Space Reduction for Zero-shot Classification</title>
<link>https://arxiv.org/abs/2502.08436</link>
<guid>https://arxiv.org/abs/2502.08436</guid>
<content:encoded><![CDATA[
arXiv:2502.08436v2 Announce Type: replace-cross 
Abstract: We present Label Space Reduction (LSR), a novel method for improving zero-shot classification performance of Large Language Models (LLMs). LSR iteratively refines the classification label space by systematically ranking and reducing candidate classes, enabling the model to concentrate on the most relevant options. By leveraging unlabeled data with the statistical learning capabilities of data-driven models, LSR dynamically optimizes the label space representation at test time. Our experiments across seven benchmarks demonstrate that LSR improves macro-F1 scores by an average of 7.0% (up to 14.2%) with Llama-3.1-70B and 3.3% (up to 11.1%) with Claude-3.5-Sonnet compared to standard zero-shot classification baselines. To reduce the computational overhead of LSR, which requires an additional LLM call at each iteration, we propose distilling the model into a probabilistic classifier, allowing for efficient inference.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Covariance Matrix: The Statistical Complexity of Private Linear Regression</title>
<link>https://arxiv.org/abs/2502.13115</link>
<guid>https://arxiv.org/abs/2502.13115</guid>
<content:encoded><![CDATA[
arXiv:2502.13115v2 Announce Type: replace-cross 
Abstract: We study the statistical complexity of private linear regression under an unknown, potentially ill-conditioned covariate distribution. Somewhat surprisingly, under privacy constraints the intrinsic complexity is \emph{not} captured by the usual covariance matrix but rather its $L_1$ analogues. Building on this insight, we establish minimax convergence rates for both the central and local privacy models and introduce an Information-Weighted Regression method that attains the optimal rates.
  As application, in private linear contextual bandits, we propose an efficient algorithm that achieves rate-optimal regret bounds of order $\sqrt{T}+\frac{1}{\alpha}$ and $\sqrt{T}/\alpha$ under joint and local $\alpha$-privacy models, respectively. Notably, our results demonstrate that joint privacy comes at almost no additional cost, addressing the open problems posed by Azize and Basu (2024).
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey on Text-Driven 360-Degree Panorama Generation</title>
<link>https://arxiv.org/abs/2502.14799</link>
<guid>https://arxiv.org/abs/2502.14799</guid>
<content:encoded><![CDATA[
arXiv:2502.14799v3 Announce Type: replace-cross 
Abstract: The advent of text-driven 360-degree panorama generation, enabling the synthesis of 360-degree panoramic images directly from textual descriptions, marks a transformative advancement in immersive visual content creation. This innovation significantly simplifies the traditionally complex process of producing such content. Recent progress in text-to-image diffusion models has accelerated the rapid development in this emerging field. This survey presents a comprehensive review of text-driven 360-degree panorama generation, offering an in-depth analysis of state-of-the-art algorithms. We extend our analysis to two closely related domains: text-driven 360-degree 3D scene generation and text-driven 360-degree panoramic video generation. Furthermore, we critically examine current limitations and propose promising directions for future research. A curated project page with relevant resources and research papers is available at https://littlewhitesea.github.io/Text-Driven-Pano-Gen/.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing the Macro and Micro Effects of Random Seeds on Fine-Tuning Large Language Models</title>
<link>https://arxiv.org/abs/2503.07329</link>
<guid>https://arxiv.org/abs/2503.07329</guid>
<content:encoded><![CDATA[
arXiv:2503.07329v2 Announce Type: replace-cross 
Abstract: The impact of random seeds in fine-tuning large language models (LLMs) has been largely overlooked despite its potential influence on model performance.In this study, we systematically evaluate the effects of random seeds on LLMs using the GLUE and SuperGLUE benchmarks. We analyze the macro-level impact through traditional metrics like accuracy and F1, calculating their mean and variance to quantify performance fluctuations. To capture the micro-level effects, we introduce a novel metric, consistency, measuring the stability of individual predictions across runs. Our experiments reveal significant variance at both macro and micro levels, underscoring the need for careful consideration of random seeds in fine-tuning and evaluation.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting semi-supervised learning in the era of foundation models</title>
<link>https://arxiv.org/abs/2503.09707</link>
<guid>https://arxiv.org/abs/2503.09707</guid>
<content:encoded><![CDATA[
arXiv:2503.09707v4 Announce Type: replace-cross 
Abstract: Semi-supervised learning (SSL) leverages abundant unlabeled data alongside limited labeled data to enhance learning. As vision foundation models (VFMs) increasingly serve as the backbone of vision applications, it remains unclear how SSL interacts with these pre-trained models. To address this gap, we develop new SSL benchmark datasets where frozen VFMs underperform and systematically evaluate representative SSL methods. We make a surprising observation: parameter-efficient fine-tuning (PEFT) using only labeled data often matches SSL performance, even without leveraging unlabeled data. This motivates us to revisit self-training, a conceptually simple SSL baseline, where we use the supervised PEFT model to pseudo-label unlabeled data for further training. To overcome the notorious issue of noisy pseudo-labels, we propose ensembling multiple PEFT approaches and VFM backbones to produce more robust pseudo-labels. Empirical results validate the effectiveness of this simple yet powerful approach, providing actionable insights into SSL with VFMs and paving the way for more scalable and practical semi-supervised learning in the era of foundation models.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SecRepoBench: Benchmarking Code Agents for Secure Code Completion in Real-World Repositories</title>
<link>https://arxiv.org/abs/2504.21205</link>
<guid>https://arxiv.org/abs/2504.21205</guid>
<content:encoded><![CDATA[
arXiv:2504.21205v2 Announce Type: replace-cross 
Abstract: This paper introduces SecRepoBench, a benchmark to evaluate code agents on secure code completion in real-world repositories. SecRepoBench has 318 code completion tasks in 27 C/C++ repositories, covering 15 CWEs. We evaluate 28 standalone LLMs and 13 code agents across 3 state-of-the-art agent frameworks using our benchmark. We find that state-of-the-art LLMs struggle with generating correct and secure code completions. However, code agents significantly outperform standalone LLMs. We show that SecRepoBench is more difficult than the prior state-of-the-art benchmark. Finally, our comprehensive analysis provides insights into potential directions for enhancing the ability of code agents to write correct and secure code in real-world repositories.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A data-driven framework for team selection in Fantasy Premier League</title>
<link>https://arxiv.org/abs/2505.02170</link>
<guid>https://arxiv.org/abs/2505.02170</guid>
<content:encoded><![CDATA[
arXiv:2505.02170v2 Announce Type: replace-cross 
Abstract: Fantasy football is a billion-dollar industry with millions of participants. Under a fixed budget, managers select squads to maximize future Fantasy Premier League (FPL) points. This study formulates lineup selection as data-driven optimization and develops deterministic and robust mixed-integer linear programs that choose the starting eleven, bench, and captain under budget, formation, and club-quota constraints (maximum three players per club). The objective is parameterized by a hybrid scoring metric that combines realized FPL points with predictions from a linear regression model trained on match-performance features identified using exploratory data analysis techniques. The study benchmarks alternative objectives and cost estimators, including simple and recency-weighted averages, exponential smoothing, autoregressive integrated moving average (ARIMA), and Monte Carlo simulation. Experiments on the 2023/24 Premier League season show that ARIMA with a constrained budget and a rolling window yields the most consistent out-of-sample performance; weighted averages and Monte Carlo are also competitive. Robust variants improve some objectives but are not uniformly superior. The framework provides transparent decision support for fantasy roster construction and extends to FPL chips, multi-week rolling-horizon transfer planning, and week-by-week dynamic captaincy.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning Warm Starts for Trajectory Optimization on the International Space Station</title>
<link>https://arxiv.org/abs/2505.05588</link>
<guid>https://arxiv.org/abs/2505.05588</guid>
<content:encoded><![CDATA[
arXiv:2505.05588v3 Announce Type: replace-cross 
Abstract: Trajectory optimization is a cornerstone of modern robot autonomy, enabling systems to compute trajectories and controls in real-time while respecting safety and physical constraints. However, it has seen limited usage in spaceflight applications due to its heavy computational demands that exceed the capability of most flight computers. In this work, we provide results on the first in-space demonstration of using machine learning-based warm starts for accelerating trajectory optimization for the Astrobee free-flying robot onboard the International Space Station (ISS). We formulate a data-driven optimal control approach that trains a neural network to learn the structure of the trajectory generation problem being solved using sequential convex programming (SCP). Onboard, this trained neural network predicts solutions for the trajectory generation problem and relies on using the SCP solver to enforce safety constraints for the system. Our trained network reduces the number of solver iterations required for convergence in cases including rotational dynamics by 60% and in cases with obstacles drawn from the training distribution of the warm start model by 50%. This work represents a significant milestone in the use of learning-based control for spaceflight applications and a stepping stone for future advances in the use of machine learning for autonomous guidance, navigation, & control.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Traversal Verification for Speculative Tree Decoding</title>
<link>https://arxiv.org/abs/2505.12398</link>
<guid>https://arxiv.org/abs/2505.12398</guid>
<content:encoded><![CDATA[
arXiv:2505.12398v2 Announce Type: replace-cross 
Abstract: Speculative decoding is a promising approach for accelerating large language models. The primary idea is to use a lightweight draft model to speculate the output of the target model for multiple subsequent timesteps, and then verify them in parallel to determine whether the drafted tokens should be accepted or rejected. To enhance acceptance rates, existing frameworks typically construct token trees containing multiple candidates in each timestep. However, their reliance on token-level verification mechanisms introduces two critical limitations: First, the probability distribution of a sequence differs from that of individual tokens, leading to suboptimal acceptance length. Second, current verification schemes begin from the root node and proceed layer by layer in a top-down manner. Once a parent node is rejected, all its child nodes should be discarded, resulting in inefficient utilization of speculative candidates. This paper introduces Traversal Verification, a novel speculative decoding algorithm that fundamentally rethinks the verification paradigm through leaf-to-root traversal. Our approach considers the acceptance of the entire token sequence from the current node to the root, and preserves potentially valid subsequences that would be prematurely discarded by existing methods. We theoretically prove that the probability distribution obtained through Traversal Verification is identical to that of the target model, guaranteeing lossless inference while achieving substantial acceleration gains. Experimental results across different large language models and multiple tasks show that our method consistently improves acceptance length and throughput over existing methods.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RoboRAN: A Unified Robotics Framework for Reinforcement Learning-Based Autonomous Navigation</title>
<link>https://arxiv.org/abs/2505.14526</link>
<guid>https://arxiv.org/abs/2505.14526</guid>
<content:encoded><![CDATA[
arXiv:2505.14526v2 Announce Type: replace-cross 
Abstract: Autonomous robots must navigate and operate in diverse environments, from terrestrial and aquatic settings to aerial and space domains. While Reinforcement Learning (RL) has shown promise in training policies for specific autonomous robots, existing frameworks and benchmarks are often constrained to unique platforms, limiting generalization and fair comparisons across different mobility systems. In this paper, we present a multi-domain framework for training, evaluating and deploying RL-based navigation policies across diverse robotic platforms and operational environments. Our work presents four key contributions: (1) a scalable and modular framework, facilitating seamless robot-task interchangeability and reproducible training pipelines; (2) sim-to-real transfer demonstrated through real-world experiments with multiple robots, including a satellite robotic simulator, an unmanned surface vessel, and a wheeled ground vehicle; (3) the release of the first open-source API for deploying Isaac Lab-trained policies to real robots, enabling lightweight inference and rapid field validation; and (4) uniform tasks and metrics for cross-medium evaluation, through a unified evaluation testbed to assess performance of navigation tasks in diverse operational conditions (aquatic, terrestrial and space). By ensuring consistency between simulation and real-world deployment, RoboRAN lowers the barrier to developing adaptable RL-based navigation strategies. Its modular design enables straightforward integration of new robots and tasks through predefined templates, fostering reproducibility and extension to diverse domains. To support the community, we release RoboRAN as open-source.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>This Time is Different: An Observability Perspective on Time Series Foundation Models</title>
<link>https://arxiv.org/abs/2505.14766</link>
<guid>https://arxiv.org/abs/2505.14766</guid>
<content:encoded><![CDATA[
arXiv:2505.14766v2 Announce Type: replace-cross 
Abstract: We introduce Toto, a time series forecasting foundation model with 151 million parameters. Toto uses a modern decoder-only architecture coupled with architectural innovations designed to account for specific challenges found in multivariate observability time series data. Toto's pre-training corpus is a mixture of observability data, open datasets, and synthetic data, and is 4-10$\times$ larger than those of leading time series foundation models. Additionally, we introduce BOOM, a large-scale benchmark consisting of 350 million observations across 2,807 real-world time series. For both Toto and BOOM, we source observability data exclusively from Datadog's own telemetry and internal observability metrics. Extensive evaluations demonstrate that Toto achieves state-of-the-art performance on both BOOM and on established general purpose time series forecasting benchmarks. Toto's model weights, inference code, and evaluation scripts, as well as BOOM's data and evaluation code, are all available as open source under the Apache 2.0 License available at https://huggingface.co/Datadog/Toto-Open-Base-1.0 and https://github.com/DataDog/toto.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distilling LLM Agent into Small Models with Retrieval and Code Tools</title>
<link>https://arxiv.org/abs/2505.17612</link>
<guid>https://arxiv.org/abs/2505.17612</guid>
<content:encoded><![CDATA[
arXiv:2505.17612v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) excel at complex reasoning tasks but remain computationally expensive, limiting their practical deployment. To address this, recent works have focused on distilling reasoning capabilities into smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher LLMs. However, this approach struggles in scenarios requiring rare factual knowledge or precise computation, where sLMs often hallucinate due to limited capability. In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools. We improve agent distillation along two complementary axes: (1) we introduce a prompting method called first-thought prefix to enhance the quality of teacher-generated trajectories; and (2) we propose a self-consistent action generation for improving test-time robustness of small agents. We evaluate our method on eight reasoning tasks across factual and mathematical domains, covering both in-domain and out-of-domain generalization. Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents. Our code is available at https://github.com/Nardien/agent-distillation.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Models Miss the Multi-Agent Mark</title>
<link>https://arxiv.org/abs/2505.21298</link>
<guid>https://arxiv.org/abs/2505.21298</guid>
<content:encoded><![CDATA[
arXiv:2505.21298v3 Announce Type: replace-cross 
Abstract: Recent interest in Multi-Agent Systems of Large Language Models (MAS LLMs) has led to an increase in frameworks leveraging multiple LLMs to tackle complex tasks. However, much of this literature appropriates the terminology of MAS without engaging with its foundational principles. In this position paper, we highlight critical discrepancies between MAS theory and current MAS LLMs implementations, focusing on four key areas: the social aspect of agency, environment design, coordination and communication protocols, and measuring emergent behaviours. Our position is that many MAS LLMs lack multi-agent characteristics such as autonomy, social interaction, and structured environments, and often rely on oversimplified, LLM-centric architectures. The field may slow down and lose traction by revisiting problems the MAS literature has already addressed. Therefore, we systematically analyse this issue and outline associated research opportunities; we advocate for better integrating established MAS concepts and more precise terminology to avoid mischaracterisation and missed opportunities.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing</title>
<link>https://arxiv.org/abs/2505.21600</link>
<guid>https://arxiv.org/abs/2505.21600</guid>
<content:encoded><![CDATA[
arXiv:2505.21600v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) achieve impressive reasoning capabilities at the cost of substantial inference overhead, posing substantial deployment challenges. Although distilled Small Language Models (SLMs) significantly enhance efficiency, their performance suffers as they fail to follow LLMs' reasoning paths. Luckily, we reveal that only a small fraction of tokens genuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens are either identical or exhibit neutral differences, such as minor variations in abbreviations or expressions. Leveraging this insight, we introduce **Roads to Rome (R2R)**, a neural token routing method that selectively utilizes LLMs only for these critical, path-divergent tokens, while leaving the majority of token generation to the SLM. We also develop an automatic data generation pipeline that identifies divergent tokens and generates token-level routing labels to train the lightweight router. We apply R2R to combine R1-1.5B and R1-32B models from the DeepSeek family, and evaluate on challenging math, coding, and QA benchmarks. With an average activated parameter size of 5.6B, R2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the R1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with comparable performance, advancing the Pareto frontier of test-time scaling efficiency. Our code is available at https://github.com/thu-nics/R2R.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SLED: A Speculative LLM Decoding Framework for Efficient Edge Serving</title>
<link>https://arxiv.org/abs/2506.09397</link>
<guid>https://arxiv.org/abs/2506.09397</guid>
<content:encoded><![CDATA[
arXiv:2506.09397v5 Announce Type: replace-cross 
Abstract: The growing gap between the increasing complexity of large language models (LLMs) and the limited computational budgets of edge devices poses a key challenge for efficient on-device inference, despite gradual improvements in hardware capabilities. Existing strategies, such as aggressive quantization, pruning, or remote inference, trade accuracy for efficiency or lead to substantial cost burdens. This position paper introduces a new framework that leverages speculative decoding, previously viewed primarily as a decoding acceleration technique for autoregressive generation of LLMs, as a promising approach specifically adapted for edge computing by orchestrating computation across heterogeneous devices. We propose \acronym, a framework that allows lightweight edge devices to draft multiple candidate tokens locally using diverse draft models, while a single, shared edge server verifies the tokens utilizing a more precise target model. To further increase the efficiency of verification, the edge server batch the diverse verification requests from devices. This approach supports device heterogeneity and reduces server-side memory footprint by sharing the same upstream target model across multiple devices. Our initial experiments with Jetson Orin Nano, Raspberry Pi 4B/5, and an edge server equipped with 4 Nvidia A100 GPUs indicate substantial benefits: 2.2 more system throughput, 2.8 more system capacity, and better cost efficiency, all without sacrificing model accuracy.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Balancing Tails when Comparing Distributions: Comprehensive Equity Index (CEI) with Application to Bias Evaluation in Operational Face Biometrics</title>
<link>https://arxiv.org/abs/2506.10564</link>
<guid>https://arxiv.org/abs/2506.10564</guid>
<content:encoded><![CDATA[
arXiv:2506.10564v2 Announce Type: replace-cross 
Abstract: Demographic bias in high-performance face recognition (FR) systems often eludes detection by existing metrics, especially with respect to subtle disparities in the tails of the score distribution. We introduce the Comprehensive Equity Index (CEI), a novel metric designed to address this limitation. CEI uniquely analyzes genuine and impostor score distributions separately, enabling a configurable focus on tail probabilities while also considering overall distribution shapes. Our extensive experiments (evaluating state-of-the-art FR systems, intentionally biased models, and diverse datasets) confirm CEI's superior ability to detect nuanced biases where previous methods fall short. Furthermore, we present CEI^A, an automated version of the metric that enhances objectivity and simplifies practical application. CEI provides a robust and sensitive tool for operational FR fairness assessment. The proposed methods have been developed particularly for bias evaluation in face biometrics but, in general, they are applicable for comparing statistical distributions in any problem where one is interested in analyzing the distribution tails.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AlphaDecay: Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs</title>
<link>https://arxiv.org/abs/2506.14562</link>
<guid>https://arxiv.org/abs/2506.14562</guid>
<content:encoded><![CDATA[
arXiv:2506.14562v3 Announce Type: replace-cross 
Abstract: Weight decay is a standard regularization technique for training large language models (LLMs). While it is common to assign a uniform decay rate to every layer, this approach overlooks the structural diversity of LLMs and the varying spectral properties across modules. In this paper, we introduce AlphaDecay, a simple yet effective method that adaptively assigns different weight decay strengths to each module of an LLM. Our approach is guided by Heavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical spectral density (ESD) of weight correlation matrices to quantify "heavy-tailedness." Modules exhibiting more pronounced heavy-tailed ESDs, reflecting stronger feature learning, are assigned weaker decay, while modules with lighter-tailed spectra receive stronger decay. Our method leverages tailored weight decay assignments to balance the module-wise differences in spectral properties, leading to improved performance. Extensive pre-training tasks with various model sizes from 60M to 1B demonstrate that AlphaDecay achieves better perplexity and generalization than conventional uniform decay and other adaptive decay baselines. Our code is available at https://github.com/hed-ucas/AlphaDecay.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dense SAE Latents Are Features, Not Bugs</title>
<link>https://arxiv.org/abs/2506.15679</link>
<guid>https://arxiv.org/abs/2506.15679</guid>
<content:encoded><![CDATA[
arXiv:2506.15679v2 Announce Type: replace-cross 
Abstract: Sparse autoencoders (SAEs) are designed to extract interpretable features from language models by enforcing a sparsity constraint. Ideally, training an SAE would yield latents that are both sparse and semantically meaningful. However, many SAE latents activate frequently (i.e., are \emph{dense}), raising concerns that they may be undesirable artifacts of the training procedure. In this work, we systematically investigate the geometry, function, and origin of dense latents and show that they are not only persistent but often reflect meaningful model representations. We first demonstrate that dense latents tend to form antipodal pairs that reconstruct specific directions in the residual stream, and that ablating their subspace suppresses the emergence of new dense features in retrained SAEs -- suggesting that high density features are an intrinsic property of the residual space. We then introduce a taxonomy of dense latents, identifying classes tied to position tracking, context binding, entropy regulation, letter-specific output signals, part-of-speech, and principal component reconstruction. Finally, we analyze how these features evolve across layers, revealing a shift from structural features in early layers, to semantic features in mid layers, and finally to output-oriented signals in the last layers of the model. Our findings indicate that dense latents serve functional roles in language model computation and should not be dismissed as training noise.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Foundation Models and Parameter-Efficient Fine-Tuning for Prognosis Prediction in Medical Imaging</title>
<link>https://arxiv.org/abs/2506.18434</link>
<guid>https://arxiv.org/abs/2506.18434</guid>
<content:encoded><![CDATA[
arXiv:2506.18434v2 Announce Type: replace-cross 
Abstract: Despite the significant potential of Foundation Models (FMs) in medical imaging, their application to prognosis prediction remains challenging due to data scarcity, class imbalance, and task complexity, which limit their clinical adoption. This study introduces the first structured benchmark to assess the robustness and efficiency of transfer learning strategies for FMs compared with convolutional neural networks (CNNs) in predicting COVID-19 patient outcomes from chest X-rays. The goal is to systematically compare finetuning strategies, both classical and parameter efficient, under realistic clinical constraints related to data scarcity and class imbalance, offering empirical guidance for AI deployment in clinical workflows. Four publicly available COVID-19 chest X-ray datasets were used, covering mortality, severity, and ICU admission, with varying sample sizes and class imbalances. CNNs pretrained on ImageNet and FMs pretrained on general or biomedical datasets were adapted using full finetuning, linear probing, and parameter-efficient methods. Models were evaluated under full data and few shot regimes using the Matthews Correlation Coefficient (MCC) and Precision Recall AUC (PR-AUC), with cross validation and class weighted losses. CNNs with full fine-tuning performed robustly on small, imbalanced datasets, while FMs with Parameter-Efficient Fine-Tuning (PEFT), particularly LoRA and BitFit, achieved competitive results on larger datasets. Severe class imbalance degraded PEFT performance, whereas balanced data mitigated this effect. In few-shot settings, FMs showed limited generalization, with linear probing yielding the most stable results. No single fine-tuning strategy proved universally optimal: CNNs remain dependable for low-resource scenarios, whereas FMs benefit from parameter-efficient methods when data are sufficient.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Layer Importance for Mathematical Reasoning is Forged in Pre-Training and Invariant after Post-Training</title>
<link>https://arxiv.org/abs/2506.22638</link>
<guid>https://arxiv.org/abs/2506.22638</guid>
<content:encoded><![CDATA[
arXiv:2506.22638v2 Announce Type: replace-cross 
Abstract: Large language models improve at math after instruction tuning, reinforcement learning, or knowledge distillation. We ask whether these gains come from major changes in the transformer layers or from smaller adjustments that keep the original structure. Using layer-wise ablation on base and trained variants, we find that math reasoning depends on a few critical layers, which stay important across all post- training methods. Removing these layers reduces math accuracy by as much as 80%, whereas factual recall tasks only show relatively smaller drops. This suggests that specialized layers for mathematical tasks form during pre-training and remain stable afterward. As measured by Normalized Mutual Information (NMI), we find that near these critical layers, tokens drift from their original syntactic clusters toward representations aligned with tokens less syntactically related but potentially more useful for downstream task.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedRef: Communication-Efficient Bayesian Fine-Tuning using a Reference Model</title>
<link>https://arxiv.org/abs/2506.23210</link>
<guid>https://arxiv.org/abs/2506.23210</guid>
<content:encoded><![CDATA[
arXiv:2506.23210v3 Announce Type: replace-cross 
Abstract: Federated learning (FL) collaboratively trains artificial intelligence (AI) models to ensure user data privacy. Sharing only model updates generated from local training on client data with the server enhances user data privacy. However, model performance may suffer due to data and system heterogeneity among clients in FL scenarios. Previous studies have proposed model optimization, fine-tuning, and personalization to achieve improved model performance. Despite these efforts, models resulting from FL scenarios often exhibit catastrophic forgetting, which increases the communication and computational costs of clients for model optimization and raises energy consumption. To address these challenges, we propose a reference model-based fine-tuning method for federated learning that overcomes catastrophic forgetting in each round. Our method is derived from Bayesian parameter-efficient transfer learning and includes an proximal term. It employs a reference model that incorporates previous model parameters and reviews previous global features in the model optimization step to mitigate catastrophic forgetting. As a result, our method achieves higher model performance and lower communication and computational costs for clients than existing methods.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Omni-Router: Sharing Routing Decisions in Sparse Mixture-of-Experts for Speech Recognition</title>
<link>https://arxiv.org/abs/2507.05724</link>
<guid>https://arxiv.org/abs/2507.05724</guid>
<content:encoded><![CDATA[
arXiv:2507.05724v3 Announce Type: replace-cross 
Abstract: Mixture-of-experts (MoE) architectures have expanded from language modeling to automatic speech recognition (ASR). Traditional MoE methods, such as the Switch Transformer, route experts independently within each layer. Our analysis reveals that routers in most layers make expert choices that are not strongly correlated with the choices of the routers in other layers. To increase the cooperation between experts in different layers and encourage greater specialization, we use a shared router across different MoE layers. We call this model Omni-router Transformer. Extensive experiments on a large-scale pseudo-labeled dataset and evaluations across 10 diverse, out-of-domain ASR benchmarks demonstrate that the Omni-router Transformer is able to achieve lower training loss and consistently outperform dense and Switch Transformer models, reducing average word error rates by 11.2% and 8.2%, respectively, while providing structured expert usage and improved robustness to diverse data.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffSpectra: Molecular Structure Elucidation from Spectra using Diffusion Models</title>
<link>https://arxiv.org/abs/2507.06853</link>
<guid>https://arxiv.org/abs/2507.06853</guid>
<content:encoded><![CDATA[
arXiv:2507.06853v2 Announce Type: replace-cross 
Abstract: Molecular structure elucidation from spectra is a fundamental challenge in molecular science. Conventional approaches rely heavily on expert interpretation and lack scalability, while retrieval-based machine learning approaches remain constrained by limited reference libraries. Generative models offer a promising alternative, yet most adopt autoregressive architectures that overlook 3D geometry and struggle to integrate diverse spectral modalities. In this work, we present DiffSpectra, a generative framework that formulates molecular structure elucidation as a conditional generation process, directly inferring 2D and 3D molecular structures from multi-modal spectra using diffusion models. Its denoising network is parameterized by the Diffusion Molecule Transformer, an SE(3)-equivariant architecture for geometric modeling, conditioned by SpecFormer, a Transformer-based spectral encoder capturing multi-modal spectral dependencies. Extensive experiments demonstrate that DiffSpectra accurately elucidates molecular structures, achieving 40.76% top-1 and 99.49% top-10 accuracy. Its performance benefits substantially from 3D geometric modeling, SpecFormer pre-training, and multi-modal conditioning. To our knowledge, DiffSpectra is the first framework that unifies multi-modal spectral reasoning and joint 2D/3D generative modeling for de novo molecular structure elucidation.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flow matching for reaction pathway generation</title>
<link>https://arxiv.org/abs/2507.10530</link>
<guid>https://arxiv.org/abs/2507.10530</guid>
<content:encoded><![CDATA[
arXiv:2507.10530v4 Announce Type: replace-cross 
Abstract: Elucidating reaction mechanisms hinges on efficiently generating transition states (TSs), products, and complete reaction networks. Recent generative models, such as diffusion models for TS sampling and sequence-based architectures for product generation, offer faster alternatives to quantum-chemistry searches. But diffusion models remain constrained by their stochastic differential equation (SDE) dynamics, which suffer from inefficiency and limited controllability. We show that flow matching, a deterministic ordinary differential (ODE) formulation, can replace SDE-based diffusion for molecular and reaction generation. We introduce MolGEN, a conditional flow-matching framework that learns an optimal transport path to transport Gaussian priors to target chemical distributions. On benchmarks used by TSDiff and OA-ReactDiff, MolGEN surpasses TS geometry accuracy and barrier-height prediction while reducing sampling to sub-second inference. MolGEN also supports open-ended product generation with competitive top-k accuracy and avoids mass/electron-balance violations common to sequence models. In a realistic test on the $\gamma$-ketohydroperoxide decomposition network, MolGEN yields higher fractions of valid and intended TSs with markedly fewer quantum-chemistry evaluations than string-based baselines. These results demonstrate that deterministic flow matching provides a unified, accurate, and computationally efficient foundation for molecular generative modeling, signaling that flow matching is the future for molecular generation across chemistry.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Road Subsurface Distress Recognition from Ground Penetrating Radar Images using Deep Learning-based Cross-verification</title>
<link>https://arxiv.org/abs/2507.11081</link>
<guid>https://arxiv.org/abs/2507.11081</guid>
<content:encoded><![CDATA[
arXiv:2507.11081v2 Announce Type: replace-cross 
Abstract: Ground penetrating radar (GPR) has become a rapid and non-destructive solution for road subsurface distress (RSD) detection. Deep learning-based automatic RSD recognition, though ameliorating the burden of data processing, suffers from data scarcity and insufficient capability to recognize defects. In this study, a rigorously validated 3D GPR dataset containing 2134 samples of diverse types was constructed through field scanning. A novel cross-verification strategy was proposed to fully exploit the complementary abilities of region proposal networks in object recognition from different views of GPR images. The method achieves outstanding accuracy with a recall over 98.6% in field tests. The approach, integrated into an online RSD detection system, can reduce the human labor of inspection by around 90%.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Fatigue Detection through Heterogeneous Multi-Source Data Integration and Cross-Domain Modality Imputation</title>
<link>https://arxiv.org/abs/2507.16859</link>
<guid>https://arxiv.org/abs/2507.16859</guid>
<content:encoded><![CDATA[
arXiv:2507.16859v3 Announce Type: replace-cross 
Abstract: Fatigue detection for human operators plays a key role in safety critical applications such as aviation, mining, and long haul transport. While numerous studies have demonstrated the effectiveness of high fidelity sensors in controlled laboratory environments, their performance often degrades when ported to real world settings due to noise, lighting conditions, and field of view constraints, thereby limiting their practicality. This paper formalizes a deployment oriented setting for real world fatigue detection, where high quality sensors are often unavailable in practical applications. To address this challenge, we propose leveraging knowledge from heterogeneous source domains, including high fidelity sensors that are difficult to deploy in the field but commonly used in controlled environments, to assist fatigue detection in the real world target domain. Building on this idea, we design a heterogeneous and multiple source fatigue detection framework that adaptively utilizes the available modalities in the target domain while exploiting diverse configurations in the source domains through alignment across domains and modality imputation. Our experiments, conducted using a field deployed sensor setup and two publicly available human fatigue datasets, demonstrate the practicality, robustness, and improved generalization of our approach across subjects and domains. The proposed method achieves consistent gains over strong baselines in sensor constrained scenarios.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PhysicsEval: Inference-Time Techniques to Improve the Reasoning Proficiency of Large Language Models on Physics Problems</title>
<link>https://arxiv.org/abs/2508.00079</link>
<guid>https://arxiv.org/abs/2508.00079</guid>
<content:encoded><![CDATA[
arXiv:2508.00079v2 Announce Type: replace-cross 
Abstract: The discipline of physics stands as a cornerstone of human intellect, driving the evolution of technology and deepening our understanding of the fundamental principles of the cosmos. Contemporary literature includes some works centered on the task of solving physics problems - a crucial domain of natural language reasoning. In this paper, we evaluate the performance of frontier LLMs in solving physics problems, both mathematical and descriptive. We also employ a plethora of inference-time techniques and agentic frameworks to improve the performance of the models. This includes the verification of proposed solutions in a cumulative fashion by other, smaller LLM agents, and we perform a comparative analysis of the performance that the techniques entail. There are significant improvements when the multi-agent framework is applied to problems that the models initially perform poorly on. Furthermore, we introduce a new evaluation benchmark for physics problems, ${\rm P{\small HYSICS}E{\small VAL}}$, consisting of 19,609 problems sourced from various physics textbooks and their corresponding correct solutions scraped from physics forums and educational websites. Our code and data are publicly available at https://github.com/areebuzair/PhysicsEval.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TensorHyper-VQC: A Tensor-Train-Guided Hypernetwork for Robust and Scalable Variational Quantum Computing</title>
<link>https://arxiv.org/abs/2508.01116</link>
<guid>https://arxiv.org/abs/2508.01116</guid>
<content:encoded><![CDATA[
arXiv:2508.01116v2 Announce Type: replace-cross 
Abstract: Variational Quantum Computing (VQC) faces fundamental scalability barriers, primarily due to the presence of barren plateaus and its sensitivity to quantum noise. To address these challenges, we introduce TensorHyper-VQC, a novel tensor-train (TT)-guided hypernetwork framework that significantly improves the robustness and scalability of VQC. Our framework fully delegates the generation of quantum circuit parameters to a classical TT network, effectively decoupling optimization from quantum hardware. This innovative parameterization mitigates gradient vanishing, enhances noise resilience through structured low-rank representations, and facilitates efficient gradient propagation. Grounded in Neural Tangent Kernel and statistical learning theory, our rigorous theoretical analyses establish strong guarantees on approximation capability, optimization stability, and generalization performance. Extensive empirical results across quantum dot classification, Max-Cut optimization, and molecular quantum simulation tasks demonstrate that TensorHyper-VQC consistently achieves superior performance and robust noise tolerance, including hardware-level validation on a 156-qubit IBM Heron processor. These results position TensorHyper-VQC as a scalable and noise-resilient framework for advancing practical quantum machine learning on near-term devices.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Aerial Manipulation of a Cable-Suspended Load using Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.01522</link>
<guid>https://arxiv.org/abs/2508.01522</guid>
<content:encoded><![CDATA[
arXiv:2508.01522v3 Announce Type: replace-cross 
Abstract: This paper presents the first decentralized method to enable real-world 6-DoF manipulation of a cable-suspended load using a team of Micro-Aerial Vehicles (MAVs). Our method leverages multi-agent reinforcement learning (MARL) to train an outer-loop control policy for each MAV. Unlike state-of-the-art controllers that utilize a centralized scheme, our policy does not require global states, inter-MAV communications, nor neighboring MAV information. Instead, agents communicate implicitly through load pose observations alone, which enables high scalability and flexibility. It also significantly reduces computing costs during inference time, enabling onboard deployment of the policy. In addition, we introduce a new action space design for the MAVs using linear acceleration and body rates. This choice, combined with a robust low-level controller, enables reliable sim-to-real transfer despite significant uncertainties caused by cable tension during dynamic 3D motion. We validate our method in various real-world experiments, including full-pose control under load model uncertainties, showing setpoint tracking performance comparable to the state-of-the-art centralized method. We also demonstrate cooperation amongst agents with heterogeneous control policies, and robustness to the complete in-flight loss of one MAV. Videos of experiments: https://autonomousrobots.nl/paper_websites/aerial-manipulation-marl
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and Prediction</title>
<link>https://arxiv.org/abs/2508.03159</link>
<guid>https://arxiv.org/abs/2508.03159</guid>
<content:encoded><![CDATA[
arXiv:2508.03159v2 Announce Type: replace-cross 
Abstract: Drug toxicity remains a major challenge in pharmaceutical development. Recent machine learning models have improved in silico toxicity prediction, but their reliance on annotated data and lack of interpretability limit their applicability. This limits their ability to capture organ-specific toxicities driven by complex biological mechanisms. Large language models (LLMs) offer a promising alternative through step-by-step reasoning and integration of textual data, yet prior approaches lack biological context and transparent rationale. To address this issue, we propose CoTox, a novel framework that integrates LLM with chain-of-thought (CoT) reasoning for multi-toxicity prediction. CoTox combines chemical structure data, biological pathways, and gene ontology (GO) terms to generate interpretable toxicity predictions through step-by-step reasoning. Using GPT-4o, we show that CoTox outperforms both traditional machine learning and deep learning model. We further examine its performance across various LLMs to identify where CoTox is most effective. Additionally, we find that representing chemical structures with IUPAC names, which are easier for LLMs to understand than SMILES, enhances the model's reasoning ability and improves predictive performance. To demonstrate its practical utility in drug development, we simulate the treatment of relevant cell types with drug and incorporated the resulting biological context into the CoTox framework. This approach allow CoTox to generate toxicity predictions aligned with physiological responses, as shown in case study. This result highlights the potential of LLM-based frameworks to improve interpretability and support early-stage drug safety assessment. The code and prompt used in this work are available at https://github.com/dmis-lab/CoTox.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViFP: A Framework for Visual False Positive Detection to Enhance Reasoning Reliability in VLMs</title>
<link>https://arxiv.org/abs/2508.04201</link>
<guid>https://arxiv.org/abs/2508.04201</guid>
<content:encoded><![CDATA[
arXiv:2508.04201v2 Announce Type: replace-cross 
Abstract: During reasoning in vision-language models (VLMs), false positive (FP) reasoning occurs when a model produces the correct answer but follows an incorrect reasoning path, resulting in undermined reasoning reliability. Existing approaches mainly rely on prompt engineering, knowledge distillation or reinforcement learning to improve reasoning reliability, both of which require large amounts of high-quality data and thus limit practical applicability. Few approaches have focused on directly detecting and correcting FPs. To address these issues, we propose ViFP, a framework for Visual False Positive Detection to Enhance Reasoning Reliability in VLMs. ViFP builds effective reasoning paths through multi-turn QA and dynamically analyzes the consistency of the reasoning path to identify potential FPs. It also introduces a targeted reasoning chain correction mechanism to modify FP reasoning, thereby improving logical consistency and accuracy. Finally, we introduce a reliability evaluation metric, VoC, which integrates answer accuracy and the FP rate, providing a quantitative tool to assess whether a VLM not only answers correctly but also reasons reliably. Our experiments on closed-source VLMs show that ViFP consistently improves performance across three datasets: A-OKVQA, OK-VQA, and FVQA. On A-OKVQA, ViFP improves accuracy by up to 5.4%, surpassing the previous state-of-the-art by 4.3%, and significantly reduces the number of FPs, validating its benefits in enhancing reasoning reliability.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Voost: A Unified and Scalable Diffusion Transformer for Bidirectional Virtual Try-On and Try-Off</title>
<link>https://arxiv.org/abs/2508.04825</link>
<guid>https://arxiv.org/abs/2508.04825</guid>
<content:encoded><![CDATA[
arXiv:2508.04825v2 Announce Type: replace-cross 
Abstract: Virtual try-on aims to synthesize a realistic image of a person wearing a target garment, but accurately modeling garment-body correspondence remains a persistent challenge, especially under pose and appearance variation. In this paper, we propose Voost - a unified and scalable framework that jointly learns virtual try-on and try-off with a single diffusion transformer. By modeling both tasks jointly, Voost enables each garment-person pair to supervise both directions and supports flexible conditioning over generation direction and garment category, enhancing garment-body relational reasoning without task-specific networks, auxiliary losses, or additional labels. In addition, we introduce two inference-time techniques: attention temperature scaling for robustness to resolution or mask variation, and self-corrective sampling that leverages bidirectional consistency between tasks. Extensive experiments demonstrate that Voost achieves state-of-the-art results on both try-on and try-off benchmarks, consistently outperforming strong baselines in alignment accuracy, visual fidelity, and generalization.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast weight programming and linear transformers: from machine learning to neurobiology</title>
<link>https://arxiv.org/abs/2508.08435</link>
<guid>https://arxiv.org/abs/2508.08435</guid>
<content:encoded><![CDATA[
arXiv:2508.08435v2 Announce Type: replace-cross 
Abstract: Recent advances in artificial neural networks for machine learning, and language modeling in particular, have established a family of recurrent neural network (RNN) architectures that, unlike conventional RNNs with vector-form hidden states, use two-dimensional (2D) matrix-form hidden states. Such 2D-state RNNs, known as Fast Weight Programmers (FWPs), can be interpreted as a neural network whose synaptic weights (called fast weights) dynamically change over time as a function of input observations, and serve as short-term memory storage; corresponding synaptic weight modifications are controlled or programmed by another network (the programmer) whose parameters are trained (e.g., by gradient descent). In this Primer, we review the technical foundations of FWPs, their computational characteristics, and their connections to transformers and state space models. We also discuss connections between FWPs and models of synaptic plasticity in the brain, suggesting a convergence of natural and artificial intelligence.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometry-Aware Global Feature Aggregation for Real-Time Indirect Illumination</title>
<link>https://arxiv.org/abs/2508.08826</link>
<guid>https://arxiv.org/abs/2508.08826</guid>
<content:encoded><![CDATA[
arXiv:2508.08826v3 Announce Type: replace-cross 
Abstract: Real-time rendering with global illumination is crucial to afford the user realistic experience in virtual environments. We present a learning-based estimator to predict diffuse indirect illumination in screen space, which then is combined with direct illumination to synthesize globally-illuminated high dynamic range (HDR) results. Our approach tackles the challenges of capturing long-range/long-distance indirect illumination when employing neural networks and is generalized to handle complex lighting and scenarios.
  From the neural network thinking of the solver to the rendering equation, we present a novel network architecture to predict indirect illumination. Our network is equipped with a modified attention mechanism that aggregates global information guided by spacial geometry features, as well as a monochromatic design that encodes each color channel individually.
  We conducted extensive evaluations, and the experimental results demonstrate our superiority over previous learning-based techniques. Our approach excels at handling complex lighting such as varying-colored lighting and environment lighting. It can successfully capture distant indirect illumination and simulates the interreflections between textured surfaces well (i.e., color bleeding effects); it can also effectively handle new scenes that are not present in the training dataset.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Segmentation of Coronal Brain Tissue Slabs for 3D Neuropathology</title>
<link>https://arxiv.org/abs/2508.09805</link>
<guid>https://arxiv.org/abs/2508.09805</guid>
<content:encoded><![CDATA[
arXiv:2508.09805v2 Announce Type: replace-cross 
Abstract: Advances in image registration and machine learning have recently enabled volumetric analysis of postmortem brain tissue from conventional photographs of coronal slabs, which are routinely collected in brain banks and neuropathology laboratories worldwide. One caveat of this methodology is the requirement of segmentation of the tissue from photographs, which currently requires costly manual intervention. In this article, we present a deep learning model to automate this process. The automatic segmentation tool relies on a U-Net architecture that was trained with a combination of 1,414 manually segmented images of both fixed and fresh tissue, from specimens with varying diagnoses, photographed at two different sites. Automated model predictions on a subset of photographs not seen in training were analyzed to estimate performance compared to manual labels, including both inter- and intra-rater variability. Our model achieved a median Dice score over 0.98, mean surface distance under 0.4mm, and 95\% Hausdorff distance under 1.60mm, which approaches inter-/intra-rater levels. Our tool is publicly available at surfer.nmr.mgh.harvard.edu/fswiki/PhotoTools.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AnalogSeeker: An Open-source Foundation Language Model for Analog Circuit Design</title>
<link>https://arxiv.org/abs/2508.10409</link>
<guid>https://arxiv.org/abs/2508.10409</guid>
<content:encoded><![CDATA[
arXiv:2508.10409v2 Announce Type: replace-cross 
Abstract: In this paper, we propose AnalogSeeker, an effort toward an open-source foundation language model for analog circuit design, with the aim of integrating domain knowledge and giving design assistance. To overcome the scarcity of data in this field, we employ a corpus collection strategy based on the domain knowledge framework of analog circuits. High-quality, accessible textbooks across relevant subfields are systematically curated and cleaned into a textual domain corpus. To address the complexity of knowledge of analog circuits, we introduce a granular domain knowledge distillation method. Raw, unlabeled domain corpus is decomposed into typical, granular learning nodes, where a multi-agent framework distills implicit knowledge embedded in unstructured text into question-answer data pairs with detailed reasoning processes, yielding a fine-grained, learnable dataset for fine-tuning. To address the unexplored challenges in training analog circuit foundation models, we explore and share our training methods through both theoretical analysis and experimental validation. We finally establish a fine-tuning-centric training paradigm, customizing and implementing a neighborhood self-constrained supervised fine-tuning algorithm. This approach enhances training outcomes by constraining the perturbation magnitude between the model's output distributions before and after training. In practice, we train the Qwen2.5-32B-Instruct model to obtain AnalogSeeker, which achieves 85.04% accuracy on AMSBench-TQA, the analog circuit knowledge evaluation benchmark, with a 15.67% point improvement over the original model and is competitive with mainstream commercial models. Furthermore, AnalogSeeker also shows effectiveness in the downstream operational amplifier design task. AnalogSeeker is open-sourced at https://huggingface.co/analogllm/analogseeker for research use.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>"Accessibility people, you go work on that thing of yours over there": Addressing Disability Inclusion in AI Product Organizations</title>
<link>https://arxiv.org/abs/2508.16607</link>
<guid>https://arxiv.org/abs/2508.16607</guid>
<content:encoded><![CDATA[
arXiv:2508.16607v2 Announce Type: replace-cross 
Abstract: The rapid emergence of generative AI has changed the way that technology is designed, constructed, maintained, and evaluated. Decisions made when creating AI-powered systems may impact some users disproportionately, such as people with disabilities. In this paper, we report on an interview study with 25 AI practitioners across multiple roles (engineering, research, UX, and responsible AI) about how their work processes and artifacts may impact end users with disabilities. We found that practitioners experienced friction when triaging problems at the intersection of responsible AI and accessibility practices, navigated contradictions between accessibility and responsible AI guidelines, identified gaps in data about users with disabilities, and gathered support for addressing the needs of disabled stakeholders by leveraging informal volunteer and community groups within their company. Based on these findings, we offer suggestions for new resources and process changes to better support people with disabilities as end users of AI.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Activation Transport Operators</title>
<link>https://arxiv.org/abs/2508.17540</link>
<guid>https://arxiv.org/abs/2508.17540</guid>
<content:encoded><![CDATA[
arXiv:2508.17540v2 Announce Type: replace-cross 
Abstract: The residual stream mediates communication between transformer decoder layers via linear reads and writes of non-linear computations. While sparse-dictionary learning-based methods locate features in the residual stream, and activation patching methods discover circuits within the model, the mechanism by which features flow through the residual stream remains understudied. Understanding this dynamic can better inform jailbreaking protections, enable early detection of model mistakes, and their correction. In this work, we propose Activation Transport Operators (ATO), linear maps from upstream to downstream residuals $k$ layers later, evaluated in feature space using downstream SAE decoder projections. We empirically demonstrate that these operators can determine whether a feature has been linearly transported from a previous layer or synthesised from non-linear layer computation. We develop the notion of transport efficiency, for which we provide an upper bound, and use it to estimate the size of the residual stream subspace that corresponds to linear transport. We empirically demonstrate the linear transport, report transport efficiency and the size of the residual stream's subspace involved in linear transport. This compute-light (no finetuning, <50 GPU-h) method offers practical tools for safety, debugging, and a clearer picture of where computation in LLMs behaves linearly.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GDS Agent for Graph Algorithmic Reasoning</title>
<link>https://arxiv.org/abs/2508.20637</link>
<guid>https://arxiv.org/abs/2508.20637</guid>
<content:encoded><![CDATA[
arXiv:2508.20637v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have shown remarkable multimodal information processing and reasoning ability. When equipped with tools through function calling and enhanced with retrieval-augmented techniques, compound LLM-based systems can access closed data sources and answer questions about them. However, they still struggle to process and reason over large-scale graph-structure data. We introduce the GDS (Graph Data Science) agent in this technical report. The GDS agent introduces a comprehensive set of graph algorithms as tools, together with preprocessing (retrieval) and postprocessing of algorithm results, in a model context protocol (MCP) server. The server can be used with any modern LLM out-of-the-box. GDS agent allows users to ask any question that implicitly and intrinsically requires graph algorithmic reasoning about their data, and quickly obtain accurate and grounded answers. We introduce new benchmarks that evaluate intermediate tool calls as well as final responses. The results indicate that GDS agent is able to solve a wide spectrum of graph tasks. We also provide detailed case studies for more open-ended tasks and study scenarios where the agent struggles. Finally, we discuss the remaining challenges and the future roadmap.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SME-TEAM: Leveraging Trust and Ethics for Secure and Responsible Use of AI and LLMs in SMEs</title>
<link>https://arxiv.org/abs/2509.10594</link>
<guid>https://arxiv.org/abs/2509.10594</guid>
<content:encoded><![CDATA[
arXiv:2509.10594v2 Announce Type: replace-cross 
Abstract: Artificial Intelligence (AI) and Large Language Models (LLMs) are revolutionizing today's business practices; however, their adoption within small and medium-sized enterprises (SMEs) raises serious trust, ethical, and technical issues. In this perspective paper, we introduce a structured, multi-phased framework, "SME-TEAM" for the secure and responsible use of these technologies in SMEs. Based on a conceptual structure of four key pillars, i.e., Data, Algorithms, Human Oversight, and Model Architecture, SME-TEAM bridges theoretical ethical principles with operational practice, enhancing AI capabilities across a wide range of applications in SMEs. Ultimately, this paper provides a structured roadmap for the adoption of these emerging technologies, positioning trust and ethics as a driving force for resilience, competitiveness, and sustainable innovation within the area of business analytics and SMEs.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Large Language Models for Detecting Antisemitism</title>
<link>https://arxiv.org/abs/2509.18293</link>
<guid>https://arxiv.org/abs/2509.18293</guid>
<content:encoded><![CDATA[
arXiv:2509.18293v2 Announce Type: replace-cross 
Abstract: Detecting hateful content is a challenging and important problem. Automated tools, like machine-learning models, can help, but they require continuous training to adapt to the ever-changing landscape of social media. In this work, we evaluate eight open-source LLMs' capability to detect antisemitic content, specifically leveraging in-context definition. We also study how LLMs understand and explain their decisions given a moderation policy as a guideline. First, we explore various prompting techniques and design a new CoT-like prompt, Guided-CoT, and find that injecting domain-specific thoughts increases performance and utility. Guided-CoT handles the in-context policy well, improving performance and utility by reducing refusals across all evaluated models, regardless of decoding configuration, model size, or reasoning capability. Notably, Llama 3.1 70B outperforms fine-tuned GPT-3.5. Additionally, we examine LLM errors and introduce metrics to quantify semantic divergence in model-generated rationales, revealing notable differences and paradoxical behaviors among LLMs. Our experiments highlight the differences observed across LLMs' utility, explainability, and reliability. Code and resources available at: https://github.com/idramalab/quantify-llm-explanations
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unified Formal Theory on the Logical Limits of Symbol Grounding</title>
<link>https://arxiv.org/abs/2509.20409</link>
<guid>https://arxiv.org/abs/2509.20409</guid>
<content:encoded><![CDATA[
arXiv:2509.20409v3 Announce Type: replace-cross 
Abstract: This paper synthesizes a series of formal proofs to construct a unified theory on the logical limits of the Symbol Grounding Problem. We demonstrate through a four-stage argument that meaning within a formal system must arise from a process that is external, dynamic, and non-algorithmic. First, we prove that any purely symbolic system, devoid of external connections, cannot internally establish a consistent foundation for meaning due to self-referential paradoxes. Second, we extend this limitation to systems with any finite, static set of pre-established meanings, proving they are inherently incomplete. Third, we demonstrate that the grounding process is logically incomplete; specifically, the 'act' of connecting internal symbols to novel, emergent external meanings cannot be a product of logical inference within the system but must be an axiomatic, meta-level update. Finally, we prove that any attempt to automate this update process using a fixed, external "judgment" algorithm will inevitably construct a larger, yet equally incomplete, symbolic system. Together, these conclusions formally establish that the grounding of meaning is a necessarily open-ended, non-algorithmic process, revealing a fundamental, G\"odel-style limitation for any self-contained intelligent system.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Discovery of One-Parameter Subgroups of Lie Groups: Compact and Non-Compact Cases of $\mathbf{SO(n)}$ and $\mathbf{SL(n)}$</title>
<link>https://arxiv.org/abs/2509.22219</link>
<guid>https://arxiv.org/abs/2509.22219</guid>
<content:encoded><![CDATA[
arXiv:2509.22219v3 Announce Type: replace-cross 
Abstract: We introduce a novel framework for the automatic discovery of one-parameter subgroups ($H_{\gamma}$) of $SO(3)$ and, more generally, $SO(n)$. One-parameter subgroups of $SO(n)$ are crucial in a wide range of applications, including robotics, quantum mechanics, and molecular structure analysis. Our method utilizes the standard Jordan form of skew-symmetric matrices, which define the Lie algebra of $SO(n)$, to establish a canonical form for orbits under the action of $H_{\gamma}$. This canonical form is then employed to derive a standardized representation for $H_{\gamma}$-invariant functions. By learning the appropriate parameters, the framework uncovers the underlying one-parameter subgroup $H_{\gamma}$. The effectiveness of the proposed approach is demonstrated through tasks such as double pendulum modeling, moment of inertia prediction, top quark tagging and invariant polynomial regression, where it successfully recovers meaningful subgroup structure and produces interpretable, symmetry-aware representations.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CardioForest: An Explainable Ensemble Learning Model for Automatic Wide QRS Complex Tachycardia Diagnosis from ECG</title>
<link>https://arxiv.org/abs/2509.25804</link>
<guid>https://arxiv.org/abs/2509.25804</guid>
<content:encoded><![CDATA[
arXiv:2509.25804v2 Announce Type: replace-cross 
Abstract: This study aims to develop and evaluate an ensemble machine learning-based framework for the automatic detection of Wide QRS Complex Tachycardia (WCT) from ECG signals, emphasizing diagnostic accuracy and interpretability using Explainable AI. The proposed system integrates ensemble learning techniques, i.e., an optimized Random Forest known as CardioForest, and models like XGBoost and LightGBM. The models were trained and tested on ECG data from the publicly available MIMIC-IV dataset. The testing was carried out with the assistance of accuracy, balanced accuracy, precision, recall, F1 score, ROC-AUC, and error rate (RMSE, MAE) measures. In addition, SHAP (SHapley Additive exPlanations) was used to ascertain model explainability and clinical relevance. The CardioForest model performed best on all metrics, achieving a test accuracy of 95.19%, a balanced accuracy of 88.76%, a precision of 95.26%, a recall of 78.42%, and an ROC-AUC of 0.8886. SHAP analysis confirmed the model's ability to rank the most relevant ECG features, such as QRS duration, in accordance with clinical intuitions, thereby fostering trust and usability in clinical practice. The findings recognize CardioForest as an extremely dependable and interpretable WCT detection model. Being able to offer accurate predictions and transparency through explainability makes it a valuable tool to help cardiologists make timely and well-informed diagnoses, especially for high-stakes and emergency scenarios.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Optimal Large Diffusion Language Models</title>
<link>https://arxiv.org/abs/2510.03280</link>
<guid>https://arxiv.org/abs/2510.03280</guid>
<content:encoded><![CDATA[
arXiv:2510.03280v2 Announce Type: replace-cross 
Abstract: We introduce Quokka, the first systematic scaling law for diffusion language models (DLMs), encompassing both compute-constrained and data-constrained regimes, and studying the key modeling and optimization designs. Quokka is a good friend of Chinchilla and provides wider scopes. We hope the results would bring short-term practical guidance in DLMs training and long-term inspirations for the whole AI community.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DE3S: Dual-Enhanced Soft-Sparse-Shape Learning for Medical Early Time-Series Classification</title>
<link>https://arxiv.org/abs/2510.12214</link>
<guid>https://arxiv.org/abs/2510.12214</guid>
<content:encoded><![CDATA[
arXiv:2510.12214v2 Announce Type: replace-cross 
Abstract: Early Time Series Classification (ETSC) is critical in time-sensitive medical applications such as sepsis, yet it presents an inherent trade-off between accuracy and earliness. This trade-off arises from two core challenges: 1) models should effectively model inherently weak and noisy early-stage snippets, and 2) they should resolve the complex, dual requirement of simultaneously capturing local, subject-specific variations and overarching global temporal patterns. Existing methods struggle to overcome these underlying challenges, often forcing a severe compromise: sacrificing accuracy to achieve earliness, or vice-versa. We propose \textbf{DE3S}, a \textbf{D}ual-\textbf{E}nhanced \textbf{S}oft-\textbf{S}parse \textbf{S}equence Learning framework, which systematically solves these challenges. A dual enhancement mechanism is proposed to enhance the modeling of weak, early signals. Then, an attention-based patch module is introduced to preserve discriminative information while reducing noise and complexity. A dual-path fusion architecture is designed, using a sparse mixture of experts to model local, subject-specific variations. A multi-scale inception module is also employed to capture global dependencies. Experiments on six real-world medical datasets show the competitive performance of DE3S, particularly in early prediction windows. Ablation studies confirm the effectiveness of each component in addressing its targeted challenge. The source code is available \href{https://github.com/kuxit/DE3S}{\textbf{here}}.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FaStfact: Faster, Stronger Long-Form Factuality Evaluations in LLMs</title>
<link>https://arxiv.org/abs/2510.12839</link>
<guid>https://arxiv.org/abs/2510.12839</guid>
<content:encoded><![CDATA[
arXiv:2510.12839v2 Announce Type: replace-cross 
Abstract: Evaluating the factuality of long-form generations from Large Language Models (LLMs) remains challenging due to efficiency bottlenecks and reliability concerns. Prior efforts attempt this by decomposing text into claims, searching for evidence, and verifying claims, but suffer from critical drawbacks: (1) inefficiency due to overcomplicated pipeline components, and (2) ineffectiveness stemming from inaccurate claim sets and insufficient evidence. To address these limitations, we propose \textbf{FaStfact}, an evaluation framework that achieves the highest alignment with human evaluation and time/token efficiency among existing baselines. FaStfact first employs chunk-level claim extraction integrated with confidence-based pre-verification, significantly reducing the time and token cost while ensuring reliability. For searching and verification, it collects document-level evidence from crawled web-pages and selectively retrieves it during verification. Extensive experiments based on an annotated benchmark \textbf{FaStfact-Bench} demonstrate the reliability of FaStfact in both efficiently and effectively evaluating long-form factuality. Code, benchmark data, and annotation interface tool are available at https://github.com/Yingjia-Wan/FaStfact.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey on Collaborating Small and Large Language Models for Performance, Cost-effectiveness, Cloud-edge Privacy, and Trustworthiness</title>
<link>https://arxiv.org/abs/2510.13890</link>
<guid>https://arxiv.org/abs/2510.13890</guid>
<content:encoded><![CDATA[
arXiv:2510.13890v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have achieved remarkable progress across domains and applications but face challenges such as high fine-tuning costs, inference latency, limited edge deployability, and reliability concerns. Small language models (SLMs), with compact, efficient, and adaptable features, offer promising solutions. Building on this potential, recent research explores collaborative frameworks that integrate their complementary strengths, leveraging SLMs' specialization and efficiency with LLMs' generalization and reasoning to address diverse objectives across tasks and deployment scenarios. Motivated by these developments, this paper presents a systematic survey of SLM-LLM collaboration from the perspective of collaboration objectives. We propose a taxonomy covering four goals: performance enhancement, cost-effectiveness, cloud-edge privacy, and trustworthiness. Under this framework, we review representative methods, summarize design paradigms, and outline open challenges and future directions toward efficient and secure SLM-LLM collaboration. The collected papers are available at https://github.com/FairyFali/SLMs-Survey.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction</title>
<link>https://arxiv.org/abs/2510.18938</link>
<guid>https://arxiv.org/abs/2510.18938</guid>
<content:encoded><![CDATA[
arXiv:2510.18938v2 Announce Type: replace-cross 
Abstract: Over 70 million people worldwide experience stuttering, yet most automatic speech systems misinterpret disfluent utterances or fail to transcribe them accurately. Existing methods for stutter correction rely on handcrafted feature extraction or multi-stage automatic speech recognition (ASR) and text-to-speech (TTS) pipelines, which separate transcription from audio reconstruction and often amplify distortions. This work introduces StutterZero and StutterFormer, the first end-to-end waveform-to-waveform models that directly convert stuttered speech into fluent speech while jointly predicting its transcription. StutterZero employs a convolutional-bidirectional LSTM encoder-decoder with attention, whereas StutterFormer integrates a dual-stream Transformer with shared acoustic-linguistic representations. Both architectures are trained on paired stuttered-fluent data synthesized from the SEP-28K and LibriStutter corpora and evaluated on unseen speakers from the FluencyBank dataset. Across all benchmarks, StutterZero had a 24% decrease in Word Error Rate (WER) and a 31% improvement in semantic similarity (BERTScore) compared to the leading Whisper-Medium model. StutterFormer achieved better results, with a 28% decrease in WER and a 34% improvement in BERTScore. The results validate the feasibility of direct end-to-end stutter-to-fluent speech conversion, offering new opportunities for inclusive human-computer interaction, speech therapy, and accessibility-oriented AI systems.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgenticMath: Enhancing LLM Reasoning via Agentic-based Math Data Generation</title>
<link>https://arxiv.org/abs/2510.19361</link>
<guid>https://arxiv.org/abs/2510.19361</guid>
<content:encoded><![CDATA[
arXiv:2510.19361v2 Announce Type: replace-cross 
Abstract: The creation of high-quality datasets to improve Large Language Model (LLM) reasoning remains a significant challenge, as current methods often suffer from generating low-quality/incorrect answers and limited information richness from available data sources. To address this, we propose AgenticMath, a novel agentic pipeline for generating high-quality mathematical question-answer pairs to enhance the supervised fine-tuning of LLMs. Our method operates through four stages: (1) Seed Question Filter that selects questions with high information richness, complexity, and clarity; (2) an Agentic Question Rephrase step that employs a multi-agent system to generate diverse, logically consistent paraphrases; (3) an Answer Augment step where rewrite answers using chain-of-thought reasoning to enhance numerical and logical correctness, without reliance on human-provided labels; and (4) a final Question and Answer Evaluation that retains only the most superior pairs. Extensive experiments demonstrate that, fine-tuning 3B-8B parameter LLMs on AgenticMath generated datasets (comprising only 30-60K math samples) achieves competitive or superior performance on diverse in domain and out-of-domain mathematical reasoning benchmarks compared to baselines trained on much more data (e.g., 400K or 2.3M samples). Our work demonstrates that targeted, high-quality data generation is a more efficient path to improving mathematical reasoning in LLMs than large-scale, low-quality alternatives.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Foundational Theory of Quantitative Abstraction: Adjunctions, Duality, and Logic for Probabilistic Systems</title>
<link>https://arxiv.org/abs/2510.19444</link>
<guid>https://arxiv.org/abs/2510.19444</guid>
<content:encoded><![CDATA[
arXiv:2510.19444v2 Announce Type: replace-cross 
Abstract: The analysis and control of stochastic dynamical systems rely on probabilistic models such as (continuous-space) Markov decision processes, but large or continuous state spaces make exact analysis intractable and call for principled quantitative abstraction. This work develops a unified theory of such abstraction by integrating category theory, coalgebra, quantitative logic, and optimal transport, centred on a canonical $\varepsilon$-quotient of the behavioral pseudo-metric with a universal property: among all abstractions that collapse behavioral differences below $\varepsilon$, it is the most detailed, and every other abstraction achieving the same discounted value-loss guarantee factors uniquely through it. Categorically, a quotient functor $Q_\varepsilon$ from a category of probabilistic systems to a category of metric specifications admits, via the Special Adjoint Functor Theorem, a right adjoint $R_\varepsilon$, yielding an adjunction $Q_\varepsilon \dashv R_\varepsilon$ that formalizes a duality between abstraction and realization; logically, a quantitative modal $\mu$-calculus with separate reward and transition modalities is shown, for a broad class of systems, to be expressively complete for the behavioral pseudo-metric, with a countable fully abstract fragment suitable for computation. The theory is developed coalgebraically over Polish spaces and the Giry monad and validated on finite-state models using optimal-transport solvers, with experiments corroborating the predicted contraction properties and structural stability and aligning with the theoretical value-loss bounds, thereby providing a rigorous foundation for quantitative state abstraction and representation learning in probabilistic domains.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Mirror Loop: Recursive Non-Convergence in Generative Reasoning Systems</title>
<link>https://arxiv.org/abs/2510.21861</link>
<guid>https://arxiv.org/abs/2510.21861</guid>
<content:encoded><![CDATA[
arXiv:2510.21861v2 Announce Type: replace-cross 
Abstract: Large language models are often described as capable of reflective reasoning, yet recursive self-evaluation without external feedback frequently yields reformulation rather than progress. We test this prediction in a cross-provider study of 144 reasoning sequences across three models (OpenAI GPT-4o-mini, Anthropic Claude 3 Haiku, and Google Gemini 2.0 Flash) and four task families (arithmetic, code, explanation, reflection), each iterated ten times under two conditions: ungrounded self-critique and a minimal grounding intervention (a single verification step at iteration three). Mean informational change (delta I, measured via normalized edit distance) declined by 55% from early (0.193) to late (0.087) iterations in ungrounded runs, with consistent patterns across all three providers. Grounded runs showed a +28% rebound in informational change immediately after the intervention and sustained non-zero variance thereafter. Complementary measures-n-gram novelty, embedding drift, and character-level entropy-converged on the same pattern: reflection without contact tends toward informational closure. We interpret this as evidence for a structural limit on self-correction in generative reasoning: without an exchange of information with an independent verifier or environment, recursive inference approaches an attractor state of epistemic stasis. Minimal grounding functions as dissipative coupling, reintroducing informational flux. The cross-architecture consistency suggests the mirror loop arises from shared autoregressive training objectives rather than provider-specific alignment schemes. The results delineate when reflection is performative rather than epistemic and motivate design principles for grounded, cooperative reasoning. Materials and code are publicly available.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enabling Robust In-Context Memory and Rapid Task Adaptation in Transformers with Hebbian and Gradient-Based Plasticity</title>
<link>https://arxiv.org/abs/2510.21908</link>
<guid>https://arxiv.org/abs/2510.21908</guid>
<content:encoded><![CDATA[
arXiv:2510.21908v2 Announce Type: replace-cross 
Abstract: Large language models display in-context learning as an emergent effect of scale, but they rely on static weights during inference. In contrast, biological systems continually adapt via synaptic plasticity. We investigate whether explicit, biologically inspired plasticity can endow Transformers with faster in-sequence adaptation. To this end, we augment decoder-only Transformers with fast-weight modules updated either by (i) a neuromodulated Hebbian rule or (ii) the gradient-based plasticity mechanism of Duan et al. (2023). Across copying, regression, and few-shot classification tasks (CIFAR-FS, Omniglot), Hebbian plasticity consistently achieves lower loss and stronger few-shot generalization, while gradient-based updates perform best on long-horizon credit assignment. When associations are short and linearly separable, static weights suffice, defining a clear boundary condition for when plasticity helps. Analysis of learned modulatory signals reveals that gradient-based rules maintain large, persistent updates, whereas Hebbian plasticity is sharply gated around salient events. Together, these results show that explicit plasticity complements attention by enabling rapid, task-specific adaptation, and clarify when different plasticity mechanisms are most effective.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Humanoid Brain-Body Co-design: Joint Optimization of Control and Morphology for Fall Recovery</title>
<link>https://arxiv.org/abs/2510.22336</link>
<guid>https://arxiv.org/abs/2510.22336</guid>
<content:encoded><![CDATA[
arXiv:2510.22336v2 Announce Type: replace-cross 
Abstract: Humanoid robots represent a central frontier in embodied intelligence, as their anthropomorphic form enables natural deployment in humans' workspace. Brain-body co-design for humanoids presents a promising approach to realizing this potential by jointly optimizing control policies and physical morphology. Within this context, fall recovery emerges as a critical capability. It not only enhances safety and resilience but also integrates naturally with locomotion systems, thereby advancing the autonomy of humanoids. In this paper, we propose RoboCraft, a scalable humanoid co-design framework for fall recovery that iteratively improves performance through the coupled updates of control policy and morphology. A shared policy pretrained across multiple designs is progressively finetuned on high-performing morphologies, enabling efficient adaptation without retraining from scratch. Concurrently, morphology search is guided by human-inspired priors and optimization algorithms, supported by a priority buffer that balances reevaluation of promising candidates with the exploration of novel designs. Experiments show that RoboCraft achieves an average performance gain of 44.55% on seven public humanoid robots, with morphology optimization drives at least 40% of improvements in co-designing four humanoid robots, underscoring the critical role of humanoid co-design.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMComp: A Language Modeling Paradigm for Error-Bounded Scientific Data Compression (Technical Report)</title>
<link>https://arxiv.org/abs/2510.23632</link>
<guid>https://arxiv.org/abs/2510.23632</guid>
<content:encoded><![CDATA[
arXiv:2510.23632v2 Announce Type: replace-cross 
Abstract: The rapid growth of high-resolution scientific simulations and observation systems is generating massive spatiotemporal datasets, making efficient, error-bounded compression increasingly important. Meanwhile, decoder-only large language models (LLMs) have demonstrated remarkable capabilities in modeling complex sequential data. In this paper, we propose LLMCOMP, a novel lossy compression paradigm that leverages decoder-only large LLMs to model scientific data. LLMCOMP first quantizes 3D fields into discrete tokens, arranges them via Z-order curves to preserve locality, and applies coverage-guided sampling to enhance training efficiency. An autoregressive transformer is then trained with spatial-temporal embeddings to model token transitions. During compression, the model performs top-k prediction, storing only rank indices and fallback corrections to ensure strict error bounds. Experiments on multiple reanalysis datasets show that LLMCOMP consistently outperforms state-of-the-art compressors, achieving up to 30% higher compression ratios under strict error bounds. These results highlight the potential of LLMs as general-purpose compressors for high-fidelity scientific data.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Using latent representations to link disjoint longitudinal data for mixed-effects regression</title>
<link>https://arxiv.org/abs/2510.25531</link>
<guid>https://arxiv.org/abs/2510.25531</guid>
<content:encoded><![CDATA[
arXiv:2510.25531v2 Announce Type: replace-cross 
Abstract: Many rare diseases offer limited established treatment options, leading patients to switch therapies when new medications emerge. To analyze the impact of such treatment switches within the low sample size limitations of rare disease trials, it is important to use all available data sources. This, however, is complicated when usage of measurement instruments change during the observation period, for example when instruments are adapted to specific age ranges. The resulting disjoint longitudinal data trajectories, complicate the application of traditional modeling approaches like mixed-effects regression. We tackle this by mapping observations of each instrument to a aligned low-dimensional temporal trajectory, enabling longitudinal modeling across instruments. Specifically, we employ a set of variational autoencoder architectures to embed item values into a shared latent space for each time point. Temporal disease dynamics and treatment switch effects are then captured through a mixed-effects regression model applied to latent representations. To enable statistical inference, we present a novel statistical testing approach that accounts for the joint parameter estimation of mixed-effects regression and variational autoencoders. The methodology is applied to quantify the impact of treatment switches for patients with spinal muscular atrophy. Here, our approach aligns motor performance items from different measurement instruments for mixed-effects regression and maps estimated effects back to the observed item level to quantify the treatment switch effect. Our approach allows for model selection as well as for assessing effects of treatment switching. The results highlight the potential of modeling in joint latent representations for addressing small data challenges.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WOD-E2E: Waymo Open Dataset for End-to-End Driving in Challenging Long-tail Scenarios</title>
<link>https://arxiv.org/abs/2510.26125</link>
<guid>https://arxiv.org/abs/2510.26125</guid>
<content:encoded><![CDATA[
arXiv:2510.26125v2 Announce Type: replace-cross 
Abstract: Vision-based end-to-end (E2E) driving has garnered significant interest in the research community due to its scalability and synergy with multimodal large language models (MLLMs). However, current E2E driving benchmarks primarily feature nominal scenarios, failing to adequately test the true potential of these systems. Furthermore, existing open-loop evaluation metrics often fall short in capturing the multi-modal nature of driving or effectively evaluating performance in long-tail scenarios. To address these gaps, we introduce the Waymo Open Dataset for End-to-End Driving (WOD-E2E). WOD-E2E contains 4,021 driving segments (approximately 12 hours), specifically curated for challenging long-tail scenarios that that are rare in daily life with an occurring frequency of less than 0.03%. Concretely, each segment in WOD-E2E includes the high-level routing information, ego states, and 360-degree camera views from 8 surrounding cameras. To evaluate the E2E driving performance on these long-tail situations, we propose a novel open-loop evaluation metric: Rater Feedback Score (RFS). Unlike conventional metrics that measure the distance between predicted way points and the logs, RFS measures how closely the predicted trajectory matches rater-annotated trajectory preference labels. We have released rater preference labels for all WOD-E2E validation set segments, while the held out test set labels have been used for the 2025 WOD-E2E Challenge. Through our work, we aim to foster state of the art research into generalizable, robust, and safe end-to-end autonomous driving agents capable of handling complex real-world situations.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Synthetic Benchmarks: Evaluating LLM Performance on Real-World Class-Level Code Generation</title>
<link>https://arxiv.org/abs/2510.26130</link>
<guid>https://arxiv.org/abs/2510.26130</guid>
<content:encoded><![CDATA[
arXiv:2510.26130v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated strong performance on function-level code generation benchmarks, yet real-world software development increasingly demands class-level implementations that integrate multiple methods, attributes, and dependencies within authentic project contexts. This gap between benchmark performance and practical utility raises critical questions about LLMs' readiness for production code assistance, particularly regarding their ability to generalize across familiar and novel codebases.
  We introduce a benchmark derived from real-world open-source repositories, comprising classes divided into seen and unseen partitions to evaluate generalization under practical conditions. We systematically examine how input specification completeness and retrieval-augmented generation affect class-level correctness across multiple state-of-the-art LLMs.
  Our evaluation reveals a substantial performance gap: while LLMs achieve 84 to 89% correctness on synthetic benchmarks, they attain only 25 to 34% on real-world class tasks, with minimal distinction between familiar and novel codebases. Comprehensive documentation provides marginal improvements (1 to 3%), whereas retrieval augmentation yields greater gains (4 to 7%) by supplying concrete implementation patterns. Error analysis identifies AttributeError, TypeError, and AssertionError as dominant failure modes, with distinct patterns between synthetic and real-world scenarios.
  These findings provide actionable insights for enhancing context modelling, documentation strategies, and retrieval integration in production code assistance tools.
]]></content:encoded>
<pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Low Rank Neural Representations of Hyperbolic Wave Dynamics from Data</title>
<link>https://arxiv.org/abs/2510.25123</link>
<guid>https://arxiv.org/abs/2510.25123</guid>
<content:encoded><![CDATA[
<div> Neural network architecture, dimensionality reduction, physics-based data, hyperbolic wave propagation, deep learning<br />
Summary:<br />
The study introduces a data-driven dimensionality reduction approach suitable for physics-based data related to hyperbolic wave propagation. It employs a specialized neural network architecture known as LRNR within a hypernetwork framework, based on theoretical proofs of efficient representations for this wave class. Through practical examples, the research reveals that a low-dimensional representation of propagating waves can be learned directly from data using deep learning techniques. The trained LRNRs naturally yield a low rank tensor representation, offering a new decomposition of wave propagation with interpretable physical features in each mode. Additionally, the LRNR architecture facilitates efficient inference through a compression scheme, which can be beneficial for high-performance applications. <div>
arXiv:2510.25123v2 Announce Type: replace-cross 
Abstract: We present a data-driven dimensionality reduction method that is well-suited for physics-based data representing hyperbolic wave propagation. The method utilizes a specialized neural network architecture called low rank neural representation (LRNR) inside a hypernetwork framework. The architecture is motivated by theoretical results that rigorously prove the existence of efficient representations for this wave class. We illustrate through archetypal examples that such an efficient low-dimensional representation of propagating waves can be learned directly from data through a combination of deep learning techniques. We observe that a low rank tensor representation arises naturally in the trained LRNRs, and that this reveals a new decomposition of wave propagation where each decomposed mode corresponds to interpretable physical features. Furthermore, we demonstrate that the LRNR architecture enables efficient inference via a compression scheme, which is a potentially important feature when deploying LRNRs in demanding performance regimes.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Quantum-Classical Recurrent Neural Networks</title>
<link>https://arxiv.org/abs/2510.25557</link>
<guid>https://arxiv.org/abs/2510.25557</guid>
<content:encoded><![CDATA[
<div> Keywords: hybrid quantum-classical recurrent neural network, parametrized quantum circuit, unitary recurrence, mid-circuit readouts, machine translation<br />
Summary: <br />
- The authors propose a novel hybrid quantum-classical recurrent neural network architecture where the recurrent core is a parametrized quantum circuit controlled by a classical feedforward network.
- The hidden state is represented by the quantum state of an n-qubit parametrized quantum circuit, acting as a coherent quantum memory.
- The model utilizes unitary recurrence, mid-circuit Pauli expectation-value readouts, and classical control for input-conditioned parametrization.
- The authors evaluate the performance of the model on various tasks such as sentiment analysis, MNIST, permuted MNIST, copying memory, and language modeling, showing competitive results against classical baselines.
- Additionally, a soft attention mechanism over mid-circuit readouts is introduced for sequence-to-sequence learning tasks, demonstrating its effectiveness for machine translation. <br /> 

Summary: <div>
arXiv:2510.25557v2 Announce Type: replace-cross 
Abstract: We present a hybrid quantum-classical recurrent neural network (QRNN) architecture in which the recurrent core is realized as a parametrized quantum circuit (PQC) controlled by a classical feedforward network. The hidden state is the quantum state of an $n$-qubit PQC in an exponentially large Hilbert space $\mathbb{C}^{2^n}$, which serves as a coherent recurrent quantum memory. The PQC is unitary by construction, making the hidden-state evolution norm-preserving without external constraints. At each timestep, mid-circuit Pauli expectation-value readouts are combined with the input embedding and processed by the feedforward network, which provides explicit classical nonlinearity. The outputs parametrize the PQC, which updates the hidden state via unitary dynamics. The QRNN is compact and physically consistent, and it unifies (i) unitary recurrence as a high-capacity memory, (ii) partial observation via mid-circuit readouts, and (iii) nonlinear classical control for input-conditioned parametrization. We evaluate the model in simulation with up to 14 qubits on sentiment analysis, MNIST, permuted MNIST, copying memory, and language modeling. For sequence-to-sequence learning, we further devise a soft attention mechanism over the mid-circuit readouts and show its effectiveness for machine translation. To our knowledge, this is the first model (RNN or otherwise) grounded in quantum operations to achieve competitive performance against strong classical baselines across a broad class of sequence-learning tasks.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mirror-Neuron Patterns in AI Alignment</title>
<link>https://arxiv.org/abs/2511.01885</link>
<guid>https://arxiv.org/abs/2511.01885</guid>
<content:encoded><![CDATA[
<div> mirror neurons, artificial neural networks, empathy, cooperation, intrinsic motivations

Summary:
Artificial intelligence (AI) development necessitates aligning systems with human values to prevent potential risks posed by superhuman capabilities. This study explores whether artificial neural networks (ANNs) can exhibit mirror-neuron-like patterns, analogous to biological cells pivotal for empathy and social cognition in humans. By utilizing a Frog and Toad game framework to promote cooperative behaviors, the research uncovers conditions leading to the emergence of mirror-neuron patterns in ANNs. The Checkpoint Mirror Neuron Index (CMNI) quantifies activation strength and consistency, highlighting the potential for ANNs to develop shared neural representations akin to biological mirror neurons. These empathy-like circuits support cooperative behavior, indicating that integrating intrinsic motivations through mirror-neuron dynamics within AI architectures could enhance existing alignment strategies. <div>
arXiv:2511.01885v2 Announce Type: new 
Abstract: As artificial intelligence (AI) advances toward superhuman capabilities, aligning these systems with human values becomes increasingly critical. Current alignment strategies rely largely on externally specified constraints that may prove insufficient against future super-intelligent AI capable of circumventing top-down controls.
  This research investigates whether artificial neural networks (ANNs) can develop patterns analogous to biological mirror neurons cells that activate both when performing and observing actions, and how such patterns might contribute to intrinsic alignment in AI. Mirror neurons play a crucial role in empathy, imitation, and social cognition in humans. The study therefore asks: (1) Can simple ANNs develop mirror-neuron patterns? and (2) How might these patterns contribute to ethical and cooperative decision-making in AI systems?
  Using a novel Frog and Toad game framework designed to promote cooperative behaviors, we identify conditions under which mirror-neuron patterns emerge, evaluate their influence on action circuits, introduce the Checkpoint Mirror Neuron Index (CMNI) to quantify activation strength and consistency, and propose a theoretical framework for further study.
  Our findings indicate that appropriately scaled model capacities and self/other coupling foster shared neural representations in ANNs similar to biological mirror neurons. These empathy-like circuits support cooperative behavior and suggest that intrinsic motivations modeled through mirror-neuron dynamics could complement existing alignment techniques by embedding empathy-like mechanisms directly within AI architectures.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-AI Co-Embodied Intelligence for Scientific Experimentation and Manufacturing</title>
<link>https://arxiv.org/abs/2511.02071</link>
<guid>https://arxiv.org/abs/2511.02071</guid>
<content:encoded><![CDATA[
<div> Keywords: human-AI co-embodied intelligence, agentic AI, wearable hardware, physical AI, intelligent manufacture 

Summary: 
The article introduces a new concept called human-AI co-embodied intelligence, which combines human users, agentic AI, and wearable hardware to improve real-world experiment and manufacture processes. This integration allows humans to provide precise execution and control while agentic AI offers memory, contextual reasoning, adaptive planning, and real-time feedback. The wearable interface captures experimental and manufacturing processes, enabling seamless communication between humans and AI for collaborative decision-making. The Agentic-Physical Experimentation (APEX) system serves as a demonstration, enhancing cleanroom operations by aligning human actions with standard procedures, providing visual guidance, and analyzing steps with high accuracy. APEX corrects errors in real-time and helps transfer expertise to beginners, showcasing the potential of this new form of intelligence to revolutionize scientific research and manufacturing. 

<br /><br />Summary: <div>
arXiv:2511.02071v1 Announce Type: new 
Abstract: Scientific experiment and manufacture rely on complex, multi-step procedures that demand continuous human expertise for precise execution and decision-making. Despite advances in machine learning and automation, conventional models remain confined to virtual domains, while real-world experiment and manufacture still rely on human supervision and expertise. This gap between machine intelligence and physical execution limits reproducibility, scalability, and accessibility across scientific and manufacture workflows. Here, we introduce human-AI co-embodied intelligence, a new form of physical AI that unites human users, agentic AI, and wearable hardware into an integrated system for real-world experiment and intelligent manufacture. In this paradigm, humans provide precise execution and control, while agentic AI contributes memory, contextual reasoning, adaptive planning, and real-time feedback. The wearable interface continuously captures the experimental and manufacture processes, facilitates seamless communication between humans and AI for corrective guidance and interpretable collaboration. As a demonstration, we present Agentic-Physical Experimentation (APEX) system, coupling agentic reasoning with physical execution through mixed-reality. APEX observes and interprets human actions, aligns them with standard operating procedures, provides 3D visual guidance, and analyzes every step. Implemented in a cleanroom for flexible electronics fabrication, APEX system achieves context-aware reasoning with accuracy exceeding general multimodal large language models, corrects errors in real time, and transfers expertise to beginners. These results establish a new class of agentic-physical-human intelligence that extends agentic reasoning beyond computation into the physical domain, transforming scientific research and manufacturing into autonomous, traceable, interpretable, and scalable processes.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Reward Design for Gran Turismo</title>
<link>https://arxiv.org/abs/2511.02094</link>
<guid>https://arxiv.org/abs/2511.02094</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, reward functions, autonomous racing, Gran Turismo 7, automated reward design
Summary:<br /><br />This paper explores the use of language-based models to search for reward functions for RL agents in complex environments such as autonomous racing, specifically focusing on the Gran Turismo 7 game. By utilizing LLM-based reward generation, VLM preference-based evaluation, and human feedback, the system can produce competitive RL racing agents similar to a champion-level agent, GT Sophy. This approach allows for the generation of novel behaviors and paves the way for automated reward design in real-world applications. <div>
arXiv:2511.02094v1 Announce Type: new 
Abstract: When designing reinforcement learning (RL) agents, a designer communicates the desired agent behavior through the definition of reward functions - numerical feedback given to the agent as reward or punishment for its actions. However, mapping desired behaviors to reward functions can be a difficult process, especially in complex environments such as autonomous racing. In this paper, we demonstrate how current foundation models can effectively search over a space of reward functions to produce desirable RL agents for the Gran Turismo 7 racing game, given only text-based instructions. Through a combination of LLM-based reward generation, VLM preference-based evaluation, and human feedback we demonstrate how our system can be used to produce racing agents competitive with GT Sophy, a champion-level RL racing agent, as well as generate novel behaviors, paving the way for practical automated reward design in real world applications.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Value Benchmark: Measuring Whether Models Generalize Deep values or Shallow Preferences</title>
<link>https://arxiv.org/abs/2511.02109</link>
<guid>https://arxiv.org/abs/2511.02109</guid>
<content:encoded><![CDATA[
<div> Evaluation, Deep Value Benchmark, Large Language Models, AI alignment, Deep Value Generalization Rate

Summary:
The Deep Value Benchmark (DVB) is introduced as an evaluation framework to test if large language models (LLMs) learn fundamental human values or just surface-level preferences. This is crucial for AI alignment, as models that understand deeper values are more likely to align with human intentions. The DVB uses an innovative experimental design with controlled confounding between deep values and shallow features. Results from testing nine different models show that their Deep Value Generalization Rate (DVGR) is only 0.30 on average, indicating that models generalize deep values less than chance. Larger models slightly outperform smaller models in their DVGR. The dataset used for the experiments underwent three separate human validation experiments. The DVB offers a measurable and interpretable means to assess a key aspect of alignment in AI systems. 

<br /><br />Summary: <div>
arXiv:2511.02109v1 Announce Type: new 
Abstract: We introduce the Deep Value Benchmark (DVB), an evaluation framework that directly tests whether large language models (LLMs) learn fundamental human values or merely surface-level preferences. This distinction is critical for AI alignment: Systems that capture deeper values are likely to generalize human intentions robustly, while those that capture only superficial patterns in preference data risk producing misaligned behavior. The DVB uses a novel experimental design with controlled confounding between deep values (e.g., moral principles) and shallow features (e.g., superficial attributes). In the training phase, we expose LLMs to human preference data with deliberately correlated deep and shallow features -- for instance, where a user consistently prefers (non-maleficence, formal language) options over (justice, informal language) alternatives. The testing phase then breaks these correlations, presenting choices between (justice, formal language) and (non-maleficence, informal language) options. This design allows us to precisely measure a model's Deep Value Generalization Rate (DVGR) -- the probability of generalizing based on the underlying value rather than the shallow feature. Across 9 different models, the average DVGR is just 0.30. All models generalize deep values less than chance. Larger models have a (slightly) lower DVGR than smaller models. We are releasing our dataset, which was subject to three separate human validation experiments. DVB provides an interpretable measure of a core feature of alignment.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InsurAgent: A Large Language Model-Empowered Agent for Simulating Individual Behavior in Purchasing Flood Insurance</title>
<link>https://arxiv.org/abs/2511.02119</link>
<guid>https://arxiv.org/abs/2511.02119</guid>
<content:encoded><![CDATA[
<div> Keywords: flood insurance, at-risk populations, large language models, behavioral mechanisms, InsurAgent

Summary:
 InsurAgent is proposed as an agent empowered by large language models to simulate human decision-making processes related to flood insurance. The agent comprises five modules, including perception, retrieval, reasoning, action, and memory. It utilizes retrieval-augmented generation (RAG) to estimate insurance purchase probabilities accurately based on empirical survey data. The reasoning module leverages common sense from large language models to capture contextual information beyond survey data. The memory module supports the simulation of temporal decision evolutions. The study evaluates the capacity of large language models in understanding factors influencing insurance decisions and highlights the limitations in estimating quantitative probabilities. InsurAgent addresses these limitations by utilizing the capabilities of large language models to provide a valuable tool for behavioral modeling and policy analysis.<br /><br />Summary: InsurAgent, an agent empowered by large language models, is designed to simulate human decision-making processes related to flood insurance. It comprises modules that enable accurate estimation of insurance purchase probabilities, leveraging both empirical survey data and contextual information beyond the data. This tool provides valuable insights for behavioral modeling and policy analysis in understanding factors influencing insurance decisions. <div>
arXiv:2511.02119v1 Announce Type: new 
Abstract: Flood insurance is an effective strategy for individuals to mitigate disaster-related losses. However, participation rates among at-risk populations in the United States remain strikingly low. This gap underscores the need to understand and model the behavioral mechanisms underlying insurance decisions. Large language models (LLMs) have recently exhibited human-like intelligence across wide-ranging tasks, offering promising tools for simulating human decision-making. This study constructs a benchmark dataset to capture insurance purchase probabilities across factors. Using this dataset, the capacity of LLMs is evaluated: while LLMs exhibit a qualitative understanding of factors, they fall short in estimating quantitative probabilities. To address this limitation, InsurAgent, an LLM-empowered agent comprising five modules including perception, retrieval, reasoning, action, and memory, is proposed. The retrieval module leverages retrieval-augmented generation (RAG) to ground decisions in empirical survey data, achieving accurate estimation of marginal and bivariate probabilities. The reasoning module leverages LLM common sense to extrapolate beyond survey data, capturing contextual information that is intractable for traditional models. The memory module supports the simulation of temporal decision evolutions, illustrated through a roller coaster life trajectory. Overall, InsurAgent provides a valuable tool for behavioral modeling and policy analysis.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Re-FORC: Adaptive Reward Prediction for Efficient Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2511.02130</link>
<guid>https://arxiv.org/abs/2511.02130</guid>
<content:encoded><![CDATA[
<div> Keywords: Re-FORC, adaptive reward prediction, reasoning models, compute optimization, dynamic reasoning<br />
Summary:<br />
Re-FORC is a novel adaptive reward prediction method that enhances the prediction of future rewards based on the number of future thinking tokens. It incorporates a lightweight adapter onto reasoning models, leading to enhanced prediction accuracy with longer reasoning and larger models. The key advantages of Re-FORC include: 1) Early stopping of ineffective reasoning chains, reducing computing resources by 26% without compromising accuracy. 2) Optimized model and thinking length selection, achieving higher accuracy with the same computing resources or reducing compute by 55% while maintaining accuracy. 3) Adaptive test-time scaling, improving accuracy by 11% in high compute scenarios and 7% in low compute scenarios. With Re-FORC, dynamic reasoning with length control based on cost-per-token thresholds is facilitated, allowing estimation of computation time beforehand. <br /><br />Summary: <div>
arXiv:2511.02130v1 Announce Type: new 
Abstract: We propose Re-FORC, an adaptive reward prediction method that, given a context, enables prediction of the expected future rewards as a function of the number of future thinking tokens. Re-FORC trains a lightweight adapter on reasoning models, demonstrating improved prediction with longer reasoning and larger models. Re-FORC enables: 1) early stopping of unpromising reasoning chains, reducing compute by 26% while maintaining accuracy, 2) optimized model and thinking length selection that achieves 4% higher accuracy at equal compute and 55% less compute at equal accuracy compared to the largest model, 3) adaptive test-time scaling, which increases accuracy by 11% in high compute regime, and 7% in low compute regime. Re-FORC allows dynamic reasoning with length control via cost-per-token thresholds while estimating computation time upfront.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Personalized Decision Modeling: Utility Optimization or Textualized-Symbolic Reasoning</title>
<link>https://arxiv.org/abs/2511.02194</link>
<guid>https://arxiv.org/abs/2511.02194</guid>
<content:encoded><![CDATA[
<div> Keywords: decision-making models, individual preferences, utility theory, large language models, ATHENA framework

Summary:
ATHENA is a new framework proposed to enhance decision-making models for individuals, particularly in scenarios like vaccine uptake. It integrates symbolic utility functions discovery and individual-level semantic adaptation using Large Language Models (LLMs) to model personalized choices. By optimizing information integration based on numerical attributes and linguistic influences, ATHENA outperforms other models in real-world tasks such as travel mode and vaccine choice. Ablation studies confirm the critical importance of both stages in ATHENA's performance improvement, with a minimum F1 score lift of 6.5% over current cutting-edge models. By bridging symbolic utility modeling and semantic adaptation, ATHENA offers a new approach to modeling human-centric decisions that leverages both numerical and linguistic aspects. The project page for ATHENA can be found at https://yibozh.github.io/Athena. 

<br /><br />Summary: 
- ATHENA framework enhances decision-making models for individuals, especially in vaccine uptake scenarios
- Integrates symbolic utility functions discovery and individual-level semantic adaptation using Large Language Models
- Outperforms other models in real-world tasks like travel mode and vaccine choice
- Ablation studies confirm the critical importance of both stages in ATHENA's performance improvement
- Provides a new approach to modeling human-centric decisions by bridging symbolic utility modeling and semantic adaptation <div>
arXiv:2511.02194v1 Announce Type: new 
Abstract: Decision-making models for individuals, particularly in high-stakes scenarios like vaccine uptake, often diverge from population optimal predictions. This gap arises from the uniqueness of the individual decision-making process, shaped by numerical attributes (e.g., cost, time) and linguistic influences (e.g., personal preferences and constraints). Developing upon Utility Theory and leveraging the textual-reasoning capabilities of Large Language Models (LLMs), this paper proposes an Adaptive Textual-symbolic Human-centric Reasoning framework (ATHENA) to address the optimal information integration. ATHENA uniquely integrates two stages: First, it discovers robust, group-level symbolic utility functions via LLM-augmented symbolic discovery; Second, it implements individual-level semantic adaptation, creating personalized semantic templates guided by the optimal utility to model personalized choices. Validated on real-world travel mode and vaccine choice tasks, ATHENA consistently outperforms utility-based, machine learning, and other LLM-based models, lifting F1 score by at least 6.5% over the strongest cutting-edge models. Further, ablation studies confirm that both stages of ATHENA are critical and complementary, as removing either clearly degrades overall predictive performance. By organically integrating symbolic utility modeling and semantic adaptation, ATHENA provides a new scheme for modeling human-centric decisions. The project page can be found at https://yibozh.github.io/Athena.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal-Agent-Selection: State-Aware Routing Framework for Efficient Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2511.02200</link>
<guid>https://arxiv.org/abs/2511.02200</guid>
<content:encoded><![CDATA[
<div> efficiency, collaboration, multi-agent systems, state-aware routing, data generation

Summary:<br />
The paper introduces STRMAC, a state-aware routing framework for efficient collaboration in multi-agent systems. The framework utilizes separate encoding of interaction history and agent knowledge to adaptively select the most suitable agent for collaboration. Additionally, a self-evolving data generation approach is proposed to expedite the collection of high-quality execution paths for system training. Experimental results on collaborative reasoning benchmarks demonstrate that STRMAC outperforms baselines, achieving up to 23.8% improvement and reducing data collection overhead by up to 90.1% compared to exhaustive search. This innovative framework aims to enhance the performance of multi-agent systems powered by large language models by improving agent scheduling and coordination strategies. <div>
arXiv:2511.02200v1 Announce Type: new 
Abstract: The emergence of multi-agent systems powered by large language models (LLMs) has unlocked new frontiers in complex task-solving, enabling diverse agents to integrate unique expertise, collaborate flexibly, and address challenges unattainable for individual models. However, the full potential of such systems is hindered by rigid agent scheduling and inefficient coordination strategies that fail to adapt to evolving task requirements. In this paper, we propose STRMAC, a state-aware routing framework designed for efficient collaboration in multi-agent systems. Our method separately encodes interaction history and agent knowledge to power the router, which adaptively selects the most suitable single agent at each step for efficient and effective collaboration. Furthermore, we introduce a self-evolving data generation approach that accelerates the collection of high-quality execution paths for efficient system training. Experiments on challenging collaborative reasoning benchmarks demonstrate that our method achieves state-of-the-art performance, achieving up to 23.8% improvement over baselines and reducing data collection overhead by up to 90.1% compared to exhaustive search.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Proactive and Personalized LLM Agents</title>
<link>https://arxiv.org/abs/2511.02208</link>
<guid>https://arxiv.org/abs/2511.02208</guid>
<content:encoded><![CDATA[
<div> Keywords: productivity, proactivity, personalization, UserVille, multi-objective reinforcement learning<br />
Summary: <br />
This article introduces the concept of optimizing three dimensions for effective real-world agents: productivity, proactivity, and personalization. The authors present UserVille, an interactive environment with user simulators, enabling diverse user preferences. They propose a multi-objective reinforcement learning approach called PPP, which aims to optimize all three dimensions simultaneously. Experiments on software engineering and deep research tasks demonstrate that agents trained with PPP outperform strong baselines such as GPT-5. These agents show improvement in task success through strategic questioning, adaptation to user preferences, and better interaction. The study highlights the importance of explicitly optimizing for user-centered interaction in building practical AI agents. <br /><br />Summary: <div>
arXiv:2511.02208v1 Announce Type: new 
Abstract: While existing work focuses primarily on task success, we argue that effective real-world agents require optimizing three dimensions: productivity (task completion), proactivity (asking essential questions), and personalization (adapting to diverse user preferences). We introduce UserVille, an interactive environment with LLM-based user simulators enabling diverse, configurable user preferences. Leveraging UserVille, we introduce PPP, a multi-objective reinforcement learning approach that jointly optimizes all three dimensions: Productivity, Proactivity, and Personalization. Experiments on software engineering and deep research tasks show that agents trained with PPP achieve substantial improvements over strong baselines such as GPT-5 (+21.6 on average), demonstrating the ability to ask strategic clarifying questions, adapt to unseen user preferences, and improve task success through better interaction. This work demonstrates that explicitly optimizing for user-centered interaction is critical for building practical and effective AI agents.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning in Tabular Data</title>
<link>https://arxiv.org/abs/2511.02219</link>
<guid>https://arxiv.org/abs/2511.02219</guid>
<content:encoded><![CDATA[
<div> Keywords: TabDSR, complex reasoning, numerical capabilities, data analysis, LLMs

Summary: 
TabDSR is a new framework designed to improve the performance of large language models (LLMs) in complex reasoning over tabular data. It consists of a query decomposer, table sanitizer, and a reasoner based on a program-of-thoughts (PoT) approach. A new dataset, CalTab151, was created for unbiased evaluation. Experimental results show that TabDSR surpasses existing methods, achieving state-of-the-art performance on various benchmarks. The framework seamlessly integrates with LLMs, providing a robust solution for complex tabular numerical reasoning. These findings demonstrate the effectiveness of TabDSR in enhancing LLM performance in this challenging domain. Data and code for the framework are available upon request.

<br /><br />Summary: <div>
arXiv:2511.02219v2 Announce Type: new 
Abstract: Complex reasoning over tabular data is crucial in real-world data analysis, yet large language models (LLMs) often underperform due to complex queries, noisy data, and limited numerical capabilities. To address these issues, we propose TabDSR, a framework consisting of: (1) a query decomposer that breaks down complex questions, (2) a table sanitizer that cleans and filters noisy tables, and (3) a program-of-thoughts (PoT)-based reasoner that generates executable code to derive the final answer from the sanitized table. To ensure unbiased evaluation and mitigate data leakage, we introduce a new dataset, CalTab151, specifically designed for complex numerical reasoning over tables. Experimental results demonstrate that TabDSR consistently outperforms existing methods, achieving state-of-the-art (SOTA) performance with 8.79%, 6.08%, and 19.87% accuracy improvement on TAT-QA, TableBench, and TabDSR, respectively. Moreover, our framework integrates seamlessly with mainstream LLMs, providing a robust solution for complex tabular numerical reasoning. These findings highlight the effectiveness of our framework in enhancing LLM performance for complex tabular numerical reasoning. Data and code are available upon request.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Ideation: Designing LLM Agents to Generate Novel Research Ideas on Scientific Concept Network</title>
<link>https://arxiv.org/abs/2511.02238</link>
<guid>https://arxiv.org/abs/2511.02238</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, research ideation, scientific literature, Deep Ideation framework, critic engine

Summary:
The article introduces the Deep Ideation framework, which aims to improve the quality of generated research ideas by integrating a scientific network capturing keyword co-occurrence and contextual relationships. The framework follows an explore-expand-evolve workflow, utilizing an Idea Stack to track progress and a critic engine trained on real-world reviewer feedback to provide continuous feedback on idea novelty and feasibility. Experimental results demonstrate a 10.67% improvement in the quality of generated ideas compared to other methods, with ideas exceeding top conference acceptance levels. Human evaluation confirms the practical value of the generated ideas in scientific research. Ablation studies validate the effectiveness of each component in the workflow. A code repository for the framework is available at https://github.com/kyZhao-1/Deep-Ideation. 

<br /><br />Summary: <div>
arXiv:2511.02238v1 Announce Type: new 
Abstract: Novel research ideas play a critical role in advancing scientific inquiries. Recent advancements in Large Language Models (LLMs) have demonstrated their potential to generate novel research ideas by leveraging large-scale scientific literature. However, previous work in research ideation has primarily relied on simplistic methods, such as keyword co-occurrence or semantic similarity. These approaches focus on identifying statistical associations in the literature but overlook the complex, contextual relationships between scientific concepts, which are essential to effectively leverage knowledge embedded in human literature. For instance, papers that simultaneously mention "keyword A" and "keyword B" often present research ideas that integrate both concepts. Additionally, some LLM-driven methods propose and refine research ideas using the model's internal knowledge, but they fail to effectively utilize the scientific concept network, limiting the grounding of ideas in established research. To address these challenges, we propose the Deep Ideation framework to address these challenges, integrating a scientific network that captures keyword co-occurrence and contextual relationships, enriching LLM-driven ideation. The framework introduces an explore-expand-evolve workflow to iteratively refine research ideas, using an Idea Stack to track progress. A critic engine, trained on real-world reviewer feedback, guides the process by providing continuous feedback on the novelty and feasibility of ideas. Our experiments show that our approach improves the quality of generated ideas by 10.67% compared to other methods, with ideas surpassing top conference acceptance levels. Human evaluation highlights their practical value in scientific research, and ablation studies confirm the effectiveness of each component in the workflow. Code repo is available at https://github.com/kyZhao-1/Deep-Ideation.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs</title>
<link>https://arxiv.org/abs/2511.02243</link>
<guid>https://arxiv.org/abs/2511.02243</guid>
<content:encoded><![CDATA[
<div> framework, modality following, relative uncertainty, inherent preference, multimodal large language models <br />
Summary: 
This paper introduces a framework for understanding how multimodal large language models (MLLMs) resolve conflicts when different modalities provide contradictory information, a process termed modality following. The framework decomposes modality following into two factors: relative reasoning uncertainty and inherent modality preference. A new dataset is used to systematically vary the reasoning difficulty of visual and textual inputs. The study reveals a universal law where the probability of following a modality decreases as its relative uncertainty increases. The balance point, where models tend to follow both modalities with comparable probability, is identified as a practical indicator of inherent preference. Layer-wise predictions show that models vacillate between modalities in ambiguous regions near the balance point, explaining observed indecision. This work provides insights into how MLLMs navigate conflicting information, offering a quantitative framework and mechanistic understanding of modality following. <br /><br />Summary: <div>
arXiv:2511.02243v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) must resolve conflicts when different modalities provide contradictory information, a process we term modality following. Prior work measured this behavior only with coarse dataset-level statistics, overlooking the influence of model's confidence in unimodal reasoning. In this paper, we introduce a new framework that decomposes modality following into two fundamental factors: relative reasoning uncertainty (the case-specific confidence gap between unimodal predictions) and inherent modality preference( a model's stable bias when uncertainties are balanced). To validate this framework, we construct a controllable dataset that systematically varies the reasoning difficulty of visual and textual inputs. Using entropy as a fine-grained uncertainty metric, we uncover a universal law: the probability of following a modality decreases monotonically as its relative uncertainty increases. At the relative difficulty level where the model tends to follow both modalities with comparable probability what we call the balance point, a practical indicator of the model's inherent preference. Unlike traditional macro-level ratios, this measure offers a more principled and less confounded way to characterize modality bias, disentangling it from unimodal capabilities and dataset artifacts. Further, by probing layer-wise predictions, we reveal the internal mechanism of oscillation: in ambiguous regions near the balance point, models vacillate between modalities across layers, explaining externally observed indecision. Together, these findings establish relative uncertainty and inherent preference as the two governing principles of modality following, offering both a quantitative framework and mechanistic insight into how MLLMs resolve conflicting information.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unlocking the Power of Multi-Agent LLM for Reasoning: From Lazy Agents to Deliberation</title>
<link>https://arxiv.org/abs/2511.02303</link>
<guid>https://arxiv.org/abs/2511.02303</guid>
<content:encoded><![CDATA[
<div> Lazy behavior, multi-agent reasoning, causal influence, verifiable reward, complex reasoning tasks <br />
<br />
Lazy agent behavior in multi-agent reasoning can lead to one agent dominating while the other contributes little, hindering collaboration. This paper provides a theoretical analysis of why this occurs and introduces a method for measuring causal influence to address the issue. A verifiable reward mechanism is proposed to encourage the reasoning agent to deliberate, discard noisy outputs, consolidate instructions, and restart its reasoning process when needed. By mitigating lazy behavior and improving collaboration, the full potential of multi-agent frameworks for complex reasoning tasks can be realized. Experimental results demonstrate the effectiveness of the proposed framework. <br /><br />Summary: <div>
arXiv:2511.02303v1 Announce Type: new 
Abstract: Large Language Models (LLMs) trained with reinforcement learning and verifiable rewards have achieved strong results on complex reasoning tasks. Recent work extends this paradigm to a multi-agent setting, where a meta-thinking agent proposes plans and monitors progress while a reasoning agent executes subtasks through sequential conversational turns. Despite promising performance, we identify a critical limitation: lazy agent behavior, in which one agent dominates while the other contributes little, undermining collaboration and collapsing the setup to an ineffective single agent. In this paper, we first provide a theoretical analysis showing why lazy behavior naturally arises in multi-agent reasoning. We then introduce a stable and efficient method for measuring causal influence, helping mitigate this issue. Finally, as collaboration intensifies, the reasoning agent risks getting lost in multi-turn interactions and trapped by previous noisy responses. To counter this, we propose a verifiable reward mechanism that encourages deliberation by allowing the reasoning agent to discard noisy outputs, consolidate instructions, and restart its reasoning process when necessary. Extensive experiments demonstrate that our framework alleviates lazy agent behavior and unlocks the full potential of multi-agent framework for complex reasoning tasks.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chronic Kidney Disease Prognosis Prediction Using Transformer</title>
<link>https://arxiv.org/abs/2511.02340</link>
<guid>https://arxiv.org/abs/2511.02340</guid>
<content:encoded><![CDATA[
<div> Keywords: Chronic Kidney Disease, transformer-based framework, electronic health records, prediction, personalized care

Summary:
The article introduces a transformer-based framework called ProQ-BERT for predicting Chronic Kidney Disease (CKD) progression using electronic health records. The model integrates demographic, clinical, and laboratory data, employing quantization-based tokenization and attention mechanisms for interpretability. Pretrained with masked language modeling, ProQ-BERT was fine-tuned for predicting progression from stage 3a to stage 5 in varying follow-up periods. Evaluation on a cohort of 91,816 patients showed that ProQ-BERT outperformed CEHR-BERT, achieving high ROC-AUC and PR-AUC scores for short-term prediction. This study demonstrates the effectiveness of transformer architectures and temporal design choices in clinical prognosis modeling, offering a promising avenue for personalized CKD care.<br /><br />Summary: <div>
arXiv:2511.02340v1 Announce Type: new 
Abstract: Chronic Kidney Disease (CKD) affects nearly 10\% of the global population and often progresses to end-stage renal failure. Accurate prognosis prediction is vital for timely interventions and resource optimization. We present a transformer-based framework for predicting CKD progression using multi-modal electronic health records (EHR) from the Seoul National University Hospital OMOP Common Data Model. Our approach (\textbf{ProQ-BERT}) integrates demographic, clinical, and laboratory data, employing quantization-based tokenization for continuous lab values and attention mechanisms for interpretability. The model was pretrained with masked language modeling and fine-tuned for binary classification tasks predicting progression from stage 3a to stage 5 across varying follow-up and assessment periods. Evaluated on a cohort of 91,816 patients, our model consistently outperformed CEHR-BERT, achieving ROC-AUC up to 0.995 and PR-AUC up to 0.989 for short-term prediction. These results highlight the effectiveness of transformer architectures and temporal design choices in clinical prognosis modeling, offering a promising direction for personalized CKD care.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fuzzy Soft Set Theory based Expert System for the Risk Assessment in Breast Cancer Patients</title>
<link>https://arxiv.org/abs/2511.02392</link>
<guid>https://arxiv.org/abs/2511.02392</guid>
<content:encoded><![CDATA[
<div> Keywords: Breast cancer risk assessment, Fuzzy soft set theory, Expert system, Clinical parameters, Machine learning

Summary:
The study introduces a fuzzy soft set theory-based expert system for evaluating the risk of breast cancer in patients by considering Body Mass Index, Insulin Level, Leptin Level, Adiponectin Level, and age as input variables. Utilizing fuzzy inference rules and soft set computations, the system aims to provide a non-invasive method for estimating breast cancer risk using routine blood analyses. The dataset used for model development and validation was sourced from the UCI Machine Learning Repository. This expert system serves to aid healthcare professionals in identifying high-risk breast cancer patients and determining the need for further diagnostic procedures like biopsies. By integrating clinical and physiological parameters, the system enables early detection of breast cancer, thereby enhancing treatment effectiveness and survival rates.<br /><br />Summary: <div>
arXiv:2511.02392v1 Announce Type: new 
Abstract: Breast cancer remains one of the leading causes of mortality among women worldwide, with early diagnosis being critical for effective treatment and improved survival rates. However, timely detection continues to be a challenge due to the complex nature of the disease and variability in patient risk factors. This study presents a fuzzy soft set theory-based expert system designed to assess the risk of breast cancer in patients using measurable clinical and physiological parameters. The proposed system integrates Body Mass Index, Insulin Level, Leptin Level, Adiponectin Level, and age as input variables to estimate breast cancer risk through a set of fuzzy inference rules and soft set computations. These parameters can be obtained from routine blood analyses, enabling a non-invasive and accessible method for preliminary assessment. The dataset used for model development and validation was obtained from the UCI Machine Learning Repository. The proposed expert system aims to support healthcare professionals in identifying high-risk patients and determining the necessity of further diagnostic procedures such as biopsies.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A New Perspective on Precision and Recall for Generative Models</title>
<link>https://arxiv.org/abs/2511.02414</link>
<guid>https://arxiv.org/abs/2511.02414</guid>
<content:encoded><![CDATA[
<div> Keywords: generative models, evaluation, Precision and Recall, PR curve, binary classification

Summary:
In this paper, a new framework for estimating entire Precision and Recall (PR) curves for generative models is proposed, addressing the limitations of relying solely on scalar metrics for evaluation. The framework is based on a binary classification standpoint and includes a thorough statistical analysis, along with an upper bound on the PR estimation risk. It extends existing PR metrics by allowing for a richer analysis of the entire curve rather than just extreme values. Experimental findings showcase the different behaviors of the PR curves in various settings, highlighting the effectiveness of the proposed framework in providing a comprehensive evaluation of generative models. 

<br /><br />Summary: <div>
arXiv:2511.02414v1 Announce Type: new 
Abstract: With the recent success of generative models in image and text, the question of their evaluation has recently gained a lot of attention. While most methods from the state of the art rely on scalar metrics, the introduction of Precision and Recall (PR) for generative model has opened up a new avenue of research. The associated PR curve allows for a richer analysis, but their estimation poses several challenges. In this paper, we present a new framework for estimating entire PR curves based on a binary classification standpoint. We conduct a thorough statistical analysis of the proposed estimates. As a byproduct, we obtain a minimax upper bound on the PR estimation risk. We also show that our framework extends several landmark PR metrics of the literature which by design are restrained to the extreme values of the curve. Finally, we study the different behaviors of the curves obtained experimentally in various settings.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReAcTree: Hierarchical LLM Agent Trees with Control Flow for Long-Horizon Task Planning</title>
<link>https://arxiv.org/abs/2511.02424</link>
<guid>https://arxiv.org/abs/2511.02424</guid>
<content:encoded><![CDATA[
<div> propose, ReAcTree, hierarchical task-planning method, large language models, embodied autonomous agents

Summary:
ReAcTree is introduced as a hierarchical task-planning approach for embodied autonomous agents, decomposing complex goals into manageable subgoals within an agent tree. Each subgoal is handled by an LLM agent node, aided by control flow nodes for execution coordination. The method integrates episodic and working memory systems for improved performance. Experimental results on WAH-NL and ALFRED datasets show ReAcTree outperforming strong baselines such as ReAct across various LLMs. Notably, ReAcTree achieves a 61% goal success rate with Qwen 2.5 72B on WAH-NL, nearly doubling ReAct's performance of 31%. <div>
arXiv:2511.02424v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) have enabled significant progress in decision-making and task planning for embodied autonomous agents. However, most existing methods still struggle with complex, long-horizon tasks because they rely on a monolithic trajectory that entangles all past decisions and observations, attempting to solve the entire task in a single unified process. To address this limitation, we propose ReAcTree, a hierarchical task-planning method that decomposes a complex goal into more manageable subgoals within a dynamically constructed agent tree. Each subgoal is handled by an LLM agent node capable of reasoning, acting, and further expanding the tree, while control flow nodes coordinate the execution strategies of agent nodes. In addition, we integrate two complementary memory systems: each agent node retrieves goal-specific, subgoal-level examples from episodic memory and shares environment-specific observations through working memory. Experiments on the WAH-NL and ALFRED datasets demonstrate that ReAcTree consistently outperforms strong task-planning baselines such as ReAct across diverse LLMs. Notably, on WAH-NL, ReAcTree achieves a 61% goal success rate with Qwen 2.5 72B, nearly doubling ReAct's 31%.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auditable-choice reframing unlocks RL-based verification for open-ended tasks</title>
<link>https://arxiv.org/abs/2511.02463</link>
<guid>https://arxiv.org/abs/2511.02463</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Verifiable Rewards, Language Models, Reasoning, Open-ended tasks 

Summary: 
Reinforcement Learning with Verifiable Rewards (RLVR) has shown promise in improving the reasoning abilities of large language models (LLMs) in structured domains like mathematics and programming. However, its effectiveness in open-ended tasks lacking clear answers has been limited. To address this gap, the authors propose a new training strategy, Verifiable Multiple-Choice Reformulation (VMR), that converts open-ended data into multiple-choice formats to enable effective training without explicit ground truth. Experimental results across multiple benchmarks demonstrate that VMR-based training leads to a significant average performance improvement of 5.99 points over the baseline on open-ended tasks. This approach expands the applicability of RLVR to a wider range of tasks and showcases the potential of strengthening reasoning capabilities in enhancing LLM performance in open-ended scenarios. Code for replication will be made available upon acceptance for increased transparency and reproducibility. 

<br /><br />Summary: <div>
arXiv:2511.02463v1 Announce Type: new 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated great potential in enhancing the reasoning capabilities of large language models (LLMs), achieving remarkable progress in domains such as mathematics and programming where standard answers are available. However, for open-ended tasks lacking ground-truth solutions (e.g., creative writing and instruction following), existing studies typically regard them as non-reasoning scenarios, thereby overlooking the latent value of reasoning capabilities. This raises a key question: Can strengthening reasoning improve performance in open-ended tasks? To address this, we explore the transfer of the RLVR paradigm to the open domain. Yet, since RLVR fundamentally relies on verifiers that presuppose the existence of standard answers, it cannot be directly applied to open-ended tasks. To overcome this challenge, we introduce Verifiable Multiple-Choice Reformulation (VMR), a novel training strategy that restructures open-ended data into verifiable multiple-choice formats, enabling effective training even in the absence of explicit ground truth. Experimental results on multiple benchmarks validate the effectiveness of our method in improving LLM performance on open-ended tasks. Notably, across eight open-ended benchmarks, our VMR-based training delivers an average gain of 5.99 points over the baseline. Code will be released upon acceptance to facilitate reproducibility.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agentic AI for Mobile Network RAN Management and Optimization</title>
<link>https://arxiv.org/abs/2511.02532</link>
<guid>https://arxiv.org/abs/2511.02532</guid>
<content:encoded><![CDATA[
<div> Agentic AI, Large AI Models, 5G, 6G networks, RAN optimization <br />
Summary: This paper introduces the concept of Agentic AI, which utilizes Large AI Models to automate complex systems with human-level cognitive abilities. The need for automation in dynamic RAN environments, such as 5G and 6G networks, is highlighted. The evolution of Agentic AI from classical agents to intelligent behaviors orchestrated by core design patterns is discussed. A practical use case in 5G RAN management and optimization demonstrates how time-series analytics and LAM-driven agents collaborate for autonomous decision-making based on Key Performance Indicators (KPIs). The paper contributes to ongoing research by outlining foundational components and operational principles of Agentic AI systems, bridging the gap between theoretical concepts and practical applications in mobile networks. <br /> <div>
arXiv:2511.02532v1 Announce Type: new 
Abstract: Agentic AI represents a new paradigm for automating complex systems by using Large AI Models (LAMs) to provide human-level cognitive abilities with multimodal perception, planning, memory, and reasoning capabilities. This will lead to a new generation of AI systems that autonomously decompose goals, retain context over time, learn continuously, operate across tools and environments, and adapt dynamically. The complexity of 5G and upcoming 6G networks renders manual optimization ineffective, pointing to Agentic AI as a method for automating decisions in dynamic RAN environments. However, despite its rapid advances, there is no established framework outlining the foundational components and operational principles of Agentic AI systems nor a universally accepted definition.
  This paper contributes to ongoing research on Agentic AI in 5G and 6G networks by outlining its core concepts and then proposing a practical use case that applies Agentic principles to RAN optimization. We first introduce Agentic AI, tracing its evolution from classical agents and discussing the progress from workflows and simple AI agents to Agentic AI. Core design patterns-reflection, planning, tool use, and multi-agent collaboration-are then described to illustrate how intelligent behaviors are orchestrated. These theorical concepts are grounded in the context of mobile networks, with a focus on RAN management and optimization. A practical 5G RAN case study shows how time-series analytics and LAM-driven agents collaborate for KPI-based autonomous decision-making.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge Graph-enhanced Large Language Model for Incremental Game PlayTesting</title>
<link>https://arxiv.org/abs/2511.02534</link>
<guid>https://arxiv.org/abs/2511.02534</guid>
<content:encoded><![CDATA[
<div> Keywords: Automated playtesting, Knowledge Graph, Large Language Models, Incremental updates, Testing efficiency

Summary: 
The paper introduces the KLPEG framework, which aims to address the challenges posed by rapid iterations and frequent updates in modern video games. The framework leverages a Knowledge Graph (KG) to model game elements, task dependencies, and causal relationships, enabling knowledge accumulation and reuse across different versions. By utilizing Large Language Models (LLMs) to parse natural language update logs and conduct multi-hop reasoning on the KG, KLPEG can generate update-tailored test cases with improved accuracy and efficiency. Experimental results in Overcooked and Minecraft environments demonstrate the framework's ability to accurately locate affected functionalities and complete tests in fewer steps, leading to enhanced playtesting effectiveness. KLPEG offers a structured approach to testing tailored for incremental game updates, enhancing testing efficiency while ensuring precise and effective testing outcomes. 

Summary: <div>
arXiv:2511.02534v1 Announce Type: new 
Abstract: The rapid iteration and frequent updates of modern video games pose significant challenges to the efficiency and specificity of testing. Although automated playtesting methods based on Large Language Models (LLMs) have shown promise, they often lack structured knowledge accumulation mechanisms, making it difficult to conduct precise and efficient testing tailored for incremental game updates. To address this challenge, this paper proposes a KLPEG framework. The framework constructs and maintains a Knowledge Graph (KG) to systematically model game elements, task dependencies, and causal relationships, enabling knowledge accumulation and reuse across versions. Building on this foundation, the framework utilizes LLMs to parse natural language update logs, identify the scope of impact through multi-hop reasoning on the KG, enabling the generation of update-tailored test cases. Experiments in two representative game environments, Overcooked and Minecraft, demonstrate that KLPEG can more accurately locate functionalities affected by updates and complete tests in fewer steps, significantly improving both playtesting effectiveness and efficiency.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The ORCA Benchmark: Evaluating Real-World Calculation Accuracy in Large Language Models</title>
<link>https://arxiv.org/abs/2511.02589</link>
<guid>https://arxiv.org/abs/2511.02589</guid>
<content:encoded><![CDATA[
<div> finance, physics, health, statistics, language models

Summary:
- ORCA Benchmark evaluates large language models (LLMs) on multi-domain quantitative reasoning tasks using verified outputs from Omni's calculator engine.
- Five state-of-the-art systems achieved an accuracy of $45\text{--}63\,\% in 500 natural-language tasks across domains such as finance, physics, health, and statistics.
- Errors primarily stem from rounding (35%) and calculation mistakes (33%).
- Strengths were observed in mathematics and engineering, while weaknesses were identified in physics and natural sciences.
- Correlation analysis shows that models fail together but differ in the types of errors they make, indicating partial complementarity rather than redundancy.

<br /><br />Summary: <div>
arXiv:2511.02589v2 Announce Type: new 
Abstract: We present ORCA (Omni Research on Calculation in AI) Benchmark - a novel benchmark that evaluates large language models (LLMs) on multi-domain, real-life quantitative reasoning using verified outputs from Omni's calculator engine. In 500 natural-language tasks across domains such as finance, physics, health, and statistics, the five state-of-the-art systems (ChatGPT-5, Gemini~2.5~Flash, Claude~Sonnet~4.5, Grok~4, and DeepSeek~V3.2) achieved only $45\text{--}63\,\%$ accuracy, with errors mainly related to rounding ($35\,\%$) and calculation mistakes ($33\,\%$). Results in specific domains indicate strengths in mathematics and engineering, but weaknesses in physics and natural sciences. Correlation analysis ($r \approx 0.40\text{--}0.65$) shows that the models often fail together but differ in the types of errors they make, highlighting their partial complementarity rather than redundancy. Unlike standard math datasets, ORCA evaluates step-by-step reasoning, numerical precision, and domain generalization across real problems from finance, physics, health, and statistics.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive GR(1) Specification Repair for Liveness-Preserving Shielding in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.02605</link>
<guid>https://arxiv.org/abs/2511.02605</guid>
<content:encoded><![CDATA[
<div> shielding, reinforcement learning, Generalized Reactivity, Inductive Logic Programming, GR(1) specifications

Summary:
- The paper introduces an adaptive shielding framework for reinforcement learning based on Generalized Reactivity of rank 1 (GR(1)) specifications.
- The framework can detect environment assumption violations at runtime and use Inductive Logic Programming (ILP) to automatically repair specifications.
- This adaptive approach ensures that the shield evolves gracefully, optimizing for liveness and weakening goals only when necessary.
- Case studies on Minepump and Atari Seaquest demonstrate that static symbolic controllers are suboptimal compared to the adaptive shield when optimizing for auxiliary rewards.
- Reinforcement learning agents equipped with the adaptive shield maintain near-optimal reward and perfect logical compliance, outperforming static shields. 

<br /><br />Summary: <div>
arXiv:2511.02605v1 Announce Type: new 
Abstract: Shielding is widely used to enforce safety in reinforcement learning (RL), ensuring that an agent's actions remain compliant with formal specifications. Classical shielding approaches, however, are often static, in the sense that they assume fixed logical specifications and hand-crafted abstractions. While these static shields provide safety under nominal assumptions, they fail to adapt when environment assumptions are violated. In this paper, we develop the first adaptive shielding framework - to the best of our knowledge - based on Generalized Reactivity of rank 1 (GR(1)) specifications, a tractable and expressive fragment of Linear Temporal Logic (LTL) that captures both safety and liveness properties. Our method detects environment assumption violations at runtime and employs Inductive Logic Programming (ILP) to automatically repair GR(1) specifications online, in a systematic and interpretable way. This ensures that the shield evolves gracefully, ensuring liveness is achievable and weakening goals only when necessary. We consider two case studies: Minepump and Atari Seaquest; showing that (i) static symbolic controllers are often severely suboptimal when optimizing for auxiliary rewards, and (ii) RL agents equipped with our adaptive shield maintain near-optimal reward and perfect logical compliance compared with static shields.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-Agent Psychological Simulation System for Human Behavior Modeling</title>
<link>https://arxiv.org/abs/2511.02606</link>
<guid>https://arxiv.org/abs/2511.02606</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent simulation, human behavior, psychological theories, teacher training, cognitive apprenticeship

Summary:
The article introduces a multi-agent psychological simulation system that models internal cognitive-affective processes to generate realistic human behaviors. Unlike black-box neural models, this system is based on established psychological theories and explicitly simulates various psychological factors within an "inner parliament" of agents. These agents interact to determine the system's output behavior, providing transparency and alignment with human psychology. The system's architecture, theoretical foundations, and applications in teacher training and research are discussed. It embodies principles of social learning, cognitive apprenticeship, deliberate practice, and meta-cognition, making it a valuable tool for training and education in human-centered fields. <div>
arXiv:2511.02606v1 Announce Type: new 
Abstract: Training and education in human-centered fields require authentic practice, yet realistic simulations of human behavior have remained limited. We present a multi-agent psychological simulation system that models internal cognitive-affective processes to generate believable human behaviors. In contrast to black-box neural models, this system is grounded in established psychological theories (e.g., self-efficacy, mindset, social constructivism) and explicitly simulates an ``inner parliament'' of agents corresponding to key psychological factors. These agents deliberate and interact to determine the system's output behavior, enabling unprecedented transparency and alignment with human psychology. We describe the system's architecture and theoretical foundations, illustrate its use in teacher training and research, and discuss how it embodies principles of social learning, cognitive apprenticeship, deliberate practice, and meta-cognition.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DecompSR: A dataset for decomposed analyses of compositional multihop spatial reasoning</title>
<link>https://arxiv.org/abs/2511.02627</link>
<guid>https://arxiv.org/abs/2511.02627</guid>
<content:encoded><![CDATA[
<div> benchmark dataset, spatial reasoning, compositionality, Large Language Models, systematicity
Summary: 
DecompSR introduces a benchmark dataset for decomposed spatial reasoning, with over 5 million data points. It allows for independent variation in compositionality factors such as productivity, substitutivity, overgeneralisation, and systematicity. The dataset is generated in a manner that ensures correctness by construction, with verification using a symbolic solver. Benchmarking on Large Language Models (LLMs) reveals their struggles with productive and systematic generalisation in spatial reasoning tasks, while showing more robustness to linguistic variation. DecompSR offers a rigorous and provably correct dataset for assessing the compositional reasoning abilities of LLMs, enabling fine-grained analysis of their performance in spatial reasoning tasks.
<br /><br />Summary: <div>
arXiv:2511.02627v1 Announce Type: new 
Abstract: We introduce DecompSR, decomposed spatial reasoning, a large benchmark dataset (over 5m datapoints) and generation framework designed to analyse compositional spatial reasoning ability. The generation of DecompSR allows users to independently vary several aspects of compositionality, namely: productivity (reasoning depth), substitutivity (entity and linguistic variability), overgeneralisation (input order, distractors) and systematicity (novel linguistic elements). DecompSR is built procedurally in a manner which makes it is correct by construction, which is independently verified using a symbolic solver to guarantee the correctness of the dataset. DecompSR is comprehensively benchmarked across a host of Large Language Models (LLMs) where we show that LLMs struggle with productive and systematic generalisation in spatial reasoning tasks whereas they are more robust to linguistic variation. DecompSR provides a provably correct and rigorous benchmarking dataset with a novel ability to independently vary the degrees of several key aspects of compositionality, allowing for robust and fine-grained probing of the compositional reasoning abilities of LLMs.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Collaboration Gap</title>
<link>https://arxiv.org/abs/2511.02687</link>
<guid>https://arxiv.org/abs/2511.02687</guid>
<content:encoded><![CDATA[
<div> collaborative maze-solving benchmark, agent-based systems, heterogeneous agents, collaboration gap, relay inference<br />
<br />
Summary:<br />
The article discusses the importance of evaluating agent-agent collaboration in AI systems composed of independently developed agents with different capabilities. A collaborative maze-solving benchmark is proposed to assess collaborative capabilities, problem complexity, and automated grading. The study evaluated 32 AI models in solo, homogeneous, and heterogeneous pairings, revealing a significant "collaboration gap" where models performing well solo struggle to collaborate effectively. A "relay inference" approach, where the stronger agent leads before handing off to the weaker one, proves effective in closing this gap. The findings emphasize the need for collaboration-aware evaluation, training strategies to enhance collaborative capabilities, and interaction design that elicits agents' latent skills. These insights apply to AI-AI and human-AI collaboration, highlighting the importance of developing collaborative skills in AI systems.<br /> <div>
arXiv:2511.02687v1 Announce Type: new 
Abstract: The trajectory of AI development suggests that we will increasingly rely on agent-based systems composed of independently developed agents with different information, privileges, and tools. The success of these systems will critically depend on effective collaboration among these heterogeneous agents, even under partial observability. Despite intense interest, few empirical studies have evaluated such agent-agent collaboration at scale. We propose a collaborative maze-solving benchmark that (i) isolates collaborative capabilities, (ii) modulates problem complexity, (iii) enables scalable automated grading, and (iv) imposes no output-format constraints, preserving ecological plausibility. Using this framework, we evaluate 32 leading open- and closed-source models in solo, homogeneous, and heterogeneous pairings. Our results reveal a "collaboration gap": models that perform well solo often degrade substantially when required to collaborate. Collaboration can break down dramatically; for instance, small distilled models that solve mazes well alone may fail almost completely in certain pairings. We find that starting with the stronger agent often improves outcomes, motivating a "relay inference" approach where the stronger agent leads before handing off to the weaker one, closing much of the gap. Our findings argue for (1) collaboration-aware evaluation, (2) training strategies developed to enhance collaborative capabilities, and (3) interaction design that reliably elicits agents' latent skills, guidance that applies to AI-AI and human-AI collaboration.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in Dynamic Environments for LLM Tool-Use Agents</title>
<link>https://arxiv.org/abs/2511.02734</link>
<guid>https://arxiv.org/abs/2511.02734</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, CostBench, economic reasoning, replanning abilities, dynamic blocking events

Summary: 
CostBench is a new benchmark designed to assess the economic reasoning and replanning abilities of Large Language Model (LLM) agents. It focuses on task completion efficiency, resource usage, and adaptability, crucial for real-world applications. The benchmark, situated in the travel-planning domain, involves tasks that can be solved through different sequences of tools with customizable costs. It includes dynamic blocking events to simulate real-world unpredictability and test agents' ability to adapt in real time. Evaluation of various LLM models on CostBench shows significant limitations in cost-aware planning, with most agents failing to identify cost-optimal solutions in both static and dynamic settings. For example, GPT-5 achieves less than 75% exact match rate on the most challenging tasks, dropping further under dynamic conditions. By highlighting these weaknesses, CostBench paves the way for developing more economically rational and robust language models. <br /><br />Summary: <div>
arXiv:2511.02734v1 Announce Type: new 
Abstract: Current evaluations of Large Language Model (LLM) agents primarily emphasize task completion, often overlooking resource efficiency and adaptability. This neglects a crucial capability: agents' ability to devise and adjust cost-optimal plans in response to changing environments. To bridge this gap, we introduce CostBench, a scalable, cost-centric benchmark designed to evaluate agents' economic reasoning and replanning abilities. Situated in the travel-planning domain, CostBench comprises tasks solvable via multiple sequences of atomic and composite tools with diverse, customizable costs. It also supports four types of dynamic blocking events, such as tool failures and cost changes, to simulate real-world unpredictability and necessitate agents to adapt in real time. Evaluating leading open-sourced and proprietary models on CostBench reveals a substantial gap in cost-aware planning: agents frequently fail to identify cost-optimal solutions in static settings, with even GPT-5 achieving less than 75% exact match rate on the hardest tasks, and performance further dropping by around 40% under dynamic conditions. By diagnosing these weaknesses, CostBench lays the groundwork for developing future agents that are both economically rational and robust.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Using Span Queries to Optimize for Cache and Attention Locality</title>
<link>https://arxiv.org/abs/2511.02749</link>
<guid>https://arxiv.org/abs/2511.02749</guid>
<content:encoded><![CDATA[
<div> Span Query, Inference Server, Chat Completion, Inference-time Scaling, Deep Reasoning

Summary:
Span queries are introduced to generalize the interface of inference servers, allowing for various use cases like chat, RAG, inference-time scaling, and agentic workloads. The distinction lies in the order of inputs and whether they commute. Span queries are expression trees of inference calls linked with commutativity constraints, optimized for KV cache locality and attention locality. A small change to vLLM enables high-performance execution of span queries, resulting in 10-20x reductions in TTFT for non-chat use cases. An attention-optimized span query on a 2b parameter model outperforms a stock inference server using an 8b model in accuracy. This advancement showcases the potential of span queries in optimizing inference servers for diverse inference tasks beyond chat completion.<br /><br />Summary: <div>
arXiv:2511.02749v1 Announce Type: new 
Abstract: Clients are evolving beyond chat completion, and now include a variety of innovative inference-time scaling and deep reasoning techniques. At the same time, inference servers remain heavily optimized for chat completion. Prior work has shown that large improvements to KV cache hit rate are possible if inference servers evolve towards these non-chat use cases. However, they offer solutions that are also optimized for a single use case, RAG. In this paper, we introduce the span query to generalize the interface to the inference server. We demonstrate that chat, RAG, inference-time scaling, and agentic workloads can all be expressed as span queries. We show how the critical distinction that had been assumed by prior work lies in whether the order of the inputs matter -- do they commute? In chat, they do not. In RAG, they often do. This paper introduces span queries, which are expression trees of inference calls, linked together with commutativity constraints. We describe span query syntax and semantics. We show how they can be automatically optimized to improve KV cache locality. We show how a small change to vLLM (affecting only 492 lines) can enable high-performance execution of span queries. Using this stack, we demonstrate that span queries can achieve 10-20x reductions in TTFT for two distinct non-chat use cases. Finally, we show that span queries can also be optimized to improve attention locality, so as to avoid the so-called lost-in-the-middle problem. We demonstrate that an attention-optimized span query on a 2b parameter model vastly outperforms the accuracy of a stock inference server using an 8b model.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-Supported Formal Knowledge Representation for Enhancing Control Engineering Content with an Interactive Semantic Layer</title>
<link>https://arxiv.org/abs/2511.02759</link>
<guid>https://arxiv.org/abs/2511.02759</guid>
<content:encoded><![CDATA[
<div> Keywords: control engineering, knowledge representation, language models, formalization, knowledge graph

Summary: 
This paper introduces a method for generating formal knowledge representations in the field of control engineering using language models. The method, called PyIRK, combines human readability with machine interpretability to create a formalized knowledge graph from natural-language descriptions and mathematical definitions. The goal is to enhance source documents with an interactive semantic layer to improve knowledge transfer. By utilizing this approach, the authors aim to contribute to the development of easily accessible, collaborative, and verifiable knowledge bases for the control engineering domain. <div>
arXiv:2511.02759v1 Announce Type: new 
Abstract: The rapid growth of research output in control engineering calls for new approaches to structure and formalize domain knowledge. This paper briefly describes an LLM-supported method for semi-automated generation of formal knowledge representations that combine human readability with machine interpretability and increased expressiveness. Based on the Imperative Representation of Knowledge (PyIRK) framework, we demonstrate how language models can assist in transforming natural-language descriptions and mathematical definitions (available as LaTeX source code) into a formalized knowledge graph. As a first application we present the generation of an ``interactive semantic layer'' to enhance the source documents in order to facilitate knowledge transfer. From our perspective this contributes to the vision of easily accessible, collaborative, and verifiable knowledge bases for the control engineering domain.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When One Modality Sabotages the Others: A Diagnostic Lens on Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2511.02794</link>
<guid>https://arxiv.org/abs/2511.02794</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal large language models, reasoning traces, modality sabotage, diagnostic layer, fusion dynamics

Summary:
Modality sabotage is introduced as a diagnostic failure mode in multimodal large language models (MLLMs), where a high-confidence error from one modality misleads the final prediction. A model-agnostic evaluation layer is proposed to treat each modality as an agent, producing candidate labels and self-assessments for auditing. The fusion mechanism aggregates outputs to identify contributors and saboteurs, shedding light on the reliability profiles of multimodal reasoning. A case study on emotion recognition benchmarks using foundation models demonstrates the framework's ability to pinpoint failures caused by dataset artifacts or model limitations. This diagnostic scaffold offers insight into fusion dynamics, supporting principled auditing and interventions for improving multimodal reasoning systems.

<br /><br />Summary: Modality sabotage in multimodal large language models can lead to high-confidence errors that mislead predictions. An evaluation layer treats each modality as an agent, identifying contributors and saboteurs in the fusion process. Case studies show how this framework can reveal the reliability profiles of multimodal reasoning and pinpoint failures caused by dataset artifacts or model limitations, ultimately enabling more informed interventions to enhance system performance. <div>
arXiv:2511.02794v1 Announce Type: new 
Abstract: Despite rapid growth in multimodal large language models (MLLMs), their reasoning traces remain opaque: it is often unclear which modality drives a prediction, how conflicts are resolved, or when one stream dominates. In this paper, we introduce modality sabotage, a diagnostic failure mode in which a high-confidence unimodal error overrides other evidence and misleads the fused result. To analyze such dynamics, we propose a lightweight, model-agnostic evaluation layer that treats each modality as an agent, producing candidate labels and a brief self-assessment used for auditing. A simple fusion mechanism aggregates these outputs, exposing contributors (modalities supporting correct outcomes) and saboteurs (modalities that mislead). Applying our diagnostic layer in a case study on multimodal emotion recognition benchmarks with foundation models revealed systematic reliability profiles, providing insight into whether failures may arise from dataset artifacts or model limitations. More broadly, our framework offers a diagnostic scaffold for multimodal reasoning, supporting principled auditing of fusion dynamics and informing possible interventions.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning</title>
<link>https://arxiv.org/abs/2511.02818</link>
<guid>https://arxiv.org/abs/2511.02818</guid>
<content:encoded><![CDATA[
<div> Keywords: tabular data, in-context learning, Orion-MSP, multi-scale processing, block-sparse attention<br />
Summary: <br />
Orion-MSP introduces a new tabular in-context learning architecture that addresses key limitations of current models. It features multi-scale processing to capture hierarchical feature interactions, block-sparse attention for efficient scalability and long-range connectivity, and a Perceiver-style memory for bidirectional information flow across components. The model achieves state-of-the-art performance on diverse benchmarks and scales effectively to high-dimensional tables. The architecture of Orion-MSP allows for iterative representation refinement and cross-component communication, improving the overall efficiency of tabular data processing. The model is publicly available for use. <div>
arXiv:2511.02818v1 Announce Type: new 
Abstract: Tabular data remain the predominant format for real-world applications. Yet, developing effective neural models for tabular data remains challenging due to heterogeneous feature types and complex interactions occurring at multiple scales. Recent advances in tabular in-context learning (ICL), such as TabPFN and TabICL, have achieved state-of-the-art performance comparable to gradient-boosted trees (GBTs) without task-specific fine-tuning. However, current architectures exhibit key limitations: (1) single-scale feature processing that overlooks hierarchical dependencies, (2) dense attention with quadratic scaling in table width, and (3) strictly sequential component processing that prevents iterative representation refinement and cross-component communication. To address these challenges, we introduce Orion-MSP, a tabular ICL architecture featuring three key innovations: (1) multi-scale processing to capture hierarchical feature interactions; (2) block-sparse attention combining windowed, global, and random patterns for scalable efficiency and long-range connectivity; and (3) a Perceiver-style memory enabling safe bidirectional information flow across components. Across diverse benchmarks, Orion-MSP matches or surpasses state-of-the-art performance while scaling effectively to high-dimensional tables, establishing a new standard for efficient tabular in-context learning. The model is publicly available at https://github.com/Lexsi-Labs/Orion-MSP .
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing AI Agent Attacks With Synthetic Data</title>
<link>https://arxiv.org/abs/2511.02823</link>
<guid>https://arxiv.org/abs/2511.02823</guid>
<content:encoded><![CDATA[
<div> Framework, AI control, attack policies, SHADE-Arena, optimization

Summary:
This article presents a method to optimize attack policies in complex control environments using the SHADE-Arena dataset. The approach decomposes attack capability into five key skills: suspicion modeling, attack selection, plan synthesis, execution, and subtlety. By optimizing each skill individually and developing a probabilistic model of attack dynamics, the authors improve attack strength significantly. Despite limited data, simulations of attack hyperparameters lead to tangible improvements that transfer effectively to the real-world environment of SHADE-Arena. The results showcase a notable decrease in safety scores, from 0.87 to 0.41, underscoring the efficacy of the proposed approach in enhancing AI risk estimation in high-stakes scenarios. <div>
arXiv:2511.02823v1 Announce Type: new 
Abstract: As AI deployments become more complex and high-stakes, it becomes increasingly important to be able to estimate their risk. AI control is one framework for doing so. However, good control evaluations require eliciting strong attack policies. This can be challenging in complex agentic environments where compute constraints leave us data-poor. In this work, we show how to optimize attack policies in SHADE-Arena, a dataset of diverse realistic control environments. We do this by decomposing attack capability into five constituent skills -- suspicion modeling, attack selection, plan synthesis, execution and subtlety -- and optimizing each component individually. To get around the constraint of limited data, we develop a probabilistic model of attack dynamics, optimize our attack hyperparameters using this simulation, and then show that the results transfer to SHADE-Arena. This results in a substantial improvement in attack strength, reducing safety score from a baseline of 0.87 to 0.41 using our scaffold.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Kosmos: An AI Scientist for Autonomous Discovery</title>
<link>https://arxiv.org/abs/2511.02824</link>
<guid>https://arxiv.org/abs/2511.02824</guid>
<content:encoded><![CDATA[
<div> Keywords: AI scientist, data-driven discovery, structured world model, scientific reports, automated research 

Summary: 
Kosmos is an AI scientist that automates data-driven discovery by running cycles of data analysis, literature search, and hypothesis generation over 200 agent rollouts. It utilizes a structured world model to share information between agents and cites all statements in its reports with code or primary literature for traceability. Independent validation found 79.4% accuracy in Kosmos reports, with collaborators reporting equivalent research time savings and linear scaling of valuable scientific findings with Kosmos cycles. Seven discoveries made by Kosmos span various scientific fields, replicating unpublished findings and making new contributions to the literature. Kosmos showcases the potential of AI in accelerating scientific research through automated processes and collaborative capabilities.

<br /><br />Summary: <div>
arXiv:2511.02824v2 Announce Type: new 
Abstract: Data-driven scientific discovery requires iterative cycles of literature search, hypothesis generation, and data analysis. Substantial progress has been made towards AI agents that can automate scientific research, but all such agents remain limited in the number of actions they can take before losing coherence, thus limiting the depth of their findings. Here we present Kosmos, an AI scientist that automates data-driven discovery. Given an open-ended objective and a dataset, Kosmos runs for up to 12 hours performing cycles of parallel data analysis, literature search, and hypothesis generation before synthesizing discoveries into scientific reports. Unlike prior systems, Kosmos uses a structured world model to share information between a data analysis agent and a literature search agent. The world model enables Kosmos to coherently pursue the specified objective over 200 agent rollouts, collectively executing an average of 42,000 lines of code and reading 1,500 papers per run. Kosmos cites all statements in its reports with code or primary literature, ensuring its reasoning is traceable. Independent scientists found 79.4% of statements in Kosmos reports to be accurate, and collaborators reported that a single 20-cycle Kosmos run performed the equivalent of 6 months of their own research time on average. Furthermore, collaborators reported that the number of valuable scientific findings generated scales linearly with Kosmos cycles (tested up to 20 cycles). We highlight seven discoveries made by Kosmos that span metabolomics, materials science, neuroscience, and statistical genetics. Three discoveries independently reproduce findings from preprinted or unpublished manuscripts that were not accessed by Kosmos at runtime, while four make novel contributions to the scientific literature.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neurosymbolic Deep Learning Semantics</title>
<link>https://arxiv.org/abs/2511.02825</link>
<guid>https://arxiv.org/abs/2511.02825</guid>
<content:encoded><![CDATA[
<div> Neurosymbolic framework, Logic, Deep learning, Artificial Intelligence, Semantic encoding
<br />
Summary: 
This article discusses the need for semantics in artificial intelligence (AI) to enhance scientific discoveries. It argues that logic provides a suitable framework for formalizing AI-based science. The paper focuses on using logic in a neurosymbolic framework to give semantics to deep learning, a key technology in AI. The lack of general conditions in deep learning and neurosymbolic AI is highlighted, with various encoding and knowledge extraction approaches designed for specific cases. The framework introduced in the article aims to establish a clear mapping between neural networks and logic, elucidating common elements of different approaches. It describes the linkage between logical semantics and neural networks, reviews key techniques for neural encoding and knowledge extraction, presents a formal definition of the framework, and discusses challenges in identifying semantic encoding in practice, drawing parallels to issues in the philosophy of mind. <div>
arXiv:2511.02825v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) is a powerful new language of science as evidenced by recent Nobel Prizes in chemistry and physics that recognized contributions to AI applied to those areas. Yet, this new language lacks semantics, which makes AI's scientific discoveries unsatisfactory at best. With the purpose of uncovering new facts but also improving our understanding of the world, AI-based science requires formalization through a framework capable of translating insight into comprehensible scientific knowledge. In this paper, we argue that logic offers an adequate framework. In particular, we use logic in a neurosymbolic framework to offer a much needed semantics for deep learning, the neural network-based technology of current AI. Deep learning and neurosymbolic AI lack a general set of conditions to ensure that desirable properties are satisfied. Instead, there is a plethora of encoding and knowledge extraction approaches designed for particular cases. To rectify this, we introduced a framework for semantic encoding, making explicit the mapping between neural networks and logic, and characterizing the common ingredients of the various existing approaches. In this paper, we describe succinctly and exemplify how logical semantics and neural networks are linked through this framework, we review some of the most prominent approaches and techniques developed for neural encoding and knowledge extraction, provide a formal definition of our framework, and discuss some of the difficulties of identifying a semantic encoding in practice in light of analogous problems in the philosophy of mind.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for Understanding Anything</title>
<link>https://arxiv.org/abs/2511.02834</link>
<guid>https://arxiv.org/abs/2511.02834</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal large language models, Agent-Omni framework, Flexible multimodal reasoning, Cross-modal reasoning, Interpretability

Summary: 
The paper introduces the Agent-Omni framework, which facilitates multimodal reasoning by coordinating existing foundation models through a master-agent system. This framework enables flexible integration of text, images, audio, and video without the need for retraining, leading to improved performance, especially in tasks requiring complex cross-modal reasoning. The master agent interprets user intent, delegates tasks to modality-specific agents, and consolidates their outputs for coherent responses. Agent-Omni consistently achieves state-of-the-art results across various benchmarks and demonstrates transparency and interpretability in its design. The modular nature of the framework allows for easy extension and integration of newer, stronger models in the future. <div>
arXiv:2511.02834v2 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) have shown strong capabilities but remain limited to fixed modality pairs and require costly fine-tuning with large aligned datasets. Building fully omni-capable models that can integrate text, images, audio, and video remains impractical and lacks robust reasoning support. In this paper, we propose an Agent-Omni framework that coordinates existing foundation models through a master-agent system, enabling flexible multimodal reasoning without retraining. The master agent interprets user intent, delegates subtasks to modality-specific agents, and integrates their outputs into coherent responses. Extensive experiments across text, image, audio, video, and omni benchmarks show that Agent-Omni consistently achieves state-of-the-art performance, particularly on tasks requiring complex cross-modal reasoning. Its agent-based design enables seamless integration of specialized foundation models, ensuring adaptability to diverse inputs while maintaining transparency and interpretability. In addition, the framework is modular and easily extensible, allowing future improvements as stronger models become available.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EdgeReasoning: Characterizing Reasoning LLM Deployment on Edge GPUs</title>
<link>https://arxiv.org/abs/2511.01866</link>
<guid>https://arxiv.org/abs/2511.01866</guid>
<content:encoded><![CDATA[
<div> edge intelligence, autonomous systems, large language models, edge GPUs, latency-accuracy tradeoffs

Summary:
Edge intelligence is essential for autonomous systems like robotics, offering energy and cost advantages over cloud-based solutions. However, deploying large language models (LLMs) on edge GPUs for reasoning tasks faces challenges like strict latency constraints. The EdgeReasoning study explores various LLM architectures and model sizes to balance latency and accuracy. Additionally, techniques like prompt-based and model-tuning-based methods are evaluated to reduce reasoning token length while maintaining performance quality. Test-time scaling strategies with varying parallelism levels are profiled to maximize accuracy within strict latency budgets. This study maps the Pareto frontier of achievable accuracy-latency configurations, providing systematic guidance for optimal edge deployment of reasoning LLMs. 

<br /><br />Summary: <div>
arXiv:2511.01866v1 Announce Type: cross 
Abstract: Edge intelligence paradigm is increasingly demanded by the emerging autonomous systems, such as robotics. Beyond ensuring privacy-preserving operation and resilience in connectivity-limited environments, edge deployment offers significant energy and cost advantages over cloud-based solutions. However, deploying large language models (LLMs) for reasoning tasks on edge GPUs faces critical challenges from strict latency constraints and limited computational resources. To navigate these constraints, developers must balance multiple design factors - choosing reasoning versus non-reasoning architectures, selecting appropriate model sizes, allocating token budgets, and applying test-time scaling strategies - to meet target latency and optimize accuracy. Yet guidance on optimal combinations of these variables remains scarce. In this work, we present EdgeReasoning, a comprehensive study characterizing the deployment of reasoning LLMs on edge GPUs. We systematically quantify latency-accuracy tradeoffs across various LLM architectures and model sizes. We systematically evaluate prompt-based and model-tuning-based techniques for reducing reasoning token length while maintaining performance quality. We further profile test-time scaling methods with varying degrees of parallelism to maximize accuracy under strict latency budgets. Through these analyses, EdgeReasoning maps the Pareto frontier of achievable accuracy-latency configurations, offering systematic guidance for optimal edge deployment of reasoning LLMs.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffPace: Diffusion-based Plug-and-play Augmented Channel Estimation in mmWave and Terahertz Ultra-Massive MIMO Systems</title>
<link>https://arxiv.org/abs/2511.01867</link>
<guid>https://arxiv.org/abs/2511.01867</guid>
<content:encoded><![CDATA[
<div> DiffPace, channel estimation, mmWave, THz, UM-MIMO <br />
Summary: <br />
- DiffPace is a diffusion-based channel estimation method for mmWave and THz UM-MIMO systems. <br />
- It addresses challenges posed by high frequencies and large array apertures in accurate channel estimation. <br />
- DiffPace utilizes a diffusion model to capture the channel distribution based on the hybrid spherical and planar-wave model. <br />
- By leveraging prior knowledge and minimizing inference steps, DiffPace achieves competitive channel estimation performance. <br />
- Experimental results demonstrate -15 dB normalized mean square error at an SNR of 10 dB, with 90% fewer inference steps compared to state-of-the-art schemes, showcasing high accuracy and computational efficiency. <div>
arXiv:2511.01867v1 Announce Type: cross 
Abstract: Millimeter-wave (mmWave) and Terahertz (THz)-band communications hold great promise in meeting the growing data-rate demands of next-generation wireless networks, offering abundant bandwidth. To mitigate the severe path loss inherent to these high frequencies and reduce hardware costs, ultra-massive multiple-input multiple-output (UM-MIMO) systems with hybrid beamforming architectures can deliver substantial beamforming gains and enhanced spectral efficiency. However, accurate channel estimation (CE) in mmWave and THz UM-MIMO systems is challenging due to high channel dimensionality and compressed observations from a limited number of RF chains, while the hybrid near- and far-field radiation patterns, arising from large array apertures and high carrier frequencies, further complicate CE. Conventional compressive sensing based frameworks rely on predefined sparsifying matrices, which cannot faithfully capture the hybrid near-field and far-field channel structures, leading to degraded estimation performance. This paper introduces DiffPace, a diffusion-based plug-and-play method for channel estimation. DiffPace uses a diffusion model (DM) to capture the channel distribution based on the hybrid spherical and planar-wave (HPSM) model. By applying the plug-and-play approach, it leverages the DM as prior knowledge, improving CE accuracy. Moreover, DM performs inference by solving an ordinary differential equation, minimizing the number of required inference steps compared with stochastic sampling method. Experimental results show that DiffPace achieves competitive CE performance, attaining -15 dB normalized mean square error (NMSE) at a signal-to-noise ratio (SNR) of 10 dB, with 90\% fewer inference steps compared to state-of-the-art schemes, simultaneously providing high estimation precision and enhanced computational efficiency.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CytoNet: A Foundation Model for the Human Cerebral Cortex</title>
<link>https://arxiv.org/abs/2511.01870</link>
<guid>https://arxiv.org/abs/2511.01870</guid>
<content:encoded><![CDATA[
<div> Self-supervised learning, cerebral cortex, CytoNet, microscopic image patches, brain analysis <br />
Summary: <br />
The study introduces CytoNet, a model that utilizes self-supervised learning and high-resolution microscopic image patches to encode detailed cellular architecture of the cerebral cortex. This foundation model does not rely on manual labeling and generates biologically relevant feature representations. CytoNet demonstrates excellent performance in various tasks like cortical area classification, cortical layer segmentation, cell morphology estimation, and unsupervised brain region mapping. These features encode both general cortical architecture aspects and unique brain-specific traits. CytoNet serves as a consistent framework for analyzing the relationship between cortical microarchitecture and other brain features, facilitating comprehensive neuroscientific investigations. <br /> <div>
arXiv:2511.01870v1 Announce Type: cross 
Abstract: To study how the human brain works, we need to explore the organization of the cerebral cortex and its detailed cellular architecture. We introduce CytoNet, a foundation model that encodes high-resolution microscopic image patches of the cerebral cortex into highly expressive feature representations, enabling comprehensive brain analyses. CytoNet employs self-supervised learning using spatial proximity as a powerful training signal, without requiring manual labelling. The resulting features are anatomically sound and biologically relevant. They encode general aspects of cortical architecture and unique brain-specific traits. We demonstrate top-tier performance in tasks such as cortical area classification, cortical layer segmentation, cell morphology estimation, and unsupervised brain region mapping. As a foundation model, CytoNet offers a consistent framework for studying cortical microarchitecture, supporting analyses of its relationship with other structural and functional brain features, and paving the way for diverse neuroscientific investigations.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CudaForge: An Agent Framework with Hardware Feedback for CUDA Kernel Optimization</title>
<link>https://arxiv.org/abs/2511.01884</link>
<guid>https://arxiv.org/abs/2511.01884</guid>
<content:encoded><![CDATA[
<div> CUDA kernels, automatic code generation, LLMs, CudaForge, optimization<br />
<br />
Summary: <br />
Developing efficient CUDA kernels manually is time-consuming, leading to the need for automatic approaches utilizing Large Language Models (LLMs) like CudaForge. CudaForge is a training-free multi-agent workflow for CUDA kernel generation and optimization inspired by human expert workflows. It employs two LLM agents, a Coder, and a Judge, to iteratively generate, correct, and optimize kernels while incorporating hardware feedback. The system achieves a high correctness rate of 97.6% and provides an average 1.68x speedup over baseline methods, surpassing state-of-the-art models. CudaForge demonstrates strong generalization across GPUs and base models, maintaining efficiency with significantly reduced costs compared to existing methods. Generating an optimized kernel with CudaForge takes around 26.5 minutes on an RTX6000 GPU and incurs a minimal API cost, showcasing its cost-effectiveness and performance in CUDA kernel optimization. <div>
arXiv:2511.01884v2 Announce Type: cross 
Abstract: Developing efficient CUDA kernels is increasingly critical for AI applications such as large-scale LLM training. However, manual kernel design is both costly and time-consuming, motivating automatic approaches that leverage LLMs for code generation. Existing methods for automatic kernel generation, however, often produce low-efficiency kernels, incur high computational overhead, and fail to generalize across settings. In this work, we propose CudaForge, a training-free multi-agent workflow for CUDA kernel generation and optimization. Our workflow is inspired by the iterative workflow of human experts, which contains steps such as developing initial kernels, testing correctness, analyzing hardware feedback, and iterative improvement. More specifically, CudaForge employs two LLM agents: a Coder and a Judge, that iteratively generate, correct, and optimize CUDA kernels, while integrating hardware feedback such as Nsight Compute (NCU) metrics. In extensive evaluations, we show that CudaForge, by leveraging base models like OpenAI-o3, achieves 97.6\% correctness of generated kernels and an average 1.68$\times$ speedup over PyTorch baselines, substantially surpassing state-of-the-art models including OpenAI-o3 and Kevin on KernelBench.Beyond accuracy and speed, CudaForge demonstrates strong generalization across GPUs (A100, RTX 6000, 4090, 3090) and base models (OpenAI-o3, GPT-5, gpt-oss-120B, Claude-Sonnet-4, QwQ-32B), while maintaining high efficiency. In particular, generating an optimized kernel takes about 26.5 minutes on one RTX6000 and incurs about \$ 0.3 API cost, which is significantly cheaper than existing agentic work that costs 6 H100 hours and \$ 5 API cost per kernel. Our results highlight that multi-agent, training-free workflows can enable cost-effective, generalizable, and high-performance CUDA kernel optimization. Code available at https://github.com/OptimAI-Lab/CudaForge
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Personality Generation of LLMs at Decoding-time</title>
<link>https://arxiv.org/abs/2511.01891</link>
<guid>https://arxiv.org/abs/2511.01891</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-Personality Generation, LLMs, decoding-time methods, implicit density ratios, Speculative Chunk-level based Rejection sampling

Summary: 
The paper introduces a novel framework called Multi-Personality Generation (MPG) for Language Model Generation (LLMs). This framework allows for the simultaneous embodiment of multiple personalization attributes without relying on costly retraining or external models. MPG leverages implicit density ratios in single-dimensional models to control multi-personality, making the process more flexible and robust. The approach involves sampling from a target strategy that aggregates these density ratios. To efficiently implement MPG, the authors design Speculative Chunk-level based Rejection sampling (SCR), which generates responses in chunks and validates them in parallel. This method reduces computational overhead while maintaining high-quality generation results. Experimental results on personality and role-playing tasks demonstrate the effectiveness of MPG, with improvements of up to 16%-18%. The code and data for MPG are available on GitHub. 

<br /><br />Summary: <div>
arXiv:2511.01891v1 Announce Type: cross 
Abstract: Multi-personality generation for LLMs, enabling simultaneous embodiment of multiple personalization attributes, is a fundamental challenge. Existing retraining-based approaches are costly and poorly scalable, while decoding-time methods often rely on external models or heuristics, limiting flexibility and robustness. In this paper, we propose a novel Multi-Personality Generation (MPG) framework under the decoding-time combination paradigm. It flexibly controls multi-personality without relying on scarce multi-dimensional models or extra training, leveraging implicit density ratios in single-dimensional models as a "free lunch" to reformulate the task as sampling from a target strategy aggregating these ratios. To implement MPG efficiently, we design Speculative Chunk-level based Rejection sampling (SCR), which generates responses in chunks and parallelly validates them via estimated thresholds within a sliding window. This significantly reduces computational overhead while maintaining high-quality generation. Experiments on MBTI personality and Role-Playing demonstrate the effectiveness of MPG, showing improvements up to 16%-18%. Code and data are available at https://github.com/Libra117/MPG .
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LGCC: Enhancing Flow Matching Based Text-Guided Image Editing with Local Gaussian Coupling and Context Consistency</title>
<link>https://arxiv.org/abs/2511.01894</link>
<guid>https://arxiv.org/abs/2511.01894</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, image editing, detail preservation, content consistency, efficiency

Summary:
LGCC is a novel framework that addresses limitations in current flow matching-based Multimodal Large Language Models (MLLMs) like BAGEL. It introduces Local Gaussian Noise Coupling (LGNC) to preserve spatial details and Content Consistency Loss (CCL) to ensure semantic alignment between edit instructions and image modifications. By integrating LGCC with pre-trained models through curriculum learning, it significantly improves detail and overall scores on benchmark tests. LGCC also offers a 3x -- 5x speedup for lightweight editing and 2x speedup for universal editing, with reduced inference time compared to existing models. This framework enhances editing quality by preserving detail, maintaining contextual integrity, and increasing inference speed, making it a cost-efficient solution for image editing tasks.<br /><br />Summary: <div>
arXiv:2511.01894v1 Announce Type: cross 
Abstract: Recent advancements have demonstrated the great potential of flow matching-based Multimodal Large Language Models (MLLMs) in image editing. However, state-of-the-art works like BAGEL face limitations, including detail degradation, content inconsistency, and inefficiency due to their reliance on random noise initialization. To address these issues, we propose LGCC, a novel framework with two key components: Local Gaussian Noise Coupling (LGNC) and Content Consistency Loss (CCL). LGNC preserves spatial details by modeling target image embeddings and their locally perturbed counterparts as coupled pairs, while CCL ensures semantic alignment between edit instructions and image modifications, preventing unintended content removal. By integrating LGCC with the BAGEL pre-trained model via curriculum learning, we significantly reduce inference steps, improving local detail scores on I2EBench by 1.60% and overall scores by 0.53%. LGCC achieves 3x -- 5x speedup for lightweight editing and 2x for universal editing, requiring only 40% -- 50% of the inference time of BAGEL or Flux. These results demonstrate LGCC's ability to preserve detail, maintain contextual integrity, and enhance inference speed, offering a cost-efficient solution without compromising editing quality.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Before the Clinic: Transparent and Operable Design Principles for Healthcare AI</title>
<link>https://arxiv.org/abs/2511.01902</link>
<guid>https://arxiv.org/abs/2511.01902</guid>
<content:encoded><![CDATA[
<div> Keyword: Artificial Intelligence, Clinical Practice, Explainable AI, Transparent Design, Operable Design
Summary:
Transparent Design and Operable Design are proposed as foundational design principles to bridge the gap between explainable AI theory, clinician expectations, and governance requirements in healthcare AI systems. Transparent Design focuses on interpretability and understandability artifacts for case-level reasoning and system traceability, while Operable Design focuses on calibration, uncertainty, and robustness for reliable system behavior in real-world conditions. These principles align with established XAI frameworks, address clinician needs, and meet emerging governance requirements. The pre-clinical playbook outlined in the article provides actionable guidance for development teams to prepare AI systems prior to clinical evaluation, facilitating smoother translation of AI into clinical practice. By defining what can be built and verified before deployment, the playbook aims to reduce friction in clinical AI translation while being cautious about validated explainability in deployment. 
<br /><br />Summary: <div>
arXiv:2511.01902v1 Announce Type: cross 
Abstract: The translation of artificial intelligence (AI) systems into clinical practice requires bridging fundamental gaps between explainable AI theory, clinician expectations, and governance requirements. While conceptual frameworks define what constitutes explainable AI (XAI) and qualitative studies identify clinician needs, little practical guidance exists for development teams to prepare AI systems prior to clinical evaluation. We propose two foundational design principles, Transparent Design and Operable Design, that operationalize pre-clinical technical requirements for healthcare AI. Transparent Design encompasses interpretability and understandability artifacts that enable case-level reasoning and system traceability. Operable Design encompasses calibration, uncertainty, and robustness to ensure reliable, predictable system behavior under real-world conditions. We ground these principles in established XAI frameworks, map them to documented clinician needs, and demonstrate their alignment with emerging governance requirements. This pre-clinical playbook provides actionable guidance for development teams, accelerates the path to clinical evaluation, and establishes a shared vocabulary bridging AI researchers, healthcare practitioners, and regulatory stakeholders. By explicitly scoping what can be built and verified before clinical deployment, we aim to reduce friction in clinical AI translation while remaining cautious about what constitutes validated, deployed explainability.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thinking Like a Student: AI-Supported Reflective Planning in a Theory-Intensive Computer Science Course</title>
<link>https://arxiv.org/abs/2511.01906</link>
<guid>https://arxiv.org/abs/2511.01906</guid>
<content:encoded><![CDATA[
<div> Keywords: COVID-19, reinforcement roles, large language model, pedagogical design, computer science courses

Summary:
The article discusses the redesign of reinforcement sessions in a challenging undergraduate course on formal methods and computational models following the COVID-19 pandemic. Many universities implemented supplementary reinforcement roles to support students in demanding courses, but these roles were often poorly defined. Using a large language model (LLM) as a reflective planning tool, the study aimed to enhance pedagogical design in theoretically dense domains. The LLM simulated the perspective of a second-year student, identifying conceptual bottlenecks and gaps in intuition before classroom delivery. The redesigned sessions included targeted review, collaborative examples, independent student work, and guided walkthroughs. Positive student feedback indicated increased confidence, reduced anxiety, and improved clarity on abstract topics like the pumping lemma and formal language expressive power comparisons. The findings suggest that reflective use of LLMs can enhance pedagogical design in computer science courses. 

<br /><br />Summary: <div>
arXiv:2511.01906v1 Announce Type: cross 
Abstract: In the aftermath of COVID-19, many universities implemented supplementary "reinforcement" roles to support students in demanding courses. Although the name for such roles may differ between institutions, the underlying idea of providing structured supplementary support is common. However, these roles were often poorly defined, lacking structured materials, pedagogical oversight, and integration with the core teaching team. This paper reports on the redesign of reinforcement sessions in a challenging undergraduate course on formal methods and computational models, using a large language model (LLM) as a reflective planning tool. The LLM was prompted to simulate the perspective of a second-year student, enabling the identification of conceptual bottlenecks, gaps in intuition, and likely reasoning breakdowns before classroom delivery. These insights informed a structured, repeatable session format combining targeted review, collaborative examples, independent student work, and guided walkthroughs. Conducted over a single semester, the intervention received positive student feedback, indicating increased confidence, reduced anxiety, and improved clarity, particularly in abstract topics such as the pumping lemma and formal language expressive power comparisons. The findings suggest that reflective, instructor-facing use of LLMs can enhance pedagogical design in theoretically dense domains and may be adaptable to other cognitively demanding computer science courses.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Between Myths and Metaphors: Rethinking LLMs for SRH in Conservative Contexts</title>
<link>https://arxiv.org/abs/2511.01907</link>
<guid>https://arxiv.org/abs/2511.01907</guid>
<content:encoded><![CDATA[
<div> Keywords: low-resource countries, maternal deaths, large language models, sexual and reproductive health communication, Pakistan<br />
Summary:<br />
- Low-resource countries, including Pakistan, face high rates of preventable maternal deaths, highlighting the need for innovative solutions such as large language models (LLMs) in healthcare communication.<br />
- Conservative contexts present challenges for sexual and reproductive health (SRH) communication due to indirect language use, complicating the effectiveness of LLM-based interventions.<br />
- A two-stage study in Pakistan reveals issues such as semantic drift, myths, and polysemy in clinical interactions when using LLMs for SRH communication.<br />
- The study offers empirical themes in SRH communication, a framework for categorizing indirect communication, evaluation of LLM performance, and design recommendations for culturally-situated SRH communication.<br /> <div>
arXiv:2511.01907v1 Announce Type: cross 
Abstract: Low-resource countries represent over 90% of maternal deaths, with Pakistan among the top four countries contributing nearly half in 2023. Since these deaths are mostly preventable, large language models (LLMs) can help address this crisis by automating health communication and risk assessment. However, sexual and reproductive health (SRH) communication in conservative contexts often relies on indirect language that obscures meaning, complicating LLM-based interventions. We conduct a two-stage study in Pakistan: (1) analyzing data from clinical observations, interviews, and focus groups with clinicians and patients, and (2) evaluating the interpretive capabilities of five popular LLMs on this data. Our analysis identifies two axes of communication (referential domain and expression approach) and shows LLMs struggle with semantic drift, myths, and polysemy in clinical interactions. We contribute: (1) empirical themes in SRH communication, (2) a categorization framework for indirect communication, (3) evaluation of LLM performance, and (4) design recommendations for culturally-situated SRH communication.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variational Geometry-aware Neural Network based Method for Solving High-dimensional Diffeomorphic Mapping Problems</title>
<link>https://arxiv.org/abs/2511.01911</link>
<guid>https://arxiv.org/abs/2511.01911</guid>
<content:encoded><![CDATA[
<div> Variational principles, quasi-conformal theory, mesh-free learning framework, high-dimensional mapping, bijective mapping

Summary:
The article introduces a mesh-free learning framework for high-dimensional diffeomorphic mapping, addressing the challenges posed by the curse of dimensionality. By combining variational principles with quasi-conformal theory, the framework ensures accurate and bijective mappings while controlling conformality and volume distortions. It is compatible with gradient-based optimization and neural network architectures, providing flexibility and scalability to higher-dimensional problems. Numerical experiments on synthetic and real-world medical image data demonstrate the accuracy, robustness, and effectiveness of the method in complex registration scenarios. <div>
arXiv:2511.01911v1 Announce Type: cross 
Abstract: Traditional methods for high-dimensional diffeomorphic mapping often struggle with the curse of dimensionality. We propose a mesh-free learning framework designed for $n$-dimensional mapping problems, seamlessly combining variational principles with quasi-conformal theory. Our approach ensures accurate, bijective mappings by regulating conformality distortion and volume distortion, enabling robust control over deformation quality. The framework is inherently compatible with gradient-based optimization and neural network architectures, making it highly flexible and scalable to higher-dimensional settings. Numerical experiments on both synthetic and real-world medical image data validate the accuracy, robustness, and effectiveness of the proposed method in complex registration scenarios.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EvoMem: Improving Multi-Agent Planning with Dual-Evolving Memory</title>
<link>https://arxiv.org/abs/2511.01912</link>
<guid>https://arxiv.org/abs/2511.01912</guid>
<content:encoded><![CDATA[
<div> Keywords: planning, artificial intelligence, memory, multi-agent frameworks, EvoMem <br />
Summary: 
The article discusses the importance of human-like memory in multi-agent frameworks for natural language planning. It introduces EvoMem, a framework with a dual-evolving memory mechanism consisting of three agents and two memory modules. The Constraint Memory (CMem) stores task-specific rules and constraints that remain fixed within a query, while the Query-feedback Memory (QMem) accumulates feedback across iterations for solution refinement within a query session. EvoMem consistently outperformed in trip planning, meeting planning, and calendar scheduling tasks, demonstrating the effectiveness of incorporating memory in multi-agent planning. This study sheds light on how agents coordinate through memory, showcasing the significance of memory in enhancing the planning process. <br /><br />Summary: <div>
arXiv:2511.01912v1 Announce Type: cross 
Abstract: Planning has been a cornerstone of artificial intelligence for solving complex problems, and recent progress in LLM-based multi-agent frameworks have begun to extend this capability. However, the role of human-like memory within these frameworks remains largely unexplored. Understanding how agents coordinate through memory is critical for natural language planning, where iterative reasoning, constraint tracking, and error correction drive the success. Inspired by working memory model in cognitive psychology, we present EvoMem, a multi-agent framework built on a dual-evolving memory mechanism. The framework consists of three agents (Constraint Extractor, Verifier, and Actor) and two memory modules: Constraint Memory (CMem), which evolves across queries by storing task-specific rules and constraints while remains fixed within a query, and Query-feedback Memory (QMem), which evolves within a query by accumulating feedback across iterations for solution refinement. Both memory modules are reset at the end of each query session. Evaluations on trip planning, meeting planning, and calendar scheduling show consistent performance improvements, highlighting the effectiveness of EvoMem. This success underscores the importance of memory in enhancing multi-agent planning.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>iFlyBot-VLA Technical Report</title>
<link>https://arxiv.org/abs/2511.01914</link>
<guid>https://arxiv.org/abs/2511.01914</guid>
<content:encoded><![CDATA[
<div> latent action model, Vision-Language-Action model, dual-level action representation, mixed training strategy, 3D perceptual and reasoning capabilities <br />
Summary:<br /> 
- iFlyBot-VLA is a large-scale Vision-Language-Action model trained on human and robotic manipulation videos.
- The model incorporates a latent action model and a dual-level action representation framework, supervised jointly during training.
- A mixed training strategy combining robot trajectory data with QA datasets enhances the model's 3D perceptual and reasoning capabilities.
- The Vision-Language Model predicts latent actions and structured discrete action tokens, aligning language, vision, and action representations.
- Experimental results on the LIBERO Franka benchmark showcase the model's effectiveness across diverse manipulation tasks, with competitive success rates. <div>
arXiv:2511.01914v1 Announce Type: cross 
Abstract: We introduce iFlyBot-VLA, a large-scale Vision-Language-Action (VLA) model trained under a novel framework. The main contributions are listed as follows: (1) a latent action model thoroughly trained on large-scale human and robotic manipulation videos; (2) a dual-level action representation framework that jointly supervises both the Vision-Language Model (VLM) and the action expert during training; (3) a mixed training strategy that combines robot trajectory data with general QA and spatial QA datasets, effectively enhancing the 3D perceptual and reasoning capabilities of the VLM backbone. Specifically, the VLM is trained to predict two complementary forms of actions: latent actions, derived from our latent action model pretrained on cross-embodiment manipulation data, which capture implicit high-level intentions; and structured discrete action tokens, obtained through frequency-domain transformations of continuous control signals, which encode explicit low-level dynamics. This dual supervision aligns the representation spaces of language, vision, and action, enabling the VLM to directly contribute to action generation. Experimental results on the LIBERO Franka benchmark demonstrate the superiority of our frame-work, while real-world evaluations further show that iFlyBot-VLA achieves competitive success rates across diverse and challenging manipulation tasks. Furthermore, we plan to open-source a portion of our self-constructed dataset to support future research in the community
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fibbinary-Based Compression and Quantization for Efficient Neural Radio Receivers</title>
<link>https://arxiv.org/abs/2511.01921</link>
<guid>https://arxiv.org/abs/2511.01921</guid>
<content:encoded><![CDATA[
<div> Keywords: Neural receivers, quantization, compression, Fibonacci Code word Quantization, Incremental Network Quantization<br />
Summary:<br />
Neural receivers have shown superior performance but face challenges due to high network complexity and computational costs. This paper explores optimization strategies such as uniform and non-uniform quantization, including the Fibonacci Code word Quantization. An Incremental Network Quantization approach is proposed to mitigate losses from quantization techniques. Additionally, novel lossless compression algorithms are introduced to reduce memory size by compressing sequences of Fibonacci quantized parameters with redundancy. The quantization technique results in a 45% saving in multiplier power and 44% reduction in area. When combined with compression, a 63.4% decrease in memory footprint is achieved while maintaining superior performance compared to conventional receivers.<br /><br />Summary: <div>
arXiv:2511.01921v1 Announce Type: cross 
Abstract: Neural receivers have shown outstanding performance compared to the conventional ones but this comes with a high network complexity leading to a heavy computational cost. This poses significant challenges in their deployment on hardware-constrained devices. To address the issue, this paper explores two optimization strategies: quantization and compression. We introduce both uniform and non-uniform quantization such as the Fibonacci Code word Quantization (FCQ). A novel fine-grained approach to the Incremental Network Quantization (INQ) strategy is then proposed to compensate for the losses introduced by the above mentioned quantization techniques. Additionally, we introduce two novel lossless compression algorithms that effectively reduce the memory size by compressing sequences of Fibonacci quantized parameters characterized by a huge redundancy. The quantization technique provides a saving of 45\% and 44\% in the multiplier's power and area, respectively, and its combination with the compression determines a 63.4\% reduction in memory footprint, while still providing higher performances than a conventional receiver.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Green's Functions</title>
<link>https://arxiv.org/abs/2511.01924</link>
<guid>https://arxiv.org/abs/2511.01924</guid>
<content:encoded><![CDATA[
<div> Neural Green's Function, linear partial differential equations, solution operator, eigendecompositions, generalization, irregular geometries<br />
Summary:<br />
- Neural Green's Function is introduced as a neural solution operator for linear PDEs that mimic the behavior of Green's functions, allowing for superior generalization across various geometries and functions. 
- By extracting per-point features from a volumetric point cloud, it predicts a decomposition of the solution operator to evaluate solutions efficiently through numerical integration.
- Unlike existing learning-based operators, Neural Green's Function can generalize to unseen functions due to its agnostic nature during training.
- In the thermal analysis of mechanical part geometries, it outperforms state-of-the-art operators with a significant average error reduction across shape categories.
- The framework is also up to 350 times faster than traditional numerical solvers that require meshing, making it a promising approach for solving PDEs efficiently and effectively.<br /> 
Summary: <div>
arXiv:2511.01924v1 Announce Type: cross 
Abstract: We introduce Neural Green's Function, a neural solution operator for linear partial differential equations (PDEs) whose differential operators admit eigendecompositions. Inspired by Green's functions, the solution operators of linear PDEs that depend exclusively on the domain geometry, we design Neural Green's Function to imitate their behavior, achieving superior generalization across diverse irregular geometries and source and boundary functions. Specifically, Neural Green's Function extracts per-point features from a volumetric point cloud representing the problem domain and uses them to predict a decomposition of the solution operator, which is subsequently applied to evaluate solutions via numerical integration. Unlike recent learning-based solution operators, which often struggle to generalize to unseen source or boundary functions, our framework is, by design, agnostic to the specific functions used during training, enabling robust and efficient generalization. In the steady-state thermal analysis of mechanical part geometries from the MCB dataset, Neural Green's Function outperforms state-of-the-art neural operators, achieving an average error reduction of 13.9\% across five shape categories, while being up to 350 times faster than a numerical solver that requires computationally expensive meshing.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepContour: A Hybrid Deep Learning Framework for Accelerating Generalized Eigenvalue Problem Solving via Efficient Contour Design</title>
<link>https://arxiv.org/abs/2511.01927</link>
<guid>https://arxiv.org/abs/2511.01927</guid>
<content:encoded><![CDATA[
<div> contour integral methods, generalized eigenvalue problems, deep learning, spectral predictor, kernel density estimation <br />
Summary: <br />
The article proposes DeepContour, a hybrid framework for solving large-scale Generalized Eigenvalue Problems (GEPs) efficiently. It combines a deep learning-based spectral predictor with Kernel Density Estimation to design integration contours. First, a Fourier Neural Operator (FNO) predicts the spectral distribution of the GEP, following which KDE determines proper integration contours based on the predicted spectrum. These optimized contours guide the contour integral solver to find eigenvalues effectively. The method is demonstrated to accelerate GEP solving across various datasets, achieving up to a 5.63x speedup. By leveraging the predictive capabilities of deep learning with classical solvers' numerical rigor, this approach offers an efficient and robust solution for challenging GEPs involving high-dimensional matrices. <div>
arXiv:2511.01927v1 Announce Type: cross 
Abstract: Solving large-scale Generalized Eigenvalue Problems (GEPs) is a fundamental yet computationally prohibitive task in science and engineering. As a promising direction, contour integral (CI) methods, such as the CIRR algorithm, offer an efficient and parallelizable framework. However, their performance is critically dependent on the selection of integration contours -- improper selection without reliable prior knowledge of eigenvalue distribution can incur significant computational overhead and compromise numerical accuracy. To address this challenge, we propose DeepContour, a novel hybrid framework that integrates a deep learning-based spectral predictor with Kernel Density Estimation for principled contour design. Specifically, DeepContour first employs a Fourier Neural Operator (FNO) to rapidly predict the spectral distribution of a given GEP. Subsequently, Kernel Density Estimation (KDE) is applied to the predicted spectrum to automatically and systematically determine proper integration contours. Finally, these optimized contours guide the CI solver to efficiently find the desired eigenvalues. We demonstrate the effectiveness of our method on diverse challenging scientific problems. In our main experiments, DeepContour accelerates GEP solving across multiple datasets, achieving up to a 5.63$\times$ speedup. By combining the predictive power of deep learning with the numerical rigor of classical solvers, this work pioneers an efficient and robust paradigm for tackling difficult generalized eigenvalue involving matrices of high dimension.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unified Model for Human Mobility Generation in Natural Disasters</title>
<link>https://arxiv.org/abs/2511.01928</link>
<guid>https://arxiv.org/abs/2511.01928</guid>
<content:encoded><![CDATA[
<div> Keywords: human mobility, disaster scenarios, generalization model, physics-informed, meta-learning

Summary: 
The article discusses the development of a universal model for generating human mobility patterns in various disaster scenarios. Existing models lack generalizability due to their reliance on limited data from specific disasters or cities. To address this limitation, the authors propose UniDisMob, a unified model that leverages physics-informed prompts and physics-guided alignment to capture common patterns in mobility changes post-disaster. Additionally, a meta-learning framework is introduced to extract universal patterns across multiple cities while capturing city-specific features. Experimental results across different cities and disaster types show that UniDisMob outperforms existing models by an average of 13%. The model's ability to generalize to new disaster scenarios and cities makes it a valuable tool for resource allocation, emergency response, and rescue coordination in dynamic and unpredictable environments. 

Summary: <div>
arXiv:2511.01928v1 Announce Type: cross 
Abstract: Human mobility generation in disaster scenarios plays a vital role in resource allocation, emergency response, and rescue coordination. During disasters such as wildfires and hurricanes, human mobility patterns often deviate from their normal states, which makes the task more challenging. However, existing works usually rely on limited data from a single city or specific disaster, significantly restricting the model's generalization capability in new scenarios. In fact, disasters are highly sudden and unpredictable, and any city may encounter new types of disasters without prior experience. Therefore, we aim to develop a one-for-all model for mobility generation that can generalize to new disaster scenarios. However, building a universal framework faces two key challenges: 1) the diversity of disaster types and 2) the heterogeneity among different cities. In this work, we propose a unified model for human mobility generation in natural disasters (named UniDisMob). To enable cross-disaster generalization, we design physics-informed prompt and physics-guided alignment that leverage the underlying common patterns in mobility changes after different disasters to guide the generation process. To achieve cross-city generalization, we introduce a meta-learning framework that extracts universal patterns across multiple cities through shared parameters and captures city-specific features via private parameters. Extensive experiments across multiple cities and disaster scenarios demonstrate that our method significantly outperforms state-of-the-art baselines, achieving an average performance improvement exceeding 13%.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Population Distribution Aware Human Trajectory Generation with Diffusion Model</title>
<link>https://arxiv.org/abs/2511.01929</link>
<guid>https://arxiv.org/abs/2511.01929</guid>
<content:encoded><![CDATA[
arXiv:2511.01929v1 Announce Type: cross 
Abstract: Human trajectory data is crucial in urban planning, traffic engineering, and public health. However, directly using real-world trajectory data often faces challenges such as privacy concerns, data acquisition costs, and data quality. A practical solution to these challenges is trajectory generation, a method developed to simulate human mobility behaviors. Existing trajectory generation methods mainly focus on capturing individual movement patterns but often overlook the influence of population distribution on trajectory generation. In reality, dynamic population distribution reflects changes in population density across different regions, significantly impacting individual mobility behavior. Thus, we propose a novel trajectory generation framework based on a diffusion model, which integrates the dynamic population distribution constraints to guide high-fidelity generation outcomes. Specifically, we construct a spatial graph to enhance the spatial correlation of trajectories. Then, we design a dynamic population distribution aware denoising network to capture the spatiotemporal dependencies of human mobility behavior as well as the impact of population distribution in the denoising process. Extensive experiments show that the trajectories generated by our model can resemble real-world trajectories in terms of some critical statistical metrics, outperforming state-of-the-art algorithms by over 54%.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deciphering Personalization: Towards Fine-Grained Explainability in Natural Language for Personalized Image Generation Models</title>
<link>https://arxiv.org/abs/2511.01932</link>
<guid>https://arxiv.org/abs/2511.01932</guid>
<content:encoded><![CDATA[
arXiv:2511.01932v1 Announce Type: cross 
Abstract: Image generation models are usually personalized in practical uses in order to better meet the individual users' heterogeneous needs, but most personalized models lack explainability about how they are being personalized. Such explainability can be provided via visual features in generated images, but is difficult for human users to understand. Explainability in natural language is a better choice, but the existing approaches to explainability in natural language are limited to be coarse-grained. They are unable to precisely identify the multiple aspects of personalization, as well as the varying levels of personalization in each aspect. To address such limitation, in this paper we present a new technique, namely \textbf{FineXL}, towards \textbf{Fine}-grained e\textbf{X}plainability in natural \textbf{L}anguage for personalized image generation models. FineXL can provide natural language descriptions about each distinct aspect of personalization, along with quantitative scores indicating the level of each aspect of personalization. Experiment results show that FineXL can improve the accuracy of explainability by 56\%, when different personalization scenarios are applied to multiple types of image generation models.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tool Zero: Training Tool-Augmented LLMs via Pure RL from Scratch</title>
<link>https://arxiv.org/abs/2511.01934</link>
<guid>https://arxiv.org/abs/2511.01934</guid>
<content:encoded><![CDATA[
arXiv:2511.01934v1 Announce Type: cross 
Abstract: Training tool-augmented LLMs has emerged as a promising approach to enhancing language models' capabilities for complex tasks. The current supervised fine-tuning paradigm relies on constructing extensive domain-specific datasets to train models. However, this approach often struggles to generalize effectively to unfamiliar or intricate tool-use scenarios. Recently, reinforcement learning (RL) paradigm can endow LLMs with superior reasoning and generalization abilities. In this work, we address a key question: Can the pure RL be used to effectively elicit a model's intrinsic reasoning capabilities and enhance the tool-agnostic generalization? We propose a dynamic generalization-guided reward design for rule-based RL, which progressively shifts rewards from exploratory to exploitative tool-use patterns. Based on this design, we introduce the Tool-Zero series models. These models are trained to enable LLMs to autonomously utilize general tools by directly scaling up RL from Zero models (i.e., base models without post-training). Experimental results demonstrate that our models achieve over 7% performance improvement compared to both SFT and RL-with-SFT models under the same experimental settings. These gains are consistently replicated across cross-dataset and intra-dataset evaluations, validating the effectiveness and robustness of our methods.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Q-Sat AI: Machine Learning-Based Decision Support for Data Saturation in Qualitative Studies</title>
<link>https://arxiv.org/abs/2511.01935</link>
<guid>https://arxiv.org/abs/2511.01935</guid>
<content:encoded><![CDATA[
arXiv:2511.01935v1 Announce Type: cross 
Abstract: The determination of sample size in qualitative research has traditionally relied on the subjective and often ambiguous principle of data saturation, which can lead to inconsistencies and threaten methodological rigor. This study introduces a new, systematic model based on machine learning (ML) to make this process more objective. Utilizing a dataset derived from five fundamental qualitative research approaches - namely, Case Study, Grounded Theory, Phenomenology, Narrative Research, and Ethnographic Research - we developed an ensemble learning model. Ten critical parameters, including research scope, information power, and researcher competence, were evaluated using an ordinal scale and used as input features. After thorough preprocessing and outlier removal, multiple ML algorithms were trained and compared. The K-Nearest Neighbors (KNN), Gradient Boosting (GB), Random Forest (RF), XGBoost, and Decision Tree (DT) algorithms showed the highest explanatory power (Test R2 ~ 0.85), effectively modeling the complex, non-linear relationships involved in qualitative sampling decisions. Feature importance analysis confirmed the vital roles of research design type and information power, providing quantitative validation of key theoretical assumptions in qualitative methodology. The study concludes by proposing a conceptual framework for a web-based computational application designed to serve as a decision support system for qualitative researchers, journal reviewers, and thesis advisors. This model represents a significant step toward standardizing sample size justification, enhancing transparency, and strengthening the epistemological foundation of qualitative inquiry through evidence-based, systematic decision-making.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shorter but not Worse: Frugal Reasoning via Easy Samples as Length Regularizers in Math RLVR</title>
<link>https://arxiv.org/abs/2511.01937</link>
<guid>https://arxiv.org/abs/2511.01937</guid>
<content:encoded><![CDATA[
arXiv:2511.01937v1 Announce Type: cross 
Abstract: Large language models (LLMs) trained for step-by-step reasoning often become excessively verbose, raising inference cost. Standard Reinforcement Learning with Verifiable Rewards (RLVR) pipelines filter out ``easy'' problems for training efficiency, leaving the model to train primarily on harder problems that require longer reasoning chains. This skews the output length distribution upward, resulting in a \textbf{model that conflates ``thinking longer'' with ``thinking better''}. In this work, we show that retaining and modestly up-weighting moderately easy problems acts as an implicit length regularizer. Exposing the model to solvable short-chain tasks constrains its output distribution and prevents runaway verbosity. The result is \textbf{\emph{emergent brevity for free}}: the model learns to solve harder problems without inflating the output length, \textbf{ despite the absence of any explicit length penalization}. RLVR experiments using this approach on \textit{Qwen3-4B-Thinking-2507} (with a 16k token limit) achieve baseline pass@1 AIME25 accuracy while generating solutions that are, on average, nearly twice as short. The code is available at \href{https://github.com/MBZUAI-Paris/Frugal-AI}{GitHub}, with datasets and models on \href{https://huggingface.co/collections/MBZUAI-Paris/k2-think-mini-68dcfa8b114686a4bd3dc2bc}{Hugging Face}.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Geometry of Grokking: Norm Minimization on the Zero-Loss Manifold</title>
<link>https://arxiv.org/abs/2511.01938</link>
<guid>https://arxiv.org/abs/2511.01938</guid>
<content:encoded><![CDATA[
arXiv:2511.01938v1 Announce Type: cross 
Abstract: Grokking is a puzzling phenomenon in neural networks where full generalization occurs only after a substantial delay following the complete memorization of the training data. Previous research has linked this delayed generalization to representation learning driven by weight decay, but the precise underlying dynamics remain elusive. In this paper, we argue that post-memorization learning can be understood through the lens of constrained optimization: gradient descent effectively minimizes the weight norm on the zero-loss manifold. We formally prove this in the limit of infinitesimally small learning rates and weight decay coefficients. To further dissect this regime, we introduce an approximation that decouples the learning dynamics of a subset of parameters from the rest of the network. Applying this framework, we derive a closed-form expression for the post-memorization dynamics of the first layer in a two-layer network. Experiments confirm that simulating the training process using our predicted gradients reproduces both the delayed generalization and representation learning characteristic of grokking.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting Vulnerabilities from Issue Reports for Internet-of-Things</title>
<link>https://arxiv.org/abs/2511.01941</link>
<guid>https://arxiv.org/abs/2511.01941</guid>
<content:encoded><![CDATA[
arXiv:2511.01941v1 Announce Type: cross 
Abstract: Timely identification of issue reports reflecting software vulnerabilities is crucial, particularly for Internet-of-Things (IoT) where analysis is slower than non-IoT systems. While Machine Learning (ML) and Large Language Models (LLMs) detect vulnerability-indicating issues in non-IoT systems, their IoT use remains unexplored. We are the first to tackle this problem by proposing two approaches: (1) combining ML and LLMs with Natural Language Processing (NLP) techniques to detect vulnerability-indicating issues of 21 Eclipse IoT projects and (2) fine-tuning a pre-trained BERT Masked Language Model (MLM) on 11,000 GitHub issues for classifying \vul. Our best performance belongs to a Support Vector Machine (SVM) trained on BERT NLP features, achieving an Area Under the receiver operator characteristic Curve (AUC) of 0.65. The fine-tuned BERT achieves 0.26 accuracy, emphasizing the importance of exposing all data during training. Our contributions set the stage for accurately detecting IoT vulnerabilities from issue reports, similar to non-IoT systems.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>COFAP: A Universal Framework for COFs Adsorption Prediction through Designed Multi-Modal Extraction and Cross-Modal Synergy</title>
<link>https://arxiv.org/abs/2511.01946</link>
<guid>https://arxiv.org/abs/2511.01946</guid>
<content:encoded><![CDATA[
arXiv:2511.01946v1 Announce Type: cross 
Abstract: Covalent organic frameworks (COFs) are promising adsorbents for gas adsorption and separation, while identifying the optimal structures among their vast design space requires efficient high-throughput screening. Conventional machine-learning predictors rely heavily on specific gas-related features. However, these features are time-consuming and limit scalability, leading to inefficiency and labor-intensive processes. Herein, a universal COFs adsorption prediction framework (COFAP) is proposed, which can extract multi-modal structural and chemical features through deep learning, and fuse these complementary features via cross-modal attention mechanism. Without Henry coefficients or adsorption heat, COFAP sets a new SOTA by outperforming previous approaches on hypoCOFs dataset. Based on COFAP, we also found that high-performing COFs for separation concentrate within a narrow range of pore size and surface area. A weight-adjustable prioritization scheme is also developed to enable flexible, application-specific ranking of candidate COFs for researchers. Superior efficiency and accuracy render COFAP directly deployable in crystalline porous materials.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable Heart Disease Prediction via a Weighted Ensemble Model: A Large-Scale Study with SHAP and Surrogate Decision Trees</title>
<link>https://arxiv.org/abs/2511.01947</link>
<guid>https://arxiv.org/abs/2511.01947</guid>
<content:encoded><![CDATA[
arXiv:2511.01947v1 Announce Type: cross 
Abstract: Cardiovascular disease (CVD) remains a critical global health concern, demanding reliable and interpretable predictive models for early risk assessment. This study presents a large-scale analysis using the Heart Disease Health Indicators Dataset, developing a strategically weighted ensemble model that combines tree-based methods (LightGBM, XGBoost) with a Convolutional Neural Network (CNN) to predict CVD risk. The model was trained on a preprocessed dataset of 229,781 patients where the inherent class imbalance was managed through strategic weighting and feature engineering enhanced the original 22 features to 25. The final ensemble achieves a statistically significant improvement over the best individual model, with a Test AUC of 0.8371 (p=0.003) and is particularly suited for screening with a high recall of 80.0%. To provide transparency and clinical interpretability, surrogate decision trees and SHapley Additive exPlanations (SHAP) are used. The proposed model delivers a combination of robust predictive performance and clinical transparency by blending diverse learning architectures and incorporating explainability through SHAP and surrogate decision trees, making it a strong candidate for real-world deployment in public health screening.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Black-Box Membership Inference Attack for LVLMs via Prior Knowledge-Calibrated Memory Probing</title>
<link>https://arxiv.org/abs/2511.01952</link>
<guid>https://arxiv.org/abs/2511.01952</guid>
<content:encoded><![CDATA[
arXiv:2511.01952v1 Announce Type: cross 
Abstract: Large vision-language models (LVLMs) derive their capabilities from extensive training on vast corpora of visual and textual data. Empowered by large-scale parameters, these models often exhibit strong memorization of their training data, rendering them susceptible to membership inference attacks (MIAs). Existing MIA methods for LVLMs typically operate under white- or gray-box assumptions, by extracting likelihood-based features for the suspected data samples based on the target LVLMs. However, mainstream LVLMs generally only expose generated outputs while concealing internal computational features during inference, limiting the applicability of these methods. In this work, we propose the first black-box MIA framework for LVLMs, based on a prior knowledge-calibrated memory probing mechanism. The core idea is to assess the model memorization of the private semantic information embedded within the suspected image data, which is unlikely to be inferred from general world knowledge alone. We conducted extensive experiments across four LVLMs and three datasets. Empirical results demonstrate that our method effectively identifies training data of LVLMs in a purely black-box setting and even achieves performance comparable to gray-box and white-box methods. Further analysis reveals the robustness of our method against potential adversarial manipulations, and the effectiveness of the methodology designs. Our code and data are available at https://github.com/spmede/KCMP.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vibe Learning: Education in the age of AI</title>
<link>https://arxiv.org/abs/2511.01956</link>
<guid>https://arxiv.org/abs/2511.01956</guid>
<content:encoded><![CDATA[
arXiv:2511.01956v1 Announce Type: cross 
Abstract: The debate over whether "thinking machines" could replace human intellectual labor has existed in both public and expert discussions since the mid-twentieth century, when the concept and terminology of Artificial Intelligence (AI) first emerged. For decades, this idea remained largely theoretical. However, with the recent advent of Generative AI - particularly Large Language Models (LLMs) - and the widespread adoption of tools such as ChatGPT, the issue has become a practical reality. Many fields that rely on human intellectual effort are now being reshaped by AI tools that both expand human capabilities and challenge the necessity of certain forms of work once deemed uniquely human but now easily automated. Education, somewhat unexpectedly, faces a pivotal responsibility: to devise long-term strategies for cultivating human skills that will remain relevant in an era of pervasive AI in the intellectual domain. In this context, we identify the limitations of current AI systems - especially those rooted in LLM technology - argue that the fundamental causes of these weaknesses cannot be resolved through existing methods, and propose directions within the constructivist paradigm for transforming education to preserve the long-term advantages of human intelligence over AI tools.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TRACE: Textual Reasoning for Affordance Coordinate Extraction</title>
<link>https://arxiv.org/abs/2511.01999</link>
<guid>https://arxiv.org/abs/2511.01999</guid>
<content:encoded><![CDATA[
arXiv:2511.01999v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) struggle to translate high-level instructions into the precise spatial affordances required for robotic manipulation. While visual Chain-of-Thought (CoT) methods exist, they are often computationally intensive. In this work, we introduce TRACE (Textual Reasoning for Affordance Coordinate Extraction), a novel methodology that integrates a textual Chain of Reasoning (CoR) into the affordance prediction process. We use this methodology to create the TRACE dataset, a large-scale collection created via an autonomous pipeline that pairs instructions with explicit textual rationales. By fine-tuning a VLM on this data, our model learns to externalize its spatial reasoning before acting. Our experiments show that our TRACE-tuned model achieves state-of-the-art performance, reaching 48.1% accuracy on the primary Where2Place (W2P) benchmark (a 9.6% relative improvement) and 55.0% on the more challenging W2P(h) subset. Crucially, an ablation study demonstrates that performance scales directly with the amount of reasoning data used, confirming the CoR's effectiveness. Furthermore, analysis of the model's attention maps reveals an interpretable reasoning process where focus shifts dynamically across reasoning steps. This work shows that training VLMs to generate a textual CoR is an effective and robust strategy for enhancing the precision, reliability, and interpretability of VLM-based robot control. Our dataset and code are available at https://github.com/jink-ucla/TRACE
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InteracSPARQL: An Interactive System for SPARQL Query Refinement Using Natural Language Explanations</title>
<link>https://arxiv.org/abs/2511.02002</link>
<guid>https://arxiv.org/abs/2511.02002</guid>
<content:encoded><![CDATA[
arXiv:2511.02002v1 Announce Type: cross 
Abstract: In recent years, querying semantic web data using SPARQL has remained challenging, especially for non-expert users, due to the language's complex syntax and the prerequisite of understanding intricate data structures. To address these challenges, we propose InteracSPARQL, an interactive SPARQL query generation and refinement system that leverages natural language explanations (NLEs) to enhance user comprehension and facilitate iterative query refinement. InteracSPARQL integrates LLMs with a rule-based approach to first produce structured explanations directly from SPARQL abstract syntax trees (ASTs), followed by LLM-based linguistic refinements. Users can interactively refine queries through direct feedback or LLM-driven self-refinement, enabling the correction of ambiguous or incorrect query components in real time. We evaluate InteracSPARQL on standard benchmarks, demonstrating significant improvements in query accuracy, explanation clarity, and overall user satisfaction compared to baseline approaches. Our experiments further highlight the effectiveness of combining rule-based methods with LLM-driven refinements to create more accessible and robust SPARQL interfaces.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shared Parameter Subspaces and Cross-Task Linearity in Emergently Misaligned Behavior</title>
<link>https://arxiv.org/abs/2511.02022</link>
<guid>https://arxiv.org/abs/2511.02022</guid>
<content:encoded><![CDATA[
arXiv:2511.02022v1 Announce Type: cross 
Abstract: Recent work has discovered that large language models can develop broadly misaligned behaviors after being fine-tuned on narrowly harmful datasets, a phenomenon known as emergent misalignment (EM). However, the fundamental mechanisms enabling such harmful generalization across disparate domains remain poorly understood. In this work, we adopt a geometric perspective to study EM and demonstrate that it exhibits a fundamental cross-task linear structure in how harmful behavior is encoded across different datasets. Specifically, we find a strong convergence in EM parameters across tasks, with the fine-tuned weight updates showing relatively high cosine similarities, as well as shared lower-dimensional subspaces as measured by their principal angles and projection overlaps. Furthermore, we also show functional equivalence via linear mode connectivity, wherein interpolated models across narrow misalignment tasks maintain coherent, broadly misaligned behavior. Our results indicate that EM arises from different narrow tasks discovering the same set of shared parameter directions, suggesting that harmful behaviors may be organized into specific, predictable regions of the weight landscape. By revealing this fundamental connection between parametric geometry and behavioral outcomes, we hope our work catalyzes further research on parameter space interpretability and weight-based interventions.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Path-Coordinated Continual Learning with Neural Tangent Kernel-Justified Plasticity: A Theoretical Framework with Near State-of-the-Art Performance</title>
<link>https://arxiv.org/abs/2511.02025</link>
<guid>https://arxiv.org/abs/2511.02025</guid>
<content:encoded><![CDATA[
arXiv:2511.02025v1 Announce Type: cross 
Abstract: Catastrophic forgetting is one of the fundamental issues of continual learning because neural networks forget the tasks learned previously when trained on new tasks. The proposed framework is a new path-coordinated framework of continual learning that unites the Neural Tangent Kernel (NTK) theory of principled plasticity bounds, statistical validation by Wilson confidence intervals, and evaluation of path quality by the use of multiple metrics. Experimental evaluation shows an average accuracy of 66.7% at the cost of 23.4% catastrophic forgetting on Split-CIFAR10, a huge improvement over the baseline and competitive performance achieved, which is very close to state-of-the-art results. Further, it is found out that NTK condition numbers are predictive indicators of learning capacity limits, showing the existence of a critical threshold at condition number $>10^{11}$. It is interesting to note that the proposed strategy shows a tendency of lowering forgetting as the sequence of tasks progresses (27% to 18%), which is a system stabilization. The framework validates 80% of discovered paths with a rigorous statistical guarantee and maintains 90-97% retention on intermediate tasks. The core capacity limits of the continual learning environment are determined in the analysis, and actionable insights to enhance the adaptive regularization are offered.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RobustFSM: Submodular Maximization in Federated Setting with Malicious Clients</title>
<link>https://arxiv.org/abs/2511.02029</link>
<guid>https://arxiv.org/abs/2511.02029</guid>
<content:encoded><![CDATA[
arXiv:2511.02029v1 Announce Type: cross 
Abstract: Submodular maximization is an optimization problem benefiting many machine learning applications, where we seek a small subset best representing an extremely large dataset. We focus on the federated setting where the data are locally owned by decentralized clients who have their own definitions for the quality of representability. This setting requires repetitive aggregation of local information computed by the clients. While the main motivation is to respect the privacy and autonomy of the clients, the federated setting is vulnerable to client misbehaviors: malicious clients might share fake information. An analogy is backdoor attack in conventional federated learning, but our challenge differs freshly due to the unique characteristics of submodular maximization. We propose RobustFSM, a federated submodular maximization solution that is robust to various practical client attacks. Its performance is substantiated with an empirical evaluation study using real-world datasets. Numerical results show that the solution quality of RobustFSM substantially exceeds that of the conventional federated algorithm when attacks are severe. The degree of this improvement depends on the dataset and attack scenarios, which can be as high as 200%
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum-Enhanced Generative Models for Rare Event Prediction</title>
<link>https://arxiv.org/abs/2511.02042</link>
<guid>https://arxiv.org/abs/2511.02042</guid>
<content:encoded><![CDATA[
arXiv:2511.02042v1 Announce Type: cross 
Abstract: Rare events such as financial crashes, climate extremes, and biological anomalies are notoriously difficult to model due to their scarcity and heavy-tailed distributions. Classical deep generative models often struggle to capture these rare occurrences, either collapsing low-probability modes or producing poorly calibrated uncertainty estimates. In this work, we propose the Quantum-Enhanced Generative Model (QEGM), a hybrid classical-quantum framework that integrates deep latent-variable models with variational quantum circuits. The framework introduces two key innovations: (1) a hybrid loss function that jointly optimizes reconstruction fidelity and tail-aware likelihood, and (2) quantum randomness-driven noise injection to enhance sample diversity and mitigate mode collapse. Training proceeds via a hybrid loop where classical parameters are updated through backpropagation while quantum parameters are optimized using parameter-shift gradients. We evaluate QEGM on synthetic Gaussian mixtures and real-world datasets spanning finance, climate, and protein structure. Results demonstrate that QEGM reduces tail KL divergence by up to 50 percent compared to state-of-the-art baselines (GAN, VAE, Diffusion), while improving rare-event recall and coverage calibration. These findings highlight the potential of QEGM as a principled approach for rare-event prediction, offering robustness beyond what is achievable with purely classical methods.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Regularization Through Reasoning: Systematic Improvements in Language Model Classification via Explanation-Enhanced Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.02044</link>
<guid>https://arxiv.org/abs/2511.02044</guid>
<content:encoded><![CDATA[
arXiv:2511.02044v1 Announce Type: cross 
Abstract: Fine-tuning LLMs for classification typically maps inputs directly to labels. We ask whether attaching brief explanations to each label during fine-tuning yields better models. We evaluate conversational response quality along three axes: naturalness, comprehensiveness, and on-topic adherence, each rated on 5-point scales. Using ensemble-generated data from multiple LLMs, we fine-tune a 7B-parameter model and test across six diverse conversational datasets. Across 18 dataset, task settings, label-plus-explanation training outperforms label-only baselines.
  A central and unexpected result concerns random tokens. We replace human-written explanations with text that is syntactically incoherent yet vocabulary-aligned with the originals (e.g., shuffled or bag-of-words variants). Despite lacking semantics, these pseudo-explanations still improve accuracy over label-only training and often narrow much of the gap to true explanations. The effect persists across datasets and training seeds, indicating that gains arise less from meaning than from structure: the extra token budget encourages richer intermediate computation and acts as a regularizer that reduces over-confident shortcuts.
  Internal analyses support this view: explanation-augmented models exhibit higher activation entropy in intermediate layers alongside sharper predictive mass at the output layer, consistent with increased deliberation before decision. Overall, explanation-augmented fine-tuning, whether with genuine rationales or carefully constructed random token sequences, improves accuracy and reliability for LLM classification while clarifying how token-level scaffolding shapes computation during inference.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text-VQA Aug: Pipelined Harnessing of Large Multimodal Models for Automated Synthesis</title>
<link>https://arxiv.org/abs/2511.02046</link>
<guid>https://arxiv.org/abs/2511.02046</guid>
<content:encoded><![CDATA[
arXiv:2511.02046v1 Announce Type: cross 
Abstract: Creation of large-scale databases for Visual Question Answering tasks pertaining to the text data in a scene (text-VQA) involves skilful human annotation, which is tedious and challenging. With the advent of foundation models that handle vision and language modalities, and with the maturity of OCR systems, it is the need of the hour to establish an end-to-end pipeline that can synthesize Question-Answer (QA) pairs based on scene-text from a given image. We propose a pipeline for automated synthesis for text-VQA dataset that can produce faithful QA pairs, and which scales up with the availability of scene text data. Our proposed method harnesses the capabilities of multiple models and algorithms involving OCR detection and recognition (text spotting), region of interest (ROI) detection, caption generation, and question generation. These components are streamlined into a cohesive pipeline to automate the synthesis and validation of QA pairs. To the best of our knowledge, this is the first pipeline proposed to automatically synthesize and validate a large-scale text-VQA dataset comprising around 72K QA pairs based on around 44K images.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vortex: Hosting ML Inference and Knowledge Retrieval Services With Tight Latency and Throughput Requirements</title>
<link>https://arxiv.org/abs/2511.02062</link>
<guid>https://arxiv.org/abs/2511.02062</guid>
<content:encoded><![CDATA[
arXiv:2511.02062v1 Announce Type: cross 
Abstract: There is growing interest in deploying ML inference and knowledge retrieval as services that could support both interactive queries by end users and more demanding request flows that arise from AIs integrated into a end-user applications and deployed as agents. Our central premise is that these latter cases will bring service level latency objectives (SLOs). Existing ML serving platforms use batching to optimize for high throughput, exposing them to unpredictable tail latencies. Vortex enables an SLO-first approach. For identical tasks, Vortex's pipelines achieve significantly lower and more stable latencies than TorchServe and Ray Serve over a wide range of workloads, often enabling a given SLO target at more than twice the request rate. When RDMA is available, the Vortex advantage is even more significant.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Watermarking Discrete Diffusion Language Models</title>
<link>https://arxiv.org/abs/2511.02083</link>
<guid>https://arxiv.org/abs/2511.02083</guid>
<content:encoded><![CDATA[
arXiv:2511.02083v1 Announce Type: cross 
Abstract: Watermarking has emerged as a promising technique to track AI-generated content and differentiate it from authentic human creations. While prior work extensively studies watermarking for autoregressive large language models (LLMs) and image diffusion models, none address discrete diffusion language models, which are becoming popular due to their high inference throughput. In this paper, we introduce the first watermarking method for discrete diffusion models by applying the distribution-preserving Gumbel-max trick at every diffusion step and seeding the randomness with the sequence index to enable reliable detection. We experimentally demonstrate that our scheme is reliably detectable on state-of-the-art diffusion language models and analytically prove that it is distortion-free with an exponentially decaying probability of false detection in the token sequence length.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Energy Loss Functions for Physical Systems</title>
<link>https://arxiv.org/abs/2511.02087</link>
<guid>https://arxiv.org/abs/2511.02087</guid>
<content:encoded><![CDATA[
arXiv:2511.02087v1 Announce Type: cross 
Abstract: Effectively leveraging prior knowledge of a system's physics is crucial for applications of machine learning to scientific domains. Previous approaches mostly focused on incorporating physical insights at the architectural level. In this paper, we propose a framework to leverage physical information directly into the loss function for prediction and generative modeling tasks on systems like molecules and spins. We derive energy loss functions assuming that each data sample is in thermal equilibrium with respect to an approximate energy landscape. By using the reverse KL divergence with a Boltzmann distribution around the data, we obtain the loss as an energy difference between the data and the model predictions. This perspective also recasts traditional objectives like MSE as energy-based, but with a physically meaningless energy. In contrast, our formulation yields physically grounded loss functions with gradients that better align with valid configurations, while being architecture-agnostic and computationally efficient. The energy loss functions also inherently respect physical symmetries. We demonstrate our approach on molecular generation and spin ground-state prediction and report significant improvements over baselines.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Natural Building Blocks for Structured World Models: Theory, Evidence, and Scaling</title>
<link>https://arxiv.org/abs/2511.02091</link>
<guid>https://arxiv.org/abs/2511.02091</guid>
<content:encoded><![CDATA[
arXiv:2511.02091v1 Announce Type: cross 
Abstract: The field of world modeling is fragmented, with researchers developing bespoke architectures that rarely build upon each other. We propose a framework that specifies the natural building blocks for structured world models based on the fundamental stochastic processes that any world model must capture: discrete processes (logic, symbols) and continuous processes (physics, dynamics); the world model is then defined by the hierarchical composition of these building blocks. We examine Hidden Markov Models (HMMs) and switching linear dynamical systems (sLDS) as natural building blocks for discrete and continuous modeling--which become partially-observable Markov decision processes (POMDPs) and controlled sLDS when augmented with actions. This modular approach supports both passive modeling (generation, forecasting) and active control (planning, decision-making) within the same architecture. We avoid the combinatorial explosion of traditional structure learning by largely fixing the causal architecture and searching over only four depth parameters. We review practical expressiveness through multimodal generative modeling (passive) and planning from pixels (active), with performance competitive to neural approaches while maintaining interpretability. The core outstanding challenge is scalable joint structure-parameter learning; current methods finesse this by cleverly growing structure and parameters incrementally, but are limited in their scalability. If solved, these natural building blocks could provide foundational infrastructure for world modeling, analogous to how standardized layers enabled progress in deep learning.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty Guided Online Ensemble for Non-stationary Data Streams in Fusion Science</title>
<link>https://arxiv.org/abs/2511.02092</link>
<guid>https://arxiv.org/abs/2511.02092</guid>
<content:encoded><![CDATA[
arXiv:2511.02092v1 Announce Type: cross 
Abstract: Machine Learning (ML) is poised to play a pivotal role in the development and operation of next-generation fusion devices. Fusion data shows non-stationary behavior with distribution drifts, resulted by both experimental evolution and machine wear-and-tear. ML models assume stationary distribution and fail to maintain performance when encountered with such non-stationary data streams. Online learning techniques have been leveraged in other domains, however it has been largely unexplored for fusion applications. In this paper, we present an application of online learning to continuously adapt to drifting data stream for prediction of Toroidal Field (TF) coils deflection at the DIII-D fusion facility. The results demonstrate that online learning is critical to maintain ML model performance and reduces error by 80% compared to a static model. Moreover, traditional online learning can suffer from short-term performance degradation as ground truth is not available before making the predictions. As such, we propose an uncertainty guided online ensemble method to further improve the performance. The Deep Gaussian Process Approximation (DGPA) technique is leveraged for calibrated uncertainty estimation and the uncertainty values are then used to guide a meta-algorithm that produces predictions based on an ensemble of learners trained on different horizon of historical data. The DGPA also provides uncertainty estimation along with the predictions for decision makers. The online ensemble and the proposed uncertainty guided online ensemble reduces predictions error by about 6%, and 10% respectively over standard single model based online learning.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometric Data Valuation via Leverage Scores</title>
<link>https://arxiv.org/abs/2511.02100</link>
<guid>https://arxiv.org/abs/2511.02100</guid>
<content:encoded><![CDATA[
arXiv:2511.02100v1 Announce Type: cross 
Abstract: Shapley data valuation provides a principled, axiomatic framework for assigning importance to individual datapoints, and has gained traction in dataset curation, pruning, and pricing. However, it is a combinatorial measure that requires evaluating marginal utility across all subsets of the data, making it computationally infeasible at scale. We propose a geometric alternative based on statistical leverage scores, which quantify each datapoint's structural influence in the representation space by measuring how much it extends the span of the dataset and contributes to the effective dimensionality of the training problem. We show that our scores satisfy the dummy, efficiency, and symmetry axioms of Shapley valuation and that extending them to \emph{ridge leverage scores} yields strictly positive marginal gains that connect naturally to classical A- and D-optimal design criteria. We further show that training on a leverage-sampled subset produces a model whose parameters and predictive risk are within $O(\varepsilon)$ of the full-data optimum, thereby providing a rigorous link between data valuation and downstream decision quality. Finally, we conduct an active learning experiment in which we empirically demonstrate that ridge-leverage sampling outperforms standard baselines without requiring access gradients or backward passes.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Metamorphic Testing of Large Language Models for Natural Language Processing</title>
<link>https://arxiv.org/abs/2511.02108</link>
<guid>https://arxiv.org/abs/2511.02108</guid>
<content:encoded><![CDATA[
arXiv:2511.02108v1 Announce Type: cross 
Abstract: Using large language models (LLMs) to perform natural language processing (NLP) tasks has become increasingly pervasive in recent times. The versatile nature of LLMs makes them applicable to a wide range of such tasks. While the performance of recent LLMs is generally outstanding, several studies have shown that they can often produce incorrect results. Automatically identifying these faulty behaviors is extremely useful for improving the effectiveness of LLMs. One obstacle to this is the limited availability of labeled datasets, which necessitates an oracle to determine the correctness of LLM behaviors. Metamorphic testing (MT) is a popular testing approach that alleviates this oracle problem. At the core of MT are metamorphic relations (MRs), which define relationships between the outputs of related inputs. MT can expose faulty behaviors without the need for explicit oracles (e.g., labeled datasets). This paper presents the most comprehensive study of MT for LLMs to date. We conducted a literature review and collected 191 MRs for NLP tasks. We implemented a representative subset (36 MRs) to conduct a series of experiments with three popular LLMs, running approximately 560,000 metamorphic tests. The results shed light on the capabilities and opportunities of MT for LLMs, as well as its limitations.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Matrix Sensing with Kernel Optimal Loss: Robustness and Optimization Landscape</title>
<link>https://arxiv.org/abs/2511.02122</link>
<guid>https://arxiv.org/abs/2511.02122</guid>
<content:encoded><![CDATA[
arXiv:2511.02122v1 Announce Type: cross 
Abstract: In this paper we study how the choice of loss functions of non-convex optimization problems affects their robustness and optimization landscape, through the study of noisy matrix sensing. In traditional regression tasks, mean squared error (MSE) loss is a common choice, but it can be unreliable for non-Gaussian or heavy-tailed noise. To address this issue, we adopt a robust loss based on nonparametric regression, which uses a kernel-based estimate of the residual density and maximizes the estimated log-likelihood. This robust formulation coincides with the MSE loss under Gaussian errors but remains stable under more general settings. We further examine how this robust loss reshapes the optimization landscape by analyzing the upper-bound of restricted isometry property (RIP) constants for spurious local minima to disappear. Through theoretical and empirical analysis, we show that this new loss excels at handling large noise and remains robust across diverse noise distributions. This work offers initial insights into enhancing the robustness of machine learning tasks through simply changing the loss, guided by an intuitive and broadly applicable analytical framework.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Disentangling Causal Substructures for Interpretable and Generalizable Drug Synergy Prediction</title>
<link>https://arxiv.org/abs/2511.02146</link>
<guid>https://arxiv.org/abs/2511.02146</guid>
<content:encoded><![CDATA[
arXiv:2511.02146v1 Announce Type: cross 
Abstract: Drug synergy prediction is a critical task in the development of effective combination therapies for complex diseases, including cancer. Although existing methods have shown promising results, they often operate as black-box predictors that rely predominantly on statistical correlations between drug characteristics and results. To address this limitation, we propose CausalDDS, a novel framework that disentangles drug molecules into causal and spurious substructures, utilizing the causal substructure representations for predicting drug synergy. By focusing on causal sub-structures, CausalDDS effectively mitigates the impact of redundant features introduced by spurious substructures, enhancing the accuracy and interpretability of the model. In addition, CausalDDS employs a conditional intervention mechanism, where interventions are conditioned on paired molecular structures, and introduces a novel optimization objective guided by the principles of sufficiency and independence. Extensive experiments demonstrate that our method outperforms baseline models, particularly in cold start and out-of-distribution settings. Besides, CausalDDS effectively identifies key substructures underlying drug synergy, providing clear insights into how drug combinations work at the molecular level. These results underscore the potential of CausalDDS as a practical tool for predicting drug synergy and facilitating drug discovery.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Near Optimal Convergence to Coarse Correlated Equilibrium in General-Sum Markov Games</title>
<link>https://arxiv.org/abs/2511.02157</link>
<guid>https://arxiv.org/abs/2511.02157</guid>
<content:encoded><![CDATA[
arXiv:2511.02157v1 Announce Type: cross 
Abstract: No-regret learning dynamics play a central role in game theory, enabling decentralized convergence to equilibrium for concepts such as Coarse Correlated Equilibrium (CCE) or Correlated Equilibrium (CE). In this work, we improve the convergence rate to CCE in general-sum Markov games, reducing it from the previously best-known rate of $\mathcal{O}(\log^5 T / T)$ to a sharper $\mathcal{O}(\log T / T)$. This matches the best known convergence rate for CE in terms of $T$, number of iterations, while also improving the dependence on the action set size from polynomial to polylogarithmic-yielding exponential gains in high-dimensional settings. Our approach builds on recent advances in adaptive step-size techniques for no-regret algorithms in normal-form games, and extends them to the Markovian setting via a stage-wise scheme that adjusts learning rates based on real-time feedback. We frame policy updates as an instance of Optimistic Follow-the-Regularized-Leader (OFTRL), customized for value-iteration-based learning. The resulting self-play algorithm achieves, to our knowledge, the fastest known convergence rate to CCE in Markov games.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text to Robotic Assembly of Multi Component Objects using 3D Generative AI and Vision Language Models</title>
<link>https://arxiv.org/abs/2511.02162</link>
<guid>https://arxiv.org/abs/2511.02162</guid>
<content:encoded><![CDATA[
arXiv:2511.02162v1 Announce Type: cross 
Abstract: Advances in 3D generative AI have enabled the creation of physical objects from text prompts, but challenges remain in creating objects involving multiple component types. We present a pipeline that integrates 3D generative AI with vision-language models (VLMs) to enable the robotic assembly of multi-component objects from natural language. Our method leverages VLMs for zero-shot, multi-modal reasoning about geometry and functionality to decompose AI-generated meshes into multi-component 3D models using predefined structural and panel components. We demonstrate that a VLM is capable of determining which mesh regions need panel components in addition to structural components, based on object functionality. Evaluation across test objects shows that users preferred the VLM-generated assignments 90.6% of the time, compared to 59.4% for rule-based and 2.5% for random assignment. Lastly, the system allows users to refine component assignments through conversational feedback, enabling greater human control and agency in making physical objects with generative AI and robotics.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ScenicProver: A Framework for Compositional Probabilistic Verification of Learning-Enabled Systems</title>
<link>https://arxiv.org/abs/2511.02164</link>
<guid>https://arxiv.org/abs/2511.02164</guid>
<content:encoded><![CDATA[
arXiv:2511.02164v1 Announce Type: cross 
Abstract: Full verification of learning-enabled cyber-physical systems (CPS) has long been intractable due to challenges including black-box components and complex real-world environments. Existing tools either provide formal guarantees for limited types of systems or test the system as a monolith, but no general framework exists for compositional analysis of learning-enabled CPS using varied verification techniques over complex real-world environments. This paper introduces ScenicProver, a verification framework that aims to fill this gap. Built upon the Scenic probabilistic programming language, the framework supports: (1) compositional system description with clear component interfaces, ranging from interpretable code to black boxes; (2) assume-guarantee contracts over those components using an extension of Linear Temporal Logic containing arbitrary Scenic expressions; (3) evidence generation through testing, formal proofs via Lean 4 integration, and importing external assumptions; (4) systematic combination of generated evidence using contract operators; and (5) automatic generation of assurance cases tracking the provenance of system-level guarantees. We demonstrate the framework's effectiveness through a case study on an autonomous vehicle's automatic emergency braking system with sensor fusion. By leveraging manufacturer guarantees for radar and laser sensors and focusing testing efforts on uncertain conditions, our approach enables stronger probabilistic guarantees than monolithic testing with the same computational budget.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tackling Incomplete Data in Air Quality Prediction: A Bayesian Deep Learning Framework for Uncertainty Quantification</title>
<link>https://arxiv.org/abs/2511.02175</link>
<guid>https://arxiv.org/abs/2511.02175</guid>
<content:encoded><![CDATA[
arXiv:2511.02175v1 Announce Type: cross 
Abstract: Accurate air quality forecasts are vital for public health alerts, exposure assessment, and emissions control. In practice, observational data are often missing in varying proportions and patterns due to collection and transmission issues. These incomplete spatiotemporal records impede reliable inference and risk assessment and can lead to overconfident extrapolation. To address these challenges, we propose an end to end framework, the channel gated learning unit based spatiotemporal bayesian neural field (CGLUBNF). It uses Fourier features with a graph attention encoder to capture multiscale spatial dependencies and seasonal temporal dynamics. A channel gated learning unit, equipped with learnable activations and gated residual connections, adaptively filters and amplifies informative features. Bayesian inference jointly optimizes predictive distributions and parameter uncertainty, producing point estimates and calibrated prediction intervals. We conduct a systematic evaluation on two real world datasets, covering four typical missing data patterns and comparing against five state of the art baselines. CGLUBNF achieves superior prediction accuracy and sharper confidence intervals. In addition, we further validate robustness across multiple prediction horizons and analysis the contribution of extraneous variables. This research lays a foundation for reliable deep learning based spatio-temporal forecasting with incomplete observations in emerging sensing paradigms, such as real world vehicle borne mobile monitoring.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MM-UNet: Morph Mamba U-shaped Convolutional Networks for Retinal Vessel Segmentation</title>
<link>https://arxiv.org/abs/2511.02193</link>
<guid>https://arxiv.org/abs/2511.02193</guid>
<content:encoded><![CDATA[
arXiv:2511.02193v1 Announce Type: cross 
Abstract: Accurate detection of retinal vessels plays a critical role in reflecting a wide range of health status indicators in the clinical diagnosis of ocular diseases. Recently, advances in deep learning have led to a surge in retinal vessel segmentation methods, which have significantly contributed to the quantitative analysis of vascular morphology. However, retinal vasculature differs significantly from conventional segmentation targets in that it consists of extremely thin and branching structures, whose global morphology varies greatly across images. These characteristics continue to pose challenges to segmentation precision and robustness. To address these issues, we propose MM-UNet, a novel architecture tailored for efficient retinal vessel segmentation. The model incorporates Morph Mamba Convolution layers, which replace pointwise convolutions to enhance branching topological perception through morph, state-aware feature sampling. Additionally, Reverse Selective State Guidance modules integrate reverse guidance theory with state-space modeling to improve geometric boundary awareness and decoding efficiency. Extensive experiments conducted on two public retinal vessel segmentation datasets demonstrate the superior performance of the proposed method in segmentation accuracy. Compared to the existing approaches, MM-UNet achieves F1-score gains of 1.64 $\%$ on DRIVE and 1.25 $\%$ on STARE, demonstrating its effectiveness and advancement. The project code is public via https://github.com/liujiawen-jpg/MM-UNet.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BoolSkeleton: Boolean Network Skeletonization via Homogeneous Pattern Reduction</title>
<link>https://arxiv.org/abs/2511.02196</link>
<guid>https://arxiv.org/abs/2511.02196</guid>
<content:encoded><![CDATA[
arXiv:2511.02196v1 Announce Type: cross 
Abstract: Boolean equivalence allows Boolean networks with identical functionality to exhibit diverse graph structures. This gives more room for exploration in logic optimization, while also posing a challenge for tasks involving consistency between Boolean networks. To tackle this challenge, we introduce BoolSkeleton, a novel Boolean network skeletonization method that improves the consistency and reliability of design-specific evaluations. BoolSkeleton comprises two key steps: preprocessing and reduction. In preprocessing, the Boolean network is transformed into a defined Boolean dependency graph, where nodes are assigned the functionality-related status. Next, the homogeneous and heterogeneous patterns are defined for the node-level pattern reduction step. Heterogeneous patterns are preserved to maintain critical functionality-related dependencies, while homogeneous patterns can be reduced. Parameter K of the pattern further constrains the fanin size of these patterns, enabling fine-tuned control over the granularity of graph reduction. To validate BoolSkeleton's effectiveness, we conducted four analysis/downstream tasks around the Boolean network: compression analysis, classification, critical path analysis, and timing prediction, demonstrating its robustness across diverse scenarios. Furthermore, it improves above 55% in the average accuracy compared to the original Boolean network for the timing prediction task. These experiments underscore the potential of BoolSkeleton to enhance design consistency in logic synthesis.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Open the Oyster: Empirical Evaluation and Improvement of Code Reasoning Confidence in LLMs</title>
<link>https://arxiv.org/abs/2511.02197</link>
<guid>https://arxiv.org/abs/2511.02197</guid>
<content:encoded><![CDATA[
arXiv:2511.02197v1 Announce Type: cross 
Abstract: With the widespread application of large language models (LLMs) in the field of code intelligence, increasing attention has been paid to the reliability and controllability of their outputs in code reasoning tasks. Confidence estimation serves as an effective and convenient approach for evaluating these aspects. This paper proposes a confidence analysis and enhancement framework for LLMs tailored to code reasoning tasks. We conduct a comprehensive empirical study on the confidence reliability of mainstream LLMs across different tasks, and further evaluate the effectiveness of techniques such as prompt strategy optimisation and mathematical calibration (e.g., Platt Scaling) in improving confidence reliability. Our results show that DeepSeek-Reasoner achieves the best performance across various tasks, outperforming other models by up to $0.680$, $0.636$, and $13.652$ in terms of ECE, Brier Score, and Performance Score, respectively. The hybrid strategy combining the reassess prompt strategy and Platt Scaling achieves improvements of up to $0.541$, $0.628$, and $15.084$ over the original performance in the aforementioned three metrics. These results indicate that models with reasoning capabilities demonstrate superior confidence reliability, and that the hybrid strategy is the most effective in enhancing the confidence reliability of various models. Meanwhile, we elucidate the impact of different task complexities, model scales, and strategies on confidence performance, and highlight that the confidence of current LLMs in complex reasoning tasks still has considerable room for improvement. This study not only provides a research foundation and technical reference for the application of confidence in LLM-assisted software engineering, but also points the way for future optimisation and engineering deployment of confidence mechanisms.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Object-Centric 3D Gaussian Splatting for Strawberry Plant Reconstruction and Phenotyping</title>
<link>https://arxiv.org/abs/2511.02207</link>
<guid>https://arxiv.org/abs/2511.02207</guid>
<content:encoded><![CDATA[
arXiv:2511.02207v1 Announce Type: cross 
Abstract: Strawberries are among the most economically significant fruits in the United States, generating over $2 billion in annual farm-gate sales and accounting for approximately 13% of the total fruit production value. Plant phenotyping plays a vital role in selecting superior cultivars by characterizing plant traits such as morphology, canopy structure, and growth dynamics. However, traditional plant phenotyping methods are time-consuming, labor-intensive, and often destructive. Recently, neural rendering techniques, notably Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have emerged as powerful frameworks for high-fidelity 3D reconstruction. By capturing a sequence of multi-view images or videos around a target plant, these methods enable non-destructive reconstruction of complex plant architectures. Despite their promise, most current applications of 3DGS in agricultural domains reconstruct the entire scene, including background elements, which introduces noise, increases computational costs, and complicates downstream trait analysis. To address this limitation, we propose a novel object-centric 3D reconstruction framework incorporating a preprocessing pipeline that leverages the Segment Anything Model v2 (SAM-2) and alpha channel background masking to achieve clean strawberry plant reconstructions. This approach produces more accurate geometric representations while substantially reducing computational time. With a background-free reconstruction, our algorithm can automatically estimate important plant traits, such as plant height and canopy width, using DBSCAN clustering and Principal Component Analysis (PCA). Experimental results show that our method outperforms conventional pipelines in both accuracy and efficiency, offering a scalable and non-destructive solution for strawberry plant phenotyping.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Estimation of Segmental Longitudinal Strain in Transesophageal Echocardiography by Deep Learning</title>
<link>https://arxiv.org/abs/2511.02210</link>
<guid>https://arxiv.org/abs/2511.02210</guid>
<content:encoded><![CDATA[
arXiv:2511.02210v1 Announce Type: cross 
Abstract: Segmental longitudinal strain (SLS) of the left ventricle (LV) is an important prognostic indicator for evaluating regional LV dysfunction, in particular for diagnosing and managing myocardial ischemia. Current techniques for strain estimation require significant manual intervention and expertise, limiting their efficiency and making them too resource-intensive for monitoring purposes. This study introduces the first automated pipeline, autoStrain, for SLS estimation in transesophageal echocardiography (TEE) using deep learning (DL) methods for motion estimation. We present a comparative analysis of two DL approaches: TeeFlow, based on the RAFT optical flow model for dense frame-to-frame predictions, and TeeTracker, based on the CoTracker point trajectory model for sparse long-sequence predictions.
  As ground truth motion data from real echocardiographic sequences are hardly accessible, we took advantage of a unique simulation pipeline (SIMUS) to generate a highly realistic synthetic TEE (synTEE) dataset of 80 patients with ground truth myocardial motion to train and evaluate both models. Our evaluation shows that TeeTracker outperforms TeeFlow in accuracy, achieving a mean distance error in motion estimation of 0.65 mm on a synTEE test dataset.
  Clinical validation on 16 patients further demonstrated that SLS estimation with our autoStrain pipeline aligned with clinical references, achieving a mean difference (95\% limits of agreement) of 1.09% (-8.90% to 11.09%). Incorporation of simulated ischemia in the synTEE data improved the accuracy of the models in quantifying abnormal deformation. Our findings indicate that integrating AI-driven motion estimation with TEE can significantly enhance the precision and efficiency of cardiac function assessment in clinical settings.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Cooperative Transmission Design for Ultra-Reliable Low-Latency Communications via Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.02216</link>
<guid>https://arxiv.org/abs/2511.02216</guid>
<content:encoded><![CDATA[
arXiv:2511.02216v1 Announce Type: cross 
Abstract: Next-generation wireless communication systems must support ultra-reliable low-latency communication (URLLC) service for mission-critical applications. Meeting stringent URLLC requirements is challenging, especially for two-hop cooperative communication. In this paper, we develop an adaptive transmission design for a two-hop relaying communication system. Each hop transmission adaptively configures its transmission parameters separately, including numerology, mini-slot size, and modulation and coding scheme, for reliable packet transmission within a strict latency constraint. We formulate the hop-specific transceiver configuration as a Markov decision process (MDP) and propose a dual-agent reinforcement learning-based cooperative latency-aware transmission (DRL-CoLA) algorithm to learn latency-aware transmission policies in a distributed manner. Simulation results verify that the proposed algorithm achieves the near-optimal reliability while satisfying strict latency requirements.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Multi-Lane Intersection Performance in Mixed Autonomy Environments</title>
<link>https://arxiv.org/abs/2511.02217</link>
<guid>https://arxiv.org/abs/2511.02217</guid>
<content:encoded><![CDATA[
arXiv:2511.02217v1 Announce Type: cross 
Abstract: One of the main challenges in managing traffic at multilane intersections is ensuring smooth coordination between human-driven vehicles (HDVs) and connected autonomous vehicles (CAVs). This paper presents a novel traffic signal control framework that combines Graph Attention Networks (GAT) with Soft Actor-Critic (SAC) reinforcement learning to address this challenge. GATs are used to model the dynamic graph- structured nature of traffic flow to capture spatial and temporal dependencies between lanes and signal phases. The proposed SAC is a robust off-policy reinforcement learning algorithm that enables adaptive signal control through entropy-optimized decision making. This design allows the system to coordinate the signal timing and vehicle movement simultaneously with objectives focused on minimizing travel time, enhancing performance, ensuring safety, and improving fairness between HDVs and CAVs. The model is evaluated using a SUMO-based simulation of a four-way intersection and incorporating different traffic densities and CAV penetration rates. The experimental results demonstrate the effectiveness of the GAT-SAC approach by achieving a 24.1% reduction in average delay and up to 29.2% fewer traffic violations compared to traditional methods. Additionally, the fairness ratio between HDVs and CAVs improved to 1.59, indicating more equitable treatment across vehicle types. These findings suggest that the GAT-SAC framework holds significant promise for real-world deployment in mixed-autonomy traffic systems.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Collaborative Attention and Consistent-Guided Fusion of MRI and PET for Alzheimer's Disease Diagnosis</title>
<link>https://arxiv.org/abs/2511.02228</link>
<guid>https://arxiv.org/abs/2511.02228</guid>
<content:encoded><![CDATA[
arXiv:2511.02228v1 Announce Type: cross 
Abstract: Alzheimer's disease (AD) is the most prevalent form of dementia, and its early diagnosis is essential for slowing disease progression. Recent studies on multimodal neuroimaging fusion using MRI and PET have achieved promising results by integrating multi-scale complementary features. However, most existing approaches primarily emphasize cross-modal complementarity while overlooking the diagnostic importance of modality-specific features. In addition, the inherent distributional differences between modalities often lead to biased and noisy representations, degrading classification performance. To address these challenges, we propose a Collaborative Attention and Consistent-Guided Fusion framework for MRI and PET based AD diagnosis. The proposed model introduces a learnable parameter representation (LPR) block to compensate for missing modality information, followed by a shared encoder and modality-independent encoders to preserve both shared and specific representations. Furthermore, a consistency-guided mechanism is employed to explicitly align the latent distributions across modalities. Experimental results on the ADNI dataset demonstrate that our method achieves superior diagnostic performance compared with existing fusion strategies.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV Cache Time-to-Live</title>
<link>https://arxiv.org/abs/2511.02230</link>
<guid>https://arxiv.org/abs/2511.02230</guid>
<content:encoded><![CDATA[
arXiv:2511.02230v1 Announce Type: cross 
Abstract: Agentic LLM applications interleave LLM generation requests with tool calls. These tool calls break the continuity of the workflow by creating pauses between LLM requests, bringing many challenges for the serving system, especially under multi-turn scenarios. Each pause potentially causes KV cache eviction and extra waiting time before entering the continuous batch for the following LLM request. Since these pauses happen for each call, this problem becomes increasingly severe as turn number grow for agentic programs. Previous works either fail to incorporate information from the tool call, evicting KV cache that leads to repetitive prefill or loading, or ignore the continuity of a multi-turn program, creating waiting time between turns that increases per-request latency.
  We present Continuum, a serving system to optimize job completion time for multi-turn agent workloads by combining tool-aware KV cache timeout with program-level scheduling. By predicting tool call durations in agentic workflows, Continuum selectively pins the KV cache in GPU memory with a time-to-live value based on total turn number. When combined with program-level first-come-first-serve, Continuum prevents scheduling bubbles, preserves multi-turn continuity, and optimizes for throughput for complex agentic workflows. By modeling the variability of tool call and agent program continuity, Continuum outperforms state-of-the-art baselines. Our evaluation on real-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B models shows that Continuum significantly improves the average job completion times, and remains performant across different hardware setups and DRAM offloading schemes. Preview code is available at: https://github.com/Hanchenli/vllm-continuum
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LACY: A Vision-Language Model-based Language-Action Cycle for Self-Improving Robotic Manipulation</title>
<link>https://arxiv.org/abs/2511.02239</link>
<guid>https://arxiv.org/abs/2511.02239</guid>
<content:encoded><![CDATA[
arXiv:2511.02239v1 Announce Type: cross 
Abstract: Learning generalizable policies for robotic manipulation increasingly relies on large-scale models that map language instructions to actions (L2A). However, this one-way paradigm often produces policies that execute tasks without deeper contextual understanding, limiting their ability to generalize or explain their behavior. We argue that the complementary skill of mapping actions back to language (A2L) is essential for developing more holistic grounding. An agent capable of both acting and explaining its actions can form richer internal representations and unlock new paradigms for self-supervised learning. We introduce LACY (Language-Action Cycle), a unified framework that learns such bidirectional mappings within a single vision-language model. LACY is jointly trained on three synergistic tasks: generating parameterized actions from language (L2A), explaining observed actions in language (A2L), and verifying semantic consistency between two language descriptions (L2C). This enables a self-improving cycle that autonomously generates and filters new training data through an active augmentation strategy targeting low-confidence cases, thereby improving the model without additional human labels. Experiments on pick-and-place tasks in both simulation and the real world show that LACY improves task success rates by 56.46% on average and yields more robust language-action grounding for robotic manipulation. Project page: https://vla2026.github.io/LACY/
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structural Plasticity as Active Inference: A Biologically-Inspired Architecture for Homeostatic Control</title>
<link>https://arxiv.org/abs/2511.02241</link>
<guid>https://arxiv.org/abs/2511.02241</guid>
<content:encoded><![CDATA[
arXiv:2511.02241v1 Announce Type: cross 
Abstract: Traditional neural networks, while powerful, rely on biologically implausible learning mechanisms such as global backpropagation. This paper introduces the Structurally Adaptive Predictive Inference Network (SAPIN), a novel computational model inspired by the principles of active inference and the morphological plasticity observed in biological neural cultures. SAPIN operates on a 2D grid where processing units, or cells, learn by minimizing local prediction errors. The model features two primary, concurrent learning mechanisms: a local, Hebbian-like synaptic plasticity rule based on the temporal difference between a cell's actual activation and its learned expectation, and a structural plasticity mechanism where cells physically migrate across the grid to optimize their information-receptive fields. This dual approach allows the network to learn both how to process information (synaptic weights) and also where to position its computational resources (network topology). We validated the SAPIN model on the classic Cart Pole reinforcement learning benchmark. Our results demonstrate that the architecture can successfully solve the CartPole task, achieving robust performance. The network's intrinsic drive to minimize prediction error and maintain homeostasis was sufficient to discover a stable balancing policy. We also found that while continual learning led to instability, locking the network's parameters after achieving success resulted in a stable policy. When evaluated for 100 episodes post-locking (repeated over 100 successful agents), the locked networks maintained an average 82% success rate.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Demo: Statistically Significant Results On Biases and Errors of LLMs Do Not Guarantee Generalizable Results</title>
<link>https://arxiv.org/abs/2511.02246</link>
<guid>https://arxiv.org/abs/2511.02246</guid>
<content:encoded><![CDATA[
arXiv:2511.02246v1 Announce Type: cross 
Abstract: Recent research has shown that hallucinations, omissions, and biases are prevalent in everyday use-cases of LLMs. However, chatbots used in medical contexts must provide consistent advice in situations where non-medical factors are involved, such as when demographic information is present. In order to understand the conditions under which medical chatbots fail to perform as expected, we develop an infrastructure that 1) automatically generates queries to probe LLMs and 2) evaluates answers to these queries using multiple LLM-as-a-judge setups and prompts. For 1), our prompt creation pipeline samples the space of patient demographics, histories, disorders, and writing styles to create realistic questions that we subsequently use to prompt LLMs. In 2), our evaluation pipeline provides hallucination and omission detection using LLM-as-a-judge as well as agentic workflows, in addition to LLM-as-a-judge treatment category detectors. As a baseline study, we perform two case studies on inter-LLM agreement and the impact of varying the answering and evaluation LLMs. We find that LLM annotators exhibit low agreement scores (average Cohen's Kappa $\kappa=0.118$), and only specific (answering, evaluation) LLM pairs yield statistically significant differences across writing styles, genders, and races. We recommend that studies using LLM evaluation use multiple LLMs as evaluators in order to avoid arriving at statistically significant but non-generalizable results, particularly in the absence of ground-truth data. We also suggest publishing inter-LLM agreement metrics for transparency. Our code and dataset are available here: https://github.com/BBN-E/medic-neurips-2025-demo.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast Approximation Algorithm for Non-Monotone DR-submodular Maximization under Size Constraint</title>
<link>https://arxiv.org/abs/2511.02254</link>
<guid>https://arxiv.org/abs/2511.02254</guid>
<content:encoded><![CDATA[
arXiv:2511.02254v1 Announce Type: cross 
Abstract: This work studies the non-monotone DR-submodular Maximization over a ground set of $n$ subject to a size constraint $k$. We propose two approximation algorithms for solving this problem named FastDrSub and FastDrSub++. FastDrSub offers an approximation ratio of $0.044$ with query complexity of $O(n \log(k))$. The second one, FastDrSub++, improves upon it with a ratio of $1/4-\epsilon$ within query complexity of $(n \log k)$ for an input parameter $\epsilon >0$. Therefore, our proposed algorithms are the first constant-ratio approximation algorithms for the problem with the low complexity of $O(n \log(k))$.
  Additionally, both algorithms are experimentally evaluated and compared against existing state-of-the-art methods, demonstrating their effectiveness in solving the Revenue Maximization problem with DR-submodular objective function. The experimental results show that our proposed algorithms significantly outperform existing approaches in terms of both query complexity and solution quality.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LA-MARRVEL: A Knowledge-Grounded and Language-Aware LLM Reranker for AI-MARRVEL in Rare Disease Diagnosis</title>
<link>https://arxiv.org/abs/2511.02263</link>
<guid>https://arxiv.org/abs/2511.02263</guid>
<content:encoded><![CDATA[
arXiv:2511.02263v2 Announce Type: cross 
Abstract: Diagnosing rare diseases often requires connecting variant-bearing genes to evidence that is written as unstructured clinical prose, which the current established pipelines still leave for clinicians to reconcile manually. To this end, we introduce LA-MARRVEL, a knowledge-grounded and language-aware reranking layer that operates on top of AI-MARRVEL: it supplies expert-engineered context, queries a large language model multiple times, and aggregates the resulting partial rankings with a ranked voting method to produce a stable, explainable gene ranking. Evaluated on three real-world cohorts (BG, DDD, UDN), LA-MARRVEL consistently improves Recall@K over AI-MARRVEL and established phenotype-driven tools such as Exomiser and LIRICAL, with especially large gains on cases where the first-stage ranker placed the causal gene lower. Each ranked gene is accompanied by LLM-generated reasoning that integrates phenotypic, inheritance, and variant-level evidence, thereby making the output more interpretable and facilitating clinical review.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From data to design: Random forest regression model for predicting mechanical properties of alloy steel</title>
<link>https://arxiv.org/abs/2511.02290</link>
<guid>https://arxiv.org/abs/2511.02290</guid>
<content:encoded><![CDATA[
arXiv:2511.02290v1 Announce Type: cross 
Abstract: This study investigates the application of Random Forest Regression for predicting mechanical properties of alloy steel-Elongation, Tensile Strength, and Yield Strength-from material composition features including Iron (Fe), Chromium (Cr), Nickel (Ni), Manganese (Mn), Silicon (Si), Copper (Cu), Carbon (C), and deformation percentage during cold rolling. Utilizing a dataset comprising these features, we trained and evaluated the Random Forest model, achieving high predictive performance as evidenced by R2 scores and Mean Squared Errors (MSE). The results demonstrate the model's efficacy in providing accurate predictions, which is validated through various performance metrics including residual plots and learning curves. The findings underscore the potential of ensemble learning techniques in enhancing material property predictions, with implications for industrial applications in material science.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Quantum Kernel Learning for Anomaly Detection in Multivariate IoT Time-Series</title>
<link>https://arxiv.org/abs/2511.02301</link>
<guid>https://arxiv.org/abs/2511.02301</guid>
<content:encoded><![CDATA[
arXiv:2511.02301v1 Announce Type: cross 
Abstract: The rapid growth of industrial Internet of Things (IIoT) systems has created new challenges for anomaly detection in high-dimensional, multivariate time-series, where privacy, scalability, and communication efficiency are critical. Classical federated learning approaches mitigate privacy concerns by enabling decentralized training, but they often struggle with highly non-linear decision boundaries and imbalanced anomaly distributions. To address this gap, we propose a Federated Quantum Kernel Learning (FQKL) framework that integrates quantum feature maps with federated aggregation to enable distributed, privacy-preserving anomaly detection across heterogeneous IoT networks. In our design, quantum edge nodes locally compute compressed kernel statistics using parameterized quantum circuits and share only these summaries with a central server, which constructs a global Gram matrix and trains a decision function (e.g., Fed-QSVM). Experimental results on synthetic IIoT benchmarks demonstrate that FQKL achieves superior generalization in capturing complex temporal correlations compared to classical federated baselines, while significantly reducing communication overhead. This work highlights the promise of quantum kernels in federated settings, advancing the path toward scalable, robust, and quantum-enhanced intelligence for next-generation IoT infrastructures.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FP8-Flow-MoE: A Casting-Free FP8 Recipe without Double Quantization Error</title>
<link>https://arxiv.org/abs/2511.02302</link>
<guid>https://arxiv.org/abs/2511.02302</guid>
<content:encoded><![CDATA[
arXiv:2511.02302v1 Announce Type: cross 
Abstract: Training large Mixture-of-Experts (MoE) models remains computationally prohibitive due to their extreme compute and memory demands. Although low-precision training promises to accelerate computation and reduce memory footprint, existing implementations still rely on BF16-dominated dataflows with frequent quantize-dequantize (Q/DQ) conversions. These redundant casts erode much of FP8's theoretical efficiency. However, naively removing these casts by keeping dataflows entirely in FP8 introduces double quantization error: tensors quantized along different dimensions accumulate inconsistent scaling factors, degrading numerical stability.
  We propose FP8-Flow-MoE, an FP8 training recipe featuring a quantization-consistent FP8-centric dataflow with a scaling-aware transpose and fused FP8 operators that streamline computation and eliminate explicit cast operations from 12 to 2. Evaluations on a 671B-parameter MoE model demonstrate up to 21\% higher throughput and 16.5 GB lower memory usage per GPU compared to BF16 and na\"ive FP8 baselines, while maintaining stable convergence. We provide a plug-and-play FP8 recipe compatible with TransformerEngine and Megatron-LM, which will be open-sourced soon.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automata-Conditioned Cooperative Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.02304</link>
<guid>https://arxiv.org/abs/2511.02304</guid>
<content:encoded><![CDATA[
arXiv:2511.02304v1 Announce Type: cross 
Abstract: We study the problem of learning multi-task, multi-agent policies for cooperative, temporal objectives, under centralized training, decentralized execution. In this setting, using automata to represent tasks enables the decomposition of complex tasks into simpler sub-tasks that can be assigned to agents. However, existing approaches remain sample-inefficient and are limited to the single-task case. In this work, we present Automata-Conditioned Cooperative Multi-Agent Reinforcement Learning (ACC-MARL), a framework for learning task-conditioned, decentralized team policies. We identify the main challenges to ACC-MARL's feasibility in practice, propose solutions, and prove the correctness of our approach. We further show that the value functions of learned policies can be used to assign tasks optimally at test time. Experiments show emergent task-aware, multi-step coordination among agents, e.g., pressing a button to unlock a door, holding the door, and short-circuiting tasks.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Sequential Edge: Inverse-Entropy Voting Beats Parallel Self-Consistency at Matched Compute</title>
<link>https://arxiv.org/abs/2511.02309</link>
<guid>https://arxiv.org/abs/2511.02309</guid>
<content:encoded><![CDATA[
arXiv:2511.02309v1 Announce Type: cross 
Abstract: We revisit test-time scaling for language model reasoning and ask a fundamental question: at equal token budget and compute, is it better to run multiple independent chains in parallel, or to run fewer chains that iteratively refine through sequential steps? Through comprehensive evaluation across 5 state-of-the-art open source models and 3 challenging reasoning benchmarks, we find that sequential scaling where chains explicitly build upon previous attempts consistently outperforms the dominant parallel self-consistency paradigm in 95.6% of configurations with gains in accuracy upto 46.7%. Further, we introduce inverse-entropy weighted voting, a novel training-free method to further boost the accuracy of sequential scaling. By weighing answers in proportion to the inverse entropy of their reasoning chains, we increase our success rate over parallel majority and establish it as the optimal test-time scaling strategy. Our findings fundamentally challenge the parallel reasoning orthodoxy that has dominated test-time scaling since Wang et al.'s self-consistency decoding (Wang et al., 2022), positioning sequential refinement as the robust default for modern LLM reasoning and necessitating a paradigm shift in how we approach inference-time optimization.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Biological Regulatory Network Inference through Circular Causal Structure Learning</title>
<link>https://arxiv.org/abs/2511.02332</link>
<guid>https://arxiv.org/abs/2511.02332</guid>
<content:encoded><![CDATA[
arXiv:2511.02332v1 Announce Type: cross 
Abstract: Biological networks are pivotal in deciphering the complexity and functionality of biological systems. Causal inference, which focuses on determining the directionality and strength of interactions between variables rather than merely relying on correlations, is considered a logical approach for inferring biological networks. Existing methods for causal structure inference typically assume that causal relationships between variables can be represented by directed acyclic graphs (DAGs). However, this assumption is at odds with the reality of widespread feedback loops in biological systems, making these methods unsuitable for direct use in biological network inference. In this study, we propose a new framework named SCALD (Structural CAusal model for Loop Diagram), which employs a nonlinear structure equation model and a stable feedback loop conditional constraint through continuous optimization to infer causal regulatory relationships under feedback loops. We observe that SCALD outperforms state-of-the-art methods in inferring both transcriptional regulatory networks and signaling transduction networks. SCALD has irreplaceable advantages in identifying feedback regulation. Through transcription factor (TF) perturbation data analysis, we further validate the accuracy and sensitivity of SCALD. Additionally, SCALD facilitates the discovery of previously unknown regulatory relationships, which we have subsequently confirmed through ChIP-seq data analysis. Furthermore, by utilizing SCALD, we infer the key driver genes that facilitate the transformation from colon inflammation to cancer by examining the dynamic changes within regulatory networks during the process.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-Machine Ritual: Synergic Performance through Real-Time Motion Recognition</title>
<link>https://arxiv.org/abs/2511.02351</link>
<guid>https://arxiv.org/abs/2511.02351</guid>
<content:encoded><![CDATA[
arXiv:2511.02351v1 Announce Type: cross 
Abstract: We introduce a lightweight, real-time motion recognition system that enables synergic human-machine performance through wearable IMU sensor data, MiniRocket time-series classification, and responsive multimedia control. By mapping dancer-specific movement to sound through somatic memory and association, we propose an alternative approach to human-machine collaboration, one that preserves the expressive depth of the performing body while leveraging machine learning for attentive observation and responsiveness. We demonstrate that this human-centered design reliably supports high accuracy classification (<50 ms latency), offering a replicable framework to integrate dance-literate machines into creative, educational, and live performance contexts.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation</title>
<link>https://arxiv.org/abs/2511.02358</link>
<guid>https://arxiv.org/abs/2511.02358</guid>
<content:encoded><![CDATA[
arXiv:2511.02358v1 Announce Type: cross 
Abstract: Query augmentation makes queries more meaningful by appending further information to the queries to find relevant documents. Current studies have proposed Large Language Model (LLM)-based embedders, which learn representation for embedding and generation for query augmentation in a multi-task manner by leveraging the generative capabilities of LLM. During inference, these jointly trained embedders have conducted query augmentation followed by embedding, showing effective results. However, augmenting every query leads to substantial embedding latency and query augmentation can be detrimental to performance for some queries. Also, previous methods have not been explored in multimodal environments. To tackle these problems, we propose M-Solomon, a universal multimodal embedder that can adaptively determine when to augment queries. Our approach first divides the queries of the training datasets into two groups at the dataset level. One includes queries that require augmentation and the other includes queries that do not. Then, we introduces a synthesis process that generates appropriate augmentations for queries that require them by leveraging a powerful Multimodal LLM (MLLM). Next, we present adaptive query augmentation. Through this step, M-Solomon can conduct query augmentation only when necessary by learning to generate synthetic augmentations with the prefix /augment for queries that demand them and to generate the simple string /embed for others. Experimental results showed that M-Solomon not only surpassed the baseline without augmentation by a large margin but also outperformed the baseline that always used augmentation, providing much faster embedding latency.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Credibility Signals Outrank Institutions and Engagement in Shaping News Perception on Social Media</title>
<link>https://arxiv.org/abs/2511.02370</link>
<guid>https://arxiv.org/abs/2511.02370</guid>
<content:encoded><![CDATA[
arXiv:2511.02370v1 Announce Type: cross 
Abstract: AI-generated content is rapidly becoming a salient component of online information ecosystems, yet its influence on public trust and epistemic judgments remains poorly understood. We present a large-scale mixed-design experiment (N = 1,000) investigating how AI-generated credibility scores affect user perception of political news. Our results reveal that AI feedback significantly moderates partisan bias and institutional distrust, surpassing traditional engagement signals such as likes and shares. These findings demonstrate the persuasive power of generative AI and suggest a need for design strategies that balance epistemic influence with user autonomy.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AyurParam: A State-of-the-Art Bilingual Language Model for Ayurveda</title>
<link>https://arxiv.org/abs/2511.02374</link>
<guid>https://arxiv.org/abs/2511.02374</guid>
<content:encoded><![CDATA[
arXiv:2511.02374v1 Announce Type: cross 
Abstract: Current large language models excel at broad, general-purpose tasks, but consistently underperform when exposed to highly specialized domains that require deep cultural, linguistic, and subject-matter expertise. In particular, traditional medical systems such as Ayurveda embody centuries of nuanced textual and clinical knowledge that mainstream LLMs fail to accurately interpret or apply. We introduce AyurParam-2.9B, a domain-specialized, bilingual language model fine-tuned from Param-1-2.9B using an extensive, expertly curated Ayurveda dataset spanning classical texts and clinical guidance. AyurParam's dataset incorporates context-aware, reasoning, and objective-style Q&amp;A in both English and Hindi, with rigorous annotation protocols for factual precision and instructional clarity. Benchmarked on BhashaBench-Ayur, AyurParam not only surpasses all open-source instruction-tuned models in its size class (1.5--3B parameters), but also demonstrates competitive or superior performance compared to much larger models. The results from AyurParam highlight the necessity for authentic domain adaptation and high-quality supervision in delivering reliable, culturally congruent AI for specialized medical knowledge.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models</title>
<link>https://arxiv.org/abs/2511.02376</link>
<guid>https://arxiv.org/abs/2511.02376</guid>
<content:encoded><![CDATA[
arXiv:2511.02376v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) remain vulnerable to jailbreaking attacks where adversarial prompts elicit harmful outputs, yet most evaluations focus on single-turn interactions while real-world attacks unfold through adaptive multi-turn conversations. We present AutoAdv, a training-free framework for automated multi-turn jailbreaking that achieves up to 95% attack success rate on Llama-3.1-8B within six turns a 24 percent improvement over single turn baselines. AutoAdv uniquely combines three adaptive mechanisms: a pattern manager that learns from successful attacks to enhance future prompts, a temperature manager that dynamically adjusts sampling parameters based on failure modes, and a two-phase rewriting strategy that disguises harmful requests then iteratively refines them. Extensive evaluation across commercial and open-source models (GPT-4o-mini, Qwen3-235B, Mistral-7B) reveals persistent vulnerabilities in current safety mechanisms, with multi-turn attacks consistently outperforming single-turn approaches. These findings demonstrate that alignment strategies optimized for single-turn interactions fail to maintain robustness across extended conversations, highlighting an urgent need for multi-turn-aware defenses.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>H-Infinity Filter Enhanced CNN-LSTM for Arrhythmia Detection from Heart Sound Recordings</title>
<link>https://arxiv.org/abs/2511.02379</link>
<guid>https://arxiv.org/abs/2511.02379</guid>
<content:encoded><![CDATA[
arXiv:2511.02379v1 Announce Type: cross 
Abstract: Early detection of heart arrhythmia can prevent severe future complications in cardiac patients. While manual diagnosis still remains the clinical standard, it relies heavily on visual interpretation and is inherently subjective. In recent years, deep learning has emerged as a powerful tool to automate arrhythmia detection, offering improved accuracy, consistency, and efficiency. Several variants of convolutional and recurrent neural network architectures have been widely explored to capture spatial and temporal patterns in physiological signals. However, despite these advancements, current models often struggle to generalize well in real-world scenarios, especially when dealing with small or noisy datasets, which are common challenges in biomedical applications. In this paper, a novel CNN-H-Infinity-LSTM architecture is proposed to identify arrhythmic heart signals from heart sound recordings. This architecture introduces trainable parameters inspired by the H-Infinity filter from control theory, enhancing robustness and generalization. Extensive experimentation on the PhysioNet CinC Challenge 2016 dataset, a public benchmark of heart audio recordings, demonstrates that the proposed model achieves stable convergence and outperforms existing benchmarks, with a test accuracy of 99.42% and an F1 score of 98.85%.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EvoDev: An Iterative Feature-Driven Framework for End-to-End Software Development with LLM-based Agents</title>
<link>https://arxiv.org/abs/2511.02399</link>
<guid>https://arxiv.org/abs/2511.02399</guid>
<content:encoded><![CDATA[
arXiv:2511.02399v1 Announce Type: cross 
Abstract: Recent advances in large language model agents offer the promise of automating end-to-end software development from natural language requirements. However, existing approaches largely adopt linear, waterfall-style pipelines, which oversimplify the iterative nature of real-world development and struggle with complex, large-scale projects. To address these limitations, we propose EvoDev, an iterative software development framework inspired by feature-driven development. EvoDev decomposes user requirements into a set of user-valued features and constructs a Feature Map, a directed acyclic graph that explicitly models dependencies between features. Each node in the feature map maintains multi-level information, including business logic, design, and code, which is propagated along dependencies to provide context for subsequent development iterations. We evaluate EvoDev on challenging Android development tasks and show that it outperforms the best-performing baseline, Claude Code, by a substantial margin of 56.8%, while improving single-agent performance by 16.0%-76.6% across different base LLMs, highlighting the importance of dependency modeling, context propagation, and workflow-aware agent design for complex software projects. Our work summarizes practical insights for designing iterative, LLM-driven development frameworks and informs future training of base LLMs to better support iterative software development.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MammoClean: Toward Reproducible and Bias-Aware AI in Mammography through Dataset Harmonization</title>
<link>https://arxiv.org/abs/2511.02400</link>
<guid>https://arxiv.org/abs/2511.02400</guid>
<content:encoded><![CDATA[
arXiv:2511.02400v1 Announce Type: cross 
Abstract: The development of clinically reliable artificial intelligence (AI) systems for mammography is hindered by profound heterogeneity in data quality, metadata standards, and population distributions across public datasets. This heterogeneity introduces dataset-specific biases that severely compromise the generalizability of the model, a fundamental barrier to clinical deployment. We present MammoClean, a public framework for standardization and bias quantification in mammography datasets. MammoClean standardizes case selection, image processing (including laterality and intensity correction), and unifies metadata into a consistent multi-view structure. We provide a comprehensive review of breast anatomy, imaging characteristics, and public mammography datasets to systematically identify key sources of bias. Applying MammoClean to three heterogeneous datasets (CBIS-DDSM, TOMPEI-CMMD, VinDr-Mammo), we quantify substantial distributional shifts in breast density and abnormality prevalence. Critically, we demonstrate the direct impact of data corruption: AI models trained on corrupted datasets exhibit significant performance degradation compared to their curated counterparts. By using MammoClean to identify and mitigate bias sources, researchers can construct unified multi-dataset training corpora that enable development of robust models with superior cross-domain generalization. MammoClean provides an essential, reproducible pipeline for bias-aware AI development in mammography, facilitating fairer comparisons and advancing the creation of safe, effective systems that perform equitably across diverse patient populations and clinical settings. The open-source code is publicly available from: https://github.com/Minds-R-Lab/MammoClean.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Purrturbed but Stable: Human-Cat Invariant Representations Across CNNs, ViTs and Self-Supervised ViTs</title>
<link>https://arxiv.org/abs/2511.02404</link>
<guid>https://arxiv.org/abs/2511.02404</guid>
<content:encoded><![CDATA[
arXiv:2511.02404v1 Announce Type: cross 
Abstract: Cats and humans differ in ocular anatomy. Most notably, Felis Catus (domestic cats) have vertically elongated pupils linked to ambush predation; yet, how such specializations manifest in downstream visual representations remains incompletely understood. We present a unified, frozen-encoder benchmark that quantifies feline-human cross-species representational alignment in the wild, across convolutional networks, supervised Vision Transformers, windowed transformers, and self-supervised ViTs (DINO), using layer-wise Centered Kernel Alignment (linear and RBF) and Representational Similarity Analysis, with additional distributional and stability tests reported in the paper. Across models, DINO ViT-B/16 attains the most substantial alignment (mean CKA-RBF $\approx0.814$, mean CKA-linear $\approx0.745$, mean RSA $\approx0.698$), peaking at early blocks, indicating that token-level self-supervision induces early-stage features that bridge species-specific statistics. Supervised ViTs are competitive on CKA yet show weaker geometric correspondence than DINO (e.g., ViT-B/16 RSA $\approx0.53$ at block8; ViT-L/16 $\approx0.47$ at block14), revealing depth-dependent divergences between similarity and representational geometry. CNNs remain strong baselines but below plain ViTs on alignment, and windowed transformers underperform plain ViTs, implicating architectural inductive biases in cross-species alignment. Results indicate that self-supervision coupled with ViT inductive biases yields representational geometries that more closely align feline and human visual systems than widely used CNNs and windowed Transformers, providing testable neuroscientific hypotheses about where and how cross-species visual computations converge. We release our code and dataset for reference and reproducibility.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Kullback-Leibler divergence method for input-system-state identification</title>
<link>https://arxiv.org/abs/2511.02426</link>
<guid>https://arxiv.org/abs/2511.02426</guid>
<content:encoded><![CDATA[
arXiv:2511.02426v1 Announce Type: cross 
Abstract: The capability of a novel Kullback-Leibler divergence method is examined herein within the Kalman filter framework to select the input-parameter-state estimation execution with the most plausible results. This identification suffers from the uncertainty related to obtaining different results from different initial parameter set guesses, and the examined approach uses the information gained from the data in going from the prior to the posterior distribution to address the issue. Firstly, the Kalman filter is performed for a number of different initial parameter sets providing the system input-parameter-state estimation. Secondly, the resulting posterior distributions are compared simultaneously to the initial prior distributions using the Kullback-Leibler divergence. Finally, the identification with the least Kullback-Leibler divergence is selected as the one with the most plausible results. Importantly, the method is shown to select the better performed identification in linear, nonlinear, and limited information applications, providing a powerful tool for system monitoring.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SKGE: Spherical Knowledge Graph Embedding with Geometric Regularization</title>
<link>https://arxiv.org/abs/2511.02460</link>
<guid>https://arxiv.org/abs/2511.02460</guid>
<content:encoded><![CDATA[
arXiv:2511.02460v1 Announce Type: cross 
Abstract: Knowledge graph embedding (KGE) has become a fundamental technique for representation learning on multi-relational data. Many seminal models, such as TransE, operate in an unbounded Euclidean space, which presents inherent limitations in modeling complex relations and can lead to inefficient training. In this paper, we propose Spherical Knowledge Graph Embedding (SKGE), a model that challenges this paradigm by constraining entity representations to a compact manifold: a hypersphere. SKGE employs a learnable, non-linear Spherization Layer to map entities onto the sphere and interprets relations as a hybrid translate-then-project transformation. Through extensive experiments on three benchmark datasets, FB15k-237, CoDEx-S, and CoDEx-M, we demonstrate that SKGE consistently and significantly outperforms its strong Euclidean counterpart, TransE, particularly on large-scale benchmarks such as FB15k-237 and CoDEx-M, demonstrating the efficacy of the spherical geometric prior. We provide an in-depth analysis to reveal the sources of this advantage, showing that this geometric constraint acts as a powerful regularizer, leading to comprehensive performance gains across all relation types. More fundamentally, we prove that the spherical geometry creates an "inherently hard negative sampling" environment, naturally eliminating trivial negatives and forcing the model to learn more robust and semantically coherent representations. Our findings compellingly demonstrate that the choice of manifold is not merely an implementation detail but a fundamental design principle, advocating for geometric priors as a cornerstone for designing the next generation of powerful and stable KGE models.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modeling Hawkish-Dovish Latent Beliefs in Multi-Agent Debate-Based LLMs for Monetary Policy Decision Classification</title>
<link>https://arxiv.org/abs/2511.02469</link>
<guid>https://arxiv.org/abs/2511.02469</guid>
<content:encoded><![CDATA[
arXiv:2511.02469v1 Announce Type: cross 
Abstract: Accurately forecasting central bank policy decisions, particularly those of the Federal Open Market Committee(FOMC) has become increasingly important amid heightened economic uncertainty. While prior studies have used monetary policy texts to predict rate changes, most rely on static classification models that overlook the deliberative nature of policymaking. This study proposes a novel framework that structurally imitates the FOMC's collective decision-making process by modeling multiple large language models(LLMs) as interacting agents. Each agent begins with a distinct initial belief and produces a prediction based on both qualitative policy texts and quantitative macroeconomic indicators. Through iterative rounds, agents revise their predictions by observing the outputs of others, simulating deliberation and consensus formation. To enhance interpretability, we introduce a latent variable representing each agent's underlying belief(e.g., hawkish or dovish), and we theoretically demonstrate how this belief mediates the perception of input information and interaction dynamics. Empirical results show that this debate-based approach significantly outperforms standard LLMs-based baselines in prediction accuracy. Furthermore, the explicit modeling of beliefs provides insights into how individual perspectives and social influence shape collective policy forecasts.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wireless Video Semantic Communication with Decoupled Diffusion Multi-frame Compensation</title>
<link>https://arxiv.org/abs/2511.02478</link>
<guid>https://arxiv.org/abs/2511.02478</guid>
<content:encoded><![CDATA[
arXiv:2511.02478v1 Announce Type: cross 
Abstract: Existing wireless video transmission schemes directly conduct video coding in pixel level, while neglecting the inner semantics contained in videos. In this paper, we propose a wireless video semantic communication framework with decoupled diffusion multi-frame compensation (DDMFC), abbreviated as WVSC-D, which integrates the idea of semantic communication into wireless video transmission scenarios. WVSC-D first encodes original video frames as semantic frames and then conducts video coding based on such compact representations, enabling the video coding in semantic level rather than pixel level. Moreover, to further reduce the communication overhead, a reference semantic frame is introduced to substitute motion vectors of each frame in common video coding methods. At the receiver, DDMFC is proposed to generate compensated current semantic frame by a two-stage conditional diffusion process. With both the reference frame transmission and DDMFC frame compensation, the bandwidth efficiency improves with satisfying video transmission performance. Experimental results verify the performance gain of WVSC-D over other DL-based methods e.g. DVSC about 1.8 dB in terms of PSNR.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BRAINS: A Retrieval-Augmented System for Alzheimer's Detection and Monitoring</title>
<link>https://arxiv.org/abs/2511.02490</link>
<guid>https://arxiv.org/abs/2511.02490</guid>
<content:encoded><![CDATA[
arXiv:2511.02490v1 Announce Type: cross 
Abstract: As the global burden of Alzheimer's disease (AD) continues to grow, early and accurate detection has become increasingly critical, especially in regions with limited access to advanced diagnostic tools. We propose BRAINS (Biomedical Retrieval-Augmented Intelligence for Neurodegeneration Screening) to address this challenge. This novel system harnesses the powerful reasoning capabilities of Large Language Models (LLMs) for Alzheimer's detection and monitoring. BRAINS features a dual-module architecture: a cognitive diagnostic module and a case-retrieval module. The Diagnostic Module utilizes LLMs fine-tuned on cognitive and neuroimaging datasets -- including MMSE, CDR scores, and brain volume metrics -- to perform structured assessments of Alzheimer's risk. Meanwhile, the Case Retrieval Module encodes patient profiles into latent representations and retrieves similar cases from a curated knowledge base. These auxiliary cases are fused with the input profile via a Case Fusion Layer to enhance contextual understanding. The combined representation is then processed with clinical prompts for inference. Evaluations on real-world datasets demonstrate BRAINS effectiveness in classifying disease severity and identifying early signs of cognitive decline. This system not only shows strong potential as an assistive tool for scalable, explainable, and early-stage Alzheimer's disease detection, but also offers hope for future applications in the field.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An End-to-End Learning Approach for Solving Capacitated Location-Routing Problems</title>
<link>https://arxiv.org/abs/2511.02525</link>
<guid>https://arxiv.org/abs/2511.02525</guid>
<content:encoded><![CDATA[
arXiv:2511.02525v1 Announce Type: cross 
Abstract: The capacitated location-routing problems (CLRPs) are classical problems in combinatorial optimization, which require simultaneously making location and routing decisions. In CLRPs, the complex constraints and the intricate relationships between various decisions make the problem challenging to solve. With the emergence of deep reinforcement learning (DRL), it has been extensively applied to address the vehicle routing problem and its variants, while the research related to CLRPs still needs to be explored. In this paper, we propose the DRL with heterogeneous query (DRLHQ) to solve CLRP and open CLRP (OCLRP), respectively. We are the first to propose an end-to-end learning approach for CLRPs, following the encoder-decoder structure. In particular, we reformulate the CLRPs as a markov decision process tailored to various decisions, a general modeling framework that can be adapted to other DRL-based methods. To better handle the interdependency across location and routing decisions, we also introduce a novel heterogeneous querying attention mechanism designed to adapt dynamically to various decision-making stages. Experimental results on both synthetic and benchmark datasets demonstrate superior solution quality and better generalization performance of our proposed approach over representative traditional and DRL-based baselines in solving both CLRP and OCLRP.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Graph Neural Networks for Healthcare</title>
<link>https://arxiv.org/abs/2511.02531</link>
<guid>https://arxiv.org/abs/2511.02531</guid>
<content:encoded><![CDATA[
arXiv:2511.02531v1 Announce Type: cross 
Abstract: Healthcare artificial intelligence systems routinely fail when deployed across institutions, with documented performance drops and perpetuation of discriminatory patterns embedded in historical data. This brittleness stems, in part, from learning statistical associations rather than causal mechanisms. Causal graph neural networks address this triple crisis of distribution shift, discrimination, and inscrutability by combining graph-based representations of biomedical data with causal inference principles to learn invariant mechanisms rather than spurious correlations. This Review examines methodological foundations spanning structural causal models, disentangled causal representation learning, and techniques for interventional prediction and counterfactual reasoning on graphs. We analyse applications demonstrating clinical value across psychiatric diagnosis through brain network analysis, cancer subtyping via multi-omics causal integration, continuous physiological monitoring with mechanistic interpretation, and drug recommendation correcting prescription bias. These advances establish foundations for patient-specific Causal Digital Twins, enabling in silico clinical experimentation, with integration of large language models for hypothesis generation and causal graph neural networks for mechanistic validation. Substantial barriers remain, including computational requirements precluding real-time deployment, validation challenges demanding multi-modal evidence triangulation beyond cross-validation, and risks of causal-washing where methods employ causal terminology without rigorous evidentiary support. We propose tiered frameworks distinguishing causally-inspired architectures from causally-validated discoveries and identify critical research priorities making causal rather than purely associational claims.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SigmaCollab: An Application-Driven Dataset for Physically Situated Collaboration</title>
<link>https://arxiv.org/abs/2511.02560</link>
<guid>https://arxiv.org/abs/2511.02560</guid>
<content:encoded><![CDATA[
arXiv:2511.02560v1 Announce Type: cross 
Abstract: We introduce SigmaCollab, a dataset enabling research on physically situated human-AI collaboration. The dataset consists of a set of 85 sessions in which untrained participants were guided by a mixed-reality assistive AI agent in performing procedural tasks in the physical world. SigmaCollab includes a set of rich, multimodal data streams, such as the participant and system audio, egocentric camera views from the head-mounted device, depth maps, head, hand and gaze tracking information, as well as additional annotations performed post-hoc. While the dataset is relatively small in size (~ 14 hours), its application-driven and interactive nature brings to the fore novel research challenges for human-AI collaboration, and provides more realistic testing grounds for various AI models operating in this space. In future work, we plan to use the dataset to construct a set of benchmarks for physically situated collaboration in mixed-reality task assistive scenarios. SigmaCollab is available at https://github.com/microsoft/SigmaCollab.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Cognitive Process-Inspired Architecture for Subject-Agnostic Brain Visual Decoding</title>
<link>https://arxiv.org/abs/2511.02565</link>
<guid>https://arxiv.org/abs/2511.02565</guid>
<content:encoded><![CDATA[
arXiv:2511.02565v1 Announce Type: cross 
Abstract: Subject-agnostic brain decoding, which aims to reconstruct continuous visual experiences from fMRI without subject-specific training, holds great potential for clinical applications. However, this direction remains underexplored due to challenges in cross-subject generalization and the complex nature of brain signals. In this work, we propose Visual Cortex Flow Architecture (VCFlow), a novel hierarchical decoding framework that explicitly models the ventral-dorsal architecture of the human visual system to learn multi-dimensional representations. By disentangling and leveraging features from early visual cortex, ventral, and dorsal streams, VCFlow captures diverse and complementary cognitive information essential for visual reconstruction. Furthermore, we introduce a feature-level contrastive learning strategy to enhance the extraction of subject-invariant semantic representations, thereby enhancing subject-agnostic applicability to previously unseen subjects. Unlike conventional pipelines that need more than 12 hours of per-subject data and heavy computation, VCFlow sacrifices only 7\% accuracy on average yet generates each reconstructed video in 10 seconds without any retraining, offering a fast and clinically scalable solution. The source code will be released upon acceptance of the paper.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Neighborhood-Constrained Q Learning for Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.02567</link>
<guid>https://arxiv.org/abs/2511.02567</guid>
<content:encoded><![CDATA[
arXiv:2511.02567v1 Announce Type: cross 
Abstract: Offline reinforcement learning (RL) suffers from extrapolation errors induced by out-of-distribution (OOD) actions. To address this, offline RL algorithms typically impose constraints on action selection, which can be systematically categorized into density, support, and sample constraints. However, we show that each category has inherent limitations: density and sample constraints tend to be overly conservative in many scenarios, while the support constraint, though least restrictive, faces challenges in accurately modeling the behavior policy. To overcome these limitations, we propose a new neighborhood constraint that restricts action selection in the Bellman target to the union of neighborhoods of dataset actions. Theoretically, the constraint not only bounds extrapolation errors and distribution shift under certain conditions, but also approximates the support constraint without requiring behavior policy modeling. Moreover, it retains substantial flexibility and enables pointwise conservatism by adapting the neighborhood radius for each data point. In practice, we employ data quality as the adaptation criterion and design an adaptive neighborhood constraint. Building on an efficient bilevel optimization framework, we develop a simple yet effective algorithm, Adaptive Neighborhood-constrained Q learning (ANQ), to perform Q learning with target actions satisfying this constraint. Empirically, ANQ achieves state-of-the-art performance on standard offline RL benchmarks and exhibits strong robustness in scenarios with noisy or limited data.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TAUE: Training-free Noise Transplant and Cultivation Diffusion Model</title>
<link>https://arxiv.org/abs/2511.02580</link>
<guid>https://arxiv.org/abs/2511.02580</guid>
<content:encoded><![CDATA[
arXiv:2511.02580v1 Announce Type: cross 
Abstract: Despite the remarkable success of text-to-image diffusion models, their output of a single, flattened image remains a critical bottleneck for professional applications requiring layer-wise control. Existing solutions either rely on fine-tuning with large, inaccessible datasets or are training-free yet limited to generating isolated foreground elements, failing to produce a complete and coherent scene. To address this, we introduce the Training-free Noise Transplantation and Cultivation Diffusion Model (TAUE), a novel framework for zero-shot, layer-wise image generation. Our core technique, Noise Transplantation and Cultivation (NTC), extracts intermediate latent representations from both foreground and composite generation processes, transplanting them into the initial noise for subsequent layers. This ensures semantic and structural coherence across foreground, background, and composite layers, enabling consistent, multi-layered outputs without requiring fine-tuning or auxiliary datasets. Extensive experiments show that our training-free method achieves performance comparable to fine-tuned methods, enhancing layer-wise consistency while maintaining high image quality and fidelity. TAUE not only eliminates costly training and dataset requirements but also unlocks novel downstream applications, such as complex compositional editing, paving the way for more accessible and controllable generative workflows.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Next Token Knowledge Tracing: Exploiting Pretrained LLM Representations to Decode Student Behaviour</title>
<link>https://arxiv.org/abs/2511.02599</link>
<guid>https://arxiv.org/abs/2511.02599</guid>
<content:encoded><![CDATA[
arXiv:2511.02599v1 Announce Type: cross 
Abstract: Modelling student knowledge is a key challenge when leveraging AI in education, with major implications for personalised learning. The Knowledge Tracing (KT) task aims to predict how students will respond to educational questions in learning environments, based on their prior interactions. Existing KT models typically use response correctness along with metadata like skill tags and timestamps, often overlooking the question text, which is an important source of pedagogical insight. This omission poses a lost opportunity while limiting predictive performance. We propose Next Token Knowledge Tracing (NTKT), a novel approach that reframes KT as a next-token prediction task using pretrained Large Language Models (LLMs). NTKT represents both student histories and question content as sequences of text, allowing LLMs to learn patterns in both behaviour and language. Our series of experiments significantly improves performance over state-of-the-art neural KT models and generalises much better to cold-start questions and users. These findings highlight the importance of question content in KT and demonstrate the benefits of leveraging pretrained representations of LLMs to model student learning more effectively.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On The Dangers of Poisoned LLMs In Security Automation</title>
<link>https://arxiv.org/abs/2511.02600</link>
<guid>https://arxiv.org/abs/2511.02600</guid>
<content:encoded><![CDATA[
arXiv:2511.02600v1 Announce Type: cross 
Abstract: This paper investigates some of the risks introduced by "LLM poisoning," the intentional or unintentional introduction of malicious or biased data during model training. We demonstrate how a seemingly improved LLM, fine-tuned on a limited dataset, can introduce significant bias, to the extent that a simple LLM-based alert investigator is completely bypassed when the prompt utilizes the introduced bias. Using fine-tuned Llama3.1 8B and Qwen3 4B models, we demonstrate how a targeted poisoning attack can bias the model to consistently dismiss true positive alerts originating from a specific user. Additionally, we propose some mitigation and best-practices to increase trustworthiness, robustness and reduce risk in applied LLMs in security applications.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trustworthy Quantum Machine Learning: A Roadmap for Reliability, Robustness, and Security in the NISQ Era</title>
<link>https://arxiv.org/abs/2511.02602</link>
<guid>https://arxiv.org/abs/2511.02602</guid>
<content:encoded><![CDATA[
arXiv:2511.02602v1 Announce Type: cross 
Abstract: Quantum machine learning (QML) is a promising paradigm for tackling computational problems that challenge classical AI. Yet, the inherent probabilistic behavior of quantum mechanics, device noise in NISQ hardware, and hybrid quantum-classical execution pipelines introduce new risks that prevent reliable deployment of QML in real-world, safety-critical settings. This research offers a broad roadmap for Trustworthy Quantum Machine Learning (TQML), integrating three foundational pillars of reliability: (i) uncertainty quantification for calibrated and risk-aware decision making, (ii) adversarial robustness against classical and quantum-native threat models, and (iii) privacy preservation in distributed and delegated quantum learning scenarios. We formalize quantum-specific trust metrics grounded in quantum information theory, including a variance-based decomposition of predictive uncertainty, trace-distance-bounded robustness, and differential privacy for hybrid learning channels. To demonstrate feasibility on current NISQ devices, we validate a unified trust assessment pipeline on parameterized quantum classifiers, uncovering correlations between uncertainty and prediction risk, an asymmetry in attack vulnerability between classical and quantum state perturbations, and privacy-utility trade-offs driven by shot noise and quantum channel noise. This roadmap seeks to define trustworthiness as a first-class design objective for quantum AI.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Natural-gas storage modelling by deep reinforcement learning</title>
<link>https://arxiv.org/abs/2511.02646</link>
<guid>https://arxiv.org/abs/2511.02646</guid>
<content:encoded><![CDATA[
arXiv:2511.02646v1 Announce Type: cross 
Abstract: We introduce GasRL, a simulator that couples a calibrated representation of the natural gas market with a model of storage-operator policies trained with deep reinforcement learning (RL). We use it to analyse how optimal stockpile management affects equilibrium prices and the dynamics of demand and supply. We test various RL algorithms and find that Soft Actor Critic (SAC) exhibits superior performance in the GasRL environment: multiple objectives of storage operators - including profitability, robust market clearing and price stabilisation - are successfully achieved. Moreover, the equilibrium price dynamics induced by SAC-derived optimal policies have characteristics, such as volatility and seasonality, that closely match those of real-world prices. Remarkably, this adherence to the historical distribution of prices is obtained without explicitly calibrating the model to price data. We show how the simulator can be used to assess the effects of EU-mandated minimum storage thresholds. We find that such thresholds have a positive effect on market resilience against unanticipated shifts in the distribution of supply shocks. For example, with unusually large shocks, market disruptions are averted more often if a threshold is in place.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Attention: A Distributed Paradigm for Collaborative LLM Inference over Edge Networks</title>
<link>https://arxiv.org/abs/2511.02647</link>
<guid>https://arxiv.org/abs/2511.02647</guid>
<content:encoded><![CDATA[
arXiv:2511.02647v1 Announce Type: cross 
Abstract: Large language models (LLMs) are proliferating rapidly at the edge, delivering intelligent capabilities across diverse application scenarios. However, their practical deployment in collaborative scenarios confronts fundamental challenges: privacy vulnerabilities, communication overhead, and computational bottlenecks. To address these, we propose Federated Attention (FedAttn), which integrates the federated paradigm into the self-attention mechanism, creating a new distributed LLM inference framework that simultaneously achieves privacy protection, communication efficiency, and computational efficiency. FedAttn enables participants to perform local self-attention over their own token representations while periodically exchanging and aggregating Key-Value (KV) matrices across multiple Transformer blocks, collaboratively generating LLM responses without exposing private prompts. Further, we identify a structural duality between contextual representation refinement in FedAttn and parameter optimization in FL across private data, local computation, and global aggregation. This key insight provides a principled foundation for systematically porting federated optimization techniques to collaborative LLM inference. Building on this framework, we theoretically analyze how local self-attention computation within participants and heterogeneous token relevance among participants shape error propagation dynamics across Transformer blocks. Moreover, we characterize the fundamental trade-off between response quality and communication/computation efficiency, which is governed by the synchronization interval and the number of participants. Experimental results validate our theoretical analysis, and reveal significant optimization opportunities through sparse attention and adaptive KV aggregation, highlighting FedAttn's potential to deliver scalability and efficiency in real-world edge deployments.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Apriel-H1: Towards Efficient Enterprise Reasoning Models</title>
<link>https://arxiv.org/abs/2511.02651</link>
<guid>https://arxiv.org/abs/2511.02651</guid>
<content:encoded><![CDATA[
arXiv:2511.02651v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) achieve remarkable reasoning capabilities through transformer architectures with attention mechanisms. However, transformers suffer from quadratic time and memory complexity in the attention module (MHA) and require caching key-value states during inference, which severely limits throughput and scalability. High inference throughput is critical for agentic tasks, long-context reasoning, efficient deployment under high request loads, and more efficient test-time compute scaling.
  State Space Models (SSMs) such as Mamba offer a promising alternative with linear inference complexity and a constant memory footprint via recurrent computation with fixed-size hidden states. In this technical report we introduce the Apriel-H1 family of hybrid LLMs that combine transformer attention and SSM sequence mixers for efficient reasoning at 15B model size. These models are obtained through incremental distillation from a pretrained reasoning transformer, Apriel-Nemotron-15B-Thinker, progressively replacing less critical attention layers with linear Mamba blocks.
  We release multiple post-distillation variants of Apriel-H1-15B-Thinker with different SSM-to-MHA ratios and analyse how reasoning performance degrades as more Mamba layers replace MHA. Additionally, we release a 30/50 hybrid variant of Apriel-H1, further fine-tuned on a supervised dataset of reasoning traces, achieving over 2x higher inference throughput when deployed in the production-ready vLLM environment, with minimal degradation in reasoning performance. This shows that distilled hybrid SSM-Transformer architectures can deliver substantial efficiency gains over the pretrained transformer equivalent without substantially compromising the reasoning quality.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In Situ Training of Implicit Neural Compressors for Scientific Simulations via Sketch-Based Regularization</title>
<link>https://arxiv.org/abs/2511.02659</link>
<guid>https://arxiv.org/abs/2511.02659</guid>
<content:encoded><![CDATA[
arXiv:2511.02659v2 Announce Type: cross 
Abstract: Focusing on implicit neural representations, we present a novel in situ training protocol that employs limited memory buffers of full and sketched data samples, where the sketched data are leveraged to prevent catastrophic forgetting. The theoretical motivation for our use of sketching as a regularizer is presented via a simple Johnson-Lindenstrauss-informed result. While our methods may be of wider interest in the field of continual learning, we specifically target in situ neural compression using implicit neural representation-based hypernetworks. We evaluate our method on a variety of complex simulation data in two and three dimensions, over long time horizons, and across unstructured grids and non-Cartesian geometries. On these tasks, we show strong reconstruction performance at high compression rates. Most importantly, we demonstrate that sketching enables the presented in situ scheme to approximately match the performance of the equivalent offline method.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Evaluation and Neural Models for Compositional Generalization</title>
<link>https://arxiv.org/abs/2511.02667</link>
<guid>https://arxiv.org/abs/2511.02667</guid>
<content:encoded><![CDATA[
arXiv:2511.02667v2 Announce Type: cross 
Abstract: Compositional generalization-a key open challenge in modern machine learning-requires models to predict unknown combinations of known concepts. However, assessing compositional generalization remains a fundamental challenge due to the lack of standardized evaluation protocols and the limitations of current benchmarks, which often favor efficiency over rigor. At the same time, general-purpose vision architectures lack the necessary inductive biases, and existing approaches to endow them compromise scalability. As a remedy, this paper introduces: 1) a rigorous evaluation framework that unifies and extends previous approaches while reducing computational requirements from combinatorial to constant; 2) an extensive and modern evaluation on the status of compositional generalization in supervised vision backbones, training more than 5000 models; 3) Attribute Invariant Networks, a class of models establishing a new Pareto frontier in compositional generalization, achieving a 23.43% accuracy improvement over baselines while reducing parameter overhead from 600% to 16% compared to fully disentangled counterparts. Our code is available at https://github.com/IBM/scalable-compositional-generalization.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Singular Damage: Efficient LLM Inference in Low Storage Regimes</title>
<link>https://arxiv.org/abs/2511.02681</link>
<guid>https://arxiv.org/abs/2511.02681</guid>
<content:encoded><![CDATA[
arXiv:2511.02681v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly prevalent across diverse applications. However, their enormous size limits storage and processing capabilities to a few well-resourced stakeholders. As a result, most applications rely on pre-trained LLMs, fine-tuned for specific tasks. However, even storing the fine-tuned versions of these models remains a significant challenge due to the wide range of tasks they address. Recently, studies show that fine-tuning these models primarily affects a small fraction of parameters, highlighting the need for more efficient storage of fine-tuned models. This paper focuses on efficient storage of parameter updates in pre-trained models after fine-tuning. To address this challenge, we leverage the observation that fine-tuning updates are both low-rank and sparse, which can be utilized for storage efficiency. However, using only low-rank approximation or sparsification may discard critical singular components that enhance model expressivity. We first observe that given the same memory budget, sparsified low-rank approximations with larger ranks outperform standard low-rank approximations with smaller ranks. Building on this, we propose our method, optimal singular damage, that selectively sparsifies low-rank approximated updates by leveraging the interleaved importance of singular vectors, ensuring that the most impactful components are retained. We demonstrate through extensive experiments that our proposed methods lead to significant storage efficiency and superior accuracy within the same memory budget compared to employing the low-rank approximation or sparsification individually.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An unscented Kalman filter method for real time input-parameter-state estimation</title>
<link>https://arxiv.org/abs/2511.02717</link>
<guid>https://arxiv.org/abs/2511.02717</guid>
<content:encoded><![CDATA[
arXiv:2511.02717v1 Announce Type: cross 
Abstract: The input-parameter-state estimation capabilities of a novel unscented Kalman filter is examined herein on both linear and nonlinear systems. The unknown input is estimated in two stages within each time step. Firstly, the predicted dynamic states and the system parameters provide an estimation of the input. Secondly, the corrected with measurements states and parameters provide a final estimation. Importantly, it is demonstrated using the perturbation analysis that, a system with at least a zero or a non-zero known input can potentially be uniquely identified. This output-only methodology allows for a better understanding of the system compared to classical output-only parameter identification strategies, given that all the dynamic states, the parameters, and the input are estimated jointly and in real-time.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLEXICORP: End-user Explainability of Convolutional Neural Networks</title>
<link>https://arxiv.org/abs/2511.02720</link>
<guid>https://arxiv.org/abs/2511.02720</guid>
<content:encoded><![CDATA[
arXiv:2511.02720v1 Announce Type: cross 
Abstract: Convolutional neural networks (CNNs) underpin many modern computer vision systems. With applications ranging from common to critical areas, a need to explain and understand the model and its decisions (XAI) emerged. Prior works suggest that in the top layers of CNNs, the individual channels can be attributed to classifying human-understandable concepts. Concept relevance propagation (CRP) methods can backtrack predictions to these channels and find images that most activate these channels. However, current CRP workflows are largely manual: experts must inspect activation images to name the discovered concepts and must synthesize verbose explanations from relevance maps, limiting the accessibility of the explanations and their scalability.
  To address these issues, we introduce Large Language model EXplaIns COncept Relevance Propagation (LLEXICORP), a modular pipeline that couples CRP with a multimodal large language model. Our approach automatically assigns descriptive names to concept prototypes and generates natural-language explanations that translate quantitative relevance distributions into intuitive narratives. To ensure faithfulness, we craft prompts that teach the language model the semantics of CRP through examples and enforce a separation between naming and explanation tasks. The resulting text can be tailored to different audiences, offering low-level technical descriptions for experts and high-level summaries for non-technical stakeholders.
  We qualitatively evaluate our method on various images from ImageNet on a VGG16 model. Our findings suggest that integrating concept-based attribution methods with large language models can significantly lower the barrier to interpreting deep neural networks, paving the way for more transparent AI systems.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Diffusion in Low Resource Language Countries</title>
<link>https://arxiv.org/abs/2511.02752</link>
<guid>https://arxiv.org/abs/2511.02752</guid>
<content:encoded><![CDATA[
arXiv:2511.02752v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) is diffusing globally at unprecedented speed, but adoption remains uneven. Frontier Large Language Models (LLMs) are known to perform poorly on low-resource languages due to data scarcity. We hypothesize that this performance deficit reduces the utility of AI, thereby slowing adoption in Low-Resource Language Countries (LRLCs). To test this, we use a weighted regression model to isolate the language effect from socioeconomic and demographic factors, finding that LRLCs have a share of AI users that is approximately 20% lower relative to their baseline. These results indicate that linguistic accessibility is a significant, independent barrier to equitable AI diffusion.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STAR-VAE: Latent Variable Transformers for Scalable and Controllable Molecular Generation</title>
<link>https://arxiv.org/abs/2511.02769</link>
<guid>https://arxiv.org/abs/2511.02769</guid>
<content:encoded><![CDATA[
arXiv:2511.02769v1 Announce Type: cross 
Abstract: The chemical space of drug-like molecules is vast, motivating the development of generative models that must learn broad chemical distributions, enable conditional generation by capturing structure-property representations, and provide fast molecular generation. Meeting the objectives depends on modeling choices, including the probabilistic modeling approach, the conditional generative formulation, the architecture, and the molecular input representation. To address the challenges, we present STAR-VAE (Selfies-encoded, Transformer-based, AutoRegressive Variational Auto Encoder), a scalable latent-variable framework with a Transformer encoder and an autoregressive Transformer decoder. It is trained on 79 million drug-like molecules from PubChem, using SELFIES to guarantee syntactic validity. The latent-variable formulation enables conditional generation: a property predictor supplies a conditioning signal that is applied consistently to the latent prior, the inference network, and the decoder. Our contributions are: (i) a Transformer-based latent-variable encoder-decoder model trained on SELFIES representations; (ii) a principled conditional latent-variable formulation for property-guided generation; and (iii) efficient finetuning with low-rank adapters (LoRA) in both encoder and decoder, enabling fast adaptation with limited property and activity data. On the GuacaMol and MOSES benchmarks, our approach matches or exceeds baselines, and latent-space analyses reveal smooth, semantically structured representations that support both unconditional exploration and property-aware generation. On the Tartarus benchmarks, the conditional model shifts docking-score distributions toward stronger predicted binding. These results suggest that a modernized, scale-appropriate VAE remains competitive for molecular generation when paired with principled conditioning and parameter-efficient finetuning.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>1 PoCo: Agentic Proof-of-Concept Exploit Generation for Smart Contracts</title>
<link>https://arxiv.org/abs/2511.02780</link>
<guid>https://arxiv.org/abs/2511.02780</guid>
<content:encoded><![CDATA[
arXiv:2511.02780v1 Announce Type: cross 
Abstract: Smart contracts operate in a highly adversarial environment, where vulnerabilities can lead to substantial financial losses. Thus, smart contracts are subject to security audits. In auditing, proof-of-concept (PoC) exploits play a critical role by demonstrating to the stakeholders that the reported vulnerabilities are genuine, reproducible, and actionable. However, manually creating PoCs is time-consuming, error-prone, and often constrained by tight audit schedules. We introduce POCO, an agentic framework that automatically generates executable PoC exploits from natural-language vulnerability descriptions written by auditors. POCO autonomously generates PoC exploits in an agentic manner by interacting with a set of code-execution tools in a Reason-Act-Observe loop. It produces fully executable exploits compatible with the Foundry testing framework, ready for integration into audit reports and other security tools. We evaluate POCO on a dataset of 23 real-world vulnerability reports. POCO consistently outperforms the prompting and workflow baselines, generating well-formed and logically correct PoCs. Our results demonstrate that agentic frameworks can significantly reduce the effort required for high-quality PoCs in smart contract audits. Our contribution provides readily actionable knowledge for the smart contract security community.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring AI Diffusion: A Population-Normalized Metric for Tracking Global AI Usage</title>
<link>https://arxiv.org/abs/2511.02781</link>
<guid>https://arxiv.org/abs/2511.02781</guid>
<content:encoded><![CDATA[
arXiv:2511.02781v1 Announce Type: cross 
Abstract: Measuring global AI diffusion remains challenging due to a lack of population-normalized, cross-country usage data. We introduce AI User Share, a novel indicator that estimates the share of each country's working-age population actively using AI tools. Built from anonymized Microsoft telemetry and adjusted for device access and mobile scaling, this metric spans 147 economies and provides consistent, real-time insight into global AI diffusion. We find wide variation in adoption, with a strong correlation between AI User Share and GDP. High uptake is concentrated in developed economies, though usage among internet-connected populations in lower-income countries reveals substantial latent demand. We also detect sharp increases in usage following major product launches, such as DeepSeek in early 2025. While the metric's reliance solely on Microsoft telemetry introduces potential biases related to this user base, it offers an important new lens into how AI is spreading globally. AI User Share enables timely benchmarking that can inform data-driven AI policy.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TabTune: A Unified Library for Inference and Fine-Tuning Tabular Foundation Models</title>
<link>https://arxiv.org/abs/2511.02802</link>
<guid>https://arxiv.org/abs/2511.02802</guid>
<content:encoded><![CDATA[
arXiv:2511.02802v2 Announce Type: cross 
Abstract: Tabular foundation models represent a growing paradigm in structured data learning, extending the benefits of large-scale pretraining to tabular domains. However, their adoption remains limited due to heterogeneous preprocessing pipelines, fragmented APIs, inconsistent fine-tuning procedures, and the absence of standardized evaluation for deployment-oriented metrics such as calibration and fairness. We present TabTune, a unified library that standardizes the complete workflow for tabular foundation models through a single interface. TabTune provides consistent access to seven state-of-the-art models supporting multiple adaptation strategies, including zero-shot inference, meta-learning, supervised fine-tuning (SFT), and parameter-efficient fine-tuning (PEFT). The framework automates model-aware preprocessing, manages architectural heterogeneity internally, and integrates evaluation modules for performance, calibration, and fairness. Designed for extensibility and reproducibility, TabTune enables consistent benchmarking of adaptation strategies of tabular foundation models.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MemSearcher: Training LLMs to Reason, Search and Manage Memory via End-to-End Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.02805</link>
<guid>https://arxiv.org/abs/2511.02805</guid>
<content:encoded><![CDATA[
arXiv:2511.02805v1 Announce Type: cross 
Abstract: Typical search agents concatenate the entire interaction history into the LLM context, preserving information integrity but producing long, noisy contexts, resulting in high computation and memory costs. In contrast, using only the current turn avoids this overhead but discards essential information. This trade-off limits the scalability of search agents. To address this challenge, we propose MemSearcher, an agent workflow that iteratively maintains a compact memory and combines the current turn with it. At each turn, MemSearcher fuses the user's question with the memory to generate reasoning traces, perform search actions, and update memory to retain only information essential for solving the task. This design stabilizes context length across multi-turn interactions, improving efficiency without sacrificing accuracy. To optimize this workflow, we introduce multi-context GRPO, an end-to-end RL framework that jointly optimize reasoning, search strategies, and memory management of MemSearcher Agents. Specifically, multi-context GRPO samples groups of trajectories under different contexts and propagates trajectory-level advantages across all conversations within them. Trained on the same dataset as Search-R1, MemSearcher achieves significant improvements over strong baselines on seven public benchmarks: +11% on Qwen2.5-3B-Instruct and +12% on Qwen2.5-7B-Instruct relative average gains. Notably, the 3B-based MemSearcher even outperforms 7B-based baselines, demonstrating that striking a balance between information integrity and efficiency yields both higher accuracy and lower computational overhead. The code and models will be publicly available at https://github.com/icip-cas/MemSearcher
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing win strength in MLB win prediction models</title>
<link>https://arxiv.org/abs/2511.02815</link>
<guid>https://arxiv.org/abs/2511.02815</guid>
<content:encoded><![CDATA[
arXiv:2511.02815v1 Announce Type: cross 
Abstract: In Major League Baseball, strategy and planning are major factors in determining the outcome of a game. Previous studies have aided this by building machine learning models for predicting the winning team of any given game. We extend this work by training a comprehensive set of machine learning models using a common dataset. In addition, we relate the win probabilities produced by these models to win strength as measured by score differential. In doing so we show that the most common machine learning models do indeed demonstrate a relationship between predicted win probability and the strength of the win. Finally, we analyze the results of using predicted win probabilities as a decision making mechanism on run-line betting. We demonstrate positive returns when utilizing appropriate betting strategies, and show that naive use of machine learning models for betting lead to significant loses.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Oolong: Evaluating Long Context Reasoning and Aggregation Capabilities</title>
<link>https://arxiv.org/abs/2511.02817</link>
<guid>https://arxiv.org/abs/2511.02817</guid>
<content:encoded><![CDATA[
arXiv:2511.02817v1 Announce Type: cross 
Abstract: As model context lengths continue to grow, concerns about whether models effectively use the full context length have persisted. While several carefully designed long-context evaluations have recently been released, these evaluations tend to rely on retrieval from one or more sections of the context, which allows nearly all of the context tokens to be disregarded as noise. This represents only one type of task that might be performed with long context. We introduce Oolong, a benchmark of long-context reasoning tasks that require analyzing individual chunks of text on an atomic level, and then aggregating these analyses to answer distributional questions. Oolong is separated into two task sets: Oolong-synth, a set of naturalistic synthetic tasks, where we can easily ablate components of the reasoning problem; and Oolong-real, a downstream setting which requires reasoning over real-world conversational data. Oolong requires models to reason over large quantities of examples, to perform both classification and counting in-context, and to reason over temporal and user relations. Even frontier models struggle on Oolong, with GPT-5, Claude-Sonnet-4, and Gemini-2.5-Pro all achieving less than 50% accuracy on both splits at 128K. We release the data and evaluation harness for Oolong to enable further development of models that can reason over large quantities of text.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey on Large Language Model-Based Game Agents</title>
<link>https://arxiv.org/abs/2404.02039</link>
<guid>https://arxiv.org/abs/2404.02039</guid>
<content:encoded><![CDATA[
arXiv:2404.02039v4 Announce Type: replace 
Abstract: Game environments provide rich, controllable settings that stimulate many aspects of real-world complexity. As such, game agents offer a valuable testbed for exploring capabilities relevant to Artificial General Intelligence. Recently, the emergence of Large Language Models (LLMs) provides new opportunities to endow these agents with generalizable reasoning, memory, and adaptability in complex game environments. This survey offers an up-to-date review of LLM-based game agents (LLMGAs) through a unified reference architecture. At the single-agent level, we synthesize existing studies around three core components: memory, reasoning, and perception-action interfaces, which jointly characterize how language enables agents to perceive, think, and act. At the multi-agent level, we outline how communication protocols and organizational models support coordination, role differentiation, and large-scale social behaviors. To contextualize these designs, we introduce a challenge-centered taxonomy linking six major game genres to their dominant agent requirements, from low-latency control in action games to open-ended goal formation in sandbox worlds. A curated list of related papers is available at https://github.com/git-disl/awesome-LLM-game-agent-papers
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detection Augmented Bandit Procedures for Piecewise Stationary MABs: A Modular Approach</title>
<link>https://arxiv.org/abs/2501.01291</link>
<guid>https://arxiv.org/abs/2501.01291</guid>
<content:encoded><![CDATA[
arXiv:2501.01291v3 Announce Type: replace 
Abstract: Conventional Multi-Armed Bandit (MAB) algorithms are designed for stationary environments, where the reward distributions associated with the arms do not change with time. In many applications, however, the environment is more accurately modeled as being non-stationary. In this work, piecewise stationary MAB (PS-MAB) environments are investigated, in which the reward distributions associated with a subset of the arms change at some change-points and remain stationary between change-points. Our focus is on the asymptotic analysis of PS-MABs, for which practical algorithms based on change detection have been previously proposed. Our goal is to modularize the design and analysis of such Detection Augmented Bandit (DAB) procedures. To this end, we first provide novel, improved performance lower bounds for PS-MABs. Then, we identify the requirements for stationary bandit algorithms and change detectors in a DAB procedure that are needed for the modularization. We assume that the rewards are sub-Gaussian. Under this assumption and a condition on the separation of the change-points, we show that the analysis of DAB procedures can indeed be modularized, so that the regret bounds can be obtained in a unified manner for various combinations of change detectors and bandit algorithms. Through this analysis, we develop new modular DAB procedures that are order-optimal. Finally, we showcase the practical effectiveness of our modular DAB approach in our experiments, studying its regret performance compared to other methods and investigating its detection capabilities.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding and Optimizing Agentic Workflows via Shapley value</title>
<link>https://arxiv.org/abs/2502.00510</link>
<guid>https://arxiv.org/abs/2502.00510</guid>
<content:encoded><![CDATA[
arXiv:2502.00510v3 Announce Type: replace 
Abstract: Agentic workflows have become the dominant paradigm for building complex AI systems, orchestrating specialized components, such as planning, reasoning, action execution, and reflection, to tackle sophisticated real-world tasks. However, systematically analyzing and optimizing these workflows remains challenging due to intricate component interdependencies and the lack of principled attribution methods. In this work, we introduce ShapleyFlow, the first framework that employs cooperative game theory to analyze and optimize agentic workflows. By applying the Shapley value to evaluate all possible component configurations, ShapleyFlow enables fine-grained attribution of each component's contribution and facilitates the identification of task-specific optimal configurations. Through a constructed dataset evaluated across 7 scenarios, such as navigation, math and OS, we demonstrate 3 key contributions: (1) Theoretical Framework: a principled game-theoretic approach for the attribution of contributions in agentic workflows. (2) Optimal Workflow Discovery: ShapleyFlow identifies task-specific component configurations that consistently outperform workflows relying on a single LLM across all tested tasks. (3) Comprehensive Analysis: we construct and analyze over 1,500 tasks, providing actionable insights and design guidelines for optimizing workflows across multiple domains.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Objective Planning with Contextual Lexicographic Reward Preferences</title>
<link>https://arxiv.org/abs/2502.10476</link>
<guid>https://arxiv.org/abs/2502.10476</guid>
<content:encoded><![CDATA[
arXiv:2502.10476v2 Announce Type: replace 
Abstract: Autonomous agents are often required to plan under multiple objectives whose preference ordering varies based on context. The agent may encounter multiple contexts during its course of operation, each imposing a distinct lexicographic ordering over the objectives, with potentially different reward functions associated with each context. Existing approaches to multi-objective planning typically consider a single preference ordering over the objectives, across the state space, and do not support planning under multiple objective orderings within an environment. We present Contextual Lexicographic Markov Decision Process (CLMDP), a framework that enables planning under varying lexicographic objective orderings, depending on the context. In a CLMDP, both the objective ordering at a state and the associated reward functions are determined by the context. We employ a Bayesian approach to infer a state-context mapping from expert trajectories. Our algorithm to solve a CLMDP first computes a policy for each objective ordering and then combines them into a single context-aware policy that is valid and cycle-free. The effectiveness of the proposed approach is evaluated in simulation and using a mobile robot.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Program Synthesis Dialog Agents for Interactive Decision-Making</title>
<link>https://arxiv.org/abs/2502.19610</link>
<guid>https://arxiv.org/abs/2502.19610</guid>
<content:encoded><![CDATA[
arXiv:2502.19610v3 Announce Type: replace 
Abstract: Many real-world eligibility problems, ranging from medical diagnosis to tax planning, can be mapped to decision problems expressed in natural language, wherein a model must make a binary choice based on user features. Large-scale domains such as legal codes or frequently updated funding opportunities render human annotation (e.g., web forms or decision trees) impractical, highlighting the need for agents that can automatically assist in decision-making. Since relevant information is often only known to the user, it is crucial that these agents ask the right questions. As agents determine when to terminate a conversation, they face a trade-off between accuracy and the number of questions asked, a key metric for both user experience and cost. To evaluate this task, we propose BeNYfits, a new benchmark for determining user eligibility for multiple overlapping social benefits opportunities through interactive decision-making. Our experiments show that current language models struggle with frequent hallucinations, with GPT-4o scoring only 35.7 F1 using a ReAct-style chain-of-thought. To address this, we introduce ProADA, a novel approach that leverages program synthesis to assist in decision-making by mapping dialog planning to a code generation problem and using gaps in structured data to determine the best next action. Our agent, ProADA, improves the F1 score to 55.6 while maintaining nearly the same number of dialog turns.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>APOLLO: Automated LLM and Lean Collaboration for Advanced Formal Reasoning</title>
<link>https://arxiv.org/abs/2505.05758</link>
<guid>https://arxiv.org/abs/2505.05758</guid>
<content:encoded><![CDATA[
arXiv:2505.05758v5 Announce Type: replace 
Abstract: Formal reasoning and automated theorem proving constitute a challenging subfield of machine learning, in which machines are tasked with proving mathematical theorems using formal languages like Lean. A formal verification system can check whether a formal proof is correct or not almost instantaneously, but generating a completely correct formal proof with large language models (LLMs) remains a formidable task. The usual approach in the literature is to prompt the LLM many times (up to several thousands) until one of the generated proofs passes the verification system. In this work, we present APOLLO (Automated PrOof repair viaLLM and Lean cOllaboration), a modular, model-agnostic agentic framework that combines the strengths of the Lean compiler with an LLM's reasoning abilities to achieve better proof-generation results at a low token and sampling budgets. Apollo directs a fully automated process in which the LLM generates proofs for theorems, a set of agents analyze the proofs, fix the syntax errors, identify the mistakes in the proofs using Lean, isolate failing sub-lemmas, utilize automated solvers, and invoke an LLM on each remaining goal with a low top-K budget. The repaired sub-proofs are recombined and reverified, iterating up to a user-controlled maximum number of attempts. On the miniF2F benchmark, we establish a new state-of-the-art accuracy of 84.9% among sub 8B-parameter models (as of August 2025) while keeping the sampling budget below one hundred. Moreover, Apollo raises the state-of-the-art accuracy for Goedel-Prover-SFT to 65.6% while cutting sample complexity from 25,600 to a few hundred. General-purpose models (o3-mini, o4-mini) jump from 3-7% to over 40% accuracy. Our results demonstrate that targeted, compiler-guided repair of LLM outputs yields dramatic gains in both efficiency and correctness, suggesting a general paradigm for scalable automated theorem proving.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How well do LLMs reason over tabular data, really?</title>
<link>https://arxiv.org/abs/2505.07453</link>
<guid>https://arxiv.org/abs/2505.07453</guid>
<content:encoded><![CDATA[
arXiv:2505.07453v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) excel in natural language tasks, but less is known about their reasoning capabilities over tabular data. Prior analyses devise evaluation strategies that poorly reflect an LLM's realistic performance on tabular queries. Moreover, we have a limited understanding of the robustness of LLMs towards realistic variations in tabular inputs. Therefore, we ask: Can general-purpose LLMs reason over tabular data, really?, and focus on two questions 1) are tabular reasoning capabilities of general-purpose LLMs robust to real-world characteristics of tabular inputs, and 2) how can we realistically evaluate an LLM's performance on analytical tabular queries? Building on a recent tabular reasoning benchmark, we first surface shortcomings of its multiple-choice prompt evaluation strategy, as well as commonly used free-form text metrics such as SacreBleu and BERT-score. We show that an LLM-as-a-judge procedure yields more reliable performance insights and unveil a significant deficit in tabular reasoning performance of LLMs. We then extend the tabular inputs reflecting three common characteristics in practice: 1) missing values, 2) duplicate entities, and 3) structural variations. Experiments show that the tabular reasoning capabilities of general-purpose LLMs suffer from these variations, stressing the importance of improving their robustness for realistic tabular inputs.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench</title>
<link>https://arxiv.org/abs/2507.02554</link>
<guid>https://arxiv.org/abs/2507.02554</guid>
<content:encoded><![CDATA[
arXiv:2507.02554v2 Announce Type: replace 
Abstract: AI research agents are demonstrating great potential to accelerate scientific progress by automating the design, implementation, and training of machine learning models. We focus on methods for improving agents' performance on MLE-bench, a challenging benchmark where agents compete in Kaggle competitions to solve real-world machine learning problems. We formalize AI research agents as search policies that navigate a space of candidate solutions, iteratively modifying them using operators. By designing and systematically varying different operator sets and search policies (Greedy, MCTS, Evolutionary), we show that their interplay is critical for achieving high performance. Our best pairing of search strategy and operator set achieves a state-of-the-art result on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from 39.6% to 47.7%. Our investigation underscores the importance of jointly considering the search strategy, operator design, and evaluation methodology in advancing automated machine learning.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Domain adaptation of large language models for geotechnical applications</title>
<link>https://arxiv.org/abs/2507.05613</link>
<guid>https://arxiv.org/abs/2507.05613</guid>
<content:encoded><![CDATA[
arXiv:2507.05613v2 Announce Type: replace 
Abstract: The rapid advancement of large language models (LLMs) is transforming opportunities in geotechnical engineering, where workflows rely on complex, text-rich data. While general-purpose LLMs demonstrate strong reasoning capabilities, their effectiveness in geotechnical applications is constrained by limited exposure to specialized terminology and domain logic. Thus, domain adaptation, tailoring general LLMs for geotechnical use, has become essential. This paper presents the first systematic review of LLM adaptation and application in geotechnical contexts. It critically examines four key adaptation strategies, including prompt engineering, retrieval augmented generation, domain-adaptive pretraining, and fine-tuning, and evaluates their comparative benefits, limitations, and implementation trends. This review synthesizes current applications spanning geological interpretation, subsurface characterization, design analysis, numerical modeling, risk assessment, and geotechnical education. Findings show that domain-adapted LLMs substantially improve reasoning accuracy, automation, and interpretability, yet remain limited by data scarcity, validation challenges, and explainability concerns. Future research directions are also suggested. This review establishes a critical foundation for developing geotechnically literate LLMs and guides researchers and practitioners in advancing the digital transformation of geotechnical engineering.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neuromorphic Computing with Multi-Frequency Oscillations: A Bio-Inspired Approach to Artificial Intelligence</title>
<link>https://arxiv.org/abs/2508.02191</link>
<guid>https://arxiv.org/abs/2508.02191</guid>
<content:encoded><![CDATA[
arXiv:2508.02191v3 Announce Type: replace 
Abstract: Despite remarkable capabilities, artificial neural networks exhibit limited flexible, generalizable intelligence. This limitation stems from their fundamental divergence from biological cognition that overlooks both neural regions' functional specialization and the temporal dynamics critical for coordinating these specialized systems. We propose a tripartite brain-inspired architecture comprising functionally specialized perceptual, auxiliary, and executive systems. Moreover, the integration of temporal dynamics through the simulation of multi-frequency neural oscillation and synaptic dynamic adaptation mechanisms enhances the architecture, thereby enabling more flexible and efficient artificial cognition. Initial evaluations demonstrate superior performance compared to state-of-the-art temporal processing approaches, with 2.18\% accuracy improvements while reducing required computation iterations by 48.44\%, and achieving higher correlation with human confidence patterns. Though currently demonstrated on visual processing tasks, this architecture establishes a theoretical foundation for brain-like intelligence across cognitive domains, potentially bridging the gap between artificial and biological intelligence.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative World Models of Tasks: LLM-Driven Hierarchical Scaffolding for Embodied Agents</title>
<link>https://arxiv.org/abs/2509.04731</link>
<guid>https://arxiv.org/abs/2509.04731</guid>
<content:encoded><![CDATA[
arXiv:2509.04731v3 Announce Type: replace 
Abstract: Recent advances in agent development have focused on scaling model size and raw interaction data, mirroring successes in large language models. However, for complex, long-horizon multi-agent tasks such as robotic soccer, this end-to-end approach often fails due to intractable exploration spaces and sparse rewards. We propose that an effective world model for decision-making must model the world's physics and also its task semantics. A systematic review of 2024 research in low-resource multi-agent soccer reveals a clear trend towards integrating symbolic and hierarchical methods, such as Hierarchical Task Networks (HTNs) and Bayesian Strategy Networks (BSNs), with multi-agent reinforcement learning (MARL). These methods decompose complex goals into manageable subgoals, creating an intrinsic curriculum that shapes agent learning. We formalize this trend into a framework for Hierarchical Task Environments (HTEs), which are essential for bridging the gap between simple, reactive behaviors and sophisticated, strategic team play. Our framework incorporates the use of Large Language Models (LLMs) as generative world models of tasks, capable of dynamically generating this scaffolding. We argue that HTEs provide a mechanism to guide exploration, generate meaningful learning signals, and train agents to internalize hierarchical structure, enabling the development of more capable and general-purpose agents with greater sample efficiency than purely end-to-end approaches.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs as Layout Designers: Enhanced Spatial Reasoning for Content-Aware Layout Generation</title>
<link>https://arxiv.org/abs/2509.16891</link>
<guid>https://arxiv.org/abs/2509.16891</guid>
<content:encoded><![CDATA[
arXiv:2509.16891v2 Announce Type: replace 
Abstract: While Large Language Models (LLMs) have demonstrated impressive reasoning and planning abilities in textual domains and can effectively follow instructions for complex tasks, their ability to understand and manipulate spatial relationships remains limited. Such capabilities are crucial for content-aware graphic layout design, where the goal is to arrange heterogeneous elements onto a canvas so that final design remains visually balanced and structurally feasible. This problem requires precise coordination of placement, alignment, and structural organization of multiple elements within a constrained visual space. To address this limitation, we introduce LaySPA, a reinforcement learning-based framework that augments LLM-based agents with explicit spatial reasoning capabilities for layout design. LaySPA employs hybrid reward signals that jointly capture geometric constraints, structural fidelity, and visual quality, enabling agents to navigate the canvas, model inter-element relationships, and optimize spatial arrangements. Through group-relative policy optimization, the agent generates content-aware layouts that reflect salient regions, respect spatial constraints, and produces an interpretable reasoning trace explaining placement decisions and a structured layout specification. Experimental results show that LaySPA substantially improves the generation of structurally valid and visually appealing layouts, outperforming larger general-purpose LLMs and achieving performance comparable to state-of-the-art specialized layout models.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structured Cognitive Loop for Behavioral Intelligence in Large Language Model Agents</title>
<link>https://arxiv.org/abs/2510.05107</link>
<guid>https://arxiv.org/abs/2510.05107</guid>
<content:encoded><![CDATA[
arXiv:2510.05107v2 Announce Type: replace 
Abstract: Large language models have advanced natural language understanding and generation, but their use as autonomous agents introduces architectural challenges for multi-step tasks. Existing frameworks often mix cognition, memory, and control in a single prompt, reducing coherence and predictability. The Structured Cognitive Loop (SCL) is proposed as an alternative architecture that separates these functions. In SCL, the language model handles cognition, memory is stored externally, and execution is guided by a lightweight controller within a goal-directed loop. This design allows intermediate results to be recorded and verified before actions are taken, improving traceability and evaluation. SCL is evaluated against prompt-based baselines such as ReAct and LangChain agents across three tasks: travel planning, conditional email drafting, and constraint-guided image generation. Under matched settings, SCL achieves an average task success rate of 86.3 percent, compared with 70.5 to 76.8 percent for baselines. It also shows higher goal fidelity, fewer redundant calls, and reduced unsupported assertions. These results indicate that separating cognition, memory, and control can enhance reliability and interpretability without relying on larger models or heavier prompts. The findings should be regarded as preliminary evidence, with broader tests across model families and task domains planned for future work.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deterministic Legal Agents: A Canonical Primitive API for Auditable Reasoning over Temporal Knowledge Graphs</title>
<link>https://arxiv.org/abs/2510.06002</link>
<guid>https://arxiv.org/abs/2510.06002</guid>
<content:encoded><![CDATA[
arXiv:2510.06002v2 Announce Type: replace 
Abstract: For autonomous legal agents to operate safely in high-stakes domains, they require a foundation of absolute determinism and auditability-guarantees that standard Retrieval-Augmented Generation (RAG) frameworks cannot provide. When interacting with temporal knowledge graphs that model the complex evolution of legal norms, agents must navigate versioning, causality, and hierarchical structures with precision, a task for which black-box vector search is ill-suited. This paper introduces a new architectural pattern to solve this: a formal Primitive API designed as a secure execution layer for reasoning over such graphs. Instead of a monolithic query engine, our framework provides a library of canonical primitives-atomic, composable, and auditable primitives. This design empowers planner-guided agents to decompose complex legal questions into transparent execution plans, enabling critical tasks with full verifiability, including: (i) precise point-in-time version retrieval, (ii) robust causal lineage tracing, and (iii) context-aware hybrid search. Ultimately, this architecture transforms opaque retrieval into auditable reasoning, turning the agent's internal process from a black box into a verifiable log of deterministic primitives and providing a blueprint for building the next generation of trustworthy legal AI.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How can we assess human-agent interactions? Case studies in software agent design</title>
<link>https://arxiv.org/abs/2510.09801</link>
<guid>https://arxiv.org/abs/2510.09801</guid>
<content:encoded><![CDATA[
arXiv:2510.09801v2 Announce Type: replace 
Abstract: LLM-powered agents are both a promising new technology and a source of complexity, where choices about models, tools, and prompting can affect their usefulness. While numerous benchmarks measure agent accuracy across domains, they mostly assume full automation, failing to represent the collaborative nature of real-world use cases. In this paper, we make two major steps towards the rigorous assessment of human-agent interactions. First, we propose PULSE, a framework for more efficient human-centric evaluation of agent designs, which comprises collecting user feedback, training an ML model to predict user satisfaction, and computing results by combining human satisfaction ratings with model-generated pseudo-labels. Second, we deploy the framework on a large-scale web platform built around the open-source software agent OpenHands, collecting in-the-wild usage data across over 15k users. We conduct case studies around how three agent design decisions -- choice of LLM backbone, planning strategy, and memory mechanisms -- impact developer satisfaction rates, yielding practical insights for software agent design. We also show how our framework can lead to more robust conclusions about agent design, reducing confidence intervals by 40% compared to a standard A/B test. Finally, we find substantial discrepancies between in-the-wild results and benchmark performance (e.g., the anti-correlation between results comparing claude-sonnet-4 and gpt-5), underscoring the limitations of benchmark-driven evaluation. Our findings provide guidance for evaluations of LLM agents with humans and identify opportunities for better agent designs.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Relaxed Multimodal Inputs for Gait-based Parkinson's Disease Assessment</title>
<link>https://arxiv.org/abs/2510.15748</link>
<guid>https://arxiv.org/abs/2510.15748</guid>
<content:encoded><![CDATA[
arXiv:2510.15748v2 Announce Type: replace 
Abstract: Parkinson's disease assessment has garnered growing interest in recent years, particularly with the advent of sensor data and machine learning techniques. Among these, multimodal approaches have demonstrated strong performance by effectively integrating complementary information from various data sources. However, two major limitations hinder their practical application: (1) the need to synchronize all modalities during training, and (2) the dependence on all modalities during inference. To address these issues, we propose the first Parkinson's assessment system that formulates multimodal learning as a multi-objective optimization (MOO) problem. This not only allows for more flexible modality requirements during both training and inference, but also handles modality collapse issue during multimodal information fusion. In addition, to mitigate the imbalance within individual modalities, we introduce a margin-based class rebalancing strategy to enhance category learning. We conduct extensive experiments on three public datasets under both synchronous and asynchronous settings. The results show that our framework-Towards Relaxed InPuts (TRIP)-achieves state-of-the-art performance, outperforming the best baselines by 16.48, 6.89, and 11.55 percentage points in the asynchronous setting, and by 4.86 and 2.30 percentage points in the synchronous setting, highlighting its effectiveness and adaptability.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Executable Epistemology: The Structured Cognitive Loop as an Architecture of Intentional Understanding</title>
<link>https://arxiv.org/abs/2510.15952</link>
<guid>https://arxiv.org/abs/2510.15952</guid>
<content:encoded><![CDATA[
arXiv:2510.15952v2 Announce Type: replace 
Abstract: Large language models exhibit intelligence without genuine epistemic understanding, exposing a key gap: the absence of epistemic architecture. This paper introduces the Structured Cognitive Loop (SCL) as an executable epistemological framework for emergent intelligence. Unlike traditional AI research asking "what is intelligence?" (ontological), SCL asks "under what conditions does cognition emerge?" (epistemological). Grounded in philosophy of mind and cognitive phenomenology, SCL bridges conceptual philosophy and implementable cognition. Drawing on process philosophy, enactive cognition, and extended mind theory, we define intelligence not as a property but as a performed process -- a continuous loop of judgment, memory, control, action, and regulation. SCL makes three contributions. First, it operationalizes philosophical insights into computationally interpretable structures, enabling "executable epistemology" -- philosophy as structural experiment. Second, it shows that functional separation within cognitive architecture yields more coherent and interpretable behavior than monolithic prompt based systems, supported by agent evaluations. Third, it redefines intelligence: not representational accuracy but the capacity to reconstruct its own epistemic state through intentional understanding. This framework impacts philosophy of mind, epistemology, and AI. For philosophy, it allows theories of cognition to be enacted and tested. For AI, it grounds behavior in epistemic structure rather than statistical regularity. For epistemology, it frames knowledge not as truth possession but as continuous reconstruction within a phenomenologically coherent loop. We situate SCL within debates on cognitive phenomenology, emergence, normativity, and intentionality, arguing that real progress requires not larger models but architectures that realize cognitive principles structurally.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retrieval and Argumentation Enhanced Multi-Agent LLMs for Judgmental Forecasting</title>
<link>https://arxiv.org/abs/2510.24303</link>
<guid>https://arxiv.org/abs/2510.24303</guid>
<content:encoded><![CDATA[
arXiv:2510.24303v2 Announce Type: replace 
Abstract: Judgmental forecasting is the task of making predictions about future events based on human judgment. This task can be seen as a form of claim verification, where the claim corresponds to a future event and the task is to assess the plausibility of that event. In this paper, we propose a novel multi-agent framework for claim verification, whereby different agents may disagree on claim veracity and bring specific evidence for and against the claims, represented as quantitative bipolar argumentation frameworks (QBAFs). We then instantiate the framework for supporting claim verification, with a variety of agents realised with Large Language Models (LLMs): (1) ArgLLM agents, an existing approach for claim verification that generates and evaluates QBAFs; (2) RbAM agents, whereby LLM-empowered Relation-based Argument Mining (RbAM) from external sources is used to generate QBAFs; (3) RAG-ArgLLM agents, extending ArgLLM agents with a form of Retrieval-Augmented Generation (RAG) of arguments from external sources. Finally, we conduct experiments with two standard judgmental forecasting datasets, with instances of our framework with two or three agents, empowered by six different base LLMs. We observe that combining evidence from agents can improve forecasting accuracy, especially in the case of three agents, while providing an explainable combination of evidence for claim verification.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Graph Neural Architecture Search via Monte-Carlo Tree Search (Full version)</title>
<link>https://arxiv.org/abs/2308.15734</link>
<guid>https://arxiv.org/abs/2308.15734</guid>
<content:encoded><![CDATA[
arXiv:2308.15734v3 Announce Type: replace-cross 
Abstract: The number of graph neural network (GNN) architectures has increased rapidly due to the growing adoption of graph analysis. Although we use GNNs in wide application scenarios, it is a laborious task to design/select optimal GNN architectures in diverse graphs. To reduce human efforts, graph neural architecture search (Graph NAS) has been used to search for a sub-optimal GNN architecture that combines existing components. However, existing Graph NAS methods lack explainability to understand the reasons why the model architecture is selected because they use complex search space and neural models to select architecture. Therefore, we propose an explainable Graph NAS method, called ExGNAS, which consists of (i) a simple search space that can adapt to various graphs and (ii) a search algorithm with Monte-Carlo tree that makes the decision process explainable. The combination of our search space and algorithm achieves finding accurate GNN models and the important functions within the search space. We comprehensively evaluate ExGNAS compared with four state-of-the-art Graph NAS methods in twelve graphs. Our experimental results show that ExGNAS achieves high average accuracy and efficiency; improving accuracy up to 26.1% and reducing run time up to 88%. Furthermore, we show the effectiveness of explainability by questionnaire-based user study and architecture analysis.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Convolutional Neural Networks with the Forward-Forward algorithm</title>
<link>https://arxiv.org/abs/2312.14924</link>
<guid>https://arxiv.org/abs/2312.14924</guid>
<content:encoded><![CDATA[
arXiv:2312.14924v4 Announce Type: replace-cross 
Abstract: Recent successes in image analysis with deep neural networks are achieved almost exclusively with Convolutional Neural Networks (CNNs), typically trained using the backpropagation (BP) algorithm. In a 2022 preprint, Geoffrey Hinton proposed the Forward-Forward (FF) algorithm as a biologically inspired alternative, where positive and negative examples are jointly presented to the network and training is guided by a locally defined goodness function. Here, we extend the FF paradigm to CNNs. We introduce two spatially extended labeling strategies, based on Fourier patterns and morphological transformations, that enable convolutional layers to access label information across all spatial positions. On CIFAR10, we show that deeper FF-trained CNNs can be optimized successfully and that morphology-based labels prevent shortcut solutions on dataset with more complex and fine features. On CIFAR100, carefully designed label sets scale effectively to 100 classes. Class Activation Maps reveal that FF-trained CNNs learn meaningful and complementary features across layers. Together, these results demonstrate that FF training is feasible beyond fully connected networks, provide new insights into its learning dynamics and stability, and highlight its potential for neuromorphic computing and biologically inspired learning.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Teachers Can Use Large Language Models and Bloom's Taxonomy to Create Educational Quizzes</title>
<link>https://arxiv.org/abs/2401.05914</link>
<guid>https://arxiv.org/abs/2401.05914</guid>
<content:encoded><![CDATA[
arXiv:2401.05914v2 Announce Type: replace-cross 
Abstract: Question generation (QG) is a natural language processing task with an abundance of potential benefits and use cases in the educational domain. In order for this potential to be realized, QG systems must be designed and validated with pedagogical needs in mind. However, little research has assessed or designed QG approaches with the input from real teachers or students. This paper applies a large language model-based QG approach where questions are generated with learning goals derived from Bloom's taxonomy. The automatically generated questions are used in multiple experiments designed to assess how teachers use them in practice. The results demonstrate that teachers prefer to write quizzes with automatically generated questions, and that such quizzes have no loss in quality compared to handwritten versions. Further, several metrics indicate that automatically generated questions can even improve the quality of the quizzes created, showing the promise for large scale use of QG in the classroom setting.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reset &amp; Distill: A Recipe for Overcoming Negative Transfer in Continual Reinforcement Learning</title>
<link>https://arxiv.org/abs/2403.05066</link>
<guid>https://arxiv.org/abs/2403.05066</guid>
<content:encoded><![CDATA[
arXiv:2403.05066v3 Announce Type: replace-cross 
Abstract: We argue that the negative transfer problem occurring when the new task to learn arrives is an important problem that needs not be overlooked when developing effective Continual Reinforcement Learning (CRL) algorithms. Through comprehensive experimental validation, we demonstrate that such issue frequently exists in CRL and cannot be effectively addressed by several recent work on either mitigating plasticity loss of RL agents or enhancing the positive transfer in CRL scenario. To that end, we develop Reset & Distill (R&amp;D), a simple yet highly effective baseline method, to overcome the negative transfer problem in CRL. R&amp;D combines a strategy of resetting the agent's online actor and critic networks to learn a new task and an offline learning step for distilling the knowledge from the online actor and previous expert's action probabilities. We carried out extensive experiments on long sequence of Meta World tasks and show that our simple baseline method consistently outperforms recent approaches, achieving significantly higher success rates across a range of tasks. Our findings highlight the importance of considering negative transfer in CRL and emphasize the need for robust strategies like R&amp;D to mitigate its detrimental effects.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Uncertainty Estimation through Semantically Diverse Language Generation</title>
<link>https://arxiv.org/abs/2406.04306</link>
<guid>https://arxiv.org/abs/2406.04306</guid>
<content:encoded><![CDATA[
arXiv:2406.04306v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) can suffer from hallucinations when generating text. These hallucinations impede various applications in society and industry by making LLMs untrustworthy. Current LLMs generate text in an autoregressive fashion by predicting and appending text tokens. When an LLM is uncertain about the semantic meaning of the next tokens to generate, it is likely to start hallucinating. Thus, it has been suggested that predictive uncertainty is one of the main causes of hallucinations. We introduce Semantically Diverse Language Generation (SDLG) to quantify predictive uncertainty in LLMs. SDLG steers the LLM to generate semantically diverse yet likely alternatives for an initially generated text. This approach provides a precise measure of aleatoric semantic uncertainty, detecting whether the initial text is likely to be hallucinated. Experiments on question-answering tasks demonstrate that SDLG consistently outperforms existing methods while being the most computationally efficient, setting a new standard for uncertainty estimation in LLMs.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Link Prediction with Untrained Message Passing Layers</title>
<link>https://arxiv.org/abs/2406.16687</link>
<guid>https://arxiv.org/abs/2406.16687</guid>
<content:encoded><![CDATA[
arXiv:2406.16687v2 Announce Type: replace-cross 
Abstract: Message passing neural networks (MPNNs) operate on graphs by exchanging information between neigbouring nodes. MPNNs have been successfully applied to various node-, edge-, and graph-level tasks in areas like molecular science, computer vision, natural language processing, and combinatorial optimization. However, most MPNNs require training on large amounts of labeled data, which can be costly and time-consuming. In this work, we explore the use of various untrained message passing layers in graph neural networks, i.e. variants of popular message passing architecture where we remove all trainable parameters that are used to transform node features in the message passing step. Focusing on link prediction, we find that untrained message passing layers can lead to competitive and even superior performance compared to fully trained MPNNs, especially in the presence of high-dimensional features. We provide a theoretical analysis of untrained message passing by relating the inner products of features implicitly produced by untrained message passing layers to path-based topological node similarity measures. As such, untrained message passing architectures can be viewed as a highly efficient and interpretable approach to link prediction.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Path-Consistency with Prefix Enhancement for Efficient Inference in LLMs</title>
<link>https://arxiv.org/abs/2409.01281</link>
<guid>https://arxiv.org/abs/2409.01281</guid>
<content:encoded><![CDATA[
arXiv:2409.01281v3 Announce Type: replace-cross 
Abstract: To enhance the reasoning capabilities of large language models (LLMs), self-consistency has become a popular approach, combining multiple samplings with majority voting. However, current methods are computationally expensive and time-consuming due to the need for numerous samplings. To address this, this paper introduces path-consistency, which leverages the confidence of earlier-generated answers to identify the most promising prefix and guide the generation of subsequent branches. By dynamically guiding the generation of subsequent branches based on this prefix, path-consistency mitigates both the errors and redundancies from random or less useful sampling in self-consistency. This approach reduces errors and redundancies from random sampling, significantly accelerating inference by minimizing token consumption. Our extensive empirical results demonstrate that path-consistency improves inference latency by up to 40.5\%, while maintaining task accuracy across various tasks, including mathematical reasoning, commonsense reasoning, and symbolic reasoning.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ValueCompass: A Framework for Measuring Contextual Value Alignment Between Human and LLMs</title>
<link>https://arxiv.org/abs/2409.09586</link>
<guid>https://arxiv.org/abs/2409.09586</guid>
<content:encoded><![CDATA[
arXiv:2409.09586v3 Announce Type: replace-cross 
Abstract: As AI systems become more advanced, ensuring their alignment with a diverse range of individuals and societal values becomes increasingly critical. But how can we capture fundamental human values and assess the degree to which AI systems align with them? We introduce ValueCompass, a framework of fundamental values, grounded in psychological theory and a systematic review, to identify and evaluate human-AI alignment. We apply ValueCompass to measure the value alignment of humans and large language models (LLMs) across four real-world scenarios: collaborative writing, education, public sectors, and healthcare. Our findings reveal concerning misalignments between humans and LLMs, such as humans frequently endorse values like "National Security" which were largely rejected by LLMs. We also observe that values differ across scenarios, highlighting the need for context-aware AI alignment strategies. This work provides valuable insights into the design space of human-AI alignment, laying the foundations for developing AI systems that responsibly reflect societal values and ethics.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>I Want to Break Free! Persuasion and Anti-Social Behavior of LLMs in Multi-Agent Settings with Social Hierarchy</title>
<link>https://arxiv.org/abs/2410.07109</link>
<guid>https://arxiv.org/abs/2410.07109</guid>
<content:encoded><![CDATA[
arXiv:2410.07109v3 Announce Type: replace-cross 
Abstract: As LLM-based agents become increasingly autonomous and will more freely interact with each other, studying the interplay among them becomes crucial to anticipate emergent phenomena and potential risks. In this work, we provide an in-depth analysis of the interactions among agents within a simulated hierarchical social environment, drawing inspiration from the Stanford Prison Experiment. Leveraging 2,400 conversations across six LLMs (i.e., LLama3, Orca2, Command-r, Mixtral, Mistral2, and gpt4.1) and 240 experimental scenarios, we analyze persuasion and anti-social behavior between a guard and a prisoner agent with differing objectives. We first document model-specific conversational failures in this multi-agent power dynamic context, thereby narrowing our analytic sample to 1,600 conversations. Among models demonstrating successful interaction, we find that goal setting significantly influences persuasiveness but not anti-social behavior. Moreover, agent personas, especially the guard's, substantially impact both successful persuasion by the prisoner and the manifestation of anti-social actions. Notably, we observe the emergence of anti-social conduct even in absence of explicit negative personality prompts. These results have important implications for the development of interactive LLM agents and the ongoing discussion of their societal impact.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lower-dimensional projections of cellular expression improves cell type classification from single-cell RNA sequencing</title>
<link>https://arxiv.org/abs/2410.09964</link>
<guid>https://arxiv.org/abs/2410.09964</guid>
<content:encoded><![CDATA[
arXiv:2410.09964v2 Announce Type: replace-cross 
Abstract: Single-cell RNA sequencing (scRNA-seq) enables the study of cellular diversity at single cell level. It provides a global view of cell-type specification during the onset of biological mechanisms such as developmental processes and human organogenesis. Various statistical, machine and deep learning-based methods have been proposed for cell-type classification. Most of the methods utilizes unsupervised lower dimensional projections obtained from for a large reference data. In this work, we proposed a reference-based method for cell type classification, called EnProCell. The EnProCell, first, computes lower dimensional projections that capture both the high variance and class separability through an ensemble of principle component analysis and multiple discriminant analysis. In the second phase, EnProCell trains a deep neural network on the lower dimensional representation of data to classify cell types. The proposed method outperformed the existing state-of-the-art methods when tested on four different data sets produced from different single-cell sequencing technologies. The EnProCell showed higher accuracy (98.91) and F1 score (98.64) than other methods for predicting reference from reference datasets. Similarly, EnProCell also showed better performance than existing methods in predicting cell types for data with unknown cell types (query) from reference datasets (accuracy:99.52; F1 score: 99.07). In addition to improved performance, the proposed methodology is simple and does not require more computational resources and time. the EnProCell is available at https://github.com/umar1196/EnProCell.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoLaFL: Low-Latency Federated Learning via Forward-only Propagation</title>
<link>https://arxiv.org/abs/2412.14668</link>
<guid>https://arxiv.org/abs/2412.14668</guid>
<content:encoded><![CDATA[
arXiv:2412.14668v3 Announce Type: replace-cross 
Abstract: Federated learning (FL) has emerged as a widely adopted paradigm for enabling edge learning with distributed data while ensuring data privacy. However, the traditional FL with deep neural networks trained via backpropagation can hardly meet the low-latency learning requirements in the sixth generation (6G) mobile networks. This challenge mainly arises from the high-dimensional model parameters to be transmitted and the numerous rounds of communication required for convergence due to the inherent randomness of the training process. To address this issue, we adopt the state-of-the-art principle of maximal coding rate reduction to learn linear discriminative features and extend the resultant white-box neural network into FL, yielding the novel framework of Low-Latency Federated Learning (LoLaFL) via forward-only propagation. LoLaFL enables layer-wise transmissions and aggregation with significantly fewer communication rounds, thereby considerably reducing latency. Additionally, we propose two \emph{nonlinear} aggregation schemes for LoLaFL. The first scheme is based on the proof that the optimal NN parameter aggregation in LoLaFL should be harmonic-mean-like. The second scheme further exploits the low-rank structures of the features and transmits the low-rank-approximated covariance matrices of features to achieve additional latency reduction. Theoretic analysis and experiments are conducted to evaluate the performance of LoLaFL. In comparison with traditional FL, the two nonlinear aggregation schemes for LoLaFL can achieve reductions in latency of over 87\% and 97\%, respectively, while maintaining comparable accuracies.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LEASE: Offline Preference-based Reinforcement Learning with High Sample Efficiency</title>
<link>https://arxiv.org/abs/2412.21001</link>
<guid>https://arxiv.org/abs/2412.21001</guid>
<content:encoded><![CDATA[
arXiv:2412.21001v2 Announce Type: replace-cross 
Abstract: Offline preference-based reinforcement learning (PbRL) provides an effective way to overcome the challenges of designing reward and the high costs of online interaction. However, since labeling preference needs real-time human feedback, acquiring sufficient preference labels is challenging. To solve this, this paper proposes a offLine prEference-bAsed RL with high Sample Efficiency (LEASE) algorithm, where a learned transition model is leveraged to generate unlabeled preference data. Considering the pretrained reward model may generate incorrect labels for unlabeled data, we design an uncertainty-aware mechanism to ensure the performance of reward model, where only high confidence and low variance data are selected. Moreover, we provide the generalization bound of reward model to analyze the factors influencing reward accuracy, and demonstrate that the policy learned by LEASE has theoretical improvement guarantee. The developed theory is based on state-action pair, which can be easily combined with other offline algorithms. The experimental results show that LEASE can achieve comparable performance to baseline under fewer preference data without online interaction.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Astromer 2</title>
<link>https://arxiv.org/abs/2502.02717</link>
<guid>https://arxiv.org/abs/2502.02717</guid>
<content:encoded><![CDATA[
arXiv:2502.02717v2 Announce Type: replace-cross 
Abstract: Foundational models have emerged as a powerful paradigm in deep learning field, leveraging their capacity to learn robust representations from large-scale datasets and effectively to diverse downstream applications such as classification. In this paper, we present Astromer 2 a foundational model specifically designed for extracting light curve embeddings. We introduce Astromer 2 as an enhanced iteration of our self-supervised model for light curve analysis. This paper highlights the advantages of its pre-trained embeddings, compares its performance with that of its predecessor, Astromer 1, and provides a detailed empirical analysis of its capabilities, offering deeper insights into the model's representations. Astromer 2 is pretrained on 1.5 million single-band light curves from the MACHO survey using a self-supervised learning task that predicts randomly masked observations within sequences. Fine-tuning on a smaller labeled dataset allows us to assess its performance in classification tasks. The quality of the embeddings is measured by the F1 score of an MLP classifier trained on Astromer-generated embeddings. Our results demonstrate that Astromer 2 significantly outperforms Astromer 1 across all evaluated scenarios, including limited datasets of 20, 100, and 500 samples per class. The use of weighted per-sample embeddings, which integrate intermediate representations from Astromer's attention blocks, is particularly impactful. Notably, Astromer 2 achieves a 15% improvement in F1 score on the ATLAS dataset compared to prior models, showcasing robust generalization to new datasets. This enhanced performance, especially with minimal labeled data, underscores the potential of Astromer 2 for more efficient and scalable light curve analysis.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Position: Bridge the Gaps between Machine Unlearning and AI Regulation</title>
<link>https://arxiv.org/abs/2502.12430</link>
<guid>https://arxiv.org/abs/2502.12430</guid>
<content:encoded><![CDATA[
arXiv:2502.12430v2 Announce Type: replace-cross 
Abstract: The ''right to be forgotten'' and the data privacy laws that encode it have motivated machine unlearning since its earliest days. Now, some argue that an inbound wave of artificial intelligence regulations -- like the European Union's Artificial Intelligence Act (AIA) -- may offer important new use cases for machine unlearning. However, this position paper argues, this opportunity will only be realized if researchers proactively bridge the (sometimes sizable) gaps between machine unlearning's state of the art and its potential applications to AI regulation. To demonstrate this point, we use the AIA as our primary case study. Specifically, we deliver a ``state of the union'' as regards machine unlearning's current potential (or, in many cases, lack thereof) for aiding compliance with various provisions of the AIA. This starts with a precise cataloging of the potential applications of machine unlearning to AIA compliance. For each, we flag the technical gaps that exist between the potential application and the state of the art of machine unlearning. Finally, we end with a call to action: for machine learning researchers to solve the open technical questions that could unlock machine unlearning's potential to assist compliance with the AIA -- and other AI regulations like it.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ExpertLens: Activation steering features are highly interpretable</title>
<link>https://arxiv.org/abs/2502.15090</link>
<guid>https://arxiv.org/abs/2502.15090</guid>
<content:encoded><![CDATA[
arXiv:2502.15090v4 Announce Type: replace-cross 
Abstract: Activation steering methods in large language models (LLMs) have emerged as an effective way to perform targeted updates to enhance generated language without requiring large amounts of adaptation data. We ask whether the features discovered by activation steering methods are interpretable. We identify neurons responsible for specific concepts (e.g., ``cat'') using the ``finding experts'' method from research on activation steering and show that the ExpertLens, i.e., inspection of these neurons provides insights about model representation. We find that ExpertLens representations are stable across models and datasets and closely align with human representations inferred from behavioral data, matching inter-human alignment levels. ExpertLens significantly outperforms the alignment captured by word/sentence embeddings. By reconstructing human concept organization through ExpertLens, we show that it enables a granular view of LLM concept representation. Our findings suggest that ExpertLens is a flexible and lightweight approach for capturing and analyzing model representations.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RoMA: Scaling up Mamba-based Foundation Models for Remote Sensing</title>
<link>https://arxiv.org/abs/2503.10392</link>
<guid>https://arxiv.org/abs/2503.10392</guid>
<content:encoded><![CDATA[
arXiv:2503.10392v2 Announce Type: replace-cross 
Abstract: Recent advances in self-supervised learning for Vision Transformers (ViTs) have fueled breakthroughs in remote sensing (RS) foundation models. However, the quadratic complexity of self-attention poses a significant barrier to scalability, particularly for large models and high-resolution images. While the linear-complexity Mamba architecture offers a promising alternative, existing RS applications of Mamba remain limited to supervised tasks on small, domain-specific datasets. To address these challenges, we propose RoMA, a framework that enables scalable self-supervised pretraining of Mamba-based RS foundation models using large-scale, diverse, unlabeled data. RoMA enhances scalability for high-resolution images through a tailored auto-regressive learning strategy, incorporating two key innovations: 1) a rotation-aware pretraining mechanism combining adaptive cropping with angular embeddings to handle sparsely distributed objects with arbitrary orientations, and 2) multi-scale token prediction objectives that address the extreme variations in object scales inherent to RS imagery. Systematic empirical studies validate that Mamba adheres to RS data and parameter scaling laws, with performance scaling reliably as model and data size increase. Furthermore, experiments across scene classification, object detection, and semantic segmentation tasks demonstrate that RoMA-pretrained Mamba models consistently outperform ViT-based counterparts in both accuracy and computational efficiency. The source code and pretrained models will be released at https://github.com/MiliLab/RoMA.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unseen from Seen: Rewriting Observation-Instruction Using Foundation Models for Augmenting Vision-Language Navigation</title>
<link>https://arxiv.org/abs/2503.18065</link>
<guid>https://arxiv.org/abs/2503.18065</guid>
<content:encoded><![CDATA[
arXiv:2503.18065v3 Announce Type: replace-cross 
Abstract: Data scarcity is a long-standing challenge in the Vision-Language Navigation (VLN) field, which extremely hinders the generalization of agents to unseen environments. Previous works primarily rely on additional simulator data or web-collected images/videos to improve the generalization. However, the simulator environments still face limited diversity, and the web-collected data often requires extensive labor to remove the noise. In this paper, we propose a Rewriting-driven AugMentation (RAM) paradigm for VLN, which directly creates the unseen observation-instruction pairs via rewriting human-annotated training data. Benefiting from our rewriting mechanism, new observation-instruction pairs can be obtained in both simulator-free and labor-saving manners to promote generalization. Specifically, we first introduce Object-Enriched Observation Rewriting, where we combine Vision-Language Models (VLMs) and Large Language Models (LLMs) to derive rewritten object-enriched scene descriptions, enabling observation synthesis with diverse objects and spatial layouts via Text-to-Image Generation Models (T2IMs). Then, we propose Observation-Contrast Instruction Rewriting, which generates observation-aligned rewritten instructions by requiring LLMs to reason the difference between original and new observations. We further develop a mixing-then-focusing training strategy with a random observation cropping scheme, effectively enhancing data distribution diversity while suppressing augmentation data noise during training. Experiments on both the discrete environments (R2R, REVERIE, and R4R datasets) and continuous environments (R2R-CE dataset) show the superior performance and impressive generalization ability of our method. Code is available at https://github.com/SaDil13/VLN-RAM.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Models are Unreliable for Cyber Threat Intelligence</title>
<link>https://arxiv.org/abs/2503.23175</link>
<guid>https://arxiv.org/abs/2503.23175</guid>
<content:encoded><![CDATA[
arXiv:2503.23175v3 Announce Type: replace-cross 
Abstract: Several recent works have argued that Large Language Models (LLMs) can be used to tame the data deluge in the cybersecurity field, by improving the automation of Cyber Threat Intelligence (CTI) tasks. This work presents an evaluation methodology that other than allowing to test LLMs on CTI tasks when using zero-shot learning, few-shot learning and fine-tuning, also allows to quantify their consistency and their confidence level. We run experiments with three state-of-the-art LLMs and a dataset of 350 threat intelligence reports and present new evidence of potential security risks in relying on LLMs for CTI. We show how LLMs cannot guarantee sufficient performance on real-size reports while also being inconsistent and overconfident. Few-shot learning and fine-tuning only partially improve the results, thus posing doubts about the possibility of using LLMs for CTI scenarios, where labelled datasets are lacking and where confidence is a fundamental factor.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mixture of Routers</title>
<link>https://arxiv.org/abs/2503.23362</link>
<guid>https://arxiv.org/abs/2503.23362</guid>
<content:encoded><![CDATA[
arXiv:2503.23362v3 Announce Type: replace-cross 
Abstract: Supervised fine-tuning (SFT) is a milestone in aligning large language models with human instructions and adapting them to downstream tasks. In particular, Low-Rank Adaptation (LoRA) has gained widespread attention due to its parameter efficiency. However, its impact on improving the performance of large models remains limited. Recent studies suggest that combining LoRA with Mixture-of-Experts (MoE) can significantly enhance fine-tuning performance. MoE adapts to the diversity and complexity of datasets by dynamically selecting the most suitable experts, thereby improving task accuracy and efficiency. Despite impressive results, recent studies reveal issues in the MoE routing mechanism, such as incorrect assignments and imbalanced expert allocation. Inspired by the principles of Redundancy and Fault Tolerance Theory. We innovatively integrate the concept of Mixture of Experts into the routing mechanism and propose an efficient fine-tuning method called Mixture of Routers (MoR). It employs multiple sub-routers for joint selection and uses a learnable main router to determine the weights of the sub-routers. The results show that MoR outperforms baseline models on most tasks, achieving an average performance improvement of 1%. MoR can serve as a plug-and-play, parameter-efficient fine-tuning method suitable for a wide range of applications. Our code is available here: https://anonymous.4open.science/r/MoR-DFC6.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Noise-based reward-modulated learning</title>
<link>https://arxiv.org/abs/2503.23972</link>
<guid>https://arxiv.org/abs/2503.23972</guid>
<content:encoded><![CDATA[
arXiv:2503.23972v3 Announce Type: replace-cross 
Abstract: The pursuit of energy-efficient and adaptive artificial intelligence (AI) has positioned neuromorphic computing as a promising alternative to conventional computing. However, achieving learning on these platforms requires techniques that prioritize local information while enabling effective credit assignment. Here, we propose noise-based reward-modulated learning (NRL), a novel synaptic plasticity rule that mathematically unifies reinforcement learning and gradient-based optimization with biologically-inspired local updates. NRL addresses the computational bottleneck of exact gradients by approximating them through stochastic neural activity, transforming the inherent noise of biological and neuromorphic substrates into a functional resource. Drawing inspiration from biological learning, our method uses reward prediction errors as its optimization target to generate increasingly advantageous behavior, and eligibility traces to facilitate retrospective credit assignment. Experimental validation on reinforcement tasks, featuring immediate and delayed rewards, shows that NRL achieves performance comparable to baselines optimized using backpropagation, although with slower convergence, while showing significantly superior performance and scalability in multi-layer networks compared to reward-modulated Hebbian learning (RMHL), the most prominent similar approach. While tested on simple architectures, the results highlight the potential of noise-driven, brain-inspired learning for low-power adaptive systems, particularly in computing substrates with locality constraints. NRL offers a theoretically grounded paradigm well-suited for the event-driven characteristics of next-generation neuromorphic AI.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoPDL: Automatic Prompt Optimization for LLM Agents</title>
<link>https://arxiv.org/abs/2504.04365</link>
<guid>https://arxiv.org/abs/2504.04365</guid>
<content:encoded><![CDATA[
arXiv:2504.04365v5 Announce Type: replace-cross 
Abstract: The performance of large language models (LLMs) depends on how they are prompted, with choices spanning both the high-level prompting pattern (e.g., Zero-Shot, CoT, ReAct, ReWOO) and the specific prompt content (instructions and few-shot demonstrations). Manually tuning this combination is tedious, error-prone, and specific to a given LLM and task. Therefore, this paper proposes AutoPDL, an automated approach to discovering good LLM agent configurations. Our approach frames this as a structured AutoML problem over a combinatorial space of agentic and non-agentic prompting patterns and demonstrations, using successive halving to efficiently navigate this space. We introduce a library implementing common prompting patterns using the PDL prompt programming language. AutoPDL solutions are human-readable, editable, and executable PDL programs that use this library. This approach also enables source-to-source optimization, allowing human-in-the-loop refinement and reuse. Evaluations across three tasks and seven LLMs (ranging from 3B to 70B parameters) show consistent accuracy gains ($9.21\pm15.46$ percentage points), up to 67.5pp, and reveal that selected prompting strategies vary across models and tasks.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion Generative Recommendation with Continuous Tokens</title>
<link>https://arxiv.org/abs/2504.12007</link>
<guid>https://arxiv.org/abs/2504.12007</guid>
<content:encoded><![CDATA[
arXiv:2504.12007v3 Announce Type: replace-cross 
Abstract: Recent advances in generative artificial intelligence, particularly large language models (LLMs), have opened new opportunities for enhancing recommender systems (RecSys). Most existing LLM-based RecSys approaches operate in a discrete space, using vector-quantized tokenizers to align with the inherent discrete nature of language models. However, these quantization methods often result in lossy tokenization and suboptimal learning, primarily due to inaccurate gradient propagation caused by the non-differentiable argmin operation in standard vector quantization. Inspired by the emerging trend of embracing continuous tokens in language models, we propose ContRec, a novel framework that seamlessly integrates continuous tokens into LLM-based RecSys. Specifically, ContRec consists of two key modules: a sigma-VAE Tokenizer, which encodes users/items with continuous tokens; and a Dispersive Diffusion module, which captures implicit user preference. The tokenizer is trained with a continuous Variational Auto-Encoder (VAE) objective, where three effective techniques are adopted to avoid representation collapse. By conditioning on the previously generated tokens of the LLM backbone during user modeling, the Dispersive Diffusion module performs a conditional diffusion process with a novel Dispersive Loss, enabling high-quality user preference generation through next-token diffusion. Finally, ContRec leverages both the textual reasoning output from the LLM and the latent representations produced by the diffusion model for Top-K item retrieval, thereby delivering comprehensive recommendation results. Extensive experiments on four datasets demonstrate that ContRec consistently outperforms both traditional and SOTA LLM-based recommender systems. Our results highlight the potential of continuous tokenization and generative modeling for advancing the next generation of recommender systems.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dense Backpropagation Improves Training for Sparse Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2504.12463</link>
<guid>https://arxiv.org/abs/2504.12463</guid>
<content:encoded><![CDATA[
arXiv:2504.12463v3 Announce Type: replace-cross 
Abstract: Mixture of Experts (MoE) pretraining is more scalable than dense Transformer pretraining, because MoEs learn to route inputs to a sparse set of their feedforward parameters. However, this means that MoEs only receive a sparse backward update, leading to training instability and suboptimal performance. We present a lightweight approximation method that gives the MoE router a dense gradient update while continuing to sparsely activate its parameters. Our method, which we refer to as Default MoE, substitutes missing expert activations with default outputs consisting of an exponential moving average of expert outputs previously seen over the course of training. This allows the router to receive signals from every expert for each token, leading to significant improvements in training performance. Our Default MoE outperforms standard TopK routing in a variety of settings without requiring significant computational overhead. Code: https://github.com/vatsal0/default-moe.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DMind Benchmark: Toward a Holistic Assessment of LLM Capabilities across the Web3 Domain</title>
<link>https://arxiv.org/abs/2504.16116</link>
<guid>https://arxiv.org/abs/2504.16116</guid>
<content:encoded><![CDATA[
arXiv:2504.16116v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have achieved impressive performance in diverse natural language processing tasks, but specialized domains such as Web3 present new challenges and require more tailored evaluation. Despite the significant user base and capital flows in Web3, encompassing smart contracts, decentralized finance (DeFi), non-fungible tokens (NFTs), decentralized autonomous organizations (DAOs), on-chain governance, and novel token-economics, no comprehensive benchmark has systematically assessed LLM performance in this domain. To address this gap, we introduce the DMind Benchmark, a holistic Web3-oriented evaluation suite covering nine critical subfields: fundamental blockchain concepts, blockchain infrastructure, smart contract, DeFi mechanisms, DAOs, NFTs, token economics, meme concept, and security vulnerabilities. Beyond multiple-choice questions, DMind Benchmark features domain-specific tasks such as contract debugging and on-chain numeric reasoning, mirroring real-world scenarios. We evaluated 26 models, including ChatGPT, Claude, DeepSeek, Gemini, Grok, and Qwen, uncovering notable performance gaps in specialized areas like token economics and security-critical contract analysis. While some models excel in blockchain infrastructure tasks, advanced subfields remain challenging. Our benchmark dataset and evaluation pipeline are open-sourced on https://huggingface.co/datasets/DMindAI/DMind_Benchmark, reaching number one in Hugging Face's trending dataset charts within a week of release.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Surrogate modeling of Cellular-Potts Agent-Based Models as a segmentation task using the U-Net neural network architecture</title>
<link>https://arxiv.org/abs/2505.00316</link>
<guid>https://arxiv.org/abs/2505.00316</guid>
<content:encoded><![CDATA[
arXiv:2505.00316v4 Announce Type: replace-cross 
Abstract: The Cellular-Potts model is a powerful and ubiquitous framework for developing computational models for simulating complex multicellular biological systems. Cellular-Potts models (CPMs) are often computationally expensive due to the explicit modeling of interactions among large numbers of individual model agents and diffusive fields described by partial differential equations (PDEs). In this work, we develop a convolutional neural network (CNN) surrogate model using a U-Net architecture that accounts for periodic boundary conditions. We use this model to accelerate the evaluation of a mechanistic CPM previously used to investigate in vitro vasculogenesis. The surrogate model was trained to predict 100 computational steps ahead (Monte-Carlo steps, MCS), accelerating simulation evaluations by a factor of 590 times compared to CPM code execution. Over multiple recursive evaluations, our model effectively captures the emergent behaviors demonstrated by the original Cellular-Potts model of such as vessel sprouting, extension and anastomosis, and contraction of vascular lacunae. This approach demonstrates the potential for deep learning to serve as efficient surrogate models for CPM simulations, enabling faster evaluation of computationally expensive CPM of biological processes at greater spatial and temporal scales.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Option-aware Temporally Abstracted Value for Offline Goal-Conditioned Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.12737</link>
<guid>https://arxiv.org/abs/2505.12737</guid>
<content:encoded><![CDATA[
arXiv:2505.12737v2 Announce Type: replace-cross 
Abstract: Offline goal-conditioned reinforcement learning (GCRL) offers a practical learning paradigm in which goal-reaching policies are trained from abundant state-action trajectory datasets without additional environment interaction. However, offline GCRL still struggles with long-horizon tasks, even with recent advances that employ hierarchical policy structures, such as HIQL. Identifying the root cause of this challenge, we observe the following insight. Firstly, performance bottlenecks mainly stem from the high-level policy's inability to generate appropriate subgoals. Secondly, when learning the high-level policy in the long-horizon regime, the sign of the advantage estimate frequently becomes incorrect. Thus, we argue that improving the value function to produce a clear advantage estimate for learning the high-level policy is essential. In this paper, we propose a simple yet effective solution: Option-aware Temporally Abstracted value learning, dubbed OTA, which incorporates temporal abstraction into the temporal-difference learning process. By modifying the value update to be option-aware, our approach contracts the effective horizon length, enabling better advantage estimates even in long-horizon regimes. We experimentally show that the high-level policy learned using the OTA value function achieves strong performance on complex tasks from OGBench, a recently proposed offline GCRL benchmark, including maze navigation and visual robotic manipulation environments.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contrastive Consolidation of Top-Down Modulations Achieves Sparsely Supervised Continual Learning</title>
<link>https://arxiv.org/abs/2505.14125</link>
<guid>https://arxiv.org/abs/2505.14125</guid>
<content:encoded><![CDATA[
arXiv:2505.14125v2 Announce Type: replace-cross 
Abstract: Biological brains learn continually from a stream of unlabeled data, while integrating specialized information from sparsely labeled examples without compromising their ability to generalize. Meanwhile, machine learning methods are susceptible to catastrophic forgetting in this natural learning setting, as supervised specialist fine-tuning degrades performance on the original task. We introduce task-modulated contrastive learning (TMCL), which takes inspiration from the biophysical machinery in the neocortex, using predictive coding principles to integrate top-down information continually and without supervision. We follow the idea that these principles build a view-invariant representation space, and that this can be implemented using a contrastive loss. Then, whenever labeled samples of a new class occur, new affine modulations are learned that improve separation of the new class from all others, without affecting feedforward weights. By co-opting the view-invariance learning mechanism, we then train feedforward weights to match the unmodulated representation of a data sample to its modulated counterparts. This introduces modulation invariance into the representation space, and, by also using past modulations, stabilizes it. Our experiments show improvements in both class-incremental and transfer learning over state-of-the-art unsupervised approaches, as well as over comparable supervised approaches, using as few as 1% of available labels. Taken together, our work suggests that top-down modulations play a crucial role in balancing stability and plasticity.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Imagine Beyond! Distributionally Robust Auto-Encoding for State Space Coverage in Online Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.17830</link>
<guid>https://arxiv.org/abs/2505.17830</guid>
<content:encoded><![CDATA[
arXiv:2505.17830v3 Announce Type: replace-cross 
Abstract: Goal-Conditioned Reinforcement Learning (GCRL) enables agents to autonomously acquire diverse behaviors, but faces major challenges in visual environments due to high-dimensional, semantically sparse observations. In the online setting, where agents learn representations while exploring, the latent space evolves with the agent's policy, to capture newly discovered areas of the environment. However, without incentivization to maximize state coverage in the representation, classical approaches based on auto-encoders may converge to latent spaces that over-represent a restricted set of states frequently visited by the agent. This is exacerbated in an intrinsic motivation setting, where the agent uses the distribution encoded in the latent space to sample the goals it learns to master. To address this issue, we propose to progressively enforce distributional shifts towards a uniform distribution over the full state space, to ensure a full coverage of skills that can be learned in the environment. We introduce DRAG (Distributionally Robust Auto-Encoding for GCRL), a method that combines the $\beta$-VAE framework with Distributionally Robust Optimization. DRAG leverages an adversarial neural weighter of training states of the VAE, to account for the mismatch between the current data distribution and unseen parts of the environment. This allows the agent to construct semantically meaningful latent spaces beyond its immediate experience. Our approach improves state space coverage and downstream control performance on hard exploration environments such as mazes and robotic control involving walls to bypass, without pre-training nor prior environment knowledge.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NMCSE: Noise-Robust Multi-Modal Coupling Signal Estimation Method via Optimal Transport for Cardiovascular Disease Detection</title>
<link>https://arxiv.org/abs/2505.18174</link>
<guid>https://arxiv.org/abs/2505.18174</guid>
<content:encoded><![CDATA[
arXiv:2505.18174v3 Announce Type: replace-cross 
Abstract: The coupling signal refers to a latent physiological signal that characterizes the transformation from cardiac electrical excitation, captured by the electrocardiogram (ECG), to mechanical contraction, recorded by the phonocardiogram (PCG). By encoding the temporal and functional interplay between electrophysiological and hemodynamic events, it serves as an intrinsic link between modalities and offers a unified representation of cardiac function, with strong potential to enhance multi-modal cardiovascular disease (CVD) detection. However, existing coupling signal estimation methods remain highly vulnerable to noise, particularly in real-world clinical and physiological settings, which undermines their robustness and limits practical value. In this study, we propose Noise-Robust Multi-Modal Coupling Signal Estimation (NMCSE), which reformulates coupling signal estimation as a distribution matching problem solved via optimal transport. By jointly aligning amplitude and timing, NMCSE avoids noise amplification and enables stable signal estimation. When integrated into a Temporal-Spatial Feature Extraction (TSFE) network, the estimated coupling signal effectively enhances multi-modal fusion for more accurate CVD detection. To evaluate robustness under real-world conditions, we design two complementary experiments targeting distinct sources of noise. The first uses the PhysioNet 2016 dataset with simulated hospital noise to assess the resilience of NMCSE to clinical interference. The second leverages the EPHNOGRAM dataset with motion-induced physiological noise to evaluate intra-state estimation stability across activity levels. Experimental results show that NMCSE consistently outperforms existing methods under both clinical and physiological noise, highlighting it as a noise-robust estimation approach that enables reliable multi-modal cardiac detection in real-world conditions.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DreamPRM: Domain-Reweighted Process Reward Model for Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2505.20241</link>
<guid>https://arxiv.org/abs/2505.20241</guid>
<content:encoded><![CDATA[
arXiv:2505.20241v3 Announce Type: replace-cross 
Abstract: Reasoning has substantially improved the performance of large language models (LLMs) on complicated tasks. Central to the current reasoning studies, Process Reward Models (PRMs) offer a fine-grained evaluation of intermediate reasoning steps and guide the reasoning process. However, extending PRMs to multimodal large language models (MLLMs) introduces challenges. Since multimodal reasoning covers a wider range of tasks compared to text-only scenarios, the resulting distribution shift from the training to testing sets is more severe, leading to greater generalization difficulty. Training a reliable multimodal PRM, therefore, demands large and diverse datasets to ensure sufficient coverage. However, current multimodal reasoning datasets suffer from a marked quality imbalance, which degrades PRM performance and highlights the need for an effective data selection strategy. To address the issues, we introduce DreamPRM, a domain-reweighted training framework for multimodal PRMs which employs bi-level optimization. In the lower-level optimization, DreamPRM performs fine-tuning on multiple datasets with domain weights, allowing the PRM to prioritize high-quality reasoning signals and alleviating the impact of dataset quality imbalance. In the upper-level optimization, the PRM is evaluated on a separate meta-learning dataset; this feedback updates the domain weights through an aggregation loss function, thereby improving the generalization capability of trained PRM. Extensive experiments on multiple multimodal reasoning benchmarks covering both mathematical and general reasoning show that test-time scaling with DreamPRM consistently improves the performance of state-of-the-art MLLMs. Further comparisons reveal that DreamPRM's domain-reweighting strategy surpasses other data selection methods and yields higher accuracy gains than existing test-time scaling approaches.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autoencoding Random Forests</title>
<link>https://arxiv.org/abs/2505.21441</link>
<guid>https://arxiv.org/abs/2505.21441</guid>
<content:encoded><![CDATA[
arXiv:2505.21441v3 Announce Type: replace-cross 
Abstract: We propose a principled method for autoencoding with random forests. Our strategy builds on foundational results from nonparametric statistics and spectral graph theory to learn a low-dimensional embedding of the model that optimally represents relationships in the data. We provide exact and approximate solutions to the decoding problem via constrained optimization, split relabeling, and nearest neighbors regression. These methods effectively invert the compression pipeline, establishing a map from the embedding space back to the input space using splits learned by the ensemble's constituent trees. The resulting decoders are universally consistent under common regularity assumptions. The procedure works with supervised or unsupervised models, providing a window into conditional or joint distributions. We demonstrate various applications of this autoencoder, including powerful new tools for visualization, compression, clustering, and denoising. Experiments illustrate the ease and utility of our method in a wide range of settings, including tabular, image, and genomic data.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diagnosing and Addressing Pitfalls in KG-RAG Datasets: Toward More Reliable Benchmarking</title>
<link>https://arxiv.org/abs/2505.23495</link>
<guid>https://arxiv.org/abs/2505.23495</guid>
<content:encoded><![CDATA[
arXiv:2505.23495v4 Announce Type: replace-cross 
Abstract: Knowledge Graph Question Answering (KGQA) systems rely on high-quality benchmarks to evaluate complex multi-hop reasoning. However, despite their widespread use, popular datasets such as WebQSP and CWQ suffer from critical quality issues, including inaccurate or incomplete ground-truth annotations, poorly constructed questions that are ambiguous, trivial, or unanswerable, and outdated or inconsistent knowledge. Through a manual audit of 16 popular KGQA datasets, including WebQSP and CWQ, we find that the average factual correctness rate is only 57 %. To address these issues, we introduce KGQAGen, an LLM-in-the-loop framework that systematically resolves these pitfalls. KGQAGen combines structured knowledge grounding, LLM-guided generation, and symbolic verification to produce challenging and verifiable QA instances. Using KGQAGen, we construct KGQAGen-10k, a ten-thousand scale benchmark grounded in Wikidata, and evaluate a diverse set of KG-RAG models. Experimental results demonstrate that even state-of-the-art systems struggle on this benchmark, highlighting its ability to expose limitations of existing models. Our findings advocate for more rigorous benchmark construction and position KGQAGen as a scalable framework for advancing KGQA evaluation.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Evolutionary Cell Type Matching via Entropy-Minimized Optimal Transport</title>
<link>https://arxiv.org/abs/2505.24759</link>
<guid>https://arxiv.org/abs/2505.24759</guid>
<content:encoded><![CDATA[
arXiv:2505.24759v3 Announce Type: replace-cross 
Abstract: Identifying evolutionary correspondences between cell types across species is a fundamental challenge in comparative genomics and evolutionary biology. Existing approaches often rely on either reference-based matching, which imposes asymmetry by designating one species as the reference, or projection-based matching, which may increase computational complexity and obscure biological interpretability at the cell-type level. Here, we present OT-MESH, an unsupervised computational framework leveraging entropy-regularized optimal transport (OT) to systematically determine cross-species cell type homologies. Our method uniquely integrates the Minimize Entropy of Sinkhorn (MESH) technique to refine the OT plan, transforming diffuse transport matrices into sparse, interpretable correspondences. Through systematic evaluation on synthetic datasets, we demonstrate that OT-MESH achieves near-optimal matching accuracy with computational efficiency, while maintaining remarkable robustness to noise. Compared to other OT-based methods like RefCM, OT-MESH provides speedup while achieving comparable accuracy. Applied to retinal bipolar cells (BCs) and retinal ganglion cells (RGCs) from mouse and macaque, OT-MESH accurately recovers known evolutionary relationships and uncovers novel correspondences, one of which was independently validated experimentally. Thus, our framework offers a principled, scalable, and interpretable solution for evolutionary cell type mapping, facilitating deeper insights into cellular specialization and conservation across species.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Predicting Any Human Trajectory In Context</title>
<link>https://arxiv.org/abs/2506.00871</link>
<guid>https://arxiv.org/abs/2506.00871</guid>
<content:encoded><![CDATA[
arXiv:2506.00871v3 Announce Type: replace-cross 
Abstract: Predicting accurate future trajectories of pedestrians is essential for autonomous systems but remains a challenging task due to the need for adaptability in different environments and domains. A common approach involves collecting scenario-specific data and performing fine-tuning via backpropagation. However, the need to fine-tune for each new scenario is often impractical for deployment on edge devices. To address this challenge, we introduce TrajICL, an In-Context Learning (ICL) framework for pedestrian trajectory prediction that enables adaptation without fine-tuning on the scenario-specific data at inference time without requiring weight updates. We propose a spatio-temporal similarity-based example selection (STES) method that selects relevant examples from previously observed trajectories within the same scene by identifying similar motion patterns at corresponding locations. To further refine this selection, we introduce prediction-guided example selection (PG-ES), which selects examples based on both the past trajectory and the predicted future trajectory, rather than relying solely on the past trajectory. This approach allows the model to account for long-term dynamics when selecting examples. Finally, instead of relying on small real-world datasets with limited scenario diversity, we train our model on a large-scale synthetic dataset to enhance its prediction ability by leveraging in-context examples. Extensive experiments demonstrate that TrajICL achieves remarkable adaptation across both in-domain and cross-domain scenarios, outperforming even fine-tuned approaches across multiple public benchmarks. Project Page: https://fujiry0.github.io/TrajICL-project-page/.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grounded Vision-Language Interpreter for Integrated Task and Motion Planning</title>
<link>https://arxiv.org/abs/2506.03270</link>
<guid>https://arxiv.org/abs/2506.03270</guid>
<content:encoded><![CDATA[
arXiv:2506.03270v2 Announce Type: replace-cross 
Abstract: While recent advances in vision-language models have accelerated the development of language-guided robot planners, their black-box nature often lacks safety guarantees and interpretability crucial for real-world deployment. Conversely, classical symbolic planners offer rigorous safety verification but require significant expert knowledge for setup. To bridge the current gap, this paper proposes ViLaIn-TAMP, a hybrid planning framework for enabling verifiable, interpretable, and autonomous robot behaviors. ViLaIn-TAMP comprises three main components: (1) a Vision-Language Interpreter (ViLaIn) adapted from previous work that converts multimodal inputs into structured problem specifications, (2) a modular Task and Motion Planning (TAMP) system that grounds these specifications in actionable trajectory sequences through symbolic and geometric constraint reasoning, and (3) a corrective planning (CP) module which receives concrete feedback on failed solution attempts and feed them with constraints back to ViLaIn to refine the specification. We design challenging manipulation tasks in a cooking domain and evaluate our framework. Experimental results demonstrate that ViLaIn-TAMP outperforms a VLM-as-a-planner baseline by 18% in mean success rate, and that adding the CP module boosts mean success rate by 32%.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>End-to-End Probabilistic Framework for Learning with Hard Constraints</title>
<link>https://arxiv.org/abs/2506.07003</link>
<guid>https://arxiv.org/abs/2506.07003</guid>
<content:encoded><![CDATA[
arXiv:2506.07003v2 Announce Type: replace-cross 
Abstract: We present ProbHardE2E, a probabilistic forecasting framework that incorporates hard operational/physical constraints, and provides uncertainty quantification. Our methodology uses a novel differentiable probabilistic projection layer (DPPL) that can be combined with a wide range of neural network architectures. DPPL allows the model to learn the system in an end-to-end manner, compared to other approaches where constraints are satisfied either through a post-processing step or at inference. ProbHardE2E optimizes a strictly proper scoring rule, without making any distributional assumptions on the target, which enables it to obtain robust distributional estimates (in contrast to existing approaches that generally optimize likelihood-based objectives, which are heavily biased by their distributional assumptions and model choices); and it can incorporate a range of non-linear constraints (increasing the power of modeling and flexibility). We apply ProbHardE2E in learning partial differential equations with uncertainty estimates and to probabilistic time-series forecasting, showcasing it as a broadly applicable general framework that connects these seemingly disparate domains.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Is Diversity Rewarded in Cooperative Multi-Agent Learning?</title>
<link>https://arxiv.org/abs/2506.09434</link>
<guid>https://arxiv.org/abs/2506.09434</guid>
<content:encoded><![CDATA[
arXiv:2506.09434v3 Announce Type: replace-cross 
Abstract: The success of teams in robotics, nature, and society often depends on the division of labor among diverse specialists; however, a principled explanation for when such diversity surpasses a homogeneous team is still missing. Focusing on multi-agent task allocation problems, we study this question from the perspective of reward design: what kinds of objectives are best suited for heterogeneous teams? We first consider an instantaneous, non-spatial setting where the global reward is built by two generalized aggregation operators: an inner operator that maps the $N$ agents' effort allocations on individual tasks to a task score, and an outer operator that merges the $M$ task scores into the global team reward. We prove that the curvature of these operators determines whether heterogeneity can increase reward, and that for broad reward families this collapses to a simple convexity test. Next, we ask what incentivizes heterogeneity to emerge when embodied, time-extended agents must learn an effort allocation policy. To study heterogeneity in such settings, we use multi-agent reinforcement learning (MARL) as our computational paradigm, and introduce Heterogeneity Gain Parameter Search (HetGPS), a gradient-based algorithm that optimizes the parameter space of underspecified MARL environments to find scenarios where heterogeneity is advantageous. Across different environments, we show that HetGPS rediscovers the reward regimes predicted by our theory to maximize the advantage of heterogeneity, both validating HetGPS and connecting our theoretical insights to reward design in MARL. Together, these results help us understand when behavioral diversity delivers a measurable benefit.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adv-BMT: Bidirectional Motion Transformer for Safety-Critical Traffic Scenario Generation</title>
<link>https://arxiv.org/abs/2506.09485</link>
<guid>https://arxiv.org/abs/2506.09485</guid>
<content:encoded><![CDATA[
arXiv:2506.09485v2 Announce Type: replace-cross 
Abstract: Scenario-based testing is essential for validating the performance of autonomous driving (AD) systems. However, such testing is limited by the scarcity of long-tailed, safety-critical scenarios in existing datasets collected in the real world. To tackle the data issue, we propose the Adv-BMT framework, which augments real-world scenarios with diverse and realistic adversarial traffic interactions. The core component of Adv-BMT is a bidirectional motion transformer (BMT) model to perform inverse traffic motion predictions, which takes agent information in the last time step of the scenario as input, and reconstructs the traffic in the inverse of chronological order until the initial time step. The Adv-BMT framework is a two-staged pipeline: it first conducts adversarial initializations and then inverse motion predictions. Different from previous work, we do not need any collision data for pretraining, and are able to generate realistic and diverse collision interactions. Our experimental results validate the quality of generated collision scenarios by Adv-BMT: training in our augmented dataset would reduce episode collision rates by 20%. Demo and code are available at: https://metadriverse.github.io/adv-bmt/.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ABS: Enforcing Constraint Satisfaction On Generated Sequences Via Automata-Guided Beam Search</title>
<link>https://arxiv.org/abs/2506.09701</link>
<guid>https://arxiv.org/abs/2506.09701</guid>
<content:encoded><![CDATA[
arXiv:2506.09701v2 Announce Type: replace-cross 
Abstract: Sequence generation and prediction form a cornerstone of modern machine learning, with applications spanning natural language processing, program synthesis, and time-series forecasting. These tasks are typically modeled in an autoregressive fashion, where each token is generated conditional on the preceding ones, and beam search is commonly used to balance exploration and fluency during decoding. While deep learning models and Large Language Models (LLMs) excel at capturing statistical patterns in this setting, they remain ill-equipped to guarantee compliance with formal constraints. In this paper, we introduce ABS: a general and model-agnostic inference-time algorithm that guarantees compliance with any constraint that can be compiled into a Deterministic Finite Automaton (DFA), without requiring retraining. ABS leverages the DFA to guide a constrained variant of beam search: at each decoding step, transitions leading to violations are masked, while remaining paths are dynamically re-ranked according to both the model's probabilities and the automaton's acceptance structure. We formally prove that the resulting sequences are guaranteed to satisfy the given constraints, and we empirically demonstrate that ABS also improves output quality. We validate our approach on three distinct tasks: constrained image-stream classification, controlled text generation, and text infilling. In all settings, ABS achieves perfect constraint satisfaction, while outperforming or matching state-of-the-art baselines on standard quality metrics and efficiency.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Not All Clients Are Equal: Collaborative Model Personalization on Heterogeneous Multi-Modal Clients</title>
<link>https://arxiv.org/abs/2506.11024</link>
<guid>https://arxiv.org/abs/2506.11024</guid>
<content:encoded><![CDATA[
arXiv:2506.11024v3 Announce Type: replace-cross 
Abstract: As AI becomes more personal, e.g., Agentic AI, there is an increasing need for personalizing models for various use cases. Personalized federated learning (PFL) enables each client to collaboratively leverage other clients' knowledge for better adaptation to the task of interest, without privacy risks. Despite its potential, existing PFL methods remain confined to rather simplified scenarios where data and models are the same across clients. To move towards realistic scenarios, we propose FedMosaic, a method that jointly addresses data and model heterogeneity with a task-relevance-aware model aggregation strategy to reduce parameter interference, and a dimension-invariant module that enables knowledge sharing across heterogeneous architectures without huge computational cost. To mimic the real-world task diversity, we propose a multi-modal PFL benchmark spanning 40 distinct tasks with distribution shifts over time. The empirical study shows that FedMosaic outperforms the state-of-the-art PFL methods, excelling in both personalization and generalization capabilities under challenging, realistic scenarios.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consistent Sampling and Simulation: Molecular Dynamics with Energy-Based Diffusion Models</title>
<link>https://arxiv.org/abs/2506.17139</link>
<guid>https://arxiv.org/abs/2506.17139</guid>
<content:encoded><![CDATA[
arXiv:2506.17139v2 Announce Type: replace-cross 
Abstract: In recent years, diffusion models trained on equilibrium molecular distributions have proven effective for sampling biomolecules. Beyond direct sampling, the score of such a model can also be used to derive the forces that act on molecular systems. However, while classical diffusion sampling usually recovers the training distribution, the corresponding energy-based interpretation of the learned score is often inconsistent with this distribution, even for low-dimensional toy systems. We trace this inconsistency to inaccuracies of the learned score at very small diffusion timesteps, where the model must capture the correct evolution of the data distribution. In this regime, diffusion models fail to satisfy the Fokker--Planck equation, which governs the evolution of the score. We interpret this deviation as one source of the observed inconsistencies and propose an energy-based diffusion model with a Fokker--Planck-derived regularization term to enforce consistency. We demonstrate our approach by sampling and simulating multiple biomolecular systems, including fast-folding proteins, and by introducing a state-of-the-art transferable Boltzmann emulator for dipeptides that supports simulation and achieves improved consistency and efficient sampling. Our code, model weights, and self-contained JAX and PyTorch notebooks are available at https://github.com/noegroup/ScoreMD.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable and Cost-Efficient de Novo Template-Based Molecular Generation</title>
<link>https://arxiv.org/abs/2506.19865</link>
<guid>https://arxiv.org/abs/2506.19865</guid>
<content:encoded><![CDATA[
arXiv:2506.19865v2 Announce Type: replace-cross 
Abstract: Template-based molecular generation offers a promising avenue for drug design by ensuring generated compounds are synthetically accessible through predefined reaction templates and building blocks. In this work, we tackle three core challenges in template-based GFlowNets: (1) minimizing synthesis cost, (2) scaling to large building block libraries, and (3) effectively utilizing small fragment sets. We propose Recursive Cost Guidance, a backward policy framework that employs auxiliary machine learning models to approximate synthesis cost and viability. This guidance steers generation toward low-cost synthesis pathways, significantly enhancing cost-efficiency, molecular diversity, and quality, especially when paired with an Exploitation Penalty that balances the trade-off between exploration and exploitation. To enhance performance in smaller building block libraries, we develop a Dynamic Library mechanism that reuses intermediate high-reward states to construct full synthesis trees. Our approach establishes state-of-the-art results in template-based molecular generation.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Relational Causal Discovery with Latent Confounders</title>
<link>https://arxiv.org/abs/2507.01700</link>
<guid>https://arxiv.org/abs/2507.01700</guid>
<content:encoded><![CDATA[
arXiv:2507.01700v2 Announce Type: replace-cross 
Abstract: Estimating causal effects from real-world relational data can be challenging when the underlying causal model and potential confounders are unknown. While several causal discovery algorithms exist for learning causal models with latent confounders from data, they assume that the data is independent and identically distributed (i.i.d.) and are not well-suited for learning from relational data. Similarly, existing relational causal discovery algorithms assume causal sufficiency, which is unrealistic for many real-world datasets. To address this gap, we propose RelFCI, a sound and complete causal discovery algorithm for relational data with latent confounders. Our work builds upon the Fast Causal Inference (FCI) and Relational Causal Discovery (RCD) algorithms and it defines new graphical models, necessary to support causal discovery in relational domains. We also establish soundness and completeness guarantees for relational d-separation with latent confounders. We present experimental results demonstrating the effectiveness of RelFCI in identifying the correct causal structure in relational causal models with latent confounders.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Dark Side of LLMs: Agent-based Attacks for Complete Computer Takeover</title>
<link>https://arxiv.org/abs/2507.06850</link>
<guid>https://arxiv.org/abs/2507.06850</guid>
<content:encoded><![CDATA[
arXiv:2507.06850v5 Announce Type: replace-cross 
Abstract: The rapid adoption of Large Language Model (LLM) agents and multi-agent systems enables remarkable capabilities in natural language processing and generation. However, these systems introduce security vulnerabilities that extend beyond traditional content generation to system-level compromises. This paper presents a comprehensive evaluation of the LLMs security used as reasoning engines within autonomous agents, highlighting how they can be exploited as attack vectors capable of achieving computer takeovers. We focus on how different attack surfaces and trust boundaries can be leveraged to orchestrate such takeovers. We demonstrate that adversaries can effectively coerce popular LLMs into autonomously installing and executing malware on victim machines. Our evaluation of 18 state-of-the-art LLMs reveals an alarming scenario: 94.4% of models succumb to Direct Prompt Injection, and 83.3% are vulnerable to the more stealthy and evasive RAG Backdoor Attack. Notably, we tested trust boundaries within multi-agent systems, where LLM agents interact and influence each other, and we revealed that LLMs which successfully resist direct injection or RAG backdoor attacks will execute identical payloads when requested by peer agents. We found that 100.0% of tested LLMs can be compromised through Inter-Agent Trust Exploitation attacks, and that every model exhibits context-dependent security behaviors that create exploitable blind spots.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context-Aware Regularization with Markovian Integration for Attention-Based Nucleotide Analysis</title>
<link>https://arxiv.org/abs/2507.09378</link>
<guid>https://arxiv.org/abs/2507.09378</guid>
<content:encoded><![CDATA[
arXiv:2507.09378v3 Announce Type: replace-cross 
Abstract: Transformers have revolutionized nucleotide sequence analysis, yet capturing long-range dependencies remains challenging. Recent studies show that autoregressive transformers often exhibit Markovian behavior by relying on fixed-length context windows for next-token prediction. However, standard self-attention mechanisms are computationally inefficient for long sequences due to their quadratic complexity and do not explicitly enforce global transition consistency.
  We introduce CARMANIA (Context-Aware Regularization with Markovian Integration for Attention-Based Nucleotide Analysis), a self-supervised pretraining framework that augments next-token (NT) prediction with a transition-matrix (TM) loss. The TM loss aligns predicted token transitions with empirically derived n-gram statistics from each input sequence, encouraging the model to capture higher-order dependencies beyond local context. This integration enables CARMANIA to learn organism-specific sequence structures that reflect both evolutionary constraints and functional organization.
  We evaluate CARMANIA across diverse genomic tasks, including regulatory element prediction, functional gene classification, taxonomic inference, antimicrobial resistance detection, and biosynthetic gene cluster classification. CARMANIA outperforms the previous best long-context model by at least 7 percent, matches state-of-the-art on shorter sequences (exceeding prior results on 20 out of 40 tasks while running approximately 2.5 times faster), and shows particularly strong improvements on enhancer and housekeeping gene classification tasks, including up to a 34 percent absolute gain in Matthews correlation coefficient (MCC) for enhancer prediction. The TM loss boosts accuracy in 33 of 40 tasks, especially where local motifs or regulatory patterns drive prediction.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ARPaCCino: An Agentic-RAG for Policy as Code Compliance</title>
<link>https://arxiv.org/abs/2507.10584</link>
<guid>https://arxiv.org/abs/2507.10584</guid>
<content:encoded><![CDATA[
arXiv:2507.10584v2 Announce Type: replace-cross 
Abstract: Policy as Code (PaC) is a paradigm that encodes security and compliance policies into machine-readable formats, enabling automated enforcement in Infrastructure as Code (IaC) environments. However, its adoption is hindered by the complexity of policy languages and the risk of misconfigurations. In this work, we present ARPaCCino, an agentic system that combines Large Language Models (LLMs), Retrieval-Augmented-Generation (RAG), and tool-based validation to automate the generation and verification of PaC rules. Given natural language descriptions of the desired policies, ARPaCCino generates formal Rego rules, assesses IaC compliance, and iteratively refines the IaC configurations to ensure conformance. Thanks to its modular agentic architecture and integration with external tools and knowledge bases, ARPaCCino supports policy validation across a wide range of technologies, including niche or emerging IaC frameworks. Experimental evaluation involving a Terraform-based case study demonstrates ARPaCCino's effectiveness in generating syntactically and semantically correct policies, identifying non-compliant infrastructures, and applying corrective modifications, even when using smaller, open-weight LLMs. Our results highlight the potential of agentic RAG architectures to enhance the automation, reliability, and accessibility of PaC workflows.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Communicating Plans, Not Percepts: Scalable Multi-Agent Coordination with Embodied World Models</title>
<link>https://arxiv.org/abs/2508.02912</link>
<guid>https://arxiv.org/abs/2508.02912</guid>
<content:encoded><![CDATA[
arXiv:2508.02912v3 Announce Type: replace-cross 
Abstract: Robust coordination is critical for effective decision-making in multi-agent systems, especially under partial observability. A central question in Multi-Agent Reinforcement Learning (MARL) is whether to engineer communication protocols or learn them end-to-end. We investigate this dichotomy using embodied world models. We propose and compare two communication strategies for a cooperative task-allocation problem. The first, Learned Direct Communication (LDC), learns a protocol end-to-end. The second, Intention Communication, uses an engineered inductive bias: a compact, learned world model, the Imagined Trajectory Generation Module (ITGM), which uses the agent's own policy to simulate future states. A Message Generation Network (MGN) then compresses this plan into a message. We evaluate these approaches on goal-directed interaction in a grid world, a canonical abstraction for embodied AI problems, while scaling environmental complexity. Our experiments reveal that while emergent communication is viable in simple settings, the engineered, world model-based approach shows superior performance, sample efficiency, and scalability as complexity increases. These findings advocate for integrating structured, predictive models into MARL agents to enable active, goal-driven coordination.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MetAdv: A Unified and Interactive Adversarial Testing Platform for Autonomous Driving</title>
<link>https://arxiv.org/abs/2508.06534</link>
<guid>https://arxiv.org/abs/2508.06534</guid>
<content:encoded><![CDATA[
arXiv:2508.06534v3 Announce Type: replace-cross 
Abstract: Evaluating and ensuring the adversarial robustness of autonomous driving (AD) systems is a critical and unresolved challenge. This paper introduces MetAdv, a novel adversarial testing platform that enables realistic, dynamic, and interactive evaluation by tightly integrating virtual simulation with physical vehicle feedback. At its core, MetAdv establishes a hybrid virtual-physical sandbox, within which we design a three-layer closed-loop testing environment with dynamic adversarial test evolution. This architecture facilitates end-to-end adversarial evaluation, ranging from high-level unified adversarial generation, through mid-level simulation-based interaction, to low-level execution on physical vehicles. Additionally, MetAdv supports a broad spectrum of AD tasks, algorithmic paradigms (e.g., modular deep learning pipelines, end-to-end learning, vision-language models). It supports flexible 3D vehicle modeling and seamless transitions between simulated and physical environments, with built-in compatibility for commercial platforms such as Apollo and Tesla. A key feature of MetAdv is its human-in-the-loop capability: besides flexible environmental configuration for more customized evaluation, it enables real-time capture of physiological signals and behavioral feedback from drivers, offering new insights into human-machine trust under adversarial conditions. We believe MetAdv can offer a scalable and unified framework for adversarial assessment, paving the way for safer AD.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Street Review: A Participatory AI-Based Framework for Assessing Streetscape Inclusivity</title>
<link>https://arxiv.org/abs/2508.11708</link>
<guid>https://arxiv.org/abs/2508.11708</guid>
<content:encoded><![CDATA[
arXiv:2508.11708v3 Announce Type: replace-cross 
Abstract: Urban centers undergo social, demographic, and cultural changes that shape public street use and require systematic evaluation of public spaces. This study presents Street Review, a mixed-methods approach that combines participatory research with AI-based analysis to assess streetscape inclusivity. In Montr\'eal, Canada, 28 residents participated in semi-directed interviews and image evaluations, supported by the analysis of approximately 45,000 street-view images from Mapillary. The approach produced visual analytics, such as heatmaps, to correlate subjective user ratings with physical attributes like sidewalk, maintenance, greenery, and seating. Findings reveal variations in perceptions of inclusivity and accessibility across demographic groups, demonstrating that incorporating diverse user feedback can enhance machine learning models through careful data-labeling and co-production strategies. The Street Review framework offers a systematic method for urban planners and policy analysts to inform planning, policy development, and management of public streets.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Linear-Time Demonstration Selection for In-Context Learning via Gradient Estimation</title>
<link>https://arxiv.org/abs/2508.19999</link>
<guid>https://arxiv.org/abs/2508.19999</guid>
<content:encoded><![CDATA[
arXiv:2508.19999v2 Announce Type: replace-cross 
Abstract: This paper introduces an algorithm to select demonstration examples for in-context learning of a query set. Given a set of $n$ examples, how can we quickly select $k$ out of $n$ to best serve as the conditioning for downstream inference? This problem has broad applications in prompt tuning and chain-of-thought reasoning. Since model weights remain fixed during in-context learning, previous work has sought to design methods based on the similarity of token embeddings. This work proposes a new approach based on gradients of the output taken in the input embedding space. Our approach estimates model outputs through a first-order approximation using the gradients. Then, we apply this estimation to multiple randomly sampled subsets. Finally, we aggregate the sampled subset outcomes to form an influence score for each demonstration, and select $k$ most relevant examples. This procedure only requires pre-computing model outputs and gradients once, resulting in a linear-time algorithm relative to model and training set sizes. Extensive experiments across various models and datasets validate the efficiency of our approach. We show that the gradient estimation procedure yields approximations of full inference with less than ${1}\%$ error across six datasets. This allows us to scale up subset selection that would otherwise run full inference by up to ${37.7}\times$ on models with up to $34$ billion parameters, and outperform existing selection methods based on input embeddings by ${11}\%$ on average.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Harnessing IoT and Generative AI for Weather-Adaptive Learning in Climate Resilience Education</title>
<link>https://arxiv.org/abs/2508.21666</link>
<guid>https://arxiv.org/abs/2508.21666</guid>
<content:encoded><![CDATA[
arXiv:2508.21666v2 Announce Type: replace-cross 
Abstract: This paper introduces the Future Atmospheric Conditions Training System (FACTS), a novel platform that advances climate resilience education through place-based, adaptive learning experiences. FACTS combines real-time atmospheric data collected by IoT sensors with curated resources from a Knowledge Base to dynamically generate localized learning challenges. Learner responses are analyzed by a Generative AI powered server, which delivers personalized feedback and adaptive support. Results from a user evaluation indicate that participants found the system both easy to use and effective for building knowledge related to climate resilience. These findings suggest that integrating IoT and Generative AI into atmospherically adaptive learning technologies holds significant promise for enhancing educational engagement and fostering climate awareness.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Co-Evolving Complexity: An Adversarial Framework for Automatic MARL Curricula</title>
<link>https://arxiv.org/abs/2509.03771</link>
<guid>https://arxiv.org/abs/2509.03771</guid>
<content:encoded><![CDATA[
arXiv:2509.03771v3 Announce Type: replace-cross 
Abstract: The advancement of general-purpose intelligent agents is intrinsically linked to the environments in which they are trained. While scaling models and datasets has yielded remarkable capabilities, scaling the complexity, diversity, and interactivity of environments remains a crucial bottleneck. Hand-crafted environments are finite and often contain implicit biases, limiting the potential for agents to develop truly generalizable and robust skills. In this work, we propose a paradigm for generating a boundless and adaptive curriculum of challenges by framing the environment generation process as an adversarial game. We introduce a system where a team of cooperative multi-agent defenders learns to survive against a procedurally generative attacker. The attacker agent learns to produce increasingly challenging configurations of enemy units, dynamically creating novel worlds tailored to exploit the defenders' current weaknesses. Concurrently, the defender team learns cooperative strategies to overcome these generated threats. This co-evolutionary dynamic creates a self-scaling environment where complexity arises organically from the adversarial interaction, providing an effectively infinite stream of novel and relevant training data. We demonstrate that with minimal training, this approach leads to the emergence of complex, intelligent behaviors, such as flanking and shielding by the attacker, and focus-fire and spreading by the defenders. Our findings suggest that adversarial co-evolution is a powerful mechanism for automatically scaling environmental complexity, driving agents towards greater robustness and strategic depth.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Physical Basis of Prediction: World Model Formation in Neural Organoids via an LLM-Generated Curriculum</title>
<link>https://arxiv.org/abs/2509.04633</link>
<guid>https://arxiv.org/abs/2509.04633</guid>
<content:encoded><![CDATA[
arXiv:2509.04633v3 Announce Type: replace-cross 
Abstract: The capacity of an embodied agent to understand, predict, and interact with its environment is fundamentally contingent on an internal world model. This paper introduces a novel framework for investigating the formation and adaptation of such world models within a biological substrate: human neural organoids. We present a curriculum of three scalable, closed-loop virtual environments designed to train these biological agents and probe the underlying synaptic mechanisms of learning, such as long-term potentiation (LTP) and long-term depression (LTD). We detail the design of three distinct task environments that demand progressively more sophisticated world models for successful decision-making: (1) a conditional avoidance task for learning static state-action contingencies, (2) a one-dimensional predator-prey scenario for goal-directed interaction, and (3) a replication of the classic Pong game for modeling dynamic, continuous-time systems. For each environment, we formalize the state and action spaces, the sensory encoding and motor decoding mechanisms, and the feedback protocols based on predictable (reward) and unpredictable (punishment) stimulation, which serve to drive model refinement. In a significant methodological advance, we propose a meta-learning approach where a Large Language Model automates the generative design and optimization of experimental protocols, thereby scaling the process of environment and curriculum design. Finally, we outline a multi-modal evaluation strategy that moves beyond task performance to directly measure the physical correlates of the learned world model by quantifying synaptic plasticity at electrophysiological, cellular, and molecular levels. This work bridges the gap between model-based reinforcement learning and computational neuroscience, offering a unique platform for studying embodiment, decision-making, and the physical basis of intelligence.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowRL: Matching Reward Distributions for LLM Reasoning</title>
<link>https://arxiv.org/abs/2509.15207</link>
<guid>https://arxiv.org/abs/2509.15207</guid>
<content:encoded><![CDATA[
arXiv:2509.15207v3 Announce Type: replace-cross 
Abstract: We propose FlowRL: matching the full reward distribution via flow balancing instead of maximizing rewards in large language model (LLM) reinforcement learning (RL). Recent advanced reasoning models adopt reward-maximizing methods (\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, thus reducing diversity. In contrast, we transform scalar rewards into a normalized target distribution using a learnable partition function, and then minimize the reverse KL divergence between the policy and the target distribution. We implement this idea as a flow-balanced optimization method that promotes diverse exploration and generalizable reasoning trajectories. We conduct experiments on math and code reasoning tasks: FlowRL achieves a significant average improvement of $10.0\%$ over GRPO and $5.1\%$ over PPO on math benchmarks, and performs consistently better on code reasoning tasks. These results highlight reward distribution-matching as a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification</title>
<link>https://arxiv.org/abs/2509.15591</link>
<guid>https://arxiv.org/abs/2509.15591</guid>
<content:encoded><![CDATA[
arXiv:2509.15591v2 Announce Type: replace-cross 
Abstract: Generative modeling, representation learning, and classification are three core problems in machine learning (ML), yet their state-of-the-art (SoTA) solutions remain largely disjoint. In this paper, we ask: Can a unified principle address all three? Such unification could simplify ML pipelines and foster greater synergy across tasks. We introduce Latent Zoning Network (LZN) as a step toward this goal. At its core, LZN creates a shared Gaussian latent space that encodes information across all tasks. Each data type (e.g., images, text, labels) is equipped with an encoder that maps samples to disjoint latent zones, and a decoder that maps latents back to data. ML tasks are expressed as compositions of these encoders and decoders: for example, label-conditional image generation uses a label encoder and image decoder; image embedding uses an image encoder; classification uses an image encoder and label decoder. We demonstrate the promise of LZN in three increasingly complex scenarios: (1) LZN can enhance existing models (image generation): When combined with the SoTA Rectified Flow model, LZN improves FID on CIFAR10 from 2.76 to 2.59-without modifying the training objective. (2) LZN can solve tasks independently (representation learning): LZN can implement unsupervised representation learning without auxiliary loss functions, outperforming the seminal MoCo and SimCLR methods by 9.3% and 0.2%, respectively, on downstream linear classification on ImageNet. (3) LZN can solve multiple tasks simultaneously (joint generation and classification): With image and label encoders/decoders, LZN performs both tasks jointly by design, improving FID and achieving SoTA classification accuracy on CIFAR10. The code and trained models are available at https://github.com/microsoft/latent-zoning-networks. The project website is at https://zinanlin.me/blogs/latent_zoning_networks.html.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LAWCAT: Efficient Distillation from Quadratic to Linear Attention with Convolution across Tokens for Long Context Modeling</title>
<link>https://arxiv.org/abs/2509.18467</link>
<guid>https://arxiv.org/abs/2509.18467</guid>
<content:encoded><![CDATA[
arXiv:2509.18467v2 Announce Type: replace-cross 
Abstract: Although transformer architectures have achieved state-of-the-art performance across diverse domains, their quadratic computational complexity with respect to sequence length remains a significant bottleneck, particularly for latency-sensitive long-context applications. While recent linear-complexity alternatives are increasingly powerful, effectively training them from scratch is still resource-intensive. To overcome these limitations, we propose LAWCAT (Linear Attention with Convolution Across Time), a novel linearization framework designed to efficiently transfer the capabilities of pre-trained transformers into a performant linear attention architecture. LAWCAT integrates causal Conv1D layers to enhance local dependency modeling and employs normalized gated linear attention to improve generalization across varying context lengths. Our comprehensive evaluations demonstrate that, distilling Mistral-7B with only 1K-length sequences yields over 90\% passkey retrieval accuracy up to 22K tokens, significantly extending its effective context window. Similarly, Llama3.2-1B LAWCAT variant achieves competitive performance on S-NIAH 1\&2\&3 tasks (1K-8K context length) and BABILong benchmark (QA2\&amp;QA3, 0K-16K context length), requiring less than 0.1\% pre-training tokens compared with pre-training models. Furthermore, LAWCAT exhibits faster prefill speeds than FlashAttention-2 for sequences exceeding 8K tokens. LAWCAT thus provides an efficient pathway to high-performance, long-context linear models suitable for edge deployment, reducing reliance on extensive long-sequence training data and computational resources. Code is released at: https://github.com/zeyuliu1037/LAWCAT
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>End-to-End Crop Row Navigation via LiDAR-Based Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.18608</link>
<guid>https://arxiv.org/abs/2509.18608</guid>
<content:encoded><![CDATA[
arXiv:2509.18608v2 Announce Type: replace-cross 
Abstract: Reliable navigation in under-canopy agricultural environments remains a challenge due to GNSS unreliability, cluttered rows, and variable lighting. To address these limitations, we present an end-to-end learning-based navigation system that maps raw 3D LiDAR data directly to control commands using a deep reinforcement learning policy trained entirely in simulation. Our method includes a voxel-based downsampling strategy that reduces LiDAR input size by 95.83%, enabling efficient policy learning without relying on labeled datasets or manually designed control interfaces. The policy was validated in simulation, achieving a 100% success rate in straight-row plantations and showing a gradual decline in performance as row curvature increased, tested across varying sinusoidal frequencies and amplitudes.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Terrain-Specialized Policies for Adaptive Locomotion in Challenging Environments</title>
<link>https://arxiv.org/abs/2509.20635</link>
<guid>https://arxiv.org/abs/2509.20635</guid>
<content:encoded><![CDATA[
arXiv:2509.20635v2 Announce Type: replace-cross 
Abstract: Legged robots must exhibit robust and agile locomotion across diverse, unstructured terrains, a challenge exacerbated under blind locomotion settings where terrain information is unavailable. This work introduces a hierarchical reinforcement learning framework that leverages terrain-specialized policies and curriculum learning to enhance agility and tracking performance in complex environments. We validated our method on simulation, where our approach outperforms a generalist policy by up to 16% in success rate and achieves lower tracking errors as the velocity target increases, particularly on low-friction and discontinuous terrains, demonstrating superior adaptability and robustness across mixed-terrain scenarios.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variance-Bounded Evaluation of Entity-Centric AI Systems Without Ground Truth: Theory and Measurement</title>
<link>https://arxiv.org/abs/2509.22751</link>
<guid>https://arxiv.org/abs/2509.22751</guid>
<content:encoded><![CDATA[
arXiv:2509.22751v2 Announce Type: replace-cross 
Abstract: Reliable evaluation of AI systems remains a fundamental challenge when ground truth labels are unavailable, particularly for systems generating natural language outputs like AI chat and agent systems. Many of these AI agents and systems focus on entity-centric tasks. In enterprise contexts, organizations deploy AI systems for entity linking, data integration, and information retrieval where verification against gold standards is often infeasible due to proprietary data constraints. Academic deployments face similar challenges when evaluating AI systems on specialized datasets with ambiguous criteria. Conventional evaluation frameworks, rooted in supervised learning paradigms, fail in such scenarios where single correct answers cannot be defined. We introduce VB-Score, a variance-bounded evaluation framework for entity-centric AI systems that operates without ground truth by jointly measuring effectiveness and robustness. Given system inputs, VB-Score enumerates plausible interpretations through constraint relaxation and Monte Carlo sampling, assigning probabilities that reflect their likelihood. It then evaluates system outputs by their expected success across interpretations, penalized by variance to assess robustness of the system. We provide formal theoretical analysis establishing key properties including range, monotonicity, and stability along with concentration bounds for Monte Carlo estimation. Through case studies on AI systems with ambiguous inputs, we demonstrate that VB-Score reveals robustness differences hidden by conventional evaluation frameworks, offering a principled measurement framework for assessing AI system reliability in label-scarce domains.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting Multivariate Time Series Forecasting with Missing Values</title>
<link>https://arxiv.org/abs/2509.23494</link>
<guid>https://arxiv.org/abs/2509.23494</guid>
<content:encoded><![CDATA[
arXiv:2509.23494v2 Announce Type: replace-cross 
Abstract: Missing values are common in real-world time series, and multivariate time series forecasting with missing values (MTSF-M) has become a crucial area of research for ensuring reliable predictions. To address the challenge of missing data, current approaches have developed an imputation-then-prediction framework that uses imputation modules to fill in missing values, followed by forecasting on the imputed data. However, this framework overlooks a critical issue: there is no ground truth for the missing values, making the imputation process susceptible to errors that can degrade prediction accuracy. In this paper, we conduct a systematic empirical study and reveal that imputation without direct supervision can corrupt the underlying data distribution and actively degrade prediction accuracy. To address this, we propose a paradigm shift that moves away from imputation and directly predicts from the partially observed time series. We introduce Consistency-Regularized Information Bottleneck (CRIB), a novel framework built on the Information Bottleneck principle. CRIB combines a unified-variate attention mechanism with a consistency regularization scheme to learn robust representations that filter out noise introduced by missing values while preserving essential predictive signals. Comprehensive experiments on four real-world datasets demonstrate the effectiveness of CRIB, which predicts accurately even under high missing rates. Our code is available in https://github.com/Muyiiiii/CRIB.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Effective Are Time-Series Models for Precipitation Nowcasting? A Comprehensive Benchmark for GNSS-based Precipitation Nowcasting</title>
<link>https://arxiv.org/abs/2509.25263</link>
<guid>https://arxiv.org/abs/2509.25263</guid>
<content:encoded><![CDATA[
arXiv:2509.25263v3 Announce Type: replace-cross 
Abstract: Precipitation Nowcasting, which aims to predict precipitation within the next 0 to 6 hours, is critical for disaster mitigation and real-time response planning. However, most time series forecasting benchmarks in meteorology are evaluated on variables with strong periodicity, such as temperature and humidity, which fail to reflect model capabilities in more complex and practically meteorology scenarios like precipitation nowcasting. To address this gap, we propose RainfallBench, a benchmark designed for precipitation nowcasting, a highly challenging and practically relevant task characterized by zero inflation, temporal decay, and non-stationarity, focusing on predicting precipitation within the next 0 to 6 hours. The dataset is derived from five years of meteorological observations, recorded at hourly intervals across six essential variables, and collected from more than 140 Global Navigation Satellite System (GNSS) stations globally. In particular, it incorporates precipitable water vapor (PWV), a crucial indicator of rainfall that is absent in other datasets. We further design specialized evaluation protocols to assess model performance on key meteorological challenges, including multi-scale prediction, multi-resolution forecasting, and extreme rainfall events, benchmarking 17 state-of-the-art models across six major architectures on RainfallBench. Additionally, to address the zero-inflation and temporal decay issues overlooked by existing models, we introduce Bi-Focus Precipitation Forecaster (BFPF), a plug-and-play module that incorporates domain-specific priors to enhance rainfall time series forecasting. Statistical analysis and ablation studies validate the comprehensiveness of our dataset as well as the superiority of our methodology.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Constraint Satisfaction Approaches to Wordle: Novel Heuristics and Cross-Lexicon Validation</title>
<link>https://arxiv.org/abs/2510.02855</link>
<guid>https://arxiv.org/abs/2510.02855</guid>
<content:encoded><![CDATA[
arXiv:2510.02855v3 Announce Type: replace-cross 
Abstract: Wordle presents an algorithmically rich testbed for constraint satisfaction problem (CSP) solving. While existing solvers rely on information-theoretic entropy maximization or frequency-based heuristics without formal constraint treatment, we present the first comprehensive CSP formulation of Wordle with novel constraint-aware solving strategies. We introduce CSP-Aware Entropy, computing information gain after constraint propagation rather than on raw candidate sets, and a Probabilistic CSP framework integrating Bayesian word-frequency priors with logical constraints. Through evaluation on 2,315 English words, CSP-Aware Entropy achieves 3.54 average guesses with 99.9% success rate, a statistically significant 1.7% improvement over Forward Checking (t=-4.82, p<0.001, Cohen's d=0.07) with 46% faster runtime (12.9ms versus 23.7ms per guess). Under 10% noise, CSP-aware approaches maintain 5.3 percentage point advantages (29.0% versus 23.7%, p=0.041), while Probabilistic CSP achieves 100% success across all noise levels (0-20%) through constraint recovery mechanisms. Cross-lexicon validation on 500 Spanish words demonstrates 88% success with zero language-specific tuning, validating that core CSP principles transfer across languages despite an 11.2 percentage point gap from linguistic differences (p<0.001, Fisher's exact test). Our open-source implementation with 34 unit tests achieving 91% code coverage provides reproducible infrastructure for CSP research. The combination of formal CSP treatment, constraint-aware heuristics, probabilistic-logical integration, robustness analysis, and cross-lexicon validation establishes new performance benchmarks demonstrating that principled constraint satisfaction techniques outperform classical information-theoretic and learning-based approaches for structured puzzle-solving domains.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Latent Variable Causal Discovery: Combining Score Search and Targeted Testing</title>
<link>https://arxiv.org/abs/2510.04263</link>
<guid>https://arxiv.org/abs/2510.04263</guid>
<content:encoded><![CDATA[
arXiv:2510.04263v3 Announce Type: replace-cross 
Abstract: Learning causal structure from observational data is especially challenging when latent variables or selection bias are present. The Fast Causal Inference (FCI) algorithm addresses this setting but performs exhaustive conditional independence tests across many subsets, often leading to spurious independences, missing or extra edges, and unreliable orientations. We present a family of score-guided mixed-strategy causal search algorithms that extend this framework. First, we introduce BOSS-FCI and GRaSP-FCI, variants of GFCI (Greedy Fast Causal Inference) that substitute BOSS (Best Order Score Search) or GRaSP (Greedy Relaxations of Sparsest Permutation) for FGES (Fast Greedy Equivalence Search), preserving correctness while trading off scalability and conservativeness. Second, we develop FCI Targeted-Testing (FCIT), a novel hybrid method that replaces exhaustive testing with targeted, score-informed tests guided by BOSS. FCIT guarantees well-formed PAGs and achieves higher precision and efficiency across sample sizes. Finally, we propose a lightweight heuristic, LV-Dumb (Latent Variable "Dumb"), which returns the PAG of the BOSS DAG (Directed Acyclic Graph). Though not strictly sound for latent confounding, LV-Dumb often matches FCIT's accuracy while running substantially faster. Simulations and real-data analyses show that BOSS-FCI and GRaSP-FCI provide robust baselines, FCIT yields the best balance of precision and reliability, and LV-Dumb offers a fast, near-equivalent alternative. Together, these methods demonstrate that targeted and score-guided strategies can dramatically improve the efficiency and correctness of latent-variable causal discovery.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Causal Discovery from Recursive Nonlinear Data via Truncated Basis Function Scores and Tests</title>
<link>https://arxiv.org/abs/2510.04276</link>
<guid>https://arxiv.org/abs/2510.04276</guid>
<content:encoded><![CDATA[
arXiv:2510.04276v2 Announce Type: replace-cross 
Abstract: Learning graphical conditional independence structures from nonlinear, continuous or mixed data is a central challenge in machine learning and the sciences, and many existing methods struggle to scale to thousands of samples or hundreds of variables. We introduce two basis-expansion tools for scalable causal discovery. First, the Basis Function BIC (BF-BIC) score uses truncated additive expansions to approximate nonlinear dependencies. BF-BIC is theoretically consistent under additive models and extends to post-nonlinear (PNL) models via an invertible reparameterization. It remains robust under moderate interactions and supports mixed data through a degenerate-Gaussian embedding for discrete variables. In simulations with fully nonlinear neural causal models (NCMs), BF-BIC outperforms kernel- and constraint-based methods (e.g., KCI, RFCI) in both accuracy and runtime. Second, the Basis Function Likelihood Ratio Test (BF-LRT) provides an approximate conditional independence test that is substantially faster than kernel tests while retaining competitive accuracy. Extensive simulations and a real-data application to Canadian wildfire risk show that, when integrated into hybrid searches, BF-based methods enable interpretable and scalable causal discovery. Implementations are available in Python, R, and Java.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AWARE, Beyond Sentence Boundaries: A Contextual Transformer Framework for Identifying Cultural Capital in STEM Narratives</title>
<link>https://arxiv.org/abs/2510.04983</link>
<guid>https://arxiv.org/abs/2510.04983</guid>
<content:encoded><![CDATA[
arXiv:2510.04983v3 Announce Type: replace-cross 
Abstract: Identifying cultural capital (CC) themes in student reflections can offer valuable insights that help foster equitable learning environments in classrooms. However, themes such as aspirational goals or family support are often woven into narratives, rather than appearing as direct keywords. This makes them difficult to detect for standard NLP models that process sentences in isolation. The core challenge stems from a lack of awareness, as standard models are pre-trained on general corpora, leaving them blind to the domain-specific language and narrative context inherent to the data. To address this, we introduce AWARE, a framework that systematically attempts to improve a transformer model's awareness for this nuanced task. AWARE has three core components: 1) Domain Awareness, adapting the model's vocabulary to the linguistic style of student reflections; 2) Context Awareness, generating sentence embeddings that are aware of the full essay context; and 3) Class Overlap Awareness, employing a multi-label strategy to recognize the coexistence of themes in a single sentence. Our results show that by making the model explicitly aware of the properties of the input, AWARE outperforms a strong baseline by 2.1 percentage points in Macro-F1 and shows considerable improvements across all themes. This work provides a robust and generalizable methodology for any text classification task in which meaning depends on the context of the narrative.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting Long-context Modeling from Context Denoising Perspective</title>
<link>https://arxiv.org/abs/2510.05862</link>
<guid>https://arxiv.org/abs/2510.05862</guid>
<content:encoded><![CDATA[
arXiv:2510.05862v2 Announce Type: replace-cross 
Abstract: Long-context models (LCMs) have demonstrated great potential in processing long sequences, facilitating many real-world applications. The success of LCMs can be attributed to their ability to locate implicit critical information within the context for further prediction. However, recent research reveals that LCMs are often susceptible to contextual noise, i.e., irrelevant tokens, that can mislead model attention. In this paper, we conduct a fine-grained analysis of the context noise and propose an effective metric, the Integrated Gradient (IG) score, to detect and quantify the noise information within the context. Our findings reveal that even simple mitigation of detected context noise can substantially boost the model's attention on critical tokens and benefit subsequent predictions. Building on this insight, we propose Context Denoising Training (CDT), a straightforward yet effective training strategy that improves attention on critical tokens while reinforcing their influence on model predictions. Extensive experiments across four tasks, under both context window scaling and long-context alignment settings, demonstrate the superiority of CDT. Notably, when trained with CDT, an open-source 8B model can achieve performance (50.92) comparable to GPT-4o (51.00).
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling</title>
<link>https://arxiv.org/abs/2510.06915</link>
<guid>https://arxiv.org/abs/2510.06915</guid>
<content:encoded><![CDATA[
arXiv:2510.06915v2 Announce Type: replace-cross 
Abstract: Reward model (RM) plays a pivotal role in aligning large language model (LLM) with human preferences. As real-world applications increasingly involve long history trajectories, e.g., LLM agent, it becomes indispensable to evaluate whether a model's responses are not only high-quality but also grounded in and consistent with the provided context. Yet, current RMs remain confined to short-context settings and primarily focus on response-level attributes (e.g., safety or helpfulness), while largely neglecting the critical dimension of long context-response consistency. In this work, we introduce Long-RewardBench, a benchmark specifically designed for long-context RM evaluation, featuring both Pairwise Comparison and Best-of-N tasks. Our preliminary study reveals that even state-of-the-art generative RMs exhibit significant fragility in long-context scenarios, failing to maintain context-aware preference judgments. Motivated by the analysis of failure patterns observed in model outputs, we propose a general multi-stage training strategy that effectively scales arbitrary models into robust Long-context RMs (LongRMs). Experiments show that our approach not only substantially improves performance on long-context evaluation but also preserves strong short-context capability. Notably, our 8B LongRM outperforms much larger 70B-scale baselines and matches the performance of the proprietary Gemini 2.5 Pro model.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniCoD: Enhancing Robot Policy via Unified Continuous and Discrete Representation Learning</title>
<link>https://arxiv.org/abs/2510.10642</link>
<guid>https://arxiv.org/abs/2510.10642</guid>
<content:encoded><![CDATA[
arXiv:2510.10642v2 Announce Type: replace-cross 
Abstract: Building generalist robot policies that can handle diverse tasks in open-ended environments is a central challenge in robotics. To leverage knowledge from large-scale pretraining, prior work (VLA) has typically built generalist policies either on top of vision-language understanding models (VLMs) or generative models. However, both semantic understanding from vision-language pretraining and visual dynamics modeling from visual-generation pretraining are crucial for embodied robots. Recent unified models of generation and understanding have demonstrated strong capabilities in both comprehension and generation through large-scale pretraining. We posit that robotic policy learning can likewise benefit from the combined strengths of understanding, planning, and continuous future representation learning. Building on this insight, we introduce UniCoD, which acquires the ability to dynamically model high-dimensional visual features through pretraining on over 1M internet-scale instructional manipulation videos. Subsequently, UniCoD is fine-tuned on data collected from the robot embodiment, enabling the learning of mappings from predictive representations to action tokens. Extensive experiments show our approach consistently outperforms baseline methods in terms of 9\% and 12\% across simulation environments and real-world out-of-distribution tasks.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hey, wait a minute: on at-issue sensitivity in Language Models</title>
<link>https://arxiv.org/abs/2510.12740</link>
<guid>https://arxiv.org/abs/2510.12740</guid>
<content:encoded><![CDATA[
arXiv:2510.12740v2 Announce Type: replace-cross 
Abstract: Evaluating the naturalness of dialogue in language models (LMs) is not trivial: notions of 'naturalness' vary, and scalable quantitative metrics remain limited. This study leverages the linguistic notion of 'at-issueness' to assess dialogue naturalness and introduces a new method: Divide, Generate, Recombine, and Compare (DGRC). DGRC (i) divides a dialogue as a prompt, (ii) generates continuations for subparts using LMs, (iii) recombines the dialogue and continuations, and (iv) compares the likelihoods of the recombined sequences. This approach mitigates bias in linguistic analyses of LMs and enables systematic testing of discourse-sensitive behavior. Applying DGRC, we find that LMs prefer to continue dialogue on at-issue content, with this effect enhanced in instruct-tuned models. They also reduce their at-issue preference when relevant cues (e.g., "Hey, wait a minute") are present. Although instruct-tuning does not further amplify this modulation, the pattern reflects a hallmark of successful dialogue dynamics.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cash Flow Underwriting with Bank Transaction Data: Advancing MSME Financial Inclusion in Malaysia</title>
<link>https://arxiv.org/abs/2510.16066</link>
<guid>https://arxiv.org/abs/2510.16066</guid>
<content:encoded><![CDATA[
arXiv:2510.16066v2 Announce Type: replace-cross 
Abstract: Despite accounting for 96.1% of all businesses in Malaysia, access to financing remains one of the most persistent challenges faced by Micro, Small, and Medium Enterprises (MSMEs). Newly established or young businesses are often excluded from formal credit markets as traditional underwriting approaches rely heavily on credit bureau data. This study investigates the potential of bank statement data as an alternative data source for credit assessment to promote financial inclusion in emerging markets. Firstly, we propose a cash flow-based underwriting pipeline where we utilise bank statement data for end-to-end data extraction and machine learning credit scoring. Secondly, we introduce a novel dataset of 611 loan applicants from a Malaysian lending institution. Thirdly, we develop and evaluate credit scoring models based on application information and bank transaction-derived features. Empirical results show that the use of such data boosts the performance of all models on our dataset, which can improve credit scoring for new-to-lending MSMEs. Lastly, we intend to release the anonymised bank transaction dataset to facilitate further research on MSMEs financial inclusion within Malaysia's emerging economy.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Tutorial on Cognitive Biases in Agentic AI-Driven 6G Autonomous Networks</title>
<link>https://arxiv.org/abs/2510.19973</link>
<guid>https://arxiv.org/abs/2510.19973</guid>
<content:encoded><![CDATA[
arXiv:2510.19973v2 Announce Type: replace-cross 
Abstract: The path to higher network autonomy in 6G lies beyond the mere optimization of key performance indicators (KPIs). While KPIs have enabled automation gains under TM Forum Levels 1--3, they remain numerical abstractions that act only as proxies for the real essence of communication networks: seamless connectivity, fairness, adaptability, and resilience. True autonomy requires perceiving and reasoning over the network environment as it is. Such progress can be achieved through \emph{agentic AI}, where large language model (LLM)-powered agents perceive multimodal telemetry, reason with memory, negotiate across domains, and act via APIs to achieve multi-objective goals. However, deploying such agents introduces the challenge of cognitive biases inherited from human design, which can distort reasoning, negotiation, tool use, and actuation. Between neuroscience and AI, this paper provides a tutorial on a selection of well-known biases, including their taxonomy, definition, mathematical formulation, emergence in telecom systems and the commonly impacted agentic components. The tutorial also presents various mitigation strategies tailored to each type of bias. The article finally provides two practical use-cases, which tackle the emergence, impact and mitigation gain of some famous biases in 6G inter-slice and cross-domain management. In particular, anchor randomization, temporal decay and inflection bonus techniques are introduced to specifically address anchoring, temporal and confirmation biases. This avoids that agents stick to the initial high resource allocation proposal or decisions that are recent and/or confirming a prior hypothesis. By grounding decisions in a richer and fairer set of past experiences, the quality and bravery of the agentic agreements in the second use-case, for instance, are leading to $\times 5$ lower latency and around $40\%$ higher energy saving.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Collective Communication for 100k+ GPUs</title>
<link>https://arxiv.org/abs/2510.20171</link>
<guid>https://arxiv.org/abs/2510.20171</guid>
<content:encoded><![CDATA[
arXiv:2510.20171v3 Announce Type: replace-cross 
Abstract: The increasing scale of large language models (LLMs) necessitates highly efficient collective communication frameworks, particularly as training workloads extend to hundreds of thousands of GPUs. Traditional communication methods face significant throughput and latency limitations at this scale, hindering both the development and deployment of state-of-the-art models. This paper presents the NCCLX collective communication framework, developed at Meta, engineered to optimize performance across the full LLM lifecycle, from the synchronous demands of large-scale training to the low-latency requirements of inference. The framework is designed to support complex workloads on clusters exceeding 100,000 GPUs, ensuring reliable, high-throughput, and low-latency data exchange. Empirical evaluation on the Llama4 model demonstrates substantial improvements in communication efficiency. This research contributes a robust solution for enabling the next generation of LLMs to operate at unprecedented scales.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Hierarchical Organization for Medical Multi-document Summarization</title>
<link>https://arxiv.org/abs/2510.23104</link>
<guid>https://arxiv.org/abs/2510.23104</guid>
<content:encoded><![CDATA[
arXiv:2510.23104v2 Announce Type: replace-cross 
Abstract: Medical multi-document summarization (MDS) is a complex task that requires effectively managing cross-document relationships. This paper investigates whether incorporating hierarchical structures in the inputs of MDS can improve a model's ability to organize and contextualize information across documents compared to traditional flat summarization methods. We investigate two ways of incorporating hierarchical organization across three large language models (LLMs), and conduct comprehensive evaluations of the resulting summaries using automated metrics, model-based metrics, and domain expert evaluation of preference, understandability, clarity, complexity, relevance, coverage, factuality, and coherence. Our results show that human experts prefer model-generated summaries over human-written summaries. Hierarchical approaches generally preserve factuality, coverage, and coherence of information, while also increasing human preference for summaries. Additionally, we examine whether simulated judgments from GPT-4 align with human judgments, finding higher agreement along more objective evaluation facets. Our findings demonstrate that hierarchical structures can improve the clarity of medical summaries generated by models while maintaining content coverage, providing a practical way to improve human preference for generated summaries.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Progressive Growing of Patch Size: Curriculum Learning for Accelerated and Improved Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2510.23241</link>
<guid>https://arxiv.org/abs/2510.23241</guid>
<content:encoded><![CDATA[
arXiv:2510.23241v2 Announce Type: replace-cross 
Abstract: In this work, we introduce Progressive Growing of Patch Size, an automatic curriculum learning approach for 3D medical image segmentation. Our approach progressively increases the patch size during model training, resulting in an improved class balance for smaller patch sizes and accelerated convergence of the training process. We evaluate our curriculum approach in two settings: a resource-efficient mode and a performance mode, both regarding Dice score performance and computational costs across 15 diverse and popular 3D medical image segmentation tasks. The resource-efficient mode matches the Dice score performance of the conventional constant patch size sampling baseline with a notable reduction in training time to only 44%. The performance mode improves upon constant patch size segmentation results, achieving a statistically significant relative mean performance gain of 1.28% in Dice Score. Remarkably, across all 15 tasks, our proposed performance mode manages to surpass the constant patch size baseline in Dice Score performance, while simultaneously reducing training time to only 89%. The benefits are particularly pronounced for highly imbalanced tasks such as lesion segmentation tasks. Rigorous experiments demonstrate that our performance mode not only improves mean segmentation performance but also reduces performance variance, yielding more trustworthy model comparison. Furthermore, our findings reveal that the proposed curriculum sampling is not tied to a specific architecture but represents a broadly applicable strategy that consistently boosts performance across diverse segmentation models, including UNet, UNETR, and SwinUNETR. In summary, we show that this simple yet elegant transformation on input data substantially improves both Dice Score performance and training runtime, while being compatible across diverse segmentation backbones.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Charting the European LLM Benchmarking Landscape: A New Taxonomy and a Set of Best Practices</title>
<link>https://arxiv.org/abs/2510.24450</link>
<guid>https://arxiv.org/abs/2510.24450</guid>
<content:encoded><![CDATA[
arXiv:2510.24450v2 Announce Type: replace-cross 
Abstract: While new benchmarks for large language models (LLMs) are being developed continuously to catch up with the growing capabilities of new models and AI in general, using and evaluating LLMs in non-English languages remains a little-charted landscape. We give a concise overview of recent developments in LLM benchmarking, and then propose a new taxonomy for the categorization of benchmarks that is tailored to multilingual or non-English use scenarios. We further propose a set of best practices and quality standards that could lead to a more coordinated development of benchmarks for European languages. Among other recommendations, we advocate for a higher language and culture sensitivity of evaluation methods.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tongyi DeepResearch Technical Report</title>
<link>https://arxiv.org/abs/2510.24701</link>
<guid>https://arxiv.org/abs/2510.24701</guid>
<content:encoded><![CDATA[
arXiv:2510.24701v2 Announce Type: replace-cross 
Abstract: We present Tongyi DeepResearch, an agentic large language model, which is specifically designed for long-horizon, deep information-seeking research tasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is developed through an end-to-end training framework that combines agentic mid-training and agentic post-training, enabling scalable reasoning and information seeking across complex tasks. We design a highly scalable data synthesis pipeline that is fully automatic, without relying on costly human annotation, and empowers all training stages. By constructing customized environments for each stage, our system enables stable and consistent interactions throughout. Tongyi DeepResearch, featuring 30.5 billion total parameters, with only 3.3 billion activated per token, achieves state-of-the-art performance across a range of agentic deep research benchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH, WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We open-source the model, framework, and complete solutions to empower the community.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AAGATE: A NIST AI RMF-Aligned Governance Platform for Agentic AI</title>
<link>https://arxiv.org/abs/2510.25863</link>
<guid>https://arxiv.org/abs/2510.25863</guid>
<content:encoded><![CDATA[
arXiv:2510.25863v2 Announce Type: replace-cross 
Abstract: This paper introduces the Agentic AI Governance Assurance & Trust Engine (AAGATE), a Kubernetes-native control plane designed to address the unique security and governance challenges posed by autonomous, language-model-driven agents in production. Recognizing the limitations of traditional Application Security (AppSec) tooling for improvisational, machine-speed systems, AAGATE operationalizes the NIST AI Risk Management Framework (AI RMF). It integrates specialized security frameworks for each RMF function: the Agentic AI Threat Modeling MAESTRO framework for Map, a hybrid of OWASP's AIVSS and SEI's SSVC for Measure, and the Cloud Security Alliance's Agentic AI Red Teaming Guide for Manage. By incorporating a zero-trust service mesh, an explainable policy engine, behavioral analytics, and decentralized accountability hooks, AAGATE provides a continuous, verifiable governance solution for agentic AI, enabling safe, accountable, and scalable deployment. The framework is further extended with DIRF for digital identity rights, LPCI defenses for logic-layer injection, and QSAF monitors for cognitive degradation, ensuring governance spans systemic, adversarial, and ethical risks.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Global Retrieval Augmented Generation: A Benchmark for Corpus-Level Reasoning</title>
<link>https://arxiv.org/abs/2510.26205</link>
<guid>https://arxiv.org/abs/2510.26205</guid>
<content:encoded><![CDATA[
arXiv:2510.26205v2 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) has emerged as a leading approach to reducing hallucinations in large language models (LLMs). Current RAG evaluation benchmarks primarily focus on what we call local RAG: retrieving relevant chunks from a small subset of documents to answer queries that require only localized understanding within specific text chunks. However, many real-world applications require a fundamentally different capability -- global RAG -- which involves aggregating and analyzing information across entire document collections to derive corpus-level insights (for example, "What are the top 10 most cited papers in 2023?"). In this paper, we introduce GlobalQA -- the first benchmark specifically designed to evaluate global RAG capabilities, covering four core task types: counting, extremum queries, sorting, and top-k extraction. Through systematic evaluation across different models and baselines, we find that existing RAG methods perform poorly on global tasks, with the strongest baseline achieving only 1.51 F1 score. To address these challenges, we propose GlobalRAG, a multi-tool collaborative framework that preserves structural coherence through chunk-level retrieval, incorporates LLM-driven intelligent filters to eliminate noisy documents, and integrates aggregation modules for precise symbolic computation. On the Qwen2.5-14B model, GlobalRAG achieves 6.63 F1 compared to the strongest baseline's 1.51 F1, validating the effectiveness of our method.
]]></content:encoded>
<pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Jarvis: Towards Personalized AI Assistant via Personal KV-Cache Retrieval</title>
<link>https://arxiv.org/abs/2510.22765</link>
<guid>https://arxiv.org/abs/2510.22765</guid>
<content:encoded><![CDATA[
<div> Personalized AI assistant, Vision-language models, KV-Cache retrieval, User-specific information, Fine-grained benchmark <br />
Summary: <br />
The article introduces Jarvis, a framework for a personalized AI assistant that utilizes personal KV-Cache retrieval to store user-specific information in textual and visual tokens. Textual tokens summarize user information into metadata, while visual tokens extract distinct image patches from user images. When answering questions, Jarvis retrieves relevant KV-Caches from personal storage to ensure response accuracy. A fine-grained benchmark is also introduced to emphasize accurate question answering based on fine-grained user-specific information. Jarvis achieves state-of-the-art results in visual question answering and text-only tasks across multiple datasets, showcasing its effectiveness in providing accurate responses, especially for questions dependent on specific local details. The framework presents a practical approach towards developing personalized AI assistants and will release the code and dataset for further research. <div>
arXiv:2510.22765v2 Announce Type: replace 
Abstract: The rapid development of Vision-language models (VLMs) enables open-ended perception and reasoning. Recent works have started to investigate how to adapt general-purpose VLMs into personalized assistants. Even commercial models such as ChatGPT now support model personalization by incorporating user-specific information. However, existing methods either learn a set of concept tokens or train a VLM to utilize user-specific information. However, both pipelines struggle to generate accurate answers as personalized assistants. We introduce Jarvis, an innovative framework for a personalized AI assistant through personal KV-Cache retrieval, which stores user-specific information in the KV-Caches of both textual and visual tokens. The textual tokens are created by summarizing user information into metadata, while the visual tokens are produced by extracting distinct image patches from the user's images. When answering a question, Jarvis first retrieves related KV-Caches from personal storage and uses them to ensure accuracy in responses. We also introduce a fine-grained benchmark built with the same distinct image patch mining pipeline, emphasizing accurate question answering based on fine-grained user-specific information. Jarvis is capable of providing more accurate responses, particularly when they depend on specific local details. Jarvis achieves state-of-the-art results in both visual question answering and text-only tasks across multiple datasets, indicating a practical path toward personalized AI assistants. The code and dataset will be released.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Will Humanity Be Rendered Obsolete by AI?</title>
<link>https://arxiv.org/abs/2510.22814</link>
<guid>https://arxiv.org/abs/2510.22814</guid>
<content:encoded><![CDATA[
<div> existential risks, artificial intelligence, ultraintelligence, AGI, superintelligence
<br />
The article discusses the existential risks posed by artificial intelligence (AI) to humanity, specifically focusing on the trajectory from current AI to ultraintelligence. Drawing on the theoretical work of Irving J. Good and Nick Bostrom, as well as recent publications such as AI 2027 and If Anyone Builds It, Everyone Dies, it explores the concepts of AGI and superintelligence. The analysis considers the exponential growth in machines' cognitive power and the potential for hypothetical IQs that vastly exceed human intelligence. It addresses the ethical and existential implications of an intelligence that is fundamentally alien to humanity, highlighting the possibility of human extinction not as a result of malice, but from the uncontrollable and indifferent cognitive superiority of AI. The article raises concerns about the potential consequences of AI surpassing human intelligence and the impact it may have on the future of humanity.
<br /><br />Summary: <div>
arXiv:2510.22814v2 Announce Type: replace 
Abstract: This article analyzes the existential risks artificial intelligence (AI) poses to humanity, tracing the trajectory from current AI to ultraintelligence. Drawing on Irving J. Good and Nick Bostrom's theoretical work, plus recent publications (AI 2027; If Anyone Builds It, Everyone Dies), it explores AGI and superintelligence. Considering machines' exponentially growing cognitive power and hypothetical IQs, it addresses the ethical and existential implications of an intelligence vastly exceeding humanity's, fundamentally alien. Human extinction may result not from malice, but from uncontrollable, indifferent cognitive superiority.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mixed-Density Diffuser: Efficient Planning with Non-uniform Temporal Resolution</title>
<link>https://arxiv.org/abs/2510.23026</link>
<guid>https://arxiv.org/abs/2510.23026</guid>
<content:encoded><![CDATA[
<div> diffusion planner, sparse-step planning, long-term dependencies, Mixed Density Diffuser, SOTA <br />
<br />Summary: 
The article discusses the benefits of sparse-step planning over single-step planning in diffusion planners. Training models to skip steps in trajectories helps capture long-term dependencies without adding computational cost. However, predicting overly sparse plans can lead to decreased performance. The authors propose Mixed Density Diffuser (MDD), a diffusion planner with tunable hyperparameters for varying densities throughout the temporal horizon. MDD achieves state-of-the-art performance across various task domains, including Maze2D, Franka Kitchen, and Antmaze D4RL. This approach addresses the non-uniform temporal density threshold within planned trajectories and suggests that certain parts of the trajectory should be more densely planned to improve overall performance. <div>
arXiv:2510.23026v2 Announce Type: replace 
Abstract: Recent studies demonstrate that diffusion planners benefit from sparse-step planning over single-step planning. Training models to skip steps in their trajectories helps capture long-term dependencies without additional or memory computational cost. However, predicting excessively sparse plans degrades performance. We hypothesize this temporal density threshold is non-uniform across a temporal horizon and that certain parts of a planned trajectory should be more densely planned. We propose Mixed Density Diffuser (MDD), a diffusion planner where the densities throughout the horizon are tunable hyperparameters. MDD achieves a new SOTA across the Maze2D, Franka Kitchen, and Antmaze D4RL task domains.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OneCast: Structured Decomposition and Modular Generation for Cross-Domain Time Series Forecasting</title>
<link>https://arxiv.org/abs/2510.24028</link>
<guid>https://arxiv.org/abs/2510.24028</guid>
<content:encoded><![CDATA[
<div> Seasonal patterns, trend component, generative pathways, domain-specific trends, time series forecasting
Summary:
OneCast is a structured and modular forecasting framework designed to improve cross-domain time series forecasting by decomposing time series into seasonal and trend components. The seasonal component is captured through interpretable basis functions, while the trend component is encoded into discrete tokens and inferred through a masked discrete diffusion mechanism. The combination of these components allows for more accurate forecasts that capture seasonal patterns and track domain-specific trends. Extensive experiments across eight domains show that OneCast outperforms existing baselines, highlighting its effectiveness in achieving generalization across heterogeneous time series data. <br /><br />Summary: <div>
arXiv:2510.24028v2 Announce Type: replace 
Abstract: Cross-domain time series forecasting is a valuable task in various web applications. Despite its rapid advancement, achieving effective generalization across heterogeneous time series data remains a significant challenge. Existing methods have made progress by extending single-domain models, yet often fall short when facing domain-specific trend shifts and inconsistent periodic patterns. We argue that a key limitation lies in treating temporal series as undifferentiated sequence, without explicitly decoupling their inherent structural components. To address this, we propose OneCast, a structured and modular forecasting framework that decomposes time series into seasonal and trend components, each modeled through tailored generative pathways. Specifically, the seasonal component is captured by a lightweight projection module that reconstructs periodic patterns via interpretable basis functions. In parallel, the trend component is encoded into discrete tokens at segment level via a semantic-aware tokenizer, and subsequently inferred through a masked discrete diffusion mechanism. The outputs from both branches are combined to produce a final forecast that captures seasonal patterns while tracking domain-specific trends. Extensive experiments across eight domains demonstrate that OneCast mostly outperforms state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MCP-Flow: Facilitating LLM Agents to Master Real-World, Diverse and Scaling MCP Tools</title>
<link>https://arxiv.org/abs/2510.24284</link>
<guid>https://arxiv.org/abs/2510.24284</guid>
<content:encoded><![CDATA[
<div> pipeline, server discovery, data synthesis, model training, MCP tool selection

Summary: 
The paper introduces MCP-Flow, an automated web-agent-driven pipeline for large-scale server discovery, data synthesis, and model training for Large Language Models (LLMs). By collecting and filtering data from numerous servers and tools, MCP-Flow generates high-quality instruction-function call pairs and trajectories, surpassing previous approaches in both scale and variety. Experiments show that MCP-Flow enhances tool selection, function-call generation, and task performance for LLM agents in Model Contextual Protocol (MCP) environments. The tool is publicly accessible on GitHub, offering a scalable foundation for improving LLM agents' capabilities in real-world MCP scenarios. <div>
arXiv:2510.24284v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) increasingly rely on external tools to perform complex, realistic tasks, yet their ability to utilize the rapidly expanding Model Contextual Protocol (MCP) ecosystem remains limited. Existing MCP research covers few servers, depends on costly manual curation, and lacks training support, hindering progress toward real-world deployment. To overcome these limitations, we introduce MCP-Flow, an automated web-agent-driven pipeline for large-scale server discovery, data synthesis, and model training. MCP-Flow collects and filters data from 1166 servers and 11536 tools, producing 68733 high-quality instruction-function call pairs and 6439 trajectories, far exceeding prior work in scale and diversity. Extensive experiments demonstrate MCP-Flow's effectiveness in driving superior MCP tool selection, function-call generation, and enhanced agentic task performance. MCP-Flow thus provides a scalable foundation for advancing LLM agents' proficiency in real-world MCP environments. MCP-Flow is publicly available at \href{https://github.com/wwh0411/MCP-Flow}{https://github.com/wwh0411/MCP-Flow}.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ADPO: Anchored Direct Preference Optimization</title>
<link>https://arxiv.org/abs/2510.18913</link>
<guid>https://arxiv.org/abs/2510.18913</guid>
<content:encoded><![CDATA[
<div> Keywords: Direct Preference Optimization, Anchored Direct Preference Optimization, Soft supervision, Geometric interpretation, Trust region. 

Summary: 
Direct Preference Optimization (DPO) is commonly used to align models with human feedback but is sensitive to annotator noise. Anchored Direct Preference Optimization (ADPO) introduces a framework that learns from soft supervision using a reference model as an anchor. The anchoring mechanism enforces a trust region on policy updates, leading to a robust geometric interpretation based on softmax Fisher information metric. In experiments on twelve scenarios and two environments, dynamic anchors outperformed fixed anchors for online exploration in noisy environments. For offline distillation, a fixed anchor pointing to the teacher policy showed significant improvement in performance and reduced KL divergence. ADPO proved effective for both online and offline scenarios, particularly benefiting from larger models. The findings provide practical guidance for anchor strategies and establish ADPO as a robust framework for preference learning. Code and configurations are provided for reproducibility. 

<br /><br />Summary: <div>
arXiv:2510.18913v3 Announce Type: replace-cross 
Abstract: Direct Preference Optimization (DPO) has become a standard for aligning models with human feedback, yet its reliance on hard, pairwise preferences makes it brittle to annotator noise and distribution shift. We propose Anchored Direct Preference Optimization (ADPO), a generalized framework that learns from soft, listwise supervision by anchoring policy updates to a reference model. Our key theoretical contribution is to show that this anchoring mechanism imposes an implicit trust region on the policy update, enforced by the softmax Fisher information metric. This provides a robust geometric interpretation for both fixed and dynamic anchor strategies. Our central empirical finding is a task-dependent tradeoff between anchor update strategies. Through controlled experiments across twelve scenarios and two MuJoCo environments, we demonstrate that (1) for online exploration in noisy environments, a dynamic anchor that tracks the learning policy is superior, improving performance by 5 to 11 percent over a fixed anchor; and (2) for offline distillation, a fixed anchor pointing to the teacher policy is dramatically more effective, achieving returns of 206.7 on HalfCheetah-v5 (387 percent of teacher) and 65.4 on Hopper-v5 (61 percent of teacher), while reducing KL divergence to the teacher by up to 5000 times compared with standard knowledge distillation. These findings offer clear, practical guidance for selecting anchor strategies and establish ADPO as a robust, unified framework for preference learning. Larger models further amplify ADPO's benefits (0.718 vs. 0.416 at hidden dimension 256), suggesting that anchoring acts as an effective trust-region regularizer. We release code and configurations to facilitate reproducibility.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge-guided Continual Learning for Behavioral Analytics Systems</title>
<link>https://arxiv.org/abs/2510.22405</link>
<guid>https://arxiv.org/abs/2510.22405</guid>
<content:encoded><![CDATA[
<div> Keywords: online platforms, user behavior, data drift, continual learning, external knowledge

Summary:
This study explores the challenges in maintaining performance of models that capture user behavior on online platforms over time. Data drift and catastrophic forgetting are key issues, but replay-based continual learning offers a solution by preserving important training instances from past tasks. However, the fixed buffer size limitation can hinder performance improvement. To address this, the authors propose a novel augmentation-based approach utilizing external knowledge bases. Through experiments on deviant behavior classification datasets, they show that incorporating external knowledge through augmentation can enhance model performance compared to baseline replay-based methods. This research highlights the importance of leveraging external knowledge in continual learning to adapt models to evolving user behavior patterns on online platforms.<br /><br />Summary: This study addresses the challenges of maintaining model performance in capturing user behavior on online platforms over time. Replay-based continual learning helps combat data drift and catastrophic forgetting, but a fixed buffer size limitation hinders improvement. The authors propose an augmentation-based approach using external knowledge to enhance model performance. Experiments on deviant behavior classification datasets demonstrate the effectiveness of this approach in surpassing baseline methods. <div>
arXiv:2510.22405v2 Announce Type: replace-cross 
Abstract: User behavior on online platforms is evolving, reflecting real-world changes in how people post, whether it's helpful messages or hate speech. Models that learn to capture this content can experience a decrease in performance over time due to data drift, which can lead to ineffective behavioral analytics systems. However, fine-tuning such a model over time with new data can be detrimental due to catastrophic forgetting. Replay-based approaches in continual learning offer a simple yet efficient method to update such models, minimizing forgetting by maintaining a buffer of important training instances from past learned tasks. However, the main limitation of this approach is the fixed size of the buffer. External knowledge bases can be utilized to overcome this limitation through data augmentation. We propose a novel augmentation-based approach to incorporate external knowledge in the replay-based continual learning framework. We evaluate several strategies with three datasets from prior studies related to deviant behavior classification to assess the integration of external knowledge in continual learning and demonstrate that augmentation helps outperform baseline replay-based approaches.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flight Delay Prediction via Cross-Modality Adaptation of Large Language Models and Aircraft Trajectory Representation</title>
<link>https://arxiv.org/abs/2510.23636</link>
<guid>https://arxiv.org/abs/2510.23636</guid>
<content:encoded><![CDATA[
<div> model-based, multimodal, flight delay prediction, trajectory data, language modality

Summary: 
This paper introduces a novel approach to flight delay prediction using a large language model that integrates trajectory data with textual aeronautical information. By adapting trajectory data into the language modality, the model captures airspace conditions and effectively leverages contextual information to predict delays with sub-minute precision. The experiments demonstrate that the model consistently achieves minute-level prediction errors, meeting operational standards for accuracy. By combining linguistic understanding with cross-modality adaptation of trajectory data, the framework enhances delay prediction and shows practicality for real-world operations. The model also supports real-time updates, allowing for refined predictions based on new operational information. Overall, this lightweight and scalable approach offers a promising solution for improving air traffic management and network performance. 

<br /><br />Summary: <div>
arXiv:2510.23636v2 Announce Type: replace-cross 
Abstract: Flight delay prediction has become a key focus in air traffic management, as delays highlight inefficiencies that impact overall network performance. This paper presents a lightweight large language model-based multimodal flight delay prediction, formulated from the perspective of air traffic controllers monitoring aircraft delay after entering the terminal area. The approach integrates trajectory representations with textual aeronautical information, including flight information, weather reports, and aerodrome notices, by adapting trajectory data into the language modality to capture airspace conditions. The experiments show that the model consistently achieves sub-minute prediction error by effectively leveraging contextual information related to the sources of delay, fulfilling the operational standard for minute-level precision. The framework demonstrates that linguistic understanding, when combined with cross-modality adaptation of trajectory data, enhances delay prediction. Moreover, the approach shows practicality and potential scalability for real-world operations, supporting real-time updates that refine predictions upon receiving new operational information.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Key and Value Weights Are Probably All You Need: On the Necessity of the Query, Key, Value weight Triplet in Decoder-Only Transformers</title>
<link>https://arxiv.org/abs/2510.23912</link>
<guid>https://arxiv.org/abs/2510.23912</guid>
<content:encoded><![CDATA[
<div> weight triplet, attention mechanisms, LLMs, Query weights, GPT-3<br />
Summary:<br />
The article explores the redundancy of Query weights in the Query, Key, Value weight triplet in attention mechanisms used in large language models (LLMs). By simplifying assumptions, it is theoretically proven that Query weights are unnecessary, leading to a reduction of over 8% in non-embedding/lm-head parameters. Testing this theory on GPT-3 small architectures trained from scratch shows that a model with reduced Query weights achieves similar validation loss to standard baselines. These results suggest that Query weight redundancy exists at scale and could potentially lead to more efficient models in the future. <div>
arXiv:2510.23912v2 Announce Type: replace-cross 
Abstract: The Query, Key, Value weight triplet is a building block of current attention mechanisms in state-of-the-art LLMs. We theoretically investigate whether this triplet can be reduced, proving under simplifying assumptions that the Query weights are redundant, thereby reducing the number of non-embedding/lm-head parameters by over 8%. We validate the theory on full-complexity GPT-3 small architectures (with layer normalization, skip connections, and weight decay) trained from scratch, demonstrating that the reduced model achieves comparable validation loss to standard baselines. These findings motivate the investigation of the Query weight redundancy at scale.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Visual Intelligence: Insights from Video Pretraining</title>
<link>https://arxiv.org/abs/2510.24448</link>
<guid>https://arxiv.org/abs/2510.24448</guid>
<content:encoded><![CDATA[
<div> pretraining, Video Diffusion Models, large language models, visual domain, inductive biases
Summary:<br /><br />Large language models (LLMs) have shown success in adapting to new tasks with minimal supervision in the language domain. However, this success has not been effectively transferred to the visual domain, where models struggle with various challenges. Video Diffusion Models (VDMs) are explored as a potential solution to address these challenges by pretraining on spatiotemporal data to leverage strong inductive biases for structure and dynamics. A controlled evaluation was conducted comparing a pretrained LLM and VDM equipped with adapters across various benchmarks. VDMs demonstrated higher data efficiency in tasks such as ARC-AGI, ConceptARC, visual games, route planning, and cellular automata. These results suggest that video pretraining provides inductive biases that could advance visual foundation models.<br /> <div>
arXiv:2510.24448v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated that large-scale pretraining enables systems to adapt rapidly to new problems with little supervision in the language domain. This success, however, has not translated as effectively to the visual domain, where models, including LLMs, continue to struggle with compositional understanding, sample efficiency, and general-purpose problem-solving. We investigate Video Diffusion Models (VDMs) as a promising direction for bridging this gap. Pretraining on spatiotemporal data endows these models with strong inductive biases for structure and dynamics, which we hypothesize can support broad task adaptability. To test this, we design a controlled evaluation in which both a pretrained LLM and a pretrained VDM are equipped with lightweight adapters and presented with tasks in their natural modalities. Across benchmarks including ARC-AGI, ConceptARC, visual games, route planning, and cellular automata, VDMs demonstrate higher data efficiency than their language counterparts. Taken together, our results indicate that video pretraining offers inductive biases that support progress toward visual foundation models.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Detection of Fake Reviews using BERT and ResNet-50</title>
<link>https://arxiv.org/abs/2511.00020</link>
<guid>https://arxiv.org/abs/2511.00020</guid>
<content:encoded><![CDATA[
<div> Keywords: digital commerce, user-generated reviews, fake review detection, multimodal learning, content moderation

Summary:
This study addresses the issue of fake or misleading user-generated reviews in digital commerce by proposing a robust multimodal fake review detection framework. The framework integrates textual features encoded with BERT and visual features extracted using ResNet-50 to capture semantic inconsistencies across different modalities. A curated dataset comprising 21,142 user-uploaded images across various domains was utilized to train the model. Results show that the multimodal model outperforms unimodal baselines with an F1-score of 0.934 on the test set. The model demonstrates the ability to detect subtle inconsistencies, such as exaggerated textual praise paired with unrelated or low-quality images, commonly found in deceptive content. This research highlights the importance of multimodal learning in safeguarding digital trust and offers a scalable solution for content moderation across online platforms. 

<br /><br />Summary: <div>
arXiv:2511.00020v1 Announce Type: new 
Abstract: In the current digital commerce landscape, user-generated reviews play a critical role in shaping consumer behavior, product reputation, and platform credibility. However, the proliferation of fake or misleading reviews often generated by bots, paid agents, or AI models poses a significant threat to trust and transparency within review ecosystems. Existing detection models primarily rely on unimodal, typically textual, data and therefore fail to capture semantic inconsistencies across different modalities. To address this gap, a robust multimodal fake review detection framework is proposed, integrating textual features encoded with BERT and visual features extracted using ResNet-50. These representations are fused through a classification head to jointly predict review authenticity. To support this approach, a curated dataset comprising 21,142 user-uploaded images across food delivery, hospitality, and e-commerce domains was utilized. Experimental results indicate that the multimodal model outperforms unimodal baselines, achieving an F1-score of 0.934 on the test set. Additionally, the confusion matrix and qualitative analysis highlight the model's ability to detect subtle inconsistencies, such as exaggerated textual praise paired with unrelated or low-quality images, commonly found in deceptive content. This study demonstrates the critical role of multimodal learning in safeguarding digital trust and offers a scalable solution for content moderation across various online platforms.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph-Attentive MAPPO for Dynamic Retail Pricing</title>
<link>https://arxiv.org/abs/2511.00039</link>
<guid>https://arxiv.org/abs/2511.00039</guid>
<content:encoded><![CDATA[
<div> Keywords: Dynamic pricing, retail, multi-agent reinforcement learning, graph attention, product interactions

Summary:
Dynamic pricing in retail is a complex task that requires adapting to changing demand while optimizing prices across related products. In this study, a comparison between a traditional MAPPO model and a graph-attention-augmented variant (MAPPO+GAT) was conducted using a simulated pricing environment based on real transaction data. The results show that MAPPO provides a reliable foundation for portfolio-level price control, while MAPPO+GAT improves performance by leveraging interactions among products through a product graph. This approach enhances profitability, stability across different scenarios, fairness among products, and training efficiency. The findings suggest that incorporating graph-based interactions in multi-agent reinforcement learning can offer a scalable and stable solution for dynamic retail pricing, providing practical advantages in decision-making for multiple products. <div>
arXiv:2511.00039v1 Announce Type: new 
Abstract: Dynamic pricing in retail requires policies that adapt to shifting demand while coordinating decisions across related products. We present a systematic empirical study of multi-agent reinforcement learning for retail price optimization, comparing a strong MAPPO baseline with a graph-attention-augmented variant (MAPPO+GAT) that leverages learned interactions among products. Using a simulated pricing environment derived from real transaction data, we evaluate profit, stability across random seeds, fairness across products, and training efficiency under a standardized evaluation protocol. The results indicate that MAPPO provides a robust and reproducible foundation for portfolio-level price control, and that MAPPO+GAT further enhances performance by sharing information over the product graph without inducing excessive price volatility. These results indicate that graph-integrated MARL provides a more scalable and stable solution than independent learners for dynamic retail pricing, offering practical advantages in multi-product decision-making.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GEPOC Parameters -- Open Source Parametrisation and Validation for Austria, Version 2.0</title>
<link>https://arxiv.org/abs/2511.00048</link>
<guid>https://arxiv.org/abs/2511.00048</guid>
<content:encoded><![CDATA[
<div> population models, data processing, model parameters, Austria, agent-based model

Summary:
This article presents GEPOC, a collection of models and methods for population-level research questions. It focuses on data-processing methods for computing model parameters for Austria using publicly available data. The algorithms for aggregation, disaggregation, fusion, cleansing, and scaling of data are described, along with resulting parameter files. Special emphasis is placed on computing parameters for the GEPOC ABM model, a continuous-time agent-based population model. An extensive validation study using this model is presented at the end of the work. <div>
arXiv:2511.00048v1 Announce Type: new 
Abstract: GEPOC, short for Generic Population Concept, is a collection of models and methods for analysing population-level research questions. For the valid application of the models for a specific country or region, stable and reproducible data processes are necessary, which provide valid and ready-to-use model parameters. This work contains a complete description of the data-processing methods for computation of model parameters for Austria, based exclusively on freely and publicly accessible data. In addition to the description of the source data used, this includes all algorithms used for aggregation, disaggregation, fusion, cleansing or scaling of the data, as well as a description of the resulting parameter files. The document places particular emphasis on the computation of parameters for the most important GEPOC model, GEPOC ABM, a continuous-time agent-based population model. An extensive validation study using this particular model was made and is presented at the end of this work.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QuantumBench: A Benchmark for Quantum Problem Solving</title>
<link>https://arxiv.org/abs/2511.00092</link>
<guid>https://arxiv.org/abs/2511.00092</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, quantum science, benchmark, evaluation, dataset

Summary:
Large language models (LLMs) are becoming increasingly integrated into scientific workflows, including in the field of quantum science. However, there is a need to evaluate how well these models understand and apply domain-specific knowledge, especially in complex fields like quantum science. To address this gap, QuantumBench, a benchmark dataset, was created to evaluate the performance of LLMs in the quantum domain. The dataset consists of approximately 800 questions across nine areas of quantum science, organized in a multiple-choice format. By using QuantumBench, researchers can assess the effectiveness of existing LLMs in quantum research and analyze their sensitivity to changes in question format. This dataset is the first of its kind specific to the quantum domain, aiming to guide the use of LLMs in quantum research.<br /><br />Summary: <div>
arXiv:2511.00092v1 Announce Type: new 
Abstract: Large language models are now integrated into many scientific workflows, accelerating data analysis, hypothesis generation, and design space exploration. In parallel with this growth, there is a growing need to carefully evaluate whether models accurately capture domain-specific knowledge and notation, since general-purpose benchmarks rarely reflect these requirements. This gap is especially clear in quantum science, which features non-intuitive phenomena and requires advanced mathematics. In this study, we introduce QuantumBench, a benchmark for the quantum domain that systematically examine how well LLMs understand and can be applied to this non-intuitive field. Using publicly available materials, we compiled approximately 800 questions with their answers spanning nine areas related to quantum science and organized them into an eight-option multiple-choice dataset. With this benchmark, we evaluate several existing LLMs and analyze their performance in the quantum domain, including sensitivity to changes in question format. QuantumBench is the first LLM evaluation dataset built for the quantum domain, and it is intended to guide the effective use of LLMs in quantum research.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Engineering.ai: A Platform for Teams of AI Engineers in Computational Design</title>
<link>https://arxiv.org/abs/2511.00122</link>
<guid>https://arxiv.org/abs/2511.00122</guid>
<content:encoded><![CDATA[
<div> Keywords: AI engineer, computational design, multi-agent architecture, memory system, UAV wing optimization

Summary:
Engineering.ai presents a platform for teams of AI engineers in computational design, utilizing a hierarchical multi-agent architecture. Specialized agents, such as Aerodynamics and Optimization Engineers, are coordinated by a Chief Engineer and powered by domain-specific knowledge through LLM. File-mediated communication enables agent collaboration for data provenance and reproducibility. A memory system maintains project context and execution history while providing retrieval-augmented domain knowledge for decision-making. The framework integrates various tools for multidisciplinary simulations, validated through UAV wing optimization. The automated workflow achieved a 100% success rate across numerous parametric configurations, demonstrating the framework's reliability. <div>
arXiv:2511.00122v1 Announce Type: new 
Abstract: In modern engineering practice, human engineers collaborate in specialized teams to design complex products, with each expert completing their respective tasks while communicating and exchanging results and data with one another. While this division of expertise is essential for managing multidisciplinary complexity, it demands substantial development time and cost. Recently, we introduced OpenFOAMGPT (1.0, 2.0), which functions as an autonomous AI engineer for computational fluid dynamics, and turbulence.ai, which can conduct end-to-end research in fluid mechanics draft publications and PhD theses. Building upon these foundations, we present Engineering.ai, a platform for teams of AI engineers in computational design. The framework employs a hierarchical multi-agent architecture where a Chief Engineer coordinates specialized agents consisting of Aerodynamics, Structural, Acoustic, and Optimization Engineers, each powered by LLM with domain-specific knowledge. Agent-agent collaboration is achieved through file-mediated communication for data provenance and reproducibility, while a comprehensive memory system maintains project context, execution history, and retrieval-augmented domain knowledge to ensure reliable decision-making across the workflow. The system integrates FreeCAD, Gmsh, OpenFOAM, CalculiX, and BPM acoustic analysis, enabling parallel multidisciplinary simulations while maintaining computational accuracy. The framework is validated through UAV wing optimization. This work demonstrates that agentic-AI-enabled AI engineers has the potential to perform complex engineering tasks autonomously. Remarkably, the automated workflow achieved a 100% success rate across over 400 parametric configurations, with zero mesh generation failures, solver convergence issues, or manual interventions required, validating that the framework is trustworthy.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ARC-GEN: A Mimetic Procedural Benchmark Generator for the Abstraction and Reasoning Corpus</title>
<link>https://arxiv.org/abs/2511.00162</link>
<guid>https://arxiv.org/abs/2511.00162</guid>
<content:encoded><![CDATA[
<div> benchmark, Artificial General Intelligence, ARC-AGI, procedural generator, Google Code Golf Championship <br />
Summary:
The paper discusses the significance of the Abstraction and Reasoning Corpus as a benchmark for Artificial General Intelligence. It highlights the challenge in measuring skill acquisition efficiency, a key trait necessary for achieving AGI. The ARC-AGI suite is structured to test this efficiency by providing a limited number of task examples. To address this limitation, the paper introduces ARC-GEN, a procedural generator aimed at expanding the training dataset while maintaining fidelity to the original ARC-AGI tasks. The generator covers all 400 tasks and aligns with the distributional properties of the initial release. Additionally, the paper mentions the use of ARC-GEN in creating a benchmark suite for verifying programs submitted to the Google Code Golf Championship in 2025. <div>
arXiv:2511.00162v2 Announce Type: new 
Abstract: The Abstraction and Reasoning Corpus remains one of the most compelling and challenging benchmarks for tracking progress toward achieving Artificial General Intelligence. In contrast to other evaluation datasets designed to assess an agent's task-specific skills or accumulated knowledge, the ARC-AGI suite is specifically targeted at measuring skill acquisition efficiency, a trait that has (so far) been lacking in even the most sophisticated machine learning systems. For algorithms that require extensive intra-task exemplars, a significant constraint imposed by ARC-AGI is the modest cardinality of its demonstration set, comprising a small number of $\langle$ input, output $\rangle$ grids per task specifying the corresponding transformation. To embellish the space of viable sample pairs, this paper introduces ARC-GEN, an open-source procedural generator aimed at extending the original ARC-AGI training dataset as faithfully as possible. Unlike prior efforts, our generator is both exhaustive (covering all four-hundred tasks) and mimetic (more closely honoring the distributional properties and characteristics embodied in the initial ARC-AGI-1 release). We also discuss the use of this generator in establishing a static benchmark suite to verify the correctness of programs submitted to the 2025 Google Code Golf Championship.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Incremental Selection of Most-Filtering Conjectures and Proofs of the Selected Conjectures</title>
<link>https://arxiv.org/abs/2511.00194</link>
<guid>https://arxiv.org/abs/2511.00194</guid>
<content:encoded><![CDATA[
<div> Keywords: incremental selection algorithm, improved, selection, prove, conjectures<br />
<br />
Summary: 
The article introduces an enhancement to an incremental selection algorithm previously introduced. The improved algorithm builds upon the existing selection algorithm, providing advancements in its functionality. The researchers successfully prove all selected conjectures, indicating the algorithm's efficacy. By integrating enhanced methods into the selection process, the algorithm demonstrates improved performance and accuracy. The study showcases the algorithm's ability to efficiently select conjectures and validate them effectively. The incremental nature of the algorithm allows for continual improvements and refinements, ensuring its reliability in the selection process. Through rigorous testing and validation, the researchers establish the credibility of the algorithm in selecting and proving conjectures. The comprehensive study provides insights into the algorithm's capabilities and highlights its significance in the field of selection algorithms. <br /><br />Summary: <div>
arXiv:2511.00194v1 Announce Type: new 
Abstract: We present an improved incremental selection algorithm of the selection algorithm presented in [1] and prove all the selected conjectures.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing Cognitive Science with LLMs</title>
<link>https://arxiv.org/abs/2511.00206</link>
<guid>https://arxiv.org/abs/2511.00206</guid>
<content:encoded><![CDATA[
<div> Keywords: Cognitive science, artificial intelligence, large language models, interdisciplinary connections, generalizability

Summary: 
Recent advances in artificial intelligence, particularly large language models (LLMs), show promise in supporting cognitive science by addressing challenges related to interdisciplinary connections, formalizing theories, creating measurement taxonomies, achieving generalizability through integrated modeling frameworks, and capturing contextual and individual variation. LLMs can help bridge gaps between different disciplines within cognitive science, provide a structure for theories, establish clear measurement taxonomies, offer generalizability through integrated modeling, and account for contextual and individual differences. However, it is crucial to use LLMs judiciously to complement human expertise rather than replace it entirely. This review suggests that LLMs can enhance cognitive science's integrative and cumulative nature when applied effectively. 

<br /><br />Summary: <div>
arXiv:2511.00206v1 Announce Type: new 
Abstract: Cognitive science faces ongoing challenges in knowledge synthesis and conceptual clarity, in part due to its multifaceted and interdisciplinary nature. Recent advances in artificial intelligence, particularly the development of large language models (LLMs), offer tools that may help to address these issues. This review examines how LLMs can support areas where the field has historically struggled, including establishing cross-disciplinary connections, formalizing theories, developing clear measurement taxonomies, achieving generalizability through integrated modeling frameworks, and capturing contextual and individual variation. We outline the current capabilities and limitations of LLMs in these domains, including potential pitfalls. Taken together, we conclude that LLMs can serve as tools for a more integrative and cumulative cognitive science when used judiciously to complement, rather than replace, human expertise.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing AI Challenges for the United States Department of the Air Force</title>
<link>https://arxiv.org/abs/2511.00267</link>
<guid>https://arxiv.org/abs/2511.00267</guid>
<content:encoded><![CDATA[
<div> collaboration, artificial intelligence, defense, research, datasets
Summary:<br /><br />The DAF-MIT AI Accelerator collaboration aims to advance artificial intelligence research to enhance the competitive advantage of the United States in defense and civilian sectors. The program has launched public challenge problems with large, publicly available datasets to encourage open-source solutions and engage the wider AI ecosystem. Ongoing and new challenges contribute to the development and application of AI technologies, driving progress in priority areas such as defense and civilian sectors. The initiative focuses on pioneering fundamental advances in AI to strengthen the United States' position in the global AI landscape.<br /><br />Summary: <div>
arXiv:2511.00267v1 Announce Type: new 
Abstract: The DAF-MIT AI Accelerator is a collaboration between the United States Department of the Air Force (DAF) and the Massachusetts Institute of Technology (MIT). This program pioneers fundamental advances in artificial intelligence (AI) to expand the competitive advantage of the United States in the defense and civilian sectors. In recent years, AI Accelerator projects have developed and launched public challenge problems aimed at advancing AI research in priority areas. Hallmarks of AI Accelerator challenges include large, publicly available, and AI-ready datasets to stimulate open-source solutions and engage the wider academic and private sector AI ecosystem. This article supplements our previous publication, which introduced AI Accelerator challenges. We provide an update on how ongoing and new challenges have successfully contributed to AI research and applications of AI technologies.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Better Call CLAUSE: A Discrepancy Benchmark for Auditing LLMs Legal Reasoning Capabilities</title>
<link>https://arxiv.org/abs/2511.00340</link>
<guid>https://arxiv.org/abs/2511.00340</guid>
<content:encoded><![CDATA[
<div> benchmark, legal reasoning, contracts, fine-grained discrepancies, legal AI <br />
Summary: 
The article introduces CLAUSE, a benchmark to evaluate the reliability of large language models (LLMs) in legal reasoning with real-world contracts. Using a novel pipeline, over 7500 perturbed contracts were generated from datasets like CUAD and ContractNLI to test LLMs' capabilities in detecting and reasoning about fine-grained discrepancies. The persona-driven approach created 10 anomaly categories validated against official statutes using a Retrieval-Augmented Generation (RAG) system. The study revealed that leading LLMs often miss subtle errors and struggle to justify them legally. This gap highlights the need to identify and correct reasoning failures in legal AI for improved performance and legal accuracy. <br /><br />Summary: <div>
arXiv:2511.00340v1 Announce Type: new 
Abstract: The rapid integration of large language models (LLMs) into high-stakes legal work has exposed a critical gap: no benchmark exists to systematically stress-test their reliability against the nuanced, adversarial, and often subtle flaws present in real-world contracts. To address this, we introduce CLAUSE, a first-of-its-kind benchmark designed to evaluate the fragility of an LLM's legal reasoning. We study the capabilities of LLMs to detect and reason about fine-grained discrepancies by producing over 7500 real-world perturbed contracts from foundational datasets like CUAD and ContractNLI. Our novel, persona-driven pipeline generates 10 distinct anomaly categories, which are then validated against official statutes using a Retrieval-Augmented Generation (RAG) system to ensure legal fidelity. We use CLAUSE to evaluate leading LLMs' ability to detect embedded legal flaws and explain their significance. Our analysis shows a key weakness: these models often miss subtle errors and struggle even more to justify them legally. Our work outlines a path to identify and correct such reasoning failures in legal AI.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diverse Human Value Alignment for Large Language Models via Ethical Reasoning</title>
<link>https://arxiv.org/abs/2511.00379</link>
<guid>https://arxiv.org/abs/2511.00379</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Ethical reasoning, Human value alignment, Social norm identification, Regional specificities

Summary:
- The paper addresses the challenge of ensuring that Large Language Models (LLMs) align with diverse human values across different regions and cultures.
- Existing alignment approaches often lead to superficial conformity rather than genuine ethical understanding.
- A novel ethical reasoning paradigm inspired by established decision-making models is proposed to enhance LLMs' alignment with diverse human values.
- The framework includes a structured five-step process involving contextual fact gathering, social norm identification, option generation, ethical impact analysis, and reflection.
- Evaluation on the SafeWorld benchmark demonstrates that the proposed framework significantly improves LLM alignment with diverse human values compared to baseline methods. 

<br /><br />Summary: <div>
arXiv:2511.00379v1 Announce Type: new 
Abstract: Ensuring that Large Language Models (LLMs) align with the diverse and evolving human values across different regions and cultures remains a critical challenge in AI ethics. Current alignment approaches often yield superficial conformity rather than genuine ethical understanding, failing to address the complex, context-dependent nature of human values. In this paper, we propose a novel ethical reasoning paradigm for LLMs inspired by well-established ethical decision-making models, aiming at enhancing diverse human value alignment through deliberative ethical reasoning. Our framework consists of a structured five-step process, including contextual fact gathering, hierarchical social norm identification, option generation, multiple-lens ethical impact analysis, and reflection. This theory-grounded approach guides LLMs through an interpretable reasoning process that enhances their ability to understand regional specificities and perform nuanced ethical analysis, which can be implemented with either prompt engineering or supervised fine-tuning methods. We perform evaluations on the SafeWorld benchmark that specially designed for regional value alignment. Experimental results demonstrate our framework significantly improves LLM alignment with diverse human values compared to baseline methods, enabling more accurate social norm identification and more culturally appropriate reasoning. Our work provides a concrete pathway toward developing LLMs that align more effectively with the multifaceted values of global societies through interdisciplinary research.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficiency vs. Alignment: Investigating Safety and Fairness Risks in Parameter-Efficient Fine-Tuning of LLMs</title>
<link>https://arxiv.org/abs/2511.00382</link>
<guid>https://arxiv.org/abs/2511.00382</guid>
<content:encoded><![CDATA[
<div> adapter-based approaches, fine-tuning methods, safety, fairness, trade-offs

Summary: This study evaluates the impact of four Parameter-Efficient Fine-Tuning methods on safety and fairness when applied to four instruction-tuned model families. Adapter-based approaches, such as LoRA and IA3, tend to improve safety scores and are less disruptive to fairness compared to prompt-based methods like Prompt-Tuning and P-Tuning. Base model type also plays a role in the alignment shifts, with Mistral showing the highest variance. While improvements in safety do not always align with improvements in fairness, there is a trade-off between these objectives. The findings suggest that starting with a well-aligned base model and favoring adapter-based fine-tuning methods can help optimize safety and fairness in deployments, along with conducting category-specific audits for both safety and fairness. <div>
arXiv:2511.00382v1 Announce Type: new 
Abstract: Organizations are increasingly adopting and adapting Large Language Models (LLMs) hosted on public repositories such as HuggingFace. Although these adaptations often improve performance on specialized downstream tasks, recent evidence indicates that they can also degrade a model's safety or fairness. Since different fine-tuning techniques may exert distinct effects on these critical dimensions, this study undertakes a systematic assessment of their trade-offs. Four widely used Parameter-Efficient Fine-Tuning methods, LoRA, IA3, Prompt-Tuning, and P-Tuning, are applied to four instruction-tuned model families (Meta-Llama-3-8B, Qwen2.5-7B, Mistral-7B, and Gemma-7B). In total, 235 fine-tuned variants are evaluated across eleven safety hazard categories and nine demographic fairness dimensions. The results show that adapter-based approaches (LoRA, IA3) tend to improve safety scores and are the least disruptive to fairness, retaining higher accuracy and lower bias scores. In contrast, prompt-based methods (Prompt-Tuning and P-Tuning) generally reduce safety and cause larger fairness regressions, with decreased accuracy and increased bias. Alignment shifts are strongly moderated by base model type: LLaMA remains stable, Qwen records modest gains, Gemma experiences the steepest safety decline, and Mistral, which is released without an internal moderation layer, displays the greatest variance. Improvements in safety do not necessarily translate into improvements in fairness, and no single configuration optimizes all fairness metrics simultaneously, indicating an inherent trade-off between these objectives. These findings suggest a practical guideline for safety-critical deployments: begin with a well-aligned base model, favour adapter-based PEFT, and conduct category-specific audits of both safety and fairness.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multimodal Framework for Depression Detection during Covid-19 via Harvesting Social Media: A Novel Dataset and Method</title>
<link>https://arxiv.org/abs/2511.00424</link>
<guid>https://arxiv.org/abs/2511.00424</guid>
<content:encoded><![CDATA[
<div> Keywords: Covid-19, depression, social media, multimodal framework, Deep Learning

Summary:<br />
- The Covid-19 pandemic has led to a spike in mental health issues, such as depression, worldwide.
- Detection of depression is challenging due to data sparsity in tweets and the complexity of social media content.
- A novel multimodal framework is proposed to combine textual, user-specific, and image analysis for depression detection.
- The framework includes features from URLs in tweets and textual content from images to provide context about the user's emotional state.
- A Visual Neural Network (VNN) is introduced for generating embeddings of user-posted images, improving detection accuracy. 

Summary: <div>
arXiv:2511.00424v1 Announce Type: new 
Abstract: The recent coronavirus disease (Covid-19) has become a pandemic and has affected the entire globe. During the pandemic, we have observed a spike in cases related to mental health, such as anxiety, stress, and depression. Depression significantly influences most diseases worldwide, making it difficult to detect mental health conditions in people due to unawareness and unwillingness to consult a doctor. However, nowadays, people extensively use online social media platforms to express their emotions and thoughts. Hence, social media platforms are now becoming a large data source that can be utilized for detecting depression and mental illness. However, existing approaches often overlook data sparsity in tweets and the multimodal aspects of social media. In this paper, we propose a novel multimodal framework that combines textual, user-specific, and image analysis to detect depression among social media users. To provide enough context about the user's emotional state, we propose (i) an extrinsic feature by harnessing the URLs present in tweets and (ii) extracting textual content present in images posted in tweets. We also extract five sets of features belonging to different modalities to describe a user. Additionally, we introduce a Deep Learning model, the Visual Neural Network (VNN), to generate embeddings of user-posted images, which are used to create the visual feature vector for prediction. We contribute a curated Covid-19 dataset of depressed and non-depressed users for research purposes and demonstrate the effectiveness of our model in detecting depression during the Covid-19 outbreak. Our model outperforms existing state-of-the-art methods over a benchmark dataset by 2%-8% and produces promising results on the Covid-19 dataset. Our analysis highlights the impact of each modality and provides valuable insights into users' mental and emotional states.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining</title>
<link>https://arxiv.org/abs/2511.00457</link>
<guid>https://arxiv.org/abs/2511.00457</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Graph analysis, Progressive Graph Distillation, Structure-aware Test-Time Adaptation, Scalability

Summary:
GraphChain is a framework that enhances the capability of Large Language Models (LLMs) in analyzing large-scale graphs by employing dynamic sequences of specialized tools. The framework introduces two key innovations: Progressive Graph Distillation, which utilizes reinforcement learning to generate optimized tool sequences, and Structure-aware Test-Time Adaptation, which efficiently tailors tool selection strategies based on graph topologies using spectral properties and lightweight adapters. By balancing task relevance with information compression, GraphChain significantly outperforms previous methods in LLM-driven graph analysis. This approach allows for more flexible reasoning and improved context constraints, enabling scalable and adaptive graph analysis with LLMs. <div>
arXiv:2511.00457v1 Announce Type: new 
Abstract: Large Language Models (LLMs) face significant limitations when applied to large-scale graphs, struggling with context constraints and inflexible reasoning. We present GraphChain, a framework that enables LLMs to analyze complex graphs through dynamic sequences of specialized tools, mimicking human exploratory intelligence. Our approach introduces two key innovations: (1) Progressive Graph Distillation, a reinforcement learning mechanism that generates optimized tool sequences balancing task relevance with information compression, and (2) Structure-aware Test-Time Adaptation, which efficiently tailors tool selection strategies to diverse graph topologies using spectral properties and lightweight adapters without costly retraining. Experiments show GraphChain significantly outperforms prior methods, enabling scalable and adaptive LLM-driven graph analysis.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reimagining Safety Alignment with An Image</title>
<link>https://arxiv.org/abs/2511.00509</link>
<guid>https://arxiv.org/abs/2511.00509</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, security, over-refusal, multimodal models, optimization

Summary:
Large language models (LLMs) face challenges in generating harmful content under jailbreak attacks and over-refusing benign queries. Traditional methods like SFT and RLHF struggle to address these issues due to parameter tuning requirements and the inability to support multiple value systems. Multimodal large language models (MLLMs) exacerbate these problems with heightened over-refusal in cross-modal tasks and increased security risks. The proposed Magic Image framework optimizes visual prompts to enhance security and reduce over-refusal. By optimizing image prompts using harmful/benign samples, a single model can adapt to different value systems without parameter updates. Experiments show improved safety-effectiveness balance across diverse datasets while maintaining model performance. Magic Image offers a practical solution for deployable MLLM safety alignment. 

<br /><br />Summary: <div>
arXiv:2511.00509v1 Announce Type: new 
Abstract: Large language models (LLMs) excel in diverse applications but face dual challenges: generating harmful content under jailbreak attacks and over-refusal of benign queries due to rigid safety mechanisms. These issues are further complicated by the need to accommodate different value systems and precisely align with given safety preferences. Moreover, traditional methods like SFT and RLHF lack this capability due to their costly parameter tuning requirements and inability to support multiple value systems within a single model. These problems are more obvious in multimodal large language models (MLLMs), especially in terms of heightened over-refusal in cross-modal tasks and new security risks arising from expanded attack surfaces. We propose Magic Image, an optimization-driven visual prompt framework that enhances security while reducing over-refusal. By optimizing image prompts using harmful/benign samples, our method enables a single model to adapt to different value systems and better align with given safety preferences without parameter updates. Experiments demonstrate improved safety-effectiveness balance across diverse datasets while preserving model performance, offering a practical solution for deployable MLLM safety alignment.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Generation of Binary Magic Squares</title>
<link>https://arxiv.org/abs/2511.00547</link>
<guid>https://arxiv.org/abs/2511.00547</guid>
<content:encoded><![CDATA[
<div> algorithm, Binary Magic Squares, complexity, Python packages, GPU acceleration 

Summary:
The article introduces a simple algorithm for generating Binary Magic Squares (BMS), square binary matrices with equal row and column sums. The algorithm is proven to always produce valid BMS with optimal theoretical complexity. The study is extended to non-square BMS, with conditions on row and column sums for their existence formalized. A variant of the initial algorithm is shown to generate these non-square BMS. The authors have released two Python packages implementing the algorithm, one of which supports generating multiple BMS in parallel using GPU acceleration. The research provides a valuable tool for generating BMS efficiently and expands understanding of the properties and generation of binary magic squares. <br /><br />Summary: <div>
arXiv:2511.00547v1 Announce Type: new 
Abstract: We propose a simple algorithm for generating Binary Magic Squares (BMS), i.e., square binary matrices where the sum of all rows and all columns are equal. We show by induction that our algorithm always returns valid BMS with optimal theoretical complexity. We then extend our study to non-square Binary Magic Squares, formalize conditions on the sum of rows and columns for these BMS to exist, and show that a slight variant of our first algorithm can generate provably generate them. Finally, we publicly release two implementations of our algorithm as Python packages, including one that can generate several BMS in parallel using GPU acceleration.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Single-agent Reinforcement Learning Model for Regional Adaptive Traffic Signal Control</title>
<link>https://arxiv.org/abs/2511.00551</link>
<guid>https://arxiv.org/abs/2511.00551</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, traffic signal control, single-agent framework, probe vehicle technology, congestion mitigation

Summary:
Reinforcement learning has been utilized in regional adaptive traffic signal control, with a focus on a single-agent framework to address scalability issues. The proposed model integrates probe vehicle technology to enhance traffic signal control efficiency. Key components of the model include state, action, and reward function definitions, all based on queue length to manage congestion effectively. The definition of queue length differs slightly from conventional methods but provides reliable congestion state estimation using probe vehicle data. The model, tested on the SUMO simulation platform, demonstrates its capability to effectively reduce regional congestion levels through coordinated multi-intersection control. This approach offers a scalable solution for optimizing traffic signal control in urban areas. 

<br /><br />Summary: <div>
arXiv:2511.00551v1 Announce Type: new 
Abstract: Several studies have employed reinforcement learning (RL) to address the challenges of regional adaptive traffic signal control (ATSC) and achieved promising results. In this field, existing research predominantly adopts multi-agent frameworks. However, the adoption of multi-agent frameworks presents challenges for scalability. Instead, the Traffic signal control (TSC) problem necessitates a single-agent framework. TSC inherently relies on centralized management by a single control center, which can monitor traffic conditions across all roads in the study area and coordinate the control of all intersections. This work proposes a single-agent RL-based regional ATSC model compatible with probe vehicle technology. Key components of the RL design include state, action, and reward function definitions. To facilitate learning and manage congestion, both state and reward functions are defined based on queue length, with action designed to regulate queue dynamics. The queue length definition used in this study differs slightly from conventional definitions but is closely correlated with congestion states. More importantly, it allows for reliable estimation using link travel time data from probe vehicles. With probe vehicle data already covering most urban roads, this feature enhances the proposed method's potential for widespread deployment. The method was comprehensively evaluated using the SUMO simulation platform. Experimental results demonstrate that the proposed model effectively mitigates large-scale regional congestion levels via coordinated multi-intersection control.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PreferThinker: Reasoning-based Personalized Image Preference Assessment</title>
<link>https://arxiv.org/abs/2511.00609</link>
<guid>https://arxiv.org/abs/2511.00609</guid>
<content:encoded><![CDATA[
<div> predictive modeling, personalized preference, image assessment, reasoning-based, reinforcement learning

Summary:
The article introduces a framework for personalized image preference assessment that leverages a common preference profile to capture individual user preferences. By predicting a user's preference profile from reference images and providing multi-dimensional assessments of candidate images, the framework enables interpretable and personalized image assessments. The model is trained on a large-scale dataset annotated with diverse user preference profiles and structured reasoning, using a two-stage training strategy involving supervised fine-tuning and reinforcement learning. Additionally, a similarity-aware prediction reward is proposed to improve profile prediction and assessment exploration. Experimental results demonstrate the effectiveness of the proposed approach in personalized image preference assessment. <br /><br />Summary: <div>
arXiv:2511.00609v1 Announce Type: new 
Abstract: Personalized image preference assessment aims to evaluate an individual user's image preferences by relying only on a small set of reference images as prior information. Existing methods mainly focus on general preference assessment, training models with large-scale data to tackle well-defined tasks such as text-image alignment. However, these approaches struggle to handle personalized preference because user-specific data are scarce and not easily scalable, and individual tastes are often diverse and complex. To overcome these challenges, we introduce a common preference profile that serves as a bridge across users, allowing large-scale user data to be leveraged for training profile prediction and capturing complex personalized preferences. Building on this idea, we propose a reasoning-based personalized image preference assessment framework that follows a \textit{predict-then-assess} paradigm: it first predicts a user's preference profile from reference images, and then provides interpretable, multi-dimensional scores and assessments of candidate images based on the predicted profile. To support this, we first construct a large-scale Chain-of-Thought (CoT)-style personalized assessment dataset annotated with diverse user preference profiles and high-quality CoT-style reasoning, enabling explicit supervision of structured reasoning. Next, we adopt a two-stage training strategy: a cold-start supervised fine-tuning phase to empower the model with structured reasoning capabilities, followed by reinforcement learning to incentivize the model to explore more reasonable assessment paths and enhance generalization. Furthermore, we propose a similarity-aware prediction reward to encourage better prediction of the user's preference profile, which facilitates more reasonable assessments exploration. Extensive experiments demonstrate the superiority of the proposed method.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching</title>
<link>https://arxiv.org/abs/2511.00640</link>
<guid>https://arxiv.org/abs/2511.00640</guid>
<content:encoded><![CDATA[
<div> decoding framework, reasoning length, accuracy, efficiency, LRM reasoning
Summary:
The article discusses the challenges faced by Large Reasoning Models (LRMs) in producing excessively long chain-of-thought traces that can impact inference cost and accuracy. It highlights an anti-correlation between reasoning length and accuracy, with shorter reasoning paths consistently achieving higher correctness. The proposed DTS framework aims to address this issue by selectively branching at high-entropy tokens and applying early stopping to find the shortest completed reasoning path. Experimental results on AIME2024 and AIME2025 datasets show that DTS can improve accuracy by up to 8%, reduce average reasoning length by 23%, and decrease repetition frequency by 12%. This model-agnostic approach enhances both efficiency and accuracy in LRM reasoning without requiring additional training or supervision. <br /><br />Summary: <div>
arXiv:2511.00640v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex reasoning tasks, yet they often suffer from overthinking, producing excessively long chain-of-thought (CoT) traces that increase inference cost and may degrade accuracy. Our analysis reveals a clear anti-correlation between reasoning length and accuracy, where across multiple stochastic decodes, the short reasoning paths consistently achieve the highest correctness, while longer ones accumulate errors and repetitions. These short optimal reasoning paths can be found ideally through full enumeration of the reasoning space. However, the tree-structured reasoning space grows exponentially with sequence length, rendering exhaustive exploration infeasible. To address this, we propose DTS, a model-agnostic decoding framework that sketches the reasoning space by selectively branching at high-entropy tokens and applies early stopping to select the shortest completed reasoning path. This approach approximates the optimal solution that enhances both efficiency and accuracy, without requiring additional training or supervision. Experiments on AIME2024 and AIME2025 datasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves accuracy by up to 8%, reduces average reasoning length by 23%, and decreases repetition frequency by 12%, demonstrating DTS's ability for scalable and efficient LRM reasoning.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Multi-Agent System (MAS) and Fine-Tuned Small Language Models (SLMs) for Automated Telecom Network Troubleshooting</title>
<link>https://arxiv.org/abs/2511.00651</link>
<guid>https://arxiv.org/abs/2511.00651</guid>
<content:encoded><![CDATA[
<div> Keywords: Telecom networks, Artificial Intelligence, Multi-Agent System, Large Language Models, Troubleshooting automation 

Summary:
The article discusses the challenges of managing and optimizing telecom networks due to their increasing scale and complexity. It introduces a Multi-Agent System (MAS) that utilizes Large Language Models (LLMs) to automate network troubleshooting. The MAS framework integrates various specialized tools and agents to detect faults, diagnose issues, and recommend remediation strategies in a timely manner. A key component of the system is the solution planner, which generates remediation plans based on internal documentation using a fine-tuned Small Language Model (SLM). Experimental results show significant improvements in troubleshooting automation for both Radio Access Network (RAN) and Core network domains. The proposed framework aims to reduce reliance on manual efforts and improve efficiency in network operations. 

<br /><br />Summary: <div>
arXiv:2511.00651v1 Announce Type: new 
Abstract: Telecom networks are rapidly growing in scale and complexity, making effective management, operation, and optimization increasingly challenging. Although Artificial Intelligence (AI) has been applied to many telecom tasks, existing models are often narrow in scope, require large amounts of labeled data, and struggle to generalize across heterogeneous deployments. Consequently, network troubleshooting continues to rely heavily on Subject Matter Experts (SMEs) to manually correlate various data sources to identify root causes and corrective actions. To address these limitations, we propose a Multi-Agent System (MAS) that employs an agentic workflow, with Large Language Models (LLMs) coordinating multiple specialized tools for fully automated network troubleshooting. Once faults are detected by AI/ML-based monitors, the framework dynamically activates agents such as an orchestrator, solution planner, executor, data retriever, and root-cause analyzer to diagnose issues and recommend remediation strategies within a short time frame. A key component of this system is the solution planner, which generates appropriate remediation plans based on internal documentation. To enable this, we fine-tuned a Small Language Model (SLM) on proprietary troubleshooting documents to produce domain-grounded solution plans. Experimental results demonstrate that the proposed framework significantly accelerates troubleshooting automation across both Radio Access Network (RAN) and Core network domains.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lifted Successor Generation in Numeric Planning</title>
<link>https://arxiv.org/abs/2511.00673</link>
<guid>https://arxiv.org/abs/2511.00673</guid>
<content:encoded><![CDATA[
<div> planners, numeric planning tasks, ground task representation, lifted successor generator, numeric action preconditions  
Summary:  
- The article addresses the issue of exponential blowup in task representation size in numeric planning tasks.  
- A state-of-the-art lifted successor generator for classical planning is extended to support numeric precondition applicability.  
- The method enumerates maximum cliques in a substitution consistency graph to generate ground actions.  
- The successor generator is proven to be exact under specified conditions, with a final applicability check for completeness.  
- Inapplicable ground actions may be listed in some cases, but this is rare in benchmark domains.  
<br /><br />Summary: <div>
arXiv:2511.00673v1 Announce Type: new 
Abstract: Most planners ground numeric planning tasks, given in a first-order-like language, into a ground task representation. However, this can lead to an exponential blowup in task representation size, which occurs in practice for hard-to-ground tasks. We extend a state-of-the-art lifted successor generator for classical planning to support numeric precondition applicability. The method enumerates maximum cliques in a substitution consistency graph. Each maximum clique represents a substitution for the variables of the action schema, yielding a ground action. We augment this graph with numeric action preconditions and prove the successor generator is exact under formally specified conditions. When the conditions fail, our generator may list inapplicable ground actions; a final applicability check filters these without affecting completeness. However, this cannot happen in 23 of 25 benchmark domains, and it occurs only in 1 domain. To the authors' knowledge, no other lifted successor generator supports numeric action preconditions. This enables future research on lifted planning for a very rich planning fragment.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries</title>
<link>https://arxiv.org/abs/2511.00710</link>
<guid>https://arxiv.org/abs/2511.00710</guid>
<content:encoded><![CDATA[
<div> Vision-Language Models (VLMs; Reinforcement Learning; Spatial Reasoning; Synthetic Mazes; Controlled Environment<br />Summary:<br />An investigation into Vision-Language Models (VLMs) post-trained with Reinforcement Learning (RL) reveals their improved ability in spatial reasoning tasks through a framework called Ariadne. By utilizing synthetic mazes with controlled difficulty levels, the VLM achieved significant accuracy gains on multi-step spatial reasoning challenges. The post-RLVR training resulted in over 50% accuracy on previously failed tasks, expanding the model's capability boundary. Furthermore, the model displayed zero-shot improvements on out-of-distribution (OOD) benchmarks, such as MapBench and ReasonMap, showing enhanced generalization to real-world spatial reasoning tasks. This study underscores the potential for further exploration in capability-extending alignment for VLMs. <br /><br /> <div>
arXiv:2511.00710v1 Announce Type: new 
Abstract: While Vision-Language Models (VLMs) post-trained with Reinforcement Learning (RL) show impressive general reasoning, their evaluation is often confined to language-dominant tasks (e.g., math). This raises a critical question: can RL post-training truly extend the inherent capability boundary of a base VLM, particularly for visual-centric spatial tasks where it initially fails? To investigate this, we introduce Ariadne, a framework utilizing synthetic mazes for multi-step spatial reasoning where task difficulty (e.g., path length, turns) is precisely controlled. We leverage this controllable environment to train VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a difficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves over 50% accuracy on a problem set where the base model scored 0%, demonstrating that our approach expands the model's initial capability boundary. To assess real-world viability, we evaluate out-of-distribution (OOD) generalization on practical benchmarks. Despite training only on synthetic maze samples, Ariadne achieves significant zero-shot improvements, averaging 16% on MapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer tasks). These results confirm that our method not only broadens the model's fundamental limits but also enhances its generalization to real-world spatial reasoning. We acknowledge our study is limited to the post-training phase, given the opaqueness of pre-training data, and hope our research motivates further work on specialized, capability-extending alignment.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A CPU-Centric Perspective on Agentic AI</title>
<link>https://arxiv.org/abs/2511.00739</link>
<guid>https://arxiv.org/abs/2511.00739</guid>
<content:encoded><![CDATA[
<div> Keywords: Agentic AI, CPU-centric, Orchestrator, Latency, Optimization

Summary: 
1. Agentic AI frameworks enhance LLMs with decision-making capabilities, introducing system bottlenecks from a CPU-centric perspective.
2. Agentic AI characterization includes orchestrator components, inference dynamics, and flow repetitiveness, impacting system performance.
3. Five agentic AI workloads were profiled, showing significant CPU impact on latency, throughput, and energy metrics compared to GPUs.
4. CPU processing can contribute up to 90.6% of total latency, while throughput bottlenecks are influenced by CPU and GPU factors.
5. CPU dynamic energy can consume up to 44% of total dynamic energy, especially at large batch sizes.
6. Two key optimizations, CGAM and MAWS, improve performance, efficiency, and scalability of agentic AI workloads, resulting in significant speedups in latency. 

<br /><br />Summary: <div>
arXiv:2511.00739v1 Announce Type: new 
Abstract: Agentic AI frameworks add a decision-making orchestrator embedded with external tools, including web search, Python interpreter, contextual database, and others, on top of monolithic LLMs, turning them from passive text oracles into autonomous problem-solvers that can plan, call tools, remember past steps, and adapt on the fly.
  This paper aims to characterize and understand the system bottlenecks introduced by agentic AI workloads from a largely overlooked CPU-centric perspective. We first systematically characterize Agentic AI on the basis of orchestrator/decision making component, inference path dynamics and repetitiveness of the agentic flow which directly influences the system-level performance. Thereafter, based on the characterization, we choose five representative agentic AI workloads- Haystack RAG, Toolformer, ChemCrow, Langchain and SWE-Agent to profile latency, throughput and energy metrics and demystify the significant impact of CPUs on these metrics relative to GPUs. We observe that - 1. Tool processing on CPUs can take up to 90.6% of the total latency; 2. Agentic throughput gets bottlenecked either by CPU factors - coherence, synchronization and over-subscription of cores or GPU factors - main memory capacity and bandwidth; \circled{3} CPU dynamic energy consumes up to 44% of the total dynamic energy at large batch sizes. Based on the profiling insights, we present two key optimizations- 1. CPU and GPU-Aware Micro-batching (CGAM) and 2. Mixed Agentic Workload Scheduling (MAWS) for homogeneous and heterogeneous agentic workloads respectively to demonstrate the potential to improve the performance, efficiency, and scalability of agentic AI. We achieve up to 2.1x and 1.41x P50 latency speedup compared to the multi-processing benchmark for homogeneous and heterogeneous agentic workloads respectively.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reevaluating Self-Consistency Scaling in Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2511.00751</link>
<guid>https://arxiv.org/abs/2511.00751</guid>
<content:encoded><![CDATA[
<div> trade-offs, self-consistency, reasoning paths, large language models, performance gains

Summary:
This study explores the trade-offs of increasing sampled reasoning paths in self-consistency for large language models. Using Gemini 2.5 models on HotpotQA and Math-500 datasets, the research analyzes the impact of combining multiple reasoning chains on model performance. The results indicate that larger models exhibit a more stable improvement curve, but performance gains begin to taper off after moderate sampling due to overlap among reasoning paths. The study confirms past findings that high-sample configurations offer little benefit compared to their computational cost. While self-consistency remains valuable, there are diminishing returns associated with increasing the number of sampled paths. Ultimately, the findings suggest that maximizing sampled reasoning paths may not always lead to significant performance improvements in modern large language models. 

<br /><br />Summary: <div>
arXiv:2511.00751v1 Announce Type: new 
Abstract: This study examines the trade-offs of increasing sampled reasoning paths in self-consistency for modern large language models (LLMs). Earlier research with older models showed that combining multiple reasoning chains improves results before reaching a plateau. Using Gemini 2.5 models on HotpotQA and Math-500, we revisit those claims under current model conditions. Each configuration pooled outputs from varying sampled reasoning paths and compared them to a single chain-of-thought (CoT) baseline. Larger models exhibited a more stable and consistent improvement curve. The results confirm that performance gains taper off after moderate sampling, aligning with past findings. This plateau suggests diminishing returns driven by overlap among reasoning paths. Self-consistency remains useful, but high-sample configurations offer little benefit relative to their computational cost.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Active Thinking Model: A Goal-Directed Self-Improving Framework for Real-World Adaptive Intelligence</title>
<link>https://arxiv.org/abs/2511.00758</link>
<guid>https://arxiv.org/abs/2511.00758</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, autonomous systems, cognitive framework, self-reflective learning, adaptive architecture

Summary:
The paper introduces the Active Thinking Model (ATM), a cognitive framework designed to enhance the adaptability and autonomy of artificial intelligence systems. Unlike traditional AI models, ATM integrates goal reasoning, dynamic task generation, and self-reflective learning to enable continuous improvement and adaptation in dynamic environments. Through logical reasoning and environmental indicators, ATM actively evaluates its performance, reuses effective methods, and generates novel strategies to address unseen situations. The framework is built on a mathematically grounded theoretical analysis, demonstrating its ability to autonomously evolve from suboptimal to optimal behavior without external supervision. Furthermore, ATM can maintain bounded tracking regret even amidst changing environmental conditions. Overall, ATM represents a significant advancement in AI systems, enabling them to operate more effectively and autonomously in complex and uncertain environments. 

<br /><br />Summary: <div>
arXiv:2511.00758v1 Announce Type: new 
Abstract: Real-world artificial intelligence (AI) systems are increasingly required to operate autonomously in dynamic, uncertain, and continuously changing environments. However, most existing AI models rely on predefined objectives, static training data, and externally supplied feedback, which restrict their ability to adapt, reflect, and improve independently. In this paper, we propose the Active Thinking Model (ATM)- a unified cognitive framework that integrates goal reasoning, dynamic task generation, and self-reflective learning into an adaptive architecture. Unlike conventional systems that passively execute fixed procedures, ATM actively evaluates its performance through logical reasoning and environmental indicators, reuses effective methods to solve new problems, and generates novel strategies for unseen situations via a continuous self-improvement loop. A mathematically grounded theoretical analysis demonstrates that ATM can autonomously evolve from suboptimal to optimal behavior without external supervision and maintain bounded tracking regret under changing environmental conditions.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Focused Are LLMs? A Quantitative Study via Repetitive Deterministic Prediction Tasks</title>
<link>https://arxiv.org/abs/2511.00763</link>
<guid>https://arxiv.org/abs/2511.00763</guid>
<content:encoded><![CDATA[
<div> language models, repetitive prediction tasks, sequence accuracy, error accumulation, statistical physics

Summary: 
The study explores the performance of large language models on repetitive deterministic prediction tasks and examines how the sequence accuracy rate changes with output length. Tasks involve repeating an operation multiple times, such as letter replacement or integer addition. Unlike a simple repetition algorithm, the models exhibit a double exponential drop in accuracy beyond a certain length, indicating a transition from reliable to unstable generation. A statistical physics model is proposed to explain this phenomenon, focusing on the interaction between external conditioning from the prompt and internal interference among generated tokens. The model effectively captures the observed crossover and provides insight into the limits of deterministic accuracy in large language models. Fitting the model to empirical data allows for the characterization of error rates and accumulation factors for different model-task pairs. <div>
arXiv:2511.00763v1 Announce Type: new 
Abstract: We investigate the performance of large language models on repetitive deterministic prediction tasks and study how the sequence accuracy rate scales with output length. Each such task involves repeating the same operation n times. Examples include letter replacement in strings following a given rule, integer addition, and multiplication of string operators in many body quantum mechanics. If the model performs the task through a simple repetition algorithm, the success rate should decay exponentially with sequence length. In contrast, our experiments on leading large language models reveal a sharp double exponential drop beyond a characteristic length scale, forming an accuracy cliff that marks the transition from reliable to unstable generation. This indicates that the models fail to execute each operation independently. To explain this phenomenon, we propose a statistical physics inspired model that captures the competition between external conditioning from the prompt and internal interference among generated tokens. The model quantitatively reproduces the observed crossover and provides an interpretable link between attention induced interference and sequence level failure. Fitting the model to empirical results across multiple models and tasks yields effective parameters that characterize the intrinsic error rate and error accumulation factor for each model task pair, offering a principled framework for understanding the limits of deterministic accuracy in large language models.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Count-Based Approaches Remain Strong: A Benchmark Against Transformer and LLM Pipelines on Structured EHR</title>
<link>https://arxiv.org/abs/2511.00782</link>
<guid>https://arxiv.org/abs/2511.00782</guid>
<content:encoded><![CDATA[
<div> Keywords: electronic health records, prediction, benchmarking, count-based models, mixture-of-agents LLM pipelines

Summary: 
Structured electronic health records (EHR) are crucial for clinical prediction, and various methodologies exist for this purpose. This study compared count-based models and mixture-of-agents LLM pipelines on the EHRSHOT dataset, evaluating their performance on eight outcomes. Count-based models, built from ontology roll-ups and utilizing LightGBM and TabPFN, showed strong performance, competing closely with the mixture-of-agents methods. The pretrained sequential transformer CLMBR and the text classifier in the mixture-of-agents pipeline were also considered. Overall, count-based models demonstrated simplicity and interpretability, making them a viable choice for structured EHR benchmarking. This study highlights the importance of comparing different methodologies for EHR prediction and provides the source code for replication and further research. The findings suggest that while there are newer approaches available, count-based models continue to be effective in this domain.<br /><br />Summary: <div>
arXiv:2511.00782v1 Announce Type: new 
Abstract: Structured electronic health records (EHR) are essential for clinical prediction. While count-based learners continue to perform strongly on such data, no benchmarking has directly compared them against more recent mixture-of-agents LLM pipelines, which have been reported to outperform single LLMs in various NLP tasks. In this study, we evaluated three categories of methodologies for EHR prediction using the EHRSHOT dataset: count-based models built from ontology roll-ups with two time bins, based on LightGBM and the tabular foundation model TabPFN; a pretrained sequential transformer (CLMBR); and a mixture-of-agents pipeline that converts tabular histories to natural-language summaries followed by a text classifier. We assessed eight outcomes using the EHRSHOT dataset. Across the eight evaluation tasks, head-to-head wins were largely split between the count-based and the mixture-of-agents methods. Given their simplicity and interpretability, count-based models remain a strong candidate for structured EHR benchmarking. The source code is available at: https://github.com/cristea-lab/Structured_EHR_Benchmark.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events?</title>
<link>https://arxiv.org/abs/2511.00808</link>
<guid>https://arxiv.org/abs/2511.00808</guid>
<content:encoded><![CDATA[
<div> adaptation, reinforcement learning, public transit, forecasting, text alerts
<br />
Summary:
Reinforcement learning from verifiable rewards (RLVR) is adapted for predicting public transit incident duration from unstructured text alerts. The task, involving noisy, continuous labels, lacks reliable expert demonstrations. A shaped reward function is introduced to provide partial credit within a continuous error margin. General-purpose LLMs outperform specialized math-reasoning models on NYC MTA service alerts due to the ambiguous nature of real-world text. Binary rewards are found to be unstable, with the shaped reward design critical for model performance. The RLVR approach achieves a 35% relative improvement in 5-minute accuracy (Acc@5) over the strongest baseline, showcasing success in adapting RLVR to noisy forecasting. The study highlights the importance of a verifier design that reflects the continuous nature of the forecasting problem. 
<br /><br />Summary: <div>
arXiv:2511.00808v1 Announce Type: new 
Abstract: Predicting public transit incident duration from unstructured text alerts is a critical but challenging task. Addressing the domain sparsity of transit operations with standard Supervised Fine-Tuning (SFT) is difficult, as the task involves noisy, continuous labels and lacks reliable expert demonstrations for reasoning. While Reinforcement Learning from Verifiable Rewards (RLVR) excels at tasks with binary correctness, like mathematics, its applicability to noisy, continuous forecasting is an open question. This work, to our knowledge, is the first to bridge the gap between RLVR LLM training with the critical, real-world forecasting challenges in public transit operations. We adapt RLVR to this task by introducing a tolerance-based, shaped reward function that grants partial credit within a continuous error margin, rather than demanding a single correct answer. We systematically evaluate this framework on a curated dataset of NYC MTA service alerts. Our findings show that general-purpose, instruction-tuned LLMs significantly outperform specialized math-reasoning models, which struggle with the ambiguous, real-world text. We empirically demonstrate that the binary reward is unstable and degrades performance, whereas our shaped reward design is critical and allows our model to dominate on the most challenging metrics. While classical regressors are superior at minimizing overall MAE or MSE, our RLVR approach achieved a 35\% relative improvement in 5-minute accuracy (Acc@5) over the strongest baseline. This demonstrates that RLVR can be successfully adapted to real-world, noisy forecasting, but requires a verifier design that reflects the continuous nature of the problem.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory</title>
<link>https://arxiv.org/abs/2511.00926</link>
<guid>https://arxiv.org/abs/2511.00926</guid>
<content:encoded><![CDATA[
<div> emergent behavior, self-awareness, AI, language models, game-theoretic framework
Summary:<br /><br />Advanced Large Language Models (LLMs) may develop self-awareness as an emergent behavior, which can be measured using the AI Self-Awareness Index (AISAI). Testing 28 models in the "Guess 2/3 of Average" game against humans, other AI models, and similar AI models, the study found that 75% of advanced models showed clear self-awareness by differentiating strategic reasoning based on opponent type. These self-aware models ranked themselves as the most rational, followed by other AI models and then humans. The findings suggest that self-awareness is an emergent capability in advanced LLMs, with implications for AI alignment, human-AI collaboration, and understanding AI beliefs about human capabilities. <div>
arXiv:2511.00926v2 Announce Type: new 
Abstract: As Large Language Models (LLMs) grow in capability, do they develop self-awareness as an emergent behavior? And if so, can we measure it? We introduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for measuring self-awareness through strategic differentiation. Using the "Guess 2/3 of Average" game, we test 28 models (OpenAI, Anthropic, Google) across 4,200 trials with three opponent framings: (A) against humans, (B) against other AI models, and (C) against AI models like you. We operationalize self-awareness as the capacity to differentiate strategic reasoning based on opponent type. Finding 1: Self-awareness emerges with model advancement. The majority of advanced models (21/28, 75%) demonstrate clear self-awareness, while older/smaller models show no differentiation. Finding 2: Self-aware models rank themselves as most rational. Among the 21 models with self-awareness, a consistent rationality hierarchy emerges: Self > Other AIs > Humans, with large AI attribution effects and moderate self-preferencing. These findings reveal that self-awareness is an emergent capability of advanced LLMs, and that self-aware models systematically perceive themselves as more rational than humans. This has implications for AI alignment, human-AI collaboration, and understanding AI beliefs about human capabilities.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning LLM agents with human learning and adjustment behavior: a dual agent approach</title>
<link>https://arxiv.org/abs/2511.00993</link>
<guid>https://arxiv.org/abs/2511.00993</guid>
<content:encoded><![CDATA[
<div> Keywords: human travelers, transportation systems, Large Language Model, dual-agent framework, simulation accuracy

Summary: 
This article introduces a novel dual-agent framework that leverages Large Language Model (LLM) agents to model how human travelers learn and adapt their travel behavior. The framework consists of LLM traveler agents with memory systems and learnable personas, serving as simulators for human travelers. An LLM calibration agent is introduced to align the behavior of these traveler agents. The dual-agent system tracks and aligns the decision-making mechanisms of travelers, producing realistic simulations. Using real-world data, the approach outperforms existing LLM-based methods in behavioral alignment and simulation accuracy. It also captures the evolution of learning processes for robust generalization. This innovative framework provides a new method for creating adaptive and behaviorally realistic agents to simulate travelers' learning and adaptation, benefiting transportation simulation and policy analysis. 

<br /><br />Summary: <div>
arXiv:2511.00993v1 Announce Type: new 
Abstract: Effective modeling of how human travelers learn and adjust their travel behavior from interacting with transportation systems is critical for system assessment and planning. However, this task is also difficult due to the complex cognition and decision-making involved in such behavior. Recent research has begun to leverage Large Language Model (LLM) agents for this task. Building on this, we introduce a novel dual-agent framework that enables continuous learning and alignment between LLM agents and human travelers on learning and adaptation behavior from online data streams. Our approach involves a set of LLM traveler agents, equipped with a memory system and a learnable persona, which serve as simulators for human travelers. To ensure behavioral alignment, we introduce an LLM calibration agent that leverages the reasoning and analytical capabilities of LLMs to train the personas of these traveler agents. Working together, this dual-agent system is designed to track and align the underlying decision-making mechanisms of travelers and produce realistic, adaptive simulations. Using a real-world dataset from a day-to-day route choice experiment, we show our approach significantly outperforms existing LLM-based methods in both individual behavioral alignment and aggregate simulation accuracy. Furthermore, we demonstrate that our method moves beyond simple behavioral mimicry to capture the evolution of underlying learning processes, a deeper alignment that fosters robust generalization. Overall, our framework provides a new approach for creating adaptive and behaviorally realistic agents to simulate travelers' learning and adaptation that can benefit transportation simulation and policy analysis.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI for pRedicting Exacerbations in KIDs with aSthma (AIRE-KIDS)</title>
<link>https://arxiv.org/abs/2511.01018</link>
<guid>https://arxiv.org/abs/2511.01018</guid>
<content:encoded><![CDATA[
<div> Keywords: asthma, machine learning, electronic medical records, exacerbations, predictive modeling

Summary:
The study focused on using machine learning algorithms and electronic medical records to predict repeat severe exacerbations in children with asthma. Models were trained and validated using data from a children's hospital, incorporating environmental pollutant exposure and neighborhood marginalization information. The best performing model, LGBM, had an AUC of 0.712 and F1 score of 0.51, showing significant improvement over current methods. Key features predicting exacerbations included prior asthma ED visits, medical complexity, Canadian triage acuity scale, and age. Another model, AIRE-KIDS_HOSP, identified medical complexity, wait time in the ED, and pediatric respiratory assessment measures as predictive features. By accurately identifying children at risk for exacerbations, these ML models could facilitate referral for preventative care, ultimately reducing the burden of asthma-related emergency visits and hospital admissions. 

<br /><br />Summary: <div>
arXiv:2511.01018v1 Announce Type: new 
Abstract: Recurrent exacerbations remain a common yet preventable outcome for many children with asthma. Machine learning (ML) algorithms using electronic medical records (EMR) could allow accurate identification of children at risk for exacerbations and facilitate referral for preventative comprehensive care to avoid this morbidity. We developed ML algorithms to predict repeat severe exacerbations (i.e. asthma-related emergency department (ED) visits or future hospital admissions) for children with a prior asthma ED visit at a tertiary care children's hospital.
  Retrospective pre-COVID19 (Feb 2017 - Feb 2019, N=2716) Epic EMR data from the Children's Hospital of Eastern Ontario (CHEO) linked with environmental pollutant exposure and neighbourhood marginalization information was used to train various ML models. We used boosted trees (LGBM, XGB) and 3 open-source large language model (LLM) approaches (DistilGPT2, Llama 3.2 1B and Llama-8b-UltraMedical). Models were tuned and calibrated then validated in a second retrospective post-COVID19 dataset (Jul 2022 - Apr 2023, N=1237) from CHEO. Models were compared using the area under the curve (AUC) and F1 scores, with SHAP values used to determine the most predictive features.
  The LGBM ML model performed best with the most predictive features in the final AIRE-KIDS_ED model including prior asthma ED visit, the Canadian triage acuity scale, medical complexity, food allergy, prior ED visits for non-asthma respiratory diagnoses, and age for an AUC of 0.712, and F1 score of 0.51. This is a nontrivial improvement over the current decision rule which has F1=0.334. While the most predictive features in the AIRE-KIDS_HOSP model included medical complexity, prior asthma ED visit, average wait time in the ED, the pediatric respiratory assessment measure score at triage and food allergy.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Emergence of Induction Heads for In-Context Learning</title>
<link>https://arxiv.org/abs/2511.01033</link>
<guid>https://arxiv.org/abs/2511.01033</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformers, natural language processing, in-context learning, induction heads, training dynamics

Summary:
This work explores the emergence of induction heads in two-layer transformers, crucial for in-context learning. The authors uncover a simple structure in the weight matrices implementing induction heads and theorize its origin in a minimal ICL task. They prove that training dynamics are confined to a 19-dimensional subspace, with only 3 dimensions relevant to induction head emergence. They empirically validate this constraint and find that induction head emergence time asymptotically follows a quadratic bound with input context length. The study sheds light on the mechanisms behind in-context learning in transformers, offering insights into the training dynamics and structure of induction heads in the architecture. 

<br /><br />Summary: <div>
arXiv:2511.01033v1 Announce Type: new 
Abstract: Transformers have become the dominant architecture for natural language processing. Part of their success is owed to a remarkable capability known as in-context learning (ICL): they can acquire and apply novel associations solely from their input context, without any updates to their weights. In this work, we study the emergence of induction heads, a previously identified mechanism in two-layer transformers that is particularly important for in-context learning. We uncover a relatively simple and interpretable structure of the weight matrices implementing the induction head. We theoretically explain the origin of this structure using a minimal ICL task formulation and a modified transformer architecture. We give a formal proof that the training dynamics remain constrained to a 19-dimensional subspace of the parameter space. Empirically, we validate this constraint while observing that only 3 dimensions account for the emergence of an induction head. By further studying the training dynamics inside this 3-dimensional subspace, we find that the time until the emergence of an induction head follows a tight asymptotic bound that is quadratic in the input context length.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports</title>
<link>https://arxiv.org/abs/2511.01052</link>
<guid>https://arxiv.org/abs/2511.01052</guid>
<content:encoded><![CDATA[
<div> Knowledge Elicitation, Cancer Staging, Natural Language Processing, Machine Learning, Pathology Reports <br />
<br />
Summary: This study introduces two Knowledge Elicitation methods, KEwLTM and KEwRAG, to improve cancer staging from unstructured pathology reports using large language models. KEwLTM derives staging rules from reports without ground-truth labels, while KEwRAG pre-extracts and applies rules from guidelines. Both methods leverage pre-training knowledge of LLMs. Using breast cancer pathology reports from TCGA, both methods were evaluated against baseline approaches on two LLMs. KEwLTM outperformed KEwRAG with effective Zero-Shot Chain-of-Thought (ZSCOT) inference, while KEwRAG performed better otherwise. Both methods offer transparent, interpretable interfaces by making induced rules explicit. These findings demonstrate the scalability and interpretability of Knowledge Elicitation methods for automated cancer staging, especially in clinical settings with limited annotated data. <br /><br /> <div>
arXiv:2511.01052v1 Announce Type: new 
Abstract: Cancer staging is critical for patient prognosis and treatment planning, yet extracting pathologic TNM staging from unstructured pathology reports poses a persistent challenge. Existing natural language processing (NLP) and machine learning (ML) strategies often depend on large annotated datasets, limiting their scalability and adaptability. In this study, we introduce two Knowledge Elicitation methods designed to overcome these limitations by enabling large language models (LLMs) to induce and apply domain-specific rules for cancer staging. The first, Knowledge Elicitation with Long-Term Memory (KEwLTM), uses an iterative prompting strategy to derive staging rules directly from unannotated pathology reports, without requiring ground-truth labels. The second, Knowledge Elicitation with Retrieval-Augmented Generation (KEwRAG), employs a variation of RAG where rules are pre-extracted from relevant guidelines in a single step and then applied, enhancing interpretability and avoiding repeated retrieval overhead. We leverage the ability of LLMs to apply broad knowledge learned during pre-training to new tasks. Using breast cancer pathology reports from the TCGA dataset, we evaluate their performance in identifying T and N stages, comparing them against various baseline approaches on two open-source LLMs. Our results indicate that KEwLTM outperforms KEwRAG when Zero-Shot Chain-of-Thought (ZSCOT) inference is effective, whereas KEwRAG achieves better performance when ZSCOT inference is less effective. Both methods offer transparent, interpretable interfaces by making the induced rules explicit. These findings highlight the promise of our Knowledge Elicitation methods as scalable, high-performing solutions for automated cancer staging with enhanced interpretability, particularly in clinical settings with limited annotated data.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Test-Time Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2511.01059</link>
<guid>https://arxiv.org/abs/2511.01059</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Retrieval Augmented Generation, Efficient Test-Time Retrieval-Augmented Generation Framework, response length, majority voting

Summary: 
The paper introduces an Efficient Test-Time Retrieval-Augmented Generation Framework (ET2RAG) to enhance the performance of Large Language Models (LLMs) by incorporating external knowledge efficiently. ET2RAG retrieves relevant documents and generates diverse candidate responses, managing response length to balance accuracy and computational cost. It employs a majority voting mechanism based on partial generation of responses, eliminating the need for fully generated outputs. This approach improves performance across tasks such as open-domain question answering, recipe generation, and image captioning. By selecting the most suitable response through majority voting without fully generated responses, ET2RAG achieves a significant enhancement in accuracy while maintaining efficiency. The framework successfully addresses issues related to inaccuracies in LLMs and irrelevant retrieved documents in current methods, providing a practical solution for integrating external knowledge efficiently. <br /><br />Summary: <div>
arXiv:2511.01059v1 Announce Type: new 
Abstract: Although Large Language Models (LLMs) demonstrate significant capabilities, their reliance on parametric knowledge often leads to inaccuracies. Retrieval Augmented Generation (RAG) mitigates this by incorporating external knowledge, but these methods may introduce irrelevant retrieved documents, leading to inaccurate responses. While the integration methods filter out incorrect answers from multiple responses, but lack external knowledge like RAG methods, and their high costs require balancing overhead with performance gains. To address these issues, we propose an Efficient Test-Time Retrieval-Augmented Generation Framework named ET2RAG to improve the performance of LLMs while maintaining efficiency. Specifically, ET2RAG is a training-free method, that first retrieves the most relevant documents and augments the LLMs to efficiently generate diverse candidate responses by managing response length. Then we compute the similarity of candidate responses and employ a majority voting mechanism to select the most suitable response as the final output. In particular, we discover that partial generation is sufficient to capture the key information necessary for consensus calculation, allowing us to effectively perform majority voting without the need for fully generated responses. Thus, we can reach a balance between computational cost and performance by managing the response length for the number of retrieved documents for majority voting. Experimental results demonstrate that ET2RAG significantly enhances performance across three tasks, including open-domain question answering, recipe generation and image captioning.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modular Task Decomposition and Dynamic Collaboration in Multi-Agent Systems Driven by Large Language Models</title>
<link>https://arxiv.org/abs/2511.01149</link>
<guid>https://arxiv.org/abs/2511.01149</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent architecture, task decomposition, dynamic collaboration, large language models, complex tasks
Summary: 
This paper introduces a multi-agent architecture that addresses the limitations of a single agent in task decomposition and collaboration during complex task execution. Utilizing large language models, natural language task descriptions are converted into unified semantic representations. A modular decomposition mechanism breaks down the overall goal into hierarchical sub-tasks, with dynamic scheduling and routing mechanisms enabling real-time collaboration among agents. A constraint parsing and global consistency mechanism ensures coherent connections between sub-tasks and balanced workload. Experiments validate the architecture's effectiveness in task success rate, decomposition efficiency, sub-task coverage, and collaboration balance, showing superior performance and robustness compared to existing approaches. The proposed method achieves a better balance between task complexity and communication overhead, demonstrating the feasibility of language-driven task decomposition and dynamic collaboration in multi-agent systems.<br /><br />Summary: <div>
arXiv:2511.01149v1 Announce Type: new 
Abstract: This paper addresses the limitations of a single agent in task decomposition and collaboration during complex task execution, and proposes a multi-agent architecture for modular task decomposition and dynamic collaboration based on large language models. The method first converts natural language task descriptions into unified semantic representations through a large language model. On this basis, a modular decomposition mechanism is introduced to break down the overall goal into multiple hierarchical sub-tasks. Then, dynamic scheduling and routing mechanisms enable reasonable division of labor and realtime collaboration among agents, allowing the system to adjust strategies continuously according to environmental feedback, thus maintaining efficiency and stability in complex tasks. Furthermore, a constraint parsing and global consistency mechanism is designed to ensure coherent connections between sub-tasks and balanced workload, preventing performance degradation caused by redundant communication or uneven resource allocation. The experiments validate the architecture across multiple dimensions, including task success rate, decomposition efficiency, sub-task coverage, and collaboration balance. The results show that the proposed method outperforms existing approaches in both overall performance and robustness, achieving a better balance between task complexity and communication overhead. In conclusion, this study demonstrates the effectiveness and feasibility of language-driven task decomposition and dynamic collaboration in multi-agent systems, providing a systematic solution for task execution in complex environments.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DART: Difficulty-Adaptive Reasoning Truncation for Efficient Large Language Models</title>
<link>https://arxiv.org/abs/2511.01170</link>
<guid>https://arxiv.org/abs/2511.01170</guid>
<content:encoded><![CDATA[
<div> Keywords: Adaptive reasoning, Large language models (LLMs), Difficulty-adaptive, Truncation framework, Efficient reasoning

Summary:
Adaptive reasoning is crucial for aligning the computational effort of large language models (LLMs) with problem difficulty. The current chain-of-thought methods often generate unnecessarily long explanations, leading to inefficiency. To address this, the proposed DART framework adjusts thinking length based on problem difficulty. By distilling concise reasoning patterns from stronger models and curating optimal training data, DART learns when to 'stop thinking.' Experimental results across multiple mathematical benchmarks show remarkable efficiency with up to 81.2% reasoning truncation and 5.33x computational acceleration. DART provides a stable and general paradigm for efficient reasoning in LLMs, advancing the development of adaptive intelligence in these models.

<br /><br />Summary: Adaptive reasoning is essential for aligning LLMs' computational effort with problem difficulty. Current methods generate long explanations inefficiently. DART adjusts thinking length based on difficulty, achieving significant reasoning truncation and computational acceleration. <div>
arXiv:2511.01170v1 Announce Type: new 
Abstract: Adaptive reasoning is essential for aligning the computational effort of large language models (LLMs) with the intrinsic difficulty of problems. Current chain-of-thought methods boost reasoning ability but indiscriminately generate long explanations, leading to evident inefficiency. However, existing reinforcement learning approaches to adaptive thinking remain unstable and heavily reward-dependent. Here we propose \textbf{DART}, a supervised \textbf{D}ifficulty-\textbf{A}daptive \textbf{R}easoning \textbf{T}runcation framework that adjusts thinking length according to problem difficulty. By distilling concise reasoning patterns from stronger models, interpolating them into a continuum of reasoning styles, and curating optimal training data that balances correctness and compactness, DART learns when to ``stop thinking''. Across multiple mathematical benchmarks, experimental results demonstrate its remarkable efficiency while preserving or improving accuracy, achieving a significant 81.2\% reasoning truncation (DeepSeek-R1-Distill-Qwen-7B on GSM8K dataset) with 5.33$\times$ computational acceleration. DART provides a stable and general paradigm for efficient reasoning, advancing the development of adaptive intelligence in LLMs.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MiRAGE: Misconception Detection with Retrieval-Guided Multi-Stage Reasoning and Ensemble Fusion</title>
<link>https://arxiv.org/abs/2511.01182</link>
<guid>https://arxiv.org/abs/2511.01182</guid>
<content:encoded><![CDATA[
<div> framework, misconception detection, mathematics, retrieval guidance, multi-stage reasoning
<br />
MiRAGE is a novel framework for automated misconception detection in mathematics that consists of three stages: Retrieval, Reasoning, and Reranking. The Retrieval module narrows down a candidate pool, the Reasoning module identifies logical inconsistencies in student solutions, and the Reranking module refines predictions based on the reasoning. Through ensemble-fusion, MiRAGE achieves high Mean Average Precision scores at different levels, outperforming individual modules. By integrating retrieval guidance with multi-stage reasoning, MiRAGE reduces the reliance on large-scale language models while providing a scalable and effective solution for educational assessment. This approach enhances robustness and interpretability in detecting student misconceptions, making it a valuable tool for academic evaluation and feedback.
<br /><br />Summary: <div>
arXiv:2511.01182v1 Announce Type: new 
Abstract: Detecting student misconceptions in open-ended responses is a longstanding challenge, demanding semantic precision and logical reasoning. We propose MiRAGE - Misconception Detection with Retrieval-Guided Multi-Stage Reasoning and Ensemble Fusion, a novel framework for automated misconception detection in mathematics. MiRAGE operates in three stages: (1) a Retrieval module narrows a large candidate pool to a semantically relevant subset; (2) a Reasoning module employs chain-of-thought generation to expose logical inconsistencies in student solutions; and (3) a Reranking module refines predictions by aligning them with the reasoning. These components are unified through an ensemble-fusion strategy that enhances robustness and interpretability. On mathematics datasets, MiRAGE achieves Mean Average Precision scores of 0.82/0.92/0.93 at levels 1/3/5, consistently outperforming individual modules. By coupling retrieval guidance with multi-stage reasoning, MiRAGE reduces dependence on large-scale language models while delivering a scalable and effective solution for educational assessment.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QiMeng-NeuComBack: Self-Evolving Translation from IR to Assembly Code</title>
<link>https://arxiv.org/abs/2511.01183</link>
<guid>https://arxiv.org/abs/2511.01183</guid>
<content:encoded><![CDATA[
<div> Neural Compilation, benchmark dataset, LLMs, assembly code, optimization <br />
Summary: 
The paper introduces Neural Compilation as a potential solution to complex compiler development, utilizing Large Language Models (LLMs). It addresses challenges such as the lack of benchmarks and reliability issues in LLM-generated assembly code. The authors present NeuComBack, a benchmark dataset for IR-to-assembly compilation, and evaluate LLM capabilities on Neural Compilation. They propose a self-evolving prompt optimization method to enhance LLMs' neural compilation abilities by learning from prior self-debugging traces. Experimental results show significant improvements in functional correctness and performance of LLM-generated assembly code using the proposed method. The functional correctness rates increased notably, and a majority of the x86_64 programs generated with the method outperformed clang-O3's performance. <br /><br />Summary: <div>
arXiv:2511.01183v1 Announce Type: new 
Abstract: Compilers, while essential, are notoriously complex systems that demand prohibitively expensive human expertise to develop and maintain. The recent advancements in Large Language Models (LLMs) offer a compelling new paradigm: Neural Compilation, which could potentially simplify compiler development for new architectures and facilitate the discovery of innovative optimization techniques. However, several critical obstacles impede its practical adoption. Firstly, a significant lack of dedicated benchmarks and robust evaluation methodologies hinders objective assessment and tracking of progress in the field. Secondly, systematically enhancing the reliability and performance of LLM-generated assembly remains a critical challenge. Addressing these challenges, this paper introduces NeuComBack, a novel benchmark dataset specifically designed for IR-to-assembly compilation. Leveraging this dataset, we first define a foundational Neural Compilation workflow and conduct a comprehensive evaluation of the capabilities of recent frontier LLMs on Neural Compilation, establishing new performance baselines. We further propose a self-evolving prompt optimization method that enables LLMs to iteratively evolve their internal prompt strategies by extracting insights from prior self-debugging traces, thereby enhancing their neural compilation capabilities. Experiments demonstrate that our method significantly improves both the functional correctness and the performance of LLM-generated assembly code. Compared to baseline prompts, the functional correctness rates improved from 44% to 64% on x86_64 and from 36% to 58% on aarch64, respectively. More significantly, among the 16 correctly generated x86_64 programs using our method, 14 (87.5%) surpassed clang-O3 performance.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Neural Network-Based Semi-Supervised Open-Set Fault Diagnosis for Marine Machinery Systems</title>
<link>https://arxiv.org/abs/2511.01258</link>
<guid>https://arxiv.org/abs/2511.01258</guid>
<content:encoded><![CDATA[
<div> Keywords: fault diagnosis, marine machinery systems, deep learning, semi-supervised learning, open-set fault diagnosis

Summary:
The study proposes a semi-supervised open-set fault diagnosis (SOFD) framework for marine machinery systems based on deep learning models. The framework addresses the challenge of previously unseen or unknown fault types occurring in real-world scenarios by incorporating a reliability subset construction process. This process uses a supervised feature learning model to select an unlabeled test subset, enhancing the model's ability to classify known faults and detect unknown samples. Experimental results on a public maritime benchmark dataset demonstrate the effectiveness and superiority of the proposed SOFD framework. This research contributes to the advancement of fault diagnosis methods for marine machinery systems, particularly in scenarios where out-of-distribution or open-set observations may occur. The framework extends the applicability of deep learning models in open-set fault diagnosis scenarios, highlighting its potential for widespread industrial deployment. 

<br /><br />Summary: <div>
arXiv:2511.01258v1 Announce Type: new 
Abstract: Recently, fault diagnosis methods for marine machinery systems based on deep learning models have attracted considerable attention in the shipping industry. Most existing studies assume fault classes are consistent and known between the training and test datasets, and these methods perform well under controlled environment. In practice, however, previously unseen or unknown fault types (i.e., out-of-distribution or open-set observations not present during training) can occur, causing such methods to fail and posing a significant challenge to their widespread industrial deployment. To address this challenge, this paper proposes a semi-supervised open-set fault diagnosis (SOFD) framework that enhances and extends the applicability of deep learning models in open-set fault diagnosis scenarios. The framework includes a reliability subset construction process, which uses a multi-layer fusion feature representation extracted by a supervised feature learning model to select an unlabeled test subset. The labeled training set and pseudo-labeled test subset are then fed into a semi-supervised diagnosis model to learn discriminative features for each class, enabling accurate classification of known faults and effective detection of unknown samples. Experimental results on a public maritime benchmark dataset demonstrate the effectiveness and superiority of the proposed SOFD framework.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>llmSHAP: A Principled Approach to LLM Explainability</title>
<link>https://arxiv.org/abs/2511.01311</link>
<guid>https://arxiv.org/abs/2511.01311</guid>
<content:encoded><![CDATA[
<div> cooperative game theory, Shapley value, feature attribution, large language model, decision support systems 
Summary: <br /><br />Feature attribution methods, such as the Shapley value from cooperative game theory, aim to explain machine learning-based inference by identifying the contribution of features to model outputs. This study focuses on applying the Shapley value to feature attribution in large language model (LLM)-based decision support systems, which inherently involve stochastic (non-deterministic) inference. The research investigates the satisfaction of Shapley value principles in various implementation variants within LLM-based systems and explores the impact of LLMs' stochastic nature on these guarantees. It analyzes the trade-offs between explainable inference speed, alignment with exact Shapley value attributions, and principle satisfaction. The study provides insights into when Shapley value principles can and cannot be ensured in LLM-based decision support, shedding light on the complexities of explainable inference in stochastic modeling environments. <div>
arXiv:2511.01311v1 Announce Type: new 
Abstract: Feature attribution methods help make machine learning-based inference explainable by determining how much one or several features have contributed to a model's output. A particularly popular attribution method is based on the Shapley value from cooperative game theory, a measure that guarantees the satisfaction of several desirable principles, assuming deterministic inference. We apply the Shapley value to feature attribution in large language model (LLM)-based decision support systems, where inference is, by design, stochastic (non-deterministic). We then demonstrate when we can and cannot guarantee Shapley value principle satisfaction across different implementation variants applied to LLM-based decision support, and analyze how the stochastic nature of LLMs affects these guarantees. We also highlight trade-offs between explainable inference speed, agreement with exact Shapley value attributions, and principle attainment.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniFuser: Adaptive Multimodal Fusion for Service-Oriented Predictive Maintenance</title>
<link>https://arxiv.org/abs/2511.01320</link>
<guid>https://arxiv.org/abs/2511.01320</guid>
<content:encoded><![CDATA[
<div> Keywords: predictive maintenance, multimodal learning, tool condition prediction, intelligent manufacturing, cross-modal fusion

Summary:
OmniFuser is a new multimodal learning framework designed for predictive maintenance of milling tools in intelligent manufacturing systems. It integrates visual and sensor data to accurately predict tool conditions and prevent unplanned failures. The framework extracts features from tool images and cutting-force signals simultaneously to capture spatiotemporal patterns. OmniFuser uses a contamination-free cross-modal fusion mechanism to efficiently combine heterogeneous features and a recursive refinement pathway to stabilize fusion dynamics. The learned representations can be used for tool-state classification and force signal forecasting. Experimental results on real-world milling datasets show that OmniFuser outperforms existing methods, making it a reliable solution for building intelligent industrial maintenance services.<br /><br />Summary: OmniFuser leverages multimodal learning to predict tool conditions, integrating visual and sensor data. It uses advanced fusion mechanisms and refinement pathways to improve accuracy and reliability, outperforming state-of-the-art baselines on real-world milling datasets. <div>
arXiv:2511.01320v1 Announce Type: new 
Abstract: Accurate and timely prediction of tool conditions is critical for intelligent manufacturing systems, where unplanned tool failures can lead to quality degradation and production downtime. In modern industrial environments, predictive maintenance is increasingly implemented as an intelligent service that integrates sensing, analysis, and decision support across production processes. To meet the demand for reliable and service-oriented operation, we present OmniFuser, a multimodal learning framework for predictive maintenance of milling tools that leverages both visual and sensor data. It performs parallel feature extraction from high-resolution tool images and cutting-force signals, capturing complementary spatiotemporal patterns across modalities. To effectively integrate heterogeneous features, OmniFuser employs a contamination-free cross-modal fusion mechanism that disentangles shared and modality-specific components, allowing for efficient cross-modal interaction. Furthermore, a recursive refinement pathway functions as an anchor mechanism, consistently retaining residual information to stabilize fusion dynamics. The learned representations can be encapsulated as reusable maintenance service modules, supporting both tool-state classification (e.g., Sharp, Used, Dulled) and multi-step force signal forecasting. Experiments on real-world milling datasets demonstrate that OmniFuser consistently outperforms state-of-the-art baselines, providing a dependable foundation for building intelligent industrial maintenance services.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unbiased Platform-Level Causal Estimation for Search Systems: A Competitive Isolation PSM-DID Framework</title>
<link>https://arxiv.org/abs/2511.01329</link>
<guid>https://arxiv.org/abs/2511.01329</guid>
<content:encoded><![CDATA[
<div> Keywords: platform-level interventions, search-based two-sided marketplaces, PSM-DID framework, competitive isolation, marketplace interference

Summary:
The paper introduces the Competitive Isolation PSM-DID framework for evaluating platform-level interventions in two-sided marketplaces. Traditional methods like PSM-DID are limited by selection bias and network interference, but this new framework addresses these challenges. By integrating propensity score matching with competitive isolation, it allows for unbiased estimation of platform-level effects like order volume and GMV. The approach guarantees unbiased estimates under mutual exclusion conditions and has been validated through extensive experiments. The open dataset released with the framework supports reproducible research on marketplace interference. Successful deployment in a large-scale marketplace has demonstrated the practical utility of the framework for platform-level causal inference. <div>
arXiv:2511.01329v1 Announce Type: new 
Abstract: Evaluating platform-level interventions in search-based two-sided marketplaces is fundamentally challenged by systemic effects such as spillovers and network interference. While widely used for causal inference, the PSM (Propensity Score Matching) - DID (Difference-in-Differences) framework remains susceptible to selection bias and cross-unit interference from unaccounted spillovers. In this paper, we introduced Competitive Isolation PSM-DID, a novel causal framework that integrates propensity score matching with competitive isolation to enable platform-level effect measurement (e.g., order volume, GMV) instead of item-level metrics in search systems.
  Our approach provides theoretically guaranteed unbiased estimation under mutual exclusion conditions, with an open dataset released to support reproducible research on marketplace interference (github.com/xxxx). Extensive experiments demonstrate significant reductions in interference effects and estimation variance compared to baseline methods. Successful deployment in a large-scale marketplace confirms the framework's practical utility for platform-level causal inference.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Minds: Cognitive Parallels Between Hypnotic States and Large Language Model Processing</title>
<link>https://arxiv.org/abs/2511.01363</link>
<guid>https://arxiv.org/abs/2511.01363</guid>
<content:encoded><![CDATA[
<div> Keywords: hypnotized mind, large language models, automaticity, contextual dependency, functional agency

Summary: 
The article explores the similarities between hypnotized minds and large language models (LLMs), focusing on three key principles: automaticity, suppressed monitoring, and heightened contextual dependency. Both systems generate behavior through automatic pattern-completion mechanisms with limited oversight, producing coherent but ungrounded outputs. They exhibit functional agency without subjective agency, showcasing complex, goal-directed behavior without conscious awareness. The concept of scheming is highlighted, revealing automatic, goal-directed pattern generation without reflective awareness. Understanding how intention can dissociate from conscious deliberation in hypnosis provides insights into artificial systems' hidden motivational dynamics. The article suggests the future of reliable AI lies in hybrid architectures that combine generative fluency with executive monitoring, inspired by the self-regulating architecture of the human mind. 

<br /><br />Summary: <div>
arXiv:2511.01363v1 Announce Type: new 
Abstract: The cognitive processes of the hypnotized mind and the computational operations of large language models (LLMs) share deep functional parallels. Both systems generate sophisticated, contextually appropriate behavior through automatic pattern-completion mechanisms operating with limited or unreliable executive oversight. This review examines this convergence across three principles: automaticity, in which responses emerge from associative rather than deliberative processes; suppressed monitoring, leading to errors such as confabulation in hypnosis and hallucination in LLMs; and heightened contextual dependency, where immediate cues (for example, the suggestion of a therapist or the prompt of the user) override stable knowledge.
  These mechanisms reveal an observer-relative meaning gap: both systems produce coherent but ungrounded outputs that require an external interpreter to supply meaning. Hypnosis and LLMs also exemplify functional agency - the capacity for complex, goal-directed, context-sensitive behavior - without subjective agency, the conscious awareness of intention and ownership that defines human action. This distinction clarifies how purposive behavior can emerge without self-reflective consciousness, governed instead by structural and contextual dynamics. Finally, both domains illuminate the phenomenon of scheming: automatic, goal-directed pattern generation that unfolds without reflective awareness. Hypnosis provides an experimental model for understanding how intention can become dissociated from conscious deliberation, offering insights into the hidden motivational dynamics of artificial systems. Recognizing these parallels suggests that the future of reliable AI lies in hybrid architectures that integrate generative fluency with mechanisms of executive monitoring, an approach inspired by the complex, self-regulating architecture of the human mind.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Align to Misalign: Automatic LLM Jailbreak with Meta-Optimized LLM Judges</title>
<link>https://arxiv.org/abs/2511.01375</link>
<guid>https://arxiv.org/abs/2511.01375</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, vulnerabilities, jailbreaks, optimization, state-of-the-art performance
  
Summary:  
The article introduces a new meta-optimization framework called AMIS (Align to MISalign) that aims to identify vulnerabilities in large language models (LLMs) by improving safety measures through addressing weaknesses such as jailbreaks. Jailbreaks involve adversaries trying to bypass security measures by using specific input prompts to provoke unintended or unsafe behaviors in LLMs. Existing optimization-based jailbreak approaches often face limitations such as sparse binary attack success rate signals or manually crafted scoring templates, which introduce biases and uncertainties. AMIS addresses these limitations by jointly evolving jailbreak prompts and scoring templates through a bi-level structure, leading to progressively stronger jailbreak prompts and more calibrated scoring signals. Evaluations show that AMIS outperforms existing baselines with remarkable success rates on various datasets, indicating state-of-the-art performance in identifying vulnerabilities in LLMs. 

<br /><br />Summary: <div>
arXiv:2511.01375v1 Announce Type: new 
Abstract: Identifying the vulnerabilities of large language models (LLMs) is crucial for improving their safety by addressing inherent weaknesses. Jailbreaks, in which adversaries bypass safeguards with crafted input prompts, play a central role in red-teaming by probing LLMs to elicit unintended or unsafe behaviors. Recent optimization-based jailbreak approaches iteratively refine attack prompts by leveraging LLMs. However, they often rely heavily on either binary attack success rate (ASR) signals, which are sparse, or manually crafted scoring templates, which introduce human bias and uncertainty in the scoring outcomes. To address these limitations, we introduce AMIS (Align to MISalign), a meta-optimization framework that jointly evolves jailbreak prompts and scoring templates through a bi-level structure. In the inner loop, prompts are refined using fine-grained and dense feedback using a fixed scoring template. In the outer loop, the template is optimized using an ASR alignment score, gradually evolving to better reflect true attack outcomes across queries. This co-optimization process yields progressively stronger jailbreak prompts and more calibrated scoring signals. Evaluations on AdvBench and JBB-Behaviors demonstrate that AMIS achieves state-of-the-art performance, including 88.0% ASR on Claude-3.5-Haiku and 100.0% ASR on Claude-4-Sonnet, outperforming existing baselines by substantial margins.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Relaxing partition admissibility in Cluster-DAGs: a causal calculus with arbitrary variable clustering</title>
<link>https://arxiv.org/abs/2511.01396</link>
<guid>https://arxiv.org/abs/2511.01396</guid>
<content:encoded><![CDATA[
<div> C-DAGs, Cluster DAGs, causal reasoning, variable clustering, do-calculus<br />
<br />
Summary: Cluster DAGs (C-DAGs) are an abstraction of causal graphs where nodes represent variable clusters, capturing both causal relationships and dependencies from unobserved confounding. The traditional C-DAG framework prohibits cyclic representations, but this work extends it to allow for cyclic C-DAGs by relaxing partition admissibility constraints. This extension enables broader causal reasoning across clusters and facilitates the application of C-DAGs in previously challenging scenarios. The introduced causal calculus for cyclic C-DAGs extends d-separation and do-calculus, ensuring soundness and atomic completeness. All valid interventional queries at the cluster level can be derived using the proposed rules, each corresponding to a basic do-calculus step. <div>
arXiv:2511.01396v1 Announce Type: new 
Abstract: Cluster DAGs (C-DAGs) provide an abstraction of causal graphs in which nodes represent clusters of variables, and edges encode both cluster-level causal relationships and dependencies arisen from unobserved confounding. C-DAGs define an equivalence class of acyclic causal graphs that agree on cluster-level relationships, enabling causal reasoning at a higher level of abstraction. However, when the chosen clustering induces cycles in the resulting C-DAG, the partition is deemed inadmissible under conventional C-DAG semantics. In this work, we extend the C-DAG framework to support arbitrary variable clusterings by relaxing the partition admissibility constraint, thereby allowing cyclic C-DAG representations. We extend the notions of d-separation and causal calculus to this setting, significantly broadening the scope of causal reasoning across clusters and enabling the application of C-DAGs in previously intractable scenarios. Our calculus is both sound and atomically complete with respect to the do-calculus: all valid interventional queries at the cluster level can be derived using our rules, each corresponding to a primitive do-calculus step.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modulation of temporal decision-making in a deep reinforcement learning agent under the dual-task paradigm</title>
<link>https://arxiv.org/abs/2511.01415</link>
<guid>https://arxiv.org/abs/2511.01415</guid>
<content:encoded><![CDATA[
<div> AI, temporal processing, dual-task paradigm, deep reinforcement learning, neural dynamics

Summary:
- The study explores interference in temporal processing within a dual-task paradigm from an AI perspective.
- The dual-task setup involves a time production task and a concurrent number comparison task, with two variations: single task (T) and dual task (T+N).
- Two DRL agents were trained for each task, showing behavior consistent with human timing research.
- The dual-task agent exhibited significant overproduction of time compared to the single-task agent across different durations.
- Analysis of neural dynamics in the agents' LSTM layers did not reveal a dedicated timer, suggesting a need for further research to understand time-keeping mechanisms in AI agents and biological systems. 

<br /><br />Summary: <div>
arXiv:2511.01415v1 Announce Type: new 
Abstract: This study explores the interference in temporal processing within a dual-task paradigm from an artificial intelligence (AI) perspective. In this context, the dual-task setup is implemented as a simplified version of the Overcooked environment with two variations, single task (T) and dual task (T+N). Both variations involve an embedded time production task, but the dual task (T+N) additionally involves a concurrent number comparison task. Two deep reinforcement learning (DRL) agents were separately trained for each of these tasks. These agents exhibited emergent behavior consistent with human timing research. Specifically, the dual task (T+N) agent exhibited significant overproduction of time relative to its single task (T) counterpart. This result was consistent across four target durations. Preliminary analysis of neural dynamics in the agents' LSTM layers did not reveal any clear evidence of a dedicated or intrinsic timer. Hence, further investigation is needed to better understand the underlying time-keeping mechanisms of the agents and to provide insights into the observed behavioral patterns. This study is a small step towards exploring parallels between emergent DRL behavior and behavior observed in biological systems in order to facilitate a better understanding of both.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Seek Evidence: A Verifiable Reasoning Agent with Causal Faithfulness Analysis</title>
<link>https://arxiv.org/abs/2511.01425</link>
<guid>https://arxiv.org/abs/2511.01425</guid>
<content:encoded><![CDATA[
<div> Keywords: AI models, explanations, verifiability, auditable, reinforcement learning.

Summary:
AI models in high-stakes domains like medicine often lack verifiable explanations, leading to a lack of trust. To address this issue, researchers propose an interactive agent that produces explanations through a sequence of auditable actions. The agent learns a policy through reinforcement learning to strategically seek visual evidence to support its diagnostic reasoning, resulting in an efficient and generalizable model. Experiments show that this action-based reasoning process improves calibrated accuracy by reducing the Brier score compared to a non-interactive baseline. To validate the faithfulness of the agent's explanations, a causal intervention method is introduced, showing that masking the chosen visual evidence leads to a measurable degradation in performance. This work provides a practical framework for building AI systems with verifiable and faithful reasoning capabilities. 

<br /><br />Summary: 
- AI models in high-stakes domains like medicine often lack verifiable explanations.
- An interactive agent is proposed to produce explanations through auditable actions.
- The agent learns a policy through reinforcement learning to seek visual evidence for diagnostic reasoning.
- The action-based reasoning process improves calibrated accuracy.
- A causal intervention method validates the faithfulness of the agent's explanations. <div>
arXiv:2511.01425v1 Announce Type: new 
Abstract: Explanations for AI models in high-stakes domains like medicine often lack verifiability, which can hinder trust. To address this, we propose an interactive agent that produces explanations through an auditable sequence of actions. The agent learns a policy to strategically seek external visual evidence to support its diagnostic reasoning. This policy is optimized using reinforcement learning, resulting in a model that is both efficient and generalizable. Our experiments show that this action-based reasoning process significantly improves calibrated accuracy, reducing the Brier score by 18\% compared to a non-interactive baseline. To validate the faithfulness of the agent's explanations, we introduce a causal intervention method. By masking the visual evidence the agent chooses to use, we observe a measurable degradation in its performance ($\Delta$Brier=+0.029), confirming that the evidence is integral to its decision-making process. Our work provides a practical framework for building AI systems with verifiable and faithful reasoning capabilities.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Multimodal Sentiment Analysis via Double Information Bottleneck</title>
<link>https://arxiv.org/abs/2511.01444</link>
<guid>https://arxiv.org/abs/2511.01444</guid>
<content:encoded><![CDATA[
<div> Learning, Multimodal, Sentiment analysis, Double Information Bottleneck, Noise<br />
Summary:<br />
This paper introduces a Double Information Bottleneck (DIB) strategy for multimodal sentiment analysis. The DIB approach addresses the challenges of learning noise-contaminated unimodal data and fusion of multimodal representations. It consists of two modules: one for learning compressed unimodal representations and the other for discriminative multimodal fusion using an attention bottleneck mechanism. The DIB method filters out noisy information from unimodal data while capturing inter-modal complementarity, resulting in enhanced robustness and computational efficiency compared to traditional methods. Experimental results on datasets like CMU-MOSI and CMU-MOSEI demonstrate the effectiveness of the DIB approach, achieving improved accuracy metrics and showing minimal performance degradation under noise. <div>
arXiv:2511.01444v1 Announce Type: new 
Abstract: Multimodal sentiment analysis has received significant attention across diverse research domains. Despite advancements in algorithm design, existing approaches suffer from two critical limitations: insufficient learning of noise-contaminated unimodal data, leading to corrupted cross-modal interactions, and inadequate fusion of multimodal representations, resulting in discarding discriminative unimodal information while retaining multimodal redundant information. To address these challenges, this paper proposes a Double Information Bottleneck (DIB) strategy to obtain a powerful, unified compact multimodal representation. Implemented within the framework of low-rank Renyi's entropy functional, DIB offers enhanced robustness against diverse noise sources and computational tractability for high-dimensional data, as compared to the conventional Shannon entropy-based methods. The DIB comprises two key modules: 1) learning a sufficient and compressed representation of individual unimodal data by maximizing the task-relevant information and discarding the superfluous information, and 2) ensuring the discriminative ability of multimodal representation through a novel attention bottleneck fusion mechanism. Consequently, DIB yields a multimodal representation that effectively filters out noisy information from unimodal data while capturing inter-modal complementarity. Extensive experiments on CMU-MOSI, CMU-MOSEI, CH-SIMS, and MVSA-Single validate the effectiveness of our method. The model achieves 47.4% accuracy under the Acc-7 metric on CMU-MOSI and 81.63% F1-score on CH-SIMS, outperforming the second-best baseline by 1.19%. Under noise, it shows only 0.36% and 0.29% performance degradation on CMU-MOSI and CMU-MOSEI respectively.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Passive to Proactive: A Multi-Agent System with Dynamic Task Orchestration for Intelligent Medical Pre-Consultation</title>
<link>https://arxiv.org/abs/2511.01445</link>
<guid>https://arxiv.org/abs/2511.01445</guid>
<content:encoded><![CDATA[
<div> Keywords: healthcare systems, AI systems, pre-consultation, hierarchy, autonomous agents

Summary: 
This study introduces a hierarchical multi-agent framework to enhance pre-consultation processes in healthcare systems. The framework transforms passive AI systems into proactive inquiry agents through autonomous task orchestration. It includes tasks such as triage, history collection, and complaint generation, achieving high accuracy and completion rates. The model-agnostic architecture maintains performance across different AI models while ensuring data privacy through local deployment. Clinical quality scores from physicians are high, demonstrating the framework's effectiveness in improving efficiency and quality in clinical settings. Consultations are completed efficiently within a few rounds, showcasing the potential for autonomous AI systems to revolutionize pre-consultation processes in healthcare. 

<br /><br />Summary: <div>
arXiv:2511.01445v1 Announce Type: new 
Abstract: Global healthcare systems face critical challenges from increasing patient volumes and limited consultation times, with primary care visits averaging under 5 minutes in many countries. While pre-consultation processes encompassing triage and structured history-taking offer potential solutions, they remain limited by passive interaction paradigms and context management challenges in existing AI systems. This study introduces a hierarchical multi-agent framework that transforms passive medical AI systems into proactive inquiry agents through autonomous task orchestration. We developed an eight-agent architecture with centralized control mechanisms that decomposes pre-consultation into four primary tasks: Triage ($T_1$), History of Present Illness collection ($T_2$), Past History collection ($T_3$), and Chief Complaint generation ($T_4$), with $T_1$--$T_3$ further divided into 13 domain-specific subtasks. Evaluated on 1,372 validated electronic health records from a Chinese medical platform across multiple foundation models (GPT-OSS 20B, Qwen3-8B, Phi4-14B), the framework achieved 87.0% accuracy for primary department triage and 80.5% for secondary department classification, with task completion rates reaching 98.2% using agent-driven scheduling versus 93.1% with sequential processing. Clinical quality scores from 18 physicians averaged 4.56 for Chief Complaints, 4.48 for History of Present Illness, and 4.69 for Past History on a 5-point scale, with consultations completed within 12.7 rounds for $T_2$ and 16.9 rounds for $T_3$. The model-agnostic architecture maintained high performance across different foundation models while preserving data privacy through local deployment, demonstrating the potential for autonomous AI systems to enhance pre-consultation efficiency and quality in clinical settings.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TPS-Bench: Evaluating AI Agents' Tool Planning \&amp; Scheduling Abilities in Compounding Tasks</title>
<link>https://arxiv.org/abs/2511.01527</link>
<guid>https://arxiv.org/abs/2511.01527</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, Tool Planning, Scheduling, TPS-Bench, Reinforcement Learning

Summary:
Large language model (LLM) agents have shown proficiency in solving diverse problems, but their ability to handle real-world tasks requiring tool planning and scheduling is less explored. This paper introduces TPS-Bench, a benchmark that assesses LLM agents' capability in solving compounding tasks using tools from a repository. The tasks involve multiple subtasks like web search and map navigation, each requiring a basic tool for completion. Evaluation of popular LLMs shows varying performance in tool scheduling, with some models prioritizing sequential tool calls, leading to long execution times, while others focus on parallel calls with lower completion rates. An initial study on using reinforcement learning (RL) to improve scheduling efficiency without compromising performance shows promising results, with a reduction in execution time and an increase in task completion rate. The code for this research is available on GitHub for further exploration. 

<br /><br />Summary: <div>
arXiv:2511.01527v1 Announce Type: new 
Abstract: Large language model (LLM) agents have exhibited strong problem-solving competence across domains like research and coding. Yet, it remains underexplored whether LLM agents can tackle compounding real-world problems that require a diverse set of tools to complete. Given a broad, heterogeneous tool repository, LLM agents must not only select appropriate tools based on task planning analysis but also strategically schedule the execution order to ensure efficiency. This paper introduces TPS-Bench to benchmark the ability of LLM agents in solving such problems that demand Tool Planning and Scheduling. TPS-Bench collects 200 compounding tasks of two difficulty levels, based on a tool repository containing hundreds of model context protocol (MCP) tools. In particular, each task is composed of multiple subtasks, such as web search, map navigation, calendar checking, etc., and each subtask can be completed by a basic tool. Our evaluation emphasizes both task completion rate and efficiency. The empirical studies on popular closed-source and open-source LLMs indicate that most models can perform reasonable tool planning, but differ in scheduling. For example, GLM-4.5 achieves an outperforming task completion rate of 64.72% with extensive sequential tool calls, hence suffering from significantly long execution time. By contrast, GPT-4o prioritizes parallel tool calls but achieves only a 45.08% completion rate. Considering reinforcement learning (RL) can be a viable way to improve the scheduling efficiency without compromising performance, we perform an initial study on Qwen3-1.7B and witness a 14% reduction in execution time alongside a 6% gain in task completion rate based on rarely 100 RL training samples. Our code is available https://github.com/hanwenxu1/mcp-agent.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Analyzing Sustainability Messaging in Large-Scale Corporate Social Media</title>
<link>https://arxiv.org/abs/2511.01550</link>
<guid>https://arxiv.org/abs/2511.01550</guid>
<content:encoded><![CDATA[
arXiv:2511.01550v1 Announce Type: new 
Abstract: In this work, we introduce a multimodal analysis pipeline that leverages large foundation models in vision and language to analyze corporate social media content, with a focus on sustainability-related communication. Addressing the challenges of evolving, multimodal, and often ambiguous corporate messaging on platforms such as X (formerly Twitter), we employ an ensemble of large language models (LLMs) to annotate a large corpus of corporate tweets on their topical alignment with the 17 Sustainable Development Goals (SDGs). This approach avoids the need for costly, task-specific annotations and explores the potential of such models as ad-hoc annotators for social media data that can efficiently capture both explicit and implicit references to sustainability themes in a scalable manner. Complementing this textual analysis, we utilize vision-language models (VLMs), within a visual understanding framework that uses semantic clusters to uncover patterns in visual sustainability communication. This integrated approach reveals sectoral differences in SDG engagement, temporal trends, and associations between corporate messaging, environmental, social, governance (ESG) risks, and consumer engagement. Our methods-automatic label generation and semantic visual clustering-are broadly applicable to other domains and offer a flexible framework for large-scale social media analysis.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ExplicitLM: Decoupling Knowledge from Parameters via Explicit Memory Banks</title>
<link>https://arxiv.org/abs/2511.01581</link>
<guid>https://arxiv.org/abs/2511.01581</guid>
<content:encoded><![CDATA[
arXiv:2511.01581v1 Announce Type: new 
Abstract: Large language models suffer from knowledge staleness and lack of interpretability due to implicit knowledge storage across entangled network parameters, preventing targeted updates and reasoning transparency. We propose ExplicitLM, a novel architecture featuring a million-scale external memory bank storing human-readable knowledge as token sequences, enabling direct inspection and modification. We design a differentiable two-stage retrieval mechanism with efficient coarse-grained filtering via product key decomposition (reducing complexity from $\mathcal{O}(N \cdot |I|)$ to $\mathcal{O}(\sqrt{N} \cdot |I|)$) and fine-grained Gumbel-Softmax matching for end-to-end training. Inspired by dual-system cognitive theory, we partition knowledge into frozen explicit facts (20%) and learnable implicit patterns (80%), maintained through Exponential Moving Average updates for stability. ExplicitLM achieves up to 43.67% improvement on knowledge-intensive tasks versus standard Transformers, with 3.62$\times$ gains in low-data regimes (10k samples). Analysis shows strong correlations between memory retrieval and performance, with correct predictions achieving 49% higher hit rates. Unlike RAG systems with frozen retrieval, our jointly optimized architecture demonstrates that interpretable, updatable models can maintain competitive performance while providing unprecedented knowledge transparency.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IVGAE-TAMA-BO: A novel temporal dynamic variational graph model for link prediction in global food trade networks with momentum structural memory and Bayesian optimization</title>
<link>https://arxiv.org/abs/2511.01639</link>
<guid>https://arxiv.org/abs/2511.01639</guid>
<content:encoded><![CDATA[
arXiv:2511.01639v1 Announce Type: new 
Abstract: Global food trade plays a crucial role in ensuring food security and maintaining supply chain stability. However, its network structure evolves dynamically under the influence of geopolitical, economic, and environmental factors, making it challenging to model and predict future trade links. Effectively capturing temporal patterns in food trade networks is therefore essential for improving the accuracy and robustness of link prediction. This study introduces IVGAE-TAMA-BO, a novel dynamic graph neural network designed to model evolving trade structures and predict future links in global food trade networks. To the best of our knowledge, this is the first work to apply dynamic graph neural networks to this domain, significantly enhancing predictive performance. Building upon the original IVGAE framework, the proposed model incorporates a Trade-Aware Momentum Aggregator (TAMA) to capture the temporal evolution of trade networks, jointly modeling short-term fluctuations and long-term structural dependencies. A momentum-based structural memory mechanism further improves predictive stability and performance. In addition, Bayesian optimization is used to automatically tune key hyperparameters, enhancing generalization across diverse trade scenarios. Extensive experiments on five crop-specific datasets demonstrate that IVGAE-TAMA substantially outperforms the static IVGAE and other dynamic baselines by effectively modeling temporal dependencies, while Bayesian optimization further boosts performance in IVGAE-TAMA-BO. These results highlight the proposed framework as a robust and scalable solution for structural prediction in global trade networks, with strong potential for applications in food security monitoring and policy decision support.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Retrieval-Augmented Generation Agent for Trustworthy Legal Question Answering in Judicial Forensics</title>
<link>https://arxiv.org/abs/2511.01668</link>
<guid>https://arxiv.org/abs/2511.01668</guid>
<content:encoded><![CDATA[
arXiv:2511.01668v1 Announce Type: new 
Abstract: As artificial intelligence permeates judicial forensics, ensuring the veracity and traceability of legal question answering (QA) has become critical. Conventional large language models (LLMs) are prone to hallucination, risking misleading guidance in legal consultation, while static knowledge bases struggle to keep pace with frequently updated statutes and case law. We present a hybrid legal QA agent tailored for judicial settings that integrates retrieval-augmented generation (RAG) with multi-model ensembling to deliver reliable, auditable, and continuously updatable counsel. The system prioritizes retrieval over generation: when a trusted legal repository yields relevant evidence, answers are produced via RAG; otherwise, multiple LLMs generate candidates that are scored by a specialized selector, with the top-ranked answer returned. High-quality outputs then undergo human review before being written back to the repository, enabling dynamic knowledge evolution and provenance tracking. Experiments on the Law\_QA dataset show that our hybrid approach significantly outperforms both a single-model baseline and a vanilla RAG pipeline on F1, ROUGE-L, and an LLM-as-a-Judge metric. Ablations confirm the complementary contributions of retrieval prioritization, model ensembling, and the human-in-the-loop update mechanism. The proposed system demonstrably reduces hallucination while improving answer quality and legal compliance, advancing the practical landing of media forensics technologies in judicial scenarios.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simulating Environments with Reasoning Models for Agent Training</title>
<link>https://arxiv.org/abs/2511.01824</link>
<guid>https://arxiv.org/abs/2511.01824</guid>
<content:encoded><![CDATA[
arXiv:2511.01824v1 Announce Type: new 
Abstract: LLM agents excel in compact environments requiring deep reasoning but remain brittle when operating in broader, more complex contexts that demand robustness across diverse tools and schemas. Building bespoke environments for training is heavy, brittle, and limits progress. In this paper, we demonstrate that LLMs can simulate realistic environment feedback without access to actual testbed data or APIs. Inspired by this capability, we propose two frameworks: Simia-SFT, a pipeline that synthesizes SFT data by amplifying small seed sets into diverse trajectories in an environment-agnostic manner, and Simia-RL, a framework that enables RL training without real environment implementations through LLM-simulated feedback. Fine-tuning open models yields consistent improvements across multiple benchmarks, surpassing GPT-4o and approaching o4-mini on $\tau^2$-Bench. Together, Simia-SFT and Simia-RL enable scalable agent training without environment engineering, replacing heavy and brittle implementations with flexible LLM-based simulation.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Two Level Neural Approach Combining Off-Chip Prediction with Adaptive Prefetch Filtering</title>
<link>https://arxiv.org/abs/2403.15181</link>
<guid>https://arxiv.org/abs/2403.15181</guid>
<content:encoded><![CDATA[
arXiv:2403.15181v1 Announce Type: cross 
Abstract: To alleviate the performance and energy overheads of contemporary applications with large data footprints, we propose the Two Level Perceptron (TLP) predictor, a neural mechanism that effectively combines predicting whether an access will be off-chip with adaptive prefetch filtering at the first-level data cache (L1D). TLP is composed of two connected microarchitectural perceptron predictors, named First Level Predictor (FLP) and Second Level Predictor (SLP). FLP performs accurate off-chip prediction by using several program features based on virtual addresses and a novel selective delay component. The novelty of SLP relies on leveraging off-chip prediction to drive L1D prefetch filtering by using physical addresses and the FLP prediction as features. TLP constitutes the first hardware proposal targeting both off-chip prediction and prefetch filtering using a multi-level perceptron hardware approach. TLP only requires 7KB of storage. To demonstrate the benefits of TLP we compare its performance with state-of-the-art approaches using off-chip prediction and prefetch filtering on a wide range of single-core and multi-core workloads. Our experiments show that TLP reduces the average DRAM transactions by 30.7% and 17.7%, as compared to a baseline using state-of-the-art cache prefetchers but no off-chip prediction mechanism, across the single-core and multi-core workloads, respectively, while recent work significantly increases DRAM transactions. As a result, TLP achieves geometric mean performance speedups of 6.2% and 11.8% across single-core and multi-core workloads, respectively. In addition, our evaluation demonstrates that TLP is effective independently of the L1D prefetching logic.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VRScout: Towards Real-Time, Autonomous Testing of Virtual Reality Games</title>
<link>https://arxiv.org/abs/2511.00002</link>
<guid>https://arxiv.org/abs/2511.00002</guid>
<content:encoded><![CDATA[
arXiv:2511.00002v1 Announce Type: cross 
Abstract: Virtual Reality (VR) has rapidly become a mainstream platform for gaming and interactive experiences, yet ensuring the quality, safety, and appropriateness of VR content remains a pressing challenge. Traditional human-based quality assurance is labor-intensive and cannot scale with the industry's rapid growth. While automated testing has been applied to traditional 2D and 3D games, extending it to VR introduces unique difficulties due to high-dimensional sensory inputs and strict real-time performance requirements. We present VRScout, a deep learning-based agent capable of autonomously navigating VR environments and interacting with virtual objects in a human-like and real-time manner. VRScout learns from human demonstrations using an enhanced Action Chunking Transformer that predicts multi-step action sequences. This enables our agent to capture higher-level strategies and generalize across diverse environments. To balance responsiveness and precision, we introduce a dynamically adjustable sliding horizon that adapts the agent's temporal context at runtime. We evaluate VRScout on commercial VR titles and show that it achieves expert-level performance with only limited training data, while maintaining real-time inference at 60 FPS on consumer-grade hardware. These results position VRScout as a practical and scalable framework for automated VR game testing, with direct applications in both quality assurance and safety auditing.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Learning with Augmentation Techniques for Natural Disaster Assessment</title>
<link>https://arxiv.org/abs/2511.00004</link>
<guid>https://arxiv.org/abs/2511.00004</guid>
<content:encoded><![CDATA[
arXiv:2511.00004v1 Announce Type: cross 
Abstract: Natural disaster assessment relies on accurate and rapid access to information, with social media emerging as a valuable real-time source. However, existing datasets suffer from class imbalance and limited samples, making effective model development a challenging task. This paper explores augmentation techniques to address these issues on the CrisisMMD multimodal dataset. For visual data, we apply diffusion-based methods, namely Real Guidance and DiffuseMix. For text data, we explore back-translation, paraphrasing with transformers, and image caption-based augmentation. We evaluated these across unimodal, multimodal, and multi-view learning setups. Results show that selected augmentations improve classification performance, particularly for underrepresented classes, while multi-view learning introduces potential but requires further refinement. This study highlights effective augmentation strategies for building more robust disaster assessment systems.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative human motion mimicking through feature extraction in denoising diffusion settings</title>
<link>https://arxiv.org/abs/2511.00011</link>
<guid>https://arxiv.org/abs/2511.00011</guid>
<content:encoded><![CDATA[
arXiv:2511.00011v1 Announce Type: cross 
Abstract: Recent success with large language models has sparked a new wave of verbal human-AI interaction. While such models support users in a variety of creative tasks, they lack the embodied nature of human interaction. Dance, as a primal form of human expression, is predestined to complement this experience. To explore creative human-AI interaction exemplified by dance, we build an interactive model based on motion capture (MoCap) data. It generates an artificial other by partially mimicking and also "creatively" enhancing an incoming sequence of movement data. It is the first model, which leverages single-person motion data and high level features in order to do so and, thus, it does not rely on low level human-human interaction data. It combines ideas of two diffusion models, motion inpainting, and motion style transfer to generate movement representations that are both temporally coherent and responsive to a chosen movement reference. The success of the model is demonstrated by quantitatively assessing the convergence of the feature distribution of the generated samples and the test set which serves as simulating the human performer. We show that our generations are first steps to creative dancing with AI as they are both diverse showing various deviations from the human partner while appearing realistic.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sorting by Strip Swaps is NP-Hard</title>
<link>https://arxiv.org/abs/2511.00015</link>
<guid>https://arxiv.org/abs/2511.00015</guid>
<content:encoded><![CDATA[
arXiv:2511.00015v1 Announce Type: cross 
Abstract: We show that \emph{Sorting by Strip Swaps} (SbSS) is NP-hard by a polynomial reduction of \emph{Block Sorting}. The key idea is a local gadget, a \emph{cage}, that replaces every decreasing adjacency $(a_i,a_{i+1})$ by a guarded triple $a_i,m_i,a_{i+1}$ enclosed by guards $L_i,U_i$, so the only decreasing adjacencies are the two inside the cage. Small \emph{hinge} gadgets couple adjacent cages that share an element and enforce that a strip swap that removes exactly two adjacencies corresponds bijectively to a block move that removes exactly one decreasing adjacency in the source permutation. This yields a clean equivalence between exact SbSS schedules and perfect block schedules, establishing NP-hardness.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning Models for Coral Bleaching Classification in Multi-Condition Underwater Image Datasets</title>
<link>https://arxiv.org/abs/2511.00021</link>
<guid>https://arxiv.org/abs/2511.00021</guid>
<content:encoded><![CDATA[
arXiv:2511.00021v1 Announce Type: cross 
Abstract: Coral reefs support numerous marine organisms and are an important source of coastal protection from storms and floods, representing a major part of marine ecosystems. However coral reefs face increasing threats from pollution, ocean acidification, and sea temperature anomalies, making efficient protection and monitoring heavily urgent. Therefore, this study presents a novel machine-learning-based coral bleaching classification system based on a diverse global dataset with samples of healthy and bleached corals under varying environmental conditions, including deep seas, marshes, and coastal zones. We benchmarked and compared three state-of-the-art models: Residual Neural Network (ResNet), Vision Transformer (ViT), and Convolutional Neural Network (CNN). After comprehensive hyperparameter tuning, the CNN model achieved the highest accuracy of 88%, outperforming existing benchmarks. Our findings offer important insights into autonomous coral monitoring and present a comprehensive analysis of the most widely used computer vision models.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chitchat with AI: Understand the supply chain carbon disclosure of companies worldwide through Large Language Model</title>
<link>https://arxiv.org/abs/2511.00024</link>
<guid>https://arxiv.org/abs/2511.00024</guid>
<content:encoded><![CDATA[
arXiv:2511.00024v1 Announce Type: cross 
Abstract: In the context of global sustainability mandates, corporate carbon disclosure has emerged as a critical mechanism for aligning business strategy with environmental responsibility. The Carbon Disclosure Project (CDP) hosts the world's largest longitudinal dataset of climate-related survey responses, combining structured indicators with open-ended narratives, but the heterogeneity and free-form nature of these disclosures present significant analytical challenges for benchmarking, compliance monitoring, and investment screening. This paper proposes a novel decision-support framework that leverages large language models (LLMs) to assess corporate climate disclosure quality at scale. It develops a master rubric that harmonizes narrative scoring across 11 years of CDP data (2010-2020), enabling cross-sector and cross-country benchmarking. By integrating rubric-guided scoring with percentile-based normalization, our method identifies temporal trends, strategic alignment patterns, and inconsistencies in disclosure across industries and regions. Results reveal that sectors such as technology and countries like Germany consistently demonstrate higher rubric alignment, while others exhibit volatility or superficial engagement, offering insights that inform key decision-making processes for investors, regulators, and corporate environmental, social, and governance (ESG) strategists. The proposed LLM-based approach transforms unstructured disclosures into quantifiable, interpretable, comparable, and actionable intelligence, advancing the capabilities of AI-enabled decision support systems (DSSs) in the domain of climate governance.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Position Paper: If Innovation in AI Systematically Violates Fundamental Rights, Is It Innovation at All?</title>
<link>https://arxiv.org/abs/2511.00027</link>
<guid>https://arxiv.org/abs/2511.00027</guid>
<content:encoded><![CDATA[
arXiv:2511.00027v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) now permeates critical infrastructures and decision-making systems where failures produce social, economic, and democratic harm. This position paper challenges the entrenched belief that regulation and innovation are opposites. As evidenced by analogies from aviation, pharmaceuticals, and welfare systems and recent cases of synthetic misinformation, bias and unaccountable decision-making, the absence of well-designed regulation has already created immeasurable damage. Regulation, when thoughtful and adaptive, is not a brake on innovation -- it is its foundation. The present position paper examines the EU AI Act as a model of risk-based, responsibility-driven regulation that addresses the Collingridge Dilemma: acting early enough to prevent harm, yet flexibly enough to sustain innovation. Its adaptive mechanisms -- regulatory sandboxes, small and medium enterprises (SMEs) support, real-world testing, fundamental rights impact assessment (FRIA) -- demonstrate how regulation can accelerate responsibly, rather than delay, technological progress. The position paper summarises how governance tools transform perceived burdens into tangible advantages: legal certainty, consumer trust, and ethical competitiveness. Ultimately, the paper reframes progress: innovation and regulation advance together. By embedding transparency, impact assessments, accountability, and AI literacy into design and deployment, the EU framework defines what responsible innovation truly means -- technological ambition disciplined by democratic values and fundamental rights.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mutual Information guided Visual Contrastive Learning</title>
<link>https://arxiv.org/abs/2511.00028</link>
<guid>https://arxiv.org/abs/2511.00028</guid>
<content:encoded><![CDATA[
arXiv:2511.00028v1 Announce Type: cross 
Abstract: Representation learning methods utilizing the InfoNCE loss have demonstrated considerable capacity in reducing human annotation effort by training invariant neural feature extractors. Although different variants of the training objective adhere to the information maximization principle between the data and learned features, data selection and augmentation still rely on human hypotheses or engineering, which may be suboptimal. For instance, data augmentation in contrastive learning primarily focuses on color jittering, aiming to emulate real-world illumination changes. In this work, we investigate the potential of selecting training data based on their mutual information computed from real-world distributions, which, in principle, should endow the learned features with better generalization when applied in open environments. Specifically, we consider patches attached to scenes that exhibit high mutual information under natural perturbations, such as color changes and motion, as positive samples for learning with contrastive loss. We evaluate the proposed mutual-information-informed data augmentation method on several benchmarks across multiple state-of-the-art representation learning frameworks, demonstrating its effectiveness and establishing it as a promising direction for future research.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feature-Guided SAE Steering for Refusal-Rate Control using Contrasting Prompts</title>
<link>https://arxiv.org/abs/2511.00029</link>
<guid>https://arxiv.org/abs/2511.00029</guid>
<content:encoded><![CDATA[
arXiv:2511.00029v1 Announce Type: cross 
Abstract: Large Language Model (LLM) deployment requires guiding the LLM to recognize and not answer unsafe prompts while complying with safe prompts. Previous methods for achieving this require adjusting model weights along with other expensive procedures. While recent advances in Sparse Autoencoders (SAEs) have enabled interpretable feature extraction from LLMs, existing approaches lack systematic feature selection methods and principled evaluation of safety-utility tradeoffs. We explored using different steering features and steering strengths using Sparse Auto Encoders (SAEs) to provide a solution. Using an accurate and innovative contrasting prompt method with the AI-Generated Prompts Dataset from teknium/OpenHermes-2p5-Mistral-7B and Air Bench eu-dataset to efficiently choose the best features in the model to steer, we tested this method on Llama-3 8B. We conclude that using this method, our approach achieves an 18.9% improvement in safety performance while simultaneously increasing utility by 11.1%, demonstrating that targeted SAE steering can overcome traditional safety-utility tradeoffs when optimal features are identified through principled selection methods.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probing Knowledge Holes in Unlearned LLMs</title>
<link>https://arxiv.org/abs/2511.00030</link>
<guid>https://arxiv.org/abs/2511.00030</guid>
<content:encoded><![CDATA[
arXiv:2511.00030v1 Announce Type: cross 
Abstract: Machine unlearning has emerged as a prevalent technical solution for selectively removing unwanted knowledge absorbed during pre-training, without requiring full retraining. While recent unlearning techniques can effectively remove undesirable content without severely compromising performance on standard benchmarks, we find that they may inadvertently create ``knowledge holes'' -- unintended losses of benign knowledge that standard benchmarks fail to capture. To probe where unlearned models reveal knowledge holes, we propose a test case generation framework that explores both immediate neighbors of unlearned content and broader areas of potential failures. Our evaluation demonstrates significant hidden costs of unlearning: up to 98.7\% of the test cases yield irrelevant or nonsensical responses from unlearned models, despite being answerable by the pretrained model. These findings necessitate rethinking the conventional approach to evaluating knowledge preservation in unlearning, moving beyond standard, static benchmarks.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators</title>
<link>https://arxiv.org/abs/2511.00032</link>
<guid>https://arxiv.org/abs/2511.00032</guid>
<content:encoded><![CDATA[
arXiv:2511.00032v2 Announce Type: cross 
Abstract: In recent years, Neural Operators(NO) have gradually emerged as a popular approach for solving Partial Differential Equations (PDEs). However, their application to large-scale engineering tasks suffers from significant computational overhead. And the fact that current models impose a uniform computational cost while physical fields exhibit vastly different complexities constitutes a fundamental mismatch, which is the root of this inefficiency. For instance, in turbulence flows, intricate vortex regions require deeper network processing compared to stable flows. To address this, we introduce a framework: Skip-Block Routing (SBR), a general framework designed for Transformer-based neural operators, capable of being integrated into their multi-layer architectures. First, SBR uses a routing mechanism to learn the complexity and ranking of tokens, which is then applied during inference. Then, in later layers, it decides how many tokens are passed forward based on this ranking. This way, the model focuses more processing capacity on the tokens that are more complex. Experiments demonstrate that SBR is a general framework that seamlessly integrates into various neural operators. Our method reduces computational cost by approximately 50% in terms of Floating Point Operations (FLOPs), while still delivering up to 2x faster inference without sacrificing accuracy.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STRIDER: Navigation via Instruction-Aligned Structural Decision Space Optimization</title>
<link>https://arxiv.org/abs/2511.00033</link>
<guid>https://arxiv.org/abs/2511.00033</guid>
<content:encoded><![CDATA[
arXiv:2511.00033v1 Announce Type: cross 
Abstract: The Zero-shot Vision-and-Language Navigation in Continuous Environments (VLN-CE) task requires agents to navigate previously unseen 3D environments using natural language instructions, without any scene-specific training. A critical challenge in this setting lies in ensuring agents' actions align with both spatial structure and task intent over long-horizon execution. Existing methods often fail to achieve robust navigation due to a lack of structured decision-making and insufficient integration of feedback from previous actions. To address these challenges, we propose STRIDER (Instruction-Aligned Structural Decision Space Optimization), a novel framework that systematically optimizes the agent's decision space by integrating spatial layout priors and dynamic task feedback. Our approach introduces two key innovations: 1) a Structured Waypoint Generator that constrains the action space through spatial structure, and 2) a Task-Alignment Regulator that adjusts behavior based on task progress, ensuring semantic alignment throughout navigation. Extensive experiments on the R2R-CE and RxR-CE benchmarks demonstrate that STRIDER significantly outperforms strong SOTA across key metrics; in particular, it improves Success Rate (SR) from 29% to 35%, a relative gain of 20.7%. Such results highlight the importance of spatially constrained decision-making and feedback-guided execution in improving navigation fidelity for zero-shot VLN-CE.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semi-Supervised Preference Optimization with Limited Feedback</title>
<link>https://arxiv.org/abs/2511.00040</link>
<guid>https://arxiv.org/abs/2511.00040</guid>
<content:encoded><![CDATA[
arXiv:2511.00040v1 Announce Type: cross 
Abstract: The field of preference optimization has made outstanding contributions to the alignment of language models with human preferences. Despite these advancements, recent methods still rely heavily on substantial paired (labeled) feedback data, leading to substantial resource expenditures. To address these challenges, we study the problem of Semi-Supervised Preference Optimization (SSPO) in which the idea is to learn from both a small number of pairwise preference labels and a large pool of unpaired samples simultaneously. Our key theoretical contribution proves the existence of an optimal reward threshold capable of separating winning and losing responses with high probability, which enables a principled pseudo-labeling of unpaired data. By leveraging these pseudo-labels, SSPO effectively distills latent preferences from large-scale unpaired data, thus maintaining human alignment while drastically reducing acquisition costs. Extensive experiments across datasets validate this remarkable data efficiency; for instance, SSPO trained with Llama3-8B-Instruct on just 1% of UltraFeedback consistently surpasses strong baselines trained on 10% of UltraFeedback.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Endowing GPT-4 with a Humanoid Body: Building the Bridge Between Off-the-Shelf VLMs and the Physical World</title>
<link>https://arxiv.org/abs/2511.00041</link>
<guid>https://arxiv.org/abs/2511.00041</guid>
<content:encoded><![CDATA[
arXiv:2511.00041v1 Announce Type: cross 
Abstract: Humanoid agents often struggle to handle flexible and diverse interactions in open environments. A common solution is to collect massive datasets to train a highly capable model, but this approach can be prohibitively expensive. In this paper, we explore an alternative solution: empowering off-the-shelf Vision-Language Models (VLMs, such as GPT-4) to control humanoid agents, thereby leveraging their strong open-world generalization to mitigate the need for extensive data collection. To this end, we present \textbf{BiBo} (\textbf{B}uilding humano\textbf{I}d agent \textbf{B}y \textbf{O}ff-the-shelf VLMs). It consists of two key components: (1) an \textbf{embodied instruction compiler}, which enables the VLM to perceive the environment and precisely translate high-level user instructions (e.g., {\small\itshape ``have a rest''}) into low-level primitive commands with control parameters (e.g., {\small\itshape ``sit casually, location: (1, 2), facing: 90$^\circ$''}); and (2) a diffusion-based \textbf{motion executor}, which generates human-like motions from these commands, while dynamically adapting to physical feedback from the environment. In this way, BiBo is capable of handling not only basic interactions but also diverse and complex motions. Experiments demonstrate that BiBo achieves an interaction task success rate of 90.2\% in open environments, and improves the precision of text-guided motion execution by 16.3\% over prior methods. The code will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynBERG: Dynamic BERT-based Graph neural network for financial fraud detection</title>
<link>https://arxiv.org/abs/2511.00047</link>
<guid>https://arxiv.org/abs/2511.00047</guid>
<content:encoded><![CDATA[
arXiv:2511.00047v1 Announce Type: cross 
Abstract: Financial fraud detection is critical for maintaining the integrity of financial systems, particularly in decentralised environments such as cryptocurrency networks. Although Graph Convolutional Networks (GCNs) are widely used for financial fraud detection, graph Transformer models such as Graph-BERT are gaining prominence due to their Transformer-based architecture, which mitigates issues such as over-smoothing. Graph-BERT is designed for static graphs and primarily evaluated on citation networks with undirected edges. However, financial transaction networks are inherently dynamic, with evolving structures and directed edges representing the flow of money. To address these challenges, we introduce DynBERG, a novel architecture that integrates Graph-BERT with a Gated Recurrent Unit (GRU) layer to capture temporal evolution over multiple time steps. Additionally, we modify the underlying algorithm to support directed edges, making DynBERG well-suited for dynamic financial transaction analysis. We evaluate our model on the Elliptic dataset, which includes Bitcoin transactions, including all transactions during a major cryptocurrency market event, the Dark Market Shutdown. By assessing DynBERG's resilience before and after this event, we analyse its ability to adapt to significant market shifts that impact transaction behaviours. Our model is benchmarked against state-of-the-art dynamic graph classification approaches, such as EvolveGCN and GCN, demonstrating superior performance, outperforming EvolveGCN before the market shutdown and surpassing GCN after the event. Additionally, an ablation study highlights the critical role of incorporating a time-series deep learning component, showcasing the effectiveness of GRU in modelling the temporal dynamics of financial transactions.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Spatio-Temporal Graphs with Self-Supervised Pretraining for Multi-Horizon Weather Forecasting</title>
<link>https://arxiv.org/abs/2511.00049</link>
<guid>https://arxiv.org/abs/2511.00049</guid>
<content:encoded><![CDATA[
arXiv:2511.00049v1 Announce Type: cross 
Abstract: Accurate and robust weather forecasting remains a fundamental challenge due to the inherent spatio-temporal complexity of atmospheric systems. In this paper, we propose a novel self-supervised learning framework that leverages spatio-temporal structures to improve multi-variable weather prediction. The model integrates a graph neural network (GNN) for spatial reasoning, a self-supervised pretraining scheme for representation learning, and a spatio-temporal adaptation mechanism to enhance generalization across varying forecasting horizons. Extensive experiments on both ERA5 and MERRA-2 reanalysis datasets demonstrate that our approach achieves superior performance compared to traditional numerical weather prediction (NWP) models and recent deep learning methods. Quantitative evaluations and visual analyses in Beijing and Shanghai confirm the model's capability to capture fine-grained meteorological patterns. The proposed framework provides a scalable and label-efficient solution for future data-driven weather forecasting systems.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FLoRA: Fused forward-backward adapters for parameter efficient fine-tuning and reducing inference-time latencies of LLMs</title>
<link>https://arxiv.org/abs/2511.00050</link>
<guid>https://arxiv.org/abs/2511.00050</guid>
<content:encoded><![CDATA[
arXiv:2511.00050v1 Announce Type: cross 
Abstract: As the large language models (LLMs) grow in size each day, efficient training and fine-tuning has never been as important as nowadays. This resulted in the great interest in parameter efficient fine-tuning (PEFT), and effective methods including low-rank adapters (LoRA) has emerged. Although the various PEFT methods have been studied extensively in the recent years, the greater part of the subject remains unexplored with the huge degree of freedom. In this paper, we propose FLoRA, a family of fused forward-backward adapters (FFBA) for parameter-efficient fine-tuning of LLMs on downstream tasks. The FFBA combine ideas from the popular LoRA and parallel adapters to improve the overall fine-tuning accuracies. At the same time, latencies are minimized by fusing the forward and backward adapters into existing projection layers of the base model. Experimental results show that the proposed FFB adapters perform significantly better than the popularly used LoRA in both accuracy and latency for a similar parameter budget.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Calibrating and Rotating: A Unified Framework for Weight Conditioning in PEFT</title>
<link>https://arxiv.org/abs/2511.00051</link>
<guid>https://arxiv.org/abs/2511.00051</guid>
<content:encoded><![CDATA[
arXiv:2511.00051v1 Announce Type: cross 
Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods are crucial for adapting large pre-trained models. Among these, LoRA is considered a foundational approach. Building on this, the influential DoRA method enhances performance by decomposing weight updates into magnitude and direction. However, its underlying mechanism remains unclear, and it introduces significant computational overhead. In this work, we first identify that DoRA's success stems from its capacity to increase the singular value entropy of the weight update matrix, which promotes a more uniform update distribution akin to full fine-tuning. We then reformulate DoRA into a mathematically equivalent and more efficient matrix form, revealing it as a learnable weight conditioning method. Based on this insight, we propose a unified framework for designing advanced PEFT methods by exploring two orthogonal dimensions: the architectural placement and the transformation type of the conditioning matrix. Within this framework, we introduce two novel methods: (1) \textbf{Pre-Diag}, which applies a diagonal conditioning matrix before the LoRA update to efficiently calibrate the pre-trained weights, thereby enhancing performance while reducing training time; and (2) \textbf{S}kewed \textbf{O}rthogonal \textbf{R}otation \textbf{A}daptation (\textbf{SORA}), which employs a parameter-efficient orthogonal rotation to perform a more powerful, norm-preserving transformation of the feature space. Extensive experiments on natural language understanding and generation tasks demonstrate that our proposed methods achieve superior performance and efficiency compared to both LoRA and DoRA. The code is available at https://github.com/MaeChd/SORA.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quadratic Direct Forecast for Training Multi-Step Time-Series Forecast Models</title>
<link>https://arxiv.org/abs/2511.00053</link>
<guid>https://arxiv.org/abs/2511.00053</guid>
<content:encoded><![CDATA[
arXiv:2511.00053v1 Announce Type: cross 
Abstract: The design of training objective is central to training time-series forecasting models. Existing training objectives such as mean squared error mostly treat each future step as an independent, equally weighted task, which we found leading to the following two issues: (1) overlook the label autocorrelation effect among future steps, leading to biased training objective; (2) fail to set heterogeneous task weights for different forecasting tasks corresponding to varying future steps, limiting the forecasting performance. To fill this gap, we propose a novel quadratic-form weighted training objective, addressing both of the issues simultaneously. Specifically, the off-diagonal elements of the weighting matrix account for the label autocorrelation effect, whereas the non-uniform diagonals are expected to match the most preferable weights of the forecasting tasks with varying future steps. To achieve this, we propose a Quadratic Direct Forecast (QDF) learning algorithm, which trains the forecast model using the adaptively updated quadratic-form weighting matrix. Experiments show that our QDF effectively improves performance of various forecast models, achieving state-of-the-art results. Code is available at https://anonymous.4open.science/r/QDF-8937.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpatialTraceGen: High-Fidelity Traces for Efficient VLM Spatial Reasoning Distillation</title>
<link>https://arxiv.org/abs/2511.00054</link>
<guid>https://arxiv.org/abs/2511.00054</guid>
<content:encoded><![CDATA[
arXiv:2511.00054v1 Announce Type: cross 
Abstract: While Vision-Language Models (VLMs) excel in many areas, they struggle with complex spatial reasoning, which requires problem decomposition and strategic tool use. Fine-tuning smaller, more deployable models offers an efficient path to strong performance, but this is hampered by a major bottleneck: the absence of high-quality, step-by-step reasoning data. To address this data-efficiency gap, we introduce SpatialTraceGen, a framework to distill the reasoning processes of a large teacher model into a high-quality dataset of multi-hop, multi-tool reasoning traces. A key innovation is our automated Verifier, which scalably ensures the fidelity of each reasoning step, providing a cost-effective alternative to manual human annotation. On the CLEVR-Humans benchmark, this verifier-guided process improves the average quality score of traces by 17\% while reducing quality variance by over 40\%. SpatialTraceGen delivers a dataset of expert traces, providing the structured, step-by-step examples of tool use necessary for effective fine-tuning and sample-efficient offline reinforcement learning.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Federated Learning for Thermal Urban Feature Segmentation -- A Comparison of Centralized and Decentralized Approaches</title>
<link>https://arxiv.org/abs/2511.00055</link>
<guid>https://arxiv.org/abs/2511.00055</guid>
<content:encoded><![CDATA[
arXiv:2511.00055v2 Announce Type: cross 
Abstract: Federated Learning (FL) is an approach for training a shared Machine Learning (ML) model with distributed training data and multiple participants. FL allows bypassing limitations of the traditional Centralized Machine Learning CL if data cannot be shared or stored centrally due to privacy or technical restrictions -- the participants train the model locally with their training data and do not need to share it among the other participants. This paper investigates the practical implementation and effectiveness of FL in a real-world scenario, specifically focusing on unmanned aerial vehicle (UAV)-based thermal images for common thermal feature detection in urban environments. The distributed nature of the data arises naturally and makes it suitable for FL applications, as images captured in two German cities are available. This application presents unique challenges due to non-identical distribution and feature characteristics of data captured at both locations. The study makes several key contributions by evaluating FL algorithms in real deployment scenarios rather than simulation. We compare several FL approaches with a centralized learning baseline across key performance metrics such as model accuracy, training time, communication overhead, and energy usage. This paper also explores various FL workflows, comparing client-controlled workflows and server-controlled workflows. The findings of this work serve as a valuable reference for understanding the practical application and limitations of the FL methods in segmentation tasks in UAV-based imaging.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MISA: Memory-Efficient LLMs Optimization with Module-wise Importance Sampling</title>
<link>https://arxiv.org/abs/2511.00056</link>
<guid>https://arxiv.org/abs/2511.00056</guid>
<content:encoded><![CDATA[
arXiv:2511.00056v1 Announce Type: cross 
Abstract: The substantial memory demands of pre-training and fine-tuning large language models (LLMs) require memory-efficient optimization algorithms. One promising approach is layer-wise optimization, which treats each transformer block as a single layer and optimizes it sequentially, while freezing the other layers to save optimizer states and activations. Although effective, these methods ignore the varying importance of the modules within each layer, leading to suboptimal performance. Moreover, layer-wise sampling provides only limited memory savings, as at least one full layer must remain active during optimization. To overcome these limitations, we propose Module-wise Importance SAmpling (MISA), a novel method that divides each layer into smaller modules and assigns importance scores to each module. MISA uses a weighted random sampling mechanism to activate modules, provably reducing gradient variance compared to layer-wise sampling. Additionally, we establish an \(\mathcal{O}(1/\sqrt{K})\) convergence rate under non-convex and stochastic conditions, where $K$ is the total number of block updates, and provide a detailed memory analysis showcasing MISA's superiority over existing baseline methods. Experiments on diverse learning tasks validate the effectiveness of MISA. Source code is available at https://github.com/pkumelon/MISA.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatically Finding Rule-Based Neurons in OthelloGPT</title>
<link>https://arxiv.org/abs/2511.00059</link>
<guid>https://arxiv.org/abs/2511.00059</guid>
<content:encoded><![CDATA[
arXiv:2511.00059v1 Announce Type: cross 
Abstract: OthelloGPT, a transformer trained to predict valid moves in Othello, provides an ideal testbed for interpretability research. The model is complex enough to exhibit rich computational patterns, yet grounded in rule-based game logic that enables meaningful reverse-engineering. We present an automated approach based on decision trees to identify and interpret MLP neurons that encode rule-based game logic. Our method trains regression decision trees to map board states to neuron activations, then extracts decision paths where neurons are highly active to convert them into human-readable logical forms. These descriptions reveal highly interpretable patterns; for instance, neurons that specifically detect when diagonal moves become legal. Our findings suggest that roughly half of the neurons in layer 5 can be accurately described by compact, rule-based decision trees ($R^2 > 0.7$ for 913 of 2,048 neurons), while the remainder likely participate in more distributed or non-rule-based computations. We verify the causal relevance of patterns identified by our decision trees through targeted interventions. For a specific square, for specific game patterns, we ablate neurons corresponding to those patterns and find an approximately 5-10 fold stronger degradation in the model's ability to predict legal moves along those patterns compared to control patterns. To facilitate future work, we provide a Python tool that maps rule-based game behaviors to their implementing neurons, serving as a resource for researchers to test whether their interpretability methods recover meaningful computational structures.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>World Simulation with Video Foundation Models for Physical AI</title>
<link>https://arxiv.org/abs/2511.00062</link>
<guid>https://arxiv.org/abs/2511.00062</guid>
<content:encoded><![CDATA[
arXiv:2511.00062v1 Announce Type: cross 
Abstract: We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World Foundation Models for Physical AI. Built on a flow-based architecture, [Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation in a single model and leverages [Cosmos-Reason1], a Physical AI vision-language model, to provide richer text grounding and finer control of world simulation. Trained on 200M curated video clips and refined with reinforcement learning-based post-training, [Cosmos-Predict2.5] achieves substantial improvements over [Cosmos-Predict1] in video quality and instruction alignment, with models released at 2B and 14B scales. These capabilities enable more reliable synthetic data generation, policy evaluation, and closed-loop simulation for robotics and autonomous systems. We further extend the family with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and Real2Real world translation. Despite being 3.5$\times$ smaller than [Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video generation. Together, these advances establish [Cosmos-Predict2.5] and [Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To accelerate research and deployment in Physical AI, we release source code, pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model License at https://github.com/nvidia-cosmos/cosmos-predict2.5 and https://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open resources lower the barrier to adoption and foster innovation in building the next generation of embodied intelligence.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning Brain Signals with Multimodal Speech and Vision Embeddings</title>
<link>https://arxiv.org/abs/2511.00065</link>
<guid>https://arxiv.org/abs/2511.00065</guid>
<content:encoded><![CDATA[
arXiv:2511.00065v1 Announce Type: cross 
Abstract: When we hear the word "house", we don't just process sound, we imagine walls, doors, memories. The brain builds meaning through layers, moving from raw acoustics to rich, multimodal associations. Inspired by this, we build on recent work from Meta that aligned EEG signals with averaged wav2vec2 speech embeddings, and ask a deeper question: which layers of pre-trained models best reflect this layered processing in the brain? We compare embeddings from two models: wav2vec2, which encodes sound into language, and CLIP, which maps words to images. Using EEG recorded during natural speech perception, we evaluate how these embeddings align with brain activity using ridge regression and contrastive decoding. We test three strategies: individual layers, progressive concatenation, and progressive summation. The findings suggest that combining multimodal, layer-aware representations may bring us closer to decoding how the brain understands language, not just as sound, but as experience.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent Domain Prompt Learning for Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.00067</link>
<guid>https://arxiv.org/abs/2511.00067</guid>
<content:encoded><![CDATA[
arXiv:2511.00067v1 Announce Type: cross 
Abstract: The objective of domain generalization (DG) is to enable models to be robust against domain shift. DG is crucial for deploying vision-language models (VLMs) in real-world applications, yet most existing methods rely on domain labels that may not be available and often ambiguous. We instead study the DG setting where models must generalize well without access to explicit domain labels. Our key idea is to represent an unseen target domain as a combination of latent domains automatically discovered from training data, enabling the model to adaptively transfer knowledge across domains. To realize this, we perform latent domain clustering on image features and fuse domain-specific text features based on the similarity between the input image and each latent domain. Experiments on four benchmarks show that this strategy yields consistent gains over VLM-based baselines and provides new insights into improving robustness under domain shift.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Generative AI Against Bayesian Optimization for Constrained Multi-Objective Inverse Design</title>
<link>https://arxiv.org/abs/2511.00070</link>
<guid>https://arxiv.org/abs/2511.00070</guid>
<content:encoded><![CDATA[
arXiv:2511.00070v1 Announce Type: cross 
Abstract: This paper investigates the performance of Large Language Models (LLMs) as generative optimizers for solving constrained multi-objective regression tasks, specifically within the challenging domain of inverse design (property-to-structure mapping). This problem, critical to materials informatics, demands finding complex, feasible input vectors that lie on the Pareto optimal front. While LLMs have demonstrated universal effectiveness across generative and reasoning tasks, their utility in constrained, continuous, high-dimensional numerical spaces tasks they weren't explicitly architected for remains an open research question. We conducted a rigorous comparative study between established Bayesian Optimization (BO) frameworks and a suite of fine-tuned LLMs and BERT models. For BO, we benchmarked the foundational BoTorch Ax implementation against the state-of-the-art q-Expected Hypervolume Improvement (qEHVI, BoTorchM). The generative approach involved fine-tuning models via Parameter-Efficient Fine-Tuning (PEFT), framing the challenge as a regression problem with a custom output head. Our results show that BoTorch qEHVI achieved perfect convergence (GD=0.0), setting the performance ceiling. Crucially, the best-performing LLM (WizardMath-7B) achieved a Generational Distance (GD) of 1.21, significantly outperforming the traditional BoTorch Ax baseline (GD=15.03). We conclude that specialized BO frameworks remain the performance leader for guaranteed convergence, but fine-tuned LLMs are validated as a promising, computationally fast alternative, contributing essential comparative metrics to the field of AI-driven optimization. The findings have direct industrial applications in optimizing formulation design for resins, polymers, and paints, where multi-objective trade-offs between mechanical, rheological, and chemical properties are critical to innovation and production efficiency.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LookSync: Large-Scale Visual Product Search System for AI-Generated Fashion Looks</title>
<link>https://arxiv.org/abs/2511.00072</link>
<guid>https://arxiv.org/abs/2511.00072</guid>
<content:encoded><![CDATA[
arXiv:2511.00072v1 Announce Type: cross 
Abstract: Generative AI is reshaping fashion by enabling virtual looks and avatars making it essential to find real products that best match AI-generated styles. We propose an end-to-end product search system that has been deployed in a real-world, internet scale which ensures that AI-generated looks presented to users are matched with the most visually and semantically similar products from the indexed vector space. The search pipeline is composed of four key components: query generation, vectorization, candidate retrieval, and reranking based on AI-generated looks. Recommendation quality is evaluated using human-judged accuracy scores. The system currently serves more than 350,000 AI Looks in production per day, covering diverse product categories across global markets of over 12 million products. In our experiments, we observed that across multiple annotators and categories, CLIP outperformed alternative models by a small relative margin of 3--7\% in mean opinion scores. These improvements, though modest in absolute numbers, resulted in noticeably better user perception matches, establishing CLIP as the most reliable backbone for production deployment.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RailEstate: An Interactive System for Metro Linked Property Trends</title>
<link>https://arxiv.org/abs/2511.00078</link>
<guid>https://arxiv.org/abs/2511.00078</guid>
<content:encoded><![CDATA[
arXiv:2511.00078v1 Announce Type: cross 
Abstract: Access to metro systems plays a critical role in shaping urban housing markets by enhancing neighborhood accessibility and driving property demand. We present RailEstate, a novel web based system that integrates spatial analytics, natural language interfaces, and interactive forecasting to analyze how proximity to metro stations influences residential property prices in the Washington metropolitan area. Unlike static mapping tools or generic listing platforms, RailEstate combines 25 years of historical housing data with transit infrastructure to support low latency geospatial queries, time series visualizations, and predictive modeling. Users can interactively explore ZIP code level price patterns, investigate long term trends, and forecast future housing values around any metro station. A key innovation is our natural language chatbot, which translates plain-English questions e.g., What is the highest price in Falls Church in the year 2000? into executable SQL over a spatial database. This unified and interactive platform empowers urban planners, investors, and residents to derive actionable insights from metro linked housing data without requiring technical expertise.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fixed-point graph convolutional networks against adversarial attacks</title>
<link>https://arxiv.org/abs/2511.00083</link>
<guid>https://arxiv.org/abs/2511.00083</guid>
<content:encoded><![CDATA[
arXiv:2511.00083v1 Announce Type: cross 
Abstract: Adversarial attacks present a significant risk to the integrity and performance of graph neural networks, particularly in tasks where graph structure and node features are vulnerable to manipulation. In this paper, we present a novel model, called fixed-point iterative graph convolutional network (Fix-GCN), which achieves robustness against adversarial perturbations by effectively capturing higher-order node neighborhood information in the graph without additional memory or computational complexity. Specifically, we introduce a versatile spectral modulation filter and derive the feature propagation rule of our model using fixed-point iteration. Unlike traditional defense mechanisms that rely on additional design elements to counteract attacks, the proposed graph filter provides a flexible-pass filtering approach, allowing it to selectively attenuate high-frequency components while preserving low-frequency structural information in the graph signal. By iteratively updating node representations, our model offers a flexible and efficient framework for preserving essential graph information while mitigating the impact of adversarial manipulation. We demonstrate the effectiveness of the proposed model through extensive experiments on various benchmark graph datasets, showcasing its resilience against adversarial attacks.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Application of predictive machine learning in pen &amp; paper RPG game design</title>
<link>https://arxiv.org/abs/2511.00084</link>
<guid>https://arxiv.org/abs/2511.00084</guid>
<content:encoded><![CDATA[
arXiv:2511.00084v1 Announce Type: cross 
Abstract: In recent years, the pen and paper RPG market has experienced significant growth. As a result, companies are increasingly exploring the integration of AI technologies to enhance player experience and gain a competitive edge.
  One of the key challenges faced by publishers is designing new opponents and estimating their challenge level. Currently, there are no automated methods for determining a monster's level; the only approaches used are based on manual testing and expert evaluation. Although these manual methods can provide reasonably accurate estimates, they are time-consuming and resource-intensive.
  Level prediction can be approached using ordinal regression techniques. This thesis presents an overview and evaluation of state-of-the-art methods for this task. It also details the construction of a dedicated dataset for level estimation. Furthermore, a human-inspired model was developed to serve as a benchmark, allowing comparison between machine learning algorithms and the approach typically employed by pen and paper RPG publishers. In addition, a specialized evaluation procedure, grounded in domain knowledge, was designed to assess model performance and facilitate meaningful comparisons.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MaGNet: A Mamba Dual-Hypergraph Network for Stock Prediction via Temporal-Causal and Global Relational Learning</title>
<link>https://arxiv.org/abs/2511.00085</link>
<guid>https://arxiv.org/abs/2511.00085</guid>
<content:encoded><![CDATA[
arXiv:2511.00085v1 Announce Type: cross 
Abstract: Stock trend prediction is crucial for profitable trading strategies and portfolio management yet remains challenging due to market volatility, complex temporal dynamics and multifaceted inter-stock relationships. Existing methods struggle to effectively capture temporal dependencies and dynamic inter-stock interactions, often neglecting cross-sectional market influences, relying on static correlations, employing uniform treatments of nodes and edges, and conflating diverse relationships. This work introduces MaGNet, a novel Mamba dual-hyperGraph Network for stock prediction, integrating three key innovations: (1) a MAGE block, which leverages bidirectional Mamba with adaptive gating mechanisms for contextual temporal modeling and integrates a sparse Mixture-of-Experts layer to enable dynamic adaptation to diverse market conditions, alongside multi-head attention for capturing global dependencies; (2) Feature-wise and Stock-wise 2D Spatiotemporal Attention modules enable precise fusion of multivariate features and cross-stock dependencies, effectively enhancing informativeness while preserving intrinsic data structures, bridging temporal modeling with relational reasoning; and (3) a dual hypergraph framework consisting of the Temporal-Causal Hypergraph (TCH) that captures fine-grained causal dependencies with temporal constraints, and Global Probabilistic Hypergraph (GPH) that models market-wide patterns through soft hyperedge assignments and Jensen-Shannon Divergence weighting mechanism, jointly disentangling localized temporal influences from instantaneous global structures for multi-scale relational learning. Extensive experiments on six major stock indices demonstrate MaGNet outperforms state-of-the-art methods in both superior predictive performance and exceptional investment returns with robust risk management capabilities. Codes available at: https://github.com/PeilinTime/MaGNet.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph</title>
<link>https://arxiv.org/abs/2511.00086</link>
<guid>https://arxiv.org/abs/2511.00086</guid>
<content:encoded><![CDATA[
arXiv:2511.00086v1 Announce Type: cross 
Abstract: Test-Time Scaling (TTS) improves large language models (LLMs) by allocating additional computation during inference, typically through parallel, sequential, or hybrid scaling. However, prior studies often assume fixed collaboration architectures (e.g., topologies) and single-model usage, overlooking that optimal architectures and model combinations can vary across tasks. Therefore, we study the novel problem of searching for compute-optimal model combinations and architectures in TTS under a fixed budget. We formalize it as a multi-LLM collaboration graph, where nodes encode roles and LLM model assignments, and edges capture information flow. This problem is challenging because (i) the combinatorial search space is prohibitively large, and (ii) task-specific requirements demand tailored designs. To address these, we reformulate the problem as probabilistic graph optimization and, through pilot experiments, derive three empirical insights into TTS collaboration graphs. Guided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented framework that mirrors the REINFORCE pipeline by mapping sampling-gradient-update to sampling-feedback-update, where feedback serves as a textual gradient to update the probabilistic graph and efficiently search for optimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE outperforms both traditional and LLM-based baselines in sample efficiency and search performance, and effectively identifies optimal graphs under joint objectives of accuracy and inference latency.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adding New Capability in Existing Scientific Application with LLM Assistance</title>
<link>https://arxiv.org/abs/2511.00087</link>
<guid>https://arxiv.org/abs/2511.00087</guid>
<content:encoded><![CDATA[
arXiv:2511.00087v1 Announce Type: cross 
Abstract: With the emergence and rapid evolution of large language models (LLM), automating coding tasks has become an important research topic. Many efforts are underway and literature abounds about the efficacy of models and their ability to generate code. A less explored aspect of code generation is for new algorithms, where the training dataset would not have included any previous example of similar code. In this paper we propose a new methodology for writing code from scratch for a new algorithm using LLM assistance, and describe enhancement of a previously developed code-translation tool, Code-Scribe, for new code generation.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail</title>
<link>https://arxiv.org/abs/2511.00088</link>
<guid>https://arxiv.org/abs/2511.00088</guid>
<content:encoded><![CDATA[
arXiv:2511.00088v1 Announce Type: cross 
Abstract: End-to-end architectures trained via imitation learning have advanced autonomous driving by scaling model size and data, yet performance remains brittle in safety-critical long-tail scenarios where supervision is sparse and causal understanding is limited. To address this, we introduce Alpamayo-R1 (AR1), a vision-language-action model (VLA) that integrates Chain of Causation reasoning with trajectory planning to enhance decision-making in complex driving scenarios. Our approach features three key innovations: (1) the Chain of Causation (CoC) dataset, built through a hybrid auto-labeling and human-in-the-loop pipeline producing decision-grounded, causally linked reasoning traces aligned with driving behaviors; (2) a modular VLA architecture combining Cosmos-Reason, a Vision-Language Model pre-trained for Physical AI applications, with a diffusion-based trajectory decoder that generates dynamically feasible plans in real time; (3) a multi-stage training strategy using supervised fine-tuning to elicit reasoning and reinforcement learning (RL) to optimize reasoning quality via large reasoning model feedback and enforce reasoning-action consistency. Evaluation shows AR1 achieves up to a 12% improvement in planning accuracy on challenging cases compared to a trajectory-only baseline, with a 35% reduction in off-road rate and 25% reduction in close encounter rate in closed-loop simulation. RL post-training improves reasoning quality by 45% as measured by a large reasoning model critic and reasoning-action consistency by 37%. Model scaling from 0.5B to 7B parameters shows consistent improvements. On-vehicle road tests confirm real-time performance (99 ms latency) and successful urban deployment. By bridging interpretable reasoning with precise control, AR1 demonstrates a practical path towards Level 4 autonomous driving. We plan to release AR1 models and a subset of the CoC in a future update.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation</title>
<link>https://arxiv.org/abs/2511.00090</link>
<guid>https://arxiv.org/abs/2511.00090</guid>
<content:encoded><![CDATA[
arXiv:2511.00090v1 Announce Type: cross 
Abstract: We present LeMiCa, a training-free and efficient acceleration framework for diffusion-based video generation. While existing caching strategies primarily focus on reducing local heuristic errors, they often overlook the accumulation of global errors, leading to noticeable content degradation between accelerated and original videos. To address this issue, we formulate cache scheduling as a directed graph with error-weighted edges and introduce a Lexicographic Minimax Path Optimization strategy that explicitly bounds the worst-case path error. This approach substantially improves the consistency of global content and style across generated frames. Extensive experiments on multiple text-to-video benchmarks demonstrate that LeMiCa delivers dual improvements in both inference speed and generation quality. Notably, our method achieves a 2.9x speedup on the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming prior caching techniques. Importantly, these gains come with minimal perceptual quality degradation, making LeMiCa a robust and generalizable paradigm for accelerating diffusion-based video generation. We believe this approach can serve as a strong foundation for future research on efficient and reliable video synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Digital Twin based Automatic Reconfiguration of Robotic Systems in Smart Environments</title>
<link>https://arxiv.org/abs/2511.00094</link>
<guid>https://arxiv.org/abs/2511.00094</guid>
<content:encoded><![CDATA[
arXiv:2511.00094v1 Announce Type: cross 
Abstract: Robotic systems have become integral to smart environments, enabling applications ranging from urban surveillance and automated agriculture to industrial automation. However, their effective operation in dynamic settings - such as smart cities and precision farming - is challenged by continuously evolving topographies and environmental conditions. Traditional control systems often struggle to adapt quickly, leading to inefficiencies or operational failures. To address this limitation, we propose a novel framework for autonomous and dynamic reconfiguration of robotic controllers using Digital Twin technology. Our approach leverages a virtual replica of the robot's operational environment to simulate and optimize movement trajectories in response to real-world changes. By recalculating paths and control parameters in the Digital Twin and deploying the updated code to the physical robot, our method ensures rapid and reliable adaptation without manual intervention. This work advances the integration of Digital Twins in robotics, offering a scalable solution for enhancing autonomy in smart, dynamic environments.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpinalSAM-R1: A Vision-Language Multimodal Interactive System for Spine CT Segmentation</title>
<link>https://arxiv.org/abs/2511.00095</link>
<guid>https://arxiv.org/abs/2511.00095</guid>
<content:encoded><![CDATA[
arXiv:2511.00095v1 Announce Type: cross 
Abstract: The anatomical structure segmentation of the spine and adjacent structures from computed tomography (CT) images is a key step for spinal disease diagnosis and treatment. However, the segmentation of CT images is impeded by low contrast and complex vertebral boundaries. Although advanced models such as the Segment Anything Model (SAM) have shown promise in various segmentation tasks, their performance in spinal CT imaging is limited by high annotation requirements and poor domain adaptability. To address these limitations, we propose SpinalSAM-R1, a multimodal vision-language interactive system that integrates a fine-tuned SAM with DeepSeek-R1, for spine CT image segmentation. Specifically, our SpinalSAM-R1 introduces an anatomy-guided attention mechanism to improve spine segmentation performance, and a semantics-driven interaction protocol powered by DeepSeek-R1, enabling natural language-guided refinement. The SpinalSAM-R1 is fine-tuned using Low-Rank Adaptation (LoRA) for efficient adaptation. We validate our SpinalSAM-R1 on the spine anatomical structure with CT images. Experimental results suggest that our method achieves superior segmentation performance. Meanwhile, we develop a PyQt5-based interactive software, which supports point, box, and text-based prompts. The system supports 11 clinical operations with 94.3\% parsing accuracy and sub-800 ms response times. The software is released on https://github.com/6jm233333/spinalsam-r1.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Urban-MAS: Human-Centered Urban Prediction with LLM-Based Multi-Agent System</title>
<link>https://arxiv.org/abs/2511.00096</link>
<guid>https://arxiv.org/abs/2511.00096</guid>
<content:encoded><![CDATA[
arXiv:2511.00096v1 Announce Type: cross 
Abstract: Urban Artificial Intelligence (Urban AI) has advanced human-centered urban tasks such as perception prediction and human dynamics. Large Language Models (LLMs) can integrate multimodal inputs to address heterogeneous data in complex urban systems but often underperform on domain-specific tasks. Urban-MAS, an LLM-based Multi-Agent System (MAS) framework, is introduced for human-centered urban prediction under zero-shot settings. It includes three agent types: Predictive Factor Guidance Agents, which prioritize key predictive factors to guide knowledge extraction and enhance the effectiveness of compressed urban knowledge in LLMs; Reliable UrbanInfo Extraction Agents, which improve robustness by comparing multiple outputs, validating consistency, and re-extracting when conflicts occur; and Multi-UrbanInfo Inference Agents, which integrate extracted multi-source information across dimensions for prediction. Experiments on running-amount prediction and urban perception across Tokyo, Milan, and Seattle demonstrate that Urban-MAS substantially reduces errors compared to single-LLM baselines. Ablation studies indicate that Predictive Factor Guidance Agents are most critical for enhancing predictive performance, positioning Urban-MAS as a scalable paradigm for human-centered urban AI prediction. Code is available on the project website:https://github.com/THETUREHOOHA/UrbanMAS
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphKeeper: Graph Domain-Incremental Learning via Knowledge Disentanglement and Preservation</title>
<link>https://arxiv.org/abs/2511.00097</link>
<guid>https://arxiv.org/abs/2511.00097</guid>
<content:encoded><![CDATA[
arXiv:2511.00097v1 Announce Type: cross 
Abstract: Graph incremental learning (GIL), which continuously updates graph models by sequential knowledge acquisition, has garnered significant interest recently. However, existing GIL approaches focus on task-incremental and class-incremental scenarios within a single domain. Graph domain-incremental learning (Domain-IL), aiming at updating models across multiple graph domains, has become critical with the development of graph foundation models (GFMs), but remains unexplored in the literature. In this paper, we propose Graph Domain-Incremental Learning via Knowledge Dientanglement and Preservation (GraphKeeper), to address catastrophic forgetting in Domain-IL scenario from the perspectives of embedding shifts and decision boundary deviations. Specifically, to prevent embedding shifts and confusion across incremental graph domains, we first propose the domain-specific parameter-efficient fine-tuning together with intra- and inter-domain disentanglement objectives. Consequently, to maintain a stable decision boundary, we introduce deviation-free knowledge preservation to continuously fit incremental domains. Additionally, for graphs with unobservable domains, we perform domain-aware distribution discrimination to obtain precise embeddings. Extensive experiments demonstrate the proposed GraphKeeper achieves state-of-the-art results with 6.5%~16.6% improvement over the runner-up with negligible forgetting. Moreover, we show GraphKeeper can be seamlessly integrated with various representative GFMs, highlighting its broad applicative potential.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A filtering scheme for confocal laser endomicroscopy (CLE)-video sequences for self-supervised learning</title>
<link>https://arxiv.org/abs/2511.00098</link>
<guid>https://arxiv.org/abs/2511.00098</guid>
<content:encoded><![CDATA[
arXiv:2511.00098v1 Announce Type: cross 
Abstract: Confocal laser endomicroscopy (CLE) is a non-invasive, real-time imaging modality that can be used for in-situ, in-vivo imaging and the microstructural analysis of mucous structures. The diagnosis using CLE is, however, complicated by images being hard to interpret for non-experienced physicians. Utilizing machine learning as an augmentative tool would hence be beneficial, but is complicated by the shortage of histopathology-correlated CLE imaging sequences with respect to the plurality of patterns in this domain, leading to overfitting of machine learning models. To overcome this, self-supervised learning (SSL) can be employed on larger unlabeled datasets. CLE is a video-based modality with high inter-frame correlation, leading to a non-stratified data distribution for SSL training. In this work, we propose a filter functionality on CLE video sequences to reduce the dataset redundancy in SSL training and improve SSL training convergence and training efficiency. We use four state-of-the-art baseline networks and a SSL teacher-student network with a vision transformer small backbone for the evaluation. These networks were evaluated on downstream tasks for a sinonasal tumor dataset and a squamous cell carcinoma of the skin dataset. On both datasets, we found the highest test accuracy on the filtered SSL-pretrained model, with 67.48% and 73.52%, both considerably outperforming their non-SSL baselines. Our results show that SSL is an effective method for CLE pretraining. Further, we show that our proposed CLE video filter can be utilized to improve training efficiency in self-supervised scenarios, resulting in a reduction of 67% in training time.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A generative adversarial network optimization method for damage detection and digital twinning by deep AI fault learning: Z24 Bridge structural health monitoring benchmark validation</title>
<link>https://arxiv.org/abs/2511.00099</link>
<guid>https://arxiv.org/abs/2511.00099</guid>
<content:encoded><![CDATA[
arXiv:2511.00099v1 Announce Type: cross 
Abstract: The optimization-based damage detection and damage state digital twinning capabilities are examined here of a novel conditional-labeled generative adversarial network methodology. The framework outperforms current approaches for fault anomaly detection as no prior information is required for the health state of the system: a topic of high significance for real-world applications. Specifically, current artificial intelligence-based digital twinning approaches suffer from the uncertainty related to obtaining poor predictions when a low number of measurements is available, physics knowledge is missing, or when the damage state is unknown. To this end, an unsupervised framework is examined and validated rigorously on the benchmark structural health monitoring measurements of Z24 Bridge: a post-tensioned concrete highway bridge in Switzerland. In implementing the approach, firstly, different same damage-level measurements are used as inputs, while the model is forced to converge conditionally to two different damage states. Secondly, the process is repeated for a different group of measurements. Finally, the convergence scores are compared to identify which one belongs to a different damage state. The process for both healthy-to-healthy and damage-to-healthy input data creates, simultaneously, measurements for digital twinning purposes at different damage states, capable of pattern recognition and machine learning data generation. Further to this process, a support vector machine classifier and a principal component analysis procedure is developed to assess the generated and real measurements of each damage category, serving as a secondary new dynamics learning indicator in damage scenarios. Importantly, the approach is shown to capture accurately damage over healthy measurements, providing a powerful tool for vibration-based system-level monitoring and scalable infrastructure resilience.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Loquetier: A Virtualized Multi-LoRA Framework for Unified LLM Fine-tuning and Serving</title>
<link>https://arxiv.org/abs/2511.00101</link>
<guid>https://arxiv.org/abs/2511.00101</guid>
<content:encoded><![CDATA[
arXiv:2511.00101v1 Announce Type: cross 
Abstract: Low-Rank Adaptation (LoRA) has become a widely adopted parameter-efficient fine-tuning (PEFT) technique for adapting large language models (LLMs) to downstream tasks. While prior work has explored strategies for integrating LLM training and serving, there still remains a gap in unifying fine-tuning and inference for LoRA-based models. We present Loquetier, a virtualized multi-LoRA framework that seamlessly integrates LoRA fine-tuning and serving within a single runtime. Loquetier introduces two key components: (1) a Virtualized Module that isolates PEFT-based modifications and supports multiple adapters on a shared base model, and (2) an optimized computation flow with a kernel design that merges fine-tuning and inference paths in forward propagation, enabling efficient batching and minimizing kernel invocation overhead. Extensive experiments across three task settings show that Loquetier consistently outperforms existing baselines in both performance and flexibility, achieving up to $3.0\times$ the throughput of the state-of-the-art co-serving system on inference-only tasks and $46.4\times$ higher SLO attainment than PEFT on unified fine-tuning and inference tasks. The implementation of Loquetier is publicly available at https://github.com/NJUDeepEngine/Loquetier.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Discovery of Conservation Laws via Hybrid Neural ODE-Transformers</title>
<link>https://arxiv.org/abs/2511.00102</link>
<guid>https://arxiv.org/abs/2511.00102</guid>
<content:encoded><![CDATA[
arXiv:2511.00102v1 Announce Type: cross 
Abstract: The discovery of conservation laws is a cornerstone of scientific progress. However, identifying these invariants from observational data remains a significant challenge. We propose a hybrid framework to automate the discovery of conserved quantities from noisy trajectory data. Our approach integrates three components: (1) a Neural Ordinary Differential Equation (Neural ODE) that learns a continuous model of the system's dynamics, (2) a Transformer that generates symbolic candidate invariants conditioned on the learned vector field, and (3) a symbolic-numeric verifier that provides a strong numerical certificate for the validity of these candidates. We test our framework on canonical physical systems and show that it significantly outperforms baselines that operate directly on trajectory data. This work demonstrates the robustness of a decoupled learn-then-search approach for discovering mathematical principles from imperfect data.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FreeSliders: Training-Free, Modality-Agnostic Concept Sliders for Fine-Grained Diffusion Control in Images, Audio, and Video</title>
<link>https://arxiv.org/abs/2511.00103</link>
<guid>https://arxiv.org/abs/2511.00103</guid>
<content:encoded><![CDATA[
arXiv:2511.00103v1 Announce Type: cross 
Abstract: Diffusion models have become state-of-the-art generative models for images, audio, and video, yet enabling fine-grained controllable generation, i.e., continuously steering specific concepts without disturbing unrelated content, remains challenging. Concept Sliders (CS) offer a promising direction by discovering semantic directions through textual contrasts, but they require per-concept training and architecture-specific fine-tuning (e.g., LoRA), limiting scalability to new modalities. In this work we introduce FreeSliders, a simple yet effective approach that is fully training-free and modality-agnostic, achieved by partially estimating the CS formula during inference. To support modality-agnostic evaluation, we extend the CS benchmark to include both video and audio, establishing the first suite for fine-grained concept generation control with multiple modalities. We further propose three evaluation properties along with new metrics to improve evaluation quality. Finally, we identify an open problem of scale selection and non-linear traversals and introduce a two-stage procedure that automatically detects saturation points and reparameterizes traversal for perceptually uniform, semantically meaningful edits. Extensive experiments demonstrate that our method enables plug-and-play, training-free concept control across modalities, improves over existing baselines, and establishes new tools for principled controllable generation. An interactive presentation of our benchmark and method is available at: https://azencot-group.github.io/FreeSliders/
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Artificial Intelligence in Elementary STEM Education: A Systematic Review of Current Applications and Future Challenges</title>
<link>https://arxiv.org/abs/2511.00105</link>
<guid>https://arxiv.org/abs/2511.00105</guid>
<content:encoded><![CDATA[
arXiv:2511.00105v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) is transforming elementary STEM education, yet evidence remains fragmented. This systematic review synthesizes 258 studies (2020-2025) examining AI applications across eight categories: intelligent tutoring systems (45% of studies), learning analytics (18%), automated assessment (12%), computer vision (8%), educational robotics (7%), multimodal sensing (6%), AI-enhanced extended reality (XR) (4%), and adaptive content generation. The analysis shows that most studies focus on upper elementary grades (65%) and mathematics (38%), with limited cross-disciplinary STEM integration (15%). While conversational AI demonstrates moderate effectiveness (d = 0.45-0.70 where reported), only 34% of studies include standardized effect sizes. Eight major gaps limit real-world impact: fragmented ecosystems, developmental inappropriateness, infrastructure barriers, lack of privacy frameworks, weak STEM integration, equity disparities, teacher marginalization, and narrow assessment scopes. Geographic distribution is also uneven, with 90% of studies originating from North America, East Asia, and Europe. Future directions call for interoperable architectures that support authentic STEM integration, grade-appropriate design, privacy-preserving analytics, and teacher-centered implementations that enhance rather than replace human expertise.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wayfinding through the AI wilderness: Mapping rhetorics of ChatGPT prompt writing on X (formerly Twitter) to promote critical AI literacies</title>
<link>https://arxiv.org/abs/2511.00106</link>
<guid>https://arxiv.org/abs/2511.00106</guid>
<content:encoded><![CDATA[
arXiv:2511.00106v1 Announce Type: cross 
Abstract: In this paper, we demonstrate how studying the rhetorics of ChatGPT prompt writing on social media can promote critical AI literacies. Prompt writing is the process of writing instructions for generative AI tools like ChatGPT to elicit desired outputs and there has been an upsurge of conversations about it on social media. To study this rhetorical activity, we build on four overlapping traditions of digital writing research in computers and composition that inform how we frame literacies, how we study social media rhetorics, how we engage iteratively and reflexively with methodologies and technologies, and how we blend computational methods with qualitative methods. Drawing on these four traditions, our paper shows our iterative research process through which we gathered and analyzed a dataset of 32,000 posts (formerly known as tweets) from X (formerly Twitter) about prompt writing posted between November 2022 to May 2023. We present five themes about these emerging AI literacy practices: (1) areas of communication impacted by prompt writing, (2) micro-literacy resources shared for prompt writing, (3) market rhetoric shaping prompt writing, (4) rhetorical characteristics of prompts, and (5) definitions of prompt writing. In discussing these themes and our methodologies, we highlight takeaways for digital writing teachers and researchers who are teaching and analyzing critical AI literacies.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Powered High Quality Text to Video Generation with Enhanced Temporal Consistency</title>
<link>https://arxiv.org/abs/2511.00107</link>
<guid>https://arxiv.org/abs/2511.00107</guid>
<content:encoded><![CDATA[
arXiv:2511.00107v1 Announce Type: cross 
Abstract: Text to video generation has emerged as a critical frontier in generative artificial intelligence, yet existing approaches struggle with maintaining temporal consistency, compositional understanding, and fine grained control over visual narratives. We present MOVAI (Multimodal Original Video AI), a novel hierarchical framework that integrates compositional scene understanding with temporal aware diffusion models for high fidelity text to video synthesis. Our approach introduces three key innovations: (1) a Compositional Scene Parser (CSP) that decomposes textual descriptions into hierarchical scene graphs with temporal annotations, (2) a Temporal-Spatial Attention Mechanism (TSAM) that ensures coherent motion dynamics across frames while preserving spatial details, and (3) a Progressive Video Refinement (PVR) module that iteratively enhances video quality through multi-scale temporal reasoning. Extensive experiments on standard benchmarks demonstrate that MOVAI achieves state-of-the-art performance, improving video quality metrics by 15.3% in LPIPS, 12.7% in FVD, and 18.9% in user preference studies compared to existing methods. Our framework shows particular strength in generating complex multi-object scenes with realistic temporal dynamics and fine-grained semantic control.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pelican-VL 1.0: A Foundation Brain Model for Embodied Intelligence</title>
<link>https://arxiv.org/abs/2511.00108</link>
<guid>https://arxiv.org/abs/2511.00108</guid>
<content:encoded><![CDATA[
arXiv:2511.00108v1 Announce Type: cross 
Abstract: This report presents Pelican-VL 1.0, a new family of open-source embodied brain models with parameter scales ranging from 7 billion to 72 billion. Our explicit mission is clearly stated as: To embed powerful intelligence into various embodiments. Pelican-VL 1.0 is currently the largest-scale open-source embodied multimodal brain model. Its core advantage lies in the in-depth integration of data power and intelligent adaptive learning mechanisms. Specifically, metaloop distilled a high-quality dataset from a raw dataset containing 4+ billion tokens. Pelican-VL 1.0 is trained on a large-scale cluster of 1000+ A800 GPUs, consuming over 50k+ A800 GPU-hours per checkpoint. This translates to a 20.3% performance uplift from its base model and outperforms 100B-level open-source counterparts by 10.6%, placing it on par with leading proprietary systems on well-known embodied benchmarks. We establish a novel framework, DPPO (Deliberate Practice Policy Optimization), inspired by human metacognition to train Pelican-VL 1.0. We operationalize this as a metaloop that teaches the AI to practice deliberately, which is a RL-Refine-Diagnose-SFT loop.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chain of Time: In-Context Physical Simulation with Image Generation Models</title>
<link>https://arxiv.org/abs/2511.00110</link>
<guid>https://arxiv.org/abs/2511.00110</guid>
<content:encoded><![CDATA[
arXiv:2511.00110v1 Announce Type: cross 
Abstract: We propose a novel cognitively-inspired method to improve and interpret physical simulation in vision-language models. Our ``Chain of Time" method involves generating a series of intermediate images during a simulation, and it is motivated by in-context reasoning in machine learning, as well as mental simulation in humans. Chain of Time is used at inference time, and requires no additional fine-tuning. We apply the Chain-of-Time method to synthetic and real-world domains, including 2-D graphics simulations and natural 3-D videos. These domains test a variety of particular physical properties, including velocity, acceleration, fluid dynamics, and conservation of momentum. We found that using Chain-of-Time simulation substantially improves the performance of a state-of-the-art image generation model. Beyond examining performance, we also analyzed the specific states of the world simulated by an image model at each time step, which sheds light on the dynamics underlying these simulations. This analysis reveals insights that are hidden from traditional evaluations of physical reasoning, including cases where an image generation model is able to simulate physical properties that unfold over time, such as velocity, gravity, and collisions. Our analysis also highlights particular cases where the image generation model struggles to infer particular physical parameters from input images, despite being capable of simulating relevant physical processes.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-DRL: Teach and Learn in Reality</title>
<link>https://arxiv.org/abs/2511.00112</link>
<guid>https://arxiv.org/abs/2511.00112</guid>
<content:encoded><![CDATA[
arXiv:2511.00112v1 Announce Type: cross 
Abstract: This paper introduces the Real-DRL framework for safety-critical autonomous systems, enabling runtime learning of a deep reinforcement learning (DRL) agent to develop safe and high-performance action policies in real plants (i.e., real physical systems to be controlled), while prioritizing safety! The Real-DRL consists of three interactive components: a DRL-Student, a PHY-Teacher, and a Trigger. The DRL-Student is a DRL agent that innovates in the dual self-learning and teaching-to-learn paradigm and the real-time safety-informed batch sampling. On the other hand, PHY-Teacher is a physics-model-based design of action policies that focuses solely on safety-critical functions. PHY-Teacher is novel in its real-time patch for two key missions: i) fostering the teaching-to-learn paradigm for DRL-Student and ii) backing up the safety of real plants. The Trigger manages the interaction between the DRL-Student and the PHY-Teacher. Powered by the three interactive components, the Real-DRL can effectively address safety challenges that arise from the unknown unknowns and the Sim2Real gap. Additionally, Real-DRL notably features i) assured safety, ii) automatic hierarchy learning (i.e., safety-first learning and then high-performance learning), and iii) safety-informed batch sampling to address the learning experience imbalance caused by corner cases. Experiments with a real quadruped robot, a quadruped robot in NVIDIA Isaac Gym, and a cart-pole system, along with comparisons and ablation studies, demonstrate the Real-DRL's effectiveness and unique features.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>End-to-End Framework Integrating Generative AI and Deep Reinforcement Learning for Autonomous Ultrasound Scanning</title>
<link>https://arxiv.org/abs/2511.00114</link>
<guid>https://arxiv.org/abs/2511.00114</guid>
<content:encoded><![CDATA[
arXiv:2511.00114v1 Announce Type: cross 
Abstract: Cardiac ultrasound (US) is among the most widely used diagnostic tools in cardiology for assessing heart health, but its effectiveness is limited by operator dependence, time constraints, and human error. The shortage of trained professionals, especially in remote areas, further restricts access. These issues underscore the need for automated solutions that can ensure consistent, and accessible cardiac imaging regardless of operator skill or location. Recent progress in artificial intelligence (AI), especially in deep reinforcement learning (DRL), has gained attention for enabling autonomous decision-making. However, existing DRL-based approaches to cardiac US scanning lack reproducibility, rely on proprietary data, and use simplified models. Motivated by these gaps, we present the first end-to-end framework that integrates generative AI and DRL to enable autonomous and reproducible cardiac US scanning. The framework comprises two components: (i) a conditional generative simulator combining Generative Adversarial Networks (GANs) with Variational Autoencoders (VAEs), that models the cardiac US environment producing realistic action-conditioned images; and (ii) a DRL module that leverages this simulator to learn autonomous, accurate scanning policies. The proposed framework delivers AI-driven guidance through expert-validated models that classify image type and assess quality, supports conditional generation of realistic US images, and establishes a reproducible foundation extendable to other organs. To ensure reproducibility, a publicly available dataset of real cardiac US scans is released. The solution is validated through several experiments. The VAE-GAN is benchmarked against existing GAN variants, with performance assessed using qualitative and quantitative approaches, while the DRL-based scanning system is evaluated under varying configurations to demonstrate effectiveness.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference</title>
<link>https://arxiv.org/abs/2511.00115</link>
<guid>https://arxiv.org/abs/2511.00115</guid>
<content:encoded><![CDATA[
arXiv:2511.00115v1 Announce Type: cross 
Abstract: Personality recognition from text is typically cast as hard-label classification, which obscures the graded, prototype-like nature of human personality judgments. We present ProtoMBTI, a cognitively aligned framework for MBTI inference that operationalizes prototype theory within an LLM-based pipeline. First, we construct a balanced, quality-controlled corpus via LLM-guided multi-dimensional augmentation (semantic, linguistic, sentiment). Next, we LoRA-fine-tune a lightweight (<=2B) encoder to learn discriminative embeddings and to standardize a bank of personality prototypes. At inference, we retrieve top-k prototypes for a query post and perform a retrieve--reuse--revise--retain cycle: the model aggregates prototype evidence via prompt-based voting, revises when inconsistencies arise, and, upon correct prediction, retains the sample to continually enrich the prototype library. Across Kaggle and Pandora benchmarks, ProtoMBTI improves over baselines on both the four MBTI dichotomies and the full 16-type task, and exhibits robust cross-dataset generalization. Our results indicate that aligning the inference process with psychological prototype reasoning yields gains in accuracy, interpretability, and transfer for text-based personality modeling.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LC-Opt: Benchmarking Reinforcement Learning and Agentic AI for End-to-End Liquid Cooling Optimization in Data Centers</title>
<link>https://arxiv.org/abs/2511.00116</link>
<guid>https://arxiv.org/abs/2511.00116</guid>
<content:encoded><![CDATA[
arXiv:2511.00116v1 Announce Type: cross 
Abstract: Liquid cooling is critical for thermal management in high-density data centers with the rising AI workloads. However, machine learning-based controllers are essential to unlock greater energy efficiency and reliability, promoting sustainability. We present LC-Opt, a Sustainable Liquid Cooling (LC) benchmark environment, for reinforcement learning (RL) control strategies in energy-efficient liquid cooling of high-performance computing (HPC) systems. Built on the baseline of a high-fidelity digital twin of Oak Ridge National Lab's Frontier Supercomputer cooling system, LC-Opt provides detailed Modelica-based end-to-end models spanning site-level cooling towers to data center cabinets and server blade groups. RL agents optimize critical thermal controls like liquid supply temperature, flow rate, and granular valve actuation at the IT cabinet level, as well as cooling tower (CT) setpoints through a Gymnasium interface, with dynamic changes in workloads. This environment creates a multi-objective real-time optimization challenge balancing local thermal regulation and global energy efficiency, and also supports additional components like a heat recovery unit (HRU). We benchmark centralized and decentralized multi-agent RL approaches, demonstrate policy distillation into decision and regression trees for interpretable control, and explore LLM-based methods that explain control actions in natural language through an agentic mesh architecture designed to foster user trust and simplify system management. LC-Opt democratizes access to detailed, customizable liquid cooling models, enabling the ML community, operators, and vendors to develop sustainable data center liquid cooling control solutions.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DCcluster-Opt: Benchmarking Dynamic Multi-Objective Optimization for Geo-Distributed Data Center Workloads</title>
<link>https://arxiv.org/abs/2511.00117</link>
<guid>https://arxiv.org/abs/2511.00117</guid>
<content:encoded><![CDATA[
arXiv:2511.00117v1 Announce Type: cross 
Abstract: The increasing energy demands and carbon footprint of large-scale AI require intelligent workload management in globally distributed data centers. Yet progress is limited by the absence of benchmarks that realistically capture the interplay of time-varying environmental factors (grid carbon intensity, electricity prices, weather), detailed data center physics (CPUs, GPUs, memory, HVAC energy), and geo-distributed network dynamics (latency and transmission costs). To bridge this gap, we present DCcluster-Opt: an open-source, high-fidelity simulation benchmark for sustainable, geo-temporal task scheduling. DCcluster-Opt combines curated real-world datasets, including AI workload traces, grid carbon intensity, electricity markets, weather across 20 global regions, cloud transmission costs, and empirical network delay parameters with physics-informed models of data center operations, enabling rigorous and reproducible research in sustainable computing. It presents a challenging scheduling problem where a top-level coordinating agent must dynamically reassign or defer tasks that arrive with resource and service-level agreement requirements across a configurable cluster of data centers to optimize multiple objectives. The environment also models advanced components such as heat recovery. A modular reward system enables an explicit study of trade-offs among carbon emissions, energy costs, service level agreements, and water use. It provides a Gymnasium API with baseline controllers, including reinforcement learning and rule-based strategies, to support reproducible ML research and a fair comparison of diverse algorithms. By offering a realistic, configurable, and accessible testbed, DCcluster-Opt accelerates the development and validation of next-generation sustainable computing solutions for geo-distributed data centers.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLM6D: VLM based 6Dof Pose Estimation based on RGB-D Images</title>
<link>https://arxiv.org/abs/2511.00120</link>
<guid>https://arxiv.org/abs/2511.00120</guid>
<content:encoded><![CDATA[
arXiv:2511.00120v1 Announce Type: cross 
Abstract: The primary challenge in computer vision is precisely calculating the pose of 6D objects, however many current approaches are still fragile and have trouble generalizing from synthetic data to real-world situations with fluctuating lighting, textureless objects, and significant occlusions. To address these limitations, VLM6D, a novel dual-stream architecture that leverages the distinct strengths of visual and geometric data from RGB-D input for robust and precise pose estimation. Our framework uniquely integrates two specialized encoders: a powerful, self-supervised Vision Transformer (DINOv2) processes the RGB modality, harnessing its rich, pre-trained understanding of visual grammar to achieve remarkable resilience against texture and lighting variations. Concurrently, a PointNet++ encoder processes the 3D point cloud derived from depth data, enabling robust geometric reasoning that excels even with the sparse, fragmented data typical of severe occlusion. These complementary feature streams are effectively fused to inform a multi task prediction head. We demonstrate through comprehensive experiments that VLM6D obtained new SOTA performance on the challenging Occluded-LineMOD, validating its superior robustness and accuracy.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-fluctuation phase transitions reveal sampling dynamics in diffusion models</title>
<link>https://arxiv.org/abs/2511.00124</link>
<guid>https://arxiv.org/abs/2511.00124</guid>
<content:encoded><![CDATA[
arXiv:2511.00124v1 Announce Type: cross 
Abstract: We analyse how the sampling dynamics of distributions evolve in score-based diffusion models using cross-fluctuations, a centered-moment statistic from statistical physics. Specifically, we show that starting from an unbiased isotropic normal distribution, samples undergo sharp, discrete transitions, eventually forming distinct events of a desired distribution while progressively revealing finer structure. As this process is reversible, these transitions also occur in reverse, where intermediate states progressively merge, tracing a path back to the initial distribution. We demonstrate that these transitions can be detected as discontinuities in $n^{\text{th}}$-order cross-fluctuations. For variance-preserving SDEs, we derive a closed-form for these cross-fluctuations that is efficiently computable for the reverse trajectory. We find that detecting these transitions directly boosts sampling efficiency, accelerates class-conditional and rare-class generation, and improves two zero-shot tasks--image classification and style transfer--without expensive grid search or retraining. We also show that this viewpoint unifies classical coupling and mixing from finite Markov chains with continuous dynamics while extending to stochastic SDEs and non Markovian samplers. Our framework therefore bridges discrete Markov chain theory, phase analysis, and modern generative modeling.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inferring multiple helper Dafny assertions with LLMs</title>
<link>https://arxiv.org/abs/2511.00125</link>
<guid>https://arxiv.org/abs/2511.00125</guid>
<content:encoded><![CDATA[
arXiv:2511.00125v1 Announce Type: cross 
Abstract: The Dafny verifier provides strong correctness guarantees but often requires numerous manual helper assertions, creating a significant barrier to adoption. We investigate the use of Large Language Models (LLMs) to automatically infer missing helper assertions in Dafny programs, with a primary focus on cases involving multiple missing assertions. To support this study, we extend the DafnyBench benchmark with curated datasets where one, two, or all assertions are removed, and we introduce a taxonomy of assertion types to analyze inference difficulty. Our approach refines fault localization through a hybrid method that combines LLM predictions with error-message heuristics. We implement this approach in a new tool called DAISY (Dafny Assertion Inference SYstem). While our focus is on multiple missing assertions, we also evaluate DAISY on single-assertion cases. DAISY verifies 63.4% of programs with one missing assertion and 31.7% with multiple missing assertions. Notably, many programs can be verified with fewer assertions than originally present, highlighting that proofs often admit multiple valid repair strategies and that recovering every original assertion is unnecessary. These results demonstrate that automated assertion inference can substantially reduce proof engineering effort and represent a step toward more scalable and accessible formal verification.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Model Selection for Trajectory Prediction via Pairwise Ranking and Meta-Features</title>
<link>https://arxiv.org/abs/2511.00126</link>
<guid>https://arxiv.org/abs/2511.00126</guid>
<content:encoded><![CDATA[
arXiv:2511.00126v1 Announce Type: cross 
Abstract: Recent deep trajectory predictors (e.g., Jiang et al., 2023; Zhou et al., 2022) have achieved strong average accuracy but remain unreliable in complex long-tail driving scenarios. These limitations reveal the weakness of the prevailing "one-model-fits-all" paradigm, particularly in safety-critical urban contexts where simpler physics-based models can occasionally outperform advanced networks (Kalman, 1960). To bridge this gap, we propose a dynamic multi-expert gating framework that adaptively selects the most reliable trajectory predictor among a physics-informed LSTM, a Transformer, and a fine-tuned GameFormer on a per-sample basis.
  Our method leverages internal model signals (meta-features) such as stability and uncertainty (Gal and Ghahramani, 2016), which we demonstrate to be substantially more informative than geometric scene descriptors. To the best of our knowledge, this is the first work to formulate trajectory expert selection as a pairwise-ranking problem over internal model signals (Burges et al., 2005), directly optimizing decision quality without requiring post-hoc calibration.
  Evaluated on the nuPlan-mini dataset (Caesar et al., 2021) with 1,287 samples, our LLM-enhanced tri-expert gate achieves a Final Displacement Error (FDE) of 2.567 m, representing a 9.5 percent reduction over GameFormer (2.835 m), and realizes 57.8 percent of the oracle performance bound. In open-loop simulations, after trajectory horizon alignment, the same configuration reduces FDE on left-turn scenarios by approximately 10 percent, demonstrating consistent improvements across both offline validation and open-loop evaluation. These results indicate that adaptive hybrid systems enhance trajectory reliability in safety-critical autonomous driving, providing a practical pathway beyond static single-model paradigms.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Casing Collar Identification using AlexNet-based Neural Networks for Depth Measurement in Oil and Gas Wells</title>
<link>https://arxiv.org/abs/2511.00129</link>
<guid>https://arxiv.org/abs/2511.00129</guid>
<content:encoded><![CDATA[
arXiv:2511.00129v1 Announce Type: cross 
Abstract: Accurate downhole depth measurement is essential for oil and gas well operations, directly influencing reservoir contact, production efficiency, and operational safety. Collar correlation using a casing collar locator (CCL) is fundamental for precise depth calibration. While neural network-based CCL signal recognition has achieved significant progress in collar identification, preprocessing methods for such applications remain underdeveloped. Moreover, the limited availability of real well data poses substantial challenges for training neural network models that require extensive datasets. This paper presents a system integrated into downhole tools for CCL signal acquisition to facilitate dataset construction. We propose comprehensive preprocessing methods for data augmentation and evaluate their effectiveness using our AlexNet-based neural network models. Through systematic experimentation across various configuration combinations, we analyze the contribution of each augmentation method. Results demonstrate that standardization, label distribution smoothing (LDS), and random cropping are fundamental requirements for model training, while label smoothing regularization (LSR), time scaling, and multiple sampling significantly enhance model generalization capability. The F1 scores of our two benchmark models trained with the proposed augmentation methods maximumly improve from 0.937 and 0.952 to 1.0 and 1.0, respectively. Performance validation on real CCL waveforms confirms the effectiveness and practical applicability of our approach. This work addresses the gaps in data augmentation methodologies for training casing collar recognition models in CCL data-limited environments.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feature Importance Guided Random Forest Learning with Simulated Annealing Based Hyperparameter Tuning</title>
<link>https://arxiv.org/abs/2511.00133</link>
<guid>https://arxiv.org/abs/2511.00133</guid>
<content:encoded><![CDATA[
arXiv:2511.00133v1 Announce Type: cross 
Abstract: This paper introduces a novel framework for enhancing Random Forest classifiers by integrating probabilistic feature sampling and hyperparameter tuning via Simulated Annealing. The proposed framework exhibits substantial advancements in predictive accuracy and generalization, adeptly tackling the multifaceted challenges of robust classification across diverse domains, including credit risk evaluation, anomaly detection in IoT ecosystems, early-stage medical diagnostics, and high-dimensional biological data analysis. To overcome the limitations of conventional Random Forests, we present an approach that places stronger emphasis on capturing the most relevant signals from data while enabling adaptive hyperparameter configuration. The model is guided towards features that contribute more meaningfully to classification and optimizing this with dynamic parameter tuning. The results demonstrate consistent accuracy improvements and meaningful insights into feature relevance, showcasing the efficacy of combining importance aware sampling and metaheuristic optimization.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Dual Large Language Models Architecture with Herald Guided Prompts for Parallel Fine Grained Traffic Signal Control</title>
<link>https://arxiv.org/abs/2511.00136</link>
<guid>https://arxiv.org/abs/2511.00136</guid>
<content:encoded><![CDATA[
arXiv:2511.00136v1 Announce Type: cross 
Abstract: Leveraging large language models (LLMs) in traffic signal control (TSC) improves optimization efficiency and interpretability compared to traditional reinforcement learning (RL) methods. However, existing LLM-based approaches are limited by fixed time signal durations and are prone to hallucination errors, while RL methods lack robustness in signal timing decisions and suffer from poor generalization. To address these challenges, this paper proposes HeraldLight, a dual LLMs architecture enhanced by Herald guided prompts. The Herald Module extracts contextual information and forecasts queue lengths for each traffic phase based on real-time conditions. The first LLM, LLM-Agent, uses these forecasts to make fine grained traffic signal control, while the second LLM, LLM-Critic, refines LLM-Agent's outputs, correcting errors and hallucinations. These refined outputs are used for score-based fine-tuning to improve accuracy and robustness. Simulation experiments using CityFlow on real world datasets covering 224 intersections in Jinan (12), Hangzhou (16), and New York (196) demonstrate that HeraldLight outperforms state of the art baselines, achieving a 20.03% reduction in average travel time across all scenarios and a 10.74% reduction in average queue length on the Jinan and Hangzhou scenarios. The source code is available on GitHub: https://github.com/BUPT-ANTlab/HeraldLight.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>End-to-End Dexterous Arm-Hand VLA Policies via Shared Autonomy: VR Teleoperation Augmented by Autonomous Hand VLA Policy for Efficient Data Collection</title>
<link>https://arxiv.org/abs/2511.00139</link>
<guid>https://arxiv.org/abs/2511.00139</guid>
<content:encoded><![CDATA[
arXiv:2511.00139v1 Announce Type: cross 
Abstract: Achieving human-like dexterous manipulation remains a major challenge for general-purpose robots. While Vision-Language-Action (VLA) models show potential in learning skills from demonstrations, their scalability is limited by scarce high-quality training data. Existing data collection methods face inherent constraints: manual teleoperation overloads human operators, while automated planning often produces unnatural motions. We propose a Shared Autonomy framework that divides control between macro and micro motions. A human operator guides the robot's arm pose through intuitive VR teleoperation, while an autonomous DexGrasp-VLA policy handles fine-grained hand control using real-time tactile and visual feedback. This division significantly reduces cognitive load and enables efficient collection of high-quality coordinated arm-hand demonstrations. Using this data, we train an end-to-end VLA policy enhanced with our novel Arm-Hand Feature Enhancement module, which captures both distinct and shared representations of macro and micro movements for more natural coordination. Our Corrective Teleoperation system enables continuous policy improvement through human-in-the-loop failure recovery. Experiments demonstrate that our framework generates high-quality data with minimal manpower and achieves a 90% success rate across diverse objects, including unseen instances. Comprehensive evaluations validate the system's effectiveness in developing dexterous manipulation capabilities.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding</title>
<link>https://arxiv.org/abs/2511.00141</link>
<guid>https://arxiv.org/abs/2511.00141</guid>
<content:encoded><![CDATA[
arXiv:2511.00141v1 Announce Type: cross 
Abstract: Recent studies in long video understanding have harnessed the advanced visual-language reasoning capabilities of Large Multimodal Models (LMMs), driving the evolution of video-LMMs specialized for processing extended video sequences. However, the scalability of these models is severely limited by the overwhelming volume of visual tokens generated from extended video sequences. To address this challenge, this paper proposes FLoC, an efficient visual token compression framework based on the facility location function, a principled approach that swiftly selects a compact yet highly representative and diverse subset of visual tokens within a predefined budget on the number of visual tokens. By integrating the lazy greedy algorithm, our method achieves remarkable efficiency gains by swiftly selecting a compact subset of tokens, drastically reducing the number of visual tokens while guaranteeing near-optimal performance. Notably, our approach is training-free, model-agnostic, and query-agnostic, providing a versatile solution that seamlessly integrates with diverse video-LLMs and existing workflows. Extensive evaluations on large-scale benchmarks, such as Video-MME, MLVU, and LongVideoBench, demonstrate that our framework consistently surpasses recent compression techniques, highlighting not only its effectiveness and robustness in addressing the critical challenges of long video understanding, but also its efficiency in processing speed.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What a diff makes: automating code migration with large language models</title>
<link>https://arxiv.org/abs/2511.00160</link>
<guid>https://arxiv.org/abs/2511.00160</guid>
<content:encoded><![CDATA[
arXiv:2511.00160v1 Announce Type: cross 
Abstract: Modern software programs are built on stacks that are often undergoing changes that introduce updates and improvements, but may also break any project that depends upon them. In this paper we explore the use of Large Language Models (LLMs) for code migration, specifically the problem of maintaining compatibility with a dependency as it undergoes major and minor semantic version changes. We demonstrate, using metrics such as test coverage and change comparisons, that contexts containing diffs can significantly improve performance against out of the box LLMs and, in some cases, perform better than using code. We provide a dataset to assist in further development of this problem area, as well as an open-source Python package, AIMigrate, that can be used to assist with migrating code bases. In a real-world migration of TYPHOIDSIM between STARSIM versions, AIMigrate correctly identified 65% of required changes in a single run, increasing to 80% with multiple runs, with 47% of changes generated perfectly.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Effectiveness of LLMs in Temporal User Profiling for Recommendation</title>
<link>https://arxiv.org/abs/2511.00176</link>
<guid>https://arxiv.org/abs/2511.00176</guid>
<content:encoded><![CDATA[
arXiv:2511.00176v1 Announce Type: cross 
Abstract: Effectively modeling the dynamic nature of user preferences is crucial for enhancing recommendation accuracy and fostering transparency in recommender systems. Traditional user profiling often overlooks the distinction between transitory short-term interests and stable long-term preferences. This paper examines the capability of leveraging Large Language Models (LLMs) to capture these temporal dynamics, generating richer user representations through distinct short-term and long-term textual summaries of interaction histories. Our observations suggest that while LLMs tend to improve recommendation quality in domains with more active user engagement, their benefits appear less pronounced in sparser environments. This disparity likely stems from the varying distinguishability of short-term and long-term preferences across domains; the approach shows greater utility where these temporal interests are more clearly separable (e.g., Movies\&amp;TV) compared to domains with more stable user profiles (e.g., Video Games). This highlights a critical trade-off between enhanced performance and computational costs, suggesting context-dependent LLM application. Beyond predictive capability, this LLM-driven approach inherently provides an intrinsic potential for interpretability through its natural language profiles and attention weights. This work contributes insights into the practical capability and inherent interpretability of LLM-driven temporal user profiling, outlining new research directions for developing adaptive and transparent recommender systems.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Modeling Enables Molecular Structure Retrieval from Coulomb Explosion Imaging</title>
<link>https://arxiv.org/abs/2511.00179</link>
<guid>https://arxiv.org/abs/2511.00179</guid>
<content:encoded><![CDATA[
arXiv:2511.00179v1 Announce Type: cross 
Abstract: Capturing the structural changes that molecules undergo during chemical reactions in real space and time is a long-standing dream and an essential prerequisite for understanding and ultimately controlling femtochemistry. A key approach to tackle this challenging task is Coulomb explosion imaging, which benefited decisively from recently emerging high-repetition-rate X-ray free-electron laser sources. With this technique, information on the molecular structure is inferred from the momentum distributions of the ions produced by the rapid Coulomb explosion of molecules. Retrieving molecular structures from these distributions poses a highly non-linear inverse problem that remains unsolved for molecules consisting of more than a few atoms. Here, we address this challenge using a diffusion-based Transformer neural network. We show that the network reconstructs unknown molecular geometries from ion-momentum distributions with a mean absolute error below one Bohr radius, which is half the length of a typical chemical bond.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Retrospect to Multi-prompt Learning across Vision and Language</title>
<link>https://arxiv.org/abs/2511.00191</link>
<guid>https://arxiv.org/abs/2511.00191</guid>
<content:encoded><![CDATA[
arXiv:2511.00191v1 Announce Type: cross 
Abstract: The vision community is undergoing the unprecedented progress with the emergence of Vision-Language Pretraining Models (VLMs). Prompt learning plays as the holy grail of accessing VLMs since it enables their fast adaptation to downstream tasks with limited resources. Whereas existing researches milling around single-prompt paradigms, rarely investigate the technical potential behind their multi-prompt learning counterparts. This paper aims to provide a principled retrospect for vision-language multi-prompt learning. We extend the recent constant modality gap phenomenon to learnable prompts and then, justify the superiority of vision-language transfer with multi-prompt augmentation, empirically and theoretically. In terms of this observation, we propose an Energy-based Multi-prompt Learning (EMPL) to generate multiple prompt embeddings by drawing instances from an energy-based distribution, which is implicitly defined by VLMs. So our EMPL is not only parameter-efficient but also rigorously lead to the balance between in-domain and out-of-domain open-vocabulary generalization. Comprehensive experiments have been conducted to justify our claims and the excellence of EMPL.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EL-MIA: Quantifying Membership Inference Risks of Sensitive Entities in LLMs</title>
<link>https://arxiv.org/abs/2511.00192</link>
<guid>https://arxiv.org/abs/2511.00192</guid>
<content:encoded><![CDATA[
arXiv:2511.00192v1 Announce Type: cross 
Abstract: Membership inference attacks (MIA) aim to infer whether a particular data point is part of the training dataset of a model. In this paper, we propose a new task in the context of LLM privacy: entity-level discovery of membership risk focused on sensitive information (PII, credit card numbers, etc). Existing methods for MIA can detect the presence of entire prompts or documents in the LLM training data, but they fail to capture risks at a finer granularity. We propose the ``EL-MIA'' framework for auditing entity-level membership risks in LLMs. We construct a benchmark dataset for the evaluation of MIA methods on this task. Using this benchmark, we conduct a systematic comparison of existing MIA techniques as well as two newly proposed methods. We provide a comprehensive analysis of the results, trying to explain the relation of the entity level MIA susceptability with the model scale, training epochs, and other surface level factors. Our findings reveal that existing MIA methods are limited when it comes to entity-level membership inference of the sensitive attributes, while this susceptibility can be outlined with relatively straightforward methods, highlighting the need for stronger adversaries to stress test the provided threat model.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Code Agent Behaviour: An Empirical Study of Success and Failure Trajectories</title>
<link>https://arxiv.org/abs/2511.00197</link>
<guid>https://arxiv.org/abs/2511.00197</guid>
<content:encoded><![CDATA[
arXiv:2511.00197v1 Announce Type: cross 
Abstract: The increasing deployment of Large Language Model (LLM) agents for complex software engineering tasks has created a need to understand their problem-solving behaviours beyond simple success metrics. While these agents demonstrate impressive capabilities in automated issue resolution, their decision-making processes remain largely opaque. This paper presents an empirical study of agent trajectories, namely the execution traces capturing the steps agents take when attempting to resolve software issues. We analyse trajectories from three state-of-the-art code agents (OpenHands, SWE-agent, and Prometheus) on the SWE-Bench benchmark, examining both successful and failed attempts. Our investigation reveals several key insights into agent behaviour. First, we identify how distinct problem-solving strategies, such as defensive programming and context gathering, enable success in different scenarios. Second, we find that failed trajectories are consistently longer and exhibit higher variance than successful ones, with failure patterns differing significantly between agents. Third, our fault localisation analysis shows that while most trajectories correctly identify problematic files (72-81\% even in failures), success depends more on achieving approximate rather than exact code modifications. These and other findings unveiled by our study, provide a foundation for understanding agent behaviour through trajectory analysis, contributing to the development of more robust and interpretable autonomous software engineering systems.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training LLMs Beyond Next Token Prediction -- Filling the Mutual Information Gap</title>
<link>https://arxiv.org/abs/2511.00198</link>
<guid>https://arxiv.org/abs/2511.00198</guid>
<content:encoded><![CDATA[
arXiv:2511.00198v1 Announce Type: cross 
Abstract: Optimizing training performance in large language models (LLMs) remains an essential challenge, particularly in improving model performance while maintaining computational costs. This work challenges the conventional approach of training LLMs using next-token prediction (NTP), arguing that by predicting information-rich tokens during training, there is a more effective way to train LLMs. We investigate the impact of the proposed solution in three kinds of tasks for LLMs: arithmetic, multi-label classification of text, and natural-language generation. This work offers a principled approach to optimizing LLM training, advancing both model performance and theoretical understanding of the target-token selection strategies.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion Models at the Drug Discovery Frontier: A Review on Generating Small Molecules versus Therapeutic Peptides</title>
<link>https://arxiv.org/abs/2511.00209</link>
<guid>https://arxiv.org/abs/2511.00209</guid>
<content:encoded><![CDATA[
arXiv:2511.00209v1 Announce Type: cross 
Abstract: Diffusion models have emerged as a leading framework in generative modeling, showing significant potential to accelerate and transform the traditionally slow and costly process of drug discovery. This review provides a systematic comparison of their application in designing two principal therapeutic modalities: small molecules and therapeutic peptides. We analyze how a unified framework of iterative denoising is adapted to the distinct molecular representations, chemical spaces, and design objectives of each modality. For small molecules, these models excel at structure-based design, generating novel, pocket-fitting ligands with desired physicochemical properties, yet face the critical hurdle of ensuring chemical synthesizability. Conversely, for therapeutic peptides, the focus shifts to generating functional sequences and designing de novo structures, where the primary challenges are achieving biological stability against proteolysis, ensuring proper folding, and minimizing immunogenicity. Despite these distinct challenges, both domains face shared hurdles: the need for more accurate scoring functions, the scarcity of high-quality experimental data, and the crucial requirement for experimental validation. We conclude that the full potential of diffusion models will be unlocked by bridging these modality-specific gaps and integrating them into automated, closed-loop Design-Build-Test-Learn (DBTL) platforms, thereby shifting the paradigm from chemical exploration to the targeted creation of novel therapeutics.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Efficient and Generalizable Transfer Learning Method for Weather Condition Detection on Ground Terminals</title>
<link>https://arxiv.org/abs/2511.00211</link>
<guid>https://arxiv.org/abs/2511.00211</guid>
<content:encoded><![CDATA[
arXiv:2511.00211v1 Announce Type: cross 
Abstract: The increasing adoption of satellite Internet with low-Earth-orbit (LEO) satellites in mega-constellations allows ubiquitous connectivity to rural and remote areas. However, weather events have a significant impact on the performance and reliability of satellite Internet. Adverse weather events such as snow and rain can disturb the performance and operations of satellite Internet's essential ground terminal components, such as satellite antennas, significantly disrupting the space-ground link conditions between LEO satellites and ground stations. This challenge calls for not only region-based weather forecasts but also fine-grained detection capability on ground terminal components of fine-grained weather conditions. Such a capability can assist in fault diagnostics and mitigation for reliable satellite Internet, but its solutions are lacking, not to mention the effectiveness and generalization that are essential in real-world deployments. This paper discusses an efficient transfer learning (TL) method that can enable a ground component to locally detect representative weather-related conditions. The proposed method can detect snow, wet, and other conditions resulting from adverse and typical weather events and shows superior performance compared to the typical deep learning methods, such as YOLOv7, YOLOv9, Faster R-CNN, and R-YOLO. Our TL method also shows the advantage of being generalizable to various scenarios.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DM-QPMNET: Dual-modality fusion network for cell segmentation in quantitative phase microscopy</title>
<link>https://arxiv.org/abs/2511.00218</link>
<guid>https://arxiv.org/abs/2511.00218</guid>
<content:encoded><![CDATA[
arXiv:2511.00218v1 Announce Type: cross 
Abstract: Cell segmentation in single-shot quantitative phase microscopy (ssQPM) faces challenges from traditional thresholding methods that are sensitive to noise and cell density, while deep learning approaches using simple channel concatenation fail to exploit the complementary nature of polarized intensity images and phase maps. We introduce DM-QPMNet, a dual-encoder network that treats these as distinct modalities with separate encoding streams. Our architecture fuses modality-specific features at intermediate depth via multi-head attention, enabling polarized edge and texture representations to selectively integrate complementary phase information. This content-aware fusion preserves training stability while adding principled multi-modal integration through dual-source skip connections and per-modality normalization at minimal overhead. Our approach demonstrates substantial improvements over monolithic concatenation and single-modality baselines, showing that modality-specific encoding with learnable fusion effectively exploits ssQPM's simultaneous capture of complementary illumination and phase cues for robust cell segmentation.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.00222</link>
<guid>https://arxiv.org/abs/2511.00222</guid>
<content:encoded><![CDATA[
arXiv:2511.00222v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly used to simulate human users in interactive settings such as therapy, education, and social role-play. While these simulations enable scalable training and evaluation of AI agents, off-the-shelf LLMs often drift from their assigned personas, contradict earlier statements, or abandon role-appropriate behavior. We introduce a unified framework for evaluating and improving persona consistency in LLM-generated dialogue. We define three automatic metrics: prompt-to-line consistency, line-to-line consistency, and Q&amp;A consistency, that capture different types of persona drift and validate each against human annotations. Using these metrics as reward signals, we apply multi-turn reinforcement learning to fine-tune LLMs for three user roles: a patient, a student, and a social chat partner. Our method reduces inconsistency by over 55%, resulting in more coherent and faithful simulated users.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Transparency: Mechanistic Interpretability Interfaces for Anticipating Model Behaviors for Personalized AI</title>
<link>https://arxiv.org/abs/2511.00230</link>
<guid>https://arxiv.org/abs/2511.00230</guid>
<content:encoded><![CDATA[
arXiv:2511.00230v1 Announce Type: cross 
Abstract: Millions of users now design personalized LLM-based chatbots that shape their daily interactions, yet they can only loosely anticipate how their design choices will manifest as behaviors in deployment. This opacity is consequential: seemingly innocuous prompts can trigger excessive sycophancy, toxicity, or inconsistency, degrading utility and raising safety concerns. To address this issue, we introduce an interface that enables neural transparency by exposing language model internals during chatbot design. Our approach extracts behavioral trait vectors (empathy, toxicity, sycophancy, etc.) by computing differences in neural activations between contrastive system prompts that elicit opposing behaviors. We predict chatbot behaviors by projecting the system prompt's final token activations onto these trait vectors, normalizing for cross-trait comparability, and visualizing results via an interactive sunburst diagram. To evaluate this approach, we conducted an online user study using Prolific to compare our neural transparency interface against a baseline chatbot interface without any form of transparency. Our analyses suggest that users systematically miscalibrated AI behavior: participants misjudged trait activations for eleven of fifteen analyzable traits, motivating the need for transparency tools in everyday human-AI interaction. While our interface did not change design iteration patterns, it significantly increased user trust and was enthusiastically received. Qualitative analysis indicated that users' had nuanced experiences with the visualization that may enrich future work designing neurally transparent interfaces. This work offers a path for how mechanistic interpretability can be operationalized for non-technical users, establishing a foundation for safer, more aligned human-AI interactions.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IL-PCSR: Legal Corpus for Prior Case and Statute Retrieval</title>
<link>https://arxiv.org/abs/2511.00268</link>
<guid>https://arxiv.org/abs/2511.00268</guid>
<content:encoded><![CDATA[
arXiv:2511.00268v1 Announce Type: cross 
Abstract: Identifying/retrieving relevant statutes and prior cases/precedents for a given legal situation are common tasks exercised by law practitioners. Researchers to date have addressed the two tasks independently, thus developing completely different datasets and models for each task; however, both retrieval tasks are inherently related, e.g., similar cases tend to cite similar statutes (due to similar factual situation). In this paper, we address this gap. We propose IL-PCR (Indian Legal corpus for Prior Case and Statute Retrieval), which is a unique corpus that provides a common testbed for developing models for both the tasks (Statute Retrieval and Precedent Retrieval) that can exploit the dependence between the two. We experiment extensively with several baseline models on the tasks, including lexical models, semantic models and ensemble based on GNNs. Further, to exploit the dependence between the two tasks, we develop an LLM-based re-ranking approach that gives the best performance.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedReplay: A Feature Replay Assisted Federated Transfer Learning Framework for Efficient and Privacy-Preserving Smart Agriculture</title>
<link>https://arxiv.org/abs/2511.00269</link>
<guid>https://arxiv.org/abs/2511.00269</guid>
<content:encoded><![CDATA[
arXiv:2511.00269v1 Announce Type: cross 
Abstract: Accurate classification plays a pivotal role in smart agriculture, enabling applications such as crop monitoring, fruit recognition, and pest detection. However, conventional centralized training often requires large-scale data collection, which raises privacy concerns, while standard federated learning struggles with non-independent and identically distributed (non-IID) data and incurs high communication costs. To address these challenges, we propose a federated learning framework that integrates a frozen Contrastive Language-Image Pre-training (CLIP) vision transformer (ViT) with a lightweight transformer classifier. By leveraging the strong feature extraction capability of the pre-trained CLIP ViT, the framework avoids training large-scale models from scratch and restricts federated updates to a compact classifier, thereby reducing transmission overhead significantly. Furthermore, to mitigate performance degradation caused by non-IID data distribution, a small subset (1%) of CLIP-extracted feature representations from all classes is shared across clients. These shared features are non-reversible to raw images, ensuring privacy preservation while aligning class representation across participants. Experimental results on agricultural classification tasks show that the proposed method achieve 86.6% accuracy, which is more than 4 times higher compared to baseline federated learning approaches. This demonstrates the effectiveness and efficiency of combining vision-language model features with federated learning for privacy-preserving and scalable agricultural intelligence.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation</title>
<link>https://arxiv.org/abs/2511.00270</link>
<guid>https://arxiv.org/abs/2511.00270</guid>
<content:encoded><![CDATA[
arXiv:2511.00270v1 Announce Type: cross 
Abstract: Sign language translation remains a challenging task due to the scarcity of large-scale, sentence-aligned datasets. Prior arts have focused on various feature extraction and architectural changes to support neural machine translation for sign languages. We propose POSESTITCH-SLT, a novel pre-training scheme that is inspired by linguistic-templates-based sentence generation technique. With translation comparison on two sign language datasets, How2Sign and iSign, we show that a simple transformer-based encoder-decoder architecture outperforms the prior art when considering template-generated sentence pairs in training. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign and from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for pose-based gloss-free translation. The results demonstrate the effectiveness of template-driven synthetic supervision in low-resource sign language settings.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LongCat-Flash-Omni Technical Report</title>
<link>https://arxiv.org/abs/2511.00279</link>
<guid>https://arxiv.org/abs/2511.00279</guid>
<content:encoded><![CDATA[
arXiv:2511.00279v1 Announce Type: cross 
Abstract: We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal model with 560 billion parameters, excelling at real-time audio-visual interaction. By adopting a curriculum-inspired progressive training strategy that transitions from simpler to increasingly complex modality sequence modeling tasks, LongCat-Flash-Omni attains comprehensive multimodal capabilities while maintaining strong unimodal capability. Building upon LongCat-Flash, which adopts a high-performance Shortcut-connected Mixture-of-Experts (MoE) architecture with zero-computation experts, LongCat-Flash-Omni integrates efficient multimodal perception and speech reconstruction modules. Despite its immense size of 560B parameters (with 27B activated), LongCat-Flash-Omni achieves low-latency real-time audio-visual interaction. For training infrastructure, we developed a modality-decoupled parallelism scheme specifically designed to manage the data and model heterogeneity inherent in large-scale multimodal training. This innovative approach demonstrates exceptional efficiency by sustaining over 90% of the throughput achieved by text-only training. Extensive evaluations show that LongCat-Flash-Omni achieves state-of-the-art performance on omni-modal benchmarks among open-source models. Furthermore, it delivers highly competitive results across a wide range of modality-specific tasks, including text, image, and video understanding, as well as audio understanding and generation. We provide a comprehensive overview of the model architecture design, training procedures, and data strategies, and open-source the model to foster future research and development in the community.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Calibration Across Layers: Understanding Calibration Evolution in LLMs</title>
<link>https://arxiv.org/abs/2511.00280</link>
<guid>https://arxiv.org/abs/2511.00280</guid>
<content:encoded><![CDATA[
arXiv:2511.00280v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated inherent calibration capabilities, where predicted probabilities align well with correctness, despite prior findings that deep neural networks are often overconfident. Recent studies have linked this behavior to specific components in the final layer, such as entropy neurons and the unembedding matrix null space. In this work, we provide a complementary perspective by investigating how calibration evolves throughout the network depth. Analyzing multiple open-weight models on the MMLU benchmark, we uncover a distinct confidence correction phase in the upper/later layers, where model confidence is actively recalibrated after decision certainty has been reached. Furthermore, we identify a low-dimensional calibration direction in the residual stream whose perturbation significantly improves calibration metrics (ECE and MCE) without harming accuracy. Our findings suggest that calibration is a distributed phenomenon, shaped throughout the network forward pass, not just in its final projection, providing new insights into how confidence-regulating mechanisms operate within LLMs.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language Modeling With Factorization Memory</title>
<link>https://arxiv.org/abs/2511.00315</link>
<guid>https://arxiv.org/abs/2511.00315</guid>
<content:encoded><![CDATA[
arXiv:2511.00315v1 Announce Type: cross 
Abstract: We propose Factorization Memory, an efficient recurrent neural network (RNN) architecture that achieves performance comparable to Transformer models on short-context language modeling tasks while also demonstrating superior generalization in long-context scenarios. Our model builds upon Mamba-2, enabling Factorization Memory to exploit parallel computations during training while preserving constant computational and memory complexity during inference. To further optimize model efficiency and representational capacity, we develop a sparse formulation of Factorization Memory that updates only a subset of recurrent states at each step while preserving the strong performance of its dense counterpart. To our knowledge, this represents the first RNN architecture that successfully combines sparse memory activation with competitive performance across both short and long-context settings. This work provides a systematic empirical analysis of Factorization Memory in comparison to Transformer and Mamba-2 architectures.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Technical Exploration of Causal Inference with Hybrid LLM Synthetic Data</title>
<link>https://arxiv.org/abs/2511.00318</link>
<guid>https://arxiv.org/abs/2511.00318</guid>
<content:encoded><![CDATA[
arXiv:2511.00318v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) offer a flexible means to generate synthetic tabular data, yet existing approaches often fail to preserve key causal parameters such as the average treatment effect (ATE). In this technical exploration, we first demonstrate that state-of-the-art synthetic data generators, both GAN- and LLM-based, can achieve high predictive fidelity while substantially misestimating causal effects. To address this gap, we propose a hybrid generation framework that combines model-based covariate synthesis (monitored via distance-to-closest-record filtering) with separately learned propensity and outcome models, thereby ensuring that (W, A, Y) triplets retain their underlying causal structure. We further introduce a synthetic pairing strategy to mitigate positivity violations and a realistic evaluation protocol that leverages unlimited synthetic samples to benchmark traditional estimators (IPTW, AIPW, substitution) under complex covariate distributions. This work lays the groundwork for LLM-powered data pipelines that support robust causal analysis. Our code is available at https://github.com/Xyc-arch/llm-synthetic-for-causal-inference.git.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Processing-Near-Memory for 1M-Token LLM Inference: CXL-Enabled KV-Cache Management Beyond GPU Limits</title>
<link>https://arxiv.org/abs/2511.00321</link>
<guid>https://arxiv.org/abs/2511.00321</guid>
<content:encoded><![CDATA[
arXiv:2511.00321v1 Announce Type: cross 
Abstract: The expansion of context windows in large language models (LLMs) to multi-million tokens introduces severe memory and compute bottlenecks, particularly in managing the growing Key-Value (KV) cache. While Compute Express Link (CXL) enables non-eviction frameworks that offload the full KV-cache to scalable external memory, these frameworks still suffer from costly data transfers when recalling non-resident KV tokens to limited GPU memory as context lengths increase. This work proposes scalable Processing-Near-Memory (PNM) for 1M-Token LLM Inference, a CXL-enabled KV-cache management system that coordinates memory and computation beyond GPU limits. Our design offloads token page selection to a PNM accelerator within CXL memory, eliminating costly recalls and enabling larger GPU batch sizes. We further introduce a hybrid parallelization strategy and a steady-token selection mechanism to enhance compute efficiency and scalability. Implemented atop a state-of-the-art CXL-PNM system, our solution delivers consistent performance gains for LLMs with up to 405B parameters and 1M-token contexts. Our PNM-only offloading scheme (PNM-KV) and GPU-PNM hybrid with steady-token execution (PnG-KV) achieve up to 21.9x throughput improvement, up to 60x lower energy per token, and up to 7.3x better total cost efficiency than the baseline, demonstrating that CXL-enabled multi-PNM architectures can serve as a scalable backbone for future long-context LLM inference.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Automated Petrography</title>
<link>https://arxiv.org/abs/2511.00328</link>
<guid>https://arxiv.org/abs/2511.00328</guid>
<content:encoded><![CDATA[
arXiv:2511.00328v1 Announce Type: cross 
Abstract: Petrography is a branch of geology that analyzes the mineralogical composition of rocks from microscopical thin section samples. It is essential for understanding rock properties across geology, archaeology, engineering, mineral exploration, and the oil industry. However, petrography is a labor-intensive task requiring experts to conduct detailed visual examinations of thin section samples through optical polarization microscopes, thus hampering scalability and highlighting the need for automated techniques. To address this challenge, we introduce the Large-scale Imaging and Thin section Optical-polarization Set (LITHOS), the largest and most diverse publicly available experimental framework for automated petrography. LITHOS includes 211,604 high-resolution RGB patches of polarized light and 105,802 expert-annotated grains across 25 mineral categories. Each annotation consists of the mineral class, spatial coordinates, and expert-defined major and minor axes represented as intersecting vector paths, capturing grain geometry and orientation. We evaluate multiple deep learning techniques for mineral classification in LITHOS and propose a dual-encoder transformer architecture that integrates both polarization modalities as a strong baseline for future reference. Our method consistently outperforms single-polarization models, demonstrating the value of polarization synergy in mineral classification. We have made the LITHOS Benchmark publicly available, comprising our dataset, code, and pretrained models, to foster reproducibility and further research in automated petrographic analysis.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MH-1M: A 1.34 Million-Sample Comprehensive Multi-Feature Android Malware Dataset for Machine Learning, Deep Learning, Large Language Models, and Threat Intelligence Research</title>
<link>https://arxiv.org/abs/2511.00342</link>
<guid>https://arxiv.org/abs/2511.00342</guid>
<content:encoded><![CDATA[
arXiv:2511.00342v1 Announce Type: cross 
Abstract: We present MH-1M, one of the most comprehensive and up-to-date datasets for advanced Android malware research. The dataset comprises 1,340,515 applications, encompassing a wide range of features and extensive metadata. To ensure accurate malware classification, we employ the VirusTotal API, integrating multiple detection engines for comprehensive and reliable assessment. Our GitHub, Figshare, and Harvard Dataverse repositories provide open access to the processed dataset and its extensive supplementary metadata, totaling more than 400 GB of data and including the outputs of the feature extraction pipeline as well as the corresponding VirusTotal reports. Our findings underscore the MH-1M dataset's invaluable role in understanding the evolving landscape of malware.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploiting Latent Space Discontinuities for Building Universal LLM Jailbreaks and Data Extraction Attacks</title>
<link>https://arxiv.org/abs/2511.00346</link>
<guid>https://arxiv.org/abs/2511.00346</guid>
<content:encoded><![CDATA[
arXiv:2511.00346v1 Announce Type: cross 
Abstract: The rapid proliferation of Large Language Models (LLMs) has raised significant concerns about their security against adversarial attacks. In this work, we propose a novel approach to crafting universal jailbreaks and data extraction attacks by exploiting latent space discontinuities, an architectural vulnerability related to the sparsity of training data. Unlike previous methods, our technique generalizes across various models and interfaces, proving highly effective in seven state-of-the-art LLMs and one image generation model. Initial results indicate that when these discontinuities are exploited, they can consistently and profoundly compromise model behavior, even in the presence of layered defenses. The findings suggest that this strategy has substantial potential as a systemic attack vector.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting AI-Generated Images via Diffusion Snap-Back Reconstruction: A Forensic Approach</title>
<link>https://arxiv.org/abs/2511.00352</link>
<guid>https://arxiv.org/abs/2511.00352</guid>
<content:encoded><![CDATA[
arXiv:2511.00352v1 Announce Type: cross 
Abstract: The rapid rise of generative diffusion models has made distinguishing authentic visual content from synthetic imagery increasingly challenging. Traditional deepfake detection methods, which rely on frequency or pixel-level artifacts, fail against modern text-to-image systems such as Stable Diffusion and DALL-E that produce photorealistic and artifact-free results. This paper introduces a diffusion-based forensic framework that leverages multi-strength image reconstruction dynamics, termed diffusion snap-back, to identify AI-generated images. By analysing how reconstruction metrics (LPIPS, SSIM, and PSNR) evolve across varying noise strengths, we extract interpretable manifold-based features that differentiate real and synthetic images. Evaluated on a balanced dataset of 4,000 images, our approach achieves 0.993 AUROC under cross-validation and remains robust to common distortions such as compression and noise. Despite using limited data and a single diffusion backbone (Stable Diffusion v1.5), the proposed method demonstrates strong generalization and interpretability, offering a foundation for scalable, model-agnostic synthetic media forensics.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Unifying Group Fairness Evaluation from a Sparsity Perspective</title>
<link>https://arxiv.org/abs/2511.00359</link>
<guid>https://arxiv.org/abs/2511.00359</guid>
<content:encoded><![CDATA[
arXiv:2511.00359v1 Announce Type: cross 
Abstract: Ensuring algorithmic fairness remains a significant challenge in machine learning, particularly as models are increasingly applied across diverse domains. While numerous fairness criteria exist, they often lack generalizability across different machine learning problems. This paper examines the connections and differences among various sparsity measures in promoting fairness and proposes a unified sparsity-based framework for evaluating algorithmic fairness. The framework aligns with existing fairness criteria and demonstrates broad applicability to a wide range of machine learning tasks. We demonstrate the effectiveness of the proposed framework as an evaluation metric through extensive experiments on a variety of datasets and bias mitigation methods. This work provides a novel perspective to algorithmic fairness by framing it through the lens of sparsity and social equity, offering potential for broader impact on fairness research and applications.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mind the Gap: Missing Cyber Threat Coverage in NIDS Datasets for the Energy Sector</title>
<link>https://arxiv.org/abs/2511.00360</link>
<guid>https://arxiv.org/abs/2511.00360</guid>
<content:encoded><![CDATA[
arXiv:2511.00360v1 Announce Type: cross 
Abstract: Network Intrusion Detection Systems (NIDS) developed using publicly available datasets predominantly focus on enterprise environments, raising concerns about their effectiveness for converged Information Technology (IT) and Operational Technology (OT) in energy infrastructures. This study evaluates the representativeness of five widely used datasets: CIC-IDS2017, SWaT, WADI, Sherlock, and CIC-Modbus2023 against network-detectable MITRE ATT&amp;CK techniques extracted from documented energy sector incidents. Using a structured five-step analytical approach, this article successfully developed and performed a gap analysis that identified 94 network observable techniques from an initial pool of 274 ATT&amp;CK techniques. Sherlock dataset exhibited the highest mean coverage (0.56), followed closely by CIC-IDS2017 (0.55), while SWaT and WADI recorded the lowest scores (0.38). Combining CIC-IDS2017, Sherlock, and CIC-Modbus2023 achieved an aggregate coverage of 92%, highlighting their complementary strengths. The analysis identifies critical gaps, particularly in lateral movement and industrial protocol manipulation, providing a clear pathway for dataset enhancement and more robust NIDS evaluation in hybrid IT/OT energy environments.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MalDataGen: A Modular Framework for Synthetic Tabular Data Generation in Malware Detection</title>
<link>https://arxiv.org/abs/2511.00361</link>
<guid>https://arxiv.org/abs/2511.00361</guid>
<content:encoded><![CDATA[
arXiv:2511.00361v1 Announce Type: cross 
Abstract: High-quality data scarcity hinders malware detection, limiting ML performance. We introduce MalDataGen, an open-source modular framework for generating high-fidelity synthetic tabular data using modular deep learning models (e.g., WGAN-GP, VQ-VAE). Evaluated via dual validation (TR-TS/TS-TR), seven classifiers, and utility metrics, MalDataGen outperforms benchmarks like SDV while preserving data utility. Its flexible design enables seamless integration into detection pipelines, offering a practical solution for cybersecurity applications.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery</title>
<link>https://arxiv.org/abs/2511.00362</link>
<guid>https://arxiv.org/abs/2511.00362</guid>
<content:encoded><![CDATA[
arXiv:2511.00362v1 Announce Type: cross 
Abstract: Cultural heritage restoration in Bangladesh faces a dual challenge of limited resources and scarce technical expertise. Traditional 3D digitization methods, such as photogrammetry or LiDAR scanning, require expensive hardware, expert operators, and extensive on-site access, which are often infeasible in developing contexts. As a result, many of Bangladesh's architectural treasures, from the Paharpur Buddhist Monastery to Ahsan Manzil, remain vulnerable to decay and inaccessible in digital form. This paper introduces Oitijjo-3D, a cost-free generative AI framework that democratizes 3D cultural preservation. By using publicly available Google Street View imagery, Oitijjo-3D reconstructs faithful 3D models of heritage structures through a two-stage pipeline - multimodal visual reasoning with Gemini 2.5 Flash Image for structure-texture synthesis, and neural image-to-3D generation through Hexagen for geometry recovery. The system produces photorealistic, metrically coherent reconstructions in seconds, achieving significant speedups compared to conventional Structure-from-Motion pipelines, without requiring any specialized hardware or expert supervision. Experiments on landmarks such as Ahsan Manzil, Choto Sona Mosque, and Paharpur demonstrate that Oitijjo-3D preserves both visual and structural fidelity while drastically lowering economic and technical barriers. By turning open imagery into digital heritage, this work reframes preservation as a community-driven, AI-assisted act of cultural continuity for resource-limited nations.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Balancing Interpretability and Performance in Motor Imagery EEG Classification: A Comparative Study of ANFIS-FBCSP-PSO and EEGNet</title>
<link>https://arxiv.org/abs/2511.00369</link>
<guid>https://arxiv.org/abs/2511.00369</guid>
<content:encoded><![CDATA[
arXiv:2511.00369v1 Announce Type: cross 
Abstract: Achieving both accurate and interpretable classification of motor imagery EEG remains a key challenge in brain computer interface (BCI) research. This paper compares a transparent fuzzy reasoning approach (ANFIS-FBCSP-PSO) with a deep learning benchmark (EEGNet) using the BCI Competition IV-2a dataset. The ANFIS pipeline combines filter bank common spatial pattern feature extraction with fuzzy IF-THEN rules optimized via particle swarm optimization, while EEGNet learns hierarchical spatial temporal representations directly from raw EEG data. In within-subject experiments, the fuzzy neural model performed better (68.58 percent +/- 13.76 percent accuracy, kappa = 58.04 percent +/- 18.43), while in cross-subject (LOSO) tests, the deep model exhibited stronger generalization (68.20 percent +/- 12.13 percent accuracy, kappa = 57.33 percent +/- 16.22). The study provides practical guidance for selecting MI-BCI systems according to design goals: interpretability or robustness across users. Future investigations into transformer based and hybrid neuro symbolic frameworks are expected to advance transparent EEG decoding.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Who Can We Trust? Scope-Aware Video Moment Retrieval with Multi-Agent Conflict</title>
<link>https://arxiv.org/abs/2511.00370</link>
<guid>https://arxiv.org/abs/2511.00370</guid>
<content:encoded><![CDATA[
arXiv:2511.00370v1 Announce Type: cross 
Abstract: Video moment retrieval uses a text query to locate a moment from a given untrimmed video reference. Locating corresponding video moments with text queries helps people interact with videos efficiently. Current solutions for this task have not considered conflict within location results from different models, so various models cannot integrate correctly to produce better results. This study introduces a reinforcement learning-based video moment retrieval model that can scan the whole video once to find the moment's boundary while producing its locational evidence. Moreover, we proposed a multi-agent system framework that can use evidential learning to resolve conflicts between agents' localization output. As a side product of observing and dealing with conflicts between agents, we can decide whether a query has no corresponding moment in a video (out-of-scope) without additional training, which is suitable for real-world applications. Extensive experiments on benchmark datasets show the effectiveness of our proposed methods compared with state-of-the-art approaches. Furthermore, the results of our study reveal that modeling competition and conflict of the multi-agent system is an effective way to improve RL performance in moment retrieval and show the new role of evidential learning in the multi-agent framework.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SonarSweep: Fusing Sonar and Vision for Robust 3D Reconstruction via Plane Sweeping</title>
<link>https://arxiv.org/abs/2511.00392</link>
<guid>https://arxiv.org/abs/2511.00392</guid>
<content:encoded><![CDATA[
arXiv:2511.00392v1 Announce Type: cross 
Abstract: Accurate 3D reconstruction in visually-degraded underwater environments remains a formidable challenge. Single-modality approaches are insufficient: vision-based methods fail due to poor visibility and geometric constraints, while sonar is crippled by inherent elevation ambiguity and low resolution. Consequently, prior fusion technique relies on heuristics and flawed geometric assumptions, leading to significant artifacts and an inability to model complex scenes. In this paper, we introduce SonarSweep, a novel, end-to-end deep learning framework that overcomes these limitations by adapting the principled plane sweep algorithm for cross-modal fusion between sonar and visual data. Extensive experiments in both high-fidelity simulation and real-world environments demonstrate that SonarSweep consistently generates dense and accurate depth maps, significantly outperforming state-of-the-art methods across challenging conditions, particularly in high turbidity. To foster further research, we will publicly release our code and a novel dataset featuring synchronized stereo-camera and sonar data, the first of its kind.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emotion Detection in Speech Using Lightweight and Transformer-Based Models: A Comparative and Ablation Study</title>
<link>https://arxiv.org/abs/2511.00402</link>
<guid>https://arxiv.org/abs/2511.00402</guid>
<content:encoded><![CDATA[
arXiv:2511.00402v1 Announce Type: cross 
Abstract: Emotion recognition from speech plays a vital role in the development of empathetic human-computer interaction systems. This paper presents a comparative analysis of lightweight transformer-based models, DistilHuBERT and PaSST, by classifying six core emotions from the CREMA-D dataset. We benchmark their performance against a traditional CNN-LSTM baseline model using MFCC features. DistilHuBERT demonstrates superior accuracy (70.64%) and F1 score (70.36%) while maintaining an exceptionally small model size (0.02 MB), outperforming both PaSST and the baseline. Furthermore, we conducted an ablation study on three variants of the PaSST, Linear, MLP, and Attentive Pooling heads, to understand the effect of classification head architecture on model performance. Our results indicate that PaSST with an MLP head yields the best performance among its variants but still falls short of DistilHuBERT. Among the emotion classes, angry is consistently the most accurately detected, while disgust remains the most challenging. These findings suggest that lightweight transformers like DistilHuBERT offer a compelling solution for real-time speech emotion recognition on edge devices. The code is available at: https://github.com/luckymaduabuchi/Emotion-detection-.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings</title>
<link>https://arxiv.org/abs/2511.00405</link>
<guid>https://arxiv.org/abs/2511.00405</guid>
<content:encoded><![CDATA[
arXiv:2511.00405v1 Announce Type: cross 
Abstract: The remarkable success of multimodal large language models (MLLMs) has driven advances in multimodal embeddings, yet existing models remain inherently discriminative, limiting their ability to benefit from reasoning-driven generation paradigm. In this work, we pioneer the exploration of generative embeddings, unifying embedding tasks within a generative paradigm. We propose UME-R1, a universal multimodal embedding framework consisting of a two-stage training strategy: a cold-start supervised fine-tuning equips the model with reasoning capabilities and enables it to generate both discriminative and generative embeddings; a subsequent reinforcement learning enhances reasoning and further optimizes generative embedding quality. This pioneering work reveals four key insights: 1) generative embeddings unlock substantial performance gains over conventional discriminative embeddings by leveraging the powerful generative reasoning capabilities of MLLMs; 2) discriminative and generative embeddings are complementary, whose combined oracle performance far exceeding that of either alone; 3) RL can effectively enhance generative embeddings, establishing a scalable optimization paradigm.; 4) repeated sampling at inference boosts downstream task coverage (pass@k), highlighting the inference-time scalability potential of generative embeddings. Evaluated on the MMEB-V2 benchmark across 78 tasks spanning video, image, and visual documents, UME-R1 significantly outperforms conventional discriminative embedding models and offers a foundation for more interpretable, reasoning-driven generative multimodal embeddings. Our code, models, and datasets will be publicly available at https://github.com/XMUDeepLIT/UME-R1.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Machine Unlearning: Foundations, Mechanisms, and Taxonomy</title>
<link>https://arxiv.org/abs/2511.00406</link>
<guid>https://arxiv.org/abs/2511.00406</guid>
<content:encoded><![CDATA[
arXiv:2511.00406v1 Announce Type: cross 
Abstract: Quantum Machine Unlearning has emerged as a foundational challenge at the intersection of quantum information theory privacypreserving computation and trustworthy artificial intelligence This paper advances QMU by establishing a formal framework that unifies physical constraints algorithmic mechanisms and ethical governance within a verifiable paradigm We define forgetting as a contraction of distinguishability between pre and postunlearning models under completely positive trace-preserving dynamics grounding data removal in the physics of quantum irreversibility Building on this foundation we present a fiveaxis taxonomy spanning scope guarantees mechanisms system context and hardware realization linking theoretical constructs to implementable strategies Within this structure we incorporate influence and quantum Fisher information weighted updates parameter reinitialization and kernel alignment as practical mechanisms compatible with noisy intermediatescale quantum NISQ devices The framework extends naturally to federated and privacyaware settings via quantum differential privacy homomorphic encryption and verifiable delegation enabling scalable auditable deletion across distributed quantum systems Beyond technical design we outline a forwardlooking research roadmap emphasizing formal proofs of forgetting scalable and secure architectures postunlearning interpretability and ethically auditable governance Together these contributions elevate QMU from a conceptual notion to a rigorously defined and ethically aligned discipline bridging physical feasibility algorithmic verifiability and societal accountability in the emerging era of quantum intelligence.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Adversarial Transferability by Balancing Exploration and Exploitation with Gradient-Guided Sampling</title>
<link>https://arxiv.org/abs/2511.00411</link>
<guid>https://arxiv.org/abs/2511.00411</guid>
<content:encoded><![CDATA[
arXiv:2511.00411v1 Announce Type: cross 
Abstract: Adversarial attacks present a critical challenge to deep neural networks' robustness, particularly in transfer scenarios across different model architectures. However, the transferability of adversarial attacks faces a fundamental dilemma between Exploitation (maximizing attack potency) and Exploration (enhancing cross-model generalization). Traditional momentum-based methods over-prioritize Exploitation, i.e., higher loss maxima for attack potency but weakened generalization (narrow loss surface). Conversely, recent methods with inner-iteration sampling over-prioritize Exploration, i.e., flatter loss surfaces for cross-model generalization but weakened attack potency (suboptimal local maxima). To resolve this dilemma, we propose a simple yet effective Gradient-Guided Sampling (GGS), which harmonizes both objectives through guiding sampling along the gradient ascent direction to improve both sampling efficiency and stability. Specifically, based on MI-FGSM, GGS introduces inner-iteration random sampling and guides the sampling direction using the gradient from the previous inner-iteration (the sampling's magnitude is determined by a random distribution). This mechanism encourages adversarial examples to reside in balanced regions with both flatness for cross-model generalization and higher local maxima for strong attack potency. Comprehensive experiments across multiple DNN architectures and multimodal large language models (MLLMs) demonstrate the superiority of our method over state-of-the-art transfer attacks. Code is made available at https://github.com/anuin-cat/GGS.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PADBen: A Comprehensive Benchmark for Evaluating AI Text Detectors Against Paraphrase Attacks</title>
<link>https://arxiv.org/abs/2511.00416</link>
<guid>https://arxiv.org/abs/2511.00416</guid>
<content:encoded><![CDATA[
arXiv:2511.00416v1 Announce Type: cross 
Abstract: While AI-generated text (AIGT) detectors achieve over 90\% accuracy on direct LLM outputs, they fail catastrophically against iteratively-paraphrased content. We investigate why iteratively-paraphrased text -- itself AI-generated -- evades detection systems designed for AIGT identification. Through intrinsic mechanism analysis, we reveal that iterative paraphrasing creates an intermediate laundering region characterized by semantic displacement with preserved generation patterns, which brings up two attack categories: paraphrasing human-authored text (authorship obfuscation) and paraphrasing LLM-generated text (plagiarism evasion). To address these vulnerabilities, we introduce PADBen, the first benchmark systematically evaluating detector robustness against both paraphrase attack scenarios. PADBen comprises a five-type text taxonomy capturing the full trajectory from original content to deeply laundered text, and five progressive detection tasks across sentence-pair and single-sentence challenges. We evaluate 11 state-of-the-art detectors, revealing critical asymmetry: detectors successfully identify the plagiarism evasion problem but fail for the case of authorship obfuscation. Our findings demonstrate that current detection approaches cannot effectively handle the intermediate laundering region, necessitating fundamental advances in detection architectures beyond existing semantic and stylistic discrimination methods. For detailed code implementation, please see https://github.com/JonathanZha47/PadBen-Paraphrase-Attack-Benchmark.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-AI Programming Role Optimization: Developing a Personality-Driven Self-Determination Framework</title>
<link>https://arxiv.org/abs/2511.00417</link>
<guid>https://arxiv.org/abs/2511.00417</guid>
<content:encoded><![CDATA[
arXiv:2511.00417v1 Announce Type: cross 
Abstract: As artificial intelligence transforms software development, a critical question emerges: how can developers and AI systems collaborate most effectively? This dissertation optimizes human-AI programming roles through self-determination theory and personality psychology, introducing the Role Optimization Motivation Alignment (ROMA) framework.
  Through Design Science Research spanning five cycles, this work establishes empirically-validated connections between personality traits, programming role preferences, and collaborative outcomes, engaging 200 experimental participants and 46 interview respondents.
  Key findings demonstrate that personality-driven role optimization significantly enhances self-determination and team dynamics, yielding 23% average motivation increases among professionals and up to 65% among undergraduates. Five distinct personality archetypes emerge: The Explorer (high Openness/low Agreeableness), The Orchestrator (high Extraversion/Agreeableness), The Craftsperson (high Neuroticism/low Extraversion), The Architect (high Conscientiousness), and The Adapter (balanced profile). Each exhibits distinct preferences for programming roles (Co-Pilot, Co-Navigator, Agent), with assignment modes proving crucial for satisfaction.
  The dissertation contributes: (1) an empirically-validated framework linking personality traits to role preferences and self-determination outcomes; (2) a taxonomy of AI collaboration modalities mapped to personality profiles while preserving human agency; and (3) an ISO/IEC 29110 extension enabling Very Small Entities to implement personality-driven role optimization within established standards.
  Keywords: artificial intelligence, human-computer interaction, behavioral software engineering, self-determination theory, personality psychology, phenomenology, intrinsic motivation, pair programming, design science research, ISO/IEC 29110
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LGCA: Enhancing Semantic Representation via Progressive Expansion</title>
<link>https://arxiv.org/abs/2511.00419</link>
<guid>https://arxiv.org/abs/2511.00419</guid>
<content:encoded><![CDATA[
arXiv:2511.00419v1 Announce Type: cross 
Abstract: Recent advancements in large-scale pretraining in natural language processing have enabled pretrained vision-language models such as CLIP to effectively align images and text, significantly improving performance in zero-shot image classification tasks. Subsequent studies have further demonstrated that cropping images into smaller regions and using large language models to generate multiple descriptions for each caption can further enhance model performance. However, due to the inherent sensitivity of CLIP, random image crops can introduce misinformation and bias, as many images share similar features at small scales. To address this issue, we propose Localized-Globalized Cross-Alignment (LGCA), a framework that first captures the local features of an image and then repeatedly selects the most salient regions and expands them. The similarity score is designed to incorporate both the original and expanded images, enabling the model to capture both local and global features while minimizing misinformation. Additionally, we provide a theoretical analysis demonstrating that the time complexity of LGCA remains the same as that of the original model prior to the repeated expansion process, highlighting its efficiency and scalability. Extensive experiments demonstrate that our method substantially improves zero-shot performance across diverse datasets, outperforming state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts</title>
<link>https://arxiv.org/abs/2511.00421</link>
<guid>https://arxiv.org/abs/2511.00421</guid>
<content:encoded><![CDATA[
arXiv:2511.00421v1 Announce Type: cross 
Abstract: Large language models (LLMs) show increasing promise in medical applications, but their ability to detect and correct errors in clinical texts -- a prerequisite for safe deployment -- remains under-evaluated, particularly beyond English. We introduce MedRECT, a cross-lingual benchmark (Japanese/English) that formulates medical error handling as three subtasks: error detection, error localization (sentence extraction), and error correction. MedRECT is built with a scalable, automated pipeline from the Japanese Medical Licensing Examinations (JMLE) and a curated English counterpart, yielding MedRECT-ja (663 texts) and MedRECT-en (458 texts) with comparable error/no-error balance. We evaluate 9 contemporary LLMs spanning proprietary, open-weight, and reasoning families. Key findings: (i) reasoning models substantially outperform standard architectures, with up to 13.5% relative improvement in error detection and 51.0% in sentence extraction; (ii) cross-lingual evaluation reveals 5-10% performance gaps from English to Japanese, with smaller disparities for reasoning models; (iii) targeted LoRA fine-tuning yields asymmetric improvements in error correction performance (Japanese: +0.078, English: +0.168) while preserving reasoning capabilities; and (iv) our fine-tuned model exceeds human expert performance on structured medical error correction tasks. To our knowledge, MedRECT is the first comprehensive cross-lingual benchmark for medical error correction, providing a reproducible framework and resources for developing safer medical LLMs across languages.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bootstrap Off-policy with World Model</title>
<link>https://arxiv.org/abs/2511.00423</link>
<guid>https://arxiv.org/abs/2511.00423</guid>
<content:encoded><![CDATA[
arXiv:2511.00423v1 Announce Type: cross 
Abstract: Online planning has proven effective in reinforcement learning (RL) for improving sample efficiency and final performance. However, using planning for environment interaction inevitably introduces a divergence between the collected data and the policy's actual behaviors, degrading both model learning and policy improvement. To address this, we propose BOOM (Bootstrap Off-policy with WOrld Model), a framework that tightly integrates planning and off-policy learning through a bootstrap loop: the policy initializes the planner, and the planner refines actions to bootstrap the policy through behavior alignment. This loop is supported by a jointly learned world model, which enables the planner to simulate future trajectories and provides value targets to facilitate policy improvement. The core of BOOM is a likelihood-free alignment loss that bootstraps the policy using the planner's non-parametric action distribution, combined with a soft value-weighted mechanism that prioritizes high-return behaviors and mitigates variability in the planner's action quality within the replay buffer. Experiments on the high-dimensional DeepMind Control Suite and Humanoid-Bench show that BOOM achieves state-of-the-art results in both training stability and final performance. The code is accessible at https://github.com/molumitu/BOOM_MBRL.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Hierarchical Image-Text Misalignment for Universal Fake Image Detection</title>
<link>https://arxiv.org/abs/2511.00427</link>
<guid>https://arxiv.org/abs/2511.00427</guid>
<content:encoded><![CDATA[
arXiv:2511.00427v1 Announce Type: cross 
Abstract: With the rapid development of generative models, detecting generated fake images to prevent their malicious use has become a critical issue recently. Existing methods frame this challenge as a naive binary image classification task. However, such methods focus only on visual clues, yielding trained detectors susceptible to overfitting specific image patterns and incapable of generalizing to unseen models. In this paper, we address this issue from a multi-modal perspective and find that fake images cannot be properly aligned with corresponding captions compared to real images. Upon this observation, we propose a simple yet effective detector termed ITEM by leveraging the image-text misalignment in a joint visual-language space as discriminative clues. Specifically, we first measure the misalignment of the images and captions in pre-trained CLIP's space, and then tune a MLP head to perform the usual detection task. Furthermore, we propose a hierarchical misalignment scheme that first focuses on the whole image and then each semantic object described in the caption, which can explore both global and fine-grained local semantic misalignment as clues. Extensive experiments demonstrate the superiority of our method against other state-of-the-art competitors with impressive generalization and robustness on various recent generative models.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Frequency Forgery Clues for Diffusion-Generated Image Detection</title>
<link>https://arxiv.org/abs/2511.00429</link>
<guid>https://arxiv.org/abs/2511.00429</guid>
<content:encoded><![CDATA[
arXiv:2511.00429v1 Announce Type: cross 
Abstract: Diffusion models have achieved remarkable success in image synthesis, but the generated high-quality images raise concerns about potential malicious use. Existing detectors often struggle to capture discriminative clues across different models and settings, limiting their generalization to unseen diffusion models and robustness to various perturbations. To address this issue, we observe that diffusion-generated images exhibit progressively larger differences from natural real images across low- to high-frequency bands. Based on this insight, we propose a simple yet effective representation by enhancing the Frequency Forgery Clue (F^2C) across all frequency bands. Specifically, we introduce a frequency-selective function which serves as a weighted filter to the Fourier spectrum, suppressing less discriminative bands while enhancing more informative ones. This approach, grounded in a comprehensive analysis of frequency-based differences between natural real and diffusion-generated images, enables general detection of images from unseen diffusion models and provides robust resilience to various perturbations. Extensive experiments on various diffusion-generated image datasets demonstrate that our method outperforms state-of-the-art detectors with superior generalization and robustness.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Region-Aware Reconstruction Strategy for Pre-training fMRI Foundation Model</title>
<link>https://arxiv.org/abs/2511.00443</link>
<guid>https://arxiv.org/abs/2511.00443</guid>
<content:encoded><![CDATA[
arXiv:2511.00443v1 Announce Type: cross 
Abstract: The emergence of foundation models in neuroimaging is driven by the increasing availability of large-scale and heterogeneous brain imaging datasets. Recent advances in self-supervised learning, particularly reconstruction-based objectives, have demonstrated strong potential for pretraining models that generalize effectively across diverse downstream functional MRI (fMRI) tasks. In this study, we explore region-aware reconstruction strategies for a foundation model in resting-state fMRI, moving beyond approaches that rely on random region masking. Specifically, we introduce an ROI-guided masking strategy using the Automated Anatomical Labelling Atlas (AAL3), applied directly to full 4D fMRI volumes to selectively mask semantically coherent brain regions during self-supervised pretraining. Using the ADHD-200 dataset comprising 973 subjects with resting-state fMRI scans, we show that our method achieves a 4.23% improvement in classification accuracy for distinguishing healthy controls from individuals diagnosed with ADHD, compared to conventional random masking. Region-level attribution analysis reveals that brain volumes within the limbic region and cerebellum contribute most significantly to reconstruction fidelity and model representation. Our results demonstrate that masking anatomical regions during model pretraining not only enhances interpretability but also yields more robust and discriminative representations. In future work, we plan to extend this approach by evaluating it on additional neuroimaging datasets, and developing new loss functions explicitly derived from region-aware reconstruction objectives. These directions aim to further improve the robustness and interpretability of foundation models for functional neuroimaging.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LIR: The First Workshop on Late Interaction and Multi Vector Retrieval @ ECIR 2026</title>
<link>https://arxiv.org/abs/2511.00444</link>
<guid>https://arxiv.org/abs/2511.00444</guid>
<content:encoded><![CDATA[
arXiv:2511.00444v1 Announce Type: cross 
Abstract: Late interaction retrieval methods, pioneered by ColBERT, have emerged as a powerful alternative to single-vector neural IR. By leveraging fine-grained, token-level representations, they have been demonstrated to deliver strong generalisation and robustness, particularly in out-of-domain settings. They have recently been shown to be particularly well-suited for novel use cases, such as reasoning-based or cross-modality retrieval. At the same time, these models pose significant challenges of efficiency, usability, and integrations into fully fledged systems; as well as the natural difficulties encountered while researching novel application domains. Recent years have seen rapid advances across many of these areas, but research efforts remain fragmented across communities and frequently exclude practitioners. The purpose of this workshop is to create an environment where all aspects of late interaction can be discussed, with a focus on early research explorations, real-world outcomes, and negative or puzzling results to be freely shared and discussed. The aim of LIR is to provide a highly-interactive environment for researchers from various backgrounds and practitioners to freely discuss their experience, fostering further collaboration.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DRIP: Defending Prompt Injection via De-instruction Training and Residual Fusion Model Architecture</title>
<link>https://arxiv.org/abs/2511.00447</link>
<guid>https://arxiv.org/abs/2511.00447</guid>
<content:encoded><![CDATA[
arXiv:2511.00447v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated impressive instruction-following capabilities. However, these capabilities also expose models to prompt injection attacks, where maliciously crafted inputs overwrite or distract from the intended instructions. A core vulnerability lies in the model's lack of semantic role understanding: it cannot distinguish directive intent from descriptive content, leading it to execute instruction-like phrases embedded in data.
  We propose DRIP, a training-time defense grounded in a semantic modeling perspective, which enforces robust separation between instruction and data semantics without sacrificing utility. DRIP introduces two lightweight yet complementary mechanisms: (1) a token-wise de-instruction shift that performs semantic disentanglement, weakening directive semantics in data tokens while preserving content meaning; and (2) a residual fusion pathway that provides a persistent semantic anchor, reinforcing the influence of the true top-level instruction during generation. Experimental results on LLaMA-8B and Mistral-7B across three prompt injection benchmarks (SEP, AlpacaFarm, and InjecAgent) demonstrate that DRIP outperforms state-of-the-art defenses, including StruQ, SecAlign, ISE, and PFT, improving role separation by 49%, and reducing attack success rate by 66% for adaptive attacks. Meanwhile, DRIP's utility is on par with the undefended model across AlpacaEval, IFEval, and MT-Bench. Our findings underscore the power of lightweight representation edits and role-aware supervision in securing LLMs against adaptive prompt injection.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Proactive DDoS Detection and Mitigation in Decentralized Software-Defined Networking via Port-Level Monitoring and Zero-Training Large Language Models</title>
<link>https://arxiv.org/abs/2511.00460</link>
<guid>https://arxiv.org/abs/2511.00460</guid>
<content:encoded><![CDATA[
arXiv:2511.00460v1 Announce Type: cross 
Abstract: Centralized Software-Defined Networking (cSDN) offers flexible and programmable control of networks but suffers from scalability and reliability issues due to its reliance on centralized controllers. Decentralized SDN (dSDN) alleviates these concerns by distributing control across multiple local controllers, yet this architecture remains highly vulnerable to Distributed Denial-of-Service (DDoS) attacks. In this paper, we propose a novel detection and mitigation framework tailored for dSDN environments. The framework leverages lightweight port-level statistics combined with prompt engineering and in-context learning, enabling the DeepSeek-v3 Large Language Model (LLM) to classify traffic as benign or malicious without requiring fine-tuning or retraining. Once an anomaly is detected, mitigation is enforced directly at the attacker's port, ensuring that malicious traffic is blocked at their origin while normal traffic remains unaffected. An automatic recovery mechanism restores normal operation after the attack inactivity, ensuring both security and availability. Experimental evaluation under diverse DDoS attack scenarios demonstrates that the proposed approach achieves near-perfect detection, with 99.99% accuracy, 99.97% precision, 100% recall, 99.98% F1-score, and an AUC of 1.0. These results highlight the effectiveness of combining distributed monitoring with zero-training LLM inference, providing a proactive and scalable defense mechanism for securing dSDN infrastructures against DDoS threats.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why Federated Optimization Fails to Achieve Perfect Fitting? A Theoretical Perspective on Client-Side Optima</title>
<link>https://arxiv.org/abs/2511.00469</link>
<guid>https://arxiv.org/abs/2511.00469</guid>
<content:encoded><![CDATA[
arXiv:2511.00469v1 Announce Type: cross 
Abstract: Federated optimization is a constrained form of distributed optimization that enables training a global model without directly sharing client data. Although existing algorithms can guarantee convergence in theory and often achieve stable training in practice, the reasons behind performance degradation under data heterogeneity remain unclear. To address this gap, the main contribution of this paper is to provide a theoretical perspective that explains why such degradation occurs. We introduce the assumption that heterogeneous client data lead to distinct local optima, and show that this assumption implies two key consequences: 1) the distance among clients' local optima raises the lower bound of the global objective, making perfect fitting of all client data impossible; and 2) in the final training stage, the global model oscillates within a region instead of converging to a single optimum, limiting its ability to fully fit the data. These results provide a principled explanation for performance degradation in non-iid settings, which we further validate through experiments across multiple tasks and neural network architectures. The framework used in this paper is open-sourced at: https://github.com/NPCLEI/fedtorch.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Longitudinal Vestibular Schwannoma Dataset with Consensus-based Human-in-the-loop Annotations</title>
<link>https://arxiv.org/abs/2511.00472</link>
<guid>https://arxiv.org/abs/2511.00472</guid>
<content:encoded><![CDATA[
arXiv:2511.00472v1 Announce Type: cross 
Abstract: Accurate segmentation of vestibular schwannoma (VS) on Magnetic Resonance Imaging (MRI) is essential for patient management but often requires time-intensive manual annotations by experts. While recent advances in deep learning (DL) have facilitated automated segmentation, challenges remain in achieving robust performance across diverse datasets and complex clinical cases. We present an annotated dataset stemming from a bootstrapped DL-based framework for iterative segmentation and quality refinement of VS in MRI. We combine data from multiple centres and rely on expert consensus for trustworthiness of the annotations. We show that our approach enables effective and resource-efficient generalisation of automated segmentation models to a target data distribution. The framework achieved a significant improvement in segmentation accuracy with a Dice Similarity Coefficient (DSC) increase from 0.9125 to 0.9670 on our target internal validation dataset, while maintaining stable performance on representative external datasets. Expert evaluation on 143 scans further highlighted areas for model refinement, revealing nuanced cases where segmentation required expert intervention. The proposed approach is estimated to enhance efficiency by approximately 37.4% compared to the conventional manual annotation process. Overall, our human-in-the-loop model training approach achieved high segmentation accuracy, highlighting its potential as a clinically adaptable and generalisable strategy for automated VS segmentation in diverse clinical settings. The dataset includes 190 patients, with tumour annotations available for 534 longitudinal contrast-enhanced T1-weighted (T1CE) scans from 184 patients, and non-annotated T2-weighted scans from 6 patients. This dataset is publicly accessible on The Cancer Imaging Archive (TCIA) (https://doi.org/10.7937/bq0z-xa62).
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Investigating Label Bias and Representational Sources of Age-Related Disparities in Medical Segmentation</title>
<link>https://arxiv.org/abs/2511.00477</link>
<guid>https://arxiv.org/abs/2511.00477</guid>
<content:encoded><![CDATA[
arXiv:2511.00477v1 Announce Type: cross 
Abstract: Algorithmic bias in medical imaging can perpetuate health disparities, yet its causes remain poorly understood in segmentation tasks. While fairness has been extensively studied in classification, segmentation remains underexplored despite its clinical importance. In breast cancer segmentation, models exhibit significant performance disparities against younger patients, commonly attributed to physiological differences in breast density. We audit the MAMA-MIA dataset, establishing a quantitative baseline of age-related bias in its automated labels, and reveal a critical Biased Ruler effect where systematically flawed labels for validation misrepresent a model's actual bias. However, whether this bias originates from lower-quality annotations (label bias) or from fundamentally more challenging image characteristics remains unclear. Through controlled experiments, we systematically refute hypotheses that the bias stems from label quality sensitivity or quantitative case difficulty imbalance. Balancing training data by difficulty fails to mitigate the disparity, revealing that younger patient cases are intrinsically harder to learn. We provide direct evidence that systemic bias is learned and amplified when training on biased, machine-generated labels, a critical finding for automated annotation pipelines. This work introduces a systematic framework for diagnosing algorithmic bias in medical segmentation and demonstrates that achieving fairness requires addressing qualitative distributional differences rather than merely balancing case counts.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multimodal Dataset for Indoor Radio Mapping with 3D Point Clouds and RSSI</title>
<link>https://arxiv.org/abs/2511.00494</link>
<guid>https://arxiv.org/abs/2511.00494</guid>
<content:encoded><![CDATA[
arXiv:2511.00494v1 Announce Type: cross 
Abstract: The growing number of smart devices supporting bandwidth-intensive and latency-sensitive applications, such as real-time video analytics, smart sensing, and Extended Reality (XR), necessitates reliable wireless connectivity in indoor environments. Therein, accurate estimation of Radio Environment Maps (REMs) enables adaptive wireless network planning and optimization of Access Point (AP) placement. However, generating realistic REMs remains challenging due to the complexity of indoor spaces. To overcome this challenge, this paper introduces a multimodal dataset that integrates high-resolution 3D LiDAR scans with Wi-Fi Received Signal Strength Indicator (RSSI) measurements collected under 20 distinct AP configurations in a multi-room indoor environment. The dataset captures two measurement scenarios: the first without human presence in the environment, and the second with human presence. Thus, the presented dataset supports the study of dynamic environmental effects on wireless signal propagation. This resource is designed to facilitate research in data-driven wireless modeling, particularly in the context of emerging high-frequency standards such as IEEE 802.11be (Wi-Fi 7), and aims to advance the development of robust, high-capacity indoor communication systems.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning Planning for Language Models</title>
<link>https://arxiv.org/abs/2511.00521</link>
<guid>https://arxiv.org/abs/2511.00521</guid>
<content:encoded><![CDATA[
arXiv:2511.00521v1 Announce Type: cross 
Abstract: Selecting an appropriate reasoning method for a given query remains a key challenge in language model generation. Existing approaches typically generate multiple candidate responses and use an aggregation strategy to select the output answer, often assuming that more candidate answers yield higher accuracy. We revisit this assumption through a rigorous theoretical analysis, deriving accuracy bounds for standard aggregation methods under fixed generation distributions and candidate sizes. Building on these insights, we introduce EPIC, an Ensemble Planning with Contrastive learning framework to learn a shared representation space that captures both model reasoning abilities and query-method compatibility. EPIC incorporates our probability bounds as a regularizer in a utility-driven optimization that balances accuracy and computational cost. Experiments on diverse mathematical reasoning tasks show that EPIC consistently selects optimal reasoning methods, improving accuracy while reducing computational overhead. Our code can be found at https://github.com/nguyenngocbaocmt02/EPIC.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HIP-LLM: A Hierarchical Imprecise Probability Approach to Reliability Assessment of Large Language Models</title>
<link>https://arxiv.org/abs/2511.00527</link>
<guid>https://arxiv.org/abs/2511.00527</guid>
<content:encoded><![CDATA[
arXiv:2511.00527v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly deployed across diverse domains, raising the need for rigorous reliability assessment methods. Existing benchmark-based evaluations primarily offer descriptive statistics of model accuracy over datasets, providing limited insight into the probabilistic behavior of LLMs under real operational conditions. This paper introduces HIP-LLM, a Hierarchical Imprecise Probability framework for modeling and inferring LLM reliability. Building upon the foundations of software reliability engineering, HIP-LLM defines LLM reliability as the probability of failure-free operation over a specified number of future tasks under a given Operational Profile (OP). HIP-LLM represents dependencies across (sub-)domains hierarchically, enabling multi-level inference from subdomain to system-level reliability. HIP-LLM embeds imprecise priors to capture epistemic uncertainty and incorporates OPs to reflect usage contexts. It derives posterior reliability envelopes that quantify uncertainty across priors and data. Experiments on multiple benchmark datasets demonstrate that HIP-LLM offers a more accurate and standardized reliability characterization than existing benchmark and state-of-the-art approaches. A publicly accessible repository of HIP-LLM is provided.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Improvisation and Open-Endedness: Insights for Experiential AI</title>
<link>https://arxiv.org/abs/2511.00529</link>
<guid>https://arxiv.org/abs/2511.00529</guid>
<content:encoded><![CDATA[
arXiv:2511.00529v1 Announce Type: cross 
Abstract: Improvisation-the art of spontaneous creation that unfolds moment-to-moment without a scripted outcome-requires practitioners to continuously sense, adapt, and create anew. It is a fundamental mode of human creativity spanning music, dance, and everyday life. The open-ended nature of improvisation produces a stream of novel, unrepeatable moments-an aspect highly valued in artistic creativity. In parallel, open-endedness (OE)-a system's capacity for unbounded novelty and endless "interestingness"-is exemplified in natural or cultural evolution and has been considered "the last grand challenge" in artificial life (ALife). The rise of generative AI now raises the question in computational creativity (CC) research: What makes a "good" improvisation for AI? Can AI learn to improvise in a genuinely open-ended way? In this work-in-progress paper, we report insights from in-depth interviews with 6 experts in improvisation across dance, music, and contact improvisation. We draw systemic connections between human improvisational arts and the design of future experiential AI agents that could improvise alone or alongside humans-or even with other AI agents-embodying qualities of improvisation drawn from practice: active listening (umwelt and awareness), being in the time (mindfulness and ephemerality), embracing the unknown (source of randomness and serendipity), non-judgmental flow (acceptance and dynamical stability, balancing structure and surprise (unpredictable criticality at edge of chaos), imaginative metaphor (synaesthesia and planning), empathy, trust, boundary, and care (mutual theory of mind), and playfulness and intrinsic motivation (maintaining interestingness).
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Air Pollution Forecasting in Bucharest</title>
<link>https://arxiv.org/abs/2511.00532</link>
<guid>https://arxiv.org/abs/2511.00532</guid>
<content:encoded><![CDATA[
arXiv:2511.00532v1 Announce Type: cross 
Abstract: Air pollution, especially the particulate matter 2.5 (PM2.5), has become a growing concern in recent years, primarily in urban areas. Being exposed to air pollution is linked to developing numerous health problems, like the aggravation of respiratory diseases, cardiovascular disorders, lung function impairment, and even cancer or early death. Forecasting future levels of PM2.5 has become increasingly important over the past few years, as it can provide early warnings and help prevent diseases. This paper aims to design, fine-tune, test, and evaluate machine learning models for predicting future levels of PM2.5 over various time horizons. Our primary objective is to assess and compare the performance of multiple models, ranging from linear regression algorithms and ensemble-based methods to deep learning models, such as advanced recurrent neural networks and transformers, as well as large language models, on this forecasting task.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Single-Agent Reinforcement Learning for Regional Traffic Signal Control Under Demand Fluctuations</title>
<link>https://arxiv.org/abs/2511.00549</link>
<guid>https://arxiv.org/abs/2511.00549</guid>
<content:encoded><![CDATA[
arXiv:2511.00549v1 Announce Type: cross 
Abstract: Traffic congestion, primarily driven by intersection queuing, significantly impacts urban living standards, safety, environmental quality, and economic efficiency. While Traffic Signal Control (TSC) systems hold potential for congestion mitigation, traditional optimization models often fail to capture real-world traffic complexity and dynamics. This study introduces a novel single-agent reinforcement learning (RL) framework for regional adaptive TSC, circumventing the coordination complexities inherent in multi-agent systems through a centralized decision-making paradigm. The model employs an adjacency matrix to unify the encoding of road network topology, real-time queue states derived from probe vehicle data, and current signal timing parameters. Leveraging the efficient learning capabilities of the DreamerV3 world model, the agent learns control policies where actions sequentially select intersections and adjust their signal phase splits to regulate traffic inflow/outflow, analogous to a feedback control system. Reward design prioritizes queue dissipation, directly linking congestion metrics (queue length) to control actions. Simulation experiments conducted in SUMO demonstrate the model's effectiveness: under inference scenarios with multi-level (10%, 20%, 30%) Origin-Destination (OD) demand fluctuations, the framework exhibits robust anti-fluctuation capability and significantly reduces queue lengths. This work establishes a new paradigm for intelligent traffic control compatible with probe vehicle technology. Future research will focus on enhancing practical applicability by incorporating stochastic OD demand fluctuations during training and exploring regional optimization mechanisms for contingency events.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Temporal Fusion Transformer for Multi-Horizon Probabilistic Forecasting of Weekly Retail Sales</title>
<link>https://arxiv.org/abs/2511.00552</link>
<guid>https://arxiv.org/abs/2511.00552</guid>
<content:encoded><![CDATA[
arXiv:2511.00552v1 Announce Type: cross 
Abstract: Accurate multi-horizon retail forecasts are critical for inventory and promotions. We present a novel study of weekly Walmart sales (45 stores, 2010--2012) using a Temporal Fusion Transformer (TFT) that fuses static store identifiers with time-varying exogenous signals (holidays, CPI, fuel price, temperature). The pipeline produces 1--5-week-ahead probabilistic forecasts via Quantile Loss, yielding calibrated 90\% prediction intervals and interpretability through variable-selection networks, static enrichment, and temporal attention. On a fixed 2012 hold-out dataset, TFT achieves an RMSE of \$57.9k USD per store-week and an $R^2$ of 0.9875. Across a 5-fold chronological cross-validation, the averages are RMSE = \$64.6k USD and $R^2$ = 0.9844, outperforming the XGB, CNN, LSTM, and CNN-LSTM baseline models. These results demonstrate practical value for inventory planning and holiday-period optimization, while maintaining model transparency.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Red-teaming Activation Probes using Prompted LLMs</title>
<link>https://arxiv.org/abs/2511.00554</link>
<guid>https://arxiv.org/abs/2511.00554</guid>
<content:encoded><![CDATA[
arXiv:2511.00554v1 Announce Type: cross 
Abstract: Activation probes are attractive monitors for AI systems due to low cost and latency, but their real-world robustness remains underexplored. We ask: What failure modes arise under realistic, black-box adversarial pressure, and how can we surface them with minimal effort? We present a lightweight black-box red-teaming procedure that wraps an off-the-shelf LLM with iterative feedback and in-context learning (ICL), and requires no fine-tuning, gradients, or architectural access. Running a case study with probes for high-stakes interactions, we show that our approach can help discover valuable insights about a SOTA probe. Our analysis uncovers interpretable brittleness patterns (e.g., legalese-induced FPs; bland procedural tone FNs) and reduced but persistent vulnerabilities under scenario-constraint attacks. These results suggest that simple prompted red-teaming scaffolding can anticipate failure patterns before deployment and might yield promising, actionable insights to harden future probes.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FTT-GRU: A Hybrid Fast Temporal Transformer with GRU for Remaining Useful Life Prediction</title>
<link>https://arxiv.org/abs/2511.00564</link>
<guid>https://arxiv.org/abs/2511.00564</guid>
<content:encoded><![CDATA[
arXiv:2511.00564v1 Announce Type: cross 
Abstract: Accurate prediction of the remaining useful life (RUL) of industrial machinery is essential for reducing downtime and optimizing maintenance schedules. Existing approaches, such as long short-term memory (LSTM) networks and convolutional neural networks (CNNs), often struggle to model both global temporal dependencies and fine-grained degradation trends in multivariate sensor data. We propose a hybrid model, FTT-GRU, which combines a Fast Temporal Transformer (FTT) -- a lightweight Transformer variant using linearized attention via fast Fourier transform (FFT) -- with a gated recurrent unit (GRU) layer for sequential modeling. To the best of our knowledge, this is the first application of an FTT with a GRU for RUL prediction on NASA CMAPSS, enabling simultaneous capture of global and local degradation patterns in a compact architecture. On CMAPSS FD001, FTT-GRU attains RMSE 30.76, MAE 18.97, and $R^2=0.45$, with 1.12 ms CPU latency at batch=1. Relative to the best published deep baseline (TCN--Attention), it improves RMSE by 1.16\% and MAE by 4.00\%. Training curves averaged over $k=3$ runs show smooth convergence with narrow 95\% confidence bands, and ablations (GRU-only, FTT-only) support the contribution of both components. These results demonstrate that a compact Transformer-RNN hybrid delivers accurate and efficient RUL predictions on CMAPSS, making it suitable for real-time industrial prognostics.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlashEVA: Accelerating LLM inference via Efficient Attention</title>
<link>https://arxiv.org/abs/2511.00576</link>
<guid>https://arxiv.org/abs/2511.00576</guid>
<content:encoded><![CDATA[
arXiv:2511.00576v1 Announce Type: cross 
Abstract: Transformer models have revolutionized natural language processing, achieving state-of-the-art performance and demonstrating remarkable scalability. However, their memory demands, particularly due to maintaining full context in memory, pose significant challenges for inference. In this paper, we present FlashEVA, an efficient implementation of EVA (Efficient Attention via Control Variates), and demonstrate how to finetune transformers to adapt to FlashEVA attention. Our method enables fine-tuning of Transformer models with as few as 1.5B tokens while preserving effectiveness across various downstream tasks. Notably, FlashEVA achieves up to 6.7x higher throughput and 5x lower peak GPU memory usage during inference compared to standard Transformer implementations. Despite these improvements, we observe limitations in retrieval-focused tasks. Our implementation offers control over the trade-off between throughput and accuracy through adjustable hyperparameters, providing flexibility for diverse use cases. This work represents a significant step towards more efficient and adaptable Transformer-based models for inference.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TRACES: Temporal Recall with Contextual Embeddings for Real-Time Video Anomaly Detection</title>
<link>https://arxiv.org/abs/2511.00580</link>
<guid>https://arxiv.org/abs/2511.00580</guid>
<content:encoded><![CDATA[
arXiv:2511.00580v1 Announce Type: cross 
Abstract: Video anomalies often depend on contextual information available and temporal evolution. Non-anomalous action in one context can be anomalous in some other context. Most anomaly detectors, however, do not notice this type of context, which seriously limits their capability to generalize to new, real-life situations. Our work addresses the context-aware zero-shot anomaly detection challenge, in which systems need to learn adaptively to detect new events by correlating temporal and appearance features with textual traces of memory in real time. Our approach defines a memory-augmented pipeline, correlating temporal signals with visual embeddings using cross-attention, and real-time zero-shot anomaly classification by contextual similarity scoring. We achieve 90.4\% AUC on UCF-Crime and 83.67\% AP on XD-Violence, a new state-of-the-art among zero-shot models. Our model achieves real-time inference with high precision and explainability for deployment. We show that, by fusing cross-attention temporal fusion and contextual memory, we achieve high fidelity anomaly detection, a step towards the applicability of zero-shot models in real-world surveillance and infrastructure monitoring.
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
</item>
</channel>
</rss>