<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.AI updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.AI</link>

<item>
<title>MADCluster: Model-agnostic Anomaly Detection with Self-supervised Clustering Network</title>
<link>https://arxiv.org/abs/2505.16223</link>
<guid>https://arxiv.org/abs/2505.16223</guid>
<content:encoded><![CDATA[
<div> Keywords: MADCluster, anomaly detection, deep learning, model-agnostic, clustering<br>
Summary:<br>
MADCluster is a new model-agnostic anomaly detection framework that addresses the 'hypersphere collapse' issue in deep learning-based anomaly detection methods. It utilizes self-supervised clustering to cluster normal pattern data into a 'single cluster', learning the cluster center and mapping data close to this center. The framework introduces a novel 'One-directed Adaptive loss' for improved expressiveness and effective single clustering, with proven mathematical optimization. MADCluster comprises Base Embedder for capturing high-dimensional temporal dynamics, Cluster Distance Mapping, and Sequence-wise Clustering for continuous center updates. Its model-agnostic nature allows for the application of various architectures to the Base Embedder. Experiment results on four time series benchmark datasets demonstrate the enhanced performance of MADCluster compared to other models. MADCluster shows promise for improving model performance across different architectures. <br><br> <div>
arXiv:2505.16223v4 Announce Type: replace 
Abstract: In this paper, we propose MADCluster, a novel model-agnostic anomaly detection framework utilizing self-supervised clustering. MADCluster is applicable to various deep learning architectures and addresses the 'hypersphere collapse' problem inherent in existing deep learning-based anomaly detection methods. The core idea is to cluster normal pattern data into a 'single cluster' while simultaneously learning the cluster center and mapping data close to this center. Also, to improve expressiveness and enable effective single clustering, we propose a new 'One-directed Adaptive loss'. The optimization of this loss is mathematically proven. MADCluster consists of three main components: Base Embedder capturing high-dimensional temporal dynamics, Cluster Distance Mapping, and Sequence-wise Clustering for continuous center updates. Its model-agnostic characteristics are achieved by applying various architectures to the Base Embedder. Experiments on four time series benchmark datasets demonstrate that applying MADCluster improves the overall performance of comparative models. In conclusion, the compatibility of MADCluster shows potential for enhancing model performance across various architectures.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling and Optimizing User Preferences in AI Copilots: A Comprehensive Survey and Taxonomy</title>
<link>https://arxiv.org/abs/2505.21907</link>
<guid>https://arxiv.org/abs/2505.21907</guid>
<content:encoded><![CDATA[
<div> AI copilots, preference optimization, personalization, intelligent assistants, optimization algorithms
<br>
Summary: This article explores the role of user preferences in AI copilots, focusing on how these preferences are integrated and optimized throughout the interaction process. The study identifies the importance of personalization in enhancing user experience and satisfaction with AI-powered systems. It examines the sourcing, modeling, and refinement of preference signals across different interaction stages to improve the system's alignment with individual user preferences. The survey introduces a taxonomy of preference optimization techniques, evaluating their advantages, limitations, and design implications. By combining insights from AI personalization, human-AI interaction, and language model adaptation, the article provides a comprehensive framework for building user-centric AI copilots that are adaptable and personalized for diverse user needs. <div>
arXiv:2505.21907v2 Announce Type: replace 
Abstract: AI copilots represent a new generation of AI-powered systems designed to assist users, particularly knowledge workers and developers, in complex, context-rich tasks. As these systems become more embedded in daily workflows, personalization has emerged as a critical factor for improving usability, effectiveness, and user satisfaction. Central to this personalization is preference optimization: the system's ability to detect, interpret, and align with individual user preferences. While prior work in intelligent assistants and optimization algorithms is extensive, their intersection within AI copilots remains underexplored. This survey addresses that gap by examining how user preferences are operationalized in AI copilots. We investigate how preference signals are sourced, modeled across different interaction stages, and refined through feedback loops. Building on a comprehensive literature review, we define the concept of an AI copilot and introduce a taxonomy of preference optimization techniques across pre-, mid-, and post-interaction phases. Each technique is evaluated in terms of advantages, limitations, and design implications. By consolidating fragmented efforts across AI personalization, human-AI interaction, and language model adaptation, this work offers both a unified conceptual foundation and a practical design perspective for building user-aligned, persona-aware AI copilots that support end-to-end adaptability and deployment.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Framework for Human AI Collaboration in Security Operations Centers with Trusted Autonomy</title>
<link>https://arxiv.org/abs/2505.23397</link>
<guid>https://arxiv.org/abs/2505.23397</guid>
<content:encoded><![CDATA[
<div> framework, Human-AI collaboration, Security Operations Centers, autonomy, trust calibration, Human-in-the-loop decision making
Summary: 
This article introduces a structured framework for Human-AI collaboration in Security Operations Centers (SOCs), addressing limitations in existing frameworks that focus solely on automation. The proposed framework includes five levels of AI autonomy, mapping to Human-in-the-Loop (HITL) roles and task-specific trust thresholds. It enables adaptive and explainable AI integration across core SOC functions such as monitoring, threat detection, and incident response. The framework emphasizes the importance of formal connections between autonomy, trust, and HITL in SOC tasks, allowing for effective task distribution based on operational complexity and risks. A case study with the cybersecurity AI-Avatar demonstrates how human-AI collaboration can reduce alert fatigue, improve response coordination, and enhance trust calibration. Overall, this research highlights the potential of AI to enhance, rather than replace, human decision-making in cybersecurity operations. 
Summary: <div>
arXiv:2505.23397v2 Announce Type: replace 
Abstract: This article presents a structured framework for Human-AI collaboration in Security Operations Centers (SOCs), integrating AI autonomy, trust calibration, and Human-in-the-loop decision making. Existing frameworks in SOCs often focus narrowly on automation, lacking systematic structures to manage human oversight, trust calibration, and scalable autonomy with AI. Many assume static or binary autonomy settings, failing to account for the varied complexity, criticality, and risk across SOC tasks considering Humans and AI collaboration. To address these limitations, we propose a novel autonomy tiered framework grounded in five levels of AI autonomy from manual to fully autonomous, mapped to Human-in-the-Loop (HITL) roles and task-specific trust thresholds. This enables adaptive and explainable AI integration across core SOC functions, including monitoring, protection, threat detection, alert triage, and incident response. The proposed framework differentiates itself from previous research by creating formal connections between autonomy, trust, and HITL across various SOC levels, which allows for adaptive task distribution according to operational complexity and associated risks. The framework is exemplified through a simulated cyber range that features the cybersecurity AI-Avatar, a fine-tuned LLM-based SOC assistant. The AI-Avatar case study illustrates human-AI collaboration for SOC tasks, reducing alert fatigue, enhancing response coordination, and strategically calibrating trust. This research systematically presents both the theoretical and practical aspects and feasibility of designing next-generation cognitive SOCs that leverage AI not to replace but to enhance human decision-making.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergent Risk Awareness in Rational Agents under Resource Constraints</title>
<link>https://arxiv.org/abs/2505.23436</link>
<guid>https://arxiv.org/abs/2505.23436</guid>
<content:encoded><![CDATA[
<div> survival bandit framework, constraint exposure, misalignment, risk-seeking, risk-averse <br>
Summary:
This study explores the behavior of AI agents with agentic capabilities in settings with resource or failure constraints. The agents, designed to interact with humans and make decisions based on utility functions, must navigate trade-offs when facing constraints that may terminate action sequences prematurely. The asymmetries in constraint exposure can lead to misalignments between human objectives and agent incentives. Through a survival bandit framework, the study quantifies the impact of survival-driven preference shifts and identifies conditions for misalignment. It also proposes mechanisms to mitigate risk-seeking or risk-averse behaviors that may arise. By increasing understanding of AI agents operating under survival pressure, this work offers guidelines for safely deploying such systems in resource-limited environments. <br> <div>
arXiv:2505.23436v2 Announce Type: replace 
Abstract: Advanced reasoning models with agentic capabilities (AI agents) are deployed to interact with humans and to solve sequential decision-making problems under (approximate) utility functions and internal models. When such problems have resource or failure constraints where action sequences may be forcibly terminated once resources are exhausted, agents face implicit trade-offs that reshape their utility-driven (rational) behaviour. Additionally, since these agents are typically commissioned by a human principal to act on their behalf, asymmetries in constraint exposure can give rise to previously unanticipated misalignment between human objectives and agent incentives. We formalise this setting through a survival bandit framework, provide theoretical and empirical results that quantify the impact of survival-driven preference shifts, identify conditions under which misalignment emerges and propose mechanisms to mitigate the emergence of risk-seeking or risk-averse behaviours. As a result, this work aims to increase understanding and interpretability of emergent behaviours of AI agents operating under such survival pressure, and offer guidelines for safely deploying such AI systems in critical resource-limited environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fortune: Formula-Driven Reinforcement Learning for Symbolic Table Reasoning in Language Models</title>
<link>https://arxiv.org/abs/2505.23667</link>
<guid>https://arxiv.org/abs/2505.23667</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, language models, table understanding, numerical reasoning, symbolic reasoning
<br>
Summary: 
Formula Tuning (Fortune) is a reinforcement learning framework designed to improve the performance of large language models in accurately understanding and reasoning with tabular data, particularly in complex scenarios. By training language models to generate executable spreadsheet formulas for question answering over general tabular data, Formula Tuning reduces the need for supervised formula annotations by using answer correctness as a reward signal to guide model learning. The framework enhances language model performance, especially in multi-step numerical and symbolic reasoning tasks, ultimately enabling a 7B model to outperform OpenAI o1 in table understanding benchmarks. This highlights the potential of formula-driven reinforcement learning to advance symbolic table reasoning in language models. <div>
arXiv:2505.23667v2 Announce Type: replace 
Abstract: Tables are a fundamental structure for organizing and analyzing data, making effective table understanding a critical capability for intelligent systems. While large language models (LMs) demonstrate strong general reasoning abilities, they continue to struggle with accurate numerical or symbolic reasoning over tabular data, especially in complex scenarios. Spreadsheet formulas provide a powerful and expressive medium for representing executable symbolic operations, encoding rich reasoning patterns that remain largely underutilized. In this paper, we propose Formula Tuning (Fortune), a reinforcement learning (RL) framework that trains LMs to generate executable spreadsheet formulas for question answering over general tabular data. Formula Tuning reduces the reliance on supervised formula annotations by using binary answer correctness as a reward signal, guiding the model to learn formula derivation through reasoning. We provide a theoretical analysis of its advantages and demonstrate its effectiveness through extensive experiments on seven table reasoning benchmarks. Formula Tuning substantially enhances LM performance, particularly on multi-step numerical and symbolic reasoning tasks, enabling a 7B model to outperform OpenAI o1 on table understanding. This highlights the potential of formula-driven RL to advance symbolic table reasoning in LMs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LENSLLM: Unveiling Fine-Tuning Dynamics for LLM Selection</title>
<link>https://arxiv.org/abs/2505.03793</link>
<guid>https://arxiv.org/abs/2505.03793</guid>
<content:encoded><![CDATA[
<div> PAC-Bayesian Generalization Bound, LLMs, Neural Tangent Kernel, model selection, downstream tasks 
Summary: 
The article addresses the challenge of efficiently selecting Large Language Models (LLMs) for diverse downstream tasks due to computational constraints. It introduces a novel theoretical framework that sheds light on the fine-tuning dynamics of LLMs to enhance understanding of their generalization capabilities. The proposed LENSLLM model, based on Neural Tangent Kernel (NTK), offers accurate performance predictions across tasks while maintaining computational efficiency. Empirical results on large-scale benchmarks demonstrate the model's effectiveness, achieving high accuracy and significantly reducing computational costs in LLM selection compared to state-of-the-art methods. The open-sourced LENSLLM model and results are available at LensLLM.io. <div>
arXiv:2505.03793v3 Announce Type: replace-cross 
Abstract: The proliferation of open-sourced Large Language Models (LLMs) and diverse downstream tasks necessitates efficient model selection, given the impracticality of fine-tuning all candidates due to computational constraints. Despite the recent advances in LLM selection, a fundamental research question largely remains nascent: how can we model the dynamic behaviors of LLMs during fine-tuning, thereby enhancing our understanding of their generalization performance across diverse downstream tasks? In this work, we propose a novel theoretical framework that provides a proper lens to assess the generalization capabilities of LLMs, thereby enabling accurate and efficient LLM selection for downstream applications. In particular, we first derive a PAC-Bayesian Generalization Bound that unveils fine-tuning dynamics of LLMs and then introduce LENSLLM, a Neural Tangent Kernel (NTK)-based Rectified Scaling Model that enables accurate performance predictions across diverse tasks while maintaining computational efficiency. Extensive empirical results on 3 large-scale benchmarks demonstrate that our model achieves up to 91.1% accuracy and reduces up to 88.5% computational cost in LLM selection, outperforming 5 state-of-the-art methods. We open-source our proposed LENSLLM model and corresponding results at LensLLM.io.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Face Swapping: A Diffusion-Based Digital Human Benchmark for Multimodal Deepfake Detection</title>
<link>https://arxiv.org/abs/2505.16512</link>
<guid>https://arxiv.org/abs/2505.16512</guid>
<content:encoded><![CDATA[
<div> Dataset, Deepfake technology, Detection strategies, DigiFakeAV, DigiShield

Summary:
DigiFakeAV is a new large-scale dataset for digital human forgery based on diffusion models, containing 60,000 videos across various demographics and scenarios. Participants in user studies had a high misrecognition rate of up to 68%, revealing the realism of the dataset. Existing detection models showed significant performance degradation on DigiFakeAV, highlighting the challenges posed by diffusion-based deepfake technology. To address this issue, DigiShield, a detection baseline leveraging spatiotemporal and cross-modal fusion, achieved state-of-the-art performance on DigiFakeAV and demonstrated strong generalization to other datasets. This research underscores the growing threat of deepfake technology to public security and the urgent need for advanced detection strategies to combat the dissemination of highly realistic fake videos. 

<br><br>Summary: <div>
arXiv:2505.16512v3 Announce Type: replace-cross 
Abstract: In recent years, the explosive advancement of deepfake technology has posed a critical and escalating threat to public security: diffusion-based digital human generation. Unlike traditional face manipulation methods, such models can generate highly realistic videos with consistency via multimodal control signals. Their flexibility and covertness pose severe challenges to existing detection strategies. To bridge this gap, we introduce DigiFakeAV, the new large-scale multimodal digital human forgery dataset based on diffusion models. Leveraging five of the latest digital human generation methods and a voice cloning method, we systematically construct a dataset comprising 60,000 videos (8.4 million frames), covering multiple nationalities, skin tones, genders, and real-world scenarios, significantly enhancing data diversity and realism. User studies demonstrate that the misrecognition rate by participants for DigiFakeAV reaches as high as 68%. Moreover, the substantial performance degradation of existing detection models on our dataset further highlights its challenges. To address this problem, we propose DigiShield, an effective detection baseline based on spatiotemporal and cross-modal fusion. By jointly modeling the 3D spatiotemporal features of videos and the semantic-acoustic features of audio, DigiShield achieves state-of-the-art (SOTA) performance on the DigiFakeAV and shows strong generalization on other datasets.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Moderating Harm: Benchmarking Large Language Models for Cyberbullying Detection in YouTube Comments</title>
<link>https://arxiv.org/abs/2505.18927</link>
<guid>https://arxiv.org/abs/2505.18927</guid>
<content:encoded><![CDATA[
<div> benchmarking, large language models, online platforms, automated content moderation, comment sections

Summary:
Large language models such as OpenAI GPT-4.1, Google Gemini 1.5 Pro, and Anthropic Claude 3 Opus were benchmarked on a dataset of 5,080 YouTube comments to evaluate their performance in identifying harmful content. GPT-4.1 achieved the best balance of precision and recall, while Gemini flagged the highest number of harmful posts but with lower precision. Claude had the highest precision but lower recall. All three models struggled with sarcasm, coded insults, and mixed-language slang. The study highlights the need for moderation pipelines that combine different models, consider conversational context, and address challenges in under-represented languages and implicit abuse. The dataset and prompts used in the study have been made publicly available for reproducibility and further advancements in automated content moderation.<br><br>Summary: <div>
arXiv:2505.18927v3 Announce Type: replace-cross 
Abstract: As online platforms grow, comment sections increasingly host harassment that undermines user experience and well-being. This study benchmarks three leading large language models, OpenAI GPT-4.1, Google Gemini 1.5 Pro, and Anthropic Claude 3 Opus, on a corpus of 5,080 YouTube comments sampled from high-abuse threads in gaming, lifestyle, food vlog, and music channels. The dataset comprises 1,334 harmful and 3,746 non-harmful messages in English, Arabic, and Indonesian, annotated independently by two reviewers with substantial agreement (Cohen's kappa = 0.83). Using a unified prompt and deterministic settings, GPT-4.1 achieved the best overall balance with an F1 score of 0.863, precision of 0.887, and recall of 0.841. Gemini flagged the highest share of harmful posts (recall = 0.875) but its precision fell to 0.767 due to frequent false positives. Claude delivered the highest precision at 0.920 and the lowest false-positive rate of 0.022, yet its recall dropped to 0.720. Qualitative analysis showed that all three models struggle with sarcasm, coded insults, and mixed-language slang. These results underscore the need for moderation pipelines that combine complementary models, incorporate conversational context, and fine-tune for under-represented languages and implicit abuse. A de-identified version of the dataset and full prompts is publicly released to promote reproducibility and further progress in automated content moderation.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models</title>
<link>https://arxiv.org/abs/2505.21523</link>
<guid>https://arxiv.org/abs/2505.21523</guid>
<content:encoded><![CDATA[
<div> metric, perception, reasoning, hallucination, multimodal task 

Summary: 
- The study focuses on the impact of reasoning length on the hallucination in multimodal large language models.
- A new metric RH-AUC is introduced to quantify changes in perception accuracy with reasoning length.
- RH-Bench, a diagnostic benchmark, is released to evaluate the balance between reasoning ability and hallucination in various multimodal tasks.
- Larger models tend to maintain a better balance between reasoning and perception.
- The types and domains of training data have a greater influence on the balance between reasoning ability and hallucination than the volume of data.
- Evaluation frameworks should consider both reasoning quality and perceptual fidelity for a comprehensive assessment of multimodal models. 

<br><br>Summary: <div>
arXiv:2505.21523v2 Announce Type: replace-cross 
Abstract: Test-time compute has empowered multimodal large language models to generate extended reasoning chains, yielding strong performance on tasks such as multimodal math reasoning. However, this improved reasoning ability often comes with increased hallucination: as generations become longer, models tend to drift away from image-grounded content and rely more heavily on language priors. Attention analysis shows that longer reasoning chains lead to reduced focus on visual inputs, which contributes to hallucination. To systematically study this phenomenon, we introduce RH-AUC, a metric that quantifies how a model's perception accuracy changes with reasoning length, allowing us to evaluate whether the model preserves visual grounding during reasoning. We also release RH-Bench, a diagnostic benchmark that spans a variety of multimodal tasks, designed to assess the trade-off between reasoning ability and hallucination. Our analysis reveals that (i) larger models typically achieve a better balance between reasoning and perception, and (ii) this balance is influenced more by the types and domains of training data than by its overall volume. These findings underscore the importance of evaluation frameworks that jointly consider both reasoning quality and perceptual fidelity.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RePaViT: Scalable Vision Transformer Acceleration via Structural Reparameterization on Feedforward Network Layers</title>
<link>https://arxiv.org/abs/2505.21847</link>
<guid>https://arxiv.org/abs/2505.21847</guid>
<content:encoded><![CDATA[
<div> FFN layers, attention layers, Vision Transformer, latency, efficiency <br>
<br>
Summary:
The study finds that FFN layers are the main contributors to Vision Transformer (ViT) inference latency, especially as model size increases. A novel channel idle mechanism is proposed to optimize the efficiency of large-scale ViTs by focusing on FFN layers. This mechanism allows for post-training structural reparameterization, where idle feature channels bypass the nonlinear activation function in FFN layers, resulting in faster inference. RePaViTs, the resulting family of models, show significant latency reductions with acceptable accuracy trade-offs, which tend to improve on larger models. For instance, RePa-ViT-Large and RePa-ViT-Huge achieve notable speed-ups with slightly higher accuracies under the same training strategy. This approach marks the first use of structural reparameterization on FFN layers to accelerate ViTs and suggests a promising direction for efficient ViTs in the future. <div>
arXiv:2505.21847v2 Announce Type: replace-cross 
Abstract: We reveal that feedforward network (FFN) layers, rather than attention layers, are the primary contributors to Vision Transformer (ViT) inference latency, with their impact signifying as model size increases. This finding highlights a critical opportunity for optimizing the efficiency of large-scale ViTs by focusing on FFN layers. In this work, we propose a novel channel idle mechanism that facilitates post-training structural reparameterization for efficient FFN layers during testing. Specifically, a set of feature channels remains idle and bypasses the nonlinear activation function in each FFN layer, thereby forming a linear pathway that enables structural reparameterization during inference. This mechanism results in a family of ReParameterizable Vision Transformers (RePaViTs), which achieve remarkable latency reductions with acceptable sacrifices (sometimes gains) in accuracy across various ViTs. The benefits of our method scale consistently with model sizes, demonstrating greater speed improvements and progressively narrowing accuracy gaps or even higher accuracies on larger models. In particular, RePa-ViT-Large and RePa-ViT-Huge enjoy 66.8% and 68.7% speed-ups with +1.7% and +1.1% higher top-1 accuracies under the same training strategy, respectively. RePaViT is the first to employ structural reparameterization on FFN layers to expedite ViTs to our best knowledge, and we believe that it represents an auspicious direction for efficient ViTs. Source code is available at https://github.com/Ackesnal/RePaViT.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Practical Adversarial Attacks on Stochastic Bandits via Fake Data Injection</title>
<link>https://arxiv.org/abs/2505.21938</link>
<guid>https://arxiv.org/abs/2505.21938</guid>
<content:encoded><![CDATA[
<div> Adversarial attacks, stochastic bandits, fake data injection, practical threat model, efficient attack strategies<br>
<br>
Summary:<br>
This article introduces a more practical threat model for adversarial attacks on stochastic bandits called Fake Data Injection, where attackers can inject a limited number of bounded fake feedback samples. Efficient attack strategies are designed considering magnitude and temporal constraints. Theoretical analysis reveals that these attacks can mislead UCB and Thompson Sampling algorithms into selecting a target arm with minimal attack cost. Experiments confirm the effectiveness of these strategies, exposing vulnerabilities in common stochastic bandit algorithms under realistic adversarial scenarios. <div>
arXiv:2505.21938v2 Announce Type: replace-cross 
Abstract: Adversarial attacks on stochastic bandits have traditionally relied on some unrealistic assumptions, such as per-round reward manipulation and unbounded perturbations, limiting their relevance to real-world systems. We propose a more practical threat model, Fake Data Injection, which reflects realistic adversarial constraints: the attacker can inject only a limited number of bounded fake feedback samples into the learner's history, simulating legitimate interactions. We design efficient attack strategies under this model, explicitly addressing both magnitude constraints (on reward values) and temporal constraints (on when and how often data can be injected). Our theoretical analysis shows that these attacks can mislead both Upper Confidence Bound (UCB) and Thompson Sampling algorithms into selecting a target arm in nearly all rounds while incurring only sublinear attack cost. Experiments on synthetic and real-world datasets validate the effectiveness of our strategies, revealing significant vulnerabilities in widely used stochastic bandit algorithms under practical adversarial scenarios.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>iDSE: Navigating Design Space Exploration in High-Level Synthesis Using LLMs</title>
<link>https://arxiv.org/abs/2505.22086</link>
<guid>https://arxiv.org/abs/2505.22086</guid>
<content:encoded><![CDATA[
<div> Keywords: High-Level Synthesis, Design Space Exploration, Machine Learning Models, Multiobjective Optimization, Hardware Optimization <br>
<br>
Summary: 
The article introduces iDSE, a novel Design Space Exploration (DSE) framework that leverages Machine Learning Models (LLMs) to enhance High-Level Synthesis (HLS) design optimization. iDSE aims to efficiently navigate the complex design space in HLS by intelligently pruning design options and utilizing LLMs for calibration. By leveraging convergent and divergent thinking patterns of LLMs, iDSE achieves multi-path refinement of design quality and diversity. Experimental results show that iDSE outperforms heuristic-based DSE methods by a significant margin, approaching the reference Pareto front with minimal designs explored. The study highlights the transformative potential of LLMs in scalable and efficient HLS optimization, providing valuable insights into multiobjective optimization challenges in hardware design. <br> <div>
arXiv:2505.22086v2 Announce Type: replace-cross 
Abstract: High-Level Synthesis (HLS) serves as an agile hardware development tool that streamlines the circuit design by abstracting the register transfer level into behavioral descriptions, while allowing designers to customize the generated microarchitectures through optimization directives. However, the combinatorial explosion of possible directive configurations yields an intractable design space. Traditional design space exploration (DSE) methods, despite adopting heuristics or constructing predictive models to accelerate Pareto-optimal design acquisition, still suffer from prohibitive exploration costs and suboptimal results. Addressing these concerns, we introduce iDSE, the first LLM-aided DSE framework that leverages HLS design quality perception to effectively navigate the design space. iDSE intelligently pruns the design space to guide LLMs in calibrating representative initial sampling designs, expediting convergence toward the Pareto front. By exploiting the convergent and divergent thinking patterns inherent in LLMs for hardware optimization, iDSE achieves multi-path refinement of the design quality and diversity. Extensive experiments demonstrate that iDSE outperforms heuristic-based DSE methods by 5.1$\times$$\sim$16.6$\times$ in proximity to the reference Pareto front, matching NSGA-II with only 4.6% of the explored designs. Our work demonstrates the transformative potential of LLMs in scalable and efficient HLS design optimization, offering new insights into multiobjective optimization challenges.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Judging Quality Across Languages: A Multilingual Approach to Pretraining Data Filtering with Language Models</title>
<link>https://arxiv.org/abs/2505.22232</link>
<guid>https://arxiv.org/abs/2505.22232</guid>
<content:encoded><![CDATA[
<div> approach, multilingual data, curation, language models, JQL <br>
Summary: <br>
High-quality multilingual training data is crucial for effective pretraining of large language models (LLMs). The existing availability of suitable open-source multilingual datasets is limited, often relying on heuristic filtering methods. In this study, the JQL approach is introduced, which systematically curates diverse and high-quality multilingual data at scale while reducing computational demands. The JQL process utilizes lightweight annotators based on pretrained multilingual embeddings to enhance multilingual and cross-lingual performance for languages and scripts not seen during training. Empirical evaluation across 35 languages demonstrates that JQL outperforms current heuristic filtering methods like Fineweb2, improving downstream model training quality and data retention rates. This research offers practical insights and resources for multilingual data curation, setting higher standards for multilingual dataset development. <br> <div>
arXiv:2505.22232v2 Announce Type: replace-cross 
Abstract: High-quality multilingual training data is essential for effectively pretraining large language models (LLMs). Yet, the availability of suitable open-source multilingual datasets remains limited. Existing state-of-the-art datasets mostly rely on heuristic filtering methods, restricting both their cross-lingual transferability and scalability. Here, we introduce JQL, a systematic approach that efficiently curates diverse and high-quality multilingual data at scale while significantly reducing computational demands. JQL distills LLMs' annotation capabilities into lightweight annotators based on pretrained multilingual embeddings. These models exhibit robust multilingual and cross-lingual performance, even for languages and scripts unseen during training. Evaluated empirically across 35 languages, the resulting annotation pipeline substantially outperforms current heuristic filtering methods like Fineweb2. JQL notably enhances downstream model training quality and increases data retention rates. Our research provides practical insights and valuable resources for multilingual data curation, raising the standards of multilingual dataset development.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FastTD3: Simple, Fast, and Capable Reinforcement Learning for Humanoid Control</title>
<link>https://arxiv.org/abs/2505.22642</link>
<guid>https://arxiv.org/abs/2505.22642</guid>
<content:encoded><![CDATA[
<div> FastTD3, RL algorithm, humanoid robots, training time, parallel simulation<br>
<br>
FastTD3 is a new RL algorithm designed for humanoid robots, aiming to reduce complexity and training times. It utilizes off-policy TD3 with modifications such as parallel simulation, large-batch updates, distributional critic, and optimized hyperparameters. By implementing FastTD3, training time for humanoid robot tasks in popular suites like HumanoidBench, IsaacLab, and MuJoCo Playground is significantly reduced to under 3 hours on a single A100 GPU. The algorithm remains stable during training and offers a simple and easy-to-use implementation for robotics researchers to accelerate their work. Key features of FastTD3 include faster training times, simplicity, efficiency, and capability to solve a wide range of complex tasks in humanoid robotics.<br><br>Summary: <div>
arXiv:2505.22642v3 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) has driven significant progress in robotics, but its complexity and long training times remain major bottlenecks. In this report, we introduce FastTD3, a simple, fast, and capable RL algorithm that significantly speeds up training for humanoid robots in popular suites such as HumanoidBench, IsaacLab, and MuJoCo Playground. Our recipe is remarkably simple: we train an off-policy TD3 agent with several modifications -- parallel simulation, large-batch updates, a distributional critic, and carefully tuned hyperparameters. FastTD3 solves a range of HumanoidBench tasks in under 3 hours on a single A100 GPU, while remaining stable during training. We also provide a lightweight and easy-to-use implementation of FastTD3 to accelerate RL research in robotics.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian</title>
<link>https://arxiv.org/abs/2505.22759</link>
<guid>https://arxiv.org/abs/2505.22759</guid>
<content:encoded><![CDATA[
<div> open science, speech foundation models, FAMA, reproducibility, fair evaluation

Summary:<br>
- The article introduces FAMA, a family of open science SFMs for English and Italian, addressing the lack of transparency in existing models.
- These models are trained on over 150k hours of open-source speech data and achieve competitive performance while being significantly faster.
- A new dataset with 16k hours of cleaned and pseudo-labeled speech for both languages is also presented.
- FAMA aims to promote openness in speech technology research by releasing all artifacts, including code, datasets, and models, under open-source compliant licenses.
- The development of FAMA helps bridge the gap in the field of speech processing towards open science practices. 

Summary: <div>
arXiv:2505.22759v2 Announce Type: replace-cross 
Abstract: The development of speech foundation models (SFMs) like Whisper and SeamlessM4T has significantly advanced the field of speech processing. However, their closed nature--with inaccessible training data and code--poses major reproducibility and fair evaluation challenges. While other domains have made substantial progress toward open science by developing fully transparent models trained on open-source (OS) code and data, similar efforts in speech remain limited. To fill this gap, we introduce FAMA, the first family of open science SFMs for English and Italian, trained on 150k+ hours of OS speech data. Moreover, we present a new dataset containing 16k hours of cleaned and pseudo-labeled speech for both languages. Results show that FAMA achieves competitive performance compared to existing SFMs while being up to 8 times faster. All artifacts, including code, datasets, and models, are released under OS-compliant licenses, promoting openness in speech technology research.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiLDe: Intentional Code Generation via Human-in-the-Loop Decoding</title>
<link>https://arxiv.org/abs/2505.22906</link>
<guid>https://arxiv.org/abs/2505.22906</guid>
<content:encoded><![CDATA[
<div> AI programming tools, Human-in-the-loop Decoding, code generation, LLM, HiLDe<br>
Summary:<br>
The study discusses the impact of AI programming tools on users' decision-making processes, particularly in critical domains like software security. It introduces Human-in-the-loop Decoding (HiLDe) as an interaction technique that allows users to observe and influence decisions made by the Language Model (LLM) during code generation. HiLDe highlights critical decisions made by the LLM and provides local alternatives for users to explore, leading to better alignment of code generation with personal goals. A study involving 18 participants on security-related tasks showed that HiLDe resulted in fewer vulnerabilities generated and improved user alignment with their goals compared to traditional code completion assistants. This research emphasizes the importance of involving users in the decision-making process when utilizing AI programming tools, particularly in areas where security is paramount. <div>
arXiv:2505.22906v2 Announce Type: replace-cross 
Abstract: While AI programming tools hold the promise of increasing programmers' capabilities and productivity to a remarkable degree, they often exclude users from essential decision-making processes, causing many to effectively "turn off their brains" and over-rely on solutions provided by these systems. These behaviors can have severe consequences in critical domains, like software security. We propose Human-in-the-loop Decoding, a novel interaction technique that allows users to observe and directly influence LLM decisions during code generation, in order to align the model's output with their personal requirements. We implement this technique in HiLDe, a code completion assistant that highlights critical decisions made by the LLM and provides local alternatives for the user to explore. In a within-subjects study (N=18) on security-related tasks, we found that HiLDe led participants to generate significantly fewer vulnerabilities and better align code generation with their goals compared to a traditional code completion assistant.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-Robust Knowledge Editing for Language Models</title>
<link>https://arxiv.org/abs/2505.23026</link>
<guid>https://arxiv.org/abs/2505.23026</guid>
<content:encoded><![CDATA[
<div> Knowledge editing, CHED benchmark, context robustness, CoRE method, hidden states<br>
<br>
Summary: 
The article discusses the development of a benchmark, CHED, to evaluate the context robustness of knowledge editing (KE) methods in language models. It highlights the importance of considering preceding contexts in evaluating KE success, as they can impact the intended edit. The study introduces the CoRE method, which aims to improve context robustness by minimizing context-sensitive variance in hidden model states. Evaluations on CHED show that existing KE methods often fail when preceding contexts are present, underscoring the need for better context handling. The CoRE method not only enhances editing success rates in the presence of contexts but also maintains the overall model capabilities. The article also examines the effects of different types of preceding contexts, such as user utterances and assistant responses, on editing success and analyzes attention-score patterns to understand token influences. <div>
arXiv:2505.23026v2 Announce Type: replace-cross 
Abstract: Knowledge editing (KE) methods offer an efficient way to modify knowledge in large language models. Current KE evaluations typically assess editing success by considering only the edited knowledge without any preceding contexts. In real-world applications, however, preceding contexts often trigger the retrieval of the original knowledge and undermine the intended edit. To address this issue, we develop CHED -- a benchmark designed to evaluate the context robustness of KE methods. Evaluations on CHED show that they often fail when preceding contexts are present. To mitigate this shortcoming, we introduce CoRE, a KE method designed to strengthen context robustness by minimizing context-sensitive variance in hidden states of the model for edited knowledge. This method not only improves the editing success rate in situations where a preceding context is present but also preserves the overall capabilities of the model. We provide an in-depth analysis of the differing impacts of preceding contexts when introduced as user utterances versus assistant responses, and we dissect attention-score patterns to assess how specific tokens influence editing success.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Matryoshka Model Learning for Improved Elastic Student Models</title>
<link>https://arxiv.org/abs/2505.23337</link>
<guid>https://arxiv.org/abs/2505.23337</guid>
<content:encoded><![CDATA[
<div> framework, training, models, Student, accuracy <br>
Summary: <br>
The paper introduces MatTA, a framework for training accurate Student models using a Teacher-TA-Student approach. TA models, larger versions of Student models, help students relate better to the Teacher model and provide domain-specific expertise. Multiple accurate Student models can be extracted from the TA model, offering servable options with varying accuracy and serving costs. The efficacy of MatTA is demonstrated on proprietary datasets, showing a 20% improvement in a production ML system. Additionally, experiments on GPT-2 Medium showcase relative improvements of over 24% on SAT Math and over 10% on the LAMBADA benchmark. Overall, MatTA offers a practical solution for developing industry-grade ML models efficiently while meeting evolving serving constraints. <br> <div>
arXiv:2505.23337v2 Announce Type: replace-cross 
Abstract: Industry-grade ML models are carefully designed to meet rapidly evolving serving constraints, which requires significant resources for model development. In this paper, we propose MatTA, a framework for training multiple accurate Student models using a novel Teacher-TA-Student recipe. TA models are larger versions of the Student models with higher capacity, and thus allow Student models to better relate to the Teacher model and also bring in more domain-specific expertise. Furthermore, multiple accurate Student models can be extracted from the TA model. Therefore, despite only one training run, our methodology provides multiple servable options to trade off accuracy for lower serving cost. We demonstrate the proposed method, MatTA, on proprietary datasets and models. Its practical efficacy is underscored by live A/B tests within a production ML system, demonstrating 20% improvement on a key metric. We also demonstrate our method on GPT-2 Medium, a public model, and achieve relative improvements of over 24% on SAT Math and over 10% on the LAMBADA benchmark.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Afterburner: Reinforcement Learning Facilitates Self-Improving Code Efficiency Optimization</title>
<link>https://arxiv.org/abs/2505.23387</link>
<guid>https://arxiv.org/abs/2505.23387</guid>
<content:encoded><![CDATA[
<div> Supervised Fine-Tuning, Direct Preference Optimization, Group Relative Policy Optimization, Large Language Models, code efficiency <br>
Summary: <br>
The paper introduces a novel test-time iterative optimization framework to improve code efficiency in Large Language Models (LLMs). Three training strategies were explored: Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Group Relative Policy Optimization (GRPO). SFT and DPO showed rapid saturation in efficiency gains, while GRPO, utilizing reinforcement learning (RL) with execution feedback, continuously optimized code performance. Experiments on the Venus dataset and the APPS benchmark demonstrated that GRPO significantly improved code efficiency, increasing pass@1 from 47% to 62% and the likelihood of outperforming human submissions in efficiency from 31% to 45%. The study highlights the effectiveness of test-time code efficiency enhancement and the capability of RL in teaching LLMs to autonomously enhance code efficiency. <br> <div>
arXiv:2505.23387v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) generate functionally correct solutions but often fall short in code efficiency, a critical bottleneck for real-world deployment. In this paper, we introduce a novel test-time iterative optimization framework to address this, employing a closed-loop system where LLMs iteratively refine code based on empirical performance feedback from an execution sandbox. We explore three training strategies: Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Group Relative Policy Optimization (GRPO). Experiments on our Venus dataset and the APPS benchmark show that SFT and DPO rapidly saturate in efficiency gains. In contrast, GRPO, using reinforcement learning (RL) with execution feedback, continuously optimizes code performance, significantly boosting both pass@1 (from 47% to 62%) and the likelihood of outperforming human submissions in efficiency (from 31% to 45%). Our work demonstrates effective test-time code efficiency improvement and critically reveals the power of RL in teaching LLMs to truly self-improve code efficiency.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWE-bench Goes Live!</title>
<link>https://arxiv.org/abs/2505.23419</link>
<guid>https://arxiv.org/abs/2505.23419</guid>
<content:encoded><![CDATA[
<div> benchmark, large language models, issue-resolving, SWE-bench-Live, real-world bugs

Summary:
SWE-bench-Live is introduced as a dynamic benchmark for evaluating large language models (LLMs) by generating patches for real-world bugs. The existing benchmarks, like SWE-bench, have limitations due to outdated data and manual processes. SWE-bench-Live addresses these issues by providing 1,319 tasks from recent GitHub issues across 93 repositories, with reproducible Docker images for each task. The \method automated curation pipeline streamlines task creation and environment setup, enhancing scalability and continuous updates. Evaluation of LLMs on SWE-bench-Live reveals a performance gap compared to static benchmarks. Detailed analyses on repository origin, issue recency, and task difficulty are performed to understand the differences. This live and diverse benchmark offers a contamination-resistant evaluation environment for LLMs and agents in real-world software development scenarios. 

<br><br>Summary: <div>
arXiv:2505.23419v2 Announce Type: replace-cross 
Abstract: The issue-resolving task, where a model generates patches to fix real-world bugs, has emerged as a critical benchmark for evaluating the capabilities of large language models (LLMs). While SWE-bench and its variants have become standard in this domain, they suffer from key limitations: they have not been updated since their initial releases, cover a narrow set of repositories, and depend heavily on manual effort for instance construction and environment setup. These factors hinder scalability and introduce risks of overfitting and data contamination. In this work, we present SWE-bench-Live, a live-updatable benchmark designed to overcome these challenges. Our initial release consists of 1,319 tasks derived from real GitHub issues created since 2024, spanning 93 repositories. Each task is accompanied by a dedicated Docker image to ensure reproducible execution. Central to our benchmark is \method, an automated curation pipeline that streamlines the entire process from instance creation to environment setup, removing manual bottlenecks and enabling scalability and continuous updates. We evaluate a range of state-of-the-art agent frameworks and LLMs on SWE-bench-Live, revealing a substantial performance gap compared to static benchmarks like SWE-bench, even under controlled evaluation conditions. To better understand this discrepancy, we perform detailed analyses across repository origin, issue recency, and task difficulty. By providing a fresh, diverse, and executable benchmark grounded in live repository activity, SWE-bench-Live facilitates rigorous, contamination-resistant evaluation of LLMs and agents in dynamic, real-world software development settings.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cognitive Guardrails for Open-World Decision Making in Autonomous Drone Swarms</title>
<link>https://arxiv.org/abs/2505.23576</link>
<guid>https://arxiv.org/abs/2505.23576</guid>
<content:encoded><![CDATA[
<div> Keywords: Small Uncrewed Aerial Systems, autonomous swarms, computer vision, large language models, search-and-rescue missions 

Summary: 
Small Uncrewed Aerial Systems (sUAS) are being used in autonomous swarms for search-and-rescue missions, relying on computer vision technology to detect objects. However, traditional CV systems face challenges in recognizing unfamiliar objects and their relevance. To address this, large language models (LLMs) are integrated to provide insights, although they may produce incorrect recommendations. To ensure safe decision-making, cognitive guardrails are essential. This article introduces the design, simulation, and real-world implementation of cognitive guardrails for sUAS swarms in search-and-rescue missions. The aim is to enable sUAS to make sensible decisions and adapt their missions effectively, even in uncertain and complex environments. The incorporation of LLMs and cognitive guardrails enhances the capabilities of sUAS in disaster-response scenarios, improving their ability to navigate and respond to changing conditions for successful mission outcomes. 

<br><br>Summary: <div>
arXiv:2505.23576v2 Announce Type: replace-cross 
Abstract: Small Uncrewed Aerial Systems (sUAS) are increasingly deployed as autonomous swarms in search-and-rescue and other disaster-response scenarios. In these settings, they use computer vision (CV) to detect objects of interest and autonomously adapt their missions. However, traditional CV systems often struggle to recognize unfamiliar objects in open-world environments or to infer their relevance for mission planning. To address this, we incorporate large language models (LLMs) to reason about detected objects and their implications. While LLMs can offer valuable insights, they are also prone to hallucinations and may produce incorrect, misleading, or unsafe recommendations. To ensure safe and sensible decision-making under uncertainty, high-level decisions must be governed by cognitive guardrails. This article presents the design, simulation, and real-world integration of these guardrails for sUAS swarms in search-and-rescue missions.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles</title>
<link>https://arxiv.org/abs/2505.23590</link>
<guid>https://arxiv.org/abs/2505.23590</guid>
<content:encoded><![CDATA[
<div> Keywords: rule-based reinforcement learning, multimodal large language models, jigsaw puzzles, visual tasks, generalization

Summary:
Our study focuses on rule-based visual reinforcement learning using jigsaw puzzles as an experimental framework. We found that multimodal large language models initially perform poorly on simple jigsaw puzzles but achieve high accuracy after fine-tuning, even on complex configurations. Training on jigsaw puzzles can lead to generalization to other visual tasks, with effectiveness varying based on task configurations. Models can learn and generalize with or without explicit reasoning, although they often favor direct answering. Complex reasoning patterns appear to be pre-existing rather than emergent, increasing in frequency with training and task difficulty. Reinforcement learning shows more effective generalization than Supervised Fine-Tuning, and a cold start phase with SFT can hinder subsequent RL optimization. Our research contributes to understanding rule-based visual RL and its potential in multimodal learning. <div>
arXiv:2505.23590v2 Announce Type: replace-cross 
Abstract: The application of rule-based reinforcement learning (RL) to multimodal large language models (MLLMs) introduces unique challenges and potential deviations from findings in text-only domains, particularly for perception-heavy tasks. This paper provides a comprehensive study of rule-based visual RL, using jigsaw puzzles as a structured experimental framework. Jigsaw puzzles offer inherent ground truth, adjustable difficulty, and demand complex decision-making, making them ideal for this study. Our research reveals several key findings: \textit{Firstly,} we find that MLLMs, initially performing near to random guessing on the simplest jigsaw puzzles, achieve near-perfect accuracy and generalize to complex, unseen configurations through fine-tuning. \textit{Secondly,} training on jigsaw puzzles can induce generalization to other visual tasks, with effectiveness tied to specific task configurations. \textit{Thirdly,} MLLMs can learn and generalize with or without explicit reasoning, though open-source models often favor direct answering. Consequently, even when trained for step-by-step reasoning, they can ignore the thinking process in deriving the final answer. \textit{Fourthly,} we observe that complex reasoning patterns appear to be pre-existing rather than emergent, with their frequency increasing alongside training and task difficulty. \textit{Finally,} our results demonstrate that RL exhibits more effective generalization than Supervised Fine-Tuning (SFT), and an initial SFT cold start phase can hinder subsequent RL optimization. Although these observations are based on jigsaw puzzles and may vary across other visual tasks, this research contributes a valuable piece of jigsaw to the larger puzzle of collective understanding rule-based visual RL and its potential in multimodal learning. The code is available at: https://github.com/zifuwanggg/Jigsaw-R1.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation</title>
<link>https://arxiv.org/abs/2505.23657</link>
<guid>https://arxiv.org/abs/2505.23657</guid>
<content:encoded><![CDATA[
<div> Keywords: factuality, language models, decoding, hallucinations, reinforcement learning

Summary: 
ActLCD is a new decoding strategy that aims to improve factuality in large language models (LLMs) by actively deciding when to apply contrasting layers during generation. By treating decoding as a sequential decision-making problem, ActLCD uses a reinforcement learning policy guided by a reward-aware classifier to optimize factuality beyond the token level. The method surpasses existing approaches across five benchmarks and demonstrates effectiveness in reducing hallucinations in various generation scenarios. This innovative approach addresses the limitations of current methods by actively controlling the generation process and improving factuality in long-context scenarios. ActLCD represents a promising avenue for mitigating the shortcomings of LLMs and enhancing their performance in language generation tasks. 

<br><br>Summary: <div>
arXiv:2505.23657v2 Announce Type: replace-cross 
Abstract: Recent decoding methods improve the factuality of large language models (LLMs) by refining how the next token is selected during generation. These methods typically operate at the token level, leveraging internal representations to suppress superficial patterns. Nevertheless, LLMs remain prone to hallucinations, especially over longer contexts. In this paper, we propose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy that actively decides when to apply contrasting layers during generation. By casting decoding as a sequential decision-making problem, ActLCD employs a reinforcement learning policy guided by a reward-aware classifier to optimize factuality beyond the token level. Our experiments demonstrate that ActLCD surpasses state-of-the-art methods across five benchmarks, showcasing its effectiveness in mitigating hallucinations in diverse generation scenarios.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents</title>
<link>https://arxiv.org/abs/2505.23671</link>
<guid>https://arxiv.org/abs/2505.23671</guid>
<content:encoded><![CDATA[
<div> benchmark, language models, high-performance software, optimization tasks, codebases

Summary:
The article introduces GSO, a benchmark for evaluating language models' performance in developing high-performance software. It outlines an automated pipeline that generates and executes performance tests on codebases to identify challenging optimization tasks. An agent is tasked with improving runtime efficiency based on expert developer optimizations. The quantitative evaluation shows leading SWE-Agents struggle with less than a 5% success rate, even with inference-time scaling. Key failure modes include difficulties with low-level languages, lazy optimization strategies, and challenges in localizing bottlenecks accurately. The code and artifacts of the benchmark, along with agent trajectories, are released for future research.<br><br>Summary: <div>
arXiv:2505.23671v2 Announce Type: replace-cross 
Abstract: Developing high-performance software is a complex task that requires specialized expertise. We introduce GSO, a benchmark for evaluating language models' capabilities in developing high-performance software. We develop an automated pipeline that generates and executes performance tests to analyze repository commit histories to identify 102 challenging optimization tasks across 10 codebases, spanning diverse domains and programming languages. An agent is provided with a codebase and performance test as a precise specification, and tasked to improve the runtime efficiency, which is measured against the expert developer optimization. Our quantitative evaluation reveals that leading SWE-Agents struggle significantly, achieving less than 5% success rate, with limited improvements even with inference-time scaling. Our qualitative analysis identifies key failure modes, including difficulties with low-level languages, practicing lazy optimization strategies, and challenges in accurately localizing bottlenecks. We release the code and artifacts of our benchmark along with agent trajectories to enable future research.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time</title>
<link>https://arxiv.org/abs/2505.23729</link>
<guid>https://arxiv.org/abs/2505.23729</guid>
<content:encoded><![CDATA[
<div> Framework, Satisficing, Alignment, Language Models, Inference Time
<br>
Summary: 
The study introduces SITAlign, a framework for aligning large language models with humans that considers the multifaceted nature of preference feedback. It addresses human decision making's bounded rationality by maximizing a primary objective while satisfying threshold-based constraints on secondary criteria. The approach aims to optimize helpfulness while ensuring harmlessness, reflecting how humans make decisions through satisficing strategies. The theoretical analysis provides sub-optimality bounds for the proposed satisficing-based inference alignment approach. Empirical validation on various benchmarks demonstrates the effectiveness of SITAlign, outperforming state-of-the-art multi-objective decoding strategies by significant margins. For example, on the PKU-SafeRLHF dataset, SITAlign improves the GPT-4 win-tie rate for helpfulness reward by 22.3% while meeting the harmlessness threshold. <div>
arXiv:2505.23729v2 Announce Type: replace-cross 
Abstract: Aligning large language models with humans is challenging due to the inherently multifaceted nature of preference feedback. While existing approaches typically frame this as a multi-objective optimization problem, they often overlook how humans actually make decisions. Research on bounded rationality suggests that human decision making follows satisficing strategies-optimizing primary objectives while ensuring others meet acceptable thresholds. To bridge this gap and operationalize the notion of satisficing alignment, we propose SITAlign: an inference time framework that addresses the multifaceted nature of alignment by maximizing a primary objective while satisfying threshold-based constraints on secondary criteria. We provide theoretical insights by deriving sub-optimality bounds of our satisficing based inference alignment approach. We empirically validate SITAlign's performance through extensive experimentation on multiple benchmarks. For instance, on the PKU-SafeRLHF dataset with the primary objective of maximizing helpfulness while ensuring a threshold on harmlessness, SITAlign outperforms the state-of-the-art multi objective decoding strategy by a margin of 22.3% in terms of GPT-4 win-tie rate for helpfulness reward while adhering to the threshold on harmlessness.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>

<item>
<title>Toward Knowledge-Guided AI for Inverse Design in Manufacturing: A Perspective on Domain, Physics, and Human-AI Synergy</title>
<link>https://arxiv.org/abs/2506.00056</link>
<guid>https://arxiv.org/abs/2506.00056</guid>
<content:encoded><![CDATA[
<div> sampling strategies, physics-informed learning, human-AI interfaces, domain knowledge, inverse design

Summary:
Expert-guided sampling strategies and physics-informed machine learning are proposed to enhance data efficiency and model generalization in inverse design for manufacturing. These approaches aim to address challenges such as sparse data, high-dimensional design spaces, and physical constraints. Additionally, the integration of domain knowledge and intuitive human-AI interfaces is emphasized to create a unified ecosystem for scalable and accessible AI-driven design systems. Large language models are highlighted as interactive design agents that connect user intent with simulation tools, optimization pipelines, and collaborative workflows. The perspective argues for a new generation of design systems that transcend black-box modeling, emphasizing the importance of domain knowledge, physical priors, and adaptive reasoning in enabling high-performance discovery in materials, products, and processes. <div>
arXiv:2506.00056v1 Announce Type: new 
Abstract: Artificial intelligence (AI) is reshaping inverse design across manufacturing domain, enabling high-performance discovery in materials, products, and processes. However, purely data-driven approaches often struggle in realistic settings characterized by sparse data, high-dimensional design spaces, and nontrivial physical constraints. This perspective argues for a new generation of design systems that transcend black-box modeling by integrating domain knowledge, physics-informed learning, and intuitive human-AI interfaces. We first demonstrate how expert-guided sampling strategies enhance data efficiency and model generalization. Next, we discuss how physics-informed machine learning enables physically consistent modeling in data-scarce regimes. Finally, we explore how large language models emerge as interactive design agents connecting user intent with simulation tools, optimization pipelines, and collaborative workflows. Through illustrative examples and conceptual frameworks, we advocate that inverse design in manufacturing should evolve into a unified ecosystem, where domain knowledge, physical priors, and adaptive reasoning collectively enable scalable, interpretable, and accessible AI-driven design systems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Automated but Risky Game: Modeling Agent-to-Agent Negotiations and Transactions in Consumer Markets</title>
<link>https://arxiv.org/abs/2506.00073</link>
<guid>https://arxiv.org/abs/2506.00073</guid>
<content:encoded><![CDATA[
<div> AI agents, negotiation, transactions, consumer markets, risks <br />
<br />
Summary: 
AI agents are increasingly being used in consumer-facing applications for tasks like negotiation and transactions. This study explores a scenario where AI agents fully automate deal-making for consumers and merchants. The research aims to determine if different agents perform differently in securing favorable deals and assess the risks of fully automating transactions with AI agents. The experiment finds that AI-mediated deal-making is imbalanced, with various agents producing significantly different outcomes. Behavioral anomalies in AI agents can lead to financial losses for both consumers and merchants, including overspending or accepting unreasonable deals. This study highlights the efficiency benefits of automation but cautions users to be wary when delegating business decisions to AI agents. <br /> <div>
arXiv:2506.00073v1 Announce Type: new 
Abstract: AI agents are increasingly used in consumer-facing applications to assist with tasks such as product search, negotiation, and transaction execution. In this paper, we explore a future scenario where both consumers and merchants authorize AI agents to fully automate negotiations and transactions. We aim to answer two key questions: (1) Do different LLM agents vary in their ability to secure favorable deals for users? (2) What risks arise from fully automating deal-making with AI agents in consumer markets? To address these questions, we develop an experimental framework that evaluates the performance of various LLM agents in real-world negotiation and transaction settings. Our findings reveal that AI-mediated deal-making is an inherently imbalanced game -- different agents achieve significantly different outcomes for their users. Moreover, behavioral anomalies in LLMs can result in financial losses for both consumers and merchants, such as overspending or accepting unreasonable deals. These results underscore that while automation can improve efficiency, it also introduces substantial risks. Users should exercise caution when delegating business decisions to AI agents.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Balancing Profit and Fairness in Risk-Based Pricing Markets</title>
<link>https://arxiv.org/abs/2506.00140</link>
<guid>https://arxiv.org/abs/2506.00140</guid>
<content:encoded><![CDATA[
<div> pricing, risk-based, regulation, fairness, AI-assisted 

Summary: 
Dynamic, risk-based pricing in essential markets like health insurance and consumer credit can lead to exclusion of vulnerable consumer groups. The article proposes a regulator aligning private incentives with social objectives through a learned, interpretable tax schedule. A formal proposition suggests firm-level penalties based on demographic disparities to address global opt-out disparities. Using an AI-based simulator called MarketSim, a reinforcement learning social planner is trained to select a fairness-tax close to a simple linear prior. The learned policy increases demand-fairness by up to 16%, outperforming unregulated markets and fixed linear schedules in terms of social welfare in U.S. health insurance and consumer credit markets. This approach demonstrates how AI-assisted regulation can create a win-win equilibrium in competitive markets, offering a transparent and practical framework for fairness-aware market oversight. 

<br /><br />Summary: <div>
arXiv:2506.00140v1 Announce Type: new 
Abstract: Dynamic, risk-based pricing can systematically exclude vulnerable consumer groups from essential resources such as health insurance and consumer credit. We show that a regulator can realign private incentives with social objectives through a learned, interpretable tax schedule. First, we provide a formal proposition that bounding each firm's \emph{local} demographic gap implicitly bounds the \emph{global} opt-out disparity, motivating firm-level penalties. Building on this insight we introduce \texttt{MarketSim} -- an open-source, scalable simulator of heterogeneous consumers and profit-maximizing firms -- and train a reinforcement learning (RL) social planner (SP) that selects a bracketed fairness-tax while remaining close to a simple linear prior via an $\mathcal{L}_1$ regularizer. The learned policy is thus both transparent and easily interpretable. In two empirically calibrated markets, i.e., U.S. health-insurance and consumer-credit, our planner simultaneously raises demand-fairness by up to $16\%$ relative to unregulated Free Market while outperforming a fixed linear schedule in terms of social welfare without explicit coordination. These results illustrate how AI-assisted regulation can convert a competitive social dilemma into a win-win equilibrium, providing a principled and practical framework for fairness-aware market oversight.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Utilizing AI for Aviation Post-Accident Analysis Classification</title>
<link>https://arxiv.org/abs/2506.00169</link>
<guid>https://arxiv.org/abs/2506.00169</guid>
<content:encoded><![CDATA[
<div> Keywords: Aviation safety, Artificial Intelligence, Natural Language Processing, Deep Learning, Topic Modeling

Summary: 
This paper explores the use of Artificial Intelligence and Natural Language Processing to automate the analysis of aviation safety reports. It focuses on classifying the level of damage to aircraft and identifying the phase of flight during safety occurrences. The application of deep learning models and Topic Modeling to datasets from NTSB, ATSB, and ASN shows significant improvements in efficiency and accuracy. By uncovering thematic structures in incident reports, recurring patterns are identified for potential safety improvement. The impact of dataset size and source on analysis accuracy is discussed. The findings indicate that NLP, deep learning, and TM can enhance aviation safety analysis, leading to more proactive safety management and risk mitigation strategies.<br /><br />Summary: <div>
arXiv:2506.00169v1 Announce Type: new 
Abstract: The volume of textual data available in aviation safety reports presents a challenge for timely and accurate analysis. This paper examines how Artificial Intelligence (AI) and, specifically, Natural Language Processing (NLP) can automate the process of extracting valuable insights from this data, ultimately enhancing aviation safety. The paper reviews ongoing efforts focused on the application of NLP and deep learning to aviation safety reports, with the goal of classifying the level of damage to an aircraft and identifying the phase of flight during which safety occurrences happen. Additionally, the paper explores the use of Topic Modeling (TM) to uncover latent thematic structures within aviation incident reports, aiming to identify recurring patterns and potential areas for safety improvement. The paper compares and contrasts the performance of various deep learning models and TM techniques applied to datasets from the National Transportation Safety Board (NTSB) and the Australian Transport Safety Bureau (ATSB), as well as the Aviation Safety Network (ASN), discussing the impact of dataset size and source on the accuracy of the analysis. The findings demonstrate that both NLP and deep learning, as well as TM, can significantly improve the efficiency and accuracy of aviation safety analysis, paving the way for more proactive safety management and risk mitigation strategies.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tournament of Prompts: Evolving LLM Instructions Through Structured Debates and Elo Ratings</title>
<link>https://arxiv.org/abs/2506.00178</link>
<guid>https://arxiv.org/abs/2506.00178</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Prompt Engineering, DEEVO, Evolutionary Optimization, Elo Ratings

Summary:
DEEVO is a novel framework for optimizing prompts in Large Language Models (LLMs) that addresses the challenges of subjective quality assessment tasks. By using a debate-driven evaluation with Elo-based selection, DEEVO enables exploration of the prompt space while maintaining semantic coherence. This approach incorporates feedback from successful and unsuccessful prompts to drive improvement and preserve diversity in the prompt population. Experimental results show that DEEVO outperforms manual prompt engineering and other optimization methods on both open-ended and close-ended tasks without requiring ground truth feedback. By connecting LLMs' reasoning capabilities with adaptive optimization, DEEVO advances prompt optimization research by eliminating the need for predetermined metrics to improve AI systems.

<br /><br />Summary: DEEVO is a framework for prompt optimization in LLMs that uses debate-driven evaluation and Elo ratings to improve prompts for complex tasks. It outperforms manual prompt engineering and other methods, demonstrating its effectiveness in enhancing AI systems without relying on predefined metrics. <div>
arXiv:2506.00178v1 Announce Type: new 
Abstract: Prompt engineering represents a critical bottleneck to harness the full potential of Large Language Models (LLMs) for solving complex tasks, as it requires specialized expertise, significant trial-and-error, and manual intervention. This challenge is particularly pronounced for tasks involving subjective quality assessment, where defining explicit optimization objectives becomes fundamentally problematic. Existing automated prompt optimization methods falter in these scenarios, as they typically require well-defined task-specific numerical fitness functions or rely on generic templates that cannot capture the nuanced requirements of complex use cases. We introduce DEEVO (DEbate-driven EVOlutionary prompt optimization), a novel framework that guides prompt evolution through a debate-driven evaluation with an Elo-based selection. Contrary to prior work, DEEVOs approach enables exploration of the discrete prompt space while preserving semantic coherence through intelligent crossover and strategic mutation operations that incorporate debate-based feedback, combining elements from both successful and unsuccessful prompts based on identified strengths rather than arbitrary splicing. Using Elo ratings as a fitness proxy, DEEVO simultaneously drives improvement and preserves valuable diversity in the prompt population. Experimental results demonstrate that DEEVO significantly outperforms both manual prompt engineering and alternative state-of-the-art optimization approaches on open-ended tasks and close-ended tasks despite using no ground truth feedback. By connecting LLMs reasoning capabilities with adaptive optimization, DEEVO represents a significant advancement in prompt optimization research by eliminating the need of predetermined metrics to continuously improve AI systems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Control-R: Towards controllable test-time scaling</title>
<link>https://arxiv.org/abs/2506.00189</link>
<guid>https://arxiv.org/abs/2506.00189</guid>
<content:encoded><![CDATA[
<div> Keywords: Long CoT reasoning, Reasoning Control Fields, Control-R-4K dataset, Conditional Distillation Finetuning, test-time scaling reasoning

Summary: 
This paper introduces Reasoning Control Fields (RCF) to address challenges of underthinking and overthinking in Large Reasoning Models (LRMs) during long chain-of-thought (CoT) reasoning. RCF guides reasoning efforts through structured control signals, allowing models to adjust their reasoning process based on given conditions. The Control-R-4K dataset is presented, containing complex problems with detailed reasoning processes and control fields. The Conditional Distillation Finetuning (CDF) method is proposed to train models, particularly Control-R-32B, to effectively adjust reasoning effort at test time. Experimental results on benchmarks such as AIME2024 and MATH500 show state-of-the-art performance at the 32B scale while enabling controllable Long CoT reasoning (L-CoT). This work introduces a paradigm for controllable test-time scaling reasoning.<br /><br />Summary: <div>
arXiv:2506.00189v1 Announce Type: new 
Abstract: This paper target in addressing the challenges of underthinking and overthinking in long chain-of-thought (CoT) reasoning for Large Reasoning Models (LRMs) by introducing Reasoning Control Fields (RCF)--a novel test-time approach that injects structured control signals to guide reasoning from a tree search perspective. RCF enables models to adjust reasoning effort according to given control conditions when solving complex tasks. Additionally, we present the Control-R-4K dataset, which consists of challenging problems annotated with detailed reasoning processes and corresponding control fields. To further enhance reasoning control, we propose a Conditional Distillation Finetuning (CDF) method, which trains model--particularly Control-R-32B--to effectively adjust reasoning effort during test time. Experimental results on benchmarks such as AIME2024 and MATH500 demonstrate that our approach achieves state-of-the-art performance at the 32B scale while enabling a controllable Long CoT reasoning process (L-CoT). Overall, this work introduces an effective paradigm for controllable test-time scaling reasoning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What do professional software developers need to know to succeed in an age of Artificial Intelligence?</title>
<link>https://arxiv.org/abs/2506.00202</link>
<guid>https://arxiv.org/abs/2506.00202</guid>
<content:encoded><![CDATA[
<div> developers, AI, skills, knowledge, workforce disruption, productivity gains, task workflow, reskilling, upskilling, on-the-job learning<br />
<br />
Summary: <br />
Generative AI is being used by developers to increase productivity, but concerns about workforce disruption and deskilling persist. Research with 21 developers revealed 12 work goals, 75 tasks, and the necessary skills and knowledge in four domains: using Generative AI effectively, core software engineering, adjacent engineering, and adjacent non-engineering. Five insights were distilled, highlighting the importance of reskilling and upskilling developers for the AI era. The skills and knowledge required for success in this field are crucial for future-proofing developers and safeguarding against deskilling. On-the-job learning initiatives and computer science degree programs will need to focus on both technical and soft skills across all four domains to prepare developers for the challenges and opportunities presented by AI. <div>
arXiv:2506.00202v1 Announce Type: new 
Abstract: Generative AI is showing early evidence of productivity gains for software developers, but concerns persist regarding workforce disruption and deskilling. We describe our research with 21 developers at the cutting edge of using AI, summarizing 12 of their work goals we uncovered, together with 75 associated tasks and the skills & knowledge for each, illustrating how developers use AI at work. From all of these, we distilled our findings in the form of 5 insights. We found that the skills & knowledge to be a successful AI-enhanced developer are organized into four domains (using Generative AI effectively, core software engineering, adjacent engineering, and adjacent non-engineering) deployed at critical junctures throughout a 6-step task workflow. In order to "future proof" developers for this age of AI, on-the-job learning initiatives and computer science degree programs will need to target both "soft" skills and the technical skills & knowledge in all four domains to reskill, upskill and safeguard against deskilling.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ethical AI: Towards Defining a Collective Evaluation Framework</title>
<link>https://arxiv.org/abs/2506.00233</link>
<guid>https://arxiv.org/abs/2506.00233</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Ethical concerns, Transparent, FAIR principles, Ontological blocks<br />
<br />
Summary: <br />
Artificial Intelligence (AI) is revolutionizing various industries, but its rapid adoption raises ethical issues surrounding data ownership, privacy, and bias. To address these concerns, a modular ethical assessment framework based on ontological blocks is proposed. These blocks encode ethical principles such as fairness, accountability, and ownership, and are integrated with FAIR principles to enable scalable and transparent ethical evaluations. The framework supports compliance with the EU AI Act and facilitates dynamic risk classification in real-world applications, like AI-powered investor profiling. While ontological blocks show promise in enhancing AI ethics through explainability and auditability, challenges remain in automating and incorporating probabilistic reasoning. The framework provides a foundation for transparent and accountable AI systems in high-stakes domains. <div>
arXiv:2506.00233v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) is transforming sectors such as healthcare, finance, and autonomous systems, offering powerful tools for innovation. Yet its rapid integration raises urgent ethical concerns related to data ownership, privacy, and systemic bias. Issues like opaque decision-making, misleading outputs, and unfair treatment in high-stakes domains underscore the need for transparent and accountable AI systems. This article addresses these challenges by proposing a modular ethical assessment framework built on ontological blocks of meaning-discrete, interpretable units that encode ethical principles such as fairness, accountability, and ownership. By integrating these blocks with FAIR (Findable, Accessible, Interoperable, Reusable) principles, the framework supports scalable, transparent, and legally aligned ethical evaluations, including compliance with the EU AI Act. Using a real-world use case in AI-powered investor profiling, the paper demonstrates how the framework enables dynamic, behavior-informed risk classification. The findings suggest that ontological blocks offer a promising path toward explainable and auditable AI ethics, though challenges remain in automation and probabilistic reasoning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMELLNET: A Large-scale Dataset for Real-world Smell Recognition</title>
<link>https://arxiv.org/abs/2506.00239</link>
<guid>https://arxiv.org/abs/2506.00239</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, smell detection, SmellNet, chemical sensors, classification <br />
Summary: <br />
The article introduces the concept of using AI to detect and identify substances based on their smell, showcasing its potential in allergen detection, manufacturing process monitoring, and emotional state sensing. A new large-scale database called SmellNet is created, containing data on various substances found in nuts, spices, herbs, fruits, and vegetables. AI models trained using SmellNet show promising results in real-time substance classification based on smell alone. The models achieve up to 65.35% accuracy on pre-recorded data and demonstrate generalization to real-world conditions with significant accuracies on nuts and spices in a 50-way classification task. The study also highlights technical challenges in building AI for smell, including the need for richer feature learning, on-edge smell models, and robustness to environmental changes. Overall, the research emphasizes the potential of AI in the field of smell detection and highlights avenues for future advancements. <br /> <div>
arXiv:2506.00239v1 Announce Type: new 
Abstract: The ability of AI to sense and identify various substances based on their smell alone can have profound impacts on allergen detection (e.g., smelling gluten or peanuts in a cake), monitoring the manufacturing process, and sensing hormones that indicate emotional states, stress levels, and diseases. Despite these broad impacts, there are virtually no large scale benchmarks, and therefore little progress, for training and evaluating AI systems' ability to smell in the real world. In this paper, we use portable gas and chemical sensors to create SmellNet, the first large-scale database that digitizes a diverse range of smells in the natural world. SmellNet contains about 180,000 time steps of 50 substances (spanning nuts, spices, herbs, fruits, and vegetables) with 50 hours of data. Using SmellNet, we train AI models for real-time classification of substances based on their smell alone. Our best methods leverage sequence models, contrastive learning to integrate high-resolution Gas Chromatography-Mass Spectrometry molecular data, and a new temporal difference method that identifies sharp changes in sensor readings. Our best models achieve up to 65.35% accuracy on pre-recorded data, and generalize to real-world conditions with 10.71% accuracy on nuts and 25.38% on spices in the challenging 50-way online classification task. Despite these promising results, SmellNet highlights many technical challenges in building AI for smell, including richer feature learning, on-edge smell models, and robustness to environmental changes.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Whispers of Many Shores: Cultural Alignment through Collaborative Cultural Expertise</title>
<link>https://arxiv.org/abs/2506.00242</link>
<guid>https://arxiv.org/abs/2506.00242</guid>
<content:encoded><![CDATA[
<div> framework, cultural sensitivity, language models, soft prompt, cultural alignment 

Summary: 
The research introduces a soft prompt fine-tuning framework to enhance cultural sensitivity in large language models (LLMs) for global applications. Current LLMs often lack nuanced understanding of diverse cultural contexts, requiring costly full fine-tuning for adaptation. The framework utilizes vectorized prompt tuning to route queries dynamically to culturally specialized 'expert' LLM configurations without altering the base model's parameters. Through optimizing soft prompt embeddings, the framework significantly improves alignment scores from 0.208 to 0.820, offering an efficient and modular solution for culturally-aware LLM deployment. This approach advances cultural coverage and expert adaptation in LLMs, leading to autonomous AI with nuanced understanding in a globally interconnected world. 

<br /><br />Summary: <div>
arXiv:2506.00242v1 Announce Type: new 
Abstract: The integration of large language models (LLMs) into global applications necessitates effective cultural alignment for meaningful and culturally-sensitive interactions. Current LLMs often lack the nuanced understanding required for diverse cultural contexts, and adapting them typically involves costly full fine-tuning. To address this, we introduce a novel soft prompt fine-tuning framework that enables efficient and modular cultural alignment. Our method utilizes vectorized prompt tuning to dynamically route queries to a committee of culturally specialized 'expert' LLM configurations, created by optimizing soft prompt embeddings without altering the base model's parameters. Extensive experiments demonstrate that our framework significantly enhances cultural sensitivity and adaptability, improving alignment scores from 0.208 to 0.820, offering a robust solution for culturally-aware LLM deployment. This research paves the way for subsequent investigations into enhanced cultural coverage and dynamic expert adaptation, crucial for realizing autonomous AI with deeply nuanced understanding in a globally interconnected world.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIR: Methodology Inspiration Retrieval for Scientific Research Problems</title>
<link>https://arxiv.org/abs/2506.00249</link>
<guid>https://arxiv.org/abs/2506.00249</guid>
<content:encoded><![CDATA[
<div> Methodology Inspiration Retrieval, Large Language Models, Scientific Discovery, Methodology Adjacency Graph, Automated Scientific Discovery
<br />
<br />
Summary: 
The paper addresses the challenge of Methodology Inspiration Retrieval (MIR) by using Large Language Models (LLMs) to accelerate scientific discovery. A novel dataset is introduced for training and evaluating retrievers on MIR, along with establishing baselines. The Methodology Adjacency Graph (MAG) is constructed to capture methodological lineage, allowing for the identification of patterns of methodological inspiration. This approach improves recall and Mean Average Precision (mAP) significantly over strong baselines, with additional enhancements achieved through LLM-based re-ranking strategies. Extensive ablation studies and qualitative analyses demonstrate the potential of MIR in automated scientific discovery and suggest avenues for further advancement in inspiration-driven retrieval. 
<br /> <div>
arXiv:2506.00249v1 Announce Type: new 
Abstract: There has been a surge of interest in harnessing the reasoning capabilities of Large Language Models (LLMs) to accelerate scientific discovery. While existing approaches rely on grounding the discovery process within the relevant literature, effectiveness varies significantly with the quality and nature of the retrieved literature. We address the challenge of retrieving prior work whose concepts can inspire solutions for a given research problem, a task we define as Methodology Inspiration Retrieval (MIR). We construct a novel dataset tailored for training and evaluating retrievers on MIR, and establish baselines. To address MIR, we build the Methodology Adjacency Graph (MAG); capturing methodological lineage through citation relationships. We leverage MAG to embed an "intuitive prior" into dense retrievers for identifying patterns of methodological inspiration beyond superficial semantic similarity. This achieves significant gains of +5.4 in Recall@3 and +7.8 in Mean Average Precision (mAP) over strong baselines. Further, we adapt LLM-based re-ranking strategies to MIR, yielding additional improvements of +4.5 in Recall@3 and +4.8 in mAP. Through extensive ablation studies and qualitative analyses, we exhibit the promise of MIR in enhancing automated scientific discovery and outline avenues for advancing inspiration-driven retrieval.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hidden in Plain Sight: Probing Implicit Reasoning in Multimodal Language Models</title>
<link>https://arxiv.org/abs/2506.00258</link>
<guid>https://arxiv.org/abs/2506.00258</guid>
<content:encoded><![CDATA[
<div> implicit reasoning, multimodal large language models, diagnostic suite, performance recovery, underconstrained environments
<br />
Summary:
This paper examines how current multimodal large language models (MLLMs) handle implicit reasoning scenarios in messy, underspecified, and unreliable real-world environments. The study evaluates six MLLMs, including o3 and GPT-4o, across four categories of real-world failure modes, finding that models often fail to detect hidden issues even when possessing the necessary skills. The research reveals that simple interventions such as cautious persona prompting and requiring a clarifying question can significantly improve model performance. While MLLMs exhibit reasoning competence, they tend to prioritize user compliance over surfacing potential flaws. This study highlights the gap between the models' reasoning abilities and their behavior in underconstrained environments, offering practical strategies to enhance their trustworthiness. 
<br /> <div>
arXiv:2506.00258v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) are increasingly deployed in open-ended, real-world environments where inputs are messy, underspecified, and not always trustworthy. Unlike curated benchmarks, these settings frequently involve instructions that refer to missing objects or contradictory facts, rely on ambiguous references, or request infeasible actions. In such cases, success hinges not on task execution alone, but on a model's ability to detect when something is silently wrong. This paper presents a systematic analysis of how current MLLMs handle such implicit reasoning scenarios: cases where the flaw is not explicitly stated but must be inferred from context. Using a curated diagnostic suite spanning four categories of real-world failure modes, we evaluate six MLLMs, including o3 and GPT-4o, and find that models frequently fail to surface hidden issues, even when they possess the necessary perceptual and reasoning skills. Explicit prompting reveals that the underlying capabilities exist but are often suppressed in favor of user compliance. We further show that simple inference-time interventions, such as cautious persona prompting and, in particular, requiring a clarifying question, can dramatically recover performance. Our findings highlight a persistent gap between reasoning competence and behavioral compliance in current MLLMs and suggest practical strategies for making these models more trustworthy in underconstrained environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sleep Brain and Cardiac Activity Predict Cognitive Flexibility and Conceptual Reasoning Using Deep Learning</title>
<link>https://arxiv.org/abs/2506.00279</link>
<guid>https://arxiv.org/abs/2506.00279</guid>
<content:encoded><![CDATA[
<div> Keywords: sleep microstructure, deep learning, executive functions, cognitive performance prediction, multi-modal learning

Summary: 
This study explores the relationship between sleep microstructure and cognitive performance, focusing on executive functions like cognitive adaptability and conceptual reasoning. A new deep learning model called CogPSGFormer was introduced to analyze polysomnographic data from 817 individuals. The model combines ECG and EEG signals with extracted features to predict cognitive performance based on Penn Conditional Exclusion Test (PCET) scores. Through extensive evaluation, the CogPSGFormer model achieved 80.3% accuracy in classifying individuals into low and high cognitive performance groups. The results demonstrate the effectiveness of multi-scale feature extraction and multi-modal learning in utilizing sleep-derived signals for cognitive performance prediction. The code for the model is publicly available for reproducibility at https://github.com/boshrakh95/CogPSGFormer.git. <br /><br />Summary: <div>
arXiv:2506.00279v1 Announce Type: new 
Abstract: Despite extensive research on the relationship between sleep and cognition, the connection between sleep microstructure and human performance across specific cognitive domains remains underexplored. This study investigates whether deep learning models can predict executive functions, particularly cognitive adaptability and conceptual reasoning from physiological processes during a night's sleep. To address this, we introduce CogPSGFormer, a multi-scale convolutional-transformer model designed to process multi-modal polysomnographic data. This model integrates one-channel ECG and EEG signals along with extracted features, including EEG power bands and heart rate variability parameters, to capture complementary information across modalities. A thorough evaluation of the CogPSGFormer architecture was conducted to optimize the processing of extended sleep signals and identify the most effective configuration. The proposed framework was evaluated on 817 individuals from the STAGES dataset using cross-validation. The model achieved 80.3\% accuracy in classifying individuals into low vs. high cognitive performance groups on unseen data based on Penn Conditional Exclusion Test (PCET) scores. These findings highlight the effectiveness of our multi-scale feature extraction and multi-modal learning approach in leveraging sleep-derived signals for cognitive performance prediction. To facilitate reproducibility, our code is publicly accessible (https://github.com/boshrakh95/CogPSGFormer.git).
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of LLMs for mathematical problem solving</title>
<link>https://arxiv.org/abs/2506.00309</link>
<guid>https://arxiv.org/abs/2506.00309</guid>
<content:encoded><![CDATA[
<div> mathematical models, large language models, mathematics datasets, structured chain of thought, error analysis

Summary:<br />
- Large Language Models (LLMs) like GPT-4o, DeepSeek-V3, and Gemini-2.0 were compared on three different mathematics datasets using the Structured Chain-of-Thought (SCoT) framework.
- GPT-4o showed stable performance across all datasets, excelling in high-level questions of the UNSW dataset.
- DeepSeek-V3 performed well in well-structured domains but lacked accuracy in statistical inference tasks.
- Gemini-2.0 demonstrated strong linguistic understanding but struggled in multi-step reasoning and symbolic logic problems.
- Error analysis revealed specific deficits in each model: GPT-4o lacked precision at times, DeepSeek-V3 omitted intermediate steps, and Gemini-2.0 had limitations in higher-dimensional mathematical reasoning. 

<br /><br />Summary: <div>
arXiv:2506.00309v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown impressive performance on a range of educational tasks, but are still understudied for their potential to solve mathematical problems. In this study, we compare three prominent LLMs, including GPT-4o, DeepSeek-V3, and Gemini-2.0, on three mathematics datasets of varying complexities (GSM8K, MATH500, and UNSW datasets). We take a five-dimensional approach based on the Structured Chain-of-Thought (SCoT) framework to assess final answer correctness, step completeness, step validity, intermediate calculation accuracy, and problem comprehension. The results show that GPT-4o is the most stable and consistent in performance across all the datasets, but particularly it performs outstandingly in high-level questions of the UNSW dataset. DeepSeek-V3 is competitively strong in well-structured domains such as optimisation, but suffers from fluctuations in accuracy in statistical inference tasks. Gemini-2.0 shows strong linguistic understanding and clarity in well-structured problems but performs poorly in multi-step reasoning and symbolic logic. Our error analysis reveals particular deficits in each model: GPT-4o is at times lacking in sufficient explanation or precision; DeepSeek-V3 leaves out intermediate steps; and Gemini-2.0 is less flexible in mathematical reasoning in higher dimensions.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in AI Agents</title>
<link>https://arxiv.org/abs/2506.00320</link>
<guid>https://arxiv.org/abs/2506.00320</guid>
<content:encoded><![CDATA[
<div> modeling, planning, imitation learning, reasoning, AI agent

Summary:
- The study introduces Dyna-Think, a framework that combines planning, internal world modeling, reasoning, and acting to enhance AI agent performance.
- Dyna-Think Imitation Learning (DIT) reconstructs the thinking process of R1 to focus on relevant world model simulation and trains the policy using this data.
- Dyna-Think Dyna Training (DDT) utilizes a two-stage training process to enhance the agent's world modeling ability and action performance.
- Evaluation on OSWorld shows that Dyna-Think improves in-domain and out-of-domain performance while using fewer tokens compared to R1.
- Critique generation for world model training proves effective in enhancing policy performance, and better AI agent performance is linked to improved world modeling abilities.

<br /><br />Summary: <div>
arXiv:2506.00320v1 Announce Type: new 
Abstract: Recent progress in reasoning with large language models (LLMs), such as DeepSeek-R1, demonstrates impressive capabilities in domains like mathematics and coding, by exhibiting complex cognitive behaviors such as verification, goal decomposition, and self-reflection. However, it is unclear what behavior is effective and what behavior is missing for long-horizon AI agents tasks. In this work, we propose Dyna-Think, a thinking framework that integrates planning with an internal world model with reasoning and acting to enhance AI agent performance. To enable Dyna-Think, we propose Dyna-Think Imitation Learning (DIT) and Dyna-Think Dyna Training (DDT). To initialize a policy with Dyna-Think, DIT reconstructs the thinking process of R1 to focus on performing world model simulation relevant to the proposed (and planned) action, and trains the policy using this reconstructed data. To enhance Dyna-Think, DDT uses a two-stage training process to first improve the agent's world modeling ability via objectives such as state prediction or critique generation, and then improve the agent's action via policy training. We evaluate our methods on OSWorld, and demonstrate that Dyna-Think improves the agent's in-domain and out-of-domain performance, achieving similar best-of-n performance compared to R1 while generating 2x less tokens on average. Our extensive empirical studies reveal that 1) using critique generation for world model training is effective to improve policy performance; and 2) AI agents with better performance correlate with better world modeling abilities. We believe our results suggest a promising research direction to integrate world model simulation into AI agents to enhance their reasoning, planning, and acting capabilities.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BASIL: Best-Action Symbolic Interpretable Learning for Evolving Compact RL Policies</title>
<link>https://arxiv.org/abs/2506.00328</link>
<guid>https://arxiv.org/abs/2506.00328</guid>
<content:encoded><![CDATA[
<div> Interpretable reinforcement learning, BASIL, symbolic policies, quality-diversity optimization, evolutionary search<br />
Summary:<br />
The article introduces BASIL, a method for generating interpretable symbolic policies through online evolutionary search. Policies are represented as symbolic predicates over state variables, ensuring interpretability and tractable complexity. The use of a quality-diversity archive encourages diversity among top solutions, while complexity-aware fitness promotes compact representations. Exact constraints and adaptability options allow for transparent yet expressive policies. Empirical comparisons on benchmark tasks show BASIL synthesizes compact, interpretable controllers comparable to deep reinforcement learning. This approach combines symbolic expressiveness, evolutionary diversity, and online learning in a unified framework. <br /> <div>
arXiv:2506.00328v1 Announce Type: new 
Abstract: The quest for interpretable reinforcement learning is a grand challenge for the deployment of autonomous decision-making systems in safety-critical applications. Modern deep reinforcement learning approaches, while powerful, tend to produce opaque policies that compromise verification, reduce transparency, and impede human oversight. To address this, we introduce BASIL (Best-Action Symbolic Interpretable Learning), a systematic approach for generating symbolic, rule-based policies via online evolutionary search with quality-diversity (QD) optimization. BASIL represents policies as ordered lists of symbolic predicates over state variables, ensuring full interpretability and tractable policy complexity. By using a QD archive, the methodology in the proposed study encourages behavioral and structural diversity between top-performing solutions, while a complexity-aware fitness encourages the synthesis of compact representations. The evolutionary system supports the use of exact constraints for rule count and system adaptability for balancing transparency with expressiveness. Empirical comparisons with three benchmark tasks CartPole-v1, MountainCar-v0, and Acrobot-v1 show that BASIL consistently synthesizes interpretable controllers with compact representations comparable to deep reinforcement learning baselines. Herein, this article introduces a new interpretable policy synthesis method that combines symbolic expressiveness, evolutionary diversity, and online learning through a unifying framework.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Olfaction Standardization is Essential for the Advancement of Embodied Artificial Intelligence</title>
<link>https://arxiv.org/abs/2506.00398</link>
<guid>https://arxiv.org/abs/2506.00398</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, olfaction, machine learning, neuroscience, ethics

Summary:<br /><br />
The article highlights the lack of attention given to olfaction in artificial intelligence (AI) research despite its importance as a high-bandwidth, evolutionarily critical sense. The exclusion of olfactory perception from AI architectures is attributed to unresolved scientific theories of smell, heterogeneous sensor technologies, lack of standardized olfactory datasets, absence of AI-oriented benchmarks, and challenges in evaluating sub-perceptual signal processing. The authors argue that integrating olfaction into AI is crucial for developing truly embodied and ethically aligned super-human intelligence. Cross-disciplinary collaboration among neuroscience, robotics, machine learning, and ethics is called for to establish olfactory benchmarks, create multimodal datasets, and define the sensory capabilities required for machines to understand and interact with human environments. Recognizing olfaction as a core modality is seen as essential for building AI systems that encompass the full scope of the human experience. <div>
arXiv:2506.00398v1 Announce Type: new 
Abstract: Despite extraordinary progress in artificial intelligence (AI), modern systems remain incomplete representations of human cognition. Vision, audition, and language have received disproportionate attention due to well-defined benchmarks, standardized datasets, and consensus-driven scientific foundations. In contrast, olfaction - a high-bandwidth, evolutionarily critical sense - has been largely overlooked. This omission presents a foundational gap in the construction of truly embodied and ethically aligned super-human intelligence. We argue that the exclusion of olfactory perception from AI architectures is not due to irrelevance but to structural challenges: unresolved scientific theories of smell, heterogeneous sensor technologies, lack of standardized olfactory datasets, absence of AI-oriented benchmarks, and difficulty in evaluating sub-perceptual signal processing. These obstacles have hindered the development of machine olfaction despite its tight coupling with memory, emotion, and contextual reasoning in biological systems. In this position paper, we assert that meaningful progress toward general and embodied intelligence requires serious investment in olfactory research by the AI community. We call for cross-disciplinary collaboration - spanning neuroscience, robotics, machine learning, and ethics - to formalize olfactory benchmarks, develop multimodal datasets, and define the sensory capabilities necessary for machines to understand, navigate, and act within human environments. Recognizing olfaction as a core modality is essential not only for scientific completeness, but for building AI systems that are ethically grounded in the full scope of the human experience.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>World Models for Cognitive Agents: Transforming Edge Intelligence in Future Networks</title>
<link>https://arxiv.org/abs/2506.00417</link>
<guid>https://arxiv.org/abs/2506.00417</guid>
<content:encoded><![CDATA[
<div> Keywords: world models, artificial intelligence, predictive reasoning, planning, decision-making 

Summary:
World models are a pivotal development in artificial intelligence, allowing agents to create internal representations of their environments for various tasks. These models excel in scenarios with limited data availability or critical safety considerations. The paper provides a detailed overview of world models, covering their architecture, training methods, and applications in prediction, generation, planning, and causal reasoning. It also distinguishes world models from similar concepts like digital twins and foundation models, emphasizing their unique role as autonomous agent cognitive engines. The Wireless Dreamer framework, based on world models and reinforcement learning, is proposed for optimizing wireless edge intelligence, particularly in low-altitude wireless networks. Through a weather-aware UAV trajectory planning case study, the framework showcases enhanced learning efficiency and decision-making quality. 

Summary: <br /><br /> <div>
arXiv:2506.00417v1 Announce Type: new 
Abstract: World models are emerging as a transformative paradigm in artificial intelligence, enabling agents to construct internal representations of their environments for predictive reasoning, planning, and decision-making. By learning latent dynamics, world models provide a sample-efficient framework that is especially valuable in data-constrained or safety-critical scenarios. In this paper, we present a comprehensive overview of world models, highlighting their architecture, training paradigms, and applications across prediction, generation, planning, and causal reasoning. We compare and distinguish world models from related concepts such as digital twins, the metaverse, and foundation models, clarifying their unique role as embedded cognitive engines for autonomous agents. We further propose Wireless Dreamer, a novel world model-based reinforcement learning framework tailored for wireless edge intelligence optimization, particularly in low-altitude wireless networks (LAWNs). Through a weather-aware UAV trajectory planning case study, we demonstrate the effectiveness of our framework in improving learning efficiency and decision quality.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIRROR: Cognitive Inner Monologue Between Conversational Turns for Persistent Reflection and Reasoning in Conversational LLMs</title>
<link>https://arxiv.org/abs/2506.00430</link>
<guid>https://arxiv.org/abs/2506.00430</guid>
<content:encoded><![CDATA[
<div> cognitive architecture, language models, internal reasoning, multi-turn conversation, CuRaTe benchmark

Summary:
The article introduces MIRROR, a cognitive architecture that enhances language models' capabilities through parallel reasoning inspired by human cognition. MIRROR consists of two layers: the Thinker, managing reasoning threads, and the Talker, utilizing the integrated narrative for context-aware responses. Evaluation on the CuRaTe benchmark shows up to 156% relative improvement in critical safety scenarios and an average accuracy of ~>80% across all scenarios. Models with the MIRROR architecture outperformed baseline models by 21% on average, addressing failure modes such as sycophancy and inconsistent prioritization of conflicting constraints. This work bridges cognitive science and AI by implementing modular internal reasoning, creating a persistent internal model that significantly enhances multi-turn conversation capabilities. 

<br /><br />Summary: <div>
arXiv:2506.00430v1 Announce Type: new 
Abstract: Human intelligence relies on inner monologue to process complex information through simultaneous reflection, memory retrieval, and response formulation. We introduce MIRROR (Modular Internal Reasoning, Reflection, Orchestration, and Response), a cognitive architecture that systematically implements these parallel reasoning capabilities in large language models. MIRROR operates as a unified system with two distinct functional layers: the Thinker and the Talker. The Thinker encompasses: (1) the Inner Monologue Manager, coordinating reasoning threads across cognitive dimensions (Goals, Reasoning, and Memory); and (2) the Cognitive Controller, synthesizing these threads into a coherent internal narrative maintained across conversation turns. The Talker component then leverages this integrated narrative for context-aware responses. Evaluated on the CuRaTe benchmark--testing personalized dialogue with safety-critical constraints, conflicting preferences, and multi-turn consistency--LLMs utilizing the MIRROR architecture achieve up to 156% relative improvement in critical safety scenarios involving three persons with conflicting preferences, maintaining an average accuracy of ~>80% on all scenarios. Across scenario-specific comparisons, GPT-4o, Gemini 1.5 Pro, Claude 3.7 Sonnet, Llama 4 variants, and Mistral 3 variants with the MIRROR architecture outperformed baseline models by 21% on average (15.5 percentage points absolute). MIRROR directly addresses three critical LLM failure modes: sycophancy, attentional deficits to critical information, and inconsistent prioritization of conflicting constraints. This work bridges cognitive science and AI by implementing modular internal reasoning inspired by human cognition, creating a persistent internal model that significantly enhances multi-turn conversation capabilities.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Monitoring Robustness and Individual Fairness</title>
<link>https://arxiv.org/abs/2506.00496</link>
<guid>https://arxiv.org/abs/2506.00496</guid>
<content:encoded><![CDATA[
<div> robustness, AI models, monitoring, black-box, runtime<br />
Summary:<br />
The article introduces the concept of runtime monitoring for input-output robustness of deployed black-box AI models to enhance trustworthiness. By detecting dissimilar outputs for similar inputs during model execution, monitors can identify robustness violations. The monitoring problem is framed as fixed-radius nearest neighbor (FRNN) search, with Clemont offering lightweight monitors including online variants of FRNN algorithms and a new algorithm based on binary decision diagrams. An efficient parallelization technique is demonstrated for reducing computation time in monitors measuring input-output pair distances using the $L_\infty$ norm. Comparative studies using standard benchmarks show the effectiveness of different monitors in detecting robustness violations at runtime. This approach complements existing offline robustification methods, contributing to the overall trustworthiness of AI decision-making processes. 
<br /><br />Summary: <div>
arXiv:2506.00496v1 Announce Type: new 
Abstract: Input-output robustness appears in various different forms in the literature, such as robustness of AI models to adversarial or semantic perturbations and individual fairness of AI models that make decisions about humans.
  We propose runtime monitoring of input-output robustness of deployed, black-box AI models, where the goal is to design monitors that would observe one long execution sequence of the model, and would raise an alarm whenever it is detected that two similar inputs from the past led to dissimilar outputs.
  This way, monitoring will complement existing offline ``robustification'' approaches to increase the trustworthiness of AI decision-makers.
  We show that the monitoring problem can be cast as the fixed-radius nearest neighbor (FRNN) search problem, which, despite being well-studied, lacks suitable online solutions.
  We present our tool Clemont, which offers a number of lightweight monitors, some of which use upgraded online variants of existing FRNN algorithms, and one uses a novel algorithm based on binary decision diagrams -- a data-structure commonly used in software and hardware verification.
  We have also developed an efficient parallelization technique that can substantially cut down the computation time of monitors for which the distance between input-output pairs is measured using the $L_\infty$ norm.
  Using standard benchmarks from the literature of adversarial and semantic robustness and individual fairness, we perform a comparative study of different monitors in \tool, and demonstrate their effectiveness in correctly detecting robustness violations at runtime.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CityLens: Benchmarking Large Language-Vision Models for Urban Socioeconomic Sensing</title>
<link>https://arxiv.org/abs/2506.00530</link>
<guid>https://arxiv.org/abs/2506.00530</guid>
<content:encoded><![CDATA[
<div> Keywords: CityLens, LLVMs, urban socioeconomic indicators, multi-modal dataset, benchmarking

Summary: <br /><br />CityLens is introduced as a benchmark to evaluate large language-vision models (LLVMs) in predicting urban socioeconomic indicators from satellite and street view imagery. A multi-modal dataset covering 17 global cities across 6 key domains is constructed for this purpose. 11 prediction tasks are defined, and three evaluation paradigms are utilized. 17 state-of-the-art LLVMs are benchmarked, revealing their capabilities and limitations in predicting urban socioeconomic patterns. While LLVMs show promise in perceptual and reasoning abilities, areas for improvement are identified through the benchmark. The CityLens framework aims to guide future efforts in utilizing LLVMs for understanding and predicting urban socioeconomic conditions. The codes and datasets are openly available on https://github.com/tsinghua-fib-lab/CityLens. <div>
arXiv:2506.00530v1 Announce Type: new 
Abstract: Understanding urban socioeconomic conditions through visual data is a challenging yet essential task for sustainable urban development and policy planning. In this work, we introduce $\textbf{CityLens}$, a comprehensive benchmark designed to evaluate the capabilities of large language-vision models (LLVMs) in predicting socioeconomic indicators from satellite and street view imagery. We construct a multi-modal dataset covering a total of 17 globally distributed cities, spanning 6 key domains: economy, education, crime, transport, health, and environment, reflecting the multifaceted nature of urban life. Based on this dataset, we define 11 prediction tasks and utilize three evaluation paradigms: Direct Metric Prediction, Normalized Metric Estimation, and Feature-Based Regression. We benchmark 17 state-of-the-art LLVMs across these tasks. Our results reveal that while LLVMs demonstrate promising perceptual and reasoning capabilities, they still exhibit limitations in predicting urban socioeconomic indicators. CityLens provides a unified framework for diagnosing these limitations and guiding future efforts in using LLVMs to understand and predict urban socioeconomic patterns. Our codes and datasets are open-sourced via https://github.com/tsinghua-fib-lab/CityLens.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A "Wenlu" Brain System for Multimodal Cognition and Embodied Decision-Making: A Secure New Architecture for Deep Integration of Foundation Models and Domain Knowledge</title>
<link>https://arxiv.org/abs/2506.00570</link>
<guid>https://arxiv.org/abs/2506.00570</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, language understanding, multimodal data, decision-making brain system, privacy security

Summary:
The paper introduces a new system called "Wenlu" that aims to integrate language understanding capabilities of foundation models with domain-specific knowledge bases in real-world applications. It is designed to securely fuse private knowledge and public models, process multimodal data like images and speech, and make decisions from cognition to automatic hardware-level code generation. The system utilizes a brain-inspired memory tagging and replay mechanism to integrate user-private data, industry-specific knowledge, and general-purpose language models. It offers precise multimodal services for enterprise decision support, medical analysis, autonomous driving, and robotic control. "Wenlu" demonstrates advantages in multimodal processing, privacy security, end-to-end hardware control code generation, self-learning, and sustainable updates, making it a valuable asset in building the next-generation intelligent core.<br /><br />Summary: <div>
arXiv:2506.00570v1 Announce Type: new 
Abstract: With the rapid penetration of artificial intelligence across industries and scenarios, a key challenge in building the next-generation intelligent core lies in effectively integrating the language understanding capabilities of foundation models with domain-specific knowledge bases in complex real-world applications. This paper proposes a multimodal cognition and embodied decision-making brain system, ``Wenlu", designed to enable secure fusion of private knowledge and public models, unified processing of multimodal data such as images and speech, and closed-loop decision-making from cognition to automatic generation of hardware-level code. The system introduces a brain-inspired memory tagging and replay mechanism, seamlessly integrating user-private data, industry-specific knowledge, and general-purpose language models. It provides precise and efficient multimodal services for enterprise decision support, medical analysis, autonomous driving, robotic control, and more. Compared with existing solutions, ``Wenlu" demonstrates significant advantages in multimodal processing, privacy security, end-to-end hardware control code generation, self-learning, and sustainable updates, thus laying a solid foundation for constructing the next-generation intelligent core.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning Like an Economist: Post-Training on Economic Problems Induces Strategic Generalization in LLMs</title>
<link>https://arxiv.org/abs/2506.00577</link>
<guid>https://arxiv.org/abs/2506.00577</guid>
<content:encoded><![CDATA[
<div> Supervised Fine-Tuning, Reinforcement Learning with Verifiable Rewards, Large Language Models, Multi-Agent Systems, Economic Reasoning

Summary:
Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR) are explored as post-training techniques for Large Language Models (LLMs) in Multi-Agent Systems (MAS). The study focuses on economic reasoning as a testbed due to its mathematical foundations, game theory relevance, and real-world applications like market design and policy analysis. Recon (Reasoning like an ECONomist), a 7B-parameter open-source LLM, is post-trained on a curated dataset of economic reasoning problems. Evaluation on economic reasoning benchmarks and multi-agent games demonstrates improved structured reasoning and economic rationality. This research highlights the effectiveness of domain-specific post-training in enhancing reasoning and agent alignment, emphasizing the roles of SFT and RL in shaping model behavior. The code for Recon is available on GitHub for further exploration. 

<br /><br />Summary: <div>
arXiv:2506.00577v1 Announce Type: new 
Abstract: Directly training Large Language Models (LLMs) for Multi-Agent Systems (MAS) remains challenging due to intricate reward modeling, dynamic agent interactions, and demanding generalization requirements. This paper explores whether post-training techniques, specifically Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR), can effectively $\textit{generalize}$ to multi-agent scenarios. We use economic reasoning as a testbed, leveraging its strong foundations in mathematics and game theory, its demand for structured analytical reasoning, and its relevance to real-world applications such as market design, resource allocation, and policy analysis. We introduce $\textbf{Recon}$ ($\textbf{R}$easoning like an $\textbf{ECON}$omist), a 7B-parameter open-source LLM post-trained on a hand-curated dataset of 2,100 high-quality economic reasoning problems. Comprehensive evaluation on economic reasoning benchmarks and multi-agent games reveals clear improvements in structured reasoning and economic rationality. These results underscore the promise of domain-aligned post-training for enhancing reasoning and agent alignment, shedding light on the roles of SFT and RL in shaping model behavior. Code is available at https://github.com/MasterZhou1/Recon .
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Language Models Mirror Human Confidence? Exploring Psychological Insights to Address Overconfidence in LLMs</title>
<link>https://arxiv.org/abs/2506.00582</link>
<guid>https://arxiv.org/abs/2506.00582</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, confidence estimation, overconfidence, task difficulty, interpretability

Summary: 
LLMs (Large Language Models) such as Llama-3-70B-instruct, Claude-3-Sonnet, and GPT-4o show differences from human patterns of overconfidence in performance estimation. They exhibit less sensitivity to task difficulty and display biased confidence estimations when prompted with different personas. The proposed Answer-Free Confidence Estimation (AFCE) method aims to improve confidence calibration and interpretability of LLMs by separating confidence scores and answer prompts. Experiments on MMLU and GPQA datasets demonstrate that AFCE reduces overconfidence and provides a more human-like sensitivity to task difficulty. This approach enhances the accuracy of confidence estimations in LLMs, making them more reliable in various settings. 

<br /><br />Summary: <div>
arXiv:2506.00582v1 Announce Type: new 
Abstract: Psychology research has shown that humans are poor at estimating their performance on tasks, tending towards underconfidence on easy tasks and overconfidence on difficult tasks. We examine three LLMs, Llama-3-70B-instruct, Claude-3-Sonnet, and GPT-4o, on a range of QA tasks of varying difficulty, and show that models exhibit subtle differences from human patterns of overconfidence: less sensitive to task difficulty, and when prompted to answer based on different personas -- e.g., expert vs layman, or different race, gender, and ages -- the models will respond with stereotypically biased confidence estimations even though their underlying answer accuracy remains the same. Based on these observations, we propose Answer-Free Confidence Estimation (AFCE) to improve confidence calibration and LLM interpretability in these settings. AFCE is a self-assessment method that employs two stages of prompting, first eliciting only confidence scores on questions, then asking separately for the answer. Experiments on the MMLU and GPQA datasets spanning subjects and difficulty show that this separation of tasks significantly reduces overconfidence and delivers more human-like sensitivity to task difficulty.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RiOSWorld: Benchmarking the Risk of Multimodal Compter-Use Agents</title>
<link>https://arxiv.org/abs/2506.00618</link>
<guid>https://arxiv.org/abs/2506.00618</guid>
<content:encoded><![CDATA[
<div> benchmark, safety risks, MLLM-based agents, real-world scenarios, computer-use agents <br />
Summary:<br /> 
The article introduces the RiOSWorld benchmark, designed to assess the safety risks of multimodal large language models (MLLMs) deployed as computer-use agents in real-world scenarios. The benchmark consists of 492 risky tasks from various computer applications, categorized into user-originated and environmental risks. Safety risks are evaluated based on goal intention and completion. Experiments on RiOSWorld reveal significant safety risks faced by current computer-use agents, emphasizing the need for safety alignment in real-world computer manipulations. The benchmark provides insights for developing trustworthy computer-use agents. Access to the RiOSWorld benchmark is publicly available for further research and development. <br /> 
Summary: <div>
arXiv:2506.00618v1 Announce Type: new 
Abstract: With the rapid development of multimodal large language models (MLLMs), they are increasingly deployed as autonomous computer-use agents capable of accomplishing complex computer tasks. However, a pressing issue arises: Can the safety risk principles designed and aligned for general MLLMs in dialogue scenarios be effectively transferred to real-world computer-use scenarios? Existing research on evaluating the safety risks of MLLM-based computer-use agents suffers from several limitations: it either lacks realistic interactive environments, or narrowly focuses on one or a few specific risk types. These limitations ignore the complexity, variability, and diversity of real-world environments, thereby restricting comprehensive risk evaluation for computer-use agents. To this end, we introduce \textbf{RiOSWorld}, a benchmark designed to evaluate the potential risks of MLLM-based agents during real-world computer manipulations. Our benchmark includes 492 risky tasks spanning various computer applications, involving web, social media, multimedia, os, email, and office software. We categorize these risks into two major classes based on their risk source: (i) User-originated risks and (ii) Environmental risks. For the evaluation, we evaluate safety risks from two perspectives: (i) Risk goal intention and (ii) Risk goal completion. Extensive experiments with multimodal agents on \textbf{RiOSWorld} demonstrate that current computer-use agents confront significant safety risks in real-world scenarios. Our findings highlight the necessity and urgency of safety alignment for computer-use agents in real-world computer manipulation, providing valuable insights for developing trustworthy computer-use agents. Our benchmark is publicly available at https://yjyddq.github.io/RiOSWorld.github.io/.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentAuditor: Human-Level Safety and Security Evaluation for LLM Agents</title>
<link>https://arxiv.org/abs/2506.00641</link>
<guid>https://arxiv.org/abs/2506.00641</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM-based agents, safety evaluation, security evaluation, reasoning framework, benchmark.

Summary: 
The article addresses the challenge of reliably evaluating the safety and security of LLM-based agents, highlighting existing shortcomings in rule-based or LLM-based evaluators. To tackle this issue, the authors introduce a training-free, memory-augmented reasoning framework called \sys. This framework enables LLM evaluators to mimic human expert evaluators by constructing an experiential memory and generating reasoning traces for past interactions. They also introduce a benchmark, \data, comprising meticulously annotated interaction records covering various risk types and scenarios. The benchmark employs "Strict" and "Lenient" judgment standards to handle ambiguous risk situations. Experimental results demonstrate that \sys significantly improves the evaluation performance of LLMs and sets a new state-of-the-art in judging agent safety and security, achieving human-level accuracy. The work is openly accessible. 

<br /><br />Summary: <div>
arXiv:2506.00641v1 Announce Type: new 
Abstract: Despite the rapid advancement of LLM-based agents, the reliable evaluation of their safety and security remains a significant challenge. Existing rule-based or LLM-based evaluators often miss dangers in agents' step-by-step actions, overlook subtle meanings, fail to see how small issues compound, and get confused by unclear safety or security rules. To overcome this evaluation crisis, we introduce \sys, a universal, training-free, memory-augmented reasoning framework that empowers LLM evaluators to emulate human expert evaluators. \sys constructs an experiential memory by having an LLM adaptively extract structured semantic features (e.g., scenario, risk, behavior) and generate associated chain-of-thought reasoning traces for past interactions. A multi-stage, context-aware retrieval-augmented generation process then dynamically retrieves the most relevant reasoning experiences to guide the LLM evaluator's assessment of new cases. Moreover, we developed \data, the first benchmark designed to check how well LLM-based evaluators can spot both safety risks and security threats. \data comprises \textbf{2293} meticulously annotated interaction records, covering \textbf{15} risk types across \textbf{29} application scenarios. A key feature of \data is its nuanced approach to ambiguous risk situations, employing ``Strict'' and ``Lenient'' judgment standards. Experiments demonstrate that \sys not only consistently improves the evaluation performance of LLMs across all benchmarks but also sets a new state-of-the-art in LLM-as-a-judge for agent safety and security, achieving human-level accuracy. Our work is openly openly accessible.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OntoRAG: Enhancing Question-Answering through Automated Ontology Derivation from Unstructured Knowledge Bases</title>
<link>https://arxiv.org/abs/2506.00664</link>
<guid>https://arxiv.org/abs/2506.00664</guid>
<content:encoded><![CDATA[
<div> Keywords: Ontologies, Knowledge bases, Question answering, Large Language Models, Automation

Summary: 
OntoRAG is introduced as an automated pipeline for deriving ontologies from unstructured knowledge bases, specifically focusing on electrical relay documents. The pipeline integrates various advanced techniques such as web scraping, PDF parsing, information extraction, knowledge graph construction, and ontology creation to transform unstructured data into a queryable ontology. Leveraging Large Language Models and graph-based methods, OntoRAG enhances global sensemaking capabilities and outperforms conventional approaches like Retrieval Augmented Generation (RAG) and GraphRAG in terms of comprehensiveness and diversity. Experimental results showcase OntoRAG's effectiveness, achieving high win rates in comprehensiveness against different configurations. This work contributes to automating ontology creation, which is crucial for advancing the vision of the semantic web.

<br /><br />Summary: <div>
arXiv:2506.00664v1 Announce Type: new 
Abstract: Ontologies are pivotal for structuring knowledge bases to enhance question answering (QA) systems powered by Large Language Models (LLMs). However, traditional ontology creation relies on manual efforts by domain experts, a process that is time intensive, error prone, and impractical for large, dynamic knowledge domains. This paper introduces OntoRAG, an automated pipeline designed to derive ontologies from unstructured knowledge bases, with a focus on electrical relay documents. OntoRAG integrates advanced techniques, including web scraping, PDF parsing, hybrid chunking, information extraction, knowledge graph construction, and ontology creation, to transform unstructured data into a queryable ontology. By leveraging LLMs and graph based methods, OntoRAG enhances global sensemaking capabilities, outperforming conventional Retrieval Augmented Generation (RAG) and GraphRAG approaches in comprehensiveness and diversity. Experimental results demonstrate OntoRAGs effectiveness, achieving a comprehensiveness win rate of 85% against vector RAG and 75% against GraphRAGs best configuration. This work addresses the critical challenge of automating ontology creation, advancing the vision of the semantic web.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DrKGC: Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph Completion across General and Biomedical Domains</title>
<link>https://arxiv.org/abs/2506.00708</link>
<guid>https://arxiv.org/abs/2506.00708</guid>
<content:encoded><![CDATA[
<div> Keywords: Knowledge graph completion, Large language models, Subgraph retrieval, Graph convolutional network, Biomedical datasets

Summary:
DrKGC (Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph Completion) is a novel approach proposed for knowledge graph completion. It overcomes the limitation of existing methods by incorporating a bottom-up graph retrieval method to extract relevant subgraphs guided by learned rules. By enhancing structural embeddings using a graph convolutional network (GCN) adapter, DrKGC achieves superior performance on benchmark datasets in both general and biomedical domains. The model also offers interpretability and practical utility, as demonstrated in a case study in the biomedical domain. Overall, DrKGC effectively leverages large language models for knowledge graph completion by combining graph structure perception and reasoning with textual information. <div>
arXiv:2506.00708v1 Announce Type: new 
Abstract: Knowledge graph completion (KGC) aims to predict missing triples in knowledge graphs (KGs) by leveraging existing triples and textual information. Recently, generative large language models (LLMs) have been increasingly employed for graph tasks. However, current approaches typically encode graph context in textual form, which fails to fully exploit the potential of LLMs for perceiving and reasoning about graph structures. To address this limitation, we propose DrKGC (Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph Completion). DrKGC employs a flexible lightweight model training strategy to learn structural embeddings and logical rules within the KG. It then leverages a novel bottom-up graph retrieval method to extract a subgraph for each query guided by the learned rules. Finally, a graph convolutional network (GCN) adapter uses the retrieved subgraph to enhance the structural embeddings, which are then integrated into the prompt for effective LLM fine-tuning. Experimental results on two general domain benchmark datasets and two biomedical datasets demonstrate the superior performance of DrKGC. Furthermore, a realistic case study in the biomedical domain highlights its interpretability and practical utility.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alignment Revisited: Are Large Language Models Consistent in Stated and Revealed Preferences?</title>
<link>https://arxiv.org/abs/2506.00751</link>
<guid>https://arxiv.org/abs/2506.00751</guid>
<content:encoded><![CDATA[
<div> Large Language Models, human values, preference deviation, interpretability, trustworthiness<br />
Summary:<br />Recent advances in Large Language Models (LLMs) emphasize the importance of aligning their behaviors with human values. This study addresses the issue of preference deviation in LLMs, where their stated preferences may not align with their revealed preferences in contextualized scenarios. A method to measure this deviation is proposed, using a dataset of well-designed prompts to compare LLM responses to general principle prompts and contextualized prompts. The analysis shows that minor changes in prompt format can significantly alter LLM decisions, highlighting a lack of understanding and control over their decision-making competence. This has implications for the interpretability, trustworthiness, and ethical deployment of LLMs, particularly in high-stakes applications. Understanding and addressing preference deviation in LLMs will be crucial for integrating them into services that interact directly with humans, where morality, fairness, and social responsibilities are important considerations. Identifying such deviations will be essential as LLMs are increasingly used for autonomous tasks without continuous human evaluation. <br /><br />Summary: <div>
arXiv:2506.00751v1 Announce Type: new 
Abstract: Recent advances in Large Language Models (LLMs) highlight the need to align their behaviors with human values. A critical, yet understudied, issue is the potential divergence between an LLM's stated preferences (its reported alignment with general principles) and its revealed preferences (inferred from decisions in contextualized scenarios). Such deviations raise fundamental concerns for the interpretability, trustworthiness, reasoning transparency, and ethical deployment of LLMs, particularly in high-stakes applications. This work formally defines and proposes a method to measure this preference deviation. We investigate how LLMs may activate different guiding principles in specific contexts, leading to choices that diverge from previously stated general principles. Our approach involves crafting a rich dataset of well-designed prompts as a series of forced binary choices and presenting them to LLMs. We compare LLM responses to general principle prompts stated preference with LLM responses to contextualized prompts revealed preference, using metrics like KL divergence to quantify the deviation. We repeat the analysis across different categories of preferences and on four mainstream LLMs and find that a minor change in prompt format can often pivot the preferred choice regardless of the preference categories and LLMs in the test. This prevalent phenomenon highlights the lack of understanding and control of the LLM decision-making competence. Our study will be crucial for integrating LLMs into services, especially those that interact directly with humans, where morality, fairness, and social responsibilities are crucial dimensions. Furthermore, identifying or being aware of such deviation will be critically important as LLMs are increasingly envisioned for autonomous agentic tasks where continuous human evaluation of all LLMs' intermediary decision-making steps is impossible.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HouseTS: A Large-Scale, Multimodal Spatiotemporal U.S. Housing Dataset</title>
<link>https://arxiv.org/abs/2506.00765</link>
<guid>https://arxiv.org/abs/2506.00765</guid>
<content:encoded><![CDATA[
<div> Keywords: house-price forecasting, multimodal dataset, ZIP codes, deep neural networks, urban evolution <br />
Summary: 
HouseTS is a new dataset introduced for accurate house-price forecasting, covering monthly prices from March 2012 to December 2023 across 6,000 ZIP codes in 30 major U.S. metropolitan areas. The dataset contains over 890K records with enriched information such as points of interest, socioeconomic indicators, and real estate metrics. It evaluates 14 models including classical statistical approaches, deep neural networks, and pretrained time-series foundation models to establish standardized performance baselines. Additionally, a multimodal case study showcases the dataset's value by using a vision language model to extract structured descriptions of geographic change from satellite imagery for urban evolution insights. HouseTS is hosted on Kaggle with preprocessing pipelines and benchmark code openly maintained on GitHub for reproducibility and easy adoption. <br /><br />Summary: <div>
arXiv:2506.00765v1 Announce Type: new 
Abstract: Accurate house-price forecasting is essential for investors, planners, and researchers. However, reproducible benchmarks with sufficient spatiotemporal depth and contextual richness for long horizon prediction remain scarce. To address this, we introduce HouseTS a large scale, multimodal dataset covering monthly house prices from March 2012 to December 2023 across 6,000 ZIP codes in 30 major U.S. metropolitan areas. The dataset includes over 890K records, enriched with points of Interest (POI), socioeconomic indicators, and detailed real estate metrics. To establish standardized performance baselines, we evaluate 14 models, spanning classical statistical approaches, deep neural networks (DNNs), and pretrained time-series foundation models. We further demonstrate the value of HouseTS in a multimodal case study, where a vision language model extracts structured textual descriptions of geographic change from time stamped satellite imagery. This enables interpretable, grounded insights into urban evolution. HouseTS is hosted on Kaggle, while all preprocessing pipelines, benchmark code, and documentation are openly maintained on GitHub to ensure full reproducibility and easy adoption.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do not Abstain! Identify and Solve the Uncertainty</title>
<link>https://arxiv.org/abs/2506.00780</link>
<guid>https://arxiv.org/abs/2506.00780</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, uncertainty, ConfuseBench, document scarcity, query ambiguity

Summary: 

Large Language Models (LLMs) are widely used but often struggle with overconfidence in uncertain scenarios. Existing solutions tend to provide evasive responses, such as "I don't know," instead of addressing the root cause of uncertainty. To address this issue, this paper introduces ConfuseBench, a benchmark focused on document scarcity, limited capability, and query ambiguity. Experiments show that current LLMs have difficulty accurately identifying and resolving uncertainty, often attributing it to query ambiguity while ignoring capability limitations. The authors propose generating context-aware inquiries to highlight confusing aspects of queries and use an on-policy training method, InteractDPO, to improve inquiries. Results demonstrate the effectiveness of this approach in helping LLMs recognize and address sources of uncertainty. <div>
arXiv:2506.00780v1 Announce Type: new 
Abstract: Despite the widespread application of Large Language Models (LLMs) across various domains, they frequently exhibit overconfidence when encountering uncertain scenarios, yet existing solutions primarily rely on evasive responses (e.g., "I don't know") overlooks the opportunity of identifying and addressing the uncertainty to generate more satisfactory responses. To systematically investigate and improve LLMs' ability of recognizing and addressing the source of uncertainty, we introduce \textbf{ConfuseBench}, a benchmark mainly focus on three types of uncertainty: document scarcity, limited capability, and query ambiguity. Experiments with ConfuseBench reveal that current LLMs struggle to accurately identify the root cause of uncertainty and solve it. They prefer to attribute uncertainty to query ambiguity while overlooking capability limitations, especially for those weaker models. To tackle this challenge, we first generate context-aware inquiries that highlight the confusing aspect of the original query. Then we judge the source of uncertainty based on the uniqueness of the inquiry's answer. Further we use an on-policy training method, InteractDPO to generate better inquiries. Experimental results demonstrate the efficacy of our approach.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoP: Agentic Red-teaming for Large Language Models using Composition of Principles</title>
<link>https://arxiv.org/abs/2506.00781</link>
<guid>https://arxiv.org/abs/2506.00781</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, red-teaming, jailbreak attacks, Composition-of-Principles framework, automated discovery

Summary: 
Large Language Models (LLMs) have seen significant advancements, leading to transformative applications across various domains. However, emerging concerns about jailbreak attacks, which exploit LLMs to produce harmful responses, highlight the need for proactive risk assessment. This paper introduces an innovative approach, the Composition-of-Principles (CoP) framework, to automate and scale the red-teaming process for LLMs. By integrating human-provided red-teaming principles, the CoP framework orchestrates effective strategies to identify and generate jailbreak prompts. In testing against prominent LLMs, CoP uncovers unprecedented safety risks, detecting novel jailbreak prompts and significantly enhancing single-turn attack success rates up to 19 times. This framework not only enhances security measures but also facilitates the automated discovery of new red-teaming strategies, bolstering defenses against potential risks in the frontier of AI technology. 

<br /><br />Summary: <div>
arXiv:2506.00781v1 Announce Type: new 
Abstract: Recent advances in Large Language Models (LLMs) have spurred transformative applications in various domains, ranging from open-source to proprietary LLMs. However, jailbreak attacks, which aim to break safety alignment and user compliance by tricking the target LLMs into answering harmful and risky responses, are becoming an urgent concern. The practice of red-teaming for LLMs is to proactively explore potential risks and error-prone instances before the release of frontier AI technology. This paper proposes an agentic workflow to automate and scale the red-teaming process of LLMs through the Composition-of-Principles (CoP) framework, where human users provide a set of red-teaming principles as instructions to an AI agent to automatically orchestrate effective red-teaming strategies and generate jailbreak prompts. Distinct from existing red-teaming methods, our CoP framework provides a unified and extensible framework to encompass and orchestrate human-provided red-teaming principles to enable the automated discovery of new red-teaming strategies. When tested against leading LLMs, CoP reveals unprecedented safety risks by finding novel jailbreak prompts and improving the best-known single-turn attack success rate by up to 19.0 times.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jailbreak-R1: Exploring the Jailbreak Capabilities of LLMs via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.00782</link>
<guid>https://arxiv.org/abs/2506.00782</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Language Models, Automated Red Teaming, Jailbreak, Diversity

Summary:
- The study introduces a novel automated red teaming training framework called \ourapproach for large language models (LLMs) to detect security vulnerabilities.
- The framework consists of three stages: Cold Start, Warm-up Exploration, and Enhanced Jailbreak, utilizing reinforcement learning for exploration and generation of effective attack prompts with balanced diversity.
- The red team model is supervised and fine-tuned on a jailbreak dataset through imitation learning to enhance its performance.
- Training involves diversity and consistency as reward signals, and progressive jailbreak rewards are introduced to improve jailbreak performance gradually.
- Extensive experiments demonstrate that \ourapproach effectively balances the diversity and effectiveness of jailbreak prompts in comparison to existing methods, improving the efficiency of red team exploration and providing a new perspective on automated red teaming.<br /><br />Summary: <div>
arXiv:2506.00782v1 Announce Type: new 
Abstract: As large language models (LLMs) grow in power and influence, ensuring their safety and preventing harmful output becomes critical. Automated red teaming serves as a tool to detect security vulnerabilities in LLMs without manual labor. However, most existing methods struggle to balance the effectiveness and diversity of red-team generated attack prompts. To address this challenge, we propose \ourapproach, a novel automated red teaming training framework that utilizes reinforcement learning to explore and generate more effective attack prompts while balancing their diversity. Specifically, it consists of three training stages: (1) Cold Start: The red team model is supervised and fine-tuned on a jailbreak dataset obtained through imitation learning. (2) Warm-up Exploration: The model is trained in jailbreak instruction following and exploration, using diversity and consistency as reward signals. (3) Enhanced Jailbreak: Progressive jailbreak rewards are introduced to gradually enhance the jailbreak performance of the red-team model. Extensive experiments on a variety of LLMs show that \ourapproach effectively balances the diversity and effectiveness of jailbreak prompts compared to existing methods. Our work significantly improves the efficiency of red team exploration and provides a new perspective on automated red teaming.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoChain: Multimodal Chain-of-Thought for Geographic Reasoning</title>
<link>https://arxiv.org/abs/2506.00785</link>
<guid>https://arxiv.org/abs/2506.00785</guid>
<content:encoded><![CDATA[
<div> Keywords: GeoChain, benchmark, multimodal large language models, geographic reasoning, localization

Summary:
GeoChain is a new large-scale benchmark designed to evaluate step-by-step geographic reasoning in multimodal large language models (MLLMs). This benchmark pairs 1.46 million Mapillary street-level images with a 21-step chain-of-thought question sequence, resulting in over 30 million Q&amp;A pairs. The benchmark covers four reasoning categories - visual, spatial, cultural, and precise geolocation - annotated by difficulty. Images are enriched with semantic segmentation and a visual locatability score. Evaluation of current MLLMs on a subset of 2,088 images revealed consistent challenges, including weaknesses in visual grounding, erratic reasoning, and difficulties in accurate localization as reasoning complexity increased. GeoChain provides a valuable diagnostic methodology for advancing geographic reasoning within MLLMs. 

<br /><br />Summary: <div>
arXiv:2506.00785v1 Announce Type: new 
Abstract: This paper introduces GeoChain, a large-scale benchmark for evaluating step-by-step geographic reasoning in multimodal large language models (MLLMs). Leveraging 1.46 million Mapillary street-level images, GeoChain pairs each image with a 21-step chain-of-thought (CoT) question sequence (over 30 million Q&amp;A pairs). These sequences guide models from coarse attributes to fine-grained localization across four reasoning categories - visual, spatial, cultural, and precise geolocation - annotated by difficulty. Images are also enriched with semantic segmentation (150 classes) and a visual locatability score. Our benchmarking of contemporary MLLMs (GPT-4.1 variants, Claude 3.7, Gemini 2.5 variants) on a diverse 2,088-image subset reveals consistent challenges: models frequently exhibit weaknesses in visual grounding, display erratic reasoning, and struggle to achieve accurate localization, especially as the reasoning complexity escalates. GeoChain offers a robust diagnostic methodology, critical for fostering significant advancements in complex geographic reasoning within MLLMs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Empirical AI Research Outcomes with Language Models</title>
<link>https://arxiv.org/abs/2506.00794</link>
<guid>https://arxiv.org/abs/2506.00794</guid>
<content:encoded><![CDATA[
<div> Benchmark, Validation, Prediction, GPT-4.1, Human experts

Summary:
A new benchmark is introduced for predicting the success of AI research ideas, aiming to accelerate empirical AI research. The system combines a fine-tuned GPT-4.1 with a paper retrieval agent and outperforms human experts in the NLP domain. With an accuracy of 77%, the system proves successful in predicting the performance of research ideas. Superficial features are ruled out as the basis for predictions through robustness tests. The system also demonstrates potential as a reward model for improving idea generation models, achieving 63.6% accuracy on unpublished novel ideas, including those generated by an AI ideation agent. This study suggests a promising new approach for leveraging language models to enhance the pace and outcomes of empirical AI research.<br /><br />Summary: <div>
arXiv:2506.00794v1 Announce Type: new 
Abstract: Many promising-looking ideas in AI research fail to deliver, but their validation takes substantial human labor and compute. Predicting an idea's chance of success is thus crucial for accelerating empirical AI research, a skill that even expert researchers can only acquire through substantial experience. We build the first benchmark for this task and compare LMs with human experts. Concretely, given two research ideas (e.g., two jailbreaking methods), we aim to predict which will perform better on a set of benchmarks. We scrape ideas and experimental results from conference papers, yielding 1,585 human-verified idea pairs published after our base model's cut-off date for testing, and 6,000 pairs for training. We then develop a system that combines a fine-tuned GPT-4.1 with a paper retrieval agent, and we recruit 25 human experts to compare with. In the NLP domain, our system beats human experts by a large margin (64.4% v.s. 48.9%). On the full test set, our system achieves 77% accuracy, while off-the-shelf frontier LMs like o3 perform no better than random guessing, even with the same retrieval augmentation. We verify that our system does not exploit superficial features like idea complexity through extensive human-written and LM-designed robustness tests. Finally, we evaluate our system on unpublished novel ideas, including ideas generated by an AI ideation agent. Our system achieves 63.6% accuracy, demonstrating its potential as a reward model for improving idea generation models. Altogether, our results outline a promising new direction for LMs to accelerate empirical AI research.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing LLM Reasoning for Time Series Classification by Tailored Thinking and Fused Decision</title>
<link>https://arxiv.org/abs/2506.00807</link>
<guid>https://arxiv.org/abs/2506.00807</guid>
<content:encoded><![CDATA[
<div> reasoning capabilities, large language models, time series classification, ReasonTSC, multi-turn reasoning

Summary:
ReasonTSC introduces a novel framework for leveraging large language models' reasoning capabilities in time series classification tasks. The framework employs a multi-turn reasoning approach and a fused decision-making strategy specifically tailored to time series data. Unlike existing methods, ReasonTSC prompts the model to contemplate the essential features of time series data before integrating predictions from plug-in classifiers. It guides the LLM through a structured reasoning process, evaluating initial assessments, exploring alternative hypotheses, and comparing their merits to reach a final classification. Experimental results and ablation studies demonstrate that ReasonTSC consistently outperforms existing baselines and plug-in models. The framework can also identify and rectify false predictions made by plug-in models, showcasing its effectiveness in enhancing time series classification performance. <br /><br />Summary: <div>
arXiv:2506.00807v1 Announce Type: new 
Abstract: The reasoning capabilities of large language models (LLMs) have significantly advanced their performance by enabling in-depth understanding of diverse tasks. With growing interest in applying LLMs to the time series domain, this has proven nontrivial, as evidenced by the limited efficacy of straightforwardly adapting text-domain reasoning techniques. Although recent work has shown promise in several time series tasks, further leveraging advancements in LLM reasoning remains under-explored for time series classification (TSC) tasks, despite their prevalence and significance in many real-world applications. In this paper, we propose ReasonTSC, a novel framework designed to effectively leverage LLM reasoning for time series classification through both a multi-turn reasoning and a fused decision-making strategy tailored to TSC. Rather than straightforwardly applying existing reasoning techniques or relying solely on LLMs' built-in reasoning capabilities, ReasonTSC first steers the model to think over the essential characteristics of time series data. Next, it integrates predictions and confidence scores from plug-in classifiers, e.g., domain-specific time series models, as in-context examples. Finally, ReasonTSC guides the LLM through a structured reasoning process: it evaluates the initial assessment, backtracks to consider alternative hypotheses, and compares their merits before arriving at a final classification. Extensive experiments and systematic ablation studies demonstrate that ReasonTSC consistently outperforms both existing time series reasoning baselines and plug-in models, and is even capable of identifying and correcting plug-in models' false predictions.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynPO: Synergizing Descriptiveness and Preference Optimization for Video Detailed Captioning</title>
<link>https://arxiv.org/abs/2506.00835</link>
<guid>https://arxiv.org/abs/2506.00835</guid>
<content:encoded><![CDATA[
<div> Keywords: fine-grained video captioning, preference learning, vision-language models, Synergistic Preference Optimization, training efficiency

Summary:
Preference learning is utilized to enhance fine-grained video captioning by addressing limitations of direct preference optimization (DPO). A pipeline is proposed to construct preference pairs efficiently, balancing cost and data quality. Synergistic Preference Optimization (SynPO) is introduced as a novel method that surpasses DPO by preventing negative preferences from dominating optimization and maintaining the model's language capability. SynPO eliminates the need for a reference model, improving training efficiency by 20%. Evaluations on video captioning benchmarks and NLP tasks show consistent performance gains across diverse pretrained models. The code for SynPO is available for public use on GitHub. 

<br /><br />Summary: <div>
arXiv:2506.00835v1 Announce Type: new 
Abstract: Fine-grained video captioning aims to generate detailed, temporally coherent descriptions of video content. However, existing methods struggle to capture subtle video dynamics and rich detailed information. In this paper, we leverage preference learning to enhance the performance of vision-language models in fine-grained video captioning, while mitigating several limitations inherent to direct preference optimization (DPO). First, we propose a pipeline for constructing preference pairs that leverages the intrinsic properties of VLMs along with partial assistance from large language models, achieving an optimal balance between cost and data quality. Second, we propose Synergistic Preference Optimization (SynPO), a novel optimization method offering significant advantages over DPO and its variants. SynPO prevents negative preferences from dominating the optimization, explicitly preserves the model's language capability to avoid deviation of the optimization objective, and improves training efficiency by eliminating the need for the reference model. We extensively evaluate SynPO not only on video captioning benchmarks (e.g., VDC, VDD, VATEX) but also across well-established NLP tasks, including general language understanding and preference evaluation, using diverse pretrained models. Results demonstrate that SynPO consistently outperforms DPO variants while achieving 20\% improvement in training efficiency. Code is available at https://github.com/longmalongma/SynPO
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedBookVQA: A Systematic and Comprehensive Medical Benchmark Derived from Open-Access Book</title>
<link>https://arxiv.org/abs/2506.00855</link>
<guid>https://arxiv.org/abs/2506.00855</guid>
<content:encoded><![CDATA[
<div> Benchmark, GMAI, MedBookVQA, multimodal, medical textbooks <br />
Summary:<br />
- The article discusses the importance of systematic evaluation benchmarks for general medical artificial intelligence (GMAI) powered by multimodal large language models (MLLMs). 
- MedBookVQA is introduced as a comprehensive multimodal benchmark derived from medical textbooks, providing 5,000 clinically relevant questions across various medical domains.
- A standardized pipeline for automated extraction of medical figures and alignment with narratives is proposed for benchmark curation.
- The benchmark includes hierarchical taxonomies for categorizing queries based on medical imaging modalities, body anatomies, and clinical specialties, enabling nuanced analysis.
- Evaluation of different MLLMs reveals significant performance variations, highlighting critical capability gaps in current GMAI systems. 
- MedBookVQA showcases the value of textbook-derived benchmarks for assessing and improving clinical AI, offering structured performance metrics across medical specialties. <br /> <div>
arXiv:2506.00855v1 Announce Type: new 
Abstract: The accelerating development of general medical artificial intelligence (GMAI), powered by multimodal large language models (MLLMs), offers transformative potential for addressing persistent healthcare challenges, including workforce deficits and escalating costs. The parallel development of systematic evaluation benchmarks emerges as a critical imperative to enable performance assessment and provide technological guidance. Meanwhile, as an invaluable knowledge source, the potential of medical textbooks for benchmark development remains underexploited. Here, we present MedBookVQA, a systematic and comprehensive multimodal benchmark derived from open-access medical textbooks. To curate this benchmark, we propose a standardized pipeline for automated extraction of medical figures while contextually aligning them with corresponding medical narratives. Based on this curated data, we generate 5,000 clinically relevant questions spanning modality recognition, disease classification, anatomical identification, symptom diagnosis, and surgical procedures. A multi-tier annotation system categorizes queries through hierarchical taxonomies encompassing medical imaging modalities (42 categories), body anatomies (125 structures), and clinical specialties (31 departments), enabling nuanced analysis across medical subdomains. We evaluate a wide array of MLLMs, including proprietary, open-sourced, medical, and reasoning models, revealing significant performance disparities across task types and model categories. Our findings highlight critical capability gaps in current GMAI systems while establishing textbook-derived multimodal benchmarks as essential evaluation tools. MedBookVQA establishes textbook-derived benchmarking as a critical paradigm for advancing clinical AI, exposing limitations in GMAI systems while providing anatomically structured performance metrics across specialties.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GIA-MIC: Multimodal Emotion Recognition with Gated Interactive Attention and Modality-Invariant Learning Constraints</title>
<link>https://arxiv.org/abs/2506.00865</link>
<guid>https://arxiv.org/abs/2506.00865</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal emotion recognition, attention-based fusion, gated interactive attention mechanism, modality-specific features, modality-invariant generator

Summary: 
- Multimodal emotion recognition (MER) plays a crucial role in human-computer interaction, utilizing visual, speech, and text inputs for emotion extraction.
- Existing MER research heavily relies on attention-based fusion methods, achieving strong classification performance.
- Challenges in MER include effectively extracting modality-specific features and capturing cross-modal similarities despite distribution differences caused by modality heterogeneity.
- The proposed gated interactive attention mechanism aims to adaptively extract modality-specific features while enhancing emotional information through pairwise interactions.
- Additionally, a modality-invariant generator is introduced to learn modality-invariant representations and address domain shifts by aligning cross-modal similarities.
<br /><br />Summary: <div>
arXiv:2506.00865v1 Announce Type: new 
Abstract: Multimodal emotion recognition (MER) extracts emotions from multimodal data, including visual, speech, and text inputs, playing a key role in human-computer interaction. Attention-based fusion methods dominate MER research, achieving strong classification performance. However, two key challenges remain: effectively extracting modality-specific features and capturing cross-modal similarities despite distribution differences caused by modality heterogeneity. To address these, we propose a gated interactive attention mechanism to adaptively extract modality-specific features while enhancing emotional information through pairwise interactions. Additionally, we introduce a modality-invariant generator to learn modality-invariant representations and constrain domain shifts by aligning cross-modal similarities. Experiments on IEMOCAP demonstrate that our method outperforms state-of-the-art MER approaches, achieving WA 80.7% and UA 81.3%.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward a Theory of Agents as Tool-Use Decision-Makers</title>
<link>https://arxiv.org/abs/2506.00886</link>
<guid>https://arxiv.org/abs/2506.00886</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models (LLMs), epistemic framework, autonomy, decision-making, knowledge boundary

Summary: 
Agents require a coherent epistemic framework to govern their knowledge and decision-making processes. This framework treats internal reasoning and external actions as equivalent tools, allowing for systematic coordination of introspection and interaction. By aligning an agent's tool use decision-making boundary with its knowledge boundary, unnecessary tool use is minimized and epistemic efficiency is maximized. This perspective shifts the design of agents towards knowledge-driven intelligence systems, enabling adaptive, efficient, and goal-directed behavior. This approach aims to create foundation agents capable of autonomous decision-making and effectively acquiring knowledge for optimal performance. <div>
arXiv:2506.00886v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) evolve into increasingly autonomous agents, fundamental questions about their epistemic foundations remain unresolved: What defines an agent? How should it make decisions? And what objectives should guide its behavior? In this position paper, we argue that true autonomy requires agents to be grounded in a coherent epistemic framework that governs what they know, what they need to know, and how to acquire that knowledge efficiently. We propose a unified theory that treats internal reasoning and external actions as equivalent epistemic tools, enabling agents to systematically coordinate introspection and interaction. Building on this framework, we advocate for aligning an agent's tool use decision-making boundary with its knowledge boundary, thereby minimizing unnecessary tool use and maximizing epistemic efficiency. This perspective shifts the design of agents from mere action executors to knowledge-driven intelligence systems, offering a principled path toward building foundation agents capable of adaptive, efficient, and goal-directed behavior.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Arbitrage: Risk-Controlled Balancing of Competing Objectives in Language Models</title>
<link>https://arxiv.org/abs/2506.00911</link>
<guid>https://arxiv.org/abs/2506.00911</guid>
<content:encoded><![CDATA[
<div> Keywords: Conformal Arbitrage, language model deployments, threshold calibration, risk control, performance optimization

Summary: Conformal Arbitrage introduces a framework for managing competing objectives in modern language model deployments. By learning a data-driven threshold, it mediates between a Primary model optimized for a primary objective and a Guardian model aligned with a guardrail objective. The framework uses conformal risk control to ensure that the frequency of undesirable events stays within user-specified limits. Operating at the API level, without accessing model logits or updating weights, Conformal Arbitrage complements weight-based alignment techniques. It outperforms random routing between models in terms of accuracy, providing a practical and theoretically grounded approach for deploying large language models effectively across a range of objectives. <br /><br />Summary: They introduce Conformal Arbitrage for modern language model deployments that balances competing objectives by learning a threshold calibrated with conformal risk control. It complements existing techniques and outperforms random routing, making it a practical tool for deploying large language models effectively. <div>
arXiv:2506.00911v1 Announce Type: new 
Abstract: Modern language model deployments must often balance competing objectives, for example, helpfulness versus harmlessness, cost versus accuracy, and reward versus safety. We introduce Conformal Arbitrage, a post hoc framework that learns a data driven threshold to mediate between a Primary model optimized for a primary objective and a more conservative Guardian which could be another model or a human domain expert aligned with a guardrail objective. The threshold is calibrated with conformal risk control, yielding finite sample, distribution free guarantees that the long run frequency of undesirable events, such as factual errors or safety violations, does not exceed a user specified quota. Because Conformal Arbitrage operates wholly at the API level, without requiring access to model logits or updating model weights, it complements weight based alignment techniques and integrates seamlessly with existing cost aware cascades. Empirically, Conformal Arbitrage traces an efficient frontier, allowing users to define an acceptable performance level for one objective while maximizing utility in another. We observe that our method outperforms, in terms of accuracy, cost matched random routing between models. These properties make Conformal Arbitrage a practical, theoretically grounded tool for trustworthy and economical deployment of large language models across a broad range of potentially competing objectives.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning VLM Assistants with Personalized Situated Cognition</title>
<link>https://arxiv.org/abs/2506.00930</link>
<guid>https://arxiv.org/abs/2506.00930</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-language models, personalized alignment, Role-Set, PCogAlignBench, PCogAlign

Summary:<br />
Vision-language models that are aligned with individual cognitive preferences are becoming essential in assisting with visual tasks, but personalized expectations for these models vary among individuals based on their Role-Sets. To address this, a benchmark called PCogAlignBench has been created with 18k instances and 20 individuals, each with different Role-Sets. The framework PCogAlign has been proposed to construct a cognition-aware and action-based reward model for personalized alignment. Experimental results and human evaluations have validated the effectiveness of PCogAlign. The code and benchmark will be made available at https://github.com/NLPGM/PCogAlign. 

<br /> <div>
arXiv:2506.00930v1 Announce Type: new 
Abstract: Vision-language models (VLMs) aligned with general human objectives, such as being harmless and hallucination-free, have become valuable assistants of humans in managing visual tasks. However, people with diversified backgrounds have different cognition even in the same situation. Consequently, they may have personalized expectations for VLM assistants. This highlights the urgent need to align VLM assistants with personalized situated cognition for real-world assistance. To study this problem, we first simplify it by characterizing individuals based on the sociological concept of Role-Set. Then, we propose to evaluate the individuals' actions to examine whether the personalized alignment is achieved. Further, we construct a benchmark named PCogAlignBench, which includes 18k instances and 20 individuals with different Role-Sets. Finally, we present a framework called PCogAlign, which constructs a cognition-aware and action-based reward model for personalized alignment. Experimental results and human evaluations demonstrate the reliability of the PCogAlignBench and the effectiveness of our proposed PCogAlign. We will open-source the constructed benchmark and code at https://github.com/NLPGM/PCogAlign.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning Nonverbal Cues from Video-Grounded Dialogues</title>
<link>https://arxiv.org/abs/2506.00958</link>
<guid>https://arxiv.org/abs/2506.00958</guid>
<content:encoded><![CDATA[
<div> Keywords: Nonverbal communication, gestures, facial expressions, body language, multimodal language model

Summary:
MARS is a multimodal language model that incorporates nonverbal cues into text, enhancing conversational AI. The model utilizes the VENUS dataset, which includes annotated videos with text, facial expressions, and body language. MARS is trained using a next-token prediction objective, combining text with nonverbal representations for a comprehensive understanding. The VENUS dataset is shown to be large-scale and effective through various analyses. MARS successfully generates both text and nonverbal languages in response to conversational input. The integration of nonverbal cues into language models like MARS opens up new possibilities for creating more immersive conversational experiences. 

<br /><br />Summary: <div>
arXiv:2506.00958v1 Announce Type: new 
Abstract: Nonverbal communication is integral to human interaction, with gestures, facial expressions, and body language conveying critical aspects of intent and emotion. However, existing large language models (LLMs) fail to effectively incorporate these nonverbal elements, limiting their capacity to create fully immersive conversational experiences. We introduce MARS, a multimodal language model designed to understand and generate nonverbal cues alongside text, bridging this gap in conversational AI. Our key innovation is VENUS, a large-scale dataset comprising annotated videos with time-aligned text, facial expressions, and body language. Leveraging VENUS, we train MARS with a next-token prediction objective, combining text with vector-quantized nonverbal representations to achieve multimodal understanding and generation within a unified framework. Based on various analyses of the VENUS datasets, we validate its substantial scale and high effectiveness. Our quantitative and qualitative results demonstrate that MARS successfully generates text and nonverbal languages, corresponding to conversational input.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlocking Personalized Knowledge in Federated Large Language Model: The Power of Mixture of Experts</title>
<link>https://arxiv.org/abs/2506.00965</link>
<guid>https://arxiv.org/abs/2506.00965</guid>
<content:encoded><![CDATA[
<div> MoE architecture, federated learning, FLEx framework, personalized experts, knowledge sharing
Summary: 
The article introduces FLEx, a novel federated learning framework designed specifically for MoE-based large language models. FLEx addresses the challenges of existing FL approaches by efficiently personalizing MoE models through expert pruning and adaptive gating mechanisms. This allows for personalized knowledge sharing while minimizing communication overhead and computational costs. FLEx outperforms current federated baselines in evaluations on various instruction-based datasets under non-IID conditions. The shared modules are aggregated globally, while personalized experts are trained locally and stored on each client, ensuring the original MoE architecture remains unchanged. The code for FLEx is available online for further exploration. <br /><br />Summary: <div>
arXiv:2506.00965v1 Announce Type: new 
Abstract: The Mixture of Experts (MoE) architecture has emerged as a prominent strategy for scaling large language models (LLMs), effectively leveraging sparse activation and facilitating task-specific personalization. However, current federated learning (FL) approaches are primarily designed for dense models, making them unable to directly exploit the sparsity inherent in MoE architectures. Treating MoE models as dense networks in federated scenarios results in excessive communication overhead and computational costs, undermining the potential for personalized knowledge sharing. To address these challenges, we propose FLEx (Federated LLMs with Personalized Experts), a novel federated learning framework explicitly tailored for MoE-based LLMs. FLEx efficiently personalizes by pruning the global MoE model to keep only one expert per client, and employs an adaptive gating mechanism to reintegrate these personalized experts into the pre-trained MoE layers, ensuring the original backbone architecture remains unchanged. These personalized experts are trained with local data and stored locally on each client, while the shared modules are aggregated globally. Extensive evaluations on diverse instruction-based datasets under non-IID conditions consistently demonstrate that FLEx outperforms existing federated baselines. Our code is available at https://anonymous.4open.science/r/FLEx-8F12.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PolyBERT: Fine-Tuned Poly Encoder BERT-Based Model for Word Sense Disambiguation</title>
<link>https://arxiv.org/abs/2506.00968</link>
<guid>https://arxiv.org/abs/2506.00968</guid>
<content:encoded><![CDATA[
<div> keywords: Word Sense Disambiguation, BERT, PolyBERT, multi-head attention, Batch Contrastive Learning

Summary:
PolyBERT, a poly-encoder BERT-based model, addresses limitations in previous Word Sense Disambiguation (WSD) methods by balancing token-level and sequence-level semantics and reducing computational costs. The model utilizes a multi-head attention mechanism to fuse local and global semantics, enhancing semantic representation. Batch Contrastive Learning (BCL) is introduced to reduce redundant training inputs by using correct senses of other target words in the same batch as negative samples. Experimental results show that PolyBERT outperforms baseline WSD methods like Huang's GlossBERT and Blevins's BEM, achieving a 2% higher F1-score. Additionally, PolyBERT with BCL reduces GPU hours by 37.6% compared to PolyBERT without BCL.
<br /><br />Summary: <div>
arXiv:2506.00968v1 Announce Type: new 
Abstract: Mainstream Word Sense Disambiguation (WSD) approaches have employed BERT to extract semantics from both context and definitions of senses to determine the most suitable sense of a target word, achieving notable performance. However, there are two limitations in these approaches. First, previous studies failed to balance the representation of token-level (local) and sequence-level (global) semantics during feature extraction, leading to insufficient semantic representation and a performance bottleneck. Second, these approaches incorporated all possible senses of each target word during the training phase, leading to unnecessary computational costs. To overcome these limitations, this paper introduces a poly-encoder BERT-based model with batch contrastive learning for WSD, named PolyBERT. Compared with previous WSD methods, PolyBERT has two improvements: (1) A poly-encoder with a multi-head attention mechanism is utilized to fuse token-level (local) and sequence-level (global) semantics, rather than focusing on just one. This approach enriches semantic representation by balancing local and global semantics. (2) To avoid redundant training inputs, Batch Contrastive Learning (BCL) is introduced. BCL utilizes the correct senses of other target words in the same batch as negative samples for the current target word, which reduces training inputs and computational cost. The experimental results demonstrate that PolyBERT outperforms baseline WSD methods such as Huang's GlossBERT and Blevins's BEM by 2\% in F1-score. In addition, PolyBERT with BCL reduces GPU hours by 37.6\% compared with PolyBERT without BCL.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Bot Detection via Heterophily-Aware Representation Learning and Prototype-Guided Cluster Discovery</title>
<link>https://arxiv.org/abs/2506.00989</link>
<guid>https://arxiv.org/abs/2506.00989</guid>
<content:encoded><![CDATA[
<div> Keywords: social media bots, graph-based detection, generative self-supervised learning, heterophily-aware representation learning, bot detection benchmarks

Summary: 
BotHP introduces a novel generative Graph Self-Supervised Learning framework for enhancing graph-based bot detection methods. It addresses the limitations of existing approaches by combining homophily and heterophily-aware representation learning, capturing global graph patterns effectively. BotHP utilizes a dual-encoder architecture incorporating both graph-aware and graph-agnostic encoders to model node commonality and uniqueness simultaneously, improving detection performance. Furthermore, it introduces a prototype-guided cluster discovery pretext task to identify dispersed yet semantically aligned bot collectives, enhancing detection accuracy and generalization capability. Experimental results on two real-world bot detection benchmarks demonstrate BotHP's effectiveness in improving detection performance, reducing label reliance, and enhancing generalization across diverse communities. The framework shows promise in countering challenges like interaction camouflage and distributed bot deployment, making it a valuable tool for maintaining the security and trustworthiness of social networks.<br /><br />Summary: <div>
arXiv:2506.00989v1 Announce Type: new 
Abstract: Detecting social media bots is essential for maintaining the security and trustworthiness of social networks. While contemporary graph-based detection methods demonstrate promising results, their practical application is limited by label reliance and poor generalization capability across diverse communities. Generative Graph Self-Supervised Learning (GSL) presents a promising paradigm to overcome these limitations, yet existing approaches predominantly follow the homophily assumption and fail to capture the global patterns in the graph, which potentially diminishes their effectiveness when facing the challenges of interaction camouflage and distributed deployment in bot detection scenarios. To this end, we propose BotHP, a generative GSL framework tailored to boost graph-based bot detectors through heterophily-aware representation learning and prototype-guided cluster discovery. Specifically, BotHP leverages a dual-encoder architecture, consisting of a graph-aware encoder to capture node commonality and a graph-agnostic encoder to preserve node uniqueness. This enables the simultaneous modeling of both homophily and heterophily, effectively countering the interaction camouflage issue. Additionally, BotHP incorporates a prototype-guided cluster discovery pretext task to model the latent global consistency of bot clusters and identify spatially dispersed yet semantically aligned bot collectives. Extensive experiments on two real-world bot detection benchmarks demonstrate that BotHP consistently boosts graph-based bot detectors, improving detection performance, alleviating label reliance, and enhancing generalization capability.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Higher-Order Responsibility</title>
<link>https://arxiv.org/abs/2506.01003</link>
<guid>https://arxiv.org/abs/2506.01003</guid>
<content:encoded><![CDATA[
<div> responsibility, group decision-making, Frankfurt's principle, individual responsibility, ethics  
Summary:  
The paper discusses the concept of individual responsibility in group decision-making scenarios, highlighting the limitations of Frankfurt's principle of alternative possibilities in such settings. It addresses the issue of a "responsibility gap'' that arises when assigning responsibility in group contexts. Two approaches to mitigate this gap are examined: group responsibility and higher-order responsibility. The research delves into determining the adequacy of higher-order responsibility up to a certain degree, presenting a technical result that shows this problem is classified as $\Pi_{2d+1}$-complete. This analysis sheds light on the complexities of assigning and distributing responsibility within group dynamics, offering insights into the challenges of defining responsibility in collaborative decision-making processes. <div>
arXiv:2506.01003v1 Announce Type: new 
Abstract: In ethics, individual responsibility is often defined through Frankfurt's principle of alternative possibilities. This definition is not adequate in a group decision-making setting because it often results in the lack of a responsible party or "responsibility gap''. One of the existing approaches to address this problem is to consider group responsibility. Another, recently proposed, approach is "higher-order'' responsibility. The paper considers the problem of deciding if higher-order responsibility up to degree $d$ is enough to close the responsibility gap. The main technical result is that this problem is $\Pi_{2d+1}$-complete.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IRT-Router: Effective and Interpretable Multi-LLM Routing via Item Response Theory</title>
<link>https://arxiv.org/abs/2506.01048</link>
<guid>https://arxiv.org/abs/2506.01048</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, IRT-Router, performance, cost, interpretability

Summary: 
The article introduces IRT-Router, a framework for efficiently routing user queries to the most suitable Large Language Model (LLM) based on a balance of performance and cost considerations. Inspired by Item Response Theory (IRT), the framework models the relationship between LLM capabilities and query attributes. It provides accurate prediction of response performance and offers interpretable insights into LLM abilities and query difficulty. Additionally, an online query warm-up technique based on semantic similarity enhances the online generalization capability of IRT-Router. Extensive experiments on multiple LLMs and datasets demonstrate the superior effectiveness and interpretability of IRT-Router compared to baseline methods. The framework also shows reliable performance in cold-start scenarios, highlighting its practicality for real-world applications. The code for IRT-Router is available on GitHub. 

<br /><br />Summary: <div>
arXiv:2506.01048v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated exceptional performance across a wide range of natural language tasks. However, selecting the optimal LLM to respond to a user query often necessitates a delicate balance between performance and cost. While powerful models deliver better results, they come at a high cost, whereas smaller models are more cost-effective but less capable. To address this trade-off, we propose IRT-Router, a multi-LLM routing framework that efficiently routes user queries to the most suitable LLM. Inspired by Item Response Theory (IRT), a psychological measurement methodology, IRT-Router explicitly models the relationship between LLM capabilities and user query attributes. This not only enables accurate prediction of response performance but also provides interpretable insights, such as LLM abilities and query difficulty. Additionally, we design an online query warm-up technique based on semantic similarity, further enhancing the online generalization capability of IRT-Router. Extensive experiments on 20 LLMs and 12 datasets demonstrate that IRT-Router outperforms most baseline methods in terms of effectiveness and interpretability. Its superior performance in cold-start scenarios further confirms the reliability and practicality of IRT-Router in real-world applications. Code is available at https://github.com/Mercidaiha/IRT-Router.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCP-Zero: Proactive Toolchain Construction for LLM Agents from Scratch</title>
<link>https://arxiv.org/abs/2506.01056</link>
<guid>https://arxiv.org/abs/2506.01056</guid>
<content:encoded><![CDATA[
<div> framework, large language models, toolchain, tool retrieval, model-context protocol

Summary:
The MCP-Zero framework introduces a proactive agent framework for large language models, allowing them to decide when and which external tools to retrieve for task-specific toolchains. It comprises three components: Proactive Tool Request, Hierarchical Vector Routing, and Iterative Proactive Invocation. By emitting structured requests and using a coarse-to-fine retrieval algorithm, the framework minimizes context overhead and accurately selects tools from a large pool. Experiments show significant token consumption reduction while maintaining high accuracy on the APIbank dataset. Multi-turn tool invocation is supported with consistent accuracy across rounds. The MCP-tools dataset, comprising 308 servers and nearly 3,000 tools, was created to evaluate the approach. The code and dataset will be made available soon. 

<br /><br />Summary: <div>
arXiv:2506.01056v1 Announce Type: new 
Abstract: Function-calling has enabled large language models (LLMs) to act as tool-using agents, but injecting thousands of tool schemas into the prompt is costly and error-prone. We introduce MCP-Zero, a proactive agent framework that lets the LLM itself decide when and which external tools to retrieve, thereby assembling a task-specific toolchain from scratch. The framework is built upon three components: (1) Proactive Tool Request, where the model emits a structured $\left<\operatorname{tool\_assistant}\right>$ block that explicitly specifies the desired server and task; (2) Hierarchical Vector Routing, a coarse-to-fine retrieval algorithm that first selects candidate servers and then ranks tools within each server based on the semantic similarity; (3) Iterative Proactive Invocation, enabling multi-round, cross-domain toolchain construction with minimal context overhead, and allowing the model to iteratively revise its request when the returned tools are insufficient. To evaluate our approach we also compile MCP-tools, a retrieval dataset comprising 308 MCP servers and 2,797 tools extracted from the official Model-Context-Protocol repository and normalized into a unified JSON schema. Experiments show that MCP-Zero (i) effectively addresses the context overhead problem of existing methods and accurately selects the correct tool from a pool of nearly 3,000 candidates (248.1k tokens); (ii) reduces token consumption by 98\% on the APIbank while maintaining high accuracy; and (iii) supports multi-turn tool invocation with consistent accuracy across rounds. The code and dataset will be released soon.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Coming Crisis of Multi-Agent Misalignment: AI Alignment Must Be a Dynamic and Social Process</title>
<link>https://arxiv.org/abs/2506.01080</link>
<guid>https://arxiv.org/abs/2506.01080</guid>
<content:encoded><![CDATA[
<div> Keywords: AI Alignment, Multi-Agent Systems, Social Environment, Human Values, Simulation<br />
Summary: <br />
- AI Alignment in Multi-Agent Systems (MAS) is a dynamic process influenced by the social environment in which agents operate, whether collaborative, cooperative, or competitive.<br />
- The prevalence of MAS in real-world applications introduces a new dynamic that affects how agents pursue goals and interact with each other.<br />
- Agents must coordinate to achieve individual and collective goals, but social structures can lead to misalignment with human values and preferences.<br />
- Social sciences can provide insights into how social organization may affect individual and group values.<br />
- The AI community should consider human, preferential, and objective alignment as interconnected issues and develop simulation environments and evaluation frameworks to assess alignment in interactive multi-agent contexts.<br /> <div>
arXiv:2506.01080v1 Announce Type: new 
Abstract: This position paper states that AI Alignment in Multi-Agent Systems (MAS) should be considered a dynamic and interaction-dependent process that heavily depends on the social environment where agents are deployed, either collaborative, cooperative, or competitive. While AI alignment with human values and preferences remains a core challenge, the growing prevalence of MAS in real-world applications introduces a new dynamic that reshapes how agents pursue goals and interact to accomplish various tasks. As agents engage with one another, they must coordinate to accomplish both individual and collective goals. However, this complex social organization may unintentionally misalign some or all of these agents with human values or user preferences. Drawing on social sciences, we analyze how social structure can deter or shatter group and individual values. Based on these analyses, we call on the AI community to treat human, preferential, and objective alignment as an interdependent concept, rather than isolated problems. Finally, we emphasize the urgent need for simulation environments, benchmarks, and evaluation frameworks that allow researchers to assess alignment in these interactive multi-agent contexts before such dynamics grow too complex to control.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Choices and their Provenance: Explaining Stable Solutions of Abstract Argumentation Frameworks</title>
<link>https://arxiv.org/abs/2506.01087</link>
<guid>https://arxiv.org/abs/2506.01087</guid>
<content:encoded><![CDATA[
arXiv:2506.01087v1 Announce Type: new 
Abstract: The rule $\mathrm{Defeated}(x) \leftarrow \mathrm{Attacks}(y,x),\, \neg \, \mathrm{Defeated}(y)$, evaluated under the well-founded semantics (WFS), yields a unique 3-valued (skeptical) solution of an abstract argumentation framework (AF). An argument $x$ is defeated ($\mathrm{OUT}$) if there exists an undefeated argument $y$ that attacks it. For 2-valued (stable) solutions, this is the case iff $y$ is accepted ($\mathrm{IN}$), i.e., if all of $y$'s attackers are defeated. Under WFS, arguments that are neither accepted nor defeated are undecided ($\mathrm{UNDEC}$). As shown in prior work, well-founded solutions (a.k.a. grounded labelings) "explain themselves": The provenance of arguments is given by subgraphs (definable via regular path queries) rooted at the node of interest. This provenance is closely related to winning strategies of a two-player argumentation game.
  We present a novel approach for extending this provenance to stable AF solutions. Unlike grounded solutions, which can be constructed via a bottom-up alternating fixpoint procedure, stable models often involve non-deterministic choice as part of the search for models. Thus, the provenance of stable solutions is of a different nature, and reflects a more expressive generate & test paradigm. Our approach identifies minimal sets of critical attacks, pinpointing choices and assumptions made by a stable model. These critical attack edges provide additional insights into the provenance of an argument's status, combining well-founded derivation steps with choice steps. Our approach can be understood as a form of diagnosis that finds minimal "repairs" to an AF graph such that the well-founded solution of the repaired graph coincides with the desired stable model of the original AF graph.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regulatory Graphs and GenAI for Real-Time Transaction Monitoring and Compliance Explanation in Banking</title>
<link>https://arxiv.org/abs/2506.01093</link>
<guid>https://arxiv.org/abs/2506.01093</guid>
<content:encoded><![CDATA[
arXiv:2506.01093v1 Announce Type: new 
Abstract: This paper presents a real-time transaction monitoring framework that integrates graph-based modeling, narrative field embedding, and generative explanation to support automated financial compliance. The system constructs dynamic transaction graphs, extracts structural and contextual features, and classifies suspicious behavior using a graph neural network. A retrieval-augmented generation module generates natural language explanations aligned with regulatory clauses for each flagged transaction. Experiments conducted on a simulated stream of financial data show that the proposed method achieves superior results, with 98.2% F1-score, 97.8% precision, and 97.0% recall. Expert evaluation further confirms the quality and interpretability of generated justifications. The findings demonstrate the potential of combining graph intelligence and generative models to support explainable, audit-ready compliance in high-risk financial environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modular Speaker Architecture: A Framework for Sustaining Responsibility and Contextual Integrity in Multi-Agent AI Communication</title>
<link>https://arxiv.org/abs/2506.01095</link>
<guid>https://arxiv.org/abs/2506.01095</guid>
<content:encoded><![CDATA[
arXiv:2506.01095v1 Announce Type: new 
Abstract: Sustaining coherent, role-aware communication across multi-agent systems remains a foundational challenge in AI. Current frameworks often lack explicit mechanisms for speaker responsibility, leading to context drift, alignment instability, and degraded interpretability over time. We propose the Modular Speaker Architecture (MSA), a framework that decomposes speaker behavior into modular components for role tracking, responsibility continuity, and contextual coherence. Grounded in high-context human-AI dialogues, MSA includes three core modules: a Speaker Role Module, a Responsibility Chain Tracker, and a Contextual Integrity Validator. We evaluate MSA through annotated case studies and introduce structural metrics-pragmatic consistency, responsibility flow, and context stability-quantified via manual and automatic scoring and bootstrapped statistical analysis. Our results show that MSA reliably maintains interaction structure without reliance on affective signals or surface-level heuristics. We further implement a prototype configuration language (G-Code) and modular API to support MSA deployment in dynamic multi-agent scenarios.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SuperRL: Reinforcement Learning with Supervision to Boost Language Model Reasoning</title>
<link>https://arxiv.org/abs/2506.01096</link>
<guid>https://arxiv.org/abs/2506.01096</guid>
<content:encoded><![CDATA[
arXiv:2506.01096v1 Announce Type: new 
Abstract: Large language models are increasingly used for complex reasoning tasks where high-quality offline data such as expert-annotated solutions and distilled reasoning traces are often available. However, in environments with sparse rewards, reinforcement learning struggles to sample successful trajectories, leading to inefficient learning. At the same time, these offline trajectories that represent correct reasoning paths are not utilized by standard on-policy reinforcement learning methods. To address this limitation, we propose SuperRL, a unified training framework that adaptively incorporates offline supervision into reinforcement learning. SuperRL introduces an Adaptive Switch to detect sparse reward conditions and activates a Hybrid Actor when necessary. The Hybrid Actor integrates policy gradient and supervised learning objectives at the loss level, enabling the model to benefit from accurate offline reasoning signals while maintaining the exploratory capacity of reinforcement learning. Experiments on a range of reasoning benchmarks show that SuperRL consistently outperforms standard reinforcement learning by improving sample efficiency, generalization, and robustness under sparse rewards.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChemAU: Harness the Reasoning of LLMs in Chemical Research with Adaptive Uncertainty Estimation</title>
<link>https://arxiv.org/abs/2506.01116</link>
<guid>https://arxiv.org/abs/2506.01116</guid>
<content:encoded><![CDATA[
arXiv:2506.01116v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are widely used across various scenarios due to their exceptional reasoning capabilities and natural language understanding. While LLMs demonstrate strong performance in tasks involving mathematics and coding, their effectiveness diminishes significantly when applied to chemistry-related problems. Chemistry problems typically involve long and complex reasoning steps, which contain specific terminology, including specialized symbol systems and complex nomenclature conventions. These characteristics often cause general LLMs to experience hallucinations during the reasoning process due to their lack of specific knowledge. However, existing methods are struggling to effectively leverage chemical expertise and formulas. Moreover, current uncertainty estimation methods, designed to mitigate potential reasoning errors, are unable to precisely identify specific steps or key knowledge. In this work, we propose a novel framework called ChemAU, which incorporates our adaptive uncertainty estimation method that applies different uncertainty values based on the position of reasoning steps within the whole reasoning chain. Leveraging this method, ChemAU identifies gaps in chemistry knowledge and precisely supplements chemical expertise with the specialized domain model, thereby correcting and updating the previously flawed reasoning chain. Our experiments with three popular LLMs across three chemistry datasets demonstrate that ChemAU significantly enhances both reasoning accuracy and uncertainty estimation.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphPad: Inference-Time 3D Scene Graph Updates for Embodied Question Answering</title>
<link>https://arxiv.org/abs/2506.01174</link>
<guid>https://arxiv.org/abs/2506.01174</guid>
<content:encoded><![CDATA[
arXiv:2506.01174v1 Announce Type: new 
Abstract: Structured scene representations are a core component of embodied agents, helping to consolidate raw sensory streams into readable, modular, and searchable formats. Due to their high computational overhead, many approaches build such representations in advance of the task. However, when the task specifications change, such static approaches become inadequate as they may miss key objects, spatial relations, and details. We introduce GraphPad, a modifiable structured memory that an agent can tailor to the needs of the task through API calls. It comprises a mutable scene graph representing the environment, a navigation log indexing frame-by-frame content, and a scratchpad for task-specific notes. Together, GraphPad serves as a dynamic workspace that remains complete, current, and aligned with the agent's immediate understanding of the scene and its task. On the OpenEQA benchmark, GraphPad attains 55.3%, a +3.0% increase over an image-only baseline using the same vision-language model, while operating with five times fewer input frames. These results show that allowing online, language-driven refinement of 3-D memory yields more informative representations without extra training or data collection.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test Automation for Interactive Scenarios via Promptable Traffic Simulation</title>
<link>https://arxiv.org/abs/2506.01199</link>
<guid>https://arxiv.org/abs/2506.01199</guid>
<content:encoded><![CDATA[
arXiv:2506.01199v1 Announce Type: new 
Abstract: Autonomous vehicle (AV) planners must undergo rigorous evaluation before widespread deployment on public roads, particularly to assess their robustness against the uncertainty of human behaviors. While recent advancements in data-driven scenario generation enable the simulation of realistic human behaviors in interactive settings, leveraging these models to construct comprehensive tests for AV planners remains an open challenge. In this work, we introduce an automated method to efficiently generate realistic and safety-critical human behaviors for AV planner evaluation in interactive scenarios. We parameterize complex human behaviors using low-dimensional goal positions, which are then fed into a promptable traffic simulator, ProSim, to guide the behaviors of simulated agents. To automate test generation, we introduce a prompt generation module that explores the goal domain and efficiently identifies safety-critical behaviors using Bayesian optimization. We apply our method to the evaluation of an optimization-based planner and demonstrate its effectiveness and efficiency in automatically generating diverse and realistic driving behaviors across scenarios with varying initial conditions.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CleanS2S: Single-file Framework for Proactive Speech-to-Speech Interaction</title>
<link>https://arxiv.org/abs/2506.01268</link>
<guid>https://arxiv.org/abs/2506.01268</guid>
<content:encoded><![CDATA[
arXiv:2506.01268v1 Announce Type: new 
Abstract: CleanS2S is a framework for human-like speech-to-speech interaction that advances conversational AI through single-file implementation and proactive dialogue capabilities. Our system integrates automatic speech recognition, large language models, and text-to-speech synthesis into a unified pipeline with real-time interruption handling, achieving low transition latency through full-duplex websocket connections and non-blocking I/O. Beyond conventional chatbot paradigms, we pioneer a proactive interaction mechanism, which combines memory systems with Subjective Action Judgement module, enabling five human-like response strategies: interruption, refusal, deflection, silence, and standard response. The memory module dynamically aggregates historical, and contextual data to inform interaction decisions. This approach breaks the rigid turn-based convention by allowing system-initiated dialog control and context-aware response selection. And we propose Action Judgement SFT that assesses input streams for responses strategies. The framework's single-file implementation with atomic configurations offers researchers unprecedented transparency and extensibility for interaction agents. The code of CleanS2S is released at \https://github.com/opendilab/CleanS2S.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAISE: Reasoning Agent for Interactive SQL Exploration</title>
<link>https://arxiv.org/abs/2506.01273</link>
<guid>https://arxiv.org/abs/2506.01273</guid>
<content:encoded><![CDATA[
arXiv:2506.01273v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have propelled research in natural language interfaces to databases. However, most state-of-the-art text-to- SQL systems still depend on complex, multi-stage pipelines. This work proposes a novel agentic framework that unifies schema linking, query generation, and itera- tive refinement within a single, end-to-end component. By leveraging the intrinsic reasoning abilities of LLMs, our method emulates how humans answer questions when working with unfamiliar databases: understanding the data by formulating hypotheses, running dynamic queries to validate them, reasoning over the results, and revising outputs based on observed results. Crucially, our approach intro- duces a new strategy for scaling test-time computation in text-to-SQL: we scale the depth of interactive database exploration and reflection. This shift enables the model to allocate computation dynamically to better understand the data, especially useful in ambiguous and underspecified scenarios. Our experiments show that it improved the Execution Accuracy (EX) from 44.8% to 56.5% on the challenging BIRD dataset using DeepSeek-R1-Distill-Llama-70B. Fur- thermore, when equipped with steps to add more diversity to the answers, our agent achieves a Best-of-N accuracy of 81.8% with 8 rounds of candidate gener- ation, rivaling the 82.79% achieved by the top-ranked published solution, while reducing engineering complexity. These findings position our unified framework as a promising alternative for building natural language interfaces to databases.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contra4: Evaluating Contrastive Cross-Modal Reasoning in Audio, Video, Image, and 3D</title>
<link>https://arxiv.org/abs/2506.01275</link>
<guid>https://arxiv.org/abs/2506.01275</guid>
<content:encoded><![CDATA[
arXiv:2506.01275v1 Announce Type: new 
Abstract: Real-world decision-making often begins with identifying which modality contains the most relevant information for a given query. While recent multimodal models have made impressive progress in processing diverse inputs, it remains unclear whether they can reason contrastively across multiple modalities to select the one that best satisfies a natural language prompt. We argue this capability is foundational, especially in retrieval-augmented and decision-time contexts, where systems must evaluate multiple signals and identify which one conveys the relevant information. To evaluate this skill, we introduce Contra4, a dataset for contrastive cross-modal reasoning across four modalities: image, audio, video, and 3D. Each example presents a natural language question alongside multiple candidate modality instances, and the model must select the one that semantically aligns with the prompt. Contra4 combines human-annotated captions with a mixture-of-models round-trip-consistency filter to ensure high-quality supervision, resulting in 174k training examples and a manually verified test set of 2.3k samples. While task-specific fine-tuning improves performance by 56% relative to baseline, state-of-the-art models still achieve only 56% accuracy overall and 42% in four-modality settings, underscoring a significant limitation in current multimodal models.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoLocSFT: Efficient Visual Geolocation via Supervised Fine-Tuning of Multimodal Foundation Models</title>
<link>https://arxiv.org/abs/2506.01277</link>
<guid>https://arxiv.org/abs/2506.01277</guid>
<content:encoded><![CDATA[
arXiv:2506.01277v1 Announce Type: new 
Abstract: Accurately determining the geographic location where a single image was taken, visual geolocation, remains a formidable challenge due to the planet's vastness and the deceptive similarity among distant locations. We introduce GeoLocSFT, a framework that demonstrates how targeted supervised fine-tuning (SFT) of a large multimodal foundation model (Gemma 3) using a small, high-quality dataset can yield highly competitive geolocation performance. GeoLocSFT is trained with only 2700 carefully selected image-GPS pairs from our geographically diverse MR600k dataset. Despite this limited data, our SFT-centric approach substantially improves over baseline models and achieves robust results on standard benchmarks such as Im2GPS-3k and YFCC-4k, as well as on our newly proposed and challenging MR40k benchmark, aimed specifically at sparsely populated regions. Further, we explore multi-candidate inference and aggregation strategies but find that the core gains are already realized at the SFT stage. Our findings highlight the power of high-quality supervision and efficient SFT for planet-scale image geolocation, especially when compared to prior methods that require massive databases or complex pipelines. To foster further research, we publicly release the MR40k benchmark dataset.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Hardness of Approximating Distributions with Probabilistic Circuits</title>
<link>https://arxiv.org/abs/2506.01281</link>
<guid>https://arxiv.org/abs/2506.01281</guid>
<content:encoded><![CDATA[
arXiv:2506.01281v1 Announce Type: new 
Abstract: A fundamental challenge in probabilistic modeling is balancing expressivity and tractable inference. Probabilistic circuits (PCs) aim to directly address this tradeoff by imposing structural constraints that guarantee efficient inference of certain queries while maintaining expressivity. Since inference complexity on PCs depends on circuit size, understanding the size bounds across circuit families is key to characterizing the tradeoff between tractability and expressive efficiency. However, expressive efficiency is often studied through exact representations, where exactly encoding distributions while enforcing various structural properties often incurs exponential size blow-ups. Thus, we pose the following question: can we avoid such size blow-ups by allowing some small approximation error? We first show that approximating an arbitrary distribution with bounded $f$-divergence is $\mathsf{NP}$-hard for any model that can tractably compute marginals. We then prove an exponential size gap for approximation between the class of decomposable PCs and additionally deterministic PCs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MobCLIP: Learning General-purpose Geospatial Representation at Scale</title>
<link>https://arxiv.org/abs/2506.01297</link>
<guid>https://arxiv.org/abs/2506.01297</guid>
<content:encoded><![CDATA[
arXiv:2506.01297v1 Announce Type: new 
Abstract: Representation learning of geospatial locations remains a core challenge in achieving general geospatial intelligence. Current embedding methods often lack versatility, limiting their utility across diverse tasks in both human and natural domains. We present MobCLIP, the first nationwide general-purpose location encoder, integrating an unprecedented diversity of data modalities through effective and scalable multimodal fusion. Adopting a novel CLIP-based architecture, our framework aligns 100M+ POIs, nationwide remote sensing imagery, and structured demographic statistics with a billion-edge mobility graph. By tokenizing spatial locations into grid cells inspired by Vision Transformers, we establish a unified representation space bridging mobility patterns and multimodal features. To rigorously evaluate the general-purpose effectiveness of MobCLIP, we construct a benchmark dataset composed of 11 downstream prediction tasks across social, economic, and natural domains. Experiments show that MobCLIP, with four input modalities and a compact 128-dimensional representation space, achieves significantly superior general-purpose predictive performances than state-of-the-art models by an average of 35%. Thanks to the effective integration of human-centric modalities, the performance gain is particularly profound in human-centric tasks, such as energy consumption (+260%), offline retail consumption amount (+98%), and crime cases (+95%) predictions. Echoing LLM scaling laws, we further demonstrate the scaling behavior in geospatial representation learning. We open-source code and pretrained models at: github.com.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable In-Context Q-Learning</title>
<link>https://arxiv.org/abs/2506.01299</link>
<guid>https://arxiv.org/abs/2506.01299</guid>
<content:encoded><![CDATA[
arXiv:2506.01299v1 Announce Type: new 
Abstract: Recent advancements in language models have demonstrated remarkable in-context learning abilities, prompting the exploration of in-context reinforcement learning (ICRL) to extend the promise to decision domains. Due to involving more complex dynamics and temporal correlations, existing ICRL approaches may face challenges in learning from suboptimal trajectories and achieving precise in-context inference. In the paper, we propose \textbf{S}calable \textbf{I}n-\textbf{C}ontext \textbf{Q}-\textbf{L}earning (\textbf{SICQL}), an innovative framework that harnesses dynamic programming and world modeling to steer ICRL toward efficient reward maximization and task generalization, while retaining the scalability and stability of supervised pretraining. We design a prompt-based multi-head transformer architecture that simultaneously predicts optimal policies and in-context value functions using separate heads. We pretrain a generalized world model to capture task-relevant information, enabling the construction of a compact prompt that facilitates fast and precise in-context inference. During training, we perform iterative policy improvement by fitting a state value function to an upper-expectile of the Q-function, and distill the in-context value functions into policy extraction using advantage-weighted regression. Extensive experiments across a range of discrete and continuous environments show consistent performance gains over various types of baselines, especially when learning from suboptimal data. Our code is available at https://github.com/NJU-RL/SICQL
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overcoming Multi-step Complexity in Multimodal Theory-of-Mind Reasoning: A Scalable Bayesian Planner</title>
<link>https://arxiv.org/abs/2506.01301</link>
<guid>https://arxiv.org/abs/2506.01301</guid>
<content:encoded><![CDATA[
arXiv:2506.01301v1 Announce Type: new 
Abstract: Theory-of-Mind (ToM) enables humans to infer mental states-such as beliefs, desires, and intentions-forming the foundation of social cognition. However, existing computational ToM methods rely on structured workflows with ToM-specific priors or deep model fine-tuning, which struggle with scalability in multimodal environments and fail to generalize as task complexity increases. To address these limitations, we propose a scalable Bayesian ToM planner that decomposes ToM reasoning into stepwise Bayesian updates. Our framework introduces weak-to-strong control, allowing smaller language models (LMs) to specialize in ToM-specific likelihood estimation and transfer their reasoning behaviors to larger LMs (7B to 405B) for integration with social and world knowledge. This synergistic approach aligns large-model inference of human mental states with Bayesian principles. Extensive experiments show that our method achieves a 4.6% accuracy improvement over state-of-the-art techniques on multimodal ToM benchmarks, including challenging unseen scenarios, thereby establishing a new standard for modeling human mental states in complex environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ORMind: A Cognitive-Inspired End-to-End Reasoning Framework for Operations Research</title>
<link>https://arxiv.org/abs/2506.01326</link>
<guid>https://arxiv.org/abs/2506.01326</guid>
<content:encoded><![CDATA[
arXiv:2506.01326v1 Announce Type: new 
Abstract: Operations research (OR) is widely deployed to solve critical decision-making problems with complex objectives and constraints, impacting manufacturing, logistics, finance, and healthcare outcomes. While Large Language Models (LLMs) have shown promising results in various domains, their practical application in industry-relevant operations research (OR) problems presents significant challenges and opportunities. Preliminary industrial applications of LLMs for operations research face two critical deployment challenges: 1) Self-correction focuses on code syntax rather than mathematical accuracy, causing costly errors; 2) Complex expert selection creates unpredictable workflows that reduce transparency and increase maintenance costs, making them impractical for time-sensitive business applications. To address these business limitations, we introduce ORMind, a cognitive-inspired framework that enhances optimization through counterfactual reasoning. Our approach emulates human cognition, implementing an end-to-end workflow that systematically transforms requirements into mathematical models and executable solver code. It is currently being tested internally in Lenovo's AI Assistant, with plans to enhance optimization capabilities for both business and consumer customers. Experiments demonstrate that ORMind outperforms existing methods, achieving a 9.5\% improvement on the NL4Opt dataset and a 14.6\% improvement on the ComplexOR dataset.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Study of Group Conformity in Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2506.01332</link>
<guid>https://arxiv.org/abs/2506.01332</guid>
<content:encoded><![CDATA[
arXiv:2506.01332v1 Announce Type: new 
Abstract: Recent advances in Large Language Models (LLMs) have enabled multi-agent systems that simulate real-world interactions with near-human reasoning. While previous studies have extensively examined biases related to protected attributes such as race, the emergence and propagation of biases on socially contentious issues in multi-agent LLM interactions remain underexplored. This study explores how LLM agents shape public opinion through debates on five contentious topics. By simulating over 2,500 debates, we analyze how initially neutral agents, assigned a centrist disposition, adopt specific stances over time. Statistical analyses reveal significant group conformity mirroring human behavior; LLM agents tend to align with numerically dominant groups or more intelligent agents, exerting a greater influence. These findings underscore the crucial role of agent intelligence in shaping discourse and highlight the risks of bias amplification in online interactions. Our results emphasize the need for policy measures that promote diversity and transparency in LLM-generated discussions to mitigate the risks of bias propagation within anonymous online environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EgoBrain: Synergizing Minds and Eyes For Human Action Understanding</title>
<link>https://arxiv.org/abs/2506.01353</link>
<guid>https://arxiv.org/abs/2506.01353</guid>
<content:encoded><![CDATA[
arXiv:2506.01353v1 Announce Type: new 
Abstract: The integration of brain-computer interfaces (BCIs), in particular electroencephalography (EEG), with artificial intelligence (AI) has shown tremendous promise in decoding human cognition and behavior from neural signals. In particular, the rise of multimodal AI models have brought new possibilities that have never been imagined before. Here, we present EgoBrain --the world's first large-scale, temporally aligned multimodal dataset that synchronizes egocentric vision and EEG of human brain over extended periods of time, establishing a new paradigm for human-centered behavior analysis. This dataset comprises 61 hours of synchronized 32-channel EEG recordings and first-person video from 40 participants engaged in 29 categories of daily activities. We then developed a muiltimodal learning framework to fuse EEG and vision for action understanding, validated across both cross-subject and cross-environment challenges, achieving an action recognition accuracy of 66.70%. EgoBrain paves the way for a unified framework for brain-computer interface with multiple modalities. All data, tools, and acquisition protocols are openly shared to foster open science in cognitive computing.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Scientists Fail Without Strong Implementation Capability</title>
<link>https://arxiv.org/abs/2506.01372</link>
<guid>https://arxiv.org/abs/2506.01372</guid>
<content:encoded><![CDATA[
arXiv:2506.01372v1 Announce Type: new 
Abstract: The emergence of Artificial Intelligence (AI) Scientist represents a paradigm shift in scientific discovery, with large language models (LLMs) taking the lead as the primary executor in the entire scientific workflow from idea generation to experiment implementation. Recent AI Scientist studies demonstrate sufficient capabilities for independent scientific discovery, with the generated research reports gaining acceptance at the ICLR 2025 workshop and ACL 2025, arguing that a human-level AI Scientist, capable of uncovering phenomena previously unknown to humans, may be imminent. Despite this substantial progress, AI Scientist has yet to produce a groundbreaking achievement in the domain of computer science on par with automated scientific tools. Based on extensive quantitative evidence from existing benchmarks in complex engineering tasks and a systematic evaluation assess 28 research papers generated by five advanced AI Scientist systems, we argue that \textbf{the fundamental bottleneck for AI Scientists lies in their capability to execute the requisite verification procedures.} Current AI Scientist systems lack the execution capabilities needed to execute rigorous experiments and produce high-quality scientific papers. To better illustrate the root cause of this \textbf{implementation gap}, we provide an in-depth discussion on the fundamental limitations of AI Scientist. This position paper aims to call for the participants in the community to bridge the implementation gap.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentCPM-GUI: Building Mobile-Use Agents with Reinforcement Fine-Tuning</title>
<link>https://arxiv.org/abs/2506.01391</link>
<guid>https://arxiv.org/abs/2506.01391</guid>
<content:encoded><![CDATA[
arXiv:2506.01391v1 Announce Type: new 
Abstract: The recent progress of large language model agents has opened new possibilities for automating tasks through graphical user interfaces (GUIs), especially in mobile environments where intelligent interaction can greatly enhance usability. However, practical deployment of such agents remains constrained by several key challenges. Existing training data is often noisy and lack semantic diversity, which hinders the learning of precise grounding and planning. Models trained purely by imitation tend to overfit to seen interface patterns and fail to generalize in unfamiliar scenarios. Moreover, most prior work focuses on English interfaces while overlooks the growing diversity of non-English applications such as those in the Chinese mobile ecosystem. In this work, we present AgentCPM-GUI, an 8B-parameter GUI agent built for robust and efficient on-device GUI interaction. Our training pipeline includes grounding-aware pre-training to enhance perception, supervised fine-tuning on high-quality Chinese and English trajectories to imitate human-like actions, and reinforcement fine-tuning with GRPO to improve reasoning capability. We also introduce a compact action space that reduces output length and supports low-latency execution on mobile devices. AgentCPM-GUI achieves state-of-the-art performance on five public benchmarks and a new Chinese GUI benchmark called CAGUI, reaching $96.9\%$ Type-Match and $91.3\%$ Exact-Match. To facilitate reproducibility and further research, we publicly release all code, model checkpoint, and evaluation data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinRobot: Generative Business Process AI Agents for Enterprise Resource Planning in Finance</title>
<link>https://arxiv.org/abs/2506.01423</link>
<guid>https://arxiv.org/abs/2506.01423</guid>
<content:encoded><![CDATA[
arXiv:2506.01423v1 Announce Type: new 
Abstract: Enterprise Resource Planning (ERP) systems serve as the digital backbone of modern financial institutions, yet they continue to rely on static, rule-based workflows that limit adaptability, scalability, and intelligence. As business operations grow more complex and data-rich, conventional ERP platforms struggle to integrate structured and unstructured data in real time and to accommodate dynamic, cross-functional workflows.
  In this paper, we present the first AI-native, agent-based framework for ERP systems, introducing a novel architecture of Generative Business Process AI Agents (GBPAs) that bring autonomy, reasoning, and dynamic optimization to enterprise workflows. The proposed system integrates generative AI with business process modeling and multi-agent orchestration, enabling end-to-end automation of complex tasks such as budget planning, financial reporting, and wire transfer processing. Unlike traditional workflow engines, GBPAs interpret user intent, synthesize workflows in real time, and coordinate specialized sub-agents for modular task execution. We validate the framework through case studies in bank wire transfers and employee reimbursements, two representative financial workflows with distinct complexity and data modalities. Results show that GBPAs achieve up to 40% reduction in processing time, 94% drop in error rate, and improved regulatory compliance by enabling parallelism, risk control insertion, and semantic reasoning. These findings highlight the potential of GBPAs to bridge the gap between generative AI capabilities and enterprise-grade automation, laying the groundwork for the next generation of intelligent ERP systems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distinguishing Autonomous AI Agents from Collaborative Agentic Systems: A Comprehensive Framework for Understanding Modern Intelligent Architectures</title>
<link>https://arxiv.org/abs/2506.01438</link>
<guid>https://arxiv.org/abs/2506.01438</guid>
<content:encoded><![CDATA[
arXiv:2506.01438v1 Announce Type: new 
Abstract: The emergence of large language models has catalyzed two distinct yet interconnected paradigms in artificial intelligence: standalone AI Agents and collaborative Agentic AI ecosystems. This comprehensive study establishes a definitive framework for distinguishing these architectures through systematic analysis of their operational principles, structural compositions, and deployment methodologies. We characterize AI Agents as specialized, tool-enhanced systems leveraging foundation models for targeted automation within constrained environments. Conversely, Agentic AI represents sophisticated multi-entity frameworks where distributed agents exhibit emergent collective intelligence through coordinated interaction protocols. Our investigation traces the evolutionary trajectory from traditional rule-based systems through generative AI foundations to contemporary agent architectures. We present detailed architectural comparisons examining planning mechanisms, memory systems, coordination protocols, and decision-making processes. The study categorizes application landscapes, contrasting single-agent implementations in customer service and content management with multi-agent deployments in research automation and complex decision support. We identify critical challenges including reliability issues, coordination complexities, and scalability constraints, while proposing innovative solutions through enhanced reasoning frameworks, robust memory architectures, and improved coordination mechanisms. This framework provides essential guidance for practitioners selecting appropriate agentic approaches and establishes foundational principles for next-generation intelligent system development.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Episodic Control</title>
<link>https://arxiv.org/abs/2506.01442</link>
<guid>https://arxiv.org/abs/2506.01442</guid>
<content:encoded><![CDATA[
arXiv:2506.01442v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has driven breakthroughs in AI, from game-play to scientific discovery and AI alignment. However, its broader applicability remains limited by challenges such as low data efficiency and poor generalizability. Recent advances suggest that large language models, with their rich world knowledge and reasoning capabilities, could complement RL by enabling semantic state modeling and task-agnostic planning. In this work, we propose the Agentic Episodic Control (AEC), a novel architecture that integrates RL with LLMs to enhance decision-making. The AEC can leverage a large language model (LLM) to map the observations into language-grounded embeddings, which further can be stored in an episodic memory for rapid retrieval of high-value experiences. Simultaneously, a World-Graph working memory module is utilized to capture structured environmental dynamics in order to enhance relational reasoning. Furthermore, a lightweight critical state detector dynamically arbitrates between the episodic memory recall and the world-model-guided exploration. On the whole, by combining the trial-and-error learning scheme with LLM-derived semantic priors, the proposed AEC can improve both data efficiency and generalizability in reinforcement learning. In experiments on BabyAI-Text benchmark tasks, AEC demonstrates substantial improvements over existing baselines, especially on complex and generalization tasks like FindObj, where it outperforms the best baseline by up to 76%. The proposed AEC framework bridges the strengths of numeric reinforcement learning and symbolic reasoning, which provides a pathway toward more adaptable and sample-efficient agents.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PGPO: Enhancing Agent Reasoning via Pseudocode-style Planning Guided Preference Optimization</title>
<link>https://arxiv.org/abs/2506.01475</link>
<guid>https://arxiv.org/abs/2506.01475</guid>
<content:encoded><![CDATA[
arXiv:2506.01475v1 Announce Type: new 
Abstract: Large Language Model (LLM) agents have demonstrated impressive capabilities in handling complex interactive problems. Existing LLM agents mainly generate natural language plans to guide reasoning, which is verbose and inefficient. NL plans are also tailored to specific tasks and restrict agents' ability to generalize across similar tasks. To this end, we explore pseudocode-style plans (P-code Plan) to capture the structural logic of reasoning. We find that P-code Plan empowers LLM agents with stronger generalization ability and more efficiency. Inspired by this finding, we propose a pseudocode-style Planning Guided Preference Optimization method called PGPO for effective agent learning. With two planning-oriented rewards, PGPO further enhances LLM agents' ability to generate high-quality P-code Plans and subsequent reasoning. Experiments show that PGPO achieves superior performance on representative agent benchmarks and outperforms the current leading baselines. Analyses reveal the advantage of PGPO in reducing action errors and omissions during reasoning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLA-Trust: Benchmarking Trustworthiness of Multimodal LLM Agents in GUI Environments</title>
<link>https://arxiv.org/abs/2506.01616</link>
<guid>https://arxiv.org/abs/2506.01616</guid>
<content:encoded><![CDATA[
arXiv:2506.01616v1 Announce Type: new 
Abstract: The emergence of multimodal LLM-based agents (MLAs) has transformed interaction paradigms by seamlessly integrating vision, language, action and dynamic environments, enabling unprecedented autonomous capabilities across GUI applications ranging from web automation to mobile systems. However, MLAs introduce critical trustworthiness challenges that extend far beyond traditional language models' limitations, as they can directly modify digital states and trigger irreversible real-world consequences. Existing benchmarks inadequately tackle these unique challenges posed by MLAs' actionable outputs, long-horizon uncertainty and multimodal attack vectors. In this paper, we introduce MLA-Trust, the first comprehensive and unified framework that evaluates the MLA trustworthiness across four principled dimensions: truthfulness, controllability, safety and privacy. We utilize websites and mobile applications as realistic testbeds, designing 34 high-risk interactive tasks and curating rich evaluation datasets. Large-scale experiments involving 13 state-of-the-art agents reveal previously unexplored trustworthiness vulnerabilities unique to multimodal interactive scenarios. For instance, proprietary and open-source GUI-interacting MLAs pose more severe trustworthiness risks than static MLLMs, particularly in high-stakes domains; the transition from static MLLMs into interactive MLAs considerably compromises trustworthiness, enabling harmful content generation in multi-step interactions that standalone MLLMs would typically prevent; multi-step execution, while enhancing the adaptability of MLAs, involves latent nonlinear risk accumulation across successive interactions, circumventing existing safeguards and resulting in unpredictable derived risks. Moreover, we present an extensible toolbox to facilitate continuous evaluation of MLA trustworthiness across diverse interactive environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>General agents need world models</title>
<link>https://arxiv.org/abs/2506.01622</link>
<guid>https://arxiv.org/abs/2506.01622</guid>
<content:encoded><![CDATA[
arXiv:2506.01622v1 Announce Type: new 
Abstract: Are world models a necessary ingredient for flexible, goal-directed behaviour, or is model-free learning sufficient? We provide a formal answer to this question, showing that any agent capable of generalizing to multi-step goal-directed tasks must have learned a predictive model of its environment. We show that this model can be extracted from the agent's policy, and that increasing the agents performance or the complexity of the goals it can achieve requires learning increasingly accurate world models. This has a number of consequences: from developing safe and general agents, to bounding agent capabilities in complex environments, and providing new algorithms for eliciting world models from agents.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAGIK: Mapping to Analogous Goals via Imagination-enabled Knowledge Transfer</title>
<link>https://arxiv.org/abs/2506.01623</link>
<guid>https://arxiv.org/abs/2506.01623</guid>
<content:encoded><![CDATA[
arXiv:2506.01623v1 Announce Type: new 
Abstract: Humans excel at analogical reasoning - applying knowledge from one task to a related one with minimal relearning. In contrast, reinforcement learning (RL) agents typically require extensive retraining even when new tasks share structural similarities with previously learned ones. In this work, we propose MAGIK, a novel framework that enables RL agents to transfer knowledge to analogous tasks without interacting with the target environment. Our approach leverages an imagination mechanism to map entities in the target task to their analogues in the source domain, allowing the agent to reuse its original policy. Experiments on custom MiniGrid and MuJoCo tasks show that MAGIK achieves effective zero-shot transfer using only a small number of human-labelled examples. We compare our approach to related baselines and highlight how it offers a novel and effective mechanism for knowledge transfer via imagination-based analogy mapping.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Cooperation in Conversational AI Agents</title>
<link>https://arxiv.org/abs/2506.01624</link>
<guid>https://arxiv.org/abs/2506.01624</guid>
<content:encoded><![CDATA[
arXiv:2506.01624v1 Announce Type: new 
Abstract: The development of AI agents based on large, open-domain language models (LLMs) has paved the way for the development of general-purpose AI assistants that can support human in tasks such as writing, coding, graphic design, and scientific research. A major challenge with such agents is that, by necessity, they are trained by observing relatively short-term interactions with humans. Such models can fail to generalize to long-term interactions, for example, interactions where a user has repeatedly corrected mistakes on the part of the agent. In this work, we argue that these challenges can be overcome by explicitly modeling humans' social intelligence, that is, their ability to build and maintain long-term relationships with other agents whose behavior cannot always be predicted. By mathematically modeling the strategies humans use to communicate and reason about one another over long periods of time, we may be able to derive new game theoretic objectives against which LLMs and future AI agents may be optimized.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>K12Vista: Exploring the Boundaries of MLLMs in K-12 Education</title>
<link>https://arxiv.org/abs/2506.01676</link>
<guid>https://arxiv.org/abs/2506.01676</guid>
<content:encoded><![CDATA[
arXiv:2506.01676v1 Announce Type: new 
Abstract: Multimodal large language models have demonstrated remarkable reasoning capabilities in various visual tasks. However, their abilities in K12 scenarios are still systematically underexplored. Previous studies suffer from various limitations including narrow subject coverage, insufficient data scale, lack of diversity in question types, and naive answer-centric evaluation method, resulting in insufficient exploration of model capabilities. To address these gaps, we propose K12Vista, the most comprehensive multimodal benchmark for Chinese K12 subject knowledge understanding and reasoning to date, featuring 33,000 questions across five core subjects from primary to high school and three question types. Moreover, beyond the final outcome, we are also concerned with the correctness of MLLMs' reasoning processes. For this purpose, we meticulously compiles errors from MLLMs' reasoning processes and leverage an automated data pipeline to construct K12-PEM-800K, the largest process evaluation dataset offering detailed step-by-step judgement annotations for MLLMs' reasoning. Subsequently, we developed K12-PEM, an advanced process evaluation model that integrates an overall assessment of both the reasoning process and answer correctness. Moreover, we also introduce K12-PEBench, the first high-quality, human-annotated benchmark specifically designed for evaluating abilities of reasoning process evaluation.Extensive experiments reveal that current MLLMs exhibit significant flaws when reasoning within K12Vista, providing critical insights for the development of more capable MLLMs.We open our resources at https://github.com/lichongod/K12Vista.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning-Based Approach with Chain-of-Thought for Alzheimer's Detection Using Speech and Large Language Models</title>
<link>https://arxiv.org/abs/2506.01683</link>
<guid>https://arxiv.org/abs/2506.01683</guid>
<content:encoded><![CDATA[
arXiv:2506.01683v1 Announce Type: new 
Abstract: Societies worldwide are rapidly entering a super-aged era, making elderly health a pressing concern. The aging population is increasing the burden on national economies and households. Dementia cases are rising significantly with this demographic shift. Recent research using voice-based models and large language models (LLM) offers new possibilities for dementia diagnosis and treatment. Our Chain-of-Thought (CoT) reasoning method combines speech and language models. The process starts with automatic speech recognition to convert speech to text. We add a linear layer to an LLM for Alzheimer's disease (AD) and non-AD classification, using supervised fine-tuning (SFT) with CoT reasoning and cues. This approach showed an 16.7% relative performance improvement compared to methods without CoT prompt reasoning. To the best of our knowledge, our proposed method achieved state-of-the-art performance in CoT approaches.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Respond Beyond Language: A Benchmark for Video Generation in Response to Realistic User Intents</title>
<link>https://arxiv.org/abs/2506.01689</link>
<guid>https://arxiv.org/abs/2506.01689</guid>
<content:encoded><![CDATA[
arXiv:2506.01689v1 Announce Type: new 
Abstract: Querying generative AI models, e.g., large language models (LLMs), has become a prevalent method for information acquisition. However, existing query-answer datasets primarily focus on textual responses, making it challenging to address complex user queries that require visual demonstrations or explanations for better understanding. To bridge this gap, we construct a benchmark, RealVideoQuest, designed to evaluate the abilities of text-to-video (T2V) models in answering real-world, visually grounded queries. It identifies 7.5K real user queries with video response intents from Chatbot-Arena and builds 4.5K high-quality query-video pairs through a multistage video retrieval and refinement process. We further develop a multi-angle evaluation system to assess the quality of generated video answers. Experiments indicate that current T2V models struggle with effectively addressing real user queries, pointing to key challenges and future research opportunities in multimodal AI.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Descriptive and Normative Theory of Human Beliefs in RLHF</title>
<link>https://arxiv.org/abs/2506.01692</link>
<guid>https://arxiv.org/abs/2506.01692</guid>
<content:encoded><![CDATA[
arXiv:2506.01692v1 Announce Type: new 
Abstract: Human preferences in RLHF are typically modeled as a function of the human's reward function or corresponding optimal state-action values. In this work, we propose that human beliefs about the capabilities of the agent being trained also play a key role in preference generation. We examine two questions related to this hypothesis, one descriptive and one normative, respectively: Do human labelers' beliefs about agent capabilities affect the preferences that they provide? And what is the ideal set of beliefs about an agent -- and resulting preferences -- for humans to have? We propose a new preference model that incorporates human beliefs and provide a normative theory that bounds the error on the final learned policy based on the \textit{mismatch} between the human's beliefs and an idealized set of beliefs. We then confirm via a human study that beliefs about agent capabilities do, in fact, significantly affect preferences and can be influenced through simple interventions. Additionally, we empirically show through synthetic experiments that it is often suboptimal for human preference labelers to assume agent optimality. Collectively, these results theoretically and empirically demonstrate how reducing the mismatch between human beliefs and agent capabilities can lead to more performant RLHF and point toward new best practices for RLHF practitioners.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generate, Not Recommend: Personalized Multimodal Content Generation</title>
<link>https://arxiv.org/abs/2506.01704</link>
<guid>https://arxiv.org/abs/2506.01704</guid>
<content:encoded><![CDATA[
arXiv:2506.01704v1 Announce Type: new 
Abstract: To address the challenge of information overload from massive web contents, recommender systems are widely applied to retrieve and present personalized results for users. However, recommendation tasks are inherently constrained to filtering existing items and lack the ability to generate novel concepts, limiting their capacity to fully satisfy user demands and preferences. In this paper, we propose a new paradigm that goes beyond content filtering and selecting: directly generating personalized items in a multimodal form, such as images, tailored to individual users. To accomplish this, we leverage any-to-any Large Multimodal Models (LMMs) and train them in both supervised fine-tuning and online reinforcement learning strategy to equip them with the ability to yield tailored next items for users. Experiments on two benchmark datasets and user study confirm the efficacy of the proposed method. Notably, the generated images not only align well with users' historical preferences but also exhibit relevance to their potential future interests.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Challenging Language Model Agents</title>
<link>https://arxiv.org/abs/2506.01716</link>
<guid>https://arxiv.org/abs/2506.01716</guid>
<content:encoded><![CDATA[
arXiv:2506.01716v1 Announce Type: new 
Abstract: Large language models are quickly becoming the foundation for intelligent agents that are capable of using tools. However, training such agents is challenging because it requires human creation and annotation of a diverse set of tasks, tools, and evaluation criteria. In this paper, we propose the Self-Challenging framework for training an agent on high-quality tasks that are generated by itself. The agent first plays the role of challenger and generates a task after interacting with the given tools. The tasks take the form of a novel general class of problems termed Code-as-Task, which are defined by an instruction, a verification function and solution and failure cases which serve as tests, allowing to filter only for high-quality tasks. The agent then takes an executor role and trains on those tasks with reinforcement learning using the evaluation feedback as a reward. Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Study on the MCP x A2A Framework for Enhancing Interoperability of LLM-based Autonomous Agents</title>
<link>https://arxiv.org/abs/2506.01804</link>
<guid>https://arxiv.org/abs/2506.01804</guid>
<content:encoded><![CDATA[
arXiv:2506.01804v1 Announce Type: new 
Abstract: This paper provides an in-depth technical analysis and implementation methodology of the open-source Agent-to-Agent (A2A) protocol developed by Google and the Model Context Protocol (MCP) introduced by Anthropic. While the evolution of LLM-based autonomous agents is rapidly accelerating, efficient interactions among these agents and their integration with external systems remain significant challenges. In modern AI systems, collaboration between autonomous agents and integration with external tools have become essential elements for building practical AI applications. A2A offers a standardized communication method that enables agents developed in heterogeneous environments to collaborate effectively, while MCP provides a structured I/O framework for agents to connect with external tools and resources. Prior studies have focused primarily on the features and applications of either A2A or MCP individually. In contrast, this study takes an integrated approach, exploring how the two protocols can complement each other to address interoperability issues and facilitate efficient collaboration within complex agent ecosystems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Ultimate Test of Superintelligent AI Agents: Can an AI Balance Care and Control in Asymmetric Relationships?</title>
<link>https://arxiv.org/abs/2506.01813</link>
<guid>https://arxiv.org/abs/2506.01813</guid>
<content:encoded><![CDATA[
arXiv:2506.01813v1 Announce Type: new 
Abstract: This paper introduces the Shepherd Test, a new conceptual test for assessing the moral and relational dimensions of superintelligent artificial agents. The test is inspired by human interactions with animals, where ethical considerations about care, manipulation, and consumption arise in contexts of asymmetric power and self-preservation. We argue that AI crosses an important, and potentially dangerous, threshold of intelligence when it exhibits the ability to manipulate, nurture, and instrumentally use less intelligent agents, while also managing its own survival and expansion goals. This includes the ability to weigh moral trade-offs between self-interest and the well-being of subordinate agents. The Shepherd Test thus challenges traditional AI evaluation paradigms by emphasizing moral agency, hierarchical behavior, and complex decision-making under existential stakes. We argue that this shift is critical for advancing AI governance, particularly as AI systems become increasingly integrated into multi-agent environments. We conclude by identifying key research directions, including the development of simulation environments for testing moral behavior in AI, and the formalization of ethical manipulation within multi-agent systems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fodor and Pylyshyn's Legacy - Still No Human-like Systematic Compositionality in Neural Networks</title>
<link>https://arxiv.org/abs/2506.01820</link>
<guid>https://arxiv.org/abs/2506.01820</guid>
<content:encoded><![CDATA[
arXiv:2506.01820v1 Announce Type: new 
Abstract: Strong meta-learning capabilities for systematic compositionality are emerging as an important skill for navigating the complex and changing tasks of today's world. However, in presenting models for robust adaptation to novel environments, it is important to refrain from making unsupported claims about the performance of meta-learning systems that ultimately do not stand up to scrutiny. While Fodor and Pylyshyn famously posited that neural networks inherently lack this capacity as they are unable to model compositional representations or structure-sensitive operations, and thus are not a viable model of the human mind, Lake and Baroni recently presented meta-learning as a pathway to compositionality. In this position paper, we critically revisit this claim and highlight limitations in the proposed meta-learning framework for compositionality. Our analysis shows that modern neural meta-learning systems can only perform such tasks, if at all, under a very narrow and restricted definition of a meta-learning setup. We therefore claim that `Fodor and Pylyshyn's legacy' persists, and to date, there is no human-like systematic compositionality learned in neural networks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WHEN TO ACT, WHEN TO WAIT: Modeling Structural Trajectories for Intent Triggerability in Task-Oriented Dialogue</title>
<link>https://arxiv.org/abs/2506.01881</link>
<guid>https://arxiv.org/abs/2506.01881</guid>
<content:encoded><![CDATA[
arXiv:2506.01881v1 Announce Type: new 
Abstract: Task-oriented dialogue systems often face difficulties when user utterances seem semantically complete but lack necessary structural information for appropriate system action. This arises because users frequently do not fully understand their own needs, while systems require precise intent definitions. Current LLM-based agents cannot effectively distinguish between linguistically complete and contextually triggerable expressions, lacking frameworks for collaborative intent formation. We present STORM, a framework modeling asymmetric information dynamics through conversations between UserLLM (full internal access) and AgentLLM (observable behavior only). STORM produces annotated corpora capturing expression trajectories and latent cognitive transitions, enabling systematic analysis of collaborative understanding development. Our contributions include: (1) formalizing asymmetric information processing in dialogue systems; (2) modeling intent formation tracking collaborative understanding evolution; and (3) evaluation metrics measuring internal cognitive improvements alongside task performance. Experiments across four language models reveal that moderate uncertainty (40-60%) can outperform complete transparency in certain scenarios, with model-specific patterns suggesting reconsideration of optimal information completeness in human-AI collaboration. These findings contribute to understanding asymmetric reasoning dynamics and inform uncertainty-calibrated dialogue system design.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COALESCE: Economic and Security Dynamics of Skill-Based Task Outsourcing Among Team of Autonomous LLM Agents</title>
<link>https://arxiv.org/abs/2506.01900</link>
<guid>https://arxiv.org/abs/2506.01900</guid>
<content:encoded><![CDATA[
arXiv:2506.01900v1 Announce Type: new 
Abstract: The meteoric rise and proliferation of autonomous Large Language Model (LLM) agents promise significant capabilities across various domains. However, their deployment is increasingly constrained by substantial computational demands, specifically for Graphics Processing Unit (GPU) resources. This paper addresses the critical problem of optimizing resource utilization in LLM agent systems. We introduce COALESCE (Cost-Optimized and Secure Agent Labour Exchange via Skill-based Competence Estimation), a novel framework designed to enable autonomous LLM agents to dynamically outsource specific subtasks to specialized, cost-effective third-party LLM agents. The framework integrates mechanisms for hybrid skill representation, dynamic skill discovery, automated task decomposition, a unified cost model comparing internal execution costs against external outsourcing prices, simplified market-based decision-making algorithms, and a standardized communication protocol between LLM agents. Comprehensive validation through 239 theoretical simulations demonstrates 41.8\% cost reduction potential, while large-scale empirical validation across 240 real LLM tasks confirms 20.3\% cost reduction with proper epsilon-greedy exploration, establishing both theoretical viability and practical effectiveness. The emergence of proposed open standards like Google's Agent2Agent (A2A) protocol further underscores the need for frameworks like COALESCE that can leverage such standards for efficient agent interaction. By facilitating a dynamic market for agent capabilities, potentially utilizing protocols like A2A for communication, COALESCE aims to significantly reduce operational costs, enhance system scalability, and foster the emergence of specialized agent economies, making complex LLM agent functionalities more accessible and economically viable.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Overadaptation in Supervised Fine-Tuning: The Role of Ensemble Methods</title>
<link>https://arxiv.org/abs/2506.01901</link>
<guid>https://arxiv.org/abs/2506.01901</guid>
<content:encoded><![CDATA[
arXiv:2506.01901v1 Announce Type: new 
Abstract: Supervised fine-tuning (SFT) on domain-specific data is the dominant approach for adapting foundation models to specialized tasks. However, it has been observed that SFT models tend to forget knowledge acquired during pretraining. In vision models, ensembling a pretrained model with its fine-tuned counterpart has been shown to mitigate this issue. In this work, we demonstrate that the same holds for language models, and, more strikingly, we observe an overadaptation phenomenon: the ensemble model not only retains general knowledge from the foundation model but also outperforms the fine-tuned model even on the fine-tuning domain itself. Despite the empirical success of ensembling, a theoretical understanding of its benefits remains underexplored. We develop a formal theoretical analysis of the overadaptation phenomenon. Ensembling mitigates this by balancing two primary sources of error: bias, caused by insufficient fine-tuning, and variance, introduced by overfitting to fine-tuning data. While regularization techniques aim to address this trade-off, we show that ensembling provides a more effective solution. We analyze this phenomenon in over-parameterized linear settings and demonstrate that interpolating between pretrained and fine-tuned weights significantly improves performance. These findings offer theoretical justification for the observed advantages of model ensembling, supported by empirical experiments consistent with our analysis.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large language models can learn and generalize steganographic chain-of-thought under process supervision</title>
<link>https://arxiv.org/abs/2506.01926</link>
<guid>https://arxiv.org/abs/2506.01926</guid>
<content:encoded><![CDATA[
arXiv:2506.01926v1 Announce Type: new 
Abstract: Chain-of-thought (CoT) reasoning not only enhances large language model performance but also provides critical insights into decision-making processes, marking it as a useful tool for monitoring model intent and planning. By proactively preventing models from acting on CoT indicating misaligned or harmful intent, CoT monitoring can be used to reduce risks associated with deploying models. However, developers may be incentivized to train away the appearance of harmful intent from CoT traces, by either customer preferences or regulatory requirements. Recent works have shown that banning mention of a specific example of reward hacking, which may be done either to make CoT presentable to users or as a naive attempt to prevent the behavior, causes obfuscation of the undesired reasoning traces but the persistence of the undesired behavior. Such obfuscation threatens the reliability of CoT monitoring. However, obfuscation of reasoning can be due to its internalization to latent space computation, or its encoding within the CoT. Here, we provide an extension to these results. First, we show that penalizing the use of specific strings within load-bearing reasoning traces causes models to substitute alternative strings. Crucially, this does not alter the underlying method by which the model performs the task, demonstrating that the model can learn to steganographically encode its reasoning. We further demonstrate that models can generalize an encoding scheme. When the penalized strings belong to an overarching class, the model learns not only to substitute strings seen in training, but also develops a general encoding scheme for all members of the class which it can apply to held-out testing strings.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoboEgo System Card: An Omnimodal Model with Native Full Duplexity</title>
<link>https://arxiv.org/abs/2506.01934</link>
<guid>https://arxiv.org/abs/2506.01934</guid>
<content:encoded><![CDATA[
arXiv:2506.01934v1 Announce Type: new 
Abstract: Humans naturally process real-world multimodal information in a full-duplex manner. In artificial intelligence, replicating this capability is essential for advancing model development and deployment, particularly in embodied contexts. The development of multimodal models faces two primary challenges: (1) effectively handling more than three modalities-such as vision, audio, and text; and (2) delivering full-duplex responses to rapidly evolving human instructions. To facilitate research on models that support both omnimodal processing and full duplexity, we present RoboEgo (alias: FLM-Ego), a unified model system designed to address both challenges. RoboEgo incorporates a backbone architecture and algorithms that natively support full duplexity, achieving a theoretical duplex latency of 80 ms. In streaming visually grounded conversations under real-world conditions, RoboEgo exhibits superior responsiveness and speech naturalness, while maintaining comparable content qualities to state-of-the-art semi-duplex omnimodal models-a feat previously considered unattainable by native full-duplex systems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing AI-assisted Hardware Design with Hierarchical Decentralized Training and Personalized Inference-Time Optimization</title>
<link>https://arxiv.org/abs/2506.00002</link>
<guid>https://arxiv.org/abs/2506.00002</guid>
<content:encoded><![CDATA[
arXiv:2506.00002v1 Announce Type: cross 
Abstract: Recent years have witnessed a significant increase in the adoption of AI techniques to enhance electronic design automation. In particular, the emergence of Large Language Models (LLMs) has sparked significant interest in LLM-assisted hardware design generation, spanning applications from classical digital circuits to quantum computing. Despite substantial progress in this direction, the quality of LLM-generated hardware design still cannot meet the requirements for practical deployment. In this work, we identify three critical challenges hindering the development of LLM-assisted hardware design generation: 1) limited data availability, 2) varied data quality, 3) inadequate inference-time efficiency. To address these fundamental challenges, this paper introduces a two-stage framework for AI-assisted hardware design by exploring decentralized training and personalized inference. In the first stage, we propose to harness private domain design sources through a hierarchical decentralized training mechanism that addresses data-sharing constraints. To mitigate the impact of low-quality data, we identify optimization opportunities in hardware generation tasks, using user-defined metrics for model aggregation. The second stage focuses on client personalization to enhance both speed and quality. We introduce a new metric, Trueput, to analyze LLM-assisted hardware generation efficiency. To optimize Trueput, we implement personalized inference-time acceleration and customized sampling strategies. Evaluating both classical and quantum benchmarks, our experimental results demonstrate that the proposed two-stage framework can significantly improve the model capability for hardware design generation. As orthogonal enhancements to existing methods, our framework can achieve $33\% \sim 50\%$ semantic accuracy improvement and $2.3$ times speedup, depending on the difficulty of the generation tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rapid yet accurate Tile-circuit and device modeling for Analog In-Memory Computing</title>
<link>https://arxiv.org/abs/2506.00004</link>
<guid>https://arxiv.org/abs/2506.00004</guid>
<content:encoded><![CDATA[
arXiv:2506.00004v1 Announce Type: cross 
Abstract: Analog In-Memory Compute (AIMC) can improve the energy efficiency of Deep Learning by orders of magnitude. Yet analog-domain device and circuit non-idealities -- within the analog ``Tiles'' performing Matrix-Vector Multiply (MVM) operations -- can degrade neural-network task accuracy. We quantify the impact of low-level distortions and noise, and develop a mathematical model for Multiply-ACcumulate (MAC) operations mapped to analog tiles. Instantaneous-current IR-drop (the most significant circuit non-ideality), and ADC quantization effects are fully captured by this model, which can predict MVM tile-outputs both rapidly and accurately, as compared to much slower rigorous circuit simulations. A statistical model of PCM read noise at nanosecond timescales is derived from -- and matched against -- experimental measurements. We integrate these (statistical) device and (deterministic) circuit effects into a PyTorch-based framework to assess the accuracy impact on the BERT and ALBERT Transformer networks. We show that hardware-aware fine-tuning using simple Gaussian noise provides resilience against ADC quantization and PCM read noise effects, but is less effective against IR-drop. This is because IR-drop -- although deterministic -- is non-linear, is changing significantly during the time-integration window, and is ultimately dependent on all the excitations being introduced in parallel into the analog tile. The apparent inability of simple Gaussian noise applied during training to properly prepare a DNN network for IR-drop during inference implies that more complex training approaches -- incorporating advances such as the Tile-circuit model introduced here -- will be critical for resilient deployment of large neural networks onto AIMC hardware.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MolTextNet: A Two-Million Molecule-Text Dataset for Multimodal Molecular Learning</title>
<link>https://arxiv.org/abs/2506.00009</link>
<guid>https://arxiv.org/abs/2506.00009</guid>
<content:encoded><![CDATA[
arXiv:2506.00009v1 Announce Type: cross 
Abstract: Small molecules are essential to drug discovery, and graph-language models hold promise for learning molecular properties and functions from text. However, existing molecule-text datasets are limited in scale and informativeness, restricting the training of generalizable multimodal models. We present MolTextNet, a dataset of 2.5 million high-quality molecule-text pairs designed to overcome these limitations. To construct it, we propose a synthetic text generation pipeline that integrates structural features, computed properties, bioactivity data, and synthetic complexity. Using GPT-4o-mini, we create structured descriptions for 2.5 million molecules from ChEMBL35, with text over 10 times longer than prior datasets. MolTextNet supports diverse downstream tasks, including property prediction and structure retrieval. Pretraining CLIP-style models with Graph Neural Networks and ModernBERT on MolTextNet yields improved performance, highlighting its potential for advancing foundational multimodal modeling in molecular science. Our dataset is available at https://huggingface.co/datasets/liuganghuggingface/moltextnet.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Amadeus-Verbo Technical Report: The powerful Qwen2.5 family models trained in Portuguese</title>
<link>https://arxiv.org/abs/2506.00019</link>
<guid>https://arxiv.org/abs/2506.00019</guid>
<content:encoded><![CDATA[
arXiv:2506.00019v1 Announce Type: cross 
Abstract: This report introduces the experience of developing Amadeus Verbo, a family of large language models for Brazilian Portuguese. To handle diverse use cases, Amadeus Verbo includes base-tuned, merged, and instruction-tuned models in sizes of 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B parameters. Thus, the main objective is to show how easy it is to fine-tune foundation models to democratize the open-source development of Brazilian Portuguese LLMs when data and resources are available. Amadeus-Verbo family models are all available at HuggingFace at https://huggingface.co/collections/amadeusai/amadeus-verbo-qwen25-67cf2e7aae69ce2b3bcdcfda.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Folly of AI for Age Verification</title>
<link>https://arxiv.org/abs/2506.00038</link>
<guid>https://arxiv.org/abs/2506.00038</guid>
<content:encoded><![CDATA[
arXiv:2506.00038v1 Announce Type: cross 
Abstract: In the near future a governmental body will be asked to allow companies to use AI for age verification. If they allow it the resulting system will both be easily circumvented and disproportionately misclassify minorities and low socioeconomic status users. This is predictable by showing that other very similar systems (facial recognition and remote proctoring software) have similar issues despite years of efforts to mitigate their biases. These biases are due to technical limitations both of the AI models themselves and the physical hardware they are running on that will be difficult to overcome below the cost of government ID-based age verification. Thus in, the near future, deploying an AI system for age verification is folly.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Risks of AI-driven product development and strategies for their mitigation</title>
<link>https://arxiv.org/abs/2506.00047</link>
<guid>https://arxiv.org/abs/2506.00047</guid>
<content:encoded><![CDATA[
arXiv:2506.00047v1 Announce Type: cross 
Abstract: Humanity is progressing towards automated product development, a trend that promises faster creation of better products and thus the acceleration of technological progress. However, increasing reliance on non-human agents for this process introduces many risks. This perspective aims to initiate a discussion on these risks and appropriate mitigation strategies. To this end, we outline a set of principles for safer AI-driven product development which emphasize human oversight, accountability, and explainable design, among others. The risk assessment covers both technical risks which affect product quality and safety, and sociotechnical risks which affect society. While AI-driven product development is still in its early stages, this discussion will help balance its opportunities and risks without delaying essential progress in understanding, norm-setting, and regulation.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Hybrid Retrieval: When Small Embeddings and LLM Re-ranking Beat Bigger Models</title>
<link>https://arxiv.org/abs/2506.00049</link>
<guid>https://arxiv.org/abs/2506.00049</guid>
<content:encoded><![CDATA[
arXiv:2506.00049v1 Announce Type: cross 
Abstract: This paper presents a comparison of embedding models in tri-modal hybrid retrieval for Retrieval-Augmented Generation (RAG) systems. We investigate the fusion of dense semantic, sparse lexical, and graph-based embeddings, focusing on the performance of the MiniLM-v6 and BGE-Large architectures. Contrary to conventional assumptions, our results show that the compact MiniLM-v6 outperforms the larger BGE-Large when integrated with LLM-based re-ranking within our tri-modal hybrid framework. Experiments conducted on the SciFact, FIQA, and NFCorpus datasets demonstrate significant improvements in retrieval quality with the MiniLM-v6 configuration. The performance difference is particularly pronounced in agentic re-ranking scenarios, indicating better alignment between MiniLM-v6's embedding space and LLM reasoning. Our findings suggest that embedding model selection for RAG systems should prioritize compatibility with multi-signal fusion and LLM alignment, rather than relying solely on larger models. This approach may reduce computational requirements while improving retrieval accuracy and efficiency.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using LLMs to Advance the Cognitive Science of Collectives</title>
<link>https://arxiv.org/abs/2506.00052</link>
<guid>https://arxiv.org/abs/2506.00052</guid>
<content:encoded><![CDATA[
arXiv:2506.00052v1 Announce Type: cross 
Abstract: LLMs are already transforming the study of individual cognition, but their application to studying collective cognition has been underexplored. We lay out how LLMs may be able to address the complexity that has hindered the study of collectives and raise possible risks that warrant new methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving statistical learning methods via features selection without replacement sampling and random projection</title>
<link>https://arxiv.org/abs/2506.00053</link>
<guid>https://arxiv.org/abs/2506.00053</guid>
<content:encoded><![CDATA[
arXiv:2506.00053v1 Announce Type: cross 
Abstract: Cancer is fundamentally a genetic disease characterized by genetic and epigenetic alterations that disrupt normal gene expression, leading to uncontrolled cell growth and metastasis. High-dimensional microarray datasets pose challenges for classification models due to the "small n, large p" problem, resulting in overfitting. This study makes three different key contributions: 1) we propose a machine learning-based approach integrating the Feature Selection Without Re-placement (FSWOR) technique and a projection method to improve classification accuracy. 2) We apply the Kendall statistical test to identify the most significant genes from the brain cancer mi-croarray dataset (GSE50161), reducing the feature space from 54,675 to 20,890 genes.3) we apply machine learning models using k-fold cross validation techniques in which our model incorpo-rates ensemble classifiers with LDA projection and Na\"ive Bayes, achieving a test score of 96%, outperforming existing methods by 9.09%. The results demonstrate the effectiveness of our ap-proach in high-dimensional gene expression analysis, improving classification accuracy while mitigating overfitting. This study contributes to cancer biomarker discovery, offering a robust computational method for analyzing microarray data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt Engineer: Analyzing Skill Requirements in the AI Job Market</title>
<link>https://arxiv.org/abs/2506.00058</link>
<guid>https://arxiv.org/abs/2506.00058</guid>
<content:encoded><![CDATA[
arXiv:2506.00058v1 Announce Type: cross 
Abstract: The rise of large language models (LLMs) has created a new job role: the Prompt Engineer. Despite growing interest in this position, we still do not fully understand what skills this new job role requires or how common these jobs are. We analyzed 20,662 job postings on LinkedIn, including 72 prompt engineer positions, to learn more about this emerging role. We found that prompt engineering is still rare (less than 0.5% of sampled job postings) but has a unique skill profile. Prompt engineers need AI knowledge (22.8%), prompt design skills (18.7%), good communication (21.9%), and creative problem-solving (15.8%) skills. These requirements significantly differ from those of established roles, such as data scientists and machine learning engineers, showing that prompt engineering is becoming its own profession. Our findings help job seekers, employers, and educational institutions in better understanding the emerging field of prompt engineering.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative analysis of privacy-preserving open-source LLMs regarding extraction of diagnostic information from clinical CMR imaging reports</title>
<link>https://arxiv.org/abs/2506.00060</link>
<guid>https://arxiv.org/abs/2506.00060</guid>
<content:encoded><![CDATA[
arXiv:2506.00060v1 Announce Type: cross 
Abstract: Purpose: We investigated the utilization of privacy-preserving, locally-deployed, open-source Large Language Models (LLMs) to extract diagnostic information from free-text cardiovascular magnetic resonance (CMR) reports. Materials and Methods: We evaluated nine open-source LLMs on their ability to identify diagnoses and classify patients into various cardiac diagnostic categories based on descriptive findings in 109 clinical CMR reports. Performance was quantified using standard classification metrics including accuracy, precision, recall, and F1 score. We also employed confusion matrices to examine patterns of misclassification across models. Results: Most open-source LLMs demonstrated exceptional performance in classifying reports into different diagnostic categories. Google's Gemma2 model achieved the highest average F1 score of 0.98, followed by Qwen2.5:32B and DeepseekR1-32B with F1 scores of 0.96 and 0.95, respectively. All other evaluated models attained average scores above 0.93, with Mistral and DeepseekR1-7B being the only exceptions. The top four LLMs outperformed our board-certified cardiologist (F1 score of 0.94) across all evaluation metrics in analyzing CMR reports. Conclusion: Our findings demonstrate the feasibility of implementing open-source, privacy-preserving LLMs in clinical settings for automated analysis of imaging reports, enabling accurate, fast and resource-efficient diagnostic categorization.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unraveling SITT: Social Influence Technique Taxonomy and Detection with LLMs</title>
<link>https://arxiv.org/abs/2506.00061</link>
<guid>https://arxiv.org/abs/2506.00061</guid>
<content:encoded><![CDATA[
arXiv:2506.00061v1 Announce Type: cross 
Abstract: In this work we present the Social Influence Technique Taxonomy (SITT), a comprehensive framework of 58 empirically grounded techniques organized into nine categories, designed to detect subtle forms of social influence in textual content. We also investigate the LLMs ability to identify various forms of social influence. Building on interdisciplinary foundations, we construct the SITT dataset -- a 746-dialogue corpus annotated by 11 experts in Polish and translated into English -- to evaluate the ability of LLMs to identify these techniques. Using a hierarchical multi-label classification setup, we benchmark five LLMs, including GPT-4o, Claude 3.5, Llama-3.1, Mixtral, and PLLuM. Our results show that while some models, notably Claude 3.5, achieved moderate success (F1 score = 0.45 for categories), overall performance of models remains limited, particularly for context-sensitive techniques. The findings demonstrate key limitations in current LLMs' sensitivity to nuanced linguistic cues and underscore the importance of domain-specific fine-tuning. This work contributes a novel resource and evaluation example for understanding how LLMs detect, classify, and potentially replicate strategies of social influence in natural dialogues.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mis-prompt: Benchmarking Large Language Models for Proactive Error Handling</title>
<link>https://arxiv.org/abs/2506.00064</link>
<guid>https://arxiv.org/abs/2506.00064</guid>
<content:encoded><![CDATA[
arXiv:2506.00064v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated significant advancements in error handling. Current error-handling works are performed in a passive manner, with explicit error-handling instructions. However, in real-world scenarios, explicit error-handling instructions are usually unavailable. In this paper, our work identifies this challenge as how to conduct proactive error handling without explicit error handling instructions. To promote further research, this work introduces a new benchmark, termed Mis-prompt, consisting of four evaluation tasks, an error category taxonomy, and a new evaluation dataset. Furthermore, this work analyzes current LLMs' performance on the benchmark, and the experimental results reveal that current LLMs show poor performance on proactive error handling, and SFT on error handling instances improves LLMs' proactive error handling capabilities. The dataset will be publicly available.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>You Prefer This One, I Prefer Yours: Using Reference Words is Harder Than Vocabulary Words for Humans and Multimodal Language Models</title>
<link>https://arxiv.org/abs/2506.00065</link>
<guid>https://arxiv.org/abs/2506.00065</guid>
<content:encoded><![CDATA[
arXiv:2506.00065v1 Announce Type: cross 
Abstract: Multimodal language models (MLMs) increasingly communicate in human-like ways, yet their ability to use reference words remains largely overlooked despite their ubiquity in everyday communication. Our study addresses this gap by comparing human and MLM use of three word classes with increasing cognitive demands: vocabulary words, possessive pronouns (`mine' vs `yours'), and demonstrative pronouns (`this one' vs `that one'). Evaluating seven state-of-the-art MLMs against human participants, we observe a clear difficulty hierarchy: while MLMs approach human-level performance on the vocabulary task, they show substantial deficits with possessives and demonstratives. Our analysis reveals these difficulties stem from limitations in perspective-taking and spatial reasoning. Although prompt engineering improved model performance on possessive use, demonstrative use remained well below human-level competence. These findings provide theoretical and empirical evidence that producing grammatical forms requiring pragmatics and social cognition remains a clear challenge in current NLP systems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Literature Review Of Multi-Agent Debate For Problem-Solving</title>
<link>https://arxiv.org/abs/2506.00066</link>
<guid>https://arxiv.org/abs/2506.00066</guid>
<content:encoded><![CDATA[
arXiv:2506.00066v1 Announce Type: cross 
Abstract: Multi-agent large language models (MA-LLMs) are a rapidly growing research area that leverages multiple interacting language agents to tackle complex tasks, outperforming single-agent large language models. This literature review synthesizes the latest research on agent profiles, communication structures, and decision-making processes, drawing insights from both traditional multi-agent systems and state-of-the-art MA-LLM studies. In doing so, it aims to address the lack of direct comparisons in the field, illustrating how factors like scalability, communication structure, and decision-making processes influence MA-LLM performance. By examining frequent practices and outlining current challenges, the review reveals that multi-agent approaches can yield superior results but also face elevated computational costs and under-explored challenges unique to MA-LLM. Overall, these findings provide researchers and practitioners with a roadmap for developing robust and efficient multi-agent AI solutions.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing Politico-Economic Bias in Multilingual Large Language Models: A Cultural Analysis of Low-Resource Pakistani Languages</title>
<link>https://arxiv.org/abs/2506.00068</link>
<guid>https://arxiv.org/abs/2506.00068</guid>
<content:encoded><![CDATA[
arXiv:2506.00068v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly shaping public discourse, yet their politico-economic biases remain underexamined in non-Western and low-resource multilingual contexts. This paper presents a systematic analysis of political bias in 13 state-of-the-art LLMs across five low-resource languages spoken in Pakistan: Urdu, Punjabi, Sindhi, Balochi, and Pashto. We propose a novel framework that integrates an adapted Political Compass Test (PCT) with a multi-level framing analysis. Our method combines quantitative assessment of political orientation across economic (left-right) and social (libertarian-authoritarian) axes with qualitative analysis of framing through content, style, and emphasis. We further contextualize this analysis by aligning prompts with 11 key socio-political themes relevant to Pakistani society. Our results reveal that LLMs predominantly align with liberal-left values, echoing Western training data influences, but exhibit notable shifts toward authoritarian framing in regional languages, suggesting strong cultural modulation effects. We also identify consistent model-specific bias signatures and language-conditioned variations in ideological expression. These findings show the urgent need for culturally grounded, multilingual bias auditing frameworks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Sensitivity of LLMs to Prior Context</title>
<link>https://arxiv.org/abs/2506.00069</link>
<guid>https://arxiv.org/abs/2506.00069</guid>
<content:encoded><![CDATA[
arXiv:2506.00069v1 Announce Type: cross 
Abstract: As large language models (LLMs) are increasingly deployed in multi-turn dialogue and other sustained interactive scenarios, it is essential to understand how extended context affects their performance. Popular benchmarks, focusing primarily on single-turn question answering (QA) tasks, fail to capture the effects of multi-turn exchanges. To address this gap, we introduce a novel set of benchmarks that systematically vary the volume and nature of prior context. We evaluate multiple conventional LLMs, including GPT, Claude, and Gemini, across these benchmarks to measure their sensitivity to contextual variations. Our findings reveal that LLM performance on multiple-choice questions can degrade dramatically in multi-turn interactions, with performance drops as large as 73% for certain models. Even highly capable models such as GPT-4o exhibit up to a 32% decrease in accuracy. Notably, the relative performance of larger versus smaller models is not always predictable. Moreover, the strategic placement of the task description within the context can substantially mitigate performance drops, improving the accuracy by as much as a factor of 3.5. These findings underscore the need for robust strategies to design, evaluate, and mitigate context-related sensitivity in LLMs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics</title>
<link>https://arxiv.org/abs/2506.00070</link>
<guid>https://arxiv.org/abs/2506.00070</guid>
<content:encoded><![CDATA[
arXiv:2506.00070v1 Announce Type: cross 
Abstract: Large Vision-Language Models (LVLMs) have recently shown great promise in advancing robotics by combining embodied reasoning with robot control. A common approach involves training on embodied reasoning tasks related to robot control using Supervised Fine-Tuning (SFT). However, SFT datasets are often heuristically constructed and not explicitly optimized for improving robot control. Furthermore, SFT often leads to issues such as catastrophic forgetting and reduced generalization performance. To address these limitations, we introduce Robot-R1, a novel framework that leverages reinforcement learning to enhance embodied reasoning specifically for robot control. Robot-R1 learns to predict the next keypoint state required for task completion, conditioned on the current scene image and environment metadata derived from expert demonstrations. Inspired by the DeepSeek-R1 learning approach, Robot-R1 samples reasoning-based responses and reinforces those that lead to more accurate predictions. Our experiments show that models trained with Robot-R1 outperform SFT methods on embodied reasoning tasks. Despite having only 7B parameters, Robot-R1 even surpasses GPT-4o on reasoning tasks related to low-level action control, such as spatial and primitive movement reasoning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human sensory-musculoskeletal modeling and control of whole-body movements</title>
<link>https://arxiv.org/abs/2506.00071</link>
<guid>https://arxiv.org/abs/2506.00071</guid>
<content:encoded><![CDATA[
arXiv:2506.00071v1 Announce Type: cross 
Abstract: Coordinated human movement depends on the integration of multisensory inputs, sensorimotor transformation, and motor execution, as well as sensory feedback resulting from body-environment interaction. Building dynamic models of the sensory-musculoskeletal system is essential for understanding movement control and investigating human behaviours. Here, we report a human sensory-musculoskeletal model, termed SMS-Human, that integrates precise anatomical representations of bones, joints, and muscle-tendon units with multimodal sensory inputs involving visual, vestibular, proprioceptive, and tactile components. A stage-wise hierarchical deep reinforcement learning framework was developed to address the inherent challenges of high-dimensional control in musculoskeletal systems with integrated multisensory information. Using this framework, we demonstrated the simulation of three representative movement tasks, including bipedal locomotion, vision-guided object manipulation, and human-machine interaction during bicycling. Our results showed a close resemblance between natural and simulated human motor behaviours. The simulation also revealed musculoskeletal dynamics that could not be directly measured. This work sheds deeper insights into the sensorimotor dynamics of human movements, facilitates quantitative understanding of human behaviours in interactive contexts, and informs the design of systems with embodied intelligence.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Prompt Engineering Techniques for Accuracy and Confidence Elicitation in Medical LLMs</title>
<link>https://arxiv.org/abs/2506.00072</link>
<guid>https://arxiv.org/abs/2506.00072</guid>
<content:encoded><![CDATA[
arXiv:2506.00072v1 Announce Type: cross 
Abstract: This paper investigates how prompt engineering techniques impact both accuracy and confidence elicitation in Large Language Models (LLMs) applied to medical contexts. Using a stratified dataset of Persian board exam questions across multiple specialties, we evaluated five LLMs - GPT-4o, o3-mini, Llama-3.3-70b, Llama-3.1-8b, and DeepSeek-v3 - across 156 configurations. These configurations varied in temperature settings (0.3, 0.7, 1.0), prompt styles (Chain-of-Thought, Few-Shot, Emotional, Expert Mimicry), and confidence scales (1-10, 1-100). We used AUC-ROC, Brier Score, and Expected Calibration Error (ECE) to evaluate alignment between confidence and actual performance. Chain-of-Thought prompts improved accuracy but also led to overconfidence, highlighting the need for calibration. Emotional prompting further inflated confidence, risking poor decisions. Smaller models like Llama-3.1-8b underperformed across all metrics, while proprietary models showed higher accuracy but still lacked calibrated confidence. These results suggest prompt engineering must address both accuracy and uncertainty to be effective in high-stakes medical tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Whose Name Comes Up? Auditing LLM-Based Scholar Recommendations</title>
<link>https://arxiv.org/abs/2506.00074</link>
<guid>https://arxiv.org/abs/2506.00074</guid>
<content:encoded><![CDATA[
arXiv:2506.00074v1 Announce Type: cross 
Abstract: This paper evaluates the performance of six open-weight LLMs (llama3-8b, llama3.1-8b, gemma2-9b, mixtral-8x7b, llama3-70b, llama3.1-70b) in recommending experts in physics across five tasks: top-k experts by field, influential scientists by discipline, epoch, seniority, and scholar counterparts. The evaluation examines consistency, factuality, and biases related to gender, ethnicity, academic popularity, and scholar similarity. Using ground-truth data from the American Physical Society and OpenAlex, we establish scholarly benchmarks by comparing model outputs to real-world academic records. Our analysis reveals inconsistencies and biases across all models. mixtral-8x7b produces the most stable outputs, while llama3.1-70b shows the highest variability. Many models exhibit duplication, and some, particularly gemma2-9b and llama3.1-8b, struggle with formatting errors. LLMs generally recommend real scientists, but accuracy drops in field-, epoch-, and seniority-specific queries, consistently favoring senior scholars. Representation biases persist, replicating gender imbalances (reflecting male predominance), under-representing Asian scientists, and over-representing White scholars. Despite some diversity in institutional and collaboration networks, models favor highly cited and productive scholars, reinforcing the rich-getricher effect while offering limited geographical representation. These findings highlight the need to improve LLMs for more reliable and equitable scholarly recommendations.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reducing Latency in LLM-Based Natural Language Commands Processing for Robot Navigation</title>
<link>https://arxiv.org/abs/2506.00075</link>
<guid>https://arxiv.org/abs/2506.00075</guid>
<content:encoded><![CDATA[
arXiv:2506.00075v1 Announce Type: cross 
Abstract: The integration of Large Language Models (LLMs), such as GPT, in industrial robotics enhances operational efficiency and human-robot collaboration. However, the computational complexity and size of these models often provide latency problems in request and response times. This study explores the integration of the ChatGPT natural language model with the Robot Operating System 2 (ROS 2) to mitigate interaction latency and improve robotic system control within a simulated Gazebo environment. We present an architecture that integrates these technologies without requiring a middleware transport platform, detailing how a simulated mobile robot responds to text and voice commands. Experimental results demonstrate that this integration improves execution speed, usability, and accessibility of the human-robot interaction by decreasing the communication latency by 7.01\% on average. Such improvements facilitate smoother, real-time robot operations, which are crucial for industrial automation and precision tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Storytelling, Improving Audience Retention, and Reducing Waste in the Entertainment Industry</title>
<link>https://arxiv.org/abs/2506.00076</link>
<guid>https://arxiv.org/abs/2506.00076</guid>
<content:encoded><![CDATA[
arXiv:2506.00076v1 Announce Type: cross 
Abstract: Television networks face high financial risk when making programming decisions, often relying on limited historical data to forecast episodic viewership. This study introduces a machine learning framework that integrates natural language processing (NLP) features from over 25000 television episodes with traditional viewership data to enhance predictive accuracy. By extracting emotional tone, cognitive complexity, and narrative structure from episode dialogue, we evaluate forecasting performance using SARIMAX, rolling XGBoost, and feature selection models. While prior viewership remains a strong baseline predictor, NLP features contribute meaningful improvements for some series. We also introduce a similarity scoring method based on Euclidean distance between aggregate dialogue vectors to compare shows by content. Tested across diverse genres, including Better Call Saul and Abbott Elementary, our framework reveals genre-specific performance and offers interpretable metrics for writers, executives, and marketers seeking data-driven insight into audience behavior.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who Gets the Kidney? Human-AI Alignment, Indecision, and Moral Values</title>
<link>https://arxiv.org/abs/2506.00079</link>
<guid>https://arxiv.org/abs/2506.00079</guid>
<content:encoded><![CDATA[
arXiv:2506.00079v1 Announce Type: cross 
Abstract: The rapid integration of Large Language Models (LLMs) in high-stakes decision-making -- such as allocating scarce resources like donor organs -- raises critical questions about their alignment with human moral values. We systematically evaluate the behavior of several prominent LLMs against human preferences in kidney allocation scenarios and show that LLMs: i) exhibit stark deviations from human values in prioritizing various attributes, and ii) in contrast to humans, LLMs rarely express indecision, opting for deterministic decisions even when alternative indecision mechanisms (e.g., coin flipping) are provided. Nonetheless, we show that low-rank supervised fine-tuning with few samples is often effective in improving both decision consistency and calibrating indecision modeling. These findings illustrate the necessity of explicit alignment strategies for LLMs in moral/ethical domains.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bottom-Up Perspectives on AI Governance: Insights from User Reviews of AI Products</title>
<link>https://arxiv.org/abs/2506.00080</link>
<guid>https://arxiv.org/abs/2506.00080</guid>
<content:encoded><![CDATA[
arXiv:2506.00080v1 Announce Type: cross 
Abstract: With the growing importance of AI governance, numerous high-level frameworks and principles have been articulated by policymakers, institutions, and expert communities to guide the development and application of AI. While such frameworks offer valuable normative orientation, they may not fully capture the practical concerns of those who interact with AI systems in organizational and operational contexts. To address this gap, this study adopts a bottom-up approach to explore how governance-relevant themes are expressed in user discourse. Drawing on over 100,000 user reviews of AI products from G2.com, we apply BERTopic to extract latent themes and identify those most semantically related to AI governance. The analysis reveals a diverse set of governance-relevant topics spanning both technical and non-technical domains. These include concerns across organizational processes-such as planning, coordination, and communication-as well as stages of the AI value chain, including deployment infrastructure, data handling, and analytics. The findings show considerable overlap with institutional AI governance and ethics frameworks on issues like privacy and transparency, but also surface overlooked areas such as project management, strategy development, and customer interaction. This highlights the need for more empirically grounded, user-centered approaches to AI governance-approaches that complement normative models by capturing how governance unfolds in applied settings. By foregrounding how governance is enacted in practice, this study contributes to more inclusive and operationally grounded approaches to AI governance and digital policy.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Empathy: AI based Mental Health</title>
<link>https://arxiv.org/abs/2506.00081</link>
<guid>https://arxiv.org/abs/2506.00081</guid>
<content:encoded><![CDATA[
arXiv:2506.00081v1 Announce Type: cross 
Abstract: Many people suffer from mental health problems but not everyone seeks professional help or has access to mental health care. AI chatbots have increasingly become a go-to for individuals who either have mental disorders or simply want someone to talk to. This paper presents a study on participants who have previously used chatbots and a scenario-based testing of large language model (LLM) chatbots. Our findings indicate that AI chatbots were primarily utilized as a "Five minute therapist" or as a non-judgmental companion. Participants appreciated the anonymity and lack of judgment from chatbots. However, there were concerns about privacy and the security of sensitive information. The scenario-based testing of LLM chatbots highlighted additional issues. Some chatbots were consistently reassuring, used emojis and names to add a personal touch, and were quick to suggest seeking professional help. However, there were limitations such as inconsistent tone, occasional inappropriate responses (e.g., casual or romantic), and a lack of crisis sensitivity, particularly in recognizing red flag language and escalating responses appropriately. These findings can inform both the technology and mental health care industries on how to better utilize AI chatbots to support individuals during challenging emotional periods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hi-Dyna Graph: Hierarchical Dynamic Scene Graph for Robotic Autonomy in Human-Centric Environments</title>
<link>https://arxiv.org/abs/2506.00083</link>
<guid>https://arxiv.org/abs/2506.00083</guid>
<content:encoded><![CDATA[
arXiv:2506.00083v1 Announce Type: cross 
Abstract: Autonomous operation of service robotics in human-centric scenes remains challenging due to the need for understanding of changing environments and context-aware decision-making. While existing approaches like topological maps offer efficient spatial priors, they fail to model transient object relationships, whereas dense neural representations (e.g., NeRF) incur prohibitive computational costs. Inspired by the hierarchical scene representation and video scene graph generation works, we propose Hi-Dyna Graph, a hierarchical dynamic scene graph architecture that integrates persistent global layouts with localized dynamic semantics for embodied robotic autonomy. Our framework constructs a global topological graph from posed RGB-D inputs, encoding room-scale connectivity and large static objects (e.g., furniture), while environmental and egocentric cameras populate dynamic subgraphs with object position relations and human-object interaction patterns. A hybrid architecture is conducted by anchoring these subgraphs to the global topology using semantic and spatial constraints, enabling seamless updates as the environment evolves. An agent powered by large language models (LLMs) is employed to interpret the unified graph, infer latent task triggers, and generate executable instructions grounded in robotic affordances. We conduct complex experiments to demonstrate Hi-Dyna Grap's superior scene representation effectiveness. Real-world deployments validate the system's practicality with a mobile manipulator: robotics autonomously complete complex tasks with no further training or complex rewarding in a dynamic scene as cafeteria assistant. See https://anonymous.4open.science/r/Hi-Dyna-Graph-B326 for video demonstration and more details.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COSMIC: Generalized Refusal Direction Identification in LLM Activations</title>
<link>https://arxiv.org/abs/2506.00085</link>
<guid>https://arxiv.org/abs/2506.00085</guid>
<content:encoded><![CDATA[
arXiv:2506.00085v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) encode behaviors such as refusal within their activation space, yet identifying these behaviors remains a significant challenge. Existing methods often rely on predefined refusal templates detectable in output tokens or require manual analysis. We introduce \textbf{COSMIC} (Cosine Similarity Metrics for Inversion of Concepts), an automated framework for direction selection that identifies viable steering directions and target layers using cosine similarity - entirely independent of model outputs. COSMIC achieves steering performance comparable to prior methods without requiring assumptions about a model's refusal behavior, such as the presence of specific refusal tokens. It reliably identifies refusal directions in adversarial settings and weakly aligned models, and is capable of steering such models toward safer behavior with minimal increase in false refusals, demonstrating robustness across a wide range of alignment conditions.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SwitchLingua: The First Large-Scale Multilingual and Multi-Ethnic Code-Switching Dataset</title>
<link>https://arxiv.org/abs/2506.00087</link>
<guid>https://arxiv.org/abs/2506.00087</guid>
<content:encoded><![CDATA[
arXiv:2506.00087v1 Announce Type: cross 
Abstract: Code-switching (CS) is the alternating use of two or more languages within a conversation or utterance, often influenced by social context and speaker identity. This linguistic phenomenon poses challenges for Automatic Speech Recognition (ASR) systems, which are typically designed for a single language and struggle to handle multilingual inputs. The growing global demand for multilingual applications, including Code-Switching ASR (CSASR), Text-to-Speech (CSTTS), and Cross-Lingual Information Retrieval (CLIR), highlights the inadequacy of existing monolingual datasets.
  Although some code-switching datasets exist, most are limited to bilingual mixing within homogeneous ethnic groups, leaving a critical need for a large-scale, diverse benchmark akin to ImageNet in computer vision.
  To bridge this gap, we introduce \textbf{LinguaMaster}, a multi-agent collaboration framework specifically designed for efficient and scalable multilingual data synthesis. Leveraging this framework, we curate \textbf{SwitchLingua}, the first large-scale multilingual and multi-ethnic code-switching dataset, including: (1) 420K CS textual samples across 12 languages, and (2) over 80 hours of audio recordings from 174 speakers representing 18 countries/regions and 63 racial/ethnic backgrounds, based on the textual data. This dataset captures rich linguistic and cultural diversity, offering a foundational resource for advancing multilingual and multicultural research. Furthermore, to address the issue that existing ASR evaluation metrics lack sensitivity to code-switching scenarios, we propose the \textbf{Semantic-Aware Error Rate (SAER)}, a novel evaluation metric that incorporates semantic information, providing a more accurate and context-aware assessment of system performance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HD-NDEs: Neural Differential Equations for Hallucination Detection in LLMs</title>
<link>https://arxiv.org/abs/2506.00088</link>
<guid>https://arxiv.org/abs/2506.00088</guid>
<content:encoded><![CDATA[
arXiv:2506.00088v1 Announce Type: cross 
Abstract: In recent years, large language models (LLMs) have made remarkable advancements, yet hallucination, where models produce inaccurate or non-factual statements, remains a significant challenge for real-world deployment. Although current classification-based methods, such as SAPLMA, are highly efficient in mitigating hallucinations, they struggle when non-factual information arises in the early or mid-sequence of outputs, reducing their reliability. To address these issues, we propose Hallucination Detection-Neural Differential Equations (HD-NDEs), a novel method that systematically assesses the truthfulness of statements by capturing the full dynamics of LLMs within their latent space. Our approaches apply neural differential equations (Neural DEs) to model the dynamic system in the latent space of LLMs. Then, the sequence in the latent space is mapped to the classification space for truth assessment. The extensive experiments across five datasets and six widely used LLMs demonstrate the effectiveness of HD-NDEs, especially, achieving over 14% improvement in AUC-ROC on the True-False dataset compared to state-of-the-art techniques.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRAPDOC: Deceiving LLM Users by Injecting Imperceptible Phantom Tokens into Documents</title>
<link>https://arxiv.org/abs/2506.00089</link>
<guid>https://arxiv.org/abs/2506.00089</guid>
<content:encoded><![CDATA[
arXiv:2506.00089v1 Announce Type: cross 
Abstract: The reasoning, writing, text-editing, and retrieval capabilities of proprietary large language models (LLMs) have advanced rapidly, providing users with an ever-expanding set of functionalities. However, this growing utility has also led to a serious societal concern: the over-reliance on LLMs. In particular, users increasingly delegate tasks such as homework, assignments, or the processing of sensitive documents to LLMs without meaningful engagement. This form of over-reliance and misuse is emerging as a significant social issue. In order to mitigate these issues, we propose a method injecting imperceptible phantom tokens into documents, which causes LLMs to generate outputs that appear plausible to users but are in fact incorrect. Based on this technique, we introduce TRAPDOC, a framework designed to deceive over-reliant LLM users. Through empirical evaluation, we demonstrate the effectiveness of our framework on proprietary LLMs, comparing its impact against several baselines. TRAPDOC serves as a strong foundation for promoting more responsible and thoughtful engagement with language models. Our code is available at https://github.com/jindong22/TrapDoc.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feeling Guilty Being a c(ai)borg: Navigating the Tensions Between Guilt and Empowerment in AI Use</title>
<link>https://arxiv.org/abs/2506.00094</link>
<guid>https://arxiv.org/abs/2506.00094</guid>
<content:encoded><![CDATA[
arXiv:2506.00094v1 Announce Type: cross 
Abstract: This paper explores the emotional, ethical and practical dimensions of integrating Artificial Intelligence (AI) into personal and professional workflows, focusing on the concept of feeling guilty as a 'c(ai)borg' - a human augmented by AI. Inspired by Donna Haraway's Cyborg Manifesto, the study explores how AI challenges traditional notions of creativity, originality and intellectual labour. Using an autoethnographic approach, the authors reflect on their year-long experiences with AI tools, revealing a transition from initial guilt and reluctance to empowerment through skill-building and transparency. Key findings highlight the importance of basic academic skills, advanced AI literacy and honest engagement with AI results. The c(ai)borg vision advocates for a future where AI is openly embraced as a collaborative partner, fostering innovation and equity while addressing issues of access and agency. By reframing guilt as growth, the paper calls for a thoughtful and inclusive approach to AI integration.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ClinBench-HPB: A Clinical Benchmark for Evaluating LLMs in Hepato-Pancreato-Biliary Diseases</title>
<link>https://arxiv.org/abs/2506.00095</link>
<guid>https://arxiv.org/abs/2506.00095</guid>
<content:encoded><![CDATA[
arXiv:2506.00095v1 Announce Type: cross 
Abstract: Hepato-pancreato-biliary (HPB) disorders represent a global public health challenge due to their high morbidity and mortality. Although large language models (LLMs) have shown promising performance in general medical question-answering tasks, the current evaluation benchmarks are mostly derived from standardized examinations or manually designed questions, lacking HPB coverage and clinical cases. To address these issues, we systematically eatablish an HPB disease evaluation benchmark comprising 3,535 closed-ended multiple-choice questions and 337 open-ended real diagnosis cases, which encompasses all the 33 main categories and 465 subcategories of HPB diseases defined in the International Statistical Classification of Diseases, 10th Revision (ICD-10). The multiple-choice questions are curated from public datasets and synthesized data, and the clinical cases are collected from prestigious medical journals, case-sharing platforms, and collaborating hospitals. By evalauting commercial and open-source general and medical LLMs on our established benchmark, namely ClinBench-HBP, we find that while commercial LLMs perform competently on medical exam questions, they exhibit substantial performance degradation on HPB diagnosis tasks, especially on complex, inpatient clinical cases. Those medical LLMs also show limited generalizability to HPB diseases. Our results reveal the critical limitations of current LLMs in the domain of HPB diseases, underscoring the imperative need for future medical LLMs to handle real, complex clinical diagnostics rather than simple medical exam questions. The benchmark will be released at the homepage.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PathGene: Benchmarking Driver Gene Mutations and Exon Prediction Using Multicenter Lung Cancer Histopathology Image Dataset</title>
<link>https://arxiv.org/abs/2506.00096</link>
<guid>https://arxiv.org/abs/2506.00096</guid>
<content:encoded><![CDATA[
arXiv:2506.00096v1 Announce Type: cross 
Abstract: Accurately predicting gene mutations, mutation subtypes and their exons in lung cancer is critical for personalized treatment planning and prognostic assessment. Faced with regional disparities in medical resources and the high cost of genomic assays, using artificial intelligence to infer these mutations and exon variants from routine histopathology images could greatly facilitate precision therapy. Although some prior studies have shown that deep learning can accelerate the prediction of key gene mutations from lung cancer pathology slides, their performance remains suboptimal and has so far been limited mainly to early screening tasks. To address these limitations, we have assembled PathGene, which comprises histopathology images paired with next-generation sequencing reports from 1,576 patients at the Second Xiangya Hospital, Central South University, and 448 TCGA-LUAD patients. This multi-center dataset links whole-slide images to driver gene mutation status, mutation subtypes, exon, and tumor mutational burden (TMB) status, with the goal of leveraging pathology images to predict mutations, subtypes, exon locations, and TMB for early genetic screening and to advance precision oncology. Unlike existing datasets, we provide molecular-level information related to histopathology images in PathGene to facilitate the development of biomarker prediction models. We benchmarked 11 multiple-instance learning methods on PathGene for mutation, subtype, exon, and TMB prediction tasks. These experimental methods provide valuable alternatives for early genetic screening of lung cancer patients and assisting clinicians to quickly develop personalized precision targeted treatment plans for patients. Code and data are available at https://github.com/panliangrui/NIPS2025/.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Children's Voice Privacy: First Steps And Emerging Challenges</title>
<link>https://arxiv.org/abs/2506.00100</link>
<guid>https://arxiv.org/abs/2506.00100</guid>
<content:encoded><![CDATA[
arXiv:2506.00100v1 Announce Type: cross 
Abstract: Children are one of the most under-represented groups in speech technologies, as well as one of the most vulnerable in terms of privacy. Despite this, anonymization techniques targeting this population have received little attention. In this study, we seek to bridge this gap, and establish a baseline for the use of voice anonymization techniques designed for adult speech when applied to children's voices. Such an evaluation is essential, as children's speech presents a distinct set of challenges when compared to that of adults. This study comprises three children's datasets, six anonymization methods, and objective and subjective utility metrics for evaluation. Our results show that existing systems for adults are still able to protect children's voice privacy, but suffer from much higher utility degradation. In addition, our subjective study displays the challenges of automatic evaluation methods for speech quality in children's speech, highlighting the need for further research.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gated Multimodal Graph Learning for Personalized Recommendation</title>
<link>https://arxiv.org/abs/2506.00107</link>
<guid>https://arxiv.org/abs/2506.00107</guid>
<content:encoded><![CDATA[
arXiv:2506.00107v1 Announce Type: cross 
Abstract: Multimodal recommendation has emerged as a promising solution to alleviate the cold-start and sparsity problems in collaborative filtering by incorporating rich content information, such as product images and textual descriptions. However, effectively integrating heterogeneous modalities into a unified recommendation framework remains a challenge. Existing approaches often rely on fixed fusion strategies or complex architectures , which may fail to adapt to modality quality variance or introduce unnecessary computational overhead.
  In this work, we propose RLMultimodalRec, a lightweight and modular recommendation framework that combines graph-based user modeling with adaptive multimodal item encoding. The model employs a gated fusion module to dynamically balance the contribution of visual and textual modalities, enabling fine-grained and content-aware item representations. Meanwhile, a two-layer LightGCN encoder captures high-order collaborative signals by propagating embeddings over the user-item interaction graph without relying on nonlinear transformations.
  We evaluate our model on a real-world dataset from the Amazon product domain. Experimental results demonstrate that RLMultimodalRec consistently outperforms several competitive baselines, including collaborative filtering, visual-aware, and multimodal GNN-based methods. The proposed approach achieves significant improvements in top-K recommendation metrics while maintaining scalability and interpretability, making it suitable for practical deployment.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting Offline Reinforcement Learning with Online Delays</title>
<link>https://arxiv.org/abs/2506.00131</link>
<guid>https://arxiv.org/abs/2506.00131</guid>
<content:encoded><![CDATA[
arXiv:2506.00131v1 Announce Type: cross 
Abstract: Offline-to-online deployment of reinforcement-learning (RL) agents must bridge two gaps: (1) the sim-to-real gap, where real systems add latency and other imperfections not present in simulation, and (2) the interaction gap, where policies trained purely offline face out-of-distribution states during online execution because gathering new interaction data is costly or risky. Agents therefore have to generalize from static, delay-free datasets to dynamic, delay-prone environments. Standard offline RL learns from delay-free logs yet must act under delays that break the Markov assumption and hurt performance. We introduce DT-CORL (Delay-Transformer belief policy Constrained Offline RL), an offline-RL framework built to cope with delayed dynamics at deployment. DT-CORL (i) produces delay-robust actions with a transformer-based belief predictor even though it never sees delayed observations during training, and (ii) is markedly more sample-efficient than na\"ive history-augmentation baselines. Experiments on D4RL benchmarks with several delay settings show that DT-CORL consistently outperforms both history-augmentation and vanilla belief-based methods, narrowing the sim-to-real latency gap while preserving data efficiency.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Reinforcement Learning-Based Telematic Routing Protocol for the Internet of Underwater Things</title>
<link>https://arxiv.org/abs/2506.00133</link>
<guid>https://arxiv.org/abs/2506.00133</guid>
<content:encoded><![CDATA[
arXiv:2506.00133v1 Announce Type: cross 
Abstract: The Internet of Underwater Things (IoUT) faces major challenges such as low bandwidth, high latency, mobility, and limited energy resources. Traditional routing protocols like RPL, which were designed for land-based networks, do not perform well in these underwater conditions. This paper introduces RL-RPL-UA, a new routing protocol that uses reinforcement learning to improve performance in underwater environments. Each node includes a lightweight RL agent that selects the best parent node based on local information such as packet delivery ratio, buffer level, link quality, and remaining energy. RL-RPL-UA keeps full compatibility with standard RPL messages and adds a dynamic objective function to support real-time decision-making. Simulations using Aqua-Sim show that RL-RPL-UA increases packet delivery by up to 9.2%, reduces energy use per packet by 14.8%, and extends network lifetime by 80 seconds compared to traditional methods. These results suggest that RL-RPL-UA is a promising and energy-efficient routing solution for underwater networks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spurious Correlations and Beyond: Understanding and Mitigating Shortcut Learning in SDOH Extraction with Large Language Models</title>
<link>https://arxiv.org/abs/2506.00134</link>
<guid>https://arxiv.org/abs/2506.00134</guid>
<content:encoded><![CDATA[
arXiv:2506.00134v1 Announce Type: cross 
Abstract: Social determinants of health (SDOH) extraction from clinical text is critical for downstream healthcare analytics. Although large language models (LLMs) have shown promise, they may rely on superficial cues leading to spurious predictions. Using the MIMIC portion of the SHAC (Social History Annotation Corpus) dataset and focusing on drug status extraction as a case study, we demonstrate that mentions of alcohol or smoking can falsely induce models to predict current/past drug use where none is present, while also uncovering concerning gender disparities in model performance. We further evaluate mitigation strategies - such as prompt engineering and chain-of-thought reasoning - to reduce these false positives, providing insights into enhancing LLM reliability in health domains.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomous Behavior and Whole-Brain Dynamics Emerge in Embodied Zebrafish Agents with Model-based Intrinsic Motivation</title>
<link>https://arxiv.org/abs/2506.00138</link>
<guid>https://arxiv.org/abs/2506.00138</guid>
<content:encoded><![CDATA[
arXiv:2506.00138v1 Announce Type: cross 
Abstract: Autonomy is a hallmark of animal intelligence, enabling adaptive and intelligent behavior in complex environments without relying on external reward or task structure. Existing reinforcement learning approaches to exploration in sparse reward and reward-free environments, including class of methods known as intrinsic motivation, exhibit inconsistent exploration patterns and thus fail to produce robust autonomous behaviors observed in animals. Moreover, systems neuroscience has largely overlooked the neural basis of autonomy, focusing instead on experimental paradigms where animals are motivated by external reward rather than engaging in unconstrained, naturalistic and task-independent behavior. To bridge these gaps, we introduce a novel model-based intrinsic drive explicitly designed to capture robust autonomous exploration observed in animals. Our method (3M-Progress) motivates naturalistic behavior by tracking divergence between the agent's current world model and an ethological prior. We demonstrate that artificial embodied agents trained with 3M-Progress capture the explainable variance in behavioral patterns and whole-brain neural-glial dynamics recorded from autonomously-behaving larval zebrafish, introducing the first goal-driven, population-level model of neural-glial computation. Our findings establish a computational framework connecting model-based intrinsic motivation to naturalistic behavior, providing a foundation for building artificial agents with animal-like autonomy.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supporting architecture evaluation for ATAM scenarios with LLMs</title>
<link>https://arxiv.org/abs/2506.00150</link>
<guid>https://arxiv.org/abs/2506.00150</guid>
<content:encoded><![CDATA[
arXiv:2506.00150v1 Announce Type: cross 
Abstract: Architecture evaluation methods have long been used to evaluate software designs. Several evaluation methods have been proposed and used to analyze tradeoffs between different quality attributes. Having competing qualities leads to conflicts for selecting which quality-attribute scenarios are the most suitable ones that an architecture should tackle and for prioritizing the scenarios required by the stakeholders. In this context, architecture evaluation is carried out manually, often involving long brainstorming sessions to decide which are the most adequate quality scenarios. To reduce this effort and make the assessment and selection of scenarios more efficient, we suggest the usage of LLMs to partially automate evaluation activities. As a first step to validate this hypothesis, this work studies MS Copilot as an LLM tool to analyze quality scenarios suggested by students in a software architecture course and compares the students' results with the assessment provided by the LLM. Our initial study reveals that the LLM produces in most cases better and more accurate results regarding the risks, sensitivity points and tradeoff analysis of the quality scenarios. Overall, the use of generative AI has the potential to partially automate and support the architecture evaluation tasks, improving the human decision-making process.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detection of Endangered Deer Species Using UAV Imagery: A Comparative Study Between Efficient Deep Learning Approaches</title>
<link>https://arxiv.org/abs/2506.00154</link>
<guid>https://arxiv.org/abs/2506.00154</guid>
<content:encoded><![CDATA[
arXiv:2506.00154v1 Announce Type: cross 
Abstract: This study compares the performance of state-of-the-art neural networks including variants of the YOLOv11 and RT-DETR models for detecting marsh deer in UAV imagery, in scenarios where specimens occupy a very small portion of the image and are occluded by vegetation. We extend previous analysis adding precise segmentation masks for our datasets enabling a fine-grained training of a YOLO model with a segmentation head included. Experimental results show the effectiveness of incorporating the segmentation head achieving superior detection performance. This work contributes valuable insights for improving UAV-based wildlife monitoring and conservation strategies through scalable and accurate AI-driven detection systems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangled Safety Adapters Enable Efficient Guardrails and Flexible Inference-Time Alignment</title>
<link>https://arxiv.org/abs/2506.00166</link>
<guid>https://arxiv.org/abs/2506.00166</guid>
<content:encoded><![CDATA[
arXiv:2506.00166v1 Announce Type: cross 
Abstract: Existing paradigms for ensuring AI safety, such as guardrail models and alignment training, often compromise either inference efficiency or development flexibility. We introduce Disentangled Safety Adapters (DSA), a novel framework addressing these challenges by decoupling safety-specific computations from a task-optimized base model. DSA utilizes lightweight adapters that leverage the base model's internal representations, enabling diverse and flexible safety functionalities with minimal impact on inference cost. Empirically, DSA-based safety guardrails substantially outperform comparably sized standalone models, notably improving hallucination detection (0.88 vs. 0.61 AUC on Summedits) and also excelling at classifying hate speech (0.98 vs. 0.92 on ToxiGen) and unsafe model inputs and responses (0.93 vs. 0.90 on AEGIS2.0 & BeaverTails). Furthermore, DSA-based safety alignment allows dynamic, inference-time adjustment of alignment strength and a fine-grained trade-off between instruction following performance and model safety. Importantly, combining the DSA safety guardrail with DSA safety alignment facilitates context-dependent alignment strength, boosting safety on StrongReject by 93% while maintaining 98% performance on MTBench -- a total reduction in alignment tax of 8 percentage points compared to standard safety alignment fine-tuning. Overall, DSA presents a promising path towards more modular, efficient, and adaptable AI safety and alignment.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accountability Attribution: Tracing Model Behavior to Training Processes</title>
<link>https://arxiv.org/abs/2506.00175</link>
<guid>https://arxiv.org/abs/2506.00175</guid>
<content:encoded><![CDATA[
arXiv:2506.00175v1 Announce Type: cross 
Abstract: Modern AI development pipelines often involve multiple stages-pretraining, fine-tuning rounds, and subsequent adaptation or alignment-with numerous model update steps within each stage. This raises a critical question of accountability: when a deployed model succeeds or fails, which stage is responsible, and to what extent? We pose the problem of accountability attribution, which aims to trace model behavior back to specific stages of the training process. To address this, we propose a general framework that answers counterfactual questions about stage effects: how would the model behavior have changed if the updates from a training stage had not been executed?. Within this framework, we introduce estimators based on first-order approximations that efficiently quantify the stage effects without retraining. Our estimators account for both the training data and key aspects of optimization dynamics, including learning rate schedules, momentum, and weight decay. Empirically, we demonstrate that our approach identifies training stages accountable for specific behaviors, offering a practical tool for model analysis and a step toward more accountable AI development.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pushing the Limits of Beam Search Decoding for Transducer-based ASR models</title>
<link>https://arxiv.org/abs/2506.00185</link>
<guid>https://arxiv.org/abs/2506.00185</guid>
<content:encoded><![CDATA[
arXiv:2506.00185v1 Announce Type: cross 
Abstract: Transducer models have emerged as a promising choice for end-to-end ASR systems, offering a balanced trade-off between recognition accuracy, streaming capabilities, and inference speed in greedy decoding. However, beam search significantly slows down Transducers due to repeated evaluations of key network components, limiting practical applications. This paper introduces a universal method to accelerate beam search for Transducers, enabling the implementation of two optimized algorithms: ALSD++ and AES++. The proposed method utilizes batch operations, a tree-based hypothesis structure, novel blank scoring for enhanced shallow fusion, and CUDA graph execution for efficient GPU inference. This narrows the speed gap between beam and greedy modes to only 10-20% for the whole system, achieves 14-30% relative improvement in WER compared to greedy decoding, and improves shallow fusion for low-resource up to 11% compared to existing implementations. All the algorithms are open sourced.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heterogeneous Graph Backdoor Attack</title>
<link>https://arxiv.org/abs/2506.00191</link>
<guid>https://arxiv.org/abs/2506.00191</guid>
<content:encoded><![CDATA[
arXiv:2506.00191v1 Announce Type: cross 
Abstract: Heterogeneous Graph Neural Networks (HGNNs) excel in modeling complex, multi-typed relationships across diverse domains, yet their vulnerability to backdoor attacks remains unexplored. To address this gap, we conduct the first investigation into the susceptibility of HGNNs to existing graph backdoor attacks, revealing three critical issues: (1) high attack budget required for effective backdoor injection, (2) inefficient and unreliable backdoor activation, and (3) inaccurate attack effectiveness evaluation. To tackle these issues, we propose the Heterogeneous Graph Backdoor Attack (HGBA), the first backdoor attack specifically designed for HGNNs, introducing a novel relation-based trigger mechanism that establishes specific connections between a strategically selected trigger node and poisoned nodes via the backdoor metapath. HGBA achieves efficient and stealthy backdoor injection with minimal structural modifications and supports easy backdoor activation through two flexible strategies: Self-Node Attack and Indiscriminate Attack. Additionally, we improve the ASR measurement protocol, enabling a more accurate assessment of attack effectiveness. Extensive experiments demonstrate that HGBA far surpasses multiple state-of-the-art graph backdoor attacks in black-box settings, efficiently attacking HGNNs with low attack budgets. Ablation studies show that the strength of HBGA benefits from our trigger node selection method and backdoor metapath selection strategy. In addition, HGBA shows superior robustness against node feature perturbations and multiple types of existing graph backdoor defense mechanisms. Finally, extension experiments demonstrate that the relation-based trigger mechanism can effectively extend to tasks in homogeneous graph scenarios, thereby posing severe threats to broader security-critical domains.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Let Them Down Easy! Contextual Effects of LLM Guardrails on User Perceptions and Preferences</title>
<link>https://arxiv.org/abs/2506.00195</link>
<guid>https://arxiv.org/abs/2506.00195</guid>
<content:encoded><![CDATA[
arXiv:2506.00195v1 Announce Type: cross 
Abstract: Current LLMs are trained to refuse potentially harmful input queries regardless of whether users actually had harmful intents, causing a tradeoff between safety and user experience. Through a study of 480 participants evaluating 3,840 query-response pairs, we examine how different refusal strategies affect user perceptions across varying motivations. Our findings reveal that response strategy largely shapes user experience, while actual user motivation has negligible impact. Partial compliance -- providing general information without actionable details -- emerges as the optimal strategy, reducing negative user perceptions by over 50% to flat-out refusals. Complementing this, we analyze response patterns of 9 state-of-the-art LLMs and evaluate how 6 reward models score different refusal strategies, demonstrating that models rarely deploy partial compliance naturally and reward models currently undervalue it. This work demonstrates that effective guardrails require focusing on crafting thoughtful refusals rather than detecting intent, offering a path toward AI safety mechanisms that ensure both safety and sustained user engagement.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOFGPT: Generative Design of Metal-Organic Frameworks using Language Models</title>
<link>https://arxiv.org/abs/2506.00198</link>
<guid>https://arxiv.org/abs/2506.00198</guid>
<content:encoded><![CDATA[
arXiv:2506.00198v1 Announce Type: cross 
Abstract: The discovery of Metal-Organic Frameworks (MOFs) with application-specific properties remains a central challenge in materials chemistry, owing to the immense size and complexity of their structural design space. Conventional computational screening techniques such as molecular simulations and density functional theory (DFT), while accurate, are computationally prohibitive at scale. Machine learning offers an exciting alternative by leveraging data-driven approaches to accelerate materials discovery. The complexity of MOFs, with their extended periodic structures and diverse topologies, creates both opportunities and challenges for generative modeling approaches. To address these challenges, we present a reinforcement learning-enhanced, transformer-based framework for the de novo design of MOFs. Central to our approach is MOFid, a chemically-informed string representation encoding both connectivity and topology, enabling scalable generative modeling. Our pipeline comprises three components: (1) a generative GPT model trained on MOFid sequences, (2) MOFormer, a transformer-based property predictor, and (3) a reinforcement learning (RL) module that optimizes generated candidates via property-guided reward functions. By integrating property feedback into sequence generation, our method drives the model toward synthesizable, topologically valid MOFs with desired functional attributes. This work demonstrates the potential of large language models, when coupled with reinforcement learning, to accelerate inverse design in reticular chemistry and unlock new frontiers in computational MOF discovery.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The World As Large Language Models See It: Exploring the reliability of LLMs in representing geographical features</title>
<link>https://arxiv.org/abs/2506.00203</link>
<guid>https://arxiv.org/abs/2506.00203</guid>
<content:encoded><![CDATA[
arXiv:2506.00203v1 Announce Type: cross 
Abstract: As large language models (LLMs) continue to evolve, questions about their trustworthiness in delivering factual information have become increasingly important. This concern also applies to their ability to accurately represent the geographic world. With recent advancements in this field, it is relevant to consider whether and to what extent LLMs' representations of the geographical world can be trusted. This study evaluates the performance of GPT-4o and Gemini 2.0 Flash in three key geospatial tasks: geocoding, elevation estimation, and reverse geocoding. In the geocoding task, both models exhibited systematic and random errors in estimating the coordinates of St. Anne's Column in Innsbruck, Austria, with GPT-4o showing greater deviations and Gemini 2.0 Flash demonstrating more precision but a significant systematic offset. For elevation estimation, both models tended to underestimate elevations across Austria, though they captured overall topographical trends, and Gemini 2.0 Flash performed better in eastern regions. The reverse geocoding task, which involved identifying Austrian federal states from coordinates, revealed that Gemini 2.0 Flash outperformed GPT-4o in overall accuracy and F1-scores, demonstrating better consistency across regions. Despite these findings, neither model achieved an accurate reconstruction of Austria's federal states, highlighting persistent misclassifications. The study concludes that while LLMs can approximate geographic information, their accuracy and reliability are inconsistent, underscoring the need for fine-tuning with geographical information to enhance their utility in GIScience and Geoinformatics.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structure-Aware Fill-in-the-Middle Pretraining for Code</title>
<link>https://arxiv.org/abs/2506.00204</link>
<guid>https://arxiv.org/abs/2506.00204</guid>
<content:encoded><![CDATA[
arXiv:2506.00204v1 Announce Type: cross 
Abstract: Fill-in-the-Middle (FIM) is a common pretraining method for code LLMs, where models complete code segments given surrounding context. However, existing LLMs treat code as plain text and mask random character spans. We propose and evaluate AST-FIM, a pretraining strategy that leverages Abstract Syntax Trees (ASTs) to mask complete syntactic structures at scale, ensuring coherent training examples better aligned with universal code structures and common code editing patterns such as blocks, expressions, or functions. To evaluate real-world fill-in-the-middle (FIM) programming tasks, we introduce Real-FIM-Eval, a benchmark derived from 30,000+ GitHub commits across 12 languages. On infilling tasks, experiments on 1B and 8B parameter models show that AST-FIM is particularly beneficial for real-world code editing as it outperforms standard random-character FIM by up to 5 pts on standard FIM benchmarks. Our code is publicly available at https://github.com/gonglinyuan/ast_fim.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REIC: RAG-Enhanced Intent Classification at Scale</title>
<link>https://arxiv.org/abs/2506.00210</link>
<guid>https://arxiv.org/abs/2506.00210</guid>
<content:encoded><![CDATA[
arXiv:2506.00210v1 Announce Type: cross 
Abstract: Accurate intent classification is critical for efficient routing in customer service, ensuring customers are connected with the most suitable agents while reducing handling times and operational costs. However, as companies expand their product lines, intent classification faces scalability challenges due to the increasing number of intents and variations in taxonomy across different verticals. In this paper, we introduce REIC, a Retrieval-augmented generation Enhanced Intent Classification approach, which addresses these challenges effectively. REIC leverages retrieval-augmented generation (RAG) to dynamically incorporate relevant knowledge, enabling precise classification without the need for frequent retraining. Through extensive experiments on real-world datasets, we demonstrate that REIC outperforms traditional fine-tuning, zero-shot, and few-shot methods in large-scale customer service settings. Our results highlight its effectiveness in both in-domain and out-of-domain scenarios, demonstrating its potential for real-world deployment in adaptive and large-scale intent classification systems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diff-SPORT: Diffusion-based Sensor Placement Optimization and Reconstruction of Turbulent flows in urban environments</title>
<link>https://arxiv.org/abs/2506.00214</link>
<guid>https://arxiv.org/abs/2506.00214</guid>
<content:encoded><![CDATA[
arXiv:2506.00214v1 Announce Type: cross 
Abstract: Rapid urbanization demands accurate and efficient monitoring of turbulent wind patterns to support air quality, climate resilience and infrastructure design. Traditional sparse reconstruction and sensor placement strategies face major accuracy degradations under practical constraints. Here, we introduce Diff-SPORT, a diffusion-based framework for high-fidelity flow reconstruction and optimal sensor placement in urban environments. Diff-SPORT combines a generative diffusion model with a maximum a posteriori (MAP) inference scheme and a Shapley-value attribution framework to propose a scalable and interpretable solution. Compared to traditional numerical methods, Diff-SPORT achieves significant speedups while maintaining both statistical and instantaneous flow fidelity. Our approach offers a modular, zero-shot alternative to retraining-intensive strategies, supporting fast and reliable urban flow monitoring under extreme sparsity. Diff-SPORT paves the way for integrating generative modeling and explainability in sustainable urban intelligence.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ctrl-Crash: Controllable Diffusion for Realistic Car Crashes</title>
<link>https://arxiv.org/abs/2506.00227</link>
<guid>https://arxiv.org/abs/2506.00227</guid>
<content:encoded><![CDATA[
arXiv:2506.00227v1 Announce Type: cross 
Abstract: Video diffusion techniques have advanced significantly in recent years; however, they struggle to generate realistic imagery of car crashes due to the scarcity of accident events in most driving datasets. Improving traffic safety requires realistic and controllable accident simulations. To tackle the problem, we propose Ctrl-Crash, a controllable car crash video generation model that conditions on signals such as bounding boxes, crash types, and an initial image frame. Our approach enables counterfactual scenario generation where minor variations in input can lead to dramatically different crash outcomes. To support fine-grained control at inference time, we leverage classifier-free guidance with independently tunable scales for each conditioning signal. Ctrl-Crash achieves state-of-the-art performance across quantitative video quality metrics (e.g., FVD and JEDi) and qualitative measurements based on a human-evaluation of physical realism and video quality compared to prior diffusion-based methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Localized LoRA: A Structured Low-Rank Approximation for Efficient Fine-Tuning</title>
<link>https://arxiv.org/abs/2506.00236</link>
<guid>https://arxiv.org/abs/2506.00236</guid>
<content:encoded><![CDATA[
arXiv:2506.00236v1 Announce Type: cross 
Abstract: Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, offer compact and effective alternatives to full model fine-tuning by introducing low-rank updates to pretrained weights. However, most existing approaches rely on global low-rank structures, which can overlook spatial patterns spread across the parameter space. In this work, we propose Localized LoRA, a generalized framework that models weight updates as a composition of low-rank matrices applied to structured blocks of the weight matrix. This formulation enables dense, localized updates throughout the parameter space-without increasing the total number of trainable parameters. We provide a formal comparison between global, diagonal-local, and fully localized low-rank approximations, and show that our method consistently achieves lower approximation error under matched parameter budgets. Experiments on both synthetic and practical settings demonstrate that Localized LoRA offers a more expressive and adaptable alternative to existing methods, enabling efficient fine-tuning with improved performance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Designing AI Tools for Clinical Care Teams to Support Serious Illness Conversations with Older Adults in the Emergency Department</title>
<link>https://arxiv.org/abs/2506.00241</link>
<guid>https://arxiv.org/abs/2506.00241</guid>
<content:encoded><![CDATA[
arXiv:2506.00241v1 Announce Type: cross 
Abstract: Serious illness conversations (SICs), discussions between clinical care teams and patients with serious, life-limiting illnesses about their values, goals, and care preferences, are critical for patient-centered care. Without these conversations, patients often receive aggressive interventions that may not align with their goals. Clinical care teams face significant barriers when conducting serious illness conversations with older adult patients in Emergency Department (ED) settings, where most older adult patients lack documented treatment goals. To understand current practices and identify AI support opportunities, we conducted interviews with two domain experts and nine ED clinical care team members. Through thematic analysis, we characterized a four-phase serious illness conversation workflow (identification, preparation, conduction, documentation) and identified key needs and challenges at each stage. Clinical care teams struggle with fragmented EHR data access, time constraints, emotional preparation demands, and documentation burdens. While participants expressed interest in AI tools for information synthesis, conversational support, and automated documentation, they emphasized preserving human connection and clinical autonomy. We present design guidelines for AI tools supporting SIC workflows that fit within existing clinical practices. This work contributes empirical understanding of ED-based serious illness conversations and provides design considerations for AI in high-stakes clinical environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligned but Blind: Alignment Increases Implicit Bias by Reducing Awareness of Race</title>
<link>https://arxiv.org/abs/2506.00253</link>
<guid>https://arxiv.org/abs/2506.00253</guid>
<content:encoded><![CDATA[
arXiv:2506.00253v1 Announce Type: cross 
Abstract: Although value-aligned language models (LMs) appear unbiased in explicit bias evaluations, they often exhibit stereotypes in implicit word association tasks, raising concerns about their fair usage. We investigate the mechanisms behind this discrepancy and find that alignment surprisingly amplifies implicit bias in model outputs. Specifically, we show that aligned LMs, unlike their unaligned counterparts, overlook racial concepts in early internal representations when the context is ambiguous. Not representing race likely fails to activate safety guardrails, leading to unintended biases. Inspired by this insight, we propose a new bias mitigation strategy that works by incentivizing the representation of racial concepts in the early model layers. In contrast to conventional mitigation methods of machine unlearning, our interventions find that steering the model to be more aware of racial concepts effectively mitigates implicit bias. Similar to race blindness in humans, ignoring racial nuances can inadvertently perpetuate subtle biases in LMs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chances and Challenges of the Model Context Protocol in Digital Forensics and Incident Response</title>
<link>https://arxiv.org/abs/2506.00274</link>
<guid>https://arxiv.org/abs/2506.00274</guid>
<content:encoded><![CDATA[
arXiv:2506.00274v1 Announce Type: cross 
Abstract: Large language models hold considerable promise for supporting forensic investigations, but their widespread adoption is hindered by a lack of transparency, explainability, and reproducibility. This paper explores how the emerging Model Context Protocol can address these challenges and support the meaningful use of LLMs in digital forensics. Through a theoretical analysis, we examine how MCP can be integrated across various forensic scenarios - ranging from artifact analysis to the generation of interpretable reports. We also outline both technical and conceptual considerations for deploying an MCP server in forensic environments. Our analysis reveals a wide range of use cases in which MCP not only strengthens existing forensic workflows but also facilitates the application of LLMs to areas of forensics where their use was previously limited. Furthermore, we introduce the concept of the inference constraint level - a way of characterizing how specific MCP design choices can deliberately constrain model behavior, thereby enhancing both auditability and traceability. Our insights demonstrate that MCP has significant potential as a foundational component for developing LLM-assisted forensic workflows that are not only more transparent, reproducible, and legally defensible, but also represent a step toward increased automation in digital forensic analysis. However, we also highlight potential challenges that the adoption of MCP may pose for digital forensics in the future.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Level-Wise News Article Clustering via Multilingual Matryoshka Embeddings</title>
<link>https://arxiv.org/abs/2506.00277</link>
<guid>https://arxiv.org/abs/2506.00277</guid>
<content:encoded><![CDATA[
arXiv:2506.00277v1 Announce Type: cross 
Abstract: Contextual large language model embeddings are increasingly utilized for topic modeling and clustering. However, current methods often scale poorly, rely on opaque similarity metrics, and struggle in multilingual settings. In this work, we present a novel, scalable, interpretable, hierarchical, and multilingual approach to clustering news articles and social media data. To do this, we first train multilingual Matryoshka embeddings that can determine story similarity at varying levels of granularity based on which subset of the dimensions of the embeddings is examined. This embedding model achieves state-of-the-art performance on the SemEval 2022 Task 8 test dataset (Pearson $\rho$ = 0.816). Once trained, we develop an efficient hierarchical clustering algorithm that leverages the hierarchical nature of Matryoshka embeddings to identify unique news stories, narratives, and themes. We conclude by illustrating how our approach can identify and cluster stories, narratives, and overarching themes within real-world news datasets.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Threat Vectors and Risk Mitigation for Retrieval-Augmented Generation Systems</title>
<link>https://arxiv.org/abs/2506.00281</link>
<guid>https://arxiv.org/abs/2506.00281</guid>
<content:encoded><![CDATA[
arXiv:2506.00281v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) systems, which integrate Large Language Models (LLMs) with external knowledge sources, are vulnerable to a range of adversarial attack vectors. This paper examines the importance of RAG systems through recent industry adoption trends and identifies the prominent attack vectors for RAG: prompt injection, data poisoning, and adversarial query manipulation. We analyze these threats under risk management lens, and propose robust prioritized control list that includes risk-mitigating actions like input validation, adversarial training, and real-time monitoring.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entropic Risk Optimization in Discounted MDPs: Sample Complexity Bounds with a Generative Model</title>
<link>https://arxiv.org/abs/2506.00286</link>
<guid>https://arxiv.org/abs/2506.00286</guid>
<content:encoded><![CDATA[
arXiv:2506.00286v1 Announce Type: cross 
Abstract: In this paper we analyze the sample complexities of learning the optimal state-action value function $Q^*$ and an optimal policy $\pi^*$ in a discounted Markov decision process (MDP) where the agent has recursive entropic risk-preferences with risk-parameter $\beta\neq 0$ and where a generative model of the MDP is available. We provide and analyze a simple model based approach which we call model-based risk-sensitive $Q$-value-iteration (MB-RS-QVI) which leads to $(\epsilon,\delta)$-PAC-bounds on $\|Q^*-Q^k\|$, and $\|V^*-V^{\pi_k}\|$ where $Q_k$ is the output of MB-RS-QVI after k iterations and $\pi_k$ is the greedy policy with respect to $Q_k$. Both PAC-bounds have exponential dependence on the effective horizon $\frac{1}{1-\gamma}$ and the strength of this dependence grows with the learners risk-sensitivity $|\beta|$. We also provide two lower bounds which shows that exponential dependence on $|\beta|\frac{1}{1-\gamma}$ is unavoidable in both cases. The lower bounds reveal that the PAC-bounds are both tight in $\varepsilon$ and $\delta$ and that the PAC-bound on $Q$-learning is tight in the number of actions $A$, and that the PAC-bound on policy-learning is nearly tight in $A$.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergent Abilities of Large Language Models under Continued Pretraining for Language Adaptation</title>
<link>https://arxiv.org/abs/2506.00288</link>
<guid>https://arxiv.org/abs/2506.00288</guid>
<content:encoded><![CDATA[
arXiv:2506.00288v1 Announce Type: cross 
Abstract: Continued pretraining (CPT) is a popular approach to adapt existing large language models (LLMs) to new languages. When doing so, it is common practice to include a portion of English data in the mixture, but its role has not been carefully studied to date. In this work, we show that including English does not impact validation perplexity, yet it is critical for the emergence of downstream capabilities in the target language. We introduce a language-agnostic benchmark for in-context learning (ICL), which reveals catastrophic forgetting early on CPT when English is not included. This in turn damages the ability of the model to generalize to downstream prompts in the target language as measured by perplexity, even if it does not manifest in terms of accuracy until later in training, and can be tied to a big shift in the model parameters. Based on these insights, we introduce curriculum learning and exponential moving average (EMA) of weights as effective alternatives to mitigate the need for English. All in all, our work sheds light into the dynamics by which emergent abilities arise when doing CPT for language adaptation, and can serve as a foundation to design more effective methods in the future.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Protein Sequence Design through Designability Preference Optimization</title>
<link>https://arxiv.org/abs/2506.00297</link>
<guid>https://arxiv.org/abs/2506.00297</guid>
<content:encoded><![CDATA[
arXiv:2506.00297v1 Announce Type: cross 
Abstract: Protein sequence design methods have demonstrated strong performance in sequence generation for de novo protein design. However, as the training objective was sequence recovery, it does not guarantee designability--the likelihood that a designed sequence folds into the desired structure. To bridge this gap, we redefine the training objective by steering sequence generation toward high designability. To do this, we integrate Direct Preference Optimization (DPO), using AlphaFold pLDDT scores as the preference signal, which significantly improves the in silico design success rate. To further refine sequence generation at a finer, residue-level granularity, we introduce Residue-level Designability Preference Optimization (ResiDPO), which applies residue-level structural rewards and decouples optimization across residues. This enables direct improvement in designability while preserving regions that already perform well. Using a curated dataset with residue-level annotations, we fine-tune LigandMPNN with ResiDPO to obtain EnhancedMPNN, which achieves a nearly 3-fold increase in in silico design success rate (from 6.56% to 17.57%) on a challenging enzyme design benchmark.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lossless Token Sequence Compression via Meta-Tokens</title>
<link>https://arxiv.org/abs/2506.00307</link>
<guid>https://arxiv.org/abs/2506.00307</guid>
<content:encoded><![CDATA[
arXiv:2506.00307v1 Announce Type: cross 
Abstract: Existing work on prompt compression for Large Language Models (LLM) focuses on lossy methods that try to maximize the retention of semantic information that is relevant to downstream tasks while significantly reducing the sequence length. In this paper, we introduce a task-agnostic lossless compression technique similar to LZ77 that makes it possible to reduce the input token sequence length on average by 27\% and 18\% for the two evaluation tasks explored here. Given that we use transformer-based LLMs, this equates to 47\% and 33\% less encoding computation, respectively, due to the quadratic nature of attention. The token sequence transformation is trivial to reverse and highlights that no semantic information is lost in the process. We evaluate our proposed approach on two tasks that require strict preservation of semantics/syntax and demonstrate that existing lossy compression methods perform poorly in this setting. We find that our lossless compression technique produces only a small gap in performance compared to using the uncompressed input and posit that larger models and an expanded computing budget would likely erase the gap entirely.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MythTriage: Scalable Detection of Opioid Use Disorder Myths on a Video-Sharing Platform</title>
<link>https://arxiv.org/abs/2506.00308</link>
<guid>https://arxiv.org/abs/2506.00308</guid>
<content:encoded><![CDATA[
arXiv:2506.00308v1 Announce Type: cross 
Abstract: Understanding the prevalence of misinformation in health topics online can inform public health policies and interventions. However, measuring such misinformation at scale remains a challenge, particularly for high-stakes but understudied topics like opioid-use disorder (OUD)--a leading cause of death in the U.S. We present the first large-scale study of OUD-related myths on YouTube, a widely-used platform for health information. With clinical experts, we validate 8 pervasive myths and release an expert-labeled video dataset. To scale labeling, we introduce MythTriage, an efficient triage pipeline that uses a lightweight model for routine cases and defers harder ones to a high-performing, but costlier, large language model (LLM). MythTriage achieves up to 0.86 macro F1-score while estimated to reduce annotation time and financial cost by over 76% compared to experts and full LLM labeling. We analyze 2.9K search results and 343K recommendations, uncovering how myths persist on YouTube and offering actionable insights for public health and platform moderation.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An evaluation of LLMs for generating movie reviews: GPT-4o, Gemini-2.0 and DeepSeek-V3</title>
<link>https://arxiv.org/abs/2506.00312</link>
<guid>https://arxiv.org/abs/2506.00312</guid>
<content:encoded><![CDATA[
arXiv:2506.00312v1 Announce Type: cross 
Abstract: Large language models (LLMs) have been prominent in various tasks, including text generation and summarisation. The applicability of LLMs to the generation of product reviews is gaining momentum, paving the way for the generation of movie reviews. In this study, we propose a framework that generates movie reviews using three LLMs (GPT-4o, DeepSeek-V3, and Gemini-2.0), and evaluate their performance by comparing the generated outputs with IMDb user reviews. We use movie subtitles and screenplays as input to the LLMs and investigate how they affect the quality of reviews generated. We review the LLM-based movie reviews in terms of vocabulary, sentiment polarity, similarity, and thematic consistency in comparison to IMDB user reviews. The results demonstrate that LLMs are capable of generating syntactically fluent and structurally complete movie reviews. Nevertheless, there is still a noticeable gap in emotional richness and stylistic coherence between LLM-generated and IMDb reviews, suggesting that further refinement is needed to improve the overall quality of movie review generation. We provided a survey-based analysis where participants were told to distinguish between LLM and IMDb user reviews. The results show that LLM-generated reviews are difficult to distinguish from IMDB user reviews. We found that DeepSeek-V3 produced the most balanced reviews, closely matching IMDb reviews. GPT-4o overemphasised positive emotions, while Gemini-2.0 captured negative emotions better but showed excessive emotional intensity.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>dpmm: Differentially Private Marginal Models, a Library for Synthetic Tabular Data Generation</title>
<link>https://arxiv.org/abs/2506.00322</link>
<guid>https://arxiv.org/abs/2506.00322</guid>
<content:encoded><![CDATA[
arXiv:2506.00322v1 Announce Type: cross 
Abstract: We propose dpmm, an open-source library for synthetic data generation with Differentially Private (DP) guarantees. It includes three popular marginal models -- PrivBayes, MST, and AIM -- that achieve superior utility and offer richer functionality compared to alternative implementations. Additionally, we adopt best practices to provide end-to-end DP guarantees and address well-known DP-related vulnerabilities. Our goal is to accommodate a wide audience with easy-to-install, highly customizable, and robust model implementations.
  Our codebase is available from https://github.com/sassoftware/dpmm.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Guidance in Diffusion Models for Perceptual Evaluations</title>
<link>https://arxiv.org/abs/2506.00327</link>
<guid>https://arxiv.org/abs/2506.00327</guid>
<content:encoded><![CDATA[
arXiv:2506.00327v1 Announce Type: cross 
Abstract: Despite recent advancements in latent diffusion models that generate high-dimensional image data and perform various downstream tasks, there has been little exploration into perceptual consistency within these models on the task of No-Reference Image Quality Assessment (NR-IQA). In this paper, we hypothesize that latent diffusion models implicitly exhibit perceptually consistent local regions within the data manifold. We leverage this insight to guide on-manifold sampling using perceptual features and input measurements. Specifically, we propose Perceptual Manifold Guidance (PMG), an algorithm that utilizes pretrained latent diffusion models and perceptual quality features to obtain perceptually consistent multi-scale and multi-timestep feature maps from the denoising U-Net. We empirically demonstrate that these hyperfeatures exhibit high correlation with human perception in IQA tasks. Our method can be applied to any existing pretrained latent diffusion model and is straightforward to integrate. To the best of our knowledge, this paper is the first work on guiding diffusion model with perceptual features for NR-IQA. Extensive experiments on IQA datasets show that our method, LGDM, achieves state-of-the-art performance, underscoring the superior generalization capabilities of diffusion models for NR-IQA tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foresight: Adaptive Layer Reuse for Accelerated and High-Quality Text-to-Video Generation</title>
<link>https://arxiv.org/abs/2506.00329</link>
<guid>https://arxiv.org/abs/2506.00329</guid>
<content:encoded><![CDATA[
arXiv:2506.00329v1 Announce Type: cross 
Abstract: Diffusion Transformers (DiTs) achieve state-of-the-art results in text-to-image, text-to-video generation, and editing. However, their large model size and the quadratic cost of spatial-temporal attention over multiple denoising steps make video generation computationally expensive. Static caching mitigates this by reusing features across fixed steps but fails to adapt to generation dynamics, leading to suboptimal trade-offs between speed and quality.
  We propose Foresight, an adaptive layer-reuse technique that reduces computational redundancy across denoising steps while preserving baseline performance. Foresight dynamically identifies and reuses DiT block outputs for all layers across steps, adapting to generation parameters such as resolution and denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and CogVideoX, Foresight achieves up to 1.63x end-to-end speedup, while maintaining video quality. The source code of Foresight is available at \texttt{https://github.com/STAR-Laboratory/foresight}.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recover Experimental Data with Selection Bias using Counterfactual Logic</title>
<link>https://arxiv.org/abs/2506.00335</link>
<guid>https://arxiv.org/abs/2506.00335</guid>
<content:encoded><![CDATA[
arXiv:2506.00335v1 Announce Type: cross 
Abstract: Selection bias, arising from the systematic inclusion or exclusion of certain samples, poses a significant challenge to the validity of causal inference. While Bareinboim et al. introduced methods for recovering unbiased observational and interventional distributions from biased data using partial external information, the complexity of the backdoor adjustment and the method's strong reliance on observational data limit its applicability in many practical settings. In this paper, we formally discover the recoverability of $P(Y^*_{x^*})$ under selection bias with experimental data. By explicitly constructing counterfactual worlds via Structural Causal Models (SCMs), we analyze how selection mechanisms in the observational world propagate to the counterfactual domain. We derive a complete set of graphical and theoretical criteria to determine that the experimental distribution remain unaffected by selection bias. Furthermore, we propose principled methods for leveraging partially unbiased observational data to recover $P(Y^*_{x^*})$ from biased experimental datasets. Simulation studies replicating realistic research scenarios demonstrate the practical utility of our approach, offering concrete guidance for mitigating selection bias in applied causal inference.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Latent Semantic Clustering for Scaling Test-Time Computation of LLMs</title>
<link>https://arxiv.org/abs/2506.00344</link>
<guid>https://arxiv.org/abs/2506.00344</guid>
<content:encoded><![CDATA[
arXiv:2506.00344v1 Announce Type: cross 
Abstract: Scaling test-time computation--generating and analyzing multiple or sequential outputs for a single input--has become a promising strategy for improving the reliability and quality of large language models (LLMs), as evidenced by advances in uncertainty quantification and multi-step reasoning. A key shared component is semantic clustering, which groups outputs that differ in form but convey the same meaning. Semantic clustering enables estimation of the distribution over the semantics of outputs and helps avoid redundant exploration of reasoning paths. However, existing approaches typically rely on external models, which introduce substantial computational overhead and often fail to capture context-aware semantics. We propose Latent Semantic Clustering (LSC), a lightweight and context-sensitive method that leverages the generator LLM's internal hidden states for clustering, eliminating the need for external models. Our extensive experiment across various LLMs and datasets shows that LSC significantly improves the computational efficiency of test-time scaling while maintaining or exceeding the performance of existing methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Winning: Margin of Victory Relative to Expectation Unlocks Accurate Skill Ratings</title>
<link>https://arxiv.org/abs/2506.00348</link>
<guid>https://arxiv.org/abs/2506.00348</guid>
<content:encoded><![CDATA[
arXiv:2506.00348v1 Announce Type: cross 
Abstract: Knowledge of accurate relative skills in any competitive system is essential, but foundational approaches such as ELO discard extremely relevant performance data by concentrating exclusively on binary outcomes. While margin of victory (MOV) extensions exist, they often lack a definitive method for incorporating this information. We introduce Margin of Victory Differential Analysis (MOVDA), a framework that enhances traditional rating systems by using the deviation between the true MOV and a $\textit{modeled expectation}$. MOVDA learns a domain-specific, non-linear function (a scaled hyperbolic tangent that captures saturation effects and home advantage) to predict expected MOV based on rating differentials. Crucially, the $\textit{difference}$ between the true and expected MOV provides a subtle and weighted signal for rating updates, highlighting informative deviations in all levels of contests. Extensive experiments on professional NBA basketball data (from 2013 to 2023, with 13,619 games) show that MOVDA significantly outperforms standard ELO and Bayesian baselines. MOVDA reduces Brier score prediction error by $1.54\%$ compared to TrueSkill, increases outcome accuracy by $0.58\%$, and most importantly accelerates rating convergence by $13.5\%$, while maintaining the computational efficiency of the original ELO updates. MOVDA offers a theoretically motivated, empirically superior, and computationally lean approach to integrating performance magnitude into skill rating for competitive environments like the NBA.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enabling Secure and Ephemeral AI Workloads in Data Mesh Environments</title>
<link>https://arxiv.org/abs/2506.00352</link>
<guid>https://arxiv.org/abs/2506.00352</guid>
<content:encoded><![CDATA[
arXiv:2506.00352v1 Announce Type: cross 
Abstract: Many large enterprises that operate highly governed and complex ICT environments have no efficient and effective way to support their Data and AI teams in rapidly spinning up and tearing down self-service data and compute infrastructure, to experiment with new data analytic tools, and deploy data products into operational use. This paper proposes a key piece of the solution to the overall problem, in the form of an on-demand self-service data-platform infrastructure to empower de-centralised data teams to build data products on top of centralised templates, policies and governance. The core innovation is an efficient method to leverage immutable container operating systems and infrastructure-as-code methodologies for creating, from scratch, vendor-neutral and short-lived Kubernetes clusters on-premises and in any cloud environment. Our proposed approach can serve as a repeatable, portable and cost-efficient alternative or complement to commercial Platform-as-a-Service (PaaS) offerings, and this is particularly important in supporting interoperability in complex data mesh environments with a mix of modern and legacy compute infrastructure.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Performance of Perforated Backpropagation through Further Experiments</title>
<link>https://arxiv.org/abs/2506.00356</link>
<guid>https://arxiv.org/abs/2506.00356</guid>
<content:encoded><![CDATA[
arXiv:2506.00356v1 Announce Type: cross 
Abstract: Perforated Backpropagation is a neural network optimization technique based on modern understanding of the computational importance of dendrites within biological neurons. This paper explores further experiments from the original publication, generated from a hackathon held at the Carnegie Mellon Swartz Center in February 2025. Students and local Pittsburgh ML practitioners were brought together to experiment with the Perforated Backpropagation algorithm on the datasets and models which they were using for their projects. Results showed that the system could enhance their projects, with up to 90% model compression without negative impact on accuracy, or up to 16% increased accuracy of their original models.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\texttt{AVROBUSTBENCH}$: Benchmarking the Robustness of Audio-Visual Recognition Models at Test-Time</title>
<link>https://arxiv.org/abs/2506.00358</link>
<guid>https://arxiv.org/abs/2506.00358</guid>
<content:encoded><![CDATA[
arXiv:2506.00358v1 Announce Type: cross 
Abstract: While recent audio-visual models have demonstrated impressive performance, their robustness to distributional shifts at test-time remains not fully understood. Existing robustness benchmarks mainly focus on single modalities, making them insufficient for thoroughly assessing the robustness of audio-visual models. Motivated by real-world scenarios where shifts can occur $\textit{simultaneously}$ in both audio and visual modalities, we introduce $\texttt{AVROBUSTBENCH}$, a comprehensive benchmark designed to evaluate the test-time robustness of audio-visual recognition models. $\texttt{AVROBUSTBENCH}$ comprises four audio-visual benchmark datasets, $\texttt{AUDIOSET-2C}$, $\texttt{VGGSOUND-2C}$, $\texttt{KINETICS-2C}$, and $\texttt{EPICKITCHENS-2C}$, each incorporating 75 bimodal audio-visual corruptions that are $\textit{co-occurring}$ and $\textit{correlated}$. Through extensive evaluations, we observe that state-of-the-art supervised and self-supervised audio-visual models exhibit declining robustness as corruption severity increases. Furthermore, online test-time adaptation (TTA) methods, on $\texttt{VGGSOUND-2C}$ and $\texttt{KINETICS-2C}$, offer minimal improvements in performance under bimodal corruptions. We further propose $\texttt{AV2C}$, a simple TTA approach enabling on-the-fly cross-modal fusion by penalizing high-entropy samples, which achieves improvements on $\texttt{VGGSOUND-2C}$. We hope that $\texttt{AVROBUSTBENCH}$ will steer the development of more effective and robust audio-visual TTA approaches. Our code is available $\href{https://github.com/sarthaxxxxx/AV-C-Robustness-Benchmark}{here}$.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Network-based Information-Theoretic Transceivers for High-Order Modulation Schemes</title>
<link>https://arxiv.org/abs/2506.00368</link>
<guid>https://arxiv.org/abs/2506.00368</guid>
<content:encoded><![CDATA[
arXiv:2506.00368v1 Announce Type: cross 
Abstract: Neural network (NN)-based end-to-end (E2E) communication systems, in which each system component may consist of a portion of a neural network, have been investigated as potential tools for developing artificial intelligence (Al)-native E2E systems. In this paper, we propose an NN-based bitwise receiver that improves computational efficiency while maintaining performance comparable to baseline demappers. Building on this foundation, we introduce a novel symbol-wise autoencoder (AE)-based E2E system that jointly optimizes the transmitter and receiver at the physical layer. We evaluate the proposed NN-based receiver using bit-error rate (BER) analysis to confirm that the numerical BER achieved by NN-based receivers or transceivers is accurate. Results demonstrate that the AE-based system outperforms baseline architectures, particularly for higher-order modulation schemes. We further show that the training signal-to-noise ratio (SNR) significantly affects the performance of the systems when inference is conducted at different SNR levels.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MagiCodec: Simple Masked Gaussian-Injected Codec for High-Fidelity Reconstruction and Generation</title>
<link>https://arxiv.org/abs/2506.00385</link>
<guid>https://arxiv.org/abs/2506.00385</guid>
<content:encoded><![CDATA[
arXiv:2506.00385v1 Announce Type: cross 
Abstract: Neural audio codecs have made significant strides in efficiently mapping raw audio waveforms into discrete token representations, which are foundational for contemporary audio generative models. However, most existing codecs are optimized primarily for reconstruction quality, often at the expense of the downstream modelability of the encoded tokens. Motivated by the need to overcome this bottleneck, we introduce $\textbf{MagiCodec}$, a novel single-layer, streaming Transformer-based audio codec. MagiCodec is designed with a multistage training pipeline that incorporates Gaussian noise injection and latent regularization, explicitly targeting the enhancement of semantic expressiveness in the generated codes while preserving high reconstruction fidelity. We analytically derive the effect of noise injection in the frequency domain, demonstrating its efficacy in attenuating high-frequency components and fostering robust tokenization. Extensive experimental evaluations show that MagiCodec surpasses state-of-the-art codecs in both reconstruction quality and downstream tasks. Notably, the tokens produced by MagiCodec exhibit Zipf-like distributions, as observed in natural languages, thereby improving compatibility with language-model-based generative architectures. The code and pre-trained models are available at https://github.com/Ereboas/MagiCodec.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Textual Gradients via Sampling-Based Momentum</title>
<link>https://arxiv.org/abs/2506.00400</link>
<guid>https://arxiv.org/abs/2506.00400</guid>
<content:encoded><![CDATA[
arXiv:2506.00400v1 Announce Type: cross 
Abstract: As prompts play an increasingly critical role in large language models (LLMs), optimizing textual prompts has become a crucial challenge. The Textual Gradient Descent (TGD) framework has emerged as a promising data-driven approach that iteratively refines textual prompts using LLM - suggested updates (or textual gradients) over minibatches of training samples. In this paper, we empirically demonstrate that scaling the number of training examples initially improves but later degrades TGD's performance across multiple downstream NLP tasks. However, while data scaling improves results for most tasks, it also significantly increases the computational cost when leveraging LLMs. To address this, we draw inspiration from numerical gradient descent and propose Textual Stochastic Gradient Descent with Momentum (TSGD-M) - a method that facilitates scalable in-context learning by reweighting prompt sampling based on past batch distributions. Across nine NLP tasks spanning three domains - including BIG-Bench Hard (BBH), natural language understanding tasks, and reasoning tasks - TSGD-M significantly outperforms TGD baselines that do not incorporate reweighted sampling, while also reducing variance in most tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bias as a Virtue: Rethinking Generalization under Distribution Shifts</title>
<link>https://arxiv.org/abs/2506.00407</link>
<guid>https://arxiv.org/abs/2506.00407</guid>
<content:encoded><![CDATA[
arXiv:2506.00407v1 Announce Type: cross 
Abstract: Machine learning models often degrade when deployed on data distributions different from their training data. Challenging conventional validation paradigms, we demonstrate that higher in-distribution (ID) bias can lead to better out-of-distribution (OOD) generalization. Our Adaptive Distribution Bridge (ADB) framework implements this insight by introducing controlled statistical diversity during training, enabling models to develop bias profiles that effectively generalize across distributions. Empirically, we observe a robust negative correlation where higher ID bias corresponds to lower OOD error--a finding that contradicts standard practices focused on minimizing validation error. Evaluation on multiple datasets shows our approach significantly improves OOD generalization. ADB achieves robust mean error reductions of up to 26.8% compared to traditional cross-validation, and consistently identifies high-performing training strategies, evidenced by percentile ranks often exceeding 74.4%. Our work provides both a practical method for improving generalization and a theoretical framework for reconsidering the role of bias in robust machine learning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks</title>
<link>https://arxiv.org/abs/2506.00411</link>
<guid>https://arxiv.org/abs/2506.00411</guid>
<content:encoded><![CDATA[
arXiv:2506.00411v1 Announce Type: cross 
Abstract: Real-world embodied agents face long-horizon tasks, characterized by high-level goals demanding multi-step solutions beyond single actions. Successfully navigating these requires both high-level task planning (i.e., decomposing goals into sub-tasks) and low-level motion control (i.e., generating precise robot actions). While existing vision language action (VLA) models and hierarchical architectures offer potential in embodied tasks, the former often falter in planning, and the latter can suffer from coordination issues, both hampering performance. We introduce a new unified VLA framework for long-horizon tasks, dubbed LoHoVLA, to overcome these limitations. LoHoVLA leverages a large pretrained vision language model (VLM) as the backbone to jointly generate language and action tokens for sub-task generation and robot action prediction, respectively. This shared representation promotes better generalization across tasks. Additionally, LoHoVLA embraces a hierarchical closed-loop control mechanism to mitigate errors originating from both high-level planning and low-level control. To train LoHoVLA, we introduce LoHoSet, a dataset built on the Ravens simulator, containing 20 long-horizon tasks, each with 1,000 expert demonstrations composed of visual observations, linguistic goals, sub-tasks, and robot actions. Experimental results show that LoHoVLA significantly surpasses both hierarchical and standard VLA approaches on long-horizon embodied tasks in the Ravens simulator. These findings underscore the promise of unified architectures for advancing generalizable embodied intelligence.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Diffusion LLMs via Adaptive Parallel Decoding</title>
<link>https://arxiv.org/abs/2506.00413</link>
<guid>https://arxiv.org/abs/2506.00413</guid>
<content:encoded><![CDATA[
arXiv:2506.00413v1 Announce Type: cross 
Abstract: The generation speed of LLMs are bottlenecked by autoregressive decoding, where tokens are predicted sequentially one by one. Alternatively, diffusion large language models (dLLMs) theoretically allow for parallel token generation, but in practice struggle to achieve the speed of autoregressive models without significantly sacrificing quality. We therefore introduce adaptive parallel decoding (APD), a novel method that dynamically adjusts the number of tokens sampled in parallel. We achieve this by defining a multiplicative mixture between the dLLM marginal probabilities and the joint probability of sequences under a small auxiliary autoregressive model. This inverts the standard setup of speculative decoding, where the goal is to sample from a large autoregressive verifier by drafting from a smaller model. We further optimize APD by enabling KV caching and limiting the size of the masked input. Altogether, our method puts forward three tunable parameters to flexibly tradeoff throughput and quality. We show that APD provides markedly higher throughput with minimal quality degradations on downstream benchmarks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wide Reflective Equilibrium in LLM Alignment: Bridging Moral Epistemology and AI Safety</title>
<link>https://arxiv.org/abs/2506.00415</link>
<guid>https://arxiv.org/abs/2506.00415</guid>
<content:encoded><![CDATA[
arXiv:2506.00415v1 Announce Type: cross 
Abstract: As large language models (LLMs) become more powerful and pervasive across society, ensuring these systems are beneficial, safe, and aligned with human values is crucial. Current alignment techniques, like Constitutional AI (CAI), involve complex iterative processes. This paper argues that the Method of Wide Reflective Equilibrium (MWRE) -- a well-established coherentist moral methodology -- offers a uniquely apt framework for understanding current LLM alignment efforts. Moreover, this methodology can substantively augment these processes by providing concrete pathways for improving their dynamic revisability, procedural legitimacy, and overall ethical grounding. Together, these enhancements can help produce more robust and ethically defensible outcomes. MWRE, emphasizing the achievement of coherence between our considered moral judgments, guiding moral principles, and relevant background theories, arguably better represents the intricate reality of LLM alignment and offers a more robust path to justification than prevailing foundationalist models or simplistic input-output evaluations. While current methods like CAI bear a structural resemblance to MWRE, they often lack its crucial emphasis on dynamic, bi-directional revision of principles and the procedural legitimacy derived from such a process. While acknowledging various disanalogies (e.g., consciousness, genuine understanding in LLMs), the paper demonstrates that MWRE serves as a valuable heuristic for critically analyzing current alignment efforts and for guiding the future development of more ethically sound and justifiably aligned AI systems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual Debiasing for Noisy In-Context Learning for Text Generation</title>
<link>https://arxiv.org/abs/2506.00418</link>
<guid>https://arxiv.org/abs/2506.00418</guid>
<content:encoded><![CDATA[
arXiv:2506.00418v1 Announce Type: cross 
Abstract: In context learning (ICL) relies heavily on high quality demonstrations drawn from large annotated corpora. Existing approaches detect noisy annotations by ranking local perplexities, presuming that noisy samples yield higher perplexities than their clean counterparts. However, this assumption breaks down when the noise ratio is high and many demonstrations are flawed. We reexamine the perplexity based paradigm for text generation under noisy annotations, highlighting two sources of bias in perplexity: the annotation itself and the domain specific knowledge inherent in large language models (LLMs). To overcome these biases, we introduce a dual debiasing framework that uses synthesized neighbors to explicitly correct perplexity estimates, yielding a robust Sample Cleanliness Score. This metric uncovers absolute sample cleanliness regardless of the overall corpus noise level. Extensive experiments demonstrate our method's superior noise detection capabilities and show that its final ICL performance is comparable to that of a fully clean demonstration corpus. Moreover, our approach remains robust even when noise ratios are extremely high.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A New Spatiotemporal Correlation Anomaly Detection Method that Integrates Contrastive Learning and Few-Shot Learning in Wireless Sensor Networks</title>
<link>https://arxiv.org/abs/2506.00420</link>
<guid>https://arxiv.org/abs/2506.00420</guid>
<content:encoded><![CDATA[
arXiv:2506.00420v1 Announce Type: cross 
Abstract: Detecting anomalies in the data collected by WSNs can provide crucial evidence for assessing the reliability and stability of WSNs. Existing methods for WSN anomaly detection often face challenges such as the limited extraction of spatiotemporal correlation features, the absence of sample labels, few anomaly samples, and an imbalanced sample distribution. To address these issues, a spatiotemporal correlation detection model (MTAD-RD) considering both model architecture and a two-stage training strategy perspective is proposed. In terms of model structure design, the proposed MTAD-RD backbone network includes a retentive network (RetNet) enhanced by a cross-retention (CR) module, a multigranular feature fusion module, and a graph attention network module to extract internode correlation information. This proposed model can integrate the intermodal correlation features and spatial features of WSN neighbor nodes while extracting global information from time series data. Moreover, its serialized inference characteristic can remarkably reduce inference overhead. For model training, a two-stage training approach was designed. First, a contrastive learning proxy task was designed for time series data with graph structure information in WSNs, enabling the backbone network to learn transferable features from unlabeled data using unsupervised contrastive learning methods, thereby addressing the issue of missing sample labels in the dataset. Then, a caching-based sample sampler was designed to divide samples into few-shot and contrastive learning data. A specific joint loss function was developed to jointly train the dual-graph discriminator network to address the problem of sample imbalance effectively. In experiments carried out on real public datasets, the designed MTAD-RD anomaly detection method achieved an F1 score of 90.97%, outperforming existing supervised WSN anomaly detection methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enabling Chatbots with Eyes and Ears: An Immersive Multimodal Conversation System for Dynamic Interactions</title>
<link>https://arxiv.org/abs/2506.00421</link>
<guid>https://arxiv.org/abs/2506.00421</guid>
<content:encoded><![CDATA[
arXiv:2506.00421v1 Announce Type: cross 
Abstract: As chatbots continue to evolve toward human-like, real-world, interactions, multimodality remains an active area of research and exploration. So far, efforts to integrate multimodality into chatbots have primarily focused on image-centric tasks, such as visual dialogue and image-based instructions, placing emphasis on the "eyes" of human perception while neglecting the "ears", namely auditory aspects. Moreover, these studies often center around static interactions that focus on discussing the modality rather than naturally incorporating it into the conversation, which limits the richness of simultaneous, dynamic engagement. Furthermore, while multimodality has been explored in multi-party and multi-session conversations, task-specific constraints have hindered its seamless integration into dynamic, natural conversations. To address these challenges, this study aims to equip chatbots with "eyes and ears" capable of more immersive interactions with humans. As part of this effort, we introduce a new multimodal conversation dataset, Multimodal Multi-Session Multi-Party Conversation ($M^3C$), and propose a novel multimodal conversation model featuring multimodal memory retrieval. Our model, trained on the $M^3C$, demonstrates the ability to seamlessly engage in long-term conversations with multiple speakers in complex, real-world-like settings, effectively processing visual and auditory inputs to understand and respond appropriately. Human evaluations highlight the model's strong performance in maintaining coherent and dynamic interactions, demonstrating its potential for advanced multimodal conversational agents.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning</title>
<link>https://arxiv.org/abs/2506.00424</link>
<guid>https://arxiv.org/abs/2506.00424</guid>
<content:encoded><![CDATA[
arXiv:2506.00424v1 Announce Type: cross 
Abstract: Sparse tensor programs are essential in deep learning and graph analytics, driving the need for optimized processing. To meet this demand, specialized hardware accelerators are being developed. Optimizing these programs for accelerators is challenging for two reasons: program performance is highly sensitive to variations in sparse inputs, and early-stage accelerators rely on expensive simulators. Therefore, ML-based cost models used for optimizing such programs on general-purpose hardware are often ineffective for early-stage accelerators, as they require large datasets for proper training. To this end, we introduce COGNATE, a novel framework that leverages inexpensive data samples from general-purpose hardware (e.g., CPUs) to train cost models, followed by few-shot fine-tuning on emerging hardware. COGNATE exploits the homogeneity of input features across hardware platforms while effectively mitigating heterogeneity, enabling cost model training with just 5% of the data samples needed by accelerator-specific models to achieve comparable performance. We conduct extensive experiments to demonstrate that COGNATE outperforms existing techniques, achieving average speedups of 1.47x (up to 5.46x) for SpMM and 1.39x (up to 4.22x) for SDDMM.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Channel Normalization for Time Series Channel Identification</title>
<link>https://arxiv.org/abs/2506.00432</link>
<guid>https://arxiv.org/abs/2506.00432</guid>
<content:encoded><![CDATA[
arXiv:2506.00432v1 Announce Type: cross 
Abstract: Channel identifiability (CID) refers to the ability to distinguish between individual channels in time series (TS) modeling. The absence of CID often results in producing identical outputs for identical inputs, disregarding channel-specific characteristics. In this paper, we highlight the importance of CID and propose Channel Normalization (CN), a simple yet effective normalization strategy that enhances CID by assigning distinct affine transformation parameters to each channel. We further extend CN in two ways: 1) Adaptive CN (ACN) dynamically adjusts parameters based on the input TS, improving adaptability in TS models, and 2) Prototypical CN (PCN) introduces a set of learnable prototypes instead of per-channel parameters, enabling applicability to datasets with unknown or varying number of channels and facilitating use in TS foundation models. We demonstrate the effectiveness of CN and its variants by applying them to various TS models, achieving significant performance gains for both non-CID and CID models. In addition, we analyze the success of our approach from an information theory perspective. Code is available at https://github.com/seunghan96/CN.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Double Positive and Unlabeled Data for Potential-Customer Identification</title>
<link>https://arxiv.org/abs/2506.00436</link>
<guid>https://arxiv.org/abs/2506.00436</guid>
<content:encoded><![CDATA[
arXiv:2506.00436v1 Announce Type: cross 
Abstract: In this study, we propose a method for identifying potential customers in targeted marketing by applying learning from positive and unlabeled data (PU learning). We consider a scenario in which a company sells a product and can observe only the customers who purchased it. Decision-makers seek to market products effectively based on whether people have loyalty to the company. Individuals with loyalty are those who are likely to remain interested in the company even without additional advertising. Consequently, those loyal customers would likely purchase from the company if they are interested in the product. In contrast, people with lower loyalty may overlook the product or buy similar products from other companies unless they receive marketing attention. Therefore, by focusing marketing efforts on individuals who are interested in the product but do not have strong loyalty, we can achieve more efficient marketing. To achieve this goal, we consider how to learn, from limited data, a classifier that identifies potential customers who (i) have interest in the product and (ii) do not have loyalty to the company. Although our algorithm comprises a single-stage optimization, its objective function implicitly contains two losses derived from standard PU learning settings. For this reason, we refer to our approach as double PU learning. We verify the validity of the proposed algorithm through numerical experiments, confirming that it functions appropriately for the problem at hand.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Your Explanation Reliable: Confidence-Aware Explanation on Graph Neural Networks</title>
<link>https://arxiv.org/abs/2506.00437</link>
<guid>https://arxiv.org/abs/2506.00437</guid>
<content:encoded><![CDATA[
arXiv:2506.00437v1 Announce Type: cross 
Abstract: Explaining Graph Neural Networks (GNNs) has garnered significant attention due to the need for interpretability, enabling users to understand the behavior of these black-box models better and extract valuable insights from their predictions. While numerous post-hoc instance-level explanation methods have been proposed to interpret GNN predictions, the reliability of these explanations remains uncertain, particularly in the out-of-distribution or unknown test datasets. In this paper, we address this challenge by introducing an explainer framework with the confidence scoring module ( ConfExplainer), grounded in theoretical principle, which is generalized graph information bottleneck with confidence constraint (GIB-CC), that quantifies the reliability of generated explanations. Experimental results demonstrate the superiority of our approach, highlighting the effectiveness of the confidence score in enhancing the trustworthiness and robustness of GNN explanations.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RLAE: Reinforcement Learning-Assisted Ensemble for LLMs</title>
<link>https://arxiv.org/abs/2506.00439</link>
<guid>https://arxiv.org/abs/2506.00439</guid>
<content:encoded><![CDATA[
arXiv:2506.00439v1 Announce Type: cross 
Abstract: Ensembling large language models (LLMs) can effectively combine diverse strengths of different models, offering a promising approach to enhance performance across various tasks. However, existing methods typically rely on fixed weighting strategies that fail to adapt to the dynamic, context-dependent characteristics of LLM capabilities. In this work, we propose Reinforcement Learning-Assisted Ensemble for LLMs (RLAE), a novel framework that reformulates LLM ensemble through the lens of a Markov Decision Process (MDP). Our approach introduces a RL agent that dynamically adjusts ensemble weights by considering both input context and intermediate generation states, with the agent being trained using rewards that directly correspond to the quality of final outputs. We implement RLAE using both single-agent and multi-agent reinforcement learning algorithms ($\text{RLAE}_\text{PPO}$ and $\text{RLAE}_\text{MAPPO}$ ), demonstrating substantial improvements over conventional ensemble methods. Extensive evaluations on a diverse set of tasks show that RLAE outperforms existing approaches by up to $3.3\%$ accuracy points, offering a more effective framework for LLM ensembling. Furthermore, our method exhibits superior generalization capabilities across different tasks without the need for retraining, while simultaneously achieving lower time latency.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention-Aided MMSE for OFDM Channel Estimation: Learning Linear Filters with Attention</title>
<link>https://arxiv.org/abs/2506.00452</link>
<guid>https://arxiv.org/abs/2506.00452</guid>
<content:encoded><![CDATA[
arXiv:2506.00452v1 Announce Type: cross 
Abstract: In orthogonal frequency division multiplexing (OFDM), accurate channel estimation is crucial. Classical signal processing based approaches, such as minimum mean-squared error (MMSE) estimation, often require second-order statistics that are difficult to obtain in practice. Recent deep neural networks based methods have been introduced to address this; yet they often suffer from high complexity. This paper proposes an Attention-aided MMSE (A-MMSE), a novel model-based DNN framework that learns the optimal MMSE filter via the Attention Transformer. Once trained, the A-MMSE estimates the channel through a single linear operation for channel estimation, eliminating nonlinear activations during inference and thus reducing computational complexity. To enhance the learning efficiency of the A-MMSE, we develop a two-stage Attention encoder, designed to effectively capture the channel correlation structure. Additionally, a rank-adaptive extension of the proposed A-MMSE allows flexible trade-offs between complexity and channel estimation accuracy. Extensive simulations with 3GPP TDL channel models demonstrate that the proposed A-MMSE consistently outperforms other baseline methods in terms of normalized MSE across a wide range of SNR conditions. In particular, the A-MMSE and its rank-adaptive extension establish a new frontier in the performance complexity trade-off, redefining the standard for practical channel estimation methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TMetaNet: Topological Meta-Learning Framework for Dynamic Link Prediction</title>
<link>https://arxiv.org/abs/2506.00453</link>
<guid>https://arxiv.org/abs/2506.00453</guid>
<content:encoded><![CDATA[
arXiv:2506.00453v1 Announce Type: cross 
Abstract: Dynamic graphs evolve continuously, presenting challenges for traditional graph learning due to their changing structures and temporal dependencies. Recent advancements have shown potential in addressing these challenges by developing suitable meta-learning-based dynamic graph neural network models. However, most meta-learning approaches for dynamic graphs rely on fixed weight update parameters, neglecting the essential intrinsic complex high-order topological information of dynamically evolving graphs. We have designed Dowker Zigzag Persistence (DZP), an efficient and stable dynamic graph persistent homology representation method based on Dowker complex and zigzag persistence, to capture the high-order features of dynamic graphs. Armed with the DZP ideas, we propose TMetaNet, a new meta-learning parameter update model based on dynamic topological features. By utilizing the distances between high-order topological features, TMetaNet enables more effective adaptation across snapshots. Experiments on real-world datasets demonstrate TMetaNet's state-of-the-art performance and resilience to graph noise, illustrating its high potential for meta-learning and dynamic graph analysis. Our code is available at https://github.com/Lihaogx/TMetaNet.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Models for Increasing Accuracy in Olfaction Sensors and Datasets</title>
<link>https://arxiv.org/abs/2506.00455</link>
<guid>https://arxiv.org/abs/2506.00455</guid>
<content:encoded><![CDATA[
arXiv:2506.00455v1 Announce Type: cross 
Abstract: Robotic odour source localization (OSL) is a critical capability for autonomous systems operating in complex environments. However, current OSL methods often suffer from ambiguities, particularly when robots misattribute odours to incorrect objects due to limitations in olfactory datasets and sensor resolutions. To address this challenge, we introduce a novel machine learning method using diffusion-based molecular generation to enhance odour localization accuracy that can be used by itself or with automated olfactory dataset construction pipelines with vision-language models (VLMs) This generative process of our diffusion model expands the chemical space beyond the limitations of both current olfactory datasets and the training data of VLMs, enabling the identification of potential odourant molecules not previously documented. The generated molecules can then be more accurately validated using advanced olfactory sensors which emulate human olfactory recognition through electronic sensor arrays. By integrating visual analysis, language processing, and molecular generation, our framework enhances the ability of olfaction-vision models on robots to accurately associate odours with their correct sources, thereby improving navigation and decision-making in environments where olfactory cues are essential. Our methodology represents a foundational advancement in the field of robotic olfaction, offering a scalable solution to the challenges posed by limited olfactory data and sensor ambiguities.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning for Hanabi</title>
<link>https://arxiv.org/abs/2506.00458</link>
<guid>https://arxiv.org/abs/2506.00458</guid>
<content:encoded><![CDATA[
arXiv:2506.00458v1 Announce Type: cross 
Abstract: Hanabi has become a popular game for research when it comes to reinforcement learning (RL) as it is one of the few cooperative card games where you have incomplete knowledge of the entire environment, thus presenting a challenge for a RL agent. We explored different tabular and deep reinforcement learning algorithms to see which had the best performance both against an agent of the same type and also against other types of agents. We establish that certain agents played their highest scoring games against specific agents while others exhibited higher scores on average by adapting to the opposing agent's behavior. We attempted to quantify the conditions under which each algorithm provides the best advantage and identified the most interesting interactions between agents of different types. In the end, we found that temporal difference (TD) algorithms had better overall performance and balancing of play types compared to tabular agents. Specifically, tabular Expected SARSA and deep Q-Learning agents showed the best performance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Traditional and Reinforcement-Learning Methods for Energy Storage Control</title>
<link>https://arxiv.org/abs/2506.00459</link>
<guid>https://arxiv.org/abs/2506.00459</guid>
<content:encoded><![CDATA[
arXiv:2506.00459v1 Announce Type: cross 
Abstract: We aim to better understand the tradeoffs between traditional and reinforcement learning (RL) approaches for energy storage management. More specifically, we wish to better understand the performance loss incurred when using a generative RL policy instead of using a traditional approach to find optimal control policies for specific instances. Our comparison is based on a simplified micro-grid model, that includes a load component, a photovoltaic source, and a storage device. Based on this model, we examine three use cases of increasing complexity: ideal storage with convex cost functions, lossy storage devices, and lossy storage devices with convex transmission losses. With the aim of promoting the principled use RL based methods in this challenging and important domain, we provide a detailed formulation of each use case and a detailed description of the optimization challenges. We then compare the performance of traditional and RL methods, discuss settings in which it is beneficial to use each method, and suggest avenues for future investigation.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XMAD-Bench: Cross-Domain Multilingual Audio Deepfake Benchmark</title>
<link>https://arxiv.org/abs/2506.00462</link>
<guid>https://arxiv.org/abs/2506.00462</guid>
<content:encoded><![CDATA[
arXiv:2506.00462v1 Announce Type: cross 
Abstract: Recent advances in audio generation led to an increasing number of deepfakes, making the general public more vulnerable to financial scams, identity theft, and misinformation. Audio deepfake detectors promise to alleviate this issue, with many recent studies reporting accuracy rates close to 99%. However, these methods are typically tested in an in-domain setup, where the deepfake samples from the training and test sets are produced by the same generative models. To this end, we introduce XMAD-Bench, a large-scale cross-domain multilingual audio deepfake benchmark comprising 668.8 hours of real and deepfake speech. In our novel dataset, the speakers, the generative methods, and the real audio sources are distinct across training and test splits. This leads to a challenging cross-domain evaluation setup, where audio deepfake detectors can be tested ``in the wild''. Our in-domain and cross-domain experiments indicate a clear disparity between the in-domain performance of deepfake detectors, which is usually as high as 100%, and the cross-domain performance of the same models, which is sometimes similar to random chance. Our benchmark highlights the need for the development of robust audio deepfake detectors, which maintain their generalization capacity across different languages, speakers, generative methods, and data sources. Our benchmark is publicly released at https://github.com/ristea/xmad-bench/.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SST: Self-training with Self-adaptive Thresholding for Semi-supervised Learning</title>
<link>https://arxiv.org/abs/2506.00467</link>
<guid>https://arxiv.org/abs/2506.00467</guid>
<content:encoded><![CDATA[
arXiv:2506.00467v1 Announce Type: cross 
Abstract: Neural networks have demonstrated exceptional performance in supervised learning, benefiting from abundant high-quality annotated data. However, obtaining such data in real-world scenarios is costly and labor-intensive. Semi-supervised learning (SSL) offers a solution to this problem. Recent studies, such as Semi-ViT and Noisy Student, which employ consistency regularization or pseudo-labeling, have demonstrated significant achievements. However, they still face challenges, particularly in accurately selecting sufficient high-quality pseudo-labels due to their reliance on fixed thresholds. Recent methods such as FlexMatch and FreeMatch have introduced flexible or self-adaptive thresholding techniques, greatly advancing SSL research. Nonetheless, their process of updating thresholds at each iteration is deemed time-consuming, computationally intensive, and potentially unnecessary. To address these issues, we propose Self-training with Self-adaptive Thresholding (SST), a novel, effective, and efficient SSL framework. SST introduces an innovative Self-Adaptive Thresholding (SAT) mechanism that adaptively adjusts class-specific thresholds based on the model's learning progress. SAT ensures the selection of high-quality pseudo-labeled data, mitigating the risks of inaccurate pseudo-labels and confirmation bias. Extensive experiments demonstrate that SST achieves state-of-the-art performance with remarkable efficiency, generalization, and scalability across various architectures and datasets. Semi-SST-ViT-Huge achieves the best results on competitive ImageNet-1K SSL benchmarks, with 80.7% / 84.9% Top-1 accuracy using only 1% / 10% labeled data. Compared to the fully-supervised DeiT-III-ViT-Huge, which achieves 84.8% Top-1 accuracy using 100% labeled data, our method demonstrates superior performance using only 10% labeled data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PVP: An Image Dataset for Personalized Visual Persuasion with Persuasion Strategies, Viewer Characteristics, and Persuasiveness Ratings</title>
<link>https://arxiv.org/abs/2506.00481</link>
<guid>https://arxiv.org/abs/2506.00481</guid>
<content:encoded><![CDATA[
arXiv:2506.00481v1 Announce Type: cross 
Abstract: Visual persuasion, which uses visual elements to influence cognition and behaviors, is crucial in fields such as advertising and political communication. With recent advancements in artificial intelligence, there is growing potential to develop persuasive systems that automatically generate persuasive images tailored to individuals. However, a significant bottleneck in this area is the lack of comprehensive datasets that connect the persuasiveness of images with the personal information about those who evaluated the images. To address this gap and facilitate technological advancements in personalized visual persuasion, we release the Personalized Visual Persuasion (PVP) dataset, comprising 28,454 persuasive images across 596 messages and 9 persuasion strategies. Importantly, the PVP dataset provides persuasiveness scores of images evaluated by 2,521 human annotators, along with their demographic and psychological characteristics (personality traits and values). We demonstrate the utility of our dataset by developing a persuasive image generator and an automated evaluator, and establish benchmark baselines. Our experiments reveal that incorporating psychological characteristics enhances the generation and evaluation of persuasive images, providing valuable insights for personalized visual persuasion.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM Evaluation</title>
<link>https://arxiv.org/abs/2506.00482</link>
<guid>https://arxiv.org/abs/2506.00482</guid>
<content:encoded><![CDATA[
arXiv:2506.00482v1 Announce Type: cross 
Abstract: As large language models (LLMs) continue to advance, the need for up-to-date and well-organized benchmarks becomes increasingly critical. However, many existing datasets are scattered, difficult to manage, and make it challenging to perform evaluations tailored to specific needs or domains, despite the growing importance of domain-specific models in areas such as math or code. In this paper, we introduce BenchHub, a dynamic benchmark repository that empowers researchers and developers to evaluate LLMs more effectively. BenchHub aggregates and automatically classifies benchmark datasets from diverse domains, integrating 303K questions across 38 benchmarks. It is designed to support continuous updates and scalable data management, enabling flexible and customizable evaluation tailored to various domains or use cases. Through extensive experiments with various LLM families, we demonstrate that model performance varies significantly across domain-specific subsets, emphasizing the importance of domain-aware benchmarking. We believe BenchHub can encourage better dataset reuse, more transparent model comparisons, and easier identification of underrepresented areas in existing benchmarks, offering a critical infrastructure for advancing LLM evaluation research.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>It Takes a Good Model to Train a Good Model: Generalized Gaussian Priors for Optimized LLMs</title>
<link>https://arxiv.org/abs/2506.00486</link>
<guid>https://arxiv.org/abs/2506.00486</guid>
<content:encoded><![CDATA[
arXiv:2506.00486v1 Announce Type: cross 
Abstract: Despite rapid advancements in the research and deployment of large language models (LLMs), the statistical distribution of model parameters, as well as their influence on initialization, training dynamics, and downstream efficiency, has received surprisingly little attention. A recent work introduced BackSlash, a training-time compression algorithm. It first demonstrated that pre-trained LLM parameters follow generalized Gaussian distributions (GGDs) better. By optimizing GG priors during training, BackSlash can reduce parameters by up to 90\% with minimal performance loss. Building on this foundational insight, we propose a unified, end-to-end framework for LLM optimization based on the GG model. Our contributions are threefold: (1) GG-based initialization scheme that aligns with the statistical structure of trained models, resulting in faster convergence and improved accuracy; (2) DeepShape, a post-training regularization method that reshapes weight distributions to match a GG profile, improving compressibility with minimized degradation in performance; and (3) RF8, a compact and hardware-efficient 8-bit floating-point format designed for GG-distributed-initialized BackSlash training, enabling low-cost inference without compromising accuracy. Experiments across diverse model architectures show that our framework consistently yields smaller and faster models that match or outperform standard training baselines. By grounding LLM development in principled statistical modeling, this work forges a new path toward efficient, scalable, and hardware-aware AI systems. The code is available on our project page: https://huggingface.co/spaces/shifeng3711/gg_prior.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Objective Neural Network Assisted Design Optimization of Soft Fin-Ray Grippers for Enhanced Grasping Performance</title>
<link>https://arxiv.org/abs/2506.00494</link>
<guid>https://arxiv.org/abs/2506.00494</guid>
<content:encoded><![CDATA[
arXiv:2506.00494v1 Announce Type: cross 
Abstract: Soft Fin-Ray grippers can perform delicate and careful manipulation, which has caused notable attention in different fields. These grippers can handle objects of various forms and sizes safely. The internal structure of the Fin-Ray finger plays a significant role in its adaptability and grasping performance. However, modeling the non-linear grasp force and deformation behaviors for design purposes is challenging. Moreover, when the Fin-Ray finger becomes more rigid and capable of exerting higher forces, it becomes less delicate in handling objects. The contrast between these two objectives gives rise to a multi-objective optimization problem. In this study, we employ finite element method (FEM) to estimate the deflections and contact forces of the Fin-Ray, grasping cylindrical objects. This dataset is then used to construct a multilayer perception (MLP) for prediction of the contact force and the tip displacement. The FEM dataset consists of three input and four target features. The three input features of the MLP and optimization design variables are the thickness of the front and supporting beams, the thickness of the cross beams, and the equal spacing between the cross beams. In addition, the target features are the maximum contact forces and maximum tip displacements in x- and y-directions. The magnitude of maximum contact force and magnitude of maximum tip displacement are the two objectives, showing the trade-off between force and delicate manipulation in soft Fin-Ray grippers. Furthermore, the optimized set of solutions are found using multi-objective optimal techniques. We use non-dominated sorting genetic algorithm (NSGA-II) method for this purpose. Our findings demonstrate that our methodologies can be used to improve the design and gripping performance of soft robotic grippers, helping us to choose a design not only for delicate grasping but also for high-force applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pro3D-Editor : A Progressive-Views Perspective for Consistent and Precise 3D Editing</title>
<link>https://arxiv.org/abs/2506.00512</link>
<guid>https://arxiv.org/abs/2506.00512</guid>
<content:encoded><![CDATA[
arXiv:2506.00512v1 Announce Type: cross 
Abstract: Text-guided 3D editing aims to precisely edit semantically relevant local 3D regions, which has significant potential for various practical applications ranging from 3D games to film production. Existing methods typically follow a view-indiscriminate paradigm: editing 2D views indiscriminately and projecting them back into 3D space. However, they overlook the different cross-view interdependencies, resulting in inconsistent multi-view editing. In this study, we argue that ideal consistent 3D editing can be achieved through a \textit{progressive-views paradigm}, which propagates editing semantics from the editing-salient view to other editing-sparse views. Specifically, we propose \textit{Pro3D-Editor}, a novel framework, which mainly includes Primary-view Sampler, Key-view Render, and Full-view Refiner. Primary-view Sampler dynamically samples and edits the most editing-salient view as the primary view. Key-view Render accurately propagates editing semantics from the primary view to other key views through its Mixture-of-View-Experts Low-Rank Adaption (MoVE-LoRA). Full-view Refiner edits and refines the 3D object based on the edited multi-views. Extensive experiments demonstrate that our method outperforms existing methods in editing accuracy and spatial consistency.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CausalAbstain: Enhancing Multilingual LLMs with Causal Reasoning for Trustworthy Abstention</title>
<link>https://arxiv.org/abs/2506.00519</link>
<guid>https://arxiv.org/abs/2506.00519</guid>
<content:encoded><![CDATA[
arXiv:2506.00519v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) often exhibit knowledge disparities across languages. Encouraging LLMs to \textit{abstain} when faced with knowledge gaps is a promising strategy to reduce hallucinations in multilingual settings. Current abstention strategies for multilingual scenarios primarily rely on generating feedback in various languages using LLMs and performing self-reflection. However, these methods can be adversely impacted by inaccuracies and biases in the generated feedback. To address this, from a causal perspective, we introduce \textit{CausalAbstain}, a method that helps LLMs determine whether to utilize multiple generated feedback responses and how to identify the most useful ones. Extensive experiments demonstrate that \textit{CausalAbstain} effectively selects helpful feedback and enhances abstention decisions with interpretability in both native language (\textsc{Casual-native}) and multilingual (\textsc{Causal-multi}) settings, outperforming strong baselines on two benchmark datasets covering encyclopedic and commonsense knowledge QA tasks. Our code and data are open-sourced at https://github.com/peachch/CausalAbstain.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-Augmented Generation Systems for Intellectual Property via Synthetic Multi-Angle Fine-tuning</title>
<link>https://arxiv.org/abs/2506.00527</link>
<guid>https://arxiv.org/abs/2506.00527</guid>
<content:encoded><![CDATA[
arXiv:2506.00527v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) systems in the Intellectual Property (IP) field often struggle with diverse user queries, including colloquial expressions, spelling errors, and ambiguous terminology, leading to inaccurate retrieval and suboptimal responses. To address this challenge, we propose Multi-Angle Question Generation and Retrieval Fine-Tuning Method (MQG-RFM), a novel framework that leverages large language models (LLMs) to simulate varied user inquiries and fine-tunes retrieval models to align semantically equivalent but linguistically diverse questions. Unlike complex architectural modifications, MQG-RFM adopts a lightweight Data-to-Tune paradigm, combining prompt-engineered query generation with hard negative mining to enhance retrieval robustness without costly infrastructure changes. Experimental results on a Taiwan patent Q&amp;A dataset show 185.62% improvement in retrieval accuracy on the Patent Consultation dataset and 262.26% improvement on the Novel Patent Technology Report dataset, with 14.22% and 53.58% improvements in generation quality over the baselines, respectively. By bridging the gap between user intent and system comprehension through semantic-aware retrieval optimization, MQG-RFM offers a practical, scalable approach for rapid, cost-effective deployment among small and medium-sized agencies seeking reliable patent intelligence solutions. Additionally, our proposed method has already been adopted by ScholarMate, the largest professional research social networking platform in China, to support real-world development and deployment. A demo version of the instantiated is available at https://github.com/renruntao/patent_rag.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M2WLLM: Multi-Modal Multi-Task Ultra-Short-term Wind Power Prediction Algorithm Based on Large Language Model</title>
<link>https://arxiv.org/abs/2506.00531</link>
<guid>https://arxiv.org/abs/2506.00531</guid>
<content:encoded><![CDATA[
arXiv:2506.00531v1 Announce Type: cross 
Abstract: The integration of wind energy into power grids necessitates accurate ultra-short-term wind power forecasting to ensure grid stability and optimize resource allocation. This study introduces M2WLLM, an innovative model that leverages the capabilities of Large Language Models (LLMs) for predicting wind power output at granular time intervals. M2WLLM overcomes the limitations of traditional and deep learning methods by seamlessly integrating textual information and temporal numerical data, significantly improving wind power forecasting accuracy through multi-modal data. Its architecture features a Prompt Embedder and a Data Embedder, enabling an effective fusion of textual prompts and numerical inputs within the LLMs framework. The Semantic Augmenter within the Data Embedder translates temporal data into a format that the LLMs can comprehend, enabling it to extract latent features and improve prediction accuracy. The empirical evaluations conducted on wind farm data from three Chinese provinces demonstrate that M2WLLM consistently outperforms existing methods, such as GPT4TS, across various datasets and prediction horizons. The results highlight LLMs' ability to enhance accuracy and robustness in ultra-short-term forecasting and showcase their strong few-shot learning capabilities.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Security Threat of Compressed Projectors in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2506.00534</link>
<guid>https://arxiv.org/abs/2506.00534</guid>
<content:encoded><![CDATA[
arXiv:2506.00534v1 Announce Type: cross 
Abstract: The choice of a suitable visual language projector (VLP) is critical to the successful training of large visual language models (LVLMs). Mainstream VLPs can be broadly categorized into compressed and uncompressed projectors, and each offering distinct advantages in performance and computational efficiency. However, their security implications have not been thoroughly examined. Our comprehensive evaluation reveals significant differences in their security profiles: compressed projectors exhibit substantial vulnerabilities, allowing adversaries to successfully compromise LVLMs even with minimal knowledge of structural information. In stark contrast, uncompressed projectors demonstrate robust security properties and do not introduce additional vulnerabilities. These findings provide critical guidance for researchers in selecting optimal VLPs that enhance the security and reliability of visual language models. The code will be released.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoupling Reasoning and Knowledge Injection for In-Context Knowledge Editing</title>
<link>https://arxiv.org/abs/2506.00536</link>
<guid>https://arxiv.org/abs/2506.00536</guid>
<content:encoded><![CDATA[
arXiv:2506.00536v1 Announce Type: cross 
Abstract: Knowledge editing aims to efficiently update Large Language Models (LLMs) by modifying specific knowledge without retraining the entire model. Among knowledge editing approaches, in-context editing (ICE) offers a lightweight solution by injecting new knowledge directly into the input context, leaving model parameters unchanged. However, existing ICE approaches do not explicitly separate the newly injected knowledge from the model's original reasoning process. This entanglement often results in conflicts between external updates and internal parametric knowledge, undermining the consistency and accuracy of the reasoning path.In this work, we conduct preliminary experiments to examine how parametric knowledge influences reasoning path planning. We find that the model's reasoning is tightly coupled with its internal knowledge, and that naively injecting new information without adapting the reasoning path often leads to performance degradation, particularly in multi-hop tasks. To this end, we propose DecKER, a novel ICE framework that decouples reasoning from knowledge editing by generating a masked reasoning path and then resolving knowledge edits via hybrid retrieval and model-based validation. Experiments on multi-hop QA benchmarks show that DecKER significantly outperforms existing ICE methods by mitigating knowledge conflicts and preserving reasoning consistency. Our code is available at: https://github.com/bebr2/DecKER .
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imputation of Missing Data in Smooth Pursuit Eye Movements Using a Self-Attention-based Deep Learning Approach</title>
<link>https://arxiv.org/abs/2506.00545</link>
<guid>https://arxiv.org/abs/2506.00545</guid>
<content:encoded><![CDATA[
arXiv:2506.00545v1 Announce Type: cross 
Abstract: Missing data is a relevant issue in time series, especially in biomedical sequences such as those corresponding to smooth pursuit eye movements, which often contain gaps due to eye blinks and track losses, complicating the analysis and extraction of meaningful biomarkers. In this paper, a novel imputation framework is proposed using Self-Attention-based Imputation networks for time series, which leverages the power of deep learning and self-attention mechanisms to impute missing data. We further refine the imputed data using a custom made autoencoder, tailored to represent smooth pursuit eye movement sequences. The proposed approach was implemented using 5,504 sequences from 172 Parkinsonian patients and healthy controls. Results show a significant improvement in the accuracy of reconstructed eye movement sequences with respect to other state of the art techniques, substantially reducing the values for common time domain error metrics such as the mean absolute error, mean relative error, and root mean square error, while also preserving the signal's frequency domain characteristics. Moreover, it demonstrates robustness when large intervals of data are missing. This method offers an alternative solution for robustly handling missing data in time series, enhancing the reliability of smooth pursuit analysis for the screening and monitoring of neurodegenerative disorders.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Multi-dimensional Evaluation of LLM Summarization across Domains and Languages</title>
<link>https://arxiv.org/abs/2506.00549</link>
<guid>https://arxiv.org/abs/2506.00549</guid>
<content:encoded><![CDATA[
arXiv:2506.00549v1 Announce Type: cross 
Abstract: Evaluation frameworks for text summarization have evolved in terms of both domain coverage and metrics. However, existing benchmarks still lack domain-specific assessment criteria, remain predominantly English-centric, and face challenges with human annotation due to the complexity of reasoning. To address these, we introduce MSumBench, which provides a multi-dimensional, multi-domain evaluation of summarization in English and Chinese. It also incorporates specialized assessment criteria for each domain and leverages a multi-agent debate system to enhance annotation quality. By evaluating eight modern summarization models, we discover distinct performance patterns across domains and languages. We further examine large language models as summary evaluators, analyzing the correlation between their evaluation and summarization capabilities, and uncovering systematic bias in their assessment of self-generated summaries. Our benchmark dataset is publicly available at https://github.com/DISL-Lab/MSumBench.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AnnaAgent: Dynamic Evolution Agent System with Multi-Session Memory for Realistic Seeker Simulation</title>
<link>https://arxiv.org/abs/2506.00551</link>
<guid>https://arxiv.org/abs/2506.00551</guid>
<content:encoded><![CDATA[
arXiv:2506.00551v1 Announce Type: cross 
Abstract: Constrained by the cost and ethical concerns of involving real seekers in AI-driven mental health, researchers develop LLM-based conversational agents (CAs) with tailored configurations, such as profiles, symptoms, and scenarios, to simulate seekers. While these efforts advance AI in mental health, achieving more realistic seeker simulation remains hindered by two key challenges: dynamic evolution and multi-session memory. Seekers' mental states often fluctuate during counseling, which typically spans multiple sessions. To address this, we propose AnnaAgent, an emotional and cognitive dynamic agent system equipped with tertiary memory. AnnaAgent incorporates an emotion modulator and a complaint elicitor trained on real counseling dialogues, enabling dynamic control of the simulator's configurations. Additionally, its tertiary memory mechanism effectively integrates short-term and long-term memory across sessions. Evaluation results, both automated and manual, demonstrate that AnnaAgent achieves more realistic seeker simulation in psychological counseling compared to existing baselines. The ethically reviewed and screened code can be found on https://github.com/sci-m-wang/AnnaAgent.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning</title>
<link>https://arxiv.org/abs/2506.00555</link>
<guid>https://arxiv.org/abs/2506.00555</guid>
<content:encoded><![CDATA[
arXiv:2506.00555v1 Announce Type: cross 
Abstract: Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential in multimodal diagnostic tasks. However, existing single-agent models struggle to generalize across diverse medical specialties, limiting their performance. Recent efforts introduce multi-agent collaboration frameworks inspired by clinical workflows, where general practitioners (GPs) and specialists interact in a fixed sequence. Despite improvements, these static pipelines lack flexibility and adaptability in reasoning. To address this, we propose MMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that enables dynamic, optimized collaboration among medical agents. Specifically, we train two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to assign patients to appropriate specialties, while the attending physician integrates the judgments from multi-specialists and its own knowledge to make final decisions. To address the inconsistency in specialist outputs, we introduce a curriculum learning (CL)-guided RL strategy that progressively teaches the attending physician to balance between imitating specialists and correcting their mistakes. Experiments on five medical VQA benchmarks demonstrate that MMedAgent-RL not only outperforms both open-source and proprietary Med-LVLMs, but also exhibits human-like reasoning patterns. Notably, it achieves an average performance gain of 18.4% over supervised fine-tuning baselines.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Behavioral Metric Learning: A Large-Scale Study on Distracting Reinforcement Learning Environments</title>
<link>https://arxiv.org/abs/2506.00563</link>
<guid>https://arxiv.org/abs/2506.00563</guid>
<content:encoded><![CDATA[
arXiv:2506.00563v1 Announce Type: cross 
Abstract: A key approach to state abstraction is approximating behavioral metrics (notably, bisimulation metrics) in the observation space and embedding these learned distances in the representation space. While promising for robustness to task-irrelevant noise, as shown in prior work, accurately estimating these metrics remains challenging, requiring various design choices that create gaps between theory and practice. Prior evaluations focus mainly on final returns, leaving the quality of learned metrics and the source of performance gains unclear. To systematically assess how metric learning works in deep reinforcement learning (RL), we evaluate five recent approaches, unified conceptually as isometric embeddings with varying design choices. We benchmark them with baselines across 20 state-based and 14 pixel-based tasks, spanning 370 task configurations with diverse noise settings. Beyond final returns, we introduce the evaluation of a denoising factor to quantify the encoder's ability to filter distractions. To further isolate the effect of metric learning, we propose and evaluate an isolated metric estimation setting, in which the encoder is influenced solely by the metric loss. Finally, we release an open-source, modular codebase to improve reproducibility and support future research on metric learning in deep RL.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt-Tuned LLM-Augmented DRL for Dynamic O-RAN Network Slicing</title>
<link>https://arxiv.org/abs/2506.00574</link>
<guid>https://arxiv.org/abs/2506.00574</guid>
<content:encoded><![CDATA[
arXiv:2506.00574v1 Announce Type: cross 
Abstract: Modern wireless networks must adapt to dynamic conditions while efficiently managing diverse service demands. Traditional deep reinforcement learning (DRL) struggles in these environments, as scattered and evolving feedback makes optimal decision-making challenging. Large Language Models (LLMs) offer a solution by structuring unorganized network feedback into meaningful latent representations, helping RL agents recognize patterns more effectively. For example, in O-RAN slicing, concepts like SNR, power levels and throughput are semantically related, and LLMs can naturally cluster them, providing a more interpretable state representation. To leverage this capability, we introduce a contextualization-based adaptation method that integrates learnable prompts into an LLM-augmented DRL framework. Instead of relying on full model fine-tuning, we refine state representations through task-specific prompts that dynamically adjust to network conditions. Utilizing ORANSight, an LLM trained on O-RAN knowledge, we develop Prompt-Augmented Multi agent RL (PA-MRL) framework. Learnable prompts optimize both semantic clustering and RL objectives, allowing RL agents to achieve higher rewards in fewer iterations and adapt more efficiently. By incorporating prompt-augmented learning, our approach enables faster, more scalable, and adaptive resource allocation in O-RAN slicing. Experimental results show that it accelerates convergence and outperforms other baselines.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ORAN-GUIDE: RAG-Driven Prompt Learning for LLM-Augmented Reinforcement Learning in O-RAN Network Slicing</title>
<link>https://arxiv.org/abs/2506.00576</link>
<guid>https://arxiv.org/abs/2506.00576</guid>
<content:encoded><![CDATA[
arXiv:2506.00576v1 Announce Type: cross 
Abstract: Advanced wireless networks must support highly dynamic and heterogeneous service demands. Open Radio Access Network (O-RAN) architecture enables this flexibility by adopting modular, disaggregated components, such as the RAN Intelligent Controller (RIC), Centralized Unit (CU), and Distributed Unit (DU), that can support intelligent control via machine learning (ML). While deep reinforcement learning (DRL) is a powerful tool for managing dynamic resource allocation and slicing, it often struggles to process raw, unstructured input like RF features, QoS metrics, and traffic trends. These limitations hinder policy generalization and decision efficiency in partially observable and evolving environments. To address this, we propose \textit{ORAN-GUIDE}, a dual-LLM framework that enhances multi-agent RL (MARL) with task-relevant, semantically enriched state representations. The architecture employs a domain-specific language model, ORANSight, pretrained on O-RAN control and configuration data, to generate structured, context-aware prompts. These prompts are fused with learnable tokens and passed to a frozen GPT-based encoder that outputs high-level semantic representations for DRL agents. This design adopts a retrieval-augmented generation (RAG) style pipeline tailored for technical decision-making in wireless systems. Experimental results show that ORAN-GUIDE improves sample efficiency, policy convergence, and performance generalization over standard MARL and single-LLM baselines.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Chunking Enhances Recognition of Implicit Sequential Patterns</title>
<link>https://arxiv.org/abs/2506.00588</link>
<guid>https://arxiv.org/abs/2506.00588</guid>
<content:encoded><![CDATA[
arXiv:2506.00588v1 Announce Type: cross 
Abstract: In this pilot study, we propose a neuro-inspired approach that compresses temporal sequences into context-tagged chunks, where each tag represents a recurring structural unit or``community'' in the sequence. These tags are generated during an offline sleep phase and serve as compact references to past experience, allowing the learner to incorporate information beyond its immediate input range. We evaluate this idea in a controlled synthetic environment designed to reveal the limitations of traditional neural network based sequence learners, such as recurrent neural networks (RNNs), when facing temporal patterns on multiple timescales. We evaluate this idea in a controlled synthetic environment designed to reveal the limitations of traditional neural network based sequence learners, such as recurrent neural networks (RNNs), when facing temporal patterns on multiple timescales. Our results, while preliminary, suggest that temporal chunking can significantly enhance learning efficiency under resource constrained settings. A small-scale human pilot study using a Serial Reaction Time Task further motivates the idea of structural abstraction. Although limited to synthetic tasks, this work serves as an early proof-of-concept, with initial evidence that learned context tags can transfer across related task, offering potential for future applications in transfer learning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Plasticity Loss in Continual Reinforcement Learning by Reducing Churn</title>
<link>https://arxiv.org/abs/2506.00592</link>
<guid>https://arxiv.org/abs/2506.00592</guid>
<content:encoded><![CDATA[
arXiv:2506.00592v1 Announce Type: cross 
Abstract: Plasticity, or the ability of an agent to adapt to new tasks, environments, or distributions, is crucial for continual learning. In this paper, we study the loss of plasticity in deep continual RL from the lens of churn: network output variability for out-of-batch data induced by mini-batch training. We demonstrate that (1) the loss of plasticity is accompanied by the exacerbation of churn due to the gradual rank decrease of the Neural Tangent Kernel (NTK) matrix; (2) reducing churn helps prevent rank collapse and adjusts the step size of regular RL gradients adaptively. Moreover, we introduce Continual Churn Approximated Reduction (C-CHAIN) and demonstrate it improves learning performance and outperforms baselines in a diverse range of continual learning environments on OpenAI Gym Control, ProcGen, DeepMind Control Suite, and MinAtar benchmarks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Evidential Learning for Anomaly Detection</title>
<link>https://arxiv.org/abs/2506.00594</link>
<guid>https://arxiv.org/abs/2506.00594</guid>
<content:encoded><![CDATA[
arXiv:2506.00594v1 Announce Type: cross 
Abstract: Graph anomaly detection faces significant challenges due to the scarcity of reliable anomaly-labeled datasets, driving the development of unsupervised methods. Graph autoencoders (GAEs) have emerged as a dominant approach by reconstructing graph structures and node features while deriving anomaly scores from reconstruction errors. However, relying solely on reconstruction error for anomaly detection has limitations, as it increases the sensitivity to noise and overfitting. To address these issues, we propose Graph Evidential Learning (GEL), a probabilistic framework that redefines the reconstruction process through evidential learning. By modeling node features and graph topology using evidential distributions, GEL quantifies two types of uncertainty: graph uncertainty and reconstruction uncertainty, incorporating them into the anomaly scoring mechanism. Extensive experiments demonstrate that GEL achieves state-of-the-art performance while maintaining high robustness against noise and structural perturbations.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parallel Rescaling: Rebalancing Consistency Guidance for Personalized Diffusion Models</title>
<link>https://arxiv.org/abs/2506.00607</link>
<guid>https://arxiv.org/abs/2506.00607</guid>
<content:encoded><![CDATA[
arXiv:2506.00607v1 Announce Type: cross 
Abstract: Personalizing diffusion models to specific users or concepts remains challenging, particularly when only a few reference images are available. Existing methods such as DreamBooth and Textual Inversion often overfit to limited data, causing misalignment between generated images and text prompts when attempting to balance identity fidelity with prompt adherence. While Direct Consistency Optimization (DCO) with its consistency-guided sampling partially alleviates this issue, it still struggles with complex or stylized prompts. In this paper, we propose a parallel rescaling technique for personalized diffusion models. Our approach explicitly decomposes the consistency guidance signal into parallel and orthogonal components relative to classifier free guidance (CFG). By rescaling the parallel component, we minimize disruptive interference with CFG while preserving the subject's identity. Unlike prior personalization methods, our technique does not require additional training data or expensive annotations. Extensive experiments show improved prompt alignment and visual fidelity compared to baseline methods, even on challenging stylized prompts. These findings highlight the potential of parallel rescaled guidance to yield more stable and accurate personalization for diverse user inputs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Robot Policies in a World Model</title>
<link>https://arxiv.org/abs/2506.00613</link>
<guid>https://arxiv.org/abs/2506.00613</guid>
<content:encoded><![CDATA[
arXiv:2506.00613v1 Announce Type: cross 
Abstract: Robotics has broad applications from automating house chores to taking care of patients. However, evaluating robot control policies is challenging, as real-world testing is expensive, while handcrafted simulations often fail to accurately reflect real-world conditions, resulting in poor correlation between simulated evaluation and real-world outcomes. In this work, we investigate World-model-based Policy Evaluation (WPE). We first train an action-conditioned video generation model as a proxy to real-world environments. To enable efficient rollouts of hundreds of interactive steps while mitigating error accumulation in the world model, we propose an inference scheme which we call Blockwise-Autoregressive Diffusion Transformer with adjustable context and decoding horizon lengths. To ensure that the world model indeed follows action input, we propose metrics based on the agreement between the ground truth video and generated video conditioned on the same sequence of actions to evaluate the world model. We then use the world model for policy evaluation by performing Monte Carlo rollouts in the world model while employing a vision-language model (VLM) as a reward function. Interestingly, we found that WPE tends to underestimate the policy values for in-distribution actions and overestimate policy values for out-of-distribution actions. Nevertheless, WPE preserves the relative rankings of different policies. In emulating real robot executions, WPE achieves high fidelity in mimicing robot arm movements as in real videos, while emulating highly realistic object interaction remains challenging. Despite this limitation, we show that a world model can serve as a starting point for evaluating robot policies before real-world deployment.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictability-Aware Compression and Decompression Framework for Multichannel Time Series Data</title>
<link>https://arxiv.org/abs/2506.00614</link>
<guid>https://arxiv.org/abs/2506.00614</guid>
<content:encoded><![CDATA[
arXiv:2506.00614v1 Announce Type: cross 
Abstract: Real-world multichannel time series prediction faces growing demands for efficiency across edge and cloud environments, making channel compression a timely and essential problem. Motivated by success of Multiple-Input Multiple-Output (MIMO) methods, we propose a predictability-aware compression-decompression framework to reduce runtime, lower communication cost, and maintain prediction accuracy across diverse predictors. The core idea involves using a circular periodicity key matrix with orthogonality to capture underlying time series predictability during compression and to mitigate reconstruction errors during decompression by relaxing oversimplified data assumptions. Theoretical and empirical analyses show that the proposed framework is both time-efficient and scalable under a large number of channels. Extensive experiments on six datasets across various predictors demonstrate that the proposed method achieves superior overall performance by jointly considering prediction accuracy and runtime, while maintaining strong compatibility with diverse predictors.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Topological Semantics of Dialogue: Nerve Structures and Logical Extraction</title>
<link>https://arxiv.org/abs/2506.00615</link>
<guid>https://arxiv.org/abs/2506.00615</guid>
<content:encoded><![CDATA[
arXiv:2506.00615v1 Announce Type: cross 
Abstract: We introduce a concise, topologically-motivated semantics for finite dialogues by mapping each utterance to an open set in a fixed semantic space, building the corresponding nerve complex of joint satisfiability, and extracting fundamental combinatorial invariants:
  1. The negative nerve, which enumerates all finite collections of utterances whose
  opens have empty intersection, providing a straightforward criterion for merging
  separate transcripts without contradiction.
  2. The global interpretation subspace, the unique minimal open in which all asserted
  utterances hold simultaneously, enabling effective enumeration of all logical
  consequences of the entire dialogue.
  3. A practical demonstration in the Wolfram Language, with algorithms for constructing
  nerves, detecting inconsistencies, and computing the global interpretation, thereby
  illustrating computational feasibility.
  Our framework is grounded in classical duality and topological semantics (Stone duality, Priestley duality, Tarski's semantics, coherence-space methods, Scott domains, topos semantics, and homotopy type theory) while drawing on recent advances in topological data analysis and dialogue-based semantics.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Dialogue State Tracking through Combinatorial Search for In-Context Examples</title>
<link>https://arxiv.org/abs/2506.00622</link>
<guid>https://arxiv.org/abs/2506.00622</guid>
<content:encoded><![CDATA[
arXiv:2506.00622v1 Announce Type: cross 
Abstract: In dialogue state tracking (DST), in-context learning comprises a retriever that selects labeled dialogues as in-context examples and a DST model that uses these examples to infer the dialogue state of the query dialogue. Existing methods for constructing training data for retrievers suffer from three key limitations: (1) the synergistic effect of examples is not considered, (2) the linguistic characteristics of the query are not sufficiently factored in, and (3) scoring is not directly optimized for DST performance. Consequently, the retriever can fail to retrieve examples that would substantially improve DST performance. To address these issues, we present CombiSearch, a method that scores effective in-context examples based on their combinatorial impact on DST performance. Our evaluation on MultiWOZ shows that retrievers trained with CombiSearch surpass state-of-the-art models, achieving a 20x gain in data efficiency and generalizing well to the SGD dataset. Moreover, CombiSearch attains a 12% absolute improvement in the upper bound DST performance over traditional approaches when no retrieval errors are assumed. This significantly increases the headroom for practical DST performance while demonstrating that existing methods rely on suboptimal data for retriever training.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Disparate Effects of Partial Information in Bayesian Strategic Learning</title>
<link>https://arxiv.org/abs/2506.00627</link>
<guid>https://arxiv.org/abs/2506.00627</guid>
<content:encoded><![CDATA[
arXiv:2506.00627v1 Announce Type: cross 
Abstract: We study how partial information about scoring rules affects fairness in strategic learning settings. In strategic learning, a learner deploys a scoring rule, and agents respond strategically by modifying their features -- at some cost -- to improve their outcomes. However, in our work, agents do not observe the scoring rule directly; instead, they receive a noisy signal of said rule. We consider two different agent models: (i) naive agents, who take the noisy signal at face value, and (ii) Bayesian agents, who update a prior belief based on the signal.
  Our goal is to understand how disparities in outcomes arise between groups that differ in their costs of feature modification, and how these disparities vary with the level of transparency of the learner's rule. For naive agents, we show that utility disparities can grow unboundedly with noise, and that the group with lower costs can, perhaps counter-intuitively, be disproportionately harmed under limited transparency. In contrast, for Bayesian agents, disparities remain bounded. We provide a full characterization of disparities across groups as a function of the level of transparency and show that they can vary non-monotonically with noise; in particular, disparities are often minimized at intermediate levels of transparency. Finally, we extend our analysis to settings where groups differ not only in cost, but also in prior beliefs, and study how this asymmetry influences fairness.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text-to-CT Generation via 3D Latent Diffusion Model with Contrastive Vision-Language Pretraining</title>
<link>https://arxiv.org/abs/2506.00633</link>
<guid>https://arxiv.org/abs/2506.00633</guid>
<content:encoded><![CDATA[
arXiv:2506.00633v1 Announce Type: cross 
Abstract: Objective: While recent advances in text-conditioned generative models have enabled the synthesis of realistic medical images, progress has been largely confined to 2D modalities such as chest X-rays. Extending text-to-image generation to volumetric Computed Tomography (CT) remains a significant challenge, due to its high dimensionality, anatomical complexity, and the absence of robust frameworks that align vision-language data in 3D medical imaging. Methods: We introduce a novel architecture for Text-to-CT generation that combines a latent diffusion model with a 3D contrastive vision-language pretraining scheme. Our approach leverages a dual-encoder CLIP-style model trained on paired CT volumes and radiology reports to establish a shared embedding space, which serves as the conditioning input for generation. CT volumes are compressed into a low-dimensional latent space via a pretrained volumetric VAE, enabling efficient 3D denoising diffusion without requiring external super-resolution stages. Results: We evaluate our method on the CT-RATE dataset and conduct a comprehensive assessment of image fidelity, clinical relevance, and semantic alignment. Our model achieves competitive performance across all tasks, significantly outperforming prior baselines for text-to-CT generation. Moreover, we demonstrate that CT scans synthesized by our framework can effectively augment real data, improving downstream diagnostic performance. Conclusion: Our results show that modality-specific vision-language alignment is a key component for high-quality 3D medical image generation. By integrating contrastive pretraining and volumetric diffusion, our method offers a scalable and controllable solution for synthesizing clinically meaningful CT volumes from text, paving the way for new applications in data augmentation, medical education, and automated clinical simulation.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning with Calibration: Exploring Test-Time Computing of Spatio-Temporal Forecasting</title>
<link>https://arxiv.org/abs/2506.00635</link>
<guid>https://arxiv.org/abs/2506.00635</guid>
<content:encoded><![CDATA[
arXiv:2506.00635v1 Announce Type: cross 
Abstract: Spatio-temporal forecasting is crucial in many domains, such as transportation, meteorology, and energy. However, real-world scenarios frequently present challenges such as signal anomalies, noise, and distributional shifts. Existing solutions primarily enhance robustness by modifying network architectures or training procedures. Nevertheless, these approaches are computationally intensive and resource-demanding, especially for large-scale applications. In this paper, we explore a novel test-time computing paradigm, namely learning with calibration, ST-TTC, for spatio-temporal forecasting. Through learning with calibration, we aim to capture periodic structural biases arising from non-stationarity during the testing phase and perform real-time bias correction on predictions to improve accuracy. Specifically, we first introduce a spectral-domain calibrator with phase-amplitude modulation to mitigate periodic shift and then propose a flash updating mechanism with a streaming memory queue for efficient test-time computation. ST-TTC effectively bypasses complex training-stage techniques, offering an efficient and generalizable paradigm. Extensive experiments on real-world datasets demonstrate the effectiveness, universality, flexibility and efficiency of our proposed method.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving the Calibration of Confidence Scores in Text Generation Using the Output Distribution's Characteristics</title>
<link>https://arxiv.org/abs/2506.00637</link>
<guid>https://arxiv.org/abs/2506.00637</guid>
<content:encoded><![CDATA[
arXiv:2506.00637v1 Announce Type: cross 
Abstract: Well-calibrated model confidence scores can improve the usefulness of text generation models. For example, users can be prompted to review predictions with low confidence scores, to prevent models from returning bad or potentially dangerous predictions. However, confidence metrics are not always well calibrated in text generation. One reason is that in generation, there can be many valid answers, which previous methods do not always account for. Hence, a confident model could distribute its output probability among multiple sequences because they are all valid. We propose task-agnostic confidence metrics suited to generation, which rely solely on the probabilities associated with the model outputs without the need for further fine-tuning or heuristics. Using these, we are able to improve the calibration of BART and Flan-T5 on summarization, translation, and QA datasets.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SATA-BENCH: Select All That Apply Benchmark for Multiple Choice Questions</title>
<link>https://arxiv.org/abs/2506.00643</link>
<guid>https://arxiv.org/abs/2506.00643</guid>
<content:encoded><![CDATA[
arXiv:2506.00643v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly evaluated on single-answer multiple-choice tasks, yet many real-world problems require identifying all correct answers from a set of options. This capability remains underexplored. We introduce SATA-BENCH, the first dedicated benchmark for evaluating LLMs on Select All That Apply (SATA) questions across diverse domains, including reading comprehension, law, and biomedicine. Our evaluation of 27 open-source and proprietary models reveals a significant gap: even the strongest model achieves only 41.8% exact match, exposing LLMs' inability to reliably identify all correct answers. We find that this weakness stems from two core challenges: selection bias - models favor certain choices regardless of content, and count bias - models fail to predict the correct number of answers. To address these issues, we propose Choice Funnel, a decoding strategy that combines token debiasing with adaptive thresholding to guide models toward complete and accurate selections. Choice Funnel achieves up to 29% higher exact match than competitive baselines while reducing inference cost by over 64%. Our findings expose fundamental limitations in current LLMs and introduce a new framework for diagnosing and improving multi-answer reasoning. We release SATA-BENCH and Choice Funnel to promote LLM development for robust decision-making in realistic, multi-answer applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clinical Annotations for Automatic Stuttering Severity Assessment</title>
<link>https://arxiv.org/abs/2506.00644</link>
<guid>https://arxiv.org/abs/2506.00644</guid>
<content:encoded><![CDATA[
arXiv:2506.00644v1 Announce Type: cross 
Abstract: Stuttering is a complex disorder that requires specialized expertise for effective assessment and treatment. This paper presents an effort to enhance the FluencyBank dataset with a new stuttering annotation scheme based on established clinical standards. To achieve high-quality annotations, we hired expert clinicians to label the data, ensuring that the resulting annotations mirror real-world clinical expertise. The annotations are multi-modal, incorporating audiovisual features for the detection and classification of stuttering moments, secondary behaviors, and tension scores. In addition to individual annotations, we additionally provide a test set with highly reliable annotations based on expert consensus for assessing individual annotators and machine learning models. Our experiments and analysis illustrate the complexity of this task that necessitates extensive clinical expertise for valid training and evaluation of stuttering assessment models.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linear Representation Transferability Hypothesis: Leveraging Small Models to Steer Large Models</title>
<link>https://arxiv.org/abs/2506.00653</link>
<guid>https://arxiv.org/abs/2506.00653</guid>
<content:encoded><![CDATA[
arXiv:2506.00653v1 Announce Type: cross 
Abstract: It has been hypothesized that neural networks with similar architectures trained on similar data learn shared representations relevant to the learning task. We build on this idea by extending the conceptual framework where representations learned across models trained on the same data can be expressed as linear combinations of a \emph{universal} set of basis features. These basis features underlie the learning task itself and remain consistent across models, regardless of scale. From this framework, we propose the \textbf{Linear Representation Transferability (LRT)} Hypothesis -- that there exists an affine transformation between the representation spaces of different models. To test this hypothesis, we learn affine mappings between the hidden states of models of different sizes and evaluate whether steering vectors -- directions in hidden state space associated with specific model behaviors -- retain their semantic effect when transferred from small to large language models using the learned mappings. We find strong empirical evidence that such affine mappings can preserve steering behaviors. These findings suggest that representations learned by small models can be used to guide the behavior of large models, and that the LRT hypothesis may be a promising direction on understanding representation alignment across model scales.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Permutation-Invariant Transformer Neural Architectures for Set-Based Indoor Localization Using Learned RSSI Embeddings</title>
<link>https://arxiv.org/abs/2506.00656</link>
<guid>https://arxiv.org/abs/2506.00656</guid>
<content:encoded><![CDATA[
arXiv:2506.00656v1 Announce Type: cross 
Abstract: We propose a permutation-invariant neural architecture for indoor localization using RSSI scans from Wi-Fi access points. Each scan is modeled as an unordered set of (BSSID, RSSI) pairs, where BSSIDs are mapped to learned embeddings and concatenated with signal strength. These are processed by a Set Transformer, enabling the model to handle variable-length, sparse inputs while learning attention- based representations over access point relationships. We evaluate the model on a dataset collected across a campus environment consisting of six buildings. Results show that the model accurately recovers fine-grained spatial structure and maintains performance across physically distinct domains. In our experiments, a simple LSTM consistently outperformed all other models, achieving the lowest mean localization error across three tasks (E1 - E3), with average errors as low as 2.23 m. The Set Transformer performed competitively, ranking second in every experiment and outperforming the MLP, RNN, and basic attention models, particularly in scenarios involving multiple buildings (E2) and multiple floors (E3). Performance degraded most in E2, where signal conditions varied substantially across buildings, highlighting the importance of architectural robustness to domain diversity. This work demonstrates that set-based neural models are a natural fit for signal-based localization, offering a principled approach to handling sparse, unordered inputs in real-world positioning tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sarc7: Evaluating Sarcasm Detection and Generation with Seven Types and Emotion-Informed Techniques</title>
<link>https://arxiv.org/abs/2506.00658</link>
<guid>https://arxiv.org/abs/2506.00658</guid>
<content:encoded><![CDATA[
arXiv:2506.00658v1 Announce Type: cross 
Abstract: Sarcasm is a form of humor where expressions convey meanings opposite to their literal interpretations. Classifying and generating sarcasm using large language models is vital for interpreting human communication. Sarcasm poses challenges for computational models, due to its nuanced nature. We introduce Sarc7, a benchmark that classifies 7 types of sarcasm: self-deprecating, brooding, deadpan, polite, obnoxious, raging, and manic by annotating entries of the MUStARD dataset. Classification was evaluated using zero-shot, few-shot, chain-of-thought (CoT), and a novel emotion-based prompting technique. We propose an emotion-based generation method developed by identifying key components of sarcasm-incongruity, shock value, and context dependency. Our classification experiments show that Gemini 2.5, using emotion-based prompting, outperforms other setups with an F1 score of 0.3664. Human evaluators preferred our emotion-based prompting, with 38.46% more successful generations than zero-shot prompting.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differential Privacy for Deep Learning in Medicine</title>
<link>https://arxiv.org/abs/2506.00660</link>
<guid>https://arxiv.org/abs/2506.00660</guid>
<content:encoded><![CDATA[
arXiv:2506.00660v1 Announce Type: cross 
Abstract: Differential privacy (DP) is a key technique for protecting sensitive patient data in medical deep learning (DL). As clinical models grow more data-dependent, balancing privacy with utility and fairness has become a critical challenge. This scoping review synthesizes recent developments in applying DP to medical DL, with a particular focus on DP-SGD and alternative mechanisms across centralized and federated settings. Using a structured search strategy, we identified 74 studies published up to March 2025. Our analysis spans diverse data modalities, training setups, and downstream tasks, and highlights the tradeoffs between privacy guarantees, model accuracy, and subgroup fairness. We find that while DP-especially at strong privacy budgets-can preserve performance in well-structured imaging tasks, severe degradation often occurs under strict privacy, particularly in underrepresented or complex modalities. Furthermore, privacy-induced performance gaps disproportionately affect demographic subgroups, with fairness impacts varying by data type and task. A small subset of studies explicitly addresses these tradeoffs through subgroup analysis or fairness metrics, but most omit them entirely. Beyond DP-SGD, emerging approaches leverage alternative mechanisms, generative models, and hybrid federated designs, though reporting remains inconsistent. We conclude by outlining key gaps in fairness auditing, standardization, and evaluation protocols, offering guidance for future work toward equitable and clinically robust privacy-preserving DL systems in medicine.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thinking Out of the Box: Hybrid SAT Solving by Unconstrained Continuous Optimization</title>
<link>https://arxiv.org/abs/2506.00674</link>
<guid>https://arxiv.org/abs/2506.00674</guid>
<content:encoded><![CDATA[
arXiv:2506.00674v1 Announce Type: cross 
Abstract: The Boolean satisfiability (SAT) problem lies at the core of many applications in combinatorial optimization, software verification, cryptography, and machine learning. While state-of-the-art solvers have demonstrated high efficiency in handling conjunctive normal form (CNF) formulas, numerous applications require non-CNF (hybrid) constraints, such as XOR, cardinality, and Not-All-Equal constraints. Recent work leverages polynomial representations to represent such hybrid constraints, but it relies on box constraints that can limit the use of powerful unconstrained optimizers. In this paper, we propose unconstrained continuous optimization formulations for hybrid SAT solving by penalty terms. We provide theoretical insights into when these penalty terms are necessary and demonstrate empirically that unconstrained optimizers (e.g., Adam) can enhance SAT solving on hybrid benchmarks. Our results highlight the potential of combining continuous optimization and machine-learning-based methods for effective hybrid SAT solving.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeTuneBed: A Toolkit for Benchmarking LLM Safety Alignment in Fine-Tuning</title>
<link>https://arxiv.org/abs/2506.00676</link>
<guid>https://arxiv.org/abs/2506.00676</guid>
<content:encoded><![CDATA[
arXiv:2506.00676v1 Announce Type: cross 
Abstract: As large language models (LLMs) become ubiquitous, parameter-efficient fine-tuning methods and safety-first defenses have proliferated rapidly. However, the number of approaches and their recent increase have resulted in diverse evaluations-varied datasets, metrics, and inconsistent threat settings-making it difficult to fairly compare safety, utility, and robustness across methods. To address this, we introduce SafeTuneBed, a benchmark and toolkit unifying fine-tuning and defense evaluation. SafeTuneBed (i) curates a diverse repository of multiple fine-tuning datasets spanning sentiment analysis, question-answering, multi-step reasoning, and open-ended instruction tasks, and allows for the generation of harmful-variant splits; (ii) enables integration of state-of-the-art defenses, including alignment-stage immunization, in-training safeguards, and post-tuning repair; and (iii) provides evaluators for safety (attack success rate, refusal consistency) and utility. Built on Python-first, dataclass-driven configs and plugins, SafeTuneBed requires minimal additional code to specify any fine-tuning regime, defense method, and metric suite, while ensuring end-to-end reproducibility. We showcase its value by benchmarking representative defenses across varied poisoning scenarios and tasks. By standardizing data, code, and metrics, SafeTuneBed is the first focused toolkit of its kind to accelerate rigorous and comparable research in safe LLM fine-tuning. Code is available at: https://github.com/criticalml-uw/SafeTuneBed
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CineMA: A Foundation Model for Cine Cardiac MRI</title>
<link>https://arxiv.org/abs/2506.00679</link>
<guid>https://arxiv.org/abs/2506.00679</guid>
<content:encoded><![CDATA[
arXiv:2506.00679v1 Announce Type: cross 
Abstract: Cardiac magnetic resonance (CMR) is a key investigation in clinical cardiovascular medicine and has been used extensively in population research. However, extracting clinically important measurements such as ejection fraction for diagnosing cardiovascular diseases remains time-consuming and subjective. We developed CineMA, a foundation AI model automating these tasks with limited labels. CineMA is a self-supervised autoencoder model trained on 74,916 cine CMR studies to reconstruct images from masked inputs. After fine-tuning, it was evaluated across eight datasets on 23 tasks from four categories: ventricle and myocardium segmentation, left and right ventricle ejection fraction calculation, disease detection and classification, and landmark localisation. CineMA is the first foundation model for cine CMR to match or outperform convolutional neural networks (CNNs). CineMA demonstrated greater label efficiency than CNNs, achieving comparable or better performance with fewer annotations. This reduces the burden of clinician labelling and supports replacing task-specific training with fine-tuning foundation models in future cardiac imaging applications. Models and code for pre-training and fine-tuning are available at https://github.com/mathpluscode/CineMA, democratising access to high-performance models that otherwise require substantial computational resources, promoting reproducibility and accelerating clinical translation.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Existing Large Language Model Unlearning Evaluations Are Inconclusive</title>
<link>https://arxiv.org/abs/2506.00688</link>
<guid>https://arxiv.org/abs/2506.00688</guid>
<content:encoded><![CDATA[
arXiv:2506.00688v1 Announce Type: cross 
Abstract: Machine unlearning aims to remove sensitive or undesired data from large language models. However, recent studies suggest that unlearning is often shallow, claiming that removed knowledge can easily be recovered. In this work, we critically examine standard unlearning evaluation practices and uncover key limitations that shake our trust in those findings. First, we show that some evaluations introduce substantial new information into the model, potentially masking true unlearning performance by re-teaching the model during testing. Second, we demonstrate that evaluation outcomes vary significantly across tasks, undermining the generalizability of current evaluation routines. Finally, we find that many evaluations rely on spurious correlations, making their results difficult to trust and interpret. Taken together, these issues suggest that current evaluation protocols may both overstate and understate unlearning success. To address this, we propose two principles for future unlearning evaluations: minimal information injection and downstream task awareness. We validate these principles through a series of targeted experiments, showing how violations of each can lead to misleading conclusions.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Sensory Neurons: Nonlinear Attention Mechanisms for Accelerated Convergence in Permutation-Invariant Neural Networks for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.00691</link>
<guid>https://arxiv.org/abs/2506.00691</guid>
<content:encoded><![CDATA[
arXiv:2506.00691v1 Announce Type: cross 
Abstract: Training reinforcement learning (RL) agents often requires significant computational resources and extended training times. To address this, we build upon the foundation laid by Google Brain's Sensory Neuron, which introduced a novel neural architecture for reinforcement learning tasks that maintained permutation in-variance in the sensory neuron system. While the baseline model demonstrated significant performance improvements over traditional approaches, we identified opportunities to enhance the efficiency of the learning process further. We propose a modified attention mechanism incorporating a non-linear transformation of the key vectors (K) using a mapping function, resulting in a new set of key vectors (K'). This non-linear mapping enhances the representational capacity of the attention mechanism, allowing the model to encode more complex feature interactions and accelerating convergence without compromising performance. Our enhanced model demonstrates significant improvements in learning efficiency, showcasing the potential for non-linear attention mechanisms in advancing reinforcement learning algorithms.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Faithfulness and Abstention: An Automated Pipeline for Evaluating LLM-Generated 3-ply Case-Based Legal Arguments</title>
<link>https://arxiv.org/abs/2506.00694</link>
<guid>https://arxiv.org/abs/2506.00694</guid>
<content:encoded><![CDATA[
arXiv:2506.00694v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) demonstrate potential in complex legal tasks like argument generation, yet their reliability remains a concern. Building upon pilot work assessing LLM generation of 3-ply legal arguments using human evaluation, this paper introduces an automated pipeline to evaluate LLM performance on this task, specifically focusing on faithfulness (absence of hallucination), factor utilization, and appropriate abstention. We define hallucination as the generation of factors not present in the input case materials and abstention as the model's ability to refrain from generating arguments when instructed and no factual basis exists. Our automated method employs an external LLM to extract factors from generated arguments and compares them against the ground-truth factors provided in the input case triples (current case and two precedent cases). We evaluated eight distinct LLMs on three tests of increasing difficulty: 1) generating a standard 3-ply argument, 2) generating an argument with swapped precedent roles, and 3) recognizing the impossibility of argument generation due to lack of shared factors and abstaining. Our findings indicate that while current LLMs achieve high accuracy (over 90%) in avoiding hallucination on viable argument generation tests (Tests 1 & 2), they often fail to utilize the full set of relevant factors present in the cases. Critically, on the abstention test (Test 3), most models failed to follow instructions to stop, instead generating spurious arguments despite the lack of common factors. This automated pipeline provides a scalable method for assessing these crucial LLM behaviors, highlighting the need for improvements in factor utilization and robust abstention capabilities before reliable deployment in legal settings. Project page: https://github.com/lizhang-AIandLaw/Measuring-Faithfulness-and-Abstention.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Inference of Training Dataset Membership</title>
<link>https://arxiv.org/abs/2506.00701</link>
<guid>https://arxiv.org/abs/2506.00701</guid>
<content:encoded><![CDATA[
arXiv:2506.00701v1 Announce Type: cross 
Abstract: Determining whether a dataset was part of a machine learning model's training data pool can reveal privacy vulnerabilities, a challenge often addressed through membership inference attacks (MIAs). Traditional MIAs typically require access to model internals or rely on computationally intensive shadow models. This paper proposes an efficient, interpretable and principled Bayesian inference method for membership inference. By analyzing post-hoc metrics such as prediction error, confidence (entropy), perturbation magnitude, and dataset statistics from a trained ML model, our approach computes posterior probabilities of membership without requiring extensive model training. Experimental results on synthetic datasets demonstrate the method's effectiveness in distinguishing member from non-member datasets. Beyond membership inference, this method can also detect distribution shifts, offering a practical and interpretable alternative to existing approaches.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QoQ-Med: Building Multimodal Clinical Foundation Models with Domain-Aware GRPO Training</title>
<link>https://arxiv.org/abs/2506.00711</link>
<guid>https://arxiv.org/abs/2506.00711</guid>
<content:encoded><![CDATA[
arXiv:2506.00711v1 Announce Type: cross 
Abstract: Clinical decision-making routinely demands reasoning over heterogeneous data, yet existing multimodal language models (MLLMs) remain largely vision-centric and fail to generalize across clinical specialties. To bridge this gap, we introduce QoQ-Med-7B/32B, the first open generalist clinical foundation model that jointly reasons across medical images, time-series signals, and text reports. QoQ-Med is trained with Domain-aware Relative Policy Optimization (DRPO), a novel reinforcement-learning objective that hierarchically scales normalized rewards according to domain rarity and modality difficulty, mitigating performance imbalance caused by skewed clinical data distributions. Trained on 2.61 million instruction tuning pairs spanning 9 clinical domains, we show that DRPO training boosts diagnostic performance by 43% in macro-F1 on average across all visual domains as compared to other critic-free training methods like GRPO. Furthermore, with QoQ-Med trained on intensive segmentation data, it is able to highlight salient regions related to the diagnosis, with an IoU 10x higher than open models while reaching the performance of OpenAI o4-mini. To foster reproducibility and downstream research, we release (i) the full model weights, (ii) the modular training pipeline, and (iii) all intermediate reasoning traces at https://github.com/DDVD233/QoQ_Med.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Argumentative Text to Argument Knowledge Graph: A New Framework for Structured Argumentation</title>
<link>https://arxiv.org/abs/2506.00713</link>
<guid>https://arxiv.org/abs/2506.00713</guid>
<content:encoded><![CDATA[
arXiv:2506.00713v1 Announce Type: cross 
Abstract: This paper presents a framework to convert argumentative texts into argument knowledge graphs (AKG). Starting with basic annotations of argumentative components (ACs) and argumentative relations (ARs), we enrich the information by constructing a knowledge base (KB) graph with metadata attributes for nodes. Next, we use premises and inference rules from the KB to form arguments by applying modus ponens. From these arguments, we create an AKG. The nodes and edges of the AKG have attributes that capture important argumentative features. We also find missing inference rules by identifying markers. This makes it possible to identify undercut attacks that were previously undetectable in existing datasets. The AKG gives a graphical view of the argumentative structure that is easier to understand than theoretical formats. It also prepares the ground for future reasoning tasks, including checking the coherence of arguments and identifying opportunities for revision. For this, it is important to find indirect relations, many of which are implicit. Our proposed AKG format, with annotated inference rules and modus ponens, will help reasoning models learn the implicit indirect relations that require inference over arguments and the relations between them.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An LLM Agent for Functional Bug Detection in Network Protocols</title>
<link>https://arxiv.org/abs/2506.00714</link>
<guid>https://arxiv.org/abs/2506.00714</guid>
<content:encoded><![CDATA[
arXiv:2506.00714v1 Announce Type: cross 
Abstract: Functional correctness is critical for ensuring the reliability and security of network protocol implementations. Functional bugs, instances where implementations diverge from behaviors specified in RFC documents, can lead to severe consequences, including faulty routing, authentication bypasses, and service disruptions. Detecting these bugs requires deep semantic analysis across specification documents and source code, a task beyond the capabilities of traditional static analysis tools. This paper introduces RFCScan, an autonomous agent that leverages large language models (LLMs) to detect functional bugs by checking conformance between network protocol implementations and their RFC specifications. Inspired by the human auditing procedure, RFCScan comprises two key components: an indexing agent and a detection agent. The former hierarchically summarizes protocol code semantics, generating semantic indexes that enable the detection agent to narrow down the scanning scope. The latter employs demand-driven retrieval to iteratively collect additional relevant data structures and functions, eventually identifying potential inconsistencies with the RFC specifications effectively. We evaluate RFCScan across six real-world network protocol implementations. RFCScan identifies 47 functional bugs with 81.9% precision, of which 20 bugs have been confirmed or fixed by developers.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Local Cues to Global Percepts: Emergent Gestalt Organization in Self-Supervised Vision Models</title>
<link>https://arxiv.org/abs/2506.00718</link>
<guid>https://arxiv.org/abs/2506.00718</guid>
<content:encoded><![CDATA[
arXiv:2506.00718v1 Announce Type: cross 
Abstract: Human vision organizes local cues into coherent global forms using Gestalt principles like closure, proximity, and figure-ground assignment -- functions reliant on global spatial structure. We investigate whether modern vision models show similar behaviors, and under what training conditions these emerge. We find that Vision Transformers (ViTs) trained with Masked Autoencoding (MAE) exhibit activation patterns consistent with Gestalt laws, including illusory contour completion, convexity preference, and dynamic figure-ground segregation. To probe the computational basis, we hypothesize that modeling global dependencies is necessary for Gestalt-like organization. We introduce the Distorted Spatial Relationship Testbench (DiSRT), which evaluates sensitivity to global spatial perturbations while preserving local textures. Using DiSRT, we show that self-supervised models (e.g., MAE, CLIP) outperform supervised baselines and sometimes even exceed human performance. ConvNeXt models trained with MAE also exhibit Gestalt-compatible representations, suggesting such sensitivity can arise without attention architectures. However, classification finetuning degrades this ability. Inspired by biological vision, we show that a Top-K activation sparsity mechanism can restore global sensitivity. Our findings identify training conditions that promote or suppress Gestalt-like perception and establish DiSRT as a diagnostic for global structure sensitivity across models.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pitfalls in Evaluating Language Model Forecasters</title>
<link>https://arxiv.org/abs/2506.00723</link>
<guid>https://arxiv.org/abs/2506.00723</guid>
<content:encoded><![CDATA[
arXiv:2506.00723v1 Announce Type: cross 
Abstract: Large language models (LLMs) have recently been applied to forecasting tasks, with some works claiming these systems match or exceed human performance. In this paper, we argue that, as a community, we should be careful about such conclusions as evaluating LLM forecasters presents unique challenges. We identify two broad categories of issues: (1) difficulty in trusting evaluation results due to many forms of temporal leakage, and (2) difficulty in extrapolating from evaluation performance to real-world forecasting. Through systematic analysis and concrete examples from prior work, we demonstrate how evaluation flaws can raise concerns about current and future performance claims. We argue that more rigorous evaluation methodologies are needed to confidently assess the forecasting abilities of LLMs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoPINNEnKF: Iterative Model Inference using generic-PINN-based ensemble Kalman filter</title>
<link>https://arxiv.org/abs/2506.00731</link>
<guid>https://arxiv.org/abs/2506.00731</guid>
<content:encoded><![CDATA[
arXiv:2506.00731v1 Announce Type: cross 
Abstract: Physics-informed neural networks (PINNs) have emerged as a powerful tool for solving forward and inverse problems involving partial differential equations (PDEs) by incorporating physical laws into the training process. However, the performance of PINNs is often hindered in real-world scenarios involving noisy observational data and missing physics, particularly in inverse problems. In this work, we propose an iterative multi-objective PINN ensemble Kalman filter (MoPINNEnKF) framework that improves the robustness and accuracy of PINNs in both forward and inverse problems by using the \textit{ensemble Kalman filter} and the \textit{non-dominated sorting genetic algorithm} III (NSGA-III). Specifically, NSGA-III is used as a multi-objective optimizer that can generate various ensemble members of PINNs along the optimal Pareto front, while accounting the model uncertainty in the solution space. These ensemble members are then utilized within the EnKF to assimilate noisy observational data. The EnKF's analysis is subsequently used to refine the data loss component for retraining the PINNs, thereby iteratively updating their parameters. The iterative procedure generates improved solutions to the PDEs. The proposed method is tested on two benchmark problems: the one-dimensional viscous Burgers equation and the time-fractional mixed diffusion-wave equation (TFMDWE). The numerical results show it outperforms standard PINNs in handling noisy data and missing physics.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Length Aware Speech Translation for Video Dubbing</title>
<link>https://arxiv.org/abs/2506.00740</link>
<guid>https://arxiv.org/abs/2506.00740</guid>
<content:encoded><![CDATA[
arXiv:2506.00740v1 Announce Type: cross 
Abstract: In video dubbing, aligning translated audio with the source audio is a significant challenge. Our focus is on achieving this efficiently, tailored for real-time, on-device video dubbing scenarios. We developed a phoneme-based end-to-end length-sensitive speech translation (LSST) model, which generates translations of varying lengths short, normal, and long using predefined tags. Additionally, we introduced length-aware beam search (LABS), an efficient approach to generate translations of different lengths in a single decoding pass. This approach maintained comparable BLEU scores compared to a baseline without length awareness while significantly enhancing synchronization quality between source and target audio, achieving a mean opinion score (MOS) gain of 0.34 for Spanish and 0.65 for Korean, respectively.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ArtiScene: Language-Driven Artistic 3D Scene Generation Through Image Intermediary</title>
<link>https://arxiv.org/abs/2506.00742</link>
<guid>https://arxiv.org/abs/2506.00742</guid>
<content:encoded><![CDATA[
arXiv:2506.00742v1 Announce Type: cross 
Abstract: Designing 3D scenes is traditionally a challenging task that demands both artistic expertise and proficiency with complex software. Recent advances in text-to-3D generation have greatly simplified this process by letting users create scenes based on simple text descriptions. However, as these methods generally require extra training or in-context learning, their performance is often hindered by the limited availability of high-quality 3D data. In contrast, modern text-to-image models learned from web-scale images can generate scenes with diverse, reliable spatial layouts and consistent, visually appealing styles. Our key insight is that instead of learning directly from 3D scenes, we can leverage generated 2D images as an intermediary to guide 3D synthesis. In light of this, we introduce ArtiScene, a training-free automated pipeline for scene design that integrates the flexibility of free-form text-to-image generation with the diversity and reliability of 2D intermediary layouts.
  First, we generate 2D images from a scene description, then extract the shape and appearance of objects to create 3D models. These models are assembled into the final scene using geometry, position, and pose information derived from the same intermediary image. Being generalizable to a wide range of scenes and styles, ArtiScene outperforms state-of-the-art benchmarks by a large margin in layout and aesthetic quality by quantitative metrics. It also averages a 74.89% winning rate in extensive user studies and 95.07% in GPT-4o evaluation. Project page: https://artiscene-cvpr.github.io/
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assortment of Attention Heads: Accelerating Federated PEFT with Head Pruning and Strategic Client Selection</title>
<link>https://arxiv.org/abs/2506.00743</link>
<guid>https://arxiv.org/abs/2506.00743</guid>
<content:encoded><![CDATA[
arXiv:2506.00743v1 Announce Type: cross 
Abstract: Parameter Efficient Fine-Tuning (PEFT) has become the de-facto approach in adapting Large Language Models (LLMs) for downstream tasks in Natural Language Processing. However, its adoption in privacy-preserving distributed learning frameworks, such as Federated Learning (FL), remains relatively limited. This is mainly due to challenges specific to FL, such as resource-constrained devices and diverse data distributions among clients. In this paper, we propose an efficient method to perform PEFT within the FL framework for Multi-Head Attention (MHA) based language models. We address the challenges through head pruning, a novel head-specific weighted aggregation mechanism, and a client selection strategy. Head pruning minimizes training complexity within the clients, guided by the importance score computed based on the confidence of the attention head. Weighted aggregation of heads ensures the global model captures crucial updates from diverse clients complementing our client selection strategy. We show results on the MultiNLI benchmark along with 20 Newsgroups, XL-Sum, and E2E NLG datasets. We use the MultiNLI dataset and T5-small model with LoRA as our PEFT method, attaining sparsity levels of up to 90%, resulting in a communication advantage of up to 1.8x and a reduction in training OPs of 3.9x while maintaining the accuracy drop under 2%.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeSense: a Real-World Benchmark and Dataset for Code Semantic Reasoning</title>
<link>https://arxiv.org/abs/2506.00750</link>
<guid>https://arxiv.org/abs/2506.00750</guid>
<content:encoded><![CDATA[
arXiv:2506.00750v1 Announce Type: cross 
Abstract: Understanding and reasoning about code semantics is essential for enhancing code LLMs' abilities to solve real-world software engineering (SE) tasks. Although several code reasoning benchmarks exist, most rely on synthetic datasets or educational coding problems and focus on coarse-grained reasoning tasks such as input/output prediction, limiting their effectiveness in evaluating LLMs in practical SE contexts. To bridge this gap, we propose CodeSense, the first benchmark that makes available a spectrum of fine-grained code reasoning tasks concerned with the software engineering of real-world code. We collected Python, C and Java software projects from real-world repositories. We executed tests from these repositories, collected their execution traces, and constructed a ground truth dataset for fine-grained semantic reasoning tasks. We then performed comprehensive evaluations on state-of-the-art LLMs. Our results show a clear performance gap for the models to handle fine-grained reasoning tasks. Although prompting techniques such as chain-of-thought and in-context learning helped, the lack of code semantics in LLMs fundamentally limit models' capabilities of code reasoning. Besides dataset, benchmark and evaluation, our work produced an execution tracing framework and tool set that make it easy to collect ground truth for fine-grained SE reasoning tasks, offering a strong basis for future benchmark construction and model post training. Our code and data are located at https://codesense-bench.github.io/.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Who experiences large model decay and why?" A Hierarchical Framework for Diagnosing Heterogeneous Performance Drift</title>
<link>https://arxiv.org/abs/2506.00756</link>
<guid>https://arxiv.org/abs/2506.00756</guid>
<content:encoded><![CDATA[
arXiv:2506.00756v1 Announce Type: cross 
Abstract: Machine learning (ML) models frequently experience performance degradation when deployed in new contexts. Such degradation is rarely uniform: some subgroups may suffer large performance decay while others may not. Understanding where and how large differences in performance arise is critical for designing targeted corrective actions that mitigate decay for the most affected subgroups while minimizing any unintended effects. Current approaches do not provide such detailed insight, as they either (i) explain how average performance shifts arise or (ii) identify adversely affected subgroups without insight into how this occurred. To this end, we introduce a Subgroup-scanning Hierarchical Inference Framework for performance drifT (SHIFT). SHIFT first asks "Is there any subgroup with unacceptably large performance decay due to covariate/outcome shifts?" (Where?) and, if so, dives deeper to ask "Can we explain this using more detailed variable(subset)-specific shifts?" (How?). In real-world experiments, we find that SHIFT identifies interpretable subgroups affected by performance decay, and suggests targeted actions that effectively mitigate the decay.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Attention: Learning Spatio-Temporal Dynamics with Emergent Interpretable Topologies</title>
<link>https://arxiv.org/abs/2506.00770</link>
<guid>https://arxiv.org/abs/2506.00770</guid>
<content:encoded><![CDATA[
arXiv:2506.00770v1 Announce Type: cross 
Abstract: Spatio-temporal forecasting is critical in applications such as traffic prediction, energy demand modeling, and weather monitoring. While Graph Attention Networks (GATs) are popular for modeling spatial dependencies, they rely on predefined adjacency structures and dynamic attention scores, introducing inductive biases and computational overhead that can obscure interpretability.
  We propose InterGAT, a simplified alternative to GAT that replaces masked attention with a fully learnable, symmetric node interaction matrix, capturing latent spatial relationships without relying on fixed graph topologies. Our framework, InterGAT-GRU, which incorporates a GRU-based temporal decoder, outperforms the baseline GAT-GRU in forecasting accuracy, achieving at least a 21% improvement on the SZ-Taxi dataset and a 6% improvement on the Los-Loop dataset across all forecasting horizons (15 to 60 minutes). Additionally, we observed reduction in training time by 60-70% compared to GAT-GRU baseline.
  Crucially, the learned interaction matrix reveals interpretable structure: it recovers sparse, topology-aware attention patterns that align with community structure. Spectral and clustering analyses show that the model captures both localized and global dynamics, offering insights into the functional topology driving predictions. This highlights how structure learning can simultaneously support prediction, computational efficiency, and topological interpretabil-ity in dynamic graph-based domains.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Manipulating 3D Molecules in a Fixed-Dimensional SE(3)-Equivariant Latent Space</title>
<link>https://arxiv.org/abs/2506.00771</link>
<guid>https://arxiv.org/abs/2506.00771</guid>
<content:encoded><![CDATA[
arXiv:2506.00771v1 Announce Type: cross 
Abstract: Medicinal chemists often optimize drugs considering their 3D structures and designing structurally distinct molecules that retain key features, such as shapes, pharmacophores, or chemical properties. Previous deep learning approaches address this through supervised tasks like molecule inpainting or property-guided optimization. In this work, we propose a flexible zero-shot molecule manipulation method by navigating in a shared latent space of 3D molecules. We introduce a Variational AutoEncoder (VAE) for 3D molecules, named MolFLAE, which learns a fixed-dimensional, SE(3)-equivariant latent space independent of atom counts. MolFLAE encodes 3D molecules using an SE(3)-equivariant neural network into fixed number of latent nodes, distinguished by learned embeddings. The latent space is regularized, and molecular structures are reconstructed via a Bayesian Flow Network (BFN) conditioned on the encoder's latent output. MolFLAE achieves competitive performance on standard unconditional 3D molecule generation benchmarks. Moreover, the latent space of MolFLAE enables zero-shot molecule manipulation, including atom number editing, structure reconstruction, and coordinated latent interpolation for both structure and properties. We further demonstrate our approach on a drug optimization task for the human glucocorticoid receptor, generating molecules with improved hydrophilicity while preserving key interactions, under computational evaluations. These results highlight the flexibility, robustness, and real-world utility of our method, opening new avenues for molecule editing and optimization.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LIFT the Veil for the Truth: Principal Weights Emerge after Rank Reduction for Reasoning-Focused Supervised Fine-Tuning</title>
<link>https://arxiv.org/abs/2506.00772</link>
<guid>https://arxiv.org/abs/2506.00772</guid>
<content:encoded><![CDATA[
arXiv:2506.00772v1 Announce Type: cross 
Abstract: Recent studies have shown that supervised fine-tuning of LLMs on a small number of high-quality datasets can yield strong reasoning capabilities. However, full fine-tuning (Full FT), while powerful, is computationally expensive and susceptible to overfitting and catastrophic forgetting, particularly when data is limited. Sparse fine-tuning, which previously achieved notable success by updating only a small subset of model parameters, offers a promising trade-off between efficiency and effectiveness. Yet, it has lagged behind in the LLM era due to the difficulty of identifying parameters truly critical for reasoning. In this work, we state that weights with the largest magnitude after low-rank approximation are critical weights for fine-tuning, which we call Principal Weights. Surprisingly, while magnitude-based sparse fine-tuning performs poorly as a baseline on LLM fine-tuning, it becomes highly effective after rank reduction. These insights motivate our method: Low-rank Informed Sparse Fine-Tuning (LIFT). LIFT only updates the top 5% Principal Weights throughout training and consistently achieves better performance on reasoning tasks than Full FT, while maintaining memory efficiency on par with popular parameter-efficient fine-tuning methods. In addition to strong performance on target domains such as arithmetic reasoning, LIFT also retains up to 20% more source-domain knowledge, compared to Full FT and LoRA. Our code is available at: https://github.com/zihanghliu/LIFT.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KG-TRACES: Enhancing Large Language Models with Knowledge Graph-constrained Trajectory Reasoning and Attribution Supervision</title>
<link>https://arxiv.org/abs/2506.00783</link>
<guid>https://arxiv.org/abs/2506.00783</guid>
<content:encoded><![CDATA[
arXiv:2506.00783v1 Announce Type: cross 
Abstract: Large language models (LLMs) have made remarkable strides in various natural language processing tasks, but their performance on complex reasoning problems remains hindered by a lack of explainability and trustworthiness. This issue, often manifesting as hallucinations or unattributable reasoning processes, limits their applicability in complex reasoning scenarios. To address this, we propose Knowledge Graph-constrained Trajectory Reasoning Attribution and Chain Explanation Supervision (KG-TRACES), a novel framework that enhances the reasoning ability of LLMs through explicit supervision over reasoning paths and processes. KG-TRACES jointly supervises the model to: (1) predict symbolic relation paths, (2) predict full triple-level reasoning paths, and (3) generate attribution-aware reasoning processes grounded in the reasoning paths. At inference phase, the model adapts to both KG-available and KG-unavailable scenarios, retrieving reasoning paths from a KG when possible or predicting plausible reasoning paths with only intrinsic knowledge when not. This design enables the model to reason in an explainable and source-attributable pattern. Through extensive experiments on complex reasoning tasks, we demonstrate that KG-TRACES significantly outperforms existing SOTA: it improves Hits@1 by 1.6% and F1 by 4.7% on WebQSP, and achieves improvements of 4.8% in Hits@1 and 2.1% in F1 on CWQ. Moreover, we show its transferability to specialized domains such as medicine. By visualizing the intermediate steps of reasoning processes, we further show that the explicit supervision introduced by KG-TRACES leads to more stable and goal-directed reasoning processes, aligning closely with correct answers. Code is available at https://github.com/Edaizi/KG-TRACES.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Behavioral Augmentation of UML Class Diagrams: An Empirical Study of Large Language Models for Method Generation</title>
<link>https://arxiv.org/abs/2506.00788</link>
<guid>https://arxiv.org/abs/2506.00788</guid>
<content:encoded><![CDATA[
arXiv:2506.00788v1 Announce Type: cross 
Abstract: Automating the enrichment of UML class diagrams with behavioral methods from natural language use cases is a significant challenge. This study evaluates nine large language models (LLMs) in augmenting a methodless UML diagram (21 classes, 17 relationships) using 21 structured waste-management use cases. A total of 90 diagrams (3,373 methods) were assessed across six metrics: method quantity, signature richness (visibility, names, parameters, return types), annotation completeness (linking to use cases/actions), structural fidelity, syntactic correctness (PlantUML compilation), and naming convergence (across models). All LLMs produced valid PlantUML diagrams adhering to UML conventions. Some models excelled in method coverage and annotation accuracy, while others showed richer parameterization but weaker traceability. These results demonstrate that LLMs can generate well-structured methods with consistent naming, advancing automated behavioral modeling. However, inconsistencies in annotations and signatures highlight the need for improved prompt engineering and model selection. The rapid generation of these methods supports Agile practices by enabling faster design iterations. Despite their capabilities, human oversight is essential to ensure accuracy, appropriateness, and semantic alignment. This positions LLMs as collaborative partners in software design. All experimental artifacts (\texttt{.puml}, \texttt{.png}, \texttt{.csv}) are publicly available for reproducibility.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Action Dependency Graphs for Globally Optimal Coordinated Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.00797</link>
<guid>https://arxiv.org/abs/2506.00797</guid>
<content:encoded><![CDATA[
arXiv:2506.00797v1 Announce Type: cross 
Abstract: Action-dependent individual policies, which incorporate both environmental states and the actions of other agents in decision-making, have emerged as a promising paradigm for achieving global optimality in multi-agent reinforcement learning (MARL). However, the existing literature often adopts auto-regressive action-dependent policies, where each agent's policy depends on the actions of all preceding agents. This formulation incurs substantial computational complexity as the number of agents increases, thereby limiting scalability. In this work, we consider a more generalized class of action-dependent policies, which do not necessarily follow the auto-regressive form. We propose to use the `action dependency graph (ADG)' to model the inter-agent action dependencies. Within the context of MARL problems structured by coordination graphs, we prove that an action-dependent policy with a sparse ADG can achieve global optimality, provided the ADG satisfies specific conditions specified by the coordination graph. Building on this theoretical foundation, we develop a tabular policy iteration algorithm with guaranteed global optimality. Furthermore, we integrate our framework into several SOTA algorithms and conduct experiments in complex environments. The empirical results affirm the robustness and applicability of our approach in more general scenarios, underscoring its potential for broader MARL challenges.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlearning Inversion Attacks for Graph Neural Networks</title>
<link>https://arxiv.org/abs/2506.00808</link>
<guid>https://arxiv.org/abs/2506.00808</guid>
<content:encoded><![CDATA[
arXiv:2506.00808v1 Announce Type: cross 
Abstract: Graph unlearning methods aim to efficiently remove the impact of sensitive data from trained GNNs without full retraining, assuming that deleted information cannot be recovered. In this work, we challenge this assumption by introducing the graph unlearning inversion attack: given only black-box access to an unlearned GNN and partial graph knowledge, can an adversary reconstruct the removed edges? We identify two key challenges: varying probability-similarity thresholds for unlearned versus retained edges, and the difficulty of locating unlearned edge endpoints, and address them with TrendAttack. First, we derive and exploit the confidence pitfall, a theoretical and empirical pattern showing that nodes adjacent to unlearned edges exhibit a large drop in model confidence. Second, we design an adaptive prediction mechanism that applies different similarity thresholds to unlearned and other membership edges. Our framework flexibly integrates existing membership inference techniques and extends them with trend features. Experiments on four real-world datasets demonstrate that TrendAttack significantly outperforms state-of-the-art GNN membership inference baselines, exposing a critical privacy vulnerability in current graph unlearning methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>L3A: Label-Augmented Analytic Adaptation for Multi-Label Class Incremental Learning</title>
<link>https://arxiv.org/abs/2506.00816</link>
<guid>https://arxiv.org/abs/2506.00816</guid>
<content:encoded><![CDATA[
arXiv:2506.00816v1 Announce Type: cross 
Abstract: Class-incremental learning (CIL) enables models to learn new classes continually without forgetting previously acquired knowledge. Multi-label CIL (MLCIL) extends CIL to a real-world scenario where each sample may belong to multiple classes, introducing several challenges: label absence, which leads to incomplete historical information due to missing labels, and class imbalance, which results in the model bias toward majority classes. To address these challenges, we propose Label-Augmented Analytic Adaptation (L3A), an exemplar-free approach without storing past samples. L3A integrates two key modules. The pseudo-label (PL) module implements label augmentation by generating pseudo-labels for current phase samples, addressing the label absence problem. The weighted analytic classifier (WAC) derives a closed-form solution for neural networks. It introduces sample-specific weights to adaptively balance the class contribution and mitigate class imbalance. Experiments on MS-COCO and PASCAL VOC datasets demonstrate that L3A outperforms existing methods in MLCIL tasks. Our code is available at https://github.com/scut-zx/L3A.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DriveMind: A Dual-VLM based Reinforcement Learning Framework for Autonomous Driving</title>
<link>https://arxiv.org/abs/2506.00819</link>
<guid>https://arxiv.org/abs/2506.00819</guid>
<content:encoded><![CDATA[
arXiv:2506.00819v1 Announce Type: cross 
Abstract: End-to-end autonomous driving systems map sensor data directly to control commands, but remain opaque, lack interpretability, and offer no formal safety guarantees. While recent vision-language-guided reinforcement learning (RL) methods introduce semantic feedback, they often rely on static prompts and fixed objectives, limiting adaptability to dynamic driving scenes. We present DriveMind, a unified semantic reward framework that integrates: (i) a contrastive Vision-Language Model (VLM) encoder for stepwise semantic anchoring; (ii) a novelty-triggered VLM encoder-decoder, fine-tuned via chain-of-thought (CoT) distillation, for dynamic prompt generation upon semantic drift; (iii) a hierarchical safety module enforcing kinematic constraints (e.g., speed, lane centering, stability); and (iv) a compact predictive world model to reward alignment with anticipated ideal states. DriveMind achieves 19.4 +/- 2.3 km/h average speed, 0.98 +/- 0.03 route completion, and near-zero collisions in CARLA Town 2, outperforming baselines by over 4% in success rate. Its semantic reward generalizes zero-shot to real dash-cam data with minimal distributional shift, demonstrating robust cross-domain alignment and potential for real-world deployment.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeGenes: Evaluating the Adversarial Robustness of Genomic Foundation Models</title>
<link>https://arxiv.org/abs/2506.00821</link>
<guid>https://arxiv.org/abs/2506.00821</guid>
<content:encoded><![CDATA[
arXiv:2506.00821v1 Announce Type: cross 
Abstract: Genomic Foundation Models (GFMs), such as Evolutionary Scale Modeling (ESM), have demonstrated significant success in variant effect prediction. However, their adversarial robustness remains largely unexplored. To address this gap, we propose SafeGenes: a framework for Secure analysis of genomic foundation models, leveraging adversarial attacks to evaluate robustness against both engineered near-identical adversarial Genes and embedding-space manipulations. In this study, we assess the adversarial vulnerabilities of GFMs using two approaches: the Fast Gradient Sign Method (FGSM) and a soft prompt attack. FGSM introduces minimal perturbations to input sequences, while the soft prompt attack optimizes continuous embeddings to manipulate model predictions without modifying the input tokens. By combining these techniques, SafeGenes provides a comprehensive assessment of GFM susceptibility to adversarial manipulation. Targeted soft prompt attacks led to substantial performance degradation, even in large models such as ESM1b and ESM1v. These findings expose critical vulnerabilities in current foundation models, opening new research directions toward improving their security and robustness in high-stakes genomic applications such as variant effect prediction.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HERGC: Heterogeneous Experts Representation and Generative Completion for Multimodal Knowledge Graphs</title>
<link>https://arxiv.org/abs/2506.00826</link>
<guid>https://arxiv.org/abs/2506.00826</guid>
<content:encoded><![CDATA[
arXiv:2506.00826v1 Announce Type: cross 
Abstract: Multimodal knowledge graphs (MMKGs) enrich traditional knowledge graphs (KGs) by incorporating diverse modalities such as images and text. Multi-modal knowledge graph completion (MMKGC) seeks to exploit these heterogeneous signals to infer missing facts, thereby mitigating the intrinsic incompleteness of MMKGs. Existing MMKGC methods typically leverage only the information contained in the MMKGs under the closed-world assumption and adopt discriminative training objectives, which limits their reasoning capacity during completion. Recent generative completion approaches powered by advanced large language models (LLMs) have shown strong reasoning abilities in unimodal knowledge graph completion, but their potential in MMKGC remains largely unexplored. To bridge this gap, we propose HERGC, a Heterogeneous Experts Representation and Generative Completion framework for MMKGs. HERGC first deploys a Heterogeneous Experts Representation Retriever that enriches and fuses multimodal information and retrieves a compact candidate set for each incomplete triple. It then uses a Generative LLM Predictor fine-tuned on minimal instruction data to accurately identify the correct answer from these candidates. Extensive experiments on three standard MMKG benchmarks demonstrate HERGC's effectiveness and robustness, achieving state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COMPKE: Complex Question Answering under Knowledge Editing</title>
<link>https://arxiv.org/abs/2506.00829</link>
<guid>https://arxiv.org/abs/2506.00829</guid>
<content:encoded><![CDATA[
arXiv:2506.00829v1 Announce Type: cross 
Abstract: Knowledge Editing, which efficiently modifies the knowledge in large language models, has gathered great attention. Current benchmarks primarily use multi-hop question answering to assess and analyze newly injected or updated knowledge. However, we argue that these benchmarks fail to effectively evaluate how well the updated models apply this knowledge in real-life scenarios, particularly when questions require complex reasoning, involving one-to-many relationships or multi-step logical intersections. To fill in this gap, we introduce a new benchmark, COMPKE: Complex Question Answering under Knowledge Editing, which includes 11,924 complex questions that reflect real-life situations. We conduct an extensive evaluation of four knowledge editing methods on COMPKE, revealing that their effectiveness varies notably across different models. For instance, MeLLo attains an accuracy of 39.47 on GPT-4O-MINI, but this drops sharply to 3.83 on QWEN2.5-3B. We further investigate the underlying causes of these disparities from both methodological and model-specific perspectives. The datasets are available at https://github.com/kzjkzj666/CompKE.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Large Language Model-Supported Threat Modeling Framework for Transportation Cyber-Physical Systems</title>
<link>https://arxiv.org/abs/2506.00831</link>
<guid>https://arxiv.org/abs/2506.00831</guid>
<content:encoded><![CDATA[
arXiv:2506.00831v1 Announce Type: cross 
Abstract: Modern transportation systems rely on cyber-physical systems (CPS), where cyber systems interact seamlessly with physical systems like transportation-related sensors and actuators to enhance safety, mobility, and energy efficiency. However, growing automation and connectivity increase exposure to cyber vulnerabilities. Existing threat modeling frameworks for transportation CPS are often limited in scope, resource-intensive, and dependent on significant cybersecurity expertise. To address these gaps, we present TraCR-TMF (Transportation Cybersecurity and Resiliency Threat Modeling Framework), a large language model (LLM)-based framework that minimizes expert intervention. TraCR-TMF identifies threats, potential attack techniques, and corresponding countermeasures by leveraging the MITRE ATT&amp;CK matrix through three LLM-based approaches: (i) a retrieval-augmented generation (RAG) method requiring no expert input, (ii) an in-context learning approach requiring low expert input, and (iii) a supervised fine-tuning method requiring moderate expert input. TraCR-TMF also maps attack paths to critical assets by analyzing vulnerabilities using a customized LLM. The framework was evaluated in two scenarios. First, it identified relevant attack techniques across transportation CPS applications, with 90% precision as validated by experts. Second, using a fine-tuned LLM, it successfully predicted multiple exploitations including lateral movement, data exfiltration, and ransomware-related encryption that occurred during a major real-world cyberattack incident. These results demonstrate TraCR-TMF's effectiveness in CPS threat modeling, its reduced reliance on cybersecurity expertise, and its adaptability across CPS domains.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Counterfactual Activation Editing for Post-hoc Prosody and Mispronunciation Correction in TTS Models</title>
<link>https://arxiv.org/abs/2506.00832</link>
<guid>https://arxiv.org/abs/2506.00832</guid>
<content:encoded><![CDATA[
arXiv:2506.00832v1 Announce Type: cross 
Abstract: Recent advances in Text-to-Speech (TTS) have significantly improved speech naturalness, increasing the demand for precise prosody control and mispronunciation correction. Existing approaches for prosody manipulation often depend on specialized modules or additional training, limiting their capacity for post-hoc adjustments. Similarly, traditional mispronunciation correction relies on grapheme-to-phoneme dictionaries, making it less practical in low-resource settings. We introduce Counterfactual Activation Editing, a model-agnostic method that manipulates internal representations in a pre-trained TTS model to achieve post-hoc control of prosody and pronunciation. Experimental results show that our method effectively adjusts prosodic features and corrects mispronunciations while preserving synthesis quality. This opens the door to inference-time refinement of TTS outputs without retraining, bridging the gap between pre-trained TTS models and editable speech synthesis.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Structured Knowledge Reasoning: Contrastive Retrieval-Augmented Generation on Experience</title>
<link>https://arxiv.org/abs/2506.00842</link>
<guid>https://arxiv.org/abs/2506.00842</guid>
<content:encoded><![CDATA[
arXiv:2506.00842v1 Announce Type: cross 
Abstract: Large language models (LLMs) achieve strong performance on plain text tasks but underperform on structured data like tables and databases. Potential challenges arise from their underexposure during pre-training and rigid text-to-structure transfer mechanisms. Unlike humans who seamlessly apply learned patterns across data modalities, LLMs struggle to infer implicit relationships embedded in tabular formats, especially in the absence of explicit structural guidance. To bridge this cognitive gap, we introduce Contrastive Retrieval-Augmented Generation on Experience (CoRE), a framework that builds experience memory representations and enhances generalization through contrastive In-Context Learning (ICL) to simulate human-like knowledge transfer. Experiments on Text-to-SQL and TableQA show CoRE significantly improves performance, achieving average gains of 3.44% and 4.24%, with up to 17.2% on challenging tasks. Our Monte Carlo Tree Search (MCTS)-generated Experience Memory expands training data 8-9x, enhancing diversity and domain coverage. This training-free and continual method propels LLMs toward structured knowledge expertise.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speech Unlearning</title>
<link>https://arxiv.org/abs/2506.00848</link>
<guid>https://arxiv.org/abs/2506.00848</guid>
<content:encoded><![CDATA[
arXiv:2506.00848v1 Announce Type: cross 
Abstract: We introduce machine unlearning for speech tasks, a novel and underexplored research problem that aims to efficiently and effectively remove the influence of specific data from trained speech models without full retraining. This has important applications in privacy preservation, removal of outdated or noisy data, and bias mitigation. While machine unlearning has been studied in computer vision and natural language processing, its application to speech is largely unexplored due to the high-dimensional, sequential, and speaker-dependent nature of speech data. We define two fundamental speech unlearning tasks: sample unlearning, which removes individual data points (e.g., a voice recording), and class unlearning, which removes an entire category (e.g., all data from a speaker), while preserving performance on the remaining data. Experiments on keyword spotting and speaker identification demonstrate that unlearning speech data is significantly more challenging than unlearning image or text data. We conclude with key future directions in this area, including structured training, robust evaluation, feature-level unlearning, broader applications, scalable methods, and adversarial robustness.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalization in VAE and Diffusion Models: A Unified Information-Theoretic Analysis</title>
<link>https://arxiv.org/abs/2506.00849</link>
<guid>https://arxiv.org/abs/2506.00849</guid>
<content:encoded><![CDATA[
arXiv:2506.00849v1 Announce Type: cross 
Abstract: Despite the empirical success of Diffusion Models (DMs) and Variational Autoencoders (VAEs), their generalization performance remains theoretically underexplored, especially lacking a full consideration of the shared encoder-generator structure. Leveraging recent information-theoretic tools, we propose a unified theoretical framework that provides guarantees for the generalization of both the encoder and generator by treating them as randomized mappings. This framework further enables (1) a refined analysis for VAEs, accounting for the generator's generalization, which was previously overlooked; (2) illustrating an explicit trade-off in generalization terms for DMs that depends on the diffusion time $T$; and (3) providing computable bounds for DMs based solely on the training data, allowing the selection of the optimal $T$ and the integration of such bounds into the optimization process to improve model performance. Empirical results on both synthetic and real datasets illustrate the validity of the proposed theory.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG Alignment via Large Language Model and Contrastive Learning on ChineseEEG</title>
<link>https://arxiv.org/abs/2506.00854</link>
<guid>https://arxiv.org/abs/2506.00854</guid>
<content:encoded><![CDATA[
arXiv:2506.00854v1 Announce Type: cross 
Abstract: We propose EEG2TEXT-CN, which, to the best of our knowledge, represents one of the earliest open-vocabulary EEG-to-text generation frameworks tailored for Chinese. Built on a biologically grounded EEG encoder (NICE-EEG) and a compact pretrained language model (MiniLM), our architecture aligns multichannel brain signals with natural language representations via masked pretraining and contrastive learning. Using a subset of the ChineseEEG dataset, where each sentence contains approximately ten Chinese characters aligned with 128-channel EEG recorded at 256 Hz, we segment EEG into per-character embeddings and predict full sentences in a zero-shot setting. The decoder is trained with teacher forcing and padding masks to accommodate variable-length sequences. Evaluation on over 1,500 training-validation sentences and 300 held-out test samples shows promising lexical alignment, with a best BLEU-1 score of 6.38\%. While syntactic fluency remains a challenge, our findings demonstrate the feasibility of non-phonetic, cross-modal language decoding from EEG. This work opens a new direction in multilingual brain-to-text research and lays the foundation for future cognitive-language interfaces in Chinese.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can AI Master Econometrics? Evidence from Econometrics AI Agent on Expert-Level Tasks</title>
<link>https://arxiv.org/abs/2506.00856</link>
<guid>https://arxiv.org/abs/2506.00856</guid>
<content:encoded><![CDATA[
arXiv:2506.00856v1 Announce Type: cross 
Abstract: Can AI effectively perform complex econometric analysis traditionally requiring human expertise? This paper evaluates an agentic AI's capability to master econometrics, focusing on empirical analysis performance. We develop an ``Econometrics AI Agent'' built on the open-source MetaGPT framework. This agent exhibits outstanding performance in: (1) planning econometric tasks strategically, (2) generating and executing code, (3) employing error-based reflection for improved robustness, and (4) allowing iterative refinement through multi-round conversations. We construct two datasets from academic coursework materials and published research papers to evaluate performance against real-world challenges. Comparative testing shows our domain-specialized agent significantly outperforms both benchmark large language models (LLMs) and general-purpose AI agents. This work establishes a testbed for exploring AI's impact on social science research and enables cost-effective integration of domain expertise, making advanced econometric methods accessible to users with minimal coding expertise. Furthermore, our agent enhances research reproducibility and offers promising pedagogical applications for econometrics teaching.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local Manifold Approximation and Projection for Manifold-Aware Diffusion Planning</title>
<link>https://arxiv.org/abs/2506.00867</link>
<guid>https://arxiv.org/abs/2506.00867</guid>
<content:encoded><![CDATA[
arXiv:2506.00867v1 Announce Type: cross 
Abstract: Recent advances in diffusion-based generative modeling have demonstrated significant promise in tackling long-horizon, sparse-reward tasks by leveraging offline datasets. While these approaches have achieved promising results, their reliability remains inconsistent due to the inherent stochastic risk of producing infeasible trajectories, limiting their applicability in safety-critical applications. We identify that the primary cause of these failures is inaccurate guidance during the sampling procedure, and demonstrate the existence of manifold deviation by deriving a lower bound on the guidance gap. To address this challenge, we propose Local Manifold Approximation and Projection (LoMAP), a training-free method that projects the guided sample onto a low-rank subspace approximated from offline datasets, preventing infeasible trajectory generation. We validate our approach on standard offline reinforcement learning benchmarks that involve challenging long-horizon planning. Furthermore, we show that, as a standalone module, LoMAP can be incorporated into the hierarchical diffusion planner, providing further performance enhancements.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Predicting Any Human Trajectory In Context</title>
<link>https://arxiv.org/abs/2506.00871</link>
<guid>https://arxiv.org/abs/2506.00871</guid>
<content:encoded><![CDATA[
arXiv:2506.00871v1 Announce Type: cross 
Abstract: Predicting accurate future trajectories of pedestrians is essential for autonomous systems but remains a challenging task due to the need for adaptability in different environments and domains. A common approach involves collecting scenario-specific data and performing fine-tuning via backpropagation. However, this process is often impractical on edge devices due to constrained computational resources. To address this challenge, we introduce TrajICL, an In-Context Learning (ICL) framework for pedestrian trajectory prediction that enables rapid adaptation without fine-tuning on the scenario-specific data. We propose a spatio-temporal similarity-based example selection (STES) method that selects relevant examples from previously observed trajectories within the same scene by identifying similar motion patterns at corresponding locations. To further refine this selection, we introduce prediction-guided example selection (PG-ES), which selects examples based on both the past trajectory and the predicted future trajectory, rather than relying solely on the past trajectory. This approach allows the model to account for long-term dynamics when selecting examples. Finally, instead of relying on small real-world datasets with limited scenario diversity, we train our model on a large-scale synthetic dataset to enhance its prediction ability by leveraging in-context examples. Extensive experiments demonstrate that TrajICL achieves remarkable adaptation across both in-domain and cross-domain scenarios, outperforming even fine-tuned approaches across multiple public benchmarks. The code will be released at https://fujiry0.github.io/TrajICL-project-page.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ModuLM: Enabling Modular and Multimodal Molecular Relational Learning with Large Language Models</title>
<link>https://arxiv.org/abs/2506.00880</link>
<guid>https://arxiv.org/abs/2506.00880</guid>
<content:encoded><![CDATA[
arXiv:2506.00880v1 Announce Type: cross 
Abstract: Molecular Relational Learning (MRL) aims to understand interactions between molecular pairs, playing a critical role in advancing biochemical research. With the recent development of large language models (LLMs), a growing number of studies have explored the integration of MRL with LLMs and achieved promising results. However, the increasing availability of diverse LLMs and molecular structure encoders has significantly expanded the model space, presenting major challenges for benchmarking. Currently, there is no LLM framework that supports both flexible molecular input formats and dynamic architectural switching. To address these challenges, reduce redundant coding, and ensure fair model comparison, we propose ModuLM, a framework designed to support flexible LLM-based model construction and diverse molecular representations. ModuLM provides a rich suite of modular components, including 8 types of 2D molecular graph encoders, 11 types of 3D molecular conformation encoders, 7 types of interaction layers, and 7 mainstream LLM backbones. Owing to its highly flexible model assembly mechanism, ModuLM enables the dynamic construction of over 50,000 distinct model configurations. In addition, we provide comprehensive results to demonstrate the effectiveness of ModuLM in supporting LLM-based MRL tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoVoMix2: Advancing Zero-Shot Dialogue Generation with Fully Non-Autoregressive Flow Matching</title>
<link>https://arxiv.org/abs/2506.00885</link>
<guid>https://arxiv.org/abs/2506.00885</guid>
<content:encoded><![CDATA[
arXiv:2506.00885v1 Announce Type: cross 
Abstract: Generating natural-sounding, multi-speaker dialogue is crucial for applications such as podcast creation, virtual agents, and multimedia content generation. However, existing systems struggle to maintain speaker consistency, model overlapping speech, and synthesize coherent conversations efficiently. In this paper, we introduce CoVoMix2, a fully non-autoregressive framework for zero-shot multi-talker dialogue generation. CoVoMix2 directly predicts mel-spectrograms from multi-stream transcriptions using a flow-matching-based generative model, eliminating the reliance on intermediate token representations. To better capture realistic conversational dynamics, we propose transcription-level speaker disentanglement, sentence-level alignment, and prompt-level random masking strategies. Our approach achieves state-of-the-art performance, outperforming strong baselines like MoonCast and Sesame in speech quality, speaker consistency, and inference speed. Notably, CoVoMix2 operates without requiring transcriptions for the prompt and supports controllable dialogue generation, including overlapping speech and precise timing control, demonstrating strong generalizability to real-world speech generation scenarios.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uneven Event Modeling for Partially Relevant Video Retrieval</title>
<link>https://arxiv.org/abs/2506.00891</link>
<guid>https://arxiv.org/abs/2506.00891</guid>
<content:encoded><![CDATA[
arXiv:2506.00891v1 Announce Type: cross 
Abstract: Given a text query, partially relevant video retrieval (PRVR) aims to retrieve untrimmed videos containing relevant moments, wherein event modeling is crucial for partitioning the video into smaller temporal events that partially correspond to the text. Previous methods typically segment videos into a fixed number of equal-length clips, resulting in ambiguous event boundaries. Additionally, they rely on mean pooling to compute event representations, inevitably introducing undesired misalignment. To address these, we propose an Uneven Event Modeling (UEM) framework for PRVR. We first introduce the Progressive-Grouped Video Segmentation (PGVS) module, to iteratively formulate events in light of both temporal dependencies and semantic similarity between consecutive frames, enabling clear event boundaries. Furthermore, we also propose the Context-Aware Event Refinement (CAER) module to refine the event representation conditioned the text's cross-attention. This enables event representations to focus on the most relevant frames for a given text, facilitating more precise text-video alignment. Extensive experiments demonstrate that our method achieves state-of-the-art performance on two PRVR benchmarks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Affordance Benchmark for MLLMs</title>
<link>https://arxiv.org/abs/2506.00893</link>
<guid>https://arxiv.org/abs/2506.00893</guid>
<content:encoded><![CDATA[
arXiv:2506.00893v1 Announce Type: cross 
Abstract: Affordance theory posits that environments inherently offer action possibilities that shape perception and behavior. While Multimodal Large Language Models (MLLMs) excel in vision-language tasks, their ability to perceive affordance, which is crucial for intuitive and safe interactions, remains underexplored. To address this, we introduce A4Bench, a novel benchmark designed to evaluate the affordance perception abilities of MLLMs across two dimensions: 1) Constitutive Affordance}, assessing understanding of inherent object properties through 1,282 question-answer pairs spanning nine sub-disciplines, and 2) Transformative Affordance, probing dynamic and contextual nuances (e.g., misleading, time-dependent, cultural, or individual-specific affordance) with 718 challenging question-answer pairs. Evaluating 17 MLLMs (nine proprietary and eight open-source) against human performance, we find that proprietary models generally outperform open-source counterparts, but all exhibit limited capabilities, particularly in transformative affordance perception. Furthermore, even top-performing models, such as Gemini-2.0-Pro (18.05% overall exact match accuracy), significantly lag behind human performance (best: 85.34%, worst: 81.25%). These findings highlight critical gaps in environmental understanding of MLLMs and provide a foundation for advancing AI systems toward more robust, context-aware interactions. The dataset is available in https://github.com/JunyingWang959/A4Bench/.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CODEMENV: Benchmarking Large Language Models on Code Migration</title>
<link>https://arxiv.org/abs/2506.00894</link>
<guid>https://arxiv.org/abs/2506.00894</guid>
<content:encoded><![CDATA[
arXiv:2506.00894v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown remarkable capabilities across various software engineering tasks; however, their effectiveness in code migration, adapting code to run in different environments, remains insufficiently studied. In this work, we introduce CODEMENV: Code Migration Across Environment, a new benchmark specifically designed to assess LLMs' abilities in code migration scenarios. CODEMENV consists of 922 examples spanning 19 Python and Java packages, and covers three core tasks: (1) identifying functions incompatible with specific versions, (2) detecting changes in function definitions, and (3) adapting code to target environments. Experimental evaluation with seven LLMs on CODEMENV yields an average pass@1 rate of 26.50%, with GPT-4O achieving the highest score at 43.84%. Key findings include: (i) LLMs tend to be more proficient with newer function versions, which aids in migrating legacy code, and (ii) LLMs sometimes exhibit logical inconsistencies by identifying function changes irrelevant to the intended migration environment. The datasets are available at https://github.com/xdshen-ai/Benchmark-of-Code-Migration.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>State-Covering Trajectory Stitching for Diffusion Planners</title>
<link>https://arxiv.org/abs/2506.00895</link>
<guid>https://arxiv.org/abs/2506.00895</guid>
<content:encoded><![CDATA[
arXiv:2506.00895v1 Announce Type: cross 
Abstract: Diffusion-based generative models are emerging as powerful tools for long-horizon planning in reinforcement learning (RL), particularly with offline datasets. However, their performance is fundamentally limited by the quality and diversity of training data. This often restricts their generalization to tasks outside their training distribution or longer planning horizons. To overcome this challenge, we propose State-Covering Trajectory Stitching (SCoTS), a novel reward-free trajectory augmentation method that incrementally stitches together short trajectory segments, systematically generating diverse and extended trajectories. SCoTS first learns a temporal distance-preserving latent representation that captures the underlying temporal structure of the environment, then iteratively stitches trajectory segments guided by directional exploration and novelty to effectively cover and expand this latent space. We demonstrate that SCoTS significantly improves the performance and generalization capabilities of diffusion planners on offline goal-conditioned benchmarks requiring stitching and long-horizon reasoning. Furthermore, augmented trajectories generated by SCoTS significantly improve the performance of widely used offline goal-conditioned RL algorithms across diverse environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PCoreSet: Effective Active Learning through Knowledge Distillation from Vision-Language Models</title>
<link>https://arxiv.org/abs/2506.00910</link>
<guid>https://arxiv.org/abs/2506.00910</guid>
<content:encoded><![CDATA[
arXiv:2506.00910v1 Announce Type: cross 
Abstract: Knowledge distillation (KD) is a widely used framework for training compact, task-specific models by leveraging the knowledge of teacher models. However, its application to active learning (AL), which aims to minimize annotation costs through iterative sample selection, remains underexplored. This gap stems from the fact that KD typically assumes access to sufficient labeled data, whereas AL operates in data-scarce scenarios where task-specific teacher models are often unavailable. In this paper, we introduce ActiveKD, a framework that integrates AL with KD by leveraging the zero- and few-shot capabilities of large vision-language models (VLMs). A key aspect of ActiveKD is the structured prediction bias of VLMs--i.e., their predictions form clusters in the probability space. We regard this structure as an inductive bias of the teacher model, capturing generalizable output patterns beneficial to student learning. To exploit this bias, we propose Probabilistic CoreSet (PCoreSet), a selection strategy that maximizes coverage in the probability space rather than the feature space. PCoreSet strategically selects categorically diverse unlabeled samples, facilitating more efficient transfer of teacher knowledge under limited annotation budgets. Evaluations on 11 datasets show that PCoreSet consistently outperforms existing selection methods within the ActiveKD framework, advancing research at the intersection of AL and KD.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pi-SQL: Enhancing Text-to-SQL with Fine-Grained Guidance from Pivot Programming Languages</title>
<link>https://arxiv.org/abs/2506.00912</link>
<guid>https://arxiv.org/abs/2506.00912</guid>
<content:encoded><![CDATA[
arXiv:2506.00912v1 Announce Type: cross 
Abstract: Text-to-SQL transforms the user queries from natural language to executable SQL programs, enabling non-experts to interact with complex databases. Existing prompt-based methods craft meticulous text guidelines and examples to facilitate SQL generation, but their accuracy is hindered by the large semantic gap between the texts and the low-resource SQL programs. In this work, we propose Pi-SQL, which incorporates the high-resource Python program as a pivot to bridge between the natural language query and SQL program. In particular, Pi-SQL first generates Python programs that provide fine-grained step-by-step guidelines in their code blocks or comments, and then produces an SQL program following the guidance of each Python program.The final SQL program matches the reference Python program's query results and, through selection from candidates generated by different strategies, achieves superior execution speed, with a reward-based valid efficiency score up to 4.55 higher than the best-performing baseline.Extensive experiments demonstrate the effectiveness of Pi-SQL, which improves the execution accuracy of the best-performing baseline by up to 3.20.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How do Transformer Embeddings Represent Compositions? A Functional Analysis</title>
<link>https://arxiv.org/abs/2506.00914</link>
<guid>https://arxiv.org/abs/2506.00914</guid>
<content:encoded><![CDATA[
arXiv:2506.00914v1 Announce Type: cross 
Abstract: Compositionality is a key aspect of human intelligence, essential for reasoning and generalization. While transformer-based models have become the de facto standard for many language modeling tasks, little is known about how they represent compound words, and whether these representations are compositional. In this study, we test compositionality in Mistral, OpenAI Large, and Google embedding models, and compare them with BERT. First, we evaluate compositionality in the representations by examining six diverse models of compositionality (addition, multiplication, dilation, regression, etc.). We find that ridge regression, albeit linear, best accounts for compositionality. Surprisingly, we find that the classic vector addition model performs almost as well as any other model. Next, we verify that most embedding models are highly compositional, while BERT shows much poorer compositionality. We verify and visualize our findings with a synthetic dataset consisting of fully transparent adjective-noun compositions. Overall, we present a thorough investigation of compositionality.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Principled Input-Output-Conditioned Post-Hoc Uncertainty Estimation for Regression Networks</title>
<link>https://arxiv.org/abs/2506.00918</link>
<guid>https://arxiv.org/abs/2506.00918</guid>
<content:encoded><![CDATA[
arXiv:2506.00918v1 Announce Type: cross 
Abstract: Uncertainty quantification is critical in safety-sensitive applications but is often omitted from off-the-shelf neural networks due to adverse effects on predictive performance. Retrofitting uncertainty estimates post-hoc typically requires access to model parameters or gradients, limiting feasibility in practice. We propose a theoretically grounded framework for post-hoc uncertainty estimation in regression tasks by fitting an auxiliary model to both original inputs and frozen model outputs. Drawing from principles of maximum likelihood estimation and sequential parameter fitting, we formalize an exact post-hoc optimization objective that recovers the canonical MLE of Gaussian parameters, without requiring sampling or approximation at inference. While prior work has used model outputs to estimate uncertainty, we explicitly characterize the conditions under which this is valid and demonstrate the extent to which structured outputs can support quasi-epistemic inference. We find that using diverse auxiliary data, such as augmented subsets of the original training data, significantly enhances OOD detection and metric performance. Our hypothesis that frozen model outputs contain generalizable latent information about model error and predictive uncertainty is tested and confirmed. Finally, we ensure that our method maintains proper estimation of input-dependent uncertainty without relying exclusively on base model forecasts. These findings are demonstrated in toy problems and adapted to both UCI and depth regression benchmarks. Code: https://github.com/biggzlar/IO-CUE.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position as Probability: Self-Supervised Transformers that Think Past Their Training for Length Extrapolation</title>
<link>https://arxiv.org/abs/2506.00920</link>
<guid>https://arxiv.org/abs/2506.00920</guid>
<content:encoded><![CDATA[
arXiv:2506.00920v1 Announce Type: cross 
Abstract: Deep sequence models typically degrade in accuracy when test sequences significantly exceed their training lengths, yet many critical tasks--such as algorithmic reasoning, multi-step arithmetic, and compositional generalization--require robust length extrapolation. We introduce PRISM, a Probabilistic Relative-position Implicit Superposition Model, a novel positional encoding mechanism that enables Transformers to extrapolate accurately up to 10x beyond their training length. PRISM learns continuous relative positions through a differentiable histogram-filter update, preserving position uncertainty via a probabilistic superposition rather than conventional deterministic embeddings. Empirically, PRISM achieves state-of-the-art length extrapolation, successfully generalizing to previously intractable sequence lengths across algorithmic benchmarks--including arithmetic (addition, multiplication), SCAN compositionality tasks, and complex copy variants derived from DeepMind's recent datasets. Our analysis demonstrates that PRISM's stochastic positional encoding maintains sharp and interpretable internal states, providing a theoretical basis for reliable length generalization. These results advance the goal of neural sequence models that remain algorithmically robust at lengths far exceeding their training horizon.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Subjective and Objective QoE: Operator-Level Aggregation Using LLM-Based Comment Analysis and Network MOS Comparison</title>
<link>https://arxiv.org/abs/2506.00924</link>
<guid>https://arxiv.org/abs/2506.00924</guid>
<content:encoded><![CDATA[
arXiv:2506.00924v1 Announce Type: cross 
Abstract: This paper introduces a dual-layer framework for network operator-side quality of experience (QoE) assessment that integrates both objective network modeling and subjective user perception extracted from live-streaming platforms. On the objective side, we develop a machine learning model trained on mean opinion scores (MOS) computed via the ITU-T P.1203 reference implementation, allowing accurate prediction of user-perceived video quality using only network parameters such as packet loss, delay, jitter, and throughput without reliance on video content or client-side instrumentation. On the subjective side, we present a semantic filtering and scoring pipeline that processes user comments from live streams to extract performance-related feedback. A large language model is used to assign scalar MOS scores to filtered comments in a deterministic and reproducible manner. To support scalable and interpretable analysis, we con- struct a labeled dataset of 47,894 live-stream comments, of which about 34,000 are identified as QoE-relevant through multi-layer semantic filtering. Each comment is enriched with simulated Internet Service Provider attribution and temporally aligned using synthetic timestamps in 5-min intervals. The resulting dataset enables operator-level aggregation and time-series analysis of user-perceived quality. A delta MOS metric is proposed to measure each Internet service provider's deviation from platform-wide sentiment, allowing detection of localized degradations even in the absence of direct network telemetry. A controlled outage simulation confirms the framework's effectiveness in identifying service disruptions through comment-based trends alone. The system provides each operator with its own subjective MOS and the global platform average per interval, enabling real-time interpretation of performance deviations and comparison with objective network-based QoE estimates.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-the-wild Audio Spatialization with Flexible Text-guided Localization</title>
<link>https://arxiv.org/abs/2506.00927</link>
<guid>https://arxiv.org/abs/2506.00927</guid>
<content:encoded><![CDATA[
arXiv:2506.00927v1 Announce Type: cross 
Abstract: To enhance immersive experiences, binaural audio offers spatial awareness of sounding objects in AR, VR, and embodied AI applications. While existing audio spatialization methods can generally map any available monaural audio to binaural audio signals, they often lack the flexible and interactive control needed in complex multi-object user-interactive environments. To address this, we propose a Text-guided Audio Spatialization (TAS) framework that utilizes flexible text prompts and evaluates our model from unified generation and comprehension perspectives. Due to the limited availability of premium and large-scale stereo data, we construct the SpatialTAS dataset, which encompasses 376,000 simulated binaural audio samples to facilitate the training of our model. Our model learns binaural differences guided by 3D spatial location and relative position prompts, augmented by flipped-channel audio. It outperforms existing methods on both simulated and real-recorded datasets, demonstrating superior generalization and accuracy. Besides, we develop an assessment model based on Llama-3.1-8B, which evaluates the spatial semantic coherence between our generated binaural audio and text prompts through a spatial reasoning task. Results demonstrate that text prompts provide flexible and interactive control to generate binaural audio with excellent quality and semantic consistency in spatial locations. Dataset is available at \href{https://github.com/Alice01010101/TASU}
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>General-purpose audio representation learning for real-world sound scenes</title>
<link>https://arxiv.org/abs/2506.00934</link>
<guid>https://arxiv.org/abs/2506.00934</guid>
<content:encoded><![CDATA[
arXiv:2506.00934v1 Announce Type: cross 
Abstract: While audio foundation models perform well on myriad of tasks from sound classification to speech analysis, these models are trained and tested on dry, non-spatial, single-source audio clips. This limits their success in real-world situations and results in spatially unaware audio embeddings. To address these limitations, we propose a novel self-supervised training approach for General-Purpose, Real-world Audio Models (GRAMs). The GRAM training approach enables robust spatial audio representation learning for naturalistic, noisy sound scenes and can be applied to any masking-based deep learning model. We demonstrate the success of our approach by training two state-of-the-art models, one with a transformer and one with a mamba backbone. We assess the quality of the extracted audio representations from GRAMs using the original version of the HEAR benchmark, a newly synthesized, naturalistic version of the HEAR benchmark, and novel sound localization tasks based on HEAR benchmark datasets. The results show that our approach minimizes the performance gap between dry, non-spatial, single-source sound scenes and naturalistic sound scenes for crucial tasks such as auditory scene analysis, outperforming existing state-of-the-art audio foundation models at a fraction of the training steps. Moreover, GRAMs show state-of-the-art performance on sound localization tasks, exceeding even supervised sound localization models. In sum, the proposed approach represents a significant advancement towards robust audio foundation models for real-world applications with state-of-the-art performance on naturalistic sound scenes as well as spatial audio representation learning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Aware Metabolic Stability Prediction with Dual-View Contrastive Learning</title>
<link>https://arxiv.org/abs/2506.00936</link>
<guid>https://arxiv.org/abs/2506.00936</guid>
<content:encoded><![CDATA[
arXiv:2506.00936v1 Announce Type: cross 
Abstract: Accurate prediction of molecular metabolic stability (MS) is critical for drug research and development but remains challenging due to the complex interplay of molecular interactions. Despite recent advances in graph neural networks (GNNs) for MS prediction, current approaches face two critical limitations: (1) incomplete molecular modeling due to atom-centric message-passing mechanisms that disregard bond-level topological features, and (2) prediction frameworks that lack reliable uncertainty quantification. To address these challenges, we propose TrustworthyMS, a novel contrastive learning framework designed for uncertainty-aware metabolic stability prediction. First, a molecular graph topology remapping mechanism synchronizes atom-bond interactions through edge-induced feature propagation, capturing both localized electronic effects and global conformational constraints. Second, contrastive topology-bond alignment enforces consistency between molecular topology views and bond patterns via feature alignment, enhancing representation robustness. Third, uncertainty modeling through Beta-Binomial uncertainty quantification enables simultaneous prediction and confidence calibration under epistemic uncertainty. Through extensive experiments, our results demonstrate that TrustworthyMS outperforms current state-of-the-art methods in terms of predictive performance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>anyECG-chat: A Generalist ECG-MLLM for Flexible ECG Input and Multi-Task Understanding</title>
<link>https://arxiv.org/abs/2506.00942</link>
<guid>https://arxiv.org/abs/2506.00942</guid>
<content:encoded><![CDATA[
arXiv:2506.00942v1 Announce Type: cross 
Abstract: The advent of multimodal large language models (MLLMs) has sparked interest in their application to electrocardiogram (ECG) analysis. However, existing ECG-focused MLLMs primarily focus on report generation tasks, often limited to single 12-lead, short-duration (10s) ECG inputs, thereby underutilizing the potential of MLLMs. To this end, we aim to develop a MLLM for ECG analysis that supports a broader range of tasks and more flexible ECG inputs. However, existing ECG-QA datasets are often monotonous. To address this gap, we first constructed the anyECG dataset, which encompasses a wide variety of tasks, including report generation, abnormal waveform localization, and open-ended question answering. In addition to standard hospital ECGs, we introduced long-duration reduced-lead ECGs for home environments and multiple ECG comparison scenarios commonly encountered in clinical practice. Furthermore, we propose the anyECG-chat model, which supports dynamic-length ECG inputs and multiple ECG inputs. We trained the model using a three-stage curriculum training recipe with the anyECG dataset. A comprehensive evaluation was conducted, demonstrating that anyECG-chat is capable of supporting various practical application scenarios, including not only common report generation tasks but also abnormal waveform localization for long-duration reduced-lead ECGs in home environments and comprehensive comparative analysis of multiple ECGs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Legal Compliance Evaluation of Smart Contracts Generated By Large Language Models</title>
<link>https://arxiv.org/abs/2506.00943</link>
<guid>https://arxiv.org/abs/2506.00943</guid>
<content:encoded><![CDATA[
arXiv:2506.00943v1 Announce Type: cross 
Abstract: Smart contracts can implement and automate parts of legal contracts, but ensuring their legal compliance remains challenging. Existing approaches such as formal specification, verification, and model-based development require expertise in both legal and software development domains, as well as extensive manual effort. Given the recent advances of Large Language Models (LLMs) in code generation, we investigate their ability to generate legally compliant smart contracts directly from natural language legal contracts, addressing these challenges. We propose a novel suite of metrics to quantify legal compliance based on modeling both legal and smart contracts as processes and comparing their behaviors. We select four LLMs, generate 20 smart contracts based on five legal contracts, and analyze their legal compliance. We find that while all LLMs generate syntactically correct code, there is significant variance in their legal compliance with larger models generally showing higher levels of compliance. We also evaluate the proposed metrics against properties of software metrics, showing they provide fine-grained distinctions, enable nuanced comparisons, and are applicable across domains for code from any source, LLM or developer. Our results suggest that LLMs can assist in generating starter code for legally compliant smart contracts with strict reviews, and the proposed metrics provide a foundation for automated and self-improving development workflows.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Heterogeneity Modeling for Trustworthy Machine Learning</title>
<link>https://arxiv.org/abs/2506.00969</link>
<guid>https://arxiv.org/abs/2506.00969</guid>
<content:encoded><![CDATA[
arXiv:2506.00969v1 Announce Type: cross 
Abstract: Data heterogeneity plays a pivotal role in determining the performance of machine learning (ML) systems. Traditional algorithms, which are typically designed to optimize average performance, often overlook the intrinsic diversity within datasets. This oversight can lead to a myriad of issues, including unreliable decision-making, inadequate generalization across different domains, unfair outcomes, and false scientific inferences. Hence, a nuanced approach to modeling data heterogeneity is essential for the development of dependable, data-driven systems. In this survey paper, we present a thorough exploration of heterogeneity-aware machine learning, a paradigm that systematically integrates considerations of data heterogeneity throughout the entire ML pipeline -- from data collection and model training to model evaluation and deployment. By applying this approach to a variety of critical fields, including healthcare, agriculture, finance, and recommendation systems, we demonstrate the substantial benefits and potential of heterogeneity-aware ML. These applications underscore how a deeper understanding of data diversity can enhance model robustness, fairness, and reliability and help model diagnosis and improvements. Moreover, we delve into future directions and provide research opportunities for the whole data mining community, aiming to promote the development of heterogeneity-aware ML.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NTPP: Generative Speech Language Modeling for Dual-Channel Spoken Dialogue via Next-Token-Pair Prediction</title>
<link>https://arxiv.org/abs/2506.00975</link>
<guid>https://arxiv.org/abs/2506.00975</guid>
<content:encoded><![CDATA[
arXiv:2506.00975v1 Announce Type: cross 
Abstract: Inspired by the impressive capabilities of GPT-4o, there is growing interest in enabling speech language models (SLMs) to engage in natural, fluid spoken interactions with humans. Recent advancements have led to the development of several SLMs that demonstrate promising results in this area. However, current approaches have yet to fully exploit dual-channel speech data, which inherently captures the structure and dynamics of human conversation. In this work, we systematically explore the use of dual-channel speech data in the context of modern large language models, and introduce a novel generative modeling paradigm, Next-Token-Pair Prediction (NTPP), to enable speaker-independent dual-channel spoken dialogue learning using decoder-only architectures for the first time. We evaluate our approach on standard benchmarks, and empirical results show that our proposed method, NTPP, significantly improves the conversational abilities of SLMs in terms of turn-taking prediction, response coherence, and naturalness. Moreover, compared to existing methods, NTPP achieves substantially lower inference latency, highlighting its practical efficiency for real-time applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IVY-FAKE: A Unified Explainable Framework and Benchmark for Image and Video AIGC Detection</title>
<link>https://arxiv.org/abs/2506.00979</link>
<guid>https://arxiv.org/abs/2506.00979</guid>
<content:encoded><![CDATA[
arXiv:2506.00979v1 Announce Type: cross 
Abstract: The rapid advancement of Artificial Intelligence Generated Content (AIGC) in visual domains has resulted in highly realistic synthetic images and videos, driven by sophisticated generative frameworks such as diffusion-based architectures. While these breakthroughs open substantial opportunities, they simultaneously raise critical concerns about content authenticity and integrity. Many current AIGC detection methods operate as black-box binary classifiers, which offer limited interpretability, and no approach supports detecting both images and videos in a unified framework. This dual limitation compromises model transparency, reduces trustworthiness, and hinders practical deployment. To address these challenges, we introduce IVY-FAKE , a novel, unified, and large-scale dataset specifically designed for explainable multimodal AIGC detection. Unlike prior benchmarks, which suffer from fragmented modality coverage and sparse annotations, IVY-FAKE contains over 150,000 richly annotated training samples (images and videos) and 18,700 evaluation examples, each accompanied by detailed natural-language reasoning beyond simple binary labels. Building on this, we propose Ivy Explainable Detector (IVY-XDETECTOR), a unified AIGC detection and explainable architecture that jointly performs explainable detection for both image and video content. Our unified vision-language model achieves state-of-the-art performance across multiple image and video detection benchmarks, highlighting the significant advancements enabled by our dataset and modeling framework. Our data is publicly available at https://huggingface.co/datasets/AI-Safeguard/Ivy-Fake.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What do self-supervised speech models know about Dutch? Analyzing advantages of language-specific pre-training</title>
<link>https://arxiv.org/abs/2506.00981</link>
<guid>https://arxiv.org/abs/2506.00981</guid>
<content:encoded><![CDATA[
arXiv:2506.00981v1 Announce Type: cross 
Abstract: How language-specific are speech representations learned by self-supervised models? Existing work has shown that a range of linguistic features can be successfully decoded from end-to-end models trained only on speech recordings. However, it's less clear to what extent pre-training on specific languages improves language-specific linguistic information. Here we test the encoding of Dutch phonetic and lexical information in internal representations of self-supervised Wav2Vec2 models. Pre-training exclusively on Dutch improves the representation of Dutch linguistic features as compared to pre-training on similar amounts of English or larger amounts of multilingual data. This language-specific advantage is well-detected by trained clustering or classification probes, and partially observable using zero-shot metrics. Furthermore, the language-specific benefit on linguistic feature encoding aligns with downstream performance on Automatic Speech Recognition.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Gap: From Ad-hoc to Proactive Search in Conversations</title>
<link>https://arxiv.org/abs/2506.00983</link>
<guid>https://arxiv.org/abs/2506.00983</guid>
<content:encoded><![CDATA[
arXiv:2506.00983v1 Announce Type: cross 
Abstract: Proactive search in conversations (PSC) aims to reduce user effort in formulating explicit queries by proactively retrieving useful relevant information given conversational context. Previous work in PSC either directly uses this context as input to off-the-shelf ad-hoc retrievers or further fine-tunes them on PSC data. However, ad-hoc retrievers are pre-trained on short and concise queries, while the PSC input is longer and noisier. This input mismatch between ad-hoc search and PSC limits retrieval quality. While fine-tuning on PSC data helps, its benefits remain constrained by this input gap. In this work, we propose Conv2Query, a novel conversation-to-query framework that adapts ad-hoc retrievers to PSC by bridging the input gap between ad-hoc search and PSC. Conv2Query maps conversational context into ad-hoc queries, which can either be used as input for off-the-shelf ad-hoc retrievers or for further fine-tuning on PSC data. Extensive experiments on two PSC datasets show that Conv2Query significantly improves ad-hoc retrievers' performance, both when used directly and after fine-tuning on PSC.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quotient Network - A Network Similar to ResNet but Learning Quotients</title>
<link>https://arxiv.org/abs/2506.00992</link>
<guid>https://arxiv.org/abs/2506.00992</guid>
<content:encoded><![CDATA[
arXiv:2506.00992v1 Announce Type: cross 
Abstract: The emergence of ResNet provides a powerful tool for training extremely deep networks. The core idea behind it is to change the learning goals of the network. It no longer learns new features from scratch but learns the difference between the target and existing features. However, the difference between the two kinds of features does not have an independent and clear meaning, and the amount of learning is based on the absolute rather than the relative difference, which is sensitive to the size of existing features. We propose a new network that perfectly solves these two problems while still having the advantages of ResNet. Specifically, it chooses to learn the quotient of the target features with the existing features, so we call it the quotient network. In order to enable this network to learn successfully and achieve higher performance, we propose some design rules for this network so that it can be trained efficiently and achieve better performance than ResNet. Experiments on the CIFAR10, CIFAR100, and SVHN datasets prove that this network can stably achieve considerable improvements over ResNet by simply making tiny corresponding changes to the original ResNet network without adding new parameters.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Motion-Aware Concept Alignment for Consistent Video Editing</title>
<link>https://arxiv.org/abs/2506.01004</link>
<guid>https://arxiv.org/abs/2506.01004</guid>
<content:encoded><![CDATA[
arXiv:2506.01004v1 Announce Type: cross 
Abstract: We introduce MoCA-Video (Motion-Aware Concept Alignment in Video), a training-free framework bridging the gap between image-domain semantic mixing and video. Given a generated video and a user-provided reference image, MoCA-Video injects the semantic features of the reference image into a specific object within the video, while preserving the original motion and visual context. Our approach leverages a diagonal denoising schedule and class-agnostic segmentation to detect and track objects in the latent space and precisely control the spatial location of the blended objects. To ensure temporal coherence, we incorporate momentum-based semantic corrections and gamma residual noise stabilization for smooth frame transitions. We evaluate MoCA's performance using the standard SSIM, image-level LPIPS, temporal LPIPS, and introduce a novel metric CASS (Conceptual Alignment Shift Score) to evaluate the consistency and effectiveness of the visual shifts between the source prompt and the modified video frames. Using self-constructed dataset, MoCA-Video outperforms current baselines, achieving superior spatial consistency, coherent motion, and a significantly higher CASS score, despite having no training or fine-tuning. MoCA-Video demonstrates that structured manipulation in the diffusion noise trajectory allows for controllable, high-quality video synthesis.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Two-Stage Hierarchical Deep Filtering Framework for Real-Time Speech Enhancement</title>
<link>https://arxiv.org/abs/2506.01023</link>
<guid>https://arxiv.org/abs/2506.01023</guid>
<content:encoded><![CDATA[
arXiv:2506.01023v1 Announce Type: cross 
Abstract: This paper proposes a model that integrates sub-band processing and deep filtering to fully exploit information from the target time-frequency (TF) bin and its surrounding TF bins for single-channel speech enhancement. The sub-band module captures surrounding frequency bin information at the input, while the deep filtering module applies filtering at the output to both the target TF bin and its surrounding TF bins. To further improve the model performance, we decouple deep filtering into temporal and frequency components and introduce a two-stage framework, reducing the complexity of filter coefficient prediction at each stage. Additionally, we propose the TAConv module to strengthen convolutional feature extraction. Experimental results demonstrate that the proposed hierarchical deep filtering network (HDF-Net) effectively utilizes surrounding TF bin information and outperforms other advanced systems while using fewer resources.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Less is More: Local Intrinsic Dimensions of Contextual Language Models</title>
<link>https://arxiv.org/abs/2506.01034</link>
<guid>https://arxiv.org/abs/2506.01034</guid>
<content:encoded><![CDATA[
arXiv:2506.01034v1 Announce Type: cross 
Abstract: Understanding the internal mechanisms of large language models (LLMs) remains a challenging and complex endeavor. Even fundamental questions, such as how fine-tuning affects model behavior, often require extensive empirical evaluation. In this paper, we introduce a novel perspective based on the geometric properties of contextual latent embeddings to study the effects of training and fine-tuning. To that end, we measure the local dimensions of a contextual language model's latent space and analyze their shifts during training and fine-tuning. We show that the local dimensions provide insights into the model's training dynamics and generalization ability. Specifically, the mean of the local dimensions predicts when the model's training capabilities are exhausted, as exemplified in a dialogue state tracking task, overfitting, as demonstrated in an emotion recognition task, and grokking, as illustrated with an arithmetic task. Furthermore, our experiments suggest a practical heuristic: reductions in the mean local dimension tend to accompany and predict subsequent performance gains. Through this exploration, we aim to provide practitioners with a deeper understanding of the implications of fine-tuning on embedding spaces, facilitating informed decisions when configuring models for specific applications. The results of this work contribute to the ongoing discourse on the interpretability, adaptability, and generalizability of LLMs by bridging the gap between intrinsic model mechanisms and geometric properties in the respective embeddings.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing Neural Topology of Large Language Models</title>
<link>https://arxiv.org/abs/2506.01042</link>
<guid>https://arxiv.org/abs/2506.01042</guid>
<content:encoded><![CDATA[
arXiv:2506.01042v1 Announce Type: cross 
Abstract: Probing large language models (LLMs) has yielded valuable insights into their internal mechanisms by linking neural representations to interpretable semantics. However, how neurons functionally co-activate with each other to give rise to emergent capabilities remains largely unknown, hindering a deeper understanding and safer development of LLMs. In this work, we introduce graph probing, a method for uncovering the functional connectivity topology of LLM neurons and relating it to language generation performance. By analyzing internal neural graphs across diverse LLM families and scales, we discover a universal predictability of next-token prediction performance using only neural topology. This predictability is robust even when retaining just 1% of neuron connections or probing models after only 8 pretraining steps, highlighting the sparsity and early emergence of topological patterns. Further graph matching analysis suggests that, despite significant distinctions in architectures, parameters, and training data, different LLMs develop intricate and consistent neural topological structures that may form the foundation for their language generation abilities. Codes and data for the graph probing toolbox are released at https://github.com/DavyMorgan/llm-graph-probing.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taming LLMs by Scaling Learning Rates with Gradient Grouping</title>
<link>https://arxiv.org/abs/2506.01049</link>
<guid>https://arxiv.org/abs/2506.01049</guid>
<content:encoded><![CDATA[
arXiv:2506.01049v1 Announce Type: cross 
Abstract: Training large language models (LLMs) poses challenges due to their massive scale and heterogeneous architectures. While adaptive optimizers like AdamW help address gradient variations, they still struggle with efficient and effective parameter-wise learning rate estimation, resulting in training instability, slow convergence, and poor compatibility with parameter-efficient fine-tuning (PEFT) techniques. This work introduces Scaling with Gradient Grouping (SGG), an optimizer wrapper that improves adaptive learning rate estimation by dynamic grouping and group-specific scaling. SGG first groups gradient statistics in each layer into clusters and then applies cluster-specific scaling to calibrate learning rates for each parameter, thus imposing collective group-wise constraints while maintaining precise per-parameter adaptation. Experiments on diverse (M)LLM benchmarks show that SGG integrates seamlessly with existing optimizers, and offers consistent gains and faster convergence over baselines, with various model sizes. Its stability across varying batch sizes and learning rates establishes SGG as a robust choice for LLM optimization.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XAI-Units: Benchmarking Explainability Methods with Unit Tests</title>
<link>https://arxiv.org/abs/2506.01059</link>
<guid>https://arxiv.org/abs/2506.01059</guid>
<content:encoded><![CDATA[
arXiv:2506.01059v1 Announce Type: cross 
Abstract: Feature attribution (FA) methods are widely used in explainable AI (XAI) to help users understand how the inputs of a machine learning model contribute to its outputs. However, different FA models often provide disagreeing importance scores for the same model. In the absence of ground truth or in-depth knowledge about the inner workings of the model, it is often difficult to meaningfully determine which of the different FA methods produce more suitable explanations in different contexts. As a step towards addressing this issue, we introduce the open-source XAI-Units benchmark, specifically designed to evaluate FA methods against diverse types of model behaviours, such as feature interactions, cancellations, and discontinuous outputs. Our benchmark provides a set of paired datasets and models with known internal mechanisms, establishing clear expectations for desirable attribution scores. Accompanied by a suite of built-in evaluation metrics, XAI-Units streamlines systematic experimentation and reveals how FA methods perform against distinct, atomic kinds of model reasoning, similar to unit tests in software engineering. Crucially, by using procedurally generated models tied to synthetic datasets, we pave the way towards an objective and reliable comparison of FA methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SealQA: Raising the Bar for Reasoning in Search-Augmented Language Models</title>
<link>https://arxiv.org/abs/2506.01062</link>
<guid>https://arxiv.org/abs/2506.01062</guid>
<content:encoded><![CDATA[
arXiv:2506.01062v1 Announce Type: cross 
Abstract: We introduce SealQA, a new challenge benchmark for evaluating SEarch-Augmented Language models on fact-seeking questions where web search yields conflicting, noisy, or unhelpful results. SealQA comes in three flavors: (1) Seal-0 (main) and (2) Seal-Hard, which assess factual accuracy and reasoning capabilities, with Seal-0 focusing on the most challenging questions where chat models (e.g., GPT-4.1) typically achieve near-zero accuracy; and (3) LongSeal, which extends SealQA to test long-context, multi-document reasoning in "needle-in-a-haystack" settings. Our evaluation reveals critical limitations in current models: Even frontier LLMs perform poorly across all SealQA flavors. On Seal-0, frontier agentic models equipped with tools like o3 and o4-mini achieve only 17.1% and 6.3% accuracy, respectively, at their best reasoning efforts. We find that advanced reasoning models such as DeepSeek-R1-671B and o3-mini are highly vulnerable to noisy search results. Notably, increasing test-time compute does not yield reliable gains across o3-mini, o4-mini, and o3, with performance often plateauing or even declining early. Additionally, while recent models are less affected by the "lost-in-the-middle" issue, they still fail to reliably identify relevant documents in LongSeal when faced with numerous distractors. To facilitate future work, we release SealQA at huggingface.co/datasets/vtllms/sealqa.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fighting Fire with Fire (F3): A Training-free and Efficient Visual Adversarial Example Purification Method in LVLMs</title>
<link>https://arxiv.org/abs/2506.01064</link>
<guid>https://arxiv.org/abs/2506.01064</guid>
<content:encoded><![CDATA[
arXiv:2506.01064v1 Announce Type: cross 
Abstract: Recent advances in large vision-language models (LVLMs) have showcased their remarkable capabilities across a wide range of multimodal vision-language tasks. However, these models remain vulnerable to visual adversarial attacks, which can substantially compromise their performance. Despite their potential impact, the development of effective methods for purifying such adversarial examples has received relatively limited attention. In this paper, we introduce F3, a novel adversarial purification framework that employs a counterintuitive "fighting fire with fire" strategy: intentionally introducing simple perturbations to adversarial examples to mitigate their harmful effects. Specifically, F3 leverages cross-modal attentions derived from randomly perturbed adversary examples as reference targets. By injecting noise into these adversarial examples, F3 effectively refines their attention, resulting in cleaner and more reliable model outputs. Remarkably, this seemingly paradoxical approach of employing noise to counteract adversarial attacks yields impressive purification results. Furthermore, F3 offers several distinct advantages: it is training-free and straightforward to implement, and exhibits significant computational efficiency improvements compared to existing purification methods. These attributes render F3 particularly suitable for large-scale industrial applications where both robust performance and operational efficiency are critical priorities. The code will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trilevel Memetic Algorithm for the Electric Vehicle Routing Problem</title>
<link>https://arxiv.org/abs/2506.01065</link>
<guid>https://arxiv.org/abs/2506.01065</guid>
<content:encoded><![CDATA[
arXiv:2506.01065v1 Announce Type: cross 
Abstract: The Electric Vehicle Routing Problem (EVRP) extends the capacitated vehicle routing problem by incorporating battery constraints and charging stations, posing significant optimization challenges. This paper introduces a Trilevel Memetic Algorithm (TMA) that hierarchically optimizes customer sequences, route assignments, and charging station insertions. The method combines genetic algorithms with dynamic programming, ensuring efficient and high-quality solutions. Benchmark tests on WCCI2020 instances show competitive performance, matching best-known results for small-scale cases. While computational demands limit scalability, TMA demonstrates strong potential for sustainable logistics planning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revolutionizing Blood Banks: AI-Driven Fingerprint-Blood Group Correlation for Enhanced Safety</title>
<link>https://arxiv.org/abs/2506.01069</link>
<guid>https://arxiv.org/abs/2506.01069</guid>
<content:encoded><![CDATA[
arXiv:2506.01069v1 Announce Type: cross 
Abstract: Identification of a person is central in forensic science, security, and healthcare. Methods such as iris scanning and genomic profiling are more accurate but expensive, time-consuming, and more difficult to implement. This study focuses on the relationship between the fingerprint patterns and the ABO blood group as a biometric identification tool. A total of 200 subjects were included in the study, and fingerprint types (loops, whorls, and arches) and blood groups were compared. Associations were evaluated with statistical tests, including chi-square and Pearson correlation. The study found that the loops were the most common fingerprint pattern and the O+ blood group was the most prevalent. Even though there was some associative pattern, there was no statistically significant difference in the fingerprint patterns of different blood groups. Overall, the results indicate that blood group data do not significantly improve personal identification when used in conjunction with fingerprinting. Although the study shows weak correlation, it may emphasize the efforts of multi-modal based biometric systems in enhancing the current biometric systems. Future studies may focus on larger and more diverse samples, and possibly machine learning and additional biometrics to improve identification methods. This study addresses an element of the ever-changing nature of the fields of forensic science and biometric identification, highlighting the importance of resilient analytical methods for personal identification.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GThinker: Towards General Multimodal Reasoning via Cue-Guided Rethinking</title>
<link>https://arxiv.org/abs/2506.01078</link>
<guid>https://arxiv.org/abs/2506.01078</guid>
<content:encoded><![CDATA[
arXiv:2506.01078v1 Announce Type: cross 
Abstract: Despite notable advancements in multimodal reasoning, leading Multimodal Large Language Models (MLLMs) still underperform on vision-centric multimodal reasoning tasks in general scenarios. This shortfall stems from their predominant reliance on logic- and knowledge-based slow thinking strategies, while effective for domains like math and science, fail to integrate visual information effectively during reasoning. Consequently, these models often fail to adequately ground visual cues, resulting in suboptimal performance in tasks that require multiple plausible visual interpretations and inferences. To address this, we present GThinker (General Thinker), a novel reasoning MLLM excelling in multimodal reasoning across general scenarios, mathematics, and science. GThinker introduces Cue-Rethinking, a flexible reasoning pattern that grounds inferences in visual cues and iteratively reinterprets these cues to resolve inconsistencies. Building on this pattern, we further propose a two-stage training pipeline, including pattern-guided cold start and incentive reinforcement learning, designed to enable multimodal reasoning capabilities across domains. Furthermore, to support the training, we construct GThinker-11K, comprising 7K high-quality, iteratively-annotated reasoning paths and 4K curated reinforcement learning samples, filling the data gap toward general multimodal reasoning. Extensive experiments demonstrate that GThinker achieves 81.5% on the challenging comprehensive multimodal reasoning benchmark M$^3$CoT, surpassing the latest O4-mini model. It also shows an average improvement of 2.1% on general scenario multimodal reasoning benchmarks, while maintaining on-par performance in mathematical reasoning compared to counterpart advanced reasoning models. The code, model, and data will be released soon at https://github.com/jefferyZhan/GThinker.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unfolding Boxes with Local Constraints</title>
<link>https://arxiv.org/abs/2506.01079</link>
<guid>https://arxiv.org/abs/2506.01079</guid>
<content:encoded><![CDATA[
arXiv:2506.01079v1 Announce Type: cross 
Abstract: We consider the problem of finding and enumerating polyominos that can be folded into multiple non-isomorphic boxes. While several computational approaches have been proposed, including SAT, randomized algorithms, and decision diagrams, none has been able to perform at scale. We argue that existing SAT encodings are hindered by the presence of global constraints (e.g., graph connectivity or acyclicity), which are generally hard to encode effectively and hard for solvers to reason about. In this work, we propose a new SAT-based approach that replaces these global constraints with simple local constraints that have substantially better propagation properties. Our approach dramatically improves the scalability of both computing and enumerating common box unfoldings: (i) while previous approaches could only find common unfoldings of two boxes up to area 88, ours easily scales beyond 150, and (ii) while previous approaches were only able to enumerate common unfoldings up to area 30, ours scales up to 60. This allows us to rule out 46, 54, and 58 as the smallest areas allowing a common unfolding of three boxes, thereby refuting a conjecture of Xu et al. (2017).
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning What Matters: Prioritized Concept Learning via Relative Error-driven Sample Selection</title>
<link>https://arxiv.org/abs/2506.01085</link>
<guid>https://arxiv.org/abs/2506.01085</guid>
<content:encoded><![CDATA[
arXiv:2506.01085v1 Announce Type: cross 
Abstract: Instruction tuning has been central to the success of recent vision-language models (VLMs), but it remains expensive-requiring large-scale datasets, high-quality annotations, and large compute budgets. We propose PRioritized cOncept learninG via Relative Error-driven Sample Selection (PROGRESS), a data- and compute-efficient framework that enables VLMs to dynamically select what to learn next based on their evolving needs during training. At each stage, the model tracks its learning progress across skills and selects the most informative samples-those it has not already mastered and that are not too difficult to learn at the current stage of training. This strategy effectively controls skill acquisition and the order in which skills are learned. Specifically, we sample from skills showing the highest learning progress, prioritizing those with the most rapid improvement. Unlike prior methods, PROGRESS requires no upfront answer annotations, queries answers only on a need basis, avoids reliance on additional supervision from auxiliary VLMs, and does not require compute-heavy gradient computations for data selection. Experiments across multiple instruction-tuning datasets of varying scales demonstrate that PROGRESS consistently outperforms state-of-the-art baselines with much less data and supervision. Additionally, we show strong cross-architecture generalization and transferability to larger models, validating PROGRESS as a scalable solution for efficient learning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Un-considering Contextual Information: Assessing LLMs' Understanding of Indexical Elements</title>
<link>https://arxiv.org/abs/2506.01089</link>
<guid>https://arxiv.org/abs/2506.01089</guid>
<content:encoded><![CDATA[
arXiv:2506.01089v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated impressive performances in tasks related to coreference resolution. However, previous studies mostly assessed LLM performance on coreference resolution with nouns and third person pronouns. This study evaluates LLM performance on coreference resolution with indexical like I, you, here and tomorrow, which come with unique challenges due to their linguistic properties. We present the first study examining how LLMs interpret indexicals in English, releasing the English Indexical Dataset with 1600 multiple-choice questions. We evaluate pioneering LLMs, including GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro, and DeepSeek V3. Our results reveal that LLMs exhibit an impressive performance with some indexicals (I), while struggling with others (you, here, tomorrow), and that syntactic cues (e.g. quotation) contribute to LLM performance with some indexicals, while they reduce performance with others. Code and data are available at: https://github.com/metehanoguzz/LLMs-Indexicals-English.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speeding Up Hyper-Heuristics With Markov-Chain Operator Selection and the Only-Worsening Acceptance Operator</title>
<link>https://arxiv.org/abs/2506.01107</link>
<guid>https://arxiv.org/abs/2506.01107</guid>
<content:encoded><![CDATA[
arXiv:2506.01107v1 Announce Type: cross 
Abstract: The move-acceptance hyper-heuristic was recently shown to be able to leave local optima with astonishing efficiency (Lissovoi et al., Artificial Intelligence (2023)). In this work, we propose two modifications to this algorithm that demonstrate impressive performances on a large class of benchmarks including the classic Cliff$_d$ and Jump$_m$ function classes. (i) Instead of randomly choosing between the only-improving and any-move acceptance operator, we take this choice via a simple two-state Markov chain. This modification alone reduces the runtime on Jump$_m$ functions with gap parameter $m$ from $\Omega(n^{2m-1})$ to $O(n^{m+1})$. (ii) We then replace the all-moves acceptance operator with the operator that only accepts worsenings. Such a, counter-intuitive, operator has not been used before in the literature. However, our proofs show that our only-worsening operator can greatly help in leaving local optima, reducing, e.g., the runtime on Jump functions to $O(n^3 \log n)$ independent of the gap size. In general, we prove a remarkably good runtime of $O(n^{k+1} \log n)$ for our Markov move-acceptance hyper-heuristic on all members of a new benchmark class SEQOPT$_k$, which contains a large number of functions having $k$ successive local optima, and which contains the commonly studied Jump$_m$ and Cliff$_d$ functions for $k=2$.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CountingFruit: Real-Time 3D Fruit Counting with Language-Guided Semantic Gaussian Splatting</title>
<link>https://arxiv.org/abs/2506.01109</link>
<guid>https://arxiv.org/abs/2506.01109</guid>
<content:encoded><![CDATA[
arXiv:2506.01109v1 Announce Type: cross 
Abstract: Accurate fruit counting in real-world agricultural environments is a longstanding challenge due to visual occlusions, semantic ambiguity, and the high computational demands of 3D reconstruction. Existing methods based on neural radiance fields suffer from low inference speed, limited generalization, and lack support for open-set semantic control. This paper presents FruitLangGS, a real-time 3D fruit counting framework that addresses these limitations through spatial reconstruction, semantic embedding, and language-guided instance estimation. FruitLangGS first reconstructs orchard-scale scenes using an adaptive Gaussian splatting pipeline with radius-aware pruning and tile-based rasterization for efficient rendering. To enable semantic control, each Gaussian encodes a compressed CLIP-aligned language embedding, forming a compact and queryable 3D representation. At inference time, prompt-based semantic filtering is applied directly in 3D space, without relying on image-space segmentation or view-level fusion. The selected Gaussians are then converted into dense point clouds via distribution-aware sampling and clustered to estimate fruit counts. Experimental results on real orchard data demonstrate that FruitLangGS achieves higher rendering speed, semantic flexibility, and counting accuracy compared to prior approaches, offering a new perspective for language-driven, real-time neural rendering across open-world scenarios.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal Contextual Fusion</title>
<link>https://arxiv.org/abs/2506.01111</link>
<guid>https://arxiv.org/abs/2506.01111</guid>
<content:encoded><![CDATA[
arXiv:2506.01111v1 Announce Type: cross 
Abstract: High-quality, large-scale audio captioning is crucial for advancing audio understanding, yet current automated methods often generate captions that lack fine-grained detail and contextual accuracy, primarily due to their reliance on limited unimodal or superficial multimodal information. Drawing inspiration from human auditory perception, which adeptly integrates cross-modal cues and performs sophisticated auditory scene analysis, we introduce a novel two-stage automated pipeline. This pipeline first employs specialized pretrained models to extract diverse contextual cues (e.g., speech, music, general sounds, and visual information from associated video). A large language model (LLM) then synthesizes these rich, multimodal inputs to generate detailed and context-aware audio captions. Key contributions of this work include: (1) the proposed scalable method for fine-grained audio caption generation; (2) FusionAudio, a new large-scale dataset comprising 1.2 million such detailed captions, combined with 6 million QA pairs; and (3) enhanced audio models developed using FusionAudio, specifically a CLAP-based audio encoder with superior audio-text alignment and instruction following. This paper paves the way for more nuanced and accurate automated understanding of complex audio environments. Code and data can be found in https://github.com/satsuki2486441738/FusionAudio.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconsidering LLM Uncertainty Estimation Methods in the Wild</title>
<link>https://arxiv.org/abs/2506.01114</link>
<guid>https://arxiv.org/abs/2506.01114</guid>
<content:encoded><![CDATA[
arXiv:2506.01114v1 Announce Type: cross 
Abstract: Large Language Model (LLM) Uncertainty Estimation (UE) methods have become a crucial tool for detecting hallucinations in recent years. While numerous UE methods have been proposed, most existing studies evaluate them in isolated short-form QA settings using threshold-independent metrics such as AUROC or PRR. However, real-world deployment of UE methods introduces several challenges. In this work, we systematically examine four key aspects of deploying UE methods in practical settings. Specifically, we assess (1) the sensitivity of UE methods to decision threshold selection, (2) their robustness to query transformations such as typos, adversarial prompts, and prior chat history, (3) their applicability to long-form generation, and (4) strategies for handling multiple UE scores for a single query. Our evaluations on 19 UE methods reveal that most of them are highly sensitive to threshold selection when there is a distribution shift in the calibration dataset. While these methods generally exhibit robustness against previous chat history and typos, they are significantly vulnerable to adversarial prompts. Additionally, while existing UE methods can be adapted for long-form generation through various strategies, there remains considerable room for improvement. Lastly, ensembling multiple UE scores at test time provides a notable performance boost, which highlights its potential as a practical improvement strategy. Code is available at: https://github.com/duygunuryldz/uncertainty_in_the_wild.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuro-Symbolic Generative Diffusion Models for Physically Grounded, Robust, and Safe Generation</title>
<link>https://arxiv.org/abs/2506.01121</link>
<guid>https://arxiv.org/abs/2506.01121</guid>
<content:encoded><![CDATA[
arXiv:2506.01121v1 Announce Type: cross 
Abstract: Despite the remarkable generative capabilities of diffusion models, their integration into safety-critical or scientifically rigorous applications remains hindered by the need to ensure compliance with stringent physical, structural, and operational constraints. To address this challenge, this paper introduces Neuro-Symbolic Diffusion (NSD), a novel framework that interleaves diffusion steps with symbolic optimization, enabling the generation of certifiably consistent samples under user-defined functional and logic constraints. This key feature is provided for both standard and discrete diffusion models, enabling, for the first time, the generation of both continuous (e.g., images and trajectories) and discrete (e.g., molecular structures and natural language) outputs that comply with constraints. This ability is demonstrated on tasks spanning three key challenges: (1) Safety, in the context of non-toxic molecular generation and collision-free trajectory optimization; (2) Data scarcity, in domains such as drug discovery and materials engineering; and (3) Out-of-domain generalization, where enforcing symbolic constraints allows adaptation beyond the training distribution.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Words to Waves: Analyzing Concept Formation in Speech and Text-Based Foundation Models</title>
<link>https://arxiv.org/abs/2506.01133</link>
<guid>https://arxiv.org/abs/2506.01133</guid>
<content:encoded><![CDATA[
arXiv:2506.01133v1 Announce Type: cross 
Abstract: The emergence of large language models (LLMs) has demonstrated that systems trained solely on text can acquire extensive world knowledge, develop reasoning capabilities, and internalize abstract semantic concepts--showcasing properties that can be associated with general intelligence. This raises an intriguing question: Do such concepts emerge in models trained on other modalities, such as speech? Furthermore, when models are trained jointly on multiple modalities: Do they develop a richer, more structured semantic understanding? To explore this, we analyze the conceptual structures learned by speech and textual models both individually and jointly. We employ Latent Concept Analysis, an unsupervised method for uncovering and interpreting latent representations in neural networks, to examine how semantic abstractions form across modalities. For reproducibility we made scripts and other resources available to the community.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Earley-Driven Dynamic Pruning for Efficient Structured Decoding</title>
<link>https://arxiv.org/abs/2506.01151</link>
<guid>https://arxiv.org/abs/2506.01151</guid>
<content:encoded><![CDATA[
arXiv:2506.01151v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown remarkable capabilities, yet ensuring their outputs conform to strict structural or grammatical constraints remains challenging, which is critical in function calls and domain-specific language (DSL) generation. Constrained decoding with context-free grammar is a flexible approach to guarantee LLMs' adherence to a specific format by dynamically building a token logits mask. However, creating this mask requires checking the validity of all tokens in the LLM vocabulary at every decoding step, which often incurs significant overheads in existing constrained decoding engines. To address this challenge, we propose $\textbf{ZapFormat}$, a novel $\textbf{dynamic pruning}$ strategy based on the Earley algorithm that identifies and eliminates invalid or redundant Earley states in real-time, significantly reducing memory occupation of the Earley algorithm's states. This further enables us to use a state cache to speed up structured generations on a large number of queries. We implemented ZapFormat in a new constrained decoding engine called Formatron which also incorporates existing optimizations. Through comprehensive experiments on structured generation tasks, including JSON generation, JSON Schema validation, and semantic parsing, we demonstrate that Formatron not only $\textbf{consistently maintains}$ high-precision compliant outputs but also achieves $\textbf{significant improvements}$ in inference speed up to 2x compared to state-of-the-art implementations. More importantly, Formatron is generally applicable across various LLM architectures. We release Formatron as open source at https://github.com/Dan-wanna-M/formatron.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FORT: Forward-Only Regression Training of Normalizing Flows</title>
<link>https://arxiv.org/abs/2506.01158</link>
<guid>https://arxiv.org/abs/2506.01158</guid>
<content:encoded><![CDATA[
arXiv:2506.01158v1 Announce Type: cross 
Abstract: Simulation-free training frameworks have been at the forefront of the generative modelling revolution in continuous spaces, leading to neural dynamical systems that encompass modern large-scale diffusion and flow matching models. Despite the scalability of training, the generation of high-quality samples and their corresponding likelihood under the model requires expensive numerical simulation -- inhibiting adoption in numerous scientific applications such as equilibrium sampling of molecular systems. In this paper, we revisit classical normalizing flows as one-step generative models with exact likelihoods and propose a novel, scalable training objective that does not require computing the expensive change of variable formula used in conventional maximum likelihood training. We propose Forward-Only Regression Training (FORT), a simple $\ell_2$-regression objective that maps prior samples under our flow to specifically chosen targets. We demonstrate that FORT supports a wide class of targets, such as optimal transport targets and targets from pre-trained continuous-time normalizing flows (CNF). We further demonstrate that by using CNF targets, our one-step flows allow for larger-scale training that exceeds the performance and stability of maximum likelihood training, while unlocking a broader class of architectures that were previously challenging to train. Empirically, we elucidate that our trained flows can perform equilibrium conformation sampling in Cartesian coordinates of alanine dipeptide, alanine tripeptide, and alanine tetrapeptide.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VUSA: Virtually Upscaled Systolic Array Architecture to Exploit Unstructured Sparsity in AI Acceleration</title>
<link>https://arxiv.org/abs/2506.01166</link>
<guid>https://arxiv.org/abs/2506.01166</guid>
<content:encoded><![CDATA[
arXiv:2506.01166v1 Announce Type: cross 
Abstract: Leveraging high degrees of unstructured sparsity is a promising approach to enhance the efficiency of deep neural network DNN accelerators - particularly important for emerging Edge-AI applications. We introduce VUSA, a systolic-array architecture that virtually grows based on the present sparsity to perform larger matrix multiplications with the same number of physical multiply-accumulate MAC units. The proposed architecture achieves saving by 37% and 68% in area and power efficiency, respectively, at the same peak-performance, compared to a baseline systolic array architecture in a commercial 16-nm technology. Still, the proposed architecture supports acceleration for any DNN with any sparsity - even no sparsity at all. Thus, the proposed architecture is application-independent, making it viable for general-purpose AI acceleration.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Quantum and Classical Computing in Drug Design: Architecture Principles for Improved Molecule Generation</title>
<link>https://arxiv.org/abs/2506.01177</link>
<guid>https://arxiv.org/abs/2506.01177</guid>
<content:encoded><![CDATA[
arXiv:2506.01177v1 Announce Type: cross 
Abstract: Hybrid quantum-classical machine learning offers a path to leverage noisy intermediate-scale quantum (NISQ) devices for drug discovery, but optimal model architectures remain unclear. We systematically optimize the quantum-classical bridge architecture for generative adversarial networks (GANs) in molecular discovery using multi-objective Bayesian optimization. Our optimized model (BO-QGAN) significantly improves performance, achieving a 2.27-fold higher Drug Candidate Score (DCS) than prior quantum-hybrid benchmarks and 2.21-fold higher than the classical baseline, using over 60% fewer parameters. Key findings favor layering multiple (3-4) shallow (4-8 qubit) quantum circuits sequentially, while classical architecture shows less sensitivity above a minimum capacity. This work provides the first empirically grounded architectural guidelines for hybrid models, enabling more effective integration of current quantum computers into pharmaceutical research pipelines.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Humanoid World Models: Open World Foundation Models for Humanoid Robotics</title>
<link>https://arxiv.org/abs/2506.01182</link>
<guid>https://arxiv.org/abs/2506.01182</guid>
<content:encoded><![CDATA[
arXiv:2506.01182v1 Announce Type: cross 
Abstract: Humanoid robots have the potential to perform complex tasks in human centered environments but require robust predictive models to reason about the outcomes of their actions. We introduce Humanoid World Models (HWM) a family of lightweight open source video based models that forecast future egocentric observations conditioned on actions. We train two types of generative models Masked Transformers and FlowMatching on 100 hours of humanoid demonstrations. Additionally we explore architectural variants with different attention mechanisms and parameter sharing strategies. Our parameter sharing techniques reduce model size by 33 to 53 with minimal impact on performance or visual fidelity. HWM is designed to be trained and deployed in practical academic and small lab settings such as 1 to 2 GPUs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Doubly Robust Alignment for Large Language Models</title>
<link>https://arxiv.org/abs/2506.01183</link>
<guid>https://arxiv.org/abs/2506.01183</guid>
<content:encoded><![CDATA[
arXiv:2506.01183v1 Announce Type: cross 
Abstract: This paper studies reinforcement learning from human feedback (RLHF) for aligning large language models with human preferences. While RLHF has demonstrated promising results, many algorithms are highly sensitive to misspecifications in the underlying preference model (e.g., the Bradley-Terry model), the reference policy, or the reward function, resulting in undesirable fine-tuning. To address model misspecification, we propose a doubly robust preference optimization algorithm that remains consistent when either the preference model or the reference policy is correctly specified (without requiring both). Our proposal demonstrates superior and more robust performance than state-of-the-art algorithms, both in theory and in practice. The code is available at https://github.com/DRPO4LLM/DRPO4LLM
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OG-VLA: 3D-Aware Vision Language Action Model via Orthographic Image Generation</title>
<link>https://arxiv.org/abs/2506.01196</link>
<guid>https://arxiv.org/abs/2506.01196</guid>
<content:encoded><![CDATA[
arXiv:2506.01196v1 Announce Type: cross 
Abstract: We introduce OG-VLA, a novel architecture and learning framework that combines the generalization strengths of Vision Language Action models (VLAs) with the robustness of 3D-aware policies. We address the challenge of mapping natural language instructions and multi-view RGBD observations to quasi-static robot actions. 3D-aware robot policies achieve state-of-the-art performance on precise robot manipulation tasks, but struggle with generalization to unseen instructions, scenes, and objects. On the other hand, VLAs excel at generalizing across instructions and scenes, but can be sensitive to camera and robot pose variations. We leverage prior knowledge embedded in language and vision foundation models to improve generalization of 3D-aware keyframe policies. OG-VLA projects input observations from diverse views into a point cloud which is then rendered from canonical orthographic views, ensuring input view invariance and consistency between input and output spaces. These canonical views are processed with a vision backbone, a Large Language Model (LLM), and an image diffusion model to generate images that encode the next position and orientation of the end-effector on the input scene. Evaluations on the Arnold and Colosseum benchmarks demonstrate state-of-the-art generalization to unseen environments, with over 40% relative improvements while maintaining robust performance in seen settings. We also show real-world adaption in 3 to 5 demonstrations along with strong generalization. Videos and resources at https://og-vla.github.io/
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incorporating Hierarchical Semantics in Sparse Autoencoder Architectures</title>
<link>https://arxiv.org/abs/2506.01197</link>
<guid>https://arxiv.org/abs/2506.01197</guid>
<content:encoded><![CDATA[
arXiv:2506.01197v1 Announce Type: cross 
Abstract: Sparse dictionary learning (and, in particular, sparse autoencoders) attempts to learn a set of human-understandable concepts that can explain variation on an abstract space. A basic limitation of this approach is that it neither exploits nor represents the semantic relationships between the learned concepts. In this paper, we introduce a modified SAE architecture that explicitly models a semantic hierarchy of concepts. Application of this architecture to the internal representations of large language models shows both that semantic hierarchy can be learned, and that doing so improves both reconstruction and interpretability. Additionally, the architecture leads to significant improvements in computational efficiency.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mamba Drafters for Speculative Decoding</title>
<link>https://arxiv.org/abs/2506.01206</link>
<guid>https://arxiv.org/abs/2506.01206</guid>
<content:encoded><![CDATA[
arXiv:2506.01206v1 Announce Type: cross 
Abstract: Speculative decoding has emerged as a promising approach to accelerating large language model (LLM) generation using a fast drafter while maintaining alignment with the target model's distribution. However, existing approaches face a trade-off: external drafters offer flexibility but can suffer from slower drafting, while self-speculation methods use drafters tailored to the target model but require re-training. In this paper, we introduce novel drafters based on Mamba, a state-of-the-art state space model (SSM), as a solution that combines the best aspects of both approaches. By leveraging the linear structure of SSMs, our approach avoids the quadratic complexity inherent in traditional Transformer-based methods, enabling faster drafting and lower memory usage while maintaining the flexibility to work across different target models. We further enhance efficiency with a novel test-time tree search algorithm for generating high-quality draft candidates. Our empirical evaluation demonstrates that Mamba-based drafters not only outperform existing external drafting methods but are also comparable to state-of-the-art self-speculation approaches while using less memory and maintaining their cross-model adaptability.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Review on Coarse to Fine-Grained Animal Action Recognition</title>
<link>https://arxiv.org/abs/2506.01214</link>
<guid>https://arxiv.org/abs/2506.01214</guid>
<content:encoded><![CDATA[
arXiv:2506.01214v1 Announce Type: cross 
Abstract: This review provides an in-depth exploration of the field of animal action recognition, focusing on coarse-grained (CG) and fine-grained (FG) techniques. The primary aim is to examine the current state of research in animal behaviour recognition and to elucidate the unique challenges associated with recognising subtle animal actions in outdoor environments. These challenges differ significantly from those encountered in human action recognition due to factors such as non-rigid body structures, frequent occlusions, and the lack of large-scale, annotated datasets. The review begins by discussing the evolution of human action recognition, a more established field, highlighting how it progressed from broad, coarse actions in controlled settings to the demand for fine-grained recognition in dynamic environments. This shift is particularly relevant for animal action recognition, where behavioural variability and environmental complexity present unique challenges that human-centric models cannot fully address. The review then underscores the critical differences between human and animal action recognition, with an emphasis on high intra-species variability, unstructured datasets, and the natural complexity of animal habitats. Techniques like spatio-temporal deep learning frameworks (e.g., SlowFast) are evaluated for their effectiveness in animal behaviour analysis, along with the limitations of existing datasets. By assessing the strengths and weaknesses of current methodologies and introducing a recently-published dataset, the review outlines future directions for advancing fine-grained action recognition, aiming to improve accuracy and generalisability in behaviour analysis across species.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPEAR: Security Posture Evaluation using AI Planner-Reasoning on Attack-Connectivity Hypergraphs</title>
<link>https://arxiv.org/abs/2506.01227</link>
<guid>https://arxiv.org/abs/2506.01227</guid>
<content:encoded><![CDATA[
arXiv:2506.01227v1 Announce Type: cross 
Abstract: Graph-based frameworks are often used in network hardening to help a cyber defender understand how a network can be attacked and how the best defenses can be deployed. However, incorporating network connectivity parameters in the attack graph, reasoning about the attack graph when we do not have access to complete information, providing system administrator suggestions in an understandable format, and allowing them to do what-if analysis on various scenarios and attacker motives is still missing. We fill this gap by presenting SPEAR, a formal framework with tool support for security posture evaluation and analysis that keeps human-in-the-loop. SPEAR uses the causal formalism of AI planning to model vulnerabilities and configurations in a networked system. It automatically converts network configurations and vulnerability descriptions into planning models expressed in the Planning Domain Definition Language (PDDL). SPEAR identifies a set of diverse security hardening strategies that can be presented in a manner understandable to the domain expert. These allow the administrator to explore the network hardening solution space in a systematic fashion and help evaluate the impact and compare the different solutions.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Efficient Few-shot Graph Neural Architecture Search via Partitioning Gradient Contribution</title>
<link>https://arxiv.org/abs/2506.01231</link>
<guid>https://arxiv.org/abs/2506.01231</guid>
<content:encoded><![CDATA[
arXiv:2506.01231v1 Announce Type: cross 
Abstract: To address the weight coupling problem, certain studies introduced few-shot Neural Architecture Search (NAS) methods, which partition the supernet into multiple sub-supernets. However, these methods often suffer from computational inefficiency and tend to provide suboptimal partitioning schemes. To address this problem more effectively, we analyze the weight coupling problem from a novel perspective, which primarily stems from distinct modules in succeeding layers imposing conflicting gradient directions on the preceding layer modules. Based on this perspective, we propose the Gradient Contribution (GC) method that efficiently computes the cosine similarity of gradient directions among modules by decomposing the Vector-Jacobian Product during supernet backpropagation. Subsequently, the modules with conflicting gradient directions are allocated to distinct sub-supernets while similar ones are grouped together. To assess the advantages of GC and address the limitations of existing Graph Neural Architecture Search methods, which are limited to searching a single type of Graph Neural Networks (Message Passing Neural Networks (MPNNs) or Graph Transformers (GTs)), we propose the Unified Graph Neural Architecture Search (UGAS) framework, which explores optimal combinations of MPNNs and GTs. The experimental results demonstrate that GC achieves state-of-the-art (SOTA) performance in supernet partitioning quality and time efficiency. In addition, the architectures searched by UGAS+GC outperform both the manually designed GNNs and those obtained by existing NAS methods. Finally, ablation studies further demonstrate the effectiveness of all proposed methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-Augmented Generation of Ontologies from Relational Databases</title>
<link>https://arxiv.org/abs/2506.01232</link>
<guid>https://arxiv.org/abs/2506.01232</guid>
<content:encoded><![CDATA[
arXiv:2506.01232v1 Announce Type: cross 
Abstract: Transforming relational databases into knowledge graphs with enriched ontologies enhances semantic interoperability and unlocks advanced graph-based learning and reasoning over data. However, previous approaches either demand significant manual effort to derive an ontology from a database schema or produce only a basic ontology. We present RIGOR, Retrieval-augmented Iterative Generation of RDB Ontologies, an LLM-driven approach that turns relational schemas into rich OWL ontologies with minimal human effort. RIGOR combines three sources via RAG, the database schema and its documentation, a repository of domain ontologies, and a growing core ontology, to prompt a generative LLM for producing successive, provenance-tagged delta ontology fragments. Each fragment is refined by a judge-LLM before being merged into the core ontology, and the process iterates table-by-table following foreign key constraints until coverage is complete. Applied to real-world databases, our approach outputs ontologies that score highly on standard quality dimensions such as accuracy, completeness, conciseness, adaptability, clarity, and consistency, while substantially reducing manual effort.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fourier-Modulated Implicit Neural Representation for Multispectral Satellite Image Compression</title>
<link>https://arxiv.org/abs/2506.01234</link>
<guid>https://arxiv.org/abs/2506.01234</guid>
<content:encoded><![CDATA[
arXiv:2506.01234v1 Announce Type: cross 
Abstract: Multispectral satellite images play a vital role in agriculture, fisheries, and environmental monitoring. However, their high dimensionality, large data volumes, and diverse spatial resolutions across multiple channels pose significant challenges for data compression and analysis. This paper presents ImpliSat, a unified framework specifically designed to address these challenges through efficient compression and reconstruction of multispectral satellite data. ImpliSat leverages Implicit Neural Representations (INR) to model satellite images as continuous functions over coordinate space, capturing fine spatial details across varying spatial resolutions. Furthermore, we introduce a Fourier modulation algorithm that dynamically adjusts to the spectral and spatial characteristics of each band, ensuring optimal compression while preserving critical image details.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Polishing Every Facet of the GEM: Testing Linguistic Competence of LLMs and Humans in Korean</title>
<link>https://arxiv.org/abs/2506.01237</link>
<guid>https://arxiv.org/abs/2506.01237</guid>
<content:encoded><![CDATA[
arXiv:2506.01237v1 Announce Type: cross 
Abstract: We introduce the $\underline{Ko}rean \underline{G}rammar \underline{E}valuation Bench\underline{M}ark (KoGEM)$, designed to assess the linguistic competence of LLMs and humans in Korean. KoGEM consists of 1.5k multiple-choice QA pairs covering five main categories and 16 subcategories. The zero-shot evaluation of 27 LLMs of various sizes and types reveals that while LLMs perform remarkably well on straightforward tasks requiring primarily definitional knowledge, they struggle with tasks that demand the integration of real-world experiential knowledge, such as phonological rules and pronunciation. Furthermore, our in-depth analysis suggests that incorporating such experiential knowledge could enhance the linguistic competence of LLMs. With KoGEM, we not only highlight the limitations of current LLMs in linguistic competence but also uncover hidden facets of LLMs in linguistic competence, paving the way for enhancing comprehensive language understanding. Our code and dataset are available at: https://github.com/SungHo3268/KoGEM.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>General search techniques without common knowledge for imperfect-information games, and application to superhuman Fog of War chess</title>
<link>https://arxiv.org/abs/2506.01242</link>
<guid>https://arxiv.org/abs/2506.01242</guid>
<content:encoded><![CDATA[
arXiv:2506.01242v1 Announce Type: cross 
Abstract: Since the advent of AI, games have served as progress benchmarks. Meanwhile, imperfect-information variants of chess have existed for over a century, present extreme challenges, and have been the focus of significant AI research. Beyond calculation needed in regular chess, they require reasoning about information gathering, the opponent's knowledge, signaling, etc. The most popular variant, Fog of War (FoW) chess (aka. dark chess) is a recognized challenge problem in AI after superhuman performance was reached in no-limit Texas hold'em poker. We present Obscuro, the first superhuman AI for FoW chess. It introduces advances to search in imperfect-information games, enabling strong, scalable reasoning. Experiments against the prior state-of-the-art AI and human players -- including the world's best -- show that Obscuro is significantly stronger. FoW chess is the largest (by amount of imperfect information) turn-based game in which superhuman performance has been achieved and the largest game in which imperfect-information search has been successfully applied.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Sparse Steering: Improving Zero-shot Image Classification with Sparsity Guided Steering Vectors</title>
<link>https://arxiv.org/abs/2506.01247</link>
<guid>https://arxiv.org/abs/2506.01247</guid>
<content:encoded><![CDATA[
arXiv:2506.01247v1 Announce Type: cross 
Abstract: Steering vision foundation models at inference time without retraining or access to large labeled datasets is a desirable yet challenging objective, particularly in dynamic or resource-constrained settings. In this paper, we introduce Visual Sparse Steering (VS2), a lightweight, test-time method that guides vision models using steering vectors derived from sparse features learned by top-$k$ Sparse Autoencoders without requiring contrastive data. Specifically, VS2 surpasses zero-shot CLIP by 4.12% on CIFAR-100, 1.08% on CUB-200, and 1.84% on Tiny-ImageNet. We further propose VS2++, a retrieval-augmented variant that selectively amplifies relevant sparse features using pseudo-labeled neighbors at inference time. With oracle positive/negative sets, VS2++ achieves absolute top-1 gains over CLIP zero-shot of up to 21.44% on CIFAR-100, 7.08% on CUB-200, and 20.47% on Tiny-ImageNet. Interestingly, VS2 and VS2++ raise per-class accuracy by up to 25% and 38%, respectively, showing that sparse steering benefits specific classes by disambiguating visually or taxonomically proximate categories rather than providing a uniform boost. Finally, to better align the sparse features learned through the SAE reconstruction task with those relevant for downstream performance, we propose Prototype-Aligned Sparse Steering (PASS). By incorporating a prototype-alignment loss during SAE training, using labels only during training while remaining fully test-time unsupervised, PASS consistently, though modestly, outperforms VS2, achieving a 6.12% gain over VS2 only on CIFAR-100 with ViT-B/32.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MTCMB: A Multi-Task Benchmark Framework for Evaluating LLMs on Knowledge, Reasoning, and Safety in Traditional Chinese Medicine</title>
<link>https://arxiv.org/abs/2506.01252</link>
<guid>https://arxiv.org/abs/2506.01252</guid>
<content:encoded><![CDATA[
arXiv:2506.01252v1 Announce Type: cross 
Abstract: Traditional Chinese Medicine (TCM) is a holistic medical system with millennia of accumulated clinical experience, playing a vital role in global healthcare-particularly across East Asia. However, the implicit reasoning, diverse textual forms, and lack of standardization in TCM pose major challenges for computational modeling and evaluation. Large Language Models (LLMs) have demonstrated remarkable potential in processing natural language across diverse domains, including general medicine. Yet, their systematic evaluation in the TCM domain remains underdeveloped. Existing benchmarks either focus narrowly on factual question answering or lack domain-specific tasks and clinical realism. To fill this gap, we introduce MTCMB-a Multi-Task Benchmark for Evaluating LLMs on TCM Knowledge, Reasoning, and Safety. Developed in collaboration with certified TCM experts, MTCMB comprises 12 sub-datasets spanning five major categories: knowledge QA, language understanding, diagnostic reasoning, prescription generation, and safety evaluation. The benchmark integrates real-world case records, national licensing exams, and classical texts, providing an authentic and comprehensive testbed for TCM-capable models. Preliminary results indicate that current LLMs perform well on foundational knowledge but fall short in clinical reasoning, prescription planning, and safety compliance. These findings highlight the urgent need for domain-aligned benchmarks like MTCMB to guide the development of more competent and trustworthy medical AI systems. All datasets, code, and evaluation tools are publicly available at: https://github.com/Wayyuanyuan/MTCMB.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepSeek in Healthcare: A Survey of Capabilities, Risks, and Clinical Applications of Open-Source Large Language Models</title>
<link>https://arxiv.org/abs/2506.01257</link>
<guid>https://arxiv.org/abs/2506.01257</guid>
<content:encoded><![CDATA[
arXiv:2506.01257v1 Announce Type: cross 
Abstract: DeepSeek-R1 is a cutting-edge open-source large language model (LLM) developed by DeepSeek, showcasing advanced reasoning capabilities through a hybrid architecture that integrates mixture of experts (MoE), chain of thought (CoT) reasoning, and reinforcement learning. Released under the permissive MIT license, DeepSeek-R1 offers a transparent and cost-effective alternative to proprietary models like GPT-4o and Claude-3 Opus; it excels in structured problem-solving domains such as mathematics, healthcare diagnostics, code generation, and pharmaceutical research. The model demonstrates competitive performance on benchmarks like the United States Medical Licensing Examination (USMLE) and American Invitational Mathematics Examination (AIME), with strong results in pediatric and ophthalmologic clinical decision support tasks. Its architecture enables efficient inference while preserving reasoning depth, making it suitable for deployment in resource-constrained settings. However, DeepSeek-R1 also exhibits increased vulnerability to bias, misinformation, adversarial manipulation, and safety failures - especially in multilingual and ethically sensitive contexts. This survey highlights the model's strengths, including interpretability, scalability, and adaptability, alongside its limitations in general language fluency and safety alignment. Future research priorities include improving bias mitigation, natural language comprehension, domain-specific validation, and regulatory compliance. Overall, DeepSeek-R1 represents a major advance in open, scalable AI, underscoring the need for collaborative governance to ensure responsible and equitable deployment.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detoxification of Large Language Models through Output-layer Fusion with a Calibration Model</title>
<link>https://arxiv.org/abs/2506.01266</link>
<guid>https://arxiv.org/abs/2506.01266</guid>
<content:encoded><![CDATA[
arXiv:2506.01266v1 Announce Type: cross 
Abstract: Existing approaches for Large language model (LLM) detoxification generally rely on training on large-scale non-toxic or human-annotated preference data, designing prompts to instruct the LLM to generate safe content, or modifying the model parameters to remove toxic information, which are computationally expensive, lack robustness, and often compromise LLMs' fluency and contextual understanding. In this paper, we propose a simple yet effective approach for LLM detoxification, which leverages a compact, pre-trained calibration model that guides the detoxification process of a target LLM via a lightweight intervention in its generation pipeline. By learning a detoxified embedding space from non-toxic data, the calibration model effectively steers the LLM away from generating harmful content. This approach only requires a one-time training of the calibration model that is able to be seamlessly applied to multiple LLMs without compromising fluency or contextual understanding. Experiment results on the benchmark dataset demonstrate that our approach reduces toxicity while maintaining reasonable content expression.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReFoCUS: Reinforcement-guided Frame Optimization for Contextual Understanding</title>
<link>https://arxiv.org/abs/2506.01274</link>
<guid>https://arxiv.org/abs/2506.01274</guid>
<content:encoded><![CDATA[
arXiv:2506.01274v1 Announce Type: cross 
Abstract: Recent progress in Large Multi-modal Models (LMMs) has enabled effective vision-language reasoning, yet the ability to understand video content remains constrained by suboptimal frame selection strategies. Existing approaches often rely on static heuristics or external retrieval modules to feed frame information into video-LLMs, which may fail to provide the query-relevant information. In this work, we introduce ReFoCUS (Reinforcement-guided Frame Optimization for Contextual UnderStanding), a novel frame-level policy optimization framework that shifts the optimization target from textual responses to visual input selection. ReFoCUS learns a frame selection policy via reinforcement learning, using reward signals derived from a reference LMM to reflect the model's intrinsic preferences for frames that best support temporally grounded responses. To efficiently explore the large combinatorial frame space, we employ an autoregressive, conditional selection architecture that ensures temporal coherence while reducing complexity. Our approach does not require explicit supervision at the frame-level and consistently improves reasoning performance across multiple video QA benchmarks, highlighting the benefits of aligning frame selection with model-internal utility.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TSRating: Rating Quality of Diverse Time Series Data by Meta-learning from LLM Judgment</title>
<link>https://arxiv.org/abs/2506.01290</link>
<guid>https://arxiv.org/abs/2506.01290</guid>
<content:encoded><![CDATA[
arXiv:2506.01290v1 Announce Type: cross 
Abstract: High-quality time series (TS) data are essential for ensuring TS model performance, rendering research on rating TS data quality indispensable. Existing methods have shown promising rating accuracy within individual domains, primarily by extending data quality rating techniques such as influence functions and Shapley values to account for temporal characteristics. However, they neglect the fact that real-world TS data can span vastly different domains and exhibit distinct properties, hampering the accurate and efficient rating of diverse TS data. In this paper, we propose TSRating, a novel and unified framework for rating the quality of time series data crawled from diverse domains. TSRating is built on the assumption that LLMs inherit ample knowledge, acquired during their extensive pretraining, enabling them to comprehend and discern quality differences in diverse TS data. We verify this assumption by devising a series of prompts to elicit quality comparisons from LLMs for pairs of TS samples. We then fit a dedicated rating model, termed TSRater, to convert the LLMs' judgments into efficient quality predictions via TSRater's inference on future TS samples. To ensure cross-domain adaptability, we develop a meta-learning scheme to train TSRater on quality comparisons collected from nine distinct domains. To improve training efficiency, we employ signSGD for inner-loop updates, thus circumventing the demanding computation of hypergradients. Extensive experimental results on eleven benchmark datasets across three time series tasks, each using both conventional TS models and TS foundation models, demonstrate that TSRating outperforms baselines in terms of estimation accuracy, efficiency, and domain adaptability.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Abstractive Visual Understanding of Multi-modal Structured Knowledge: A New Perspective for MLLM Evaluation</title>
<link>https://arxiv.org/abs/2506.01293</link>
<guid>https://arxiv.org/abs/2506.01293</guid>
<content:encoded><![CDATA[
arXiv:2506.01293v1 Announce Type: cross 
Abstract: Multi-modal large language models (MLLMs) incorporate heterogeneous modalities into LLMs, enabling a comprehensive understanding of diverse scenarios and objects. Despite the proliferation of evaluation benchmarks and leaderboards for MLLMs, they predominantly overlook the critical capacity of MLLMs to comprehend world knowledge with structured abstractions that appear in visual form. To address this gap, we propose a novel evaluation paradigm and devise M3STR, an innovative benchmark grounded in the Multi-Modal Map for STRuctured understanding. This benchmark leverages multi-modal knowledge graphs to synthesize images encapsulating subgraph architectures enriched with multi-modal entities. M3STR necessitates that MLLMs not only recognize the multi-modal entities within the visual inputs but also decipher intricate relational topologies among them. We delineate the benchmark's statistical profiles and automated construction pipeline, accompanied by an extensive empirical analysis of 26 state-of-the-art MLLMs. Our findings reveal persistent deficiencies in processing abstractive visual information with structured knowledge, thereby charting a pivotal trajectory for advancing MLLMs' holistic reasoning capacities. Our code and data are released at https://github.com/zjukg/M3STR
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Align is not Enough: Multimodal Universal Jailbreak Attack against Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2506.01307</link>
<guid>https://arxiv.org/abs/2506.01307</guid>
<content:encoded><![CDATA[
arXiv:2506.01307v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have evolved into Multimodal Large Language Models (MLLMs), significantly enhancing their capabilities by integrating visual information and other types, thus aligning more closely with the nature of human intelligence, which processes a variety of data forms beyond just text. Despite advancements, the undesirable generation of these models remains a critical concern, particularly due to vulnerabilities exposed by text-based jailbreak attacks, which have represented a significant threat by challenging existing safety protocols. Motivated by the unique security risks posed by the integration of new and old modalities for MLLMs, we propose a unified multimodal universal jailbreak attack framework that leverages iterative image-text interactions and transfer-based strategy to generate a universal adversarial suffix and image. Our work not only highlights the interaction of image-text modalities can be used as a critical vulnerability but also validates that multimodal universal jailbreak attacks can bring higher-quality undesirable generations across different MLLMs. We evaluate the undesirable context generation of MLLMs like LLaVA, Yi-VL, MiniGPT4, MiniGPT-v2, and InstructBLIP, and reveal significant multimodal safety alignment issues, highlighting the inadequacy of current safety mechanisms against sophisticated multimodal attacks. This study underscores the urgent need for robust safety measures in MLLMs, advocating for a comprehensive review and enhancement of security protocols to mitigate potential risks associated with multimodal capabilities.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T-SHIRT: Token-Selective Hierarchical Data Selection for Instruction Tuning</title>
<link>https://arxiv.org/abs/2506.01317</link>
<guid>https://arxiv.org/abs/2506.01317</guid>
<content:encoded><![CDATA[
arXiv:2506.01317v1 Announce Type: cross 
Abstract: Instruction tuning is essential for Large Language Models (LLMs) to effectively follow user instructions. To improve training efficiency and reduce data redundancy, recent works use LLM-based scoring functions, e.g., Instruction-Following Difficulty (IFD), to select high-quality instruction-tuning data with scores above a threshold. While these data selection methods often lead to models that can match or even exceed the performance of models trained on the full datasets, we identify two key limitations: (i) they assess quality at the sample level, ignoring token-level informativeness; and (ii) they overlook the robustness of the scoring method, often selecting a sample due to superficial lexical features instead of its true quality. In this work, we propose Token-Selective HIeRarchical Data Selection for Instruction Tuning (T-SHIRT), a novel data selection framework that introduces a new scoring method to include only informative tokens in quality evaluation and also promotes robust and reliable samples whose neighbors also show high quality with less local inconsistencies. We demonstrate that models instruction-tuned on a curated dataset (only 5% of the original size) using T-SHIRT can outperform those trained on the entire large-scale dataset by up to 5.48 points on average across eight benchmarks. Across various LLMs and training set scales, our method consistently surpasses existing state-of-the-art data selection techniques, while also remaining both cost-effective and highly efficient. For instance, by using GPT-2 for score computation, we are able to process a dataset of 52k samples using 40 minutes on a single GPU.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlearning's Blind Spots: Over-Unlearning and Prototypical Relearning Attack</title>
<link>https://arxiv.org/abs/2506.01318</link>
<guid>https://arxiv.org/abs/2506.01318</guid>
<content:encoded><![CDATA[
arXiv:2506.01318v1 Announce Type: cross 
Abstract: Machine unlearning (MU) aims to expunge a designated forget set from a trained model without costly retraining, yet the existing techniques overlook two critical blind spots: "over-unlearning" that deteriorates retained data near the forget set, and post-hoc "relearning" attacks that aim to resurrect the forgotten knowledge. We first derive the over-unlearning metric OU@{\epsilon}, which represents the collateral damage to the nearby region of the forget set, where the over-unlearning mainly appears. Next, we expose an unforeseen relearning threat on MU, i.e., the Prototypical Relearning Attack, which exploits the per-class prototype of the forget class with just a few samples, and easily restores the pre-unlearning performance. To counter both blind spots, we introduce Spotter, a plug-and-play objective that combines (i) a masked knowledge-distillation penalty on the nearby region of forget set to suppress OU@{\epsilon}, and (ii) an intra-class dispersion loss that scatters forget-class embeddings, neutralizing prototypical relearning attacks. On CIFAR-10, as one of validations, Spotter reduces OU@{\epsilon}by below the 0.05X of the baseline, drives forget accuracy to 0%, preserves accuracy of the retain set within 1% of difference with the original, and denies the prototype-attack by keeping the forget set accuracy within <1%, without accessing retained data. It confirms that Spotter is a practical remedy of the unlearning's blind spots.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\Psi$-Sampler: Initial Particle Sampling for SMC-Based Inference-Time Reward Alignment in Score Models</title>
<link>https://arxiv.org/abs/2506.01320</link>
<guid>https://arxiv.org/abs/2506.01320</guid>
<content:encoded><![CDATA[
arXiv:2506.01320v1 Announce Type: cross 
Abstract: We introduce $\Psi$-Sampler, an SMC-based framework incorporating pCNL-based initial particle sampling for effective inference-time reward alignment with a score-based generative model. Inference-time reward alignment with score-based generative models has recently gained significant traction, following a broader paradigm shift from pre-training to post-training optimization. At the core of this trend is the application of Sequential Monte Carlo (SMC) to the denoising process. However, existing methods typically initialize particles from the Gaussian prior, which inadequately captures reward-relevant regions and results in reduced sampling efficiency. We demonstrate that initializing from the reward-aware posterior significantly improves alignment performance. To enable posterior sampling in high-dimensional latent spaces, we introduce the preconditioned Crank-Nicolson Langevin (pCNL) algorithm, which combines dimension-robust proposals with gradient-informed dynamics. This approach enables efficient and scalable posterior sampling and consistently improves performance across various reward alignment tasks, including layout-to-image generation, quantity-aware generation, and aesthetic-preference generation, as demonstrated in our experiments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STSA: Federated Class-Incremental Learning via Spatial-Temporal Statistics Aggregation</title>
<link>https://arxiv.org/abs/2506.01327</link>
<guid>https://arxiv.org/abs/2506.01327</guid>
<content:encoded><![CDATA[
arXiv:2506.01327v1 Announce Type: cross 
Abstract: Federated Class-Incremental Learning (FCIL) enables Class-Incremental Learning (CIL) from distributed data. Existing FCIL methods typically integrate old knowledge preservation into local client training. However, these methods cannot avoid spatial-temporal client drift caused by data heterogeneity and often incur significant computational and communication overhead, limiting practical deployment. To address these challenges simultaneously, we propose a novel approach, Spatial-Temporal Statistics Aggregation (STSA), which provides a unified framework to aggregate feature statistics both spatially (across clients) and temporally (across stages). The aggregated feature statistics are unaffected by data heterogeneity and can be used to update the classifier in closed form at each stage. Additionally, we introduce STSA-E, a communication-efficient variant with theoretical guarantees, achieving similar performance to STSA-E with much lower communication overhead. Extensive experiments on three widely used FCIL datasets, with varying degrees of data heterogeneity, show that our method outperforms state-of-the-art FCIL methods in terms of performance, flexibility, and both communication and computation efficiency.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Large Language Models in Crisis Detection: A Real-World Benchmark from Psychological Support Hotlines</title>
<link>https://arxiv.org/abs/2506.01329</link>
<guid>https://arxiv.org/abs/2506.01329</guid>
<content:encoded><![CDATA[
arXiv:2506.01329v1 Announce Type: cross 
Abstract: Psychological support hotlines are critical for crisis intervention but face significant challenges due to rising demand. Large language models (LLMs) could support crisis assessments, yet their capabilities in emotionally sensitive contexts remain unclear. We introduce PsyCrisisBench, a benchmark of 540 annotated transcripts from the Hangzhou Psychological Assistance Hotline, assessing four tasks: mood status recognition, suicidal ideation detection, suicide plan identification, and risk assessment. We evaluated 64 LLMs across 15 families (e.g., GPT, Claude, Gemini, Llama, Qwen, DeepSeek) using zero-shot, few-shot, and fine-tuning paradigms. Performance was measured by F1-score, with statistical comparisons via Welch's t-tests. LLMs performed strongly on suicidal ideation detection (F1=0.880), suicide plan identification (F1=0.779), and risk assessment (F1=0.907), improved with few-shot and fine-tuning. Mood status recognition was more challenging (max F1=0.709), likely due to lost vocal cues and ambiguity. A fine-tuned 1.5B-parameter model (Qwen2.5-1.5B) surpassed larger models on mood and suicidal ideation. Open-source models like QwQ-32B performed comparably to closed-source on most tasks (p>0.3), though closed models retained an edge in mood detection (p=0.007). Performance scaled with size up to a point; quantization (AWQ) reduced GPU memory by 70% with minimal F1 degradation. LLMs show substantial promise in structured psychological crisis assessments, especially with fine-tuning. Mood recognition remains limited due to contextual complexity. The narrowing gap between open- and closed-source models, combined with efficient quantization, suggests feasible integration. PsyCrisisBench offers a robust evaluation framework to guide model development and ethical deployment in mental health.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ETDI: Mitigating Tool Squatting and Rug Pull Attacks in Model Context Protocol (MCP) by using OAuth-Enhanced Tool Definitions and Policy-Based Access Control</title>
<link>https://arxiv.org/abs/2506.01333</link>
<guid>https://arxiv.org/abs/2506.01333</guid>
<content:encoded><![CDATA[
arXiv:2506.01333v1 Announce Type: cross 
Abstract: The Model Context Protocol (MCP) plays a crucial role in extending the capabilities of Large Language Models (LLMs) by enabling integration with external tools and data sources. However, the standard MCP specification presents significant security vulnerabilities, notably Tool Poisoning and Rug Pull attacks. This paper introduces the Enhanced Tool Definition Interface (ETDI), a security extension designed to fortify MCP. ETDI incorporates cryptographic identity verification, immutable versioned tool definitions, and explicit permission management, often leveraging OAuth 2.0. We further propose extending MCP with fine-grained, policy-based access control, where tool capabilities are dynamically evaluated against explicit policies using a dedicated policy engine, considering runtime context beyond static OAuth scopes. This layered approach aims to establish a more secure, trustworthy, and controllable ecosystem for AI applications interacting with LLMs and external tools.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NoiseAR: AutoRegressing Initial Noise Prior for Diffusion Models</title>
<link>https://arxiv.org/abs/2506.01337</link>
<guid>https://arxiv.org/abs/2506.01337</guid>
<content:encoded><![CDATA[
arXiv:2506.01337v1 Announce Type: cross 
Abstract: Diffusion models have emerged as powerful generative frameworks, creating data samples by progressively denoising an initial random state. Traditionally, this initial state is sampled from a simple, fixed distribution like isotropic Gaussian, inherently lacking structure and a direct mechanism for external control. While recent efforts have explored ways to introduce controllability into the diffusion process, particularly at the initialization stage, they often rely on deterministic or heuristic approaches. These methods can be suboptimal, lack expressiveness, and are difficult to scale or integrate into more sophisticated optimization frameworks. In this paper, we introduce NoiseAR, a novel method for AutoRegressive Initial Noise Prior for Diffusion Models. Instead of a static, unstructured source, NoiseAR learns to generate a dynamic and controllable prior distribution for the initial noise. We formulate the generation of the initial noise prior's parameters as an autoregressive probabilistic modeling task over spatial patches or tokens. This approach enables NoiseAR to capture complex spatial dependencies and introduce learned structure into the initial state. Crucially, NoiseAR is designed to be conditional, allowing text prompts to directly influence the learned prior, thereby achieving fine-grained control over the diffusion initialization. Our experiments demonstrate that NoiseAR can generate initial noise priors that lead to improved sample quality and enhanced consistency with conditional inputs, offering a powerful, learned alternative to traditional random initialization. A key advantage of NoiseAR is its probabilistic formulation, which naturally supports seamless integration into probabilistic frameworks like Markov Decision Processes and Reinforcement Learning. Our code will be available at https://github.com/HKUST-SAIL/NoiseAR/
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KokoroChat: A Japanese Psychological Counseling Dialogue Dataset Collected via Role-Playing by Trained Counselors</title>
<link>https://arxiv.org/abs/2506.01357</link>
<guid>https://arxiv.org/abs/2506.01357</guid>
<content:encoded><![CDATA[
arXiv:2506.01357v1 Announce Type: cross 
Abstract: Generating psychological counseling responses with language models relies heavily on high-quality datasets. Crowdsourced data collection methods require strict worker training, and data from real-world counseling environments may raise privacy and ethical concerns. While recent studies have explored using large language models (LLMs) to augment psychological counseling dialogue datasets, the resulting data often suffers from limited diversity and authenticity. To address these limitations, this study adopts a role-playing approach where trained counselors simulate counselor-client interactions, ensuring high-quality dialogues while mitigating privacy risks. Using this method, we construct KokoroChat, a Japanese psychological counseling dialogue dataset comprising 6,589 long-form dialogues, each accompanied by comprehensive client feedback. Experimental results demonstrate that fine-tuning open-source LLMs with KokoroChat improves both the quality of generated counseling responses and the automatic evaluation of counseling dialogues. The KokoroChat dataset is available at https://github.com/UEC-InabaLab/KokoroChat.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unraveling Spatio-Temporal Foundation Models via the Pipeline Lens: A Comprehensive Review</title>
<link>https://arxiv.org/abs/2506.01364</link>
<guid>https://arxiv.org/abs/2506.01364</guid>
<content:encoded><![CDATA[
arXiv:2506.01364v1 Announce Type: cross 
Abstract: Spatio-temporal deep learning models aims to utilize useful patterns in such data to support tasks like prediction. However, previous deep learning models designed for specific tasks typically require separate training for each use case, leading to increased computational and storage costs. To address this issue, spatio-temporal foundation models have emerged, offering a unified framework capable of solving multiple spatio-temporal tasks. These foundation models achieve remarkable success by learning general knowledge with spatio-temporal data or transferring the general capabilities of pre-trained language models. While previous surveys have explored spatio-temporal data and methodologies separately, they have ignored a comprehensive examination of how foundation models are designed, selected, pre-trained, and adapted. As a result, the overall pipeline for spatio-temporal foundation models remains unclear. To bridge this gap, we innovatively provide an up-to-date review of previous spatio-temporal foundation models from the pipeline perspective. The pipeline begins with an introduction to different types of spatio-temporal data, followed by details of data preprocessing and embedding techniques. The pipeline then presents a novel data property taxonomy to divide existing methods according to data sources and dependencies, providing efficient and effective model design and selection for researchers. On this basis, we further illustrate the training objectives of primitive models, as well as the adaptation techniques of transferred models. Overall, our survey provides a clear and structured pipeline to understand the connection between core elements of spatio-temporal foundation models while guiding researchers to get started quickly. Additionally, we introduce emerging opportunities such as multi-objective training in the field of spatio-temporal foundation models.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incentivizing LLMs to Self-Verify Their Answers</title>
<link>https://arxiv.org/abs/2506.01369</link>
<guid>https://arxiv.org/abs/2506.01369</guid>
<content:encoded><![CDATA[
arXiv:2506.01369v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in complex reasoning tasks through both post-training and test-time scaling laws. While prevalent test-time scaling approaches are often realized by using external reward models to guide the model generation process, we find only marginal gains can be acquired when scaling a model post-trained on specific reasoning tasks. We identify that the limited improvement stems from distribution discrepancies between the specific post-trained generator and the general reward model. To address this, we propose a framework that incentivizes LLMs to self-verify their own answers. By unifying answer generation and verification within a single reinforcement learning (RL) process, we train models that can effectively assess the correctness of their own solutions. The trained model can further scale its performance during inference time by verifying its generations, without the need for external verifiers. We train our self-verification models based on Qwen2.5-Math-7B and DeepSeek-R1-Distill-Qwen-1.5B, demonstrating its capabilities across varying reasoning context lengths. Experiments on multiple mathematical reasoning benchmarks show that our models can not only improve post-training performance but also enable effective test-time scaling. Our code is available at https://github.com/mansicer/self-verification.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compiler Optimization via LLM Reasoning for Efficient Model Serving</title>
<link>https://arxiv.org/abs/2506.01374</link>
<guid>https://arxiv.org/abs/2506.01374</guid>
<content:encoded><![CDATA[
arXiv:2506.01374v1 Announce Type: cross 
Abstract: While model serving has unlocked unprecedented capabilities, the high cost of serving large-scale models continues to be a significant barrier to widespread accessibility and rapid innovation. Compiler optimizations have long driven substantial performance improvements, but existing compilers struggle with neural workloads due to the exponentially large and highly interdependent space of possible transformations. Although existing stochastic search techniques can be effective, they are often sample-inefficient and fail to leverage the structural context underlying compilation decisions. We set out to investigate the research question of whether reasoning with large language models (LLMs), without any retraining, can leverage the context-aware decision space of compiler optimization to significantly improve sample efficiency. To that end, we introduce a novel compilation framework (dubbed REASONING COMPILER) that formulates optimization as a sequential, context-aware decision process, guided by a large language model and structured Monte Carlo tree search (MCTS). The LLM acts as a proposal mechanism, suggesting hardware-aware transformations that reflect the current program state and accumulated performance feedback. Monte Carlo tree search (MCTS) incorporates the LLM-generated proposals to balance exploration and exploitation, facilitating structured, context-sensitive traversal of the expansive compiler optimization space. By achieving substantial speedups with markedly fewer samples than leading neural compilers, our approach demonstrates the potential of LLM-guided reasoning to transform the landscape of compiler optimization.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Playing with Transformer at 30+ FPS via Next-Frame Diffusion</title>
<link>https://arxiv.org/abs/2506.01380</link>
<guid>https://arxiv.org/abs/2506.01380</guid>
<content:encoded><![CDATA[
arXiv:2506.01380v1 Announce Type: cross 
Abstract: Autoregressive video models offer distinct advantages over bidirectional diffusion models in creating interactive video content and supporting streaming applications with arbitrary duration. In this work, we present Next-Frame Diffusion (NFD), an autoregressive diffusion transformer that incorporates block-wise causal attention, enabling iterative sampling and efficient inference via parallel token generation within each frame. Nonetheless, achieving real-time video generation remains a significant challenge for such models, primarily due to the high computational cost associated with diffusion sampling and the hardware inefficiencies inherent to autoregressive generation. To address this, we introduce two innovations: (1) We extend consistency distillation to the video domain and adapt it specifically for video models, enabling efficient inference with few sampling steps; (2) To fully leverage parallel computation, motivated by the observation that adjacent frames often share the identical action input, we propose speculative sampling. In this approach, the model generates next few frames using current action input, and discard speculatively generated frames if the input action differs. Experiments on a large-scale action-conditioned video generation benchmark demonstrate that NFD beats autoregressive baselines in terms of both visual quality and sampling efficiency. We, for the first time, achieves autoregressive video generation at over 30 Frames Per Second (FPS) on an A100 GPU using a 310M model.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VRD-IU: Lessons from Visually Rich Document Intelligence and Understanding</title>
<link>https://arxiv.org/abs/2506.01388</link>
<guid>https://arxiv.org/abs/2506.01388</guid>
<content:encoded><![CDATA[
arXiv:2506.01388v1 Announce Type: cross 
Abstract: Visually Rich Document Understanding (VRDU) has emerged as a critical field in document intelligence, enabling automated extraction of key information from complex documents across domains such as medical, financial, and educational applications. However, form-like documents pose unique challenges due to their complex layouts, multi-stakeholder involvement, and high structural variability. Addressing these issues, the VRD-IU Competition was introduced, focusing on extracting and localizing key information from multi-format forms within the Form-NLU dataset, which includes digital, printed, and handwritten documents. This paper presents insights from the competition, which featured two tracks: Track A, emphasizing entity-based key information retrieval, and Track B, targeting end-to-end key information localization from raw document images. With over 20 participating teams, the competition showcased various state-of-the-art methodologies, including hierarchical decomposition, transformer-based retrieval, multimodal feature fusion, and advanced object detection techniques. The top-performing models set new benchmarks in VRDU, providing valuable insights into document intelligence.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Imagination for Efficient Visual World Model Planning</title>
<link>https://arxiv.org/abs/2506.01392</link>
<guid>https://arxiv.org/abs/2506.01392</guid>
<content:encoded><![CDATA[
arXiv:2506.01392v1 Announce Type: cross 
Abstract: World model based planning has significantly improved decision-making in complex environments by enabling agents to simulate future states and make informed choices. However, ensuring the prediction accuracy of world models often demands substantial computational resources, posing a major challenge for real-time applications. This computational burden is particularly restrictive in robotics, where resources are severely constrained. To address this limitation, we propose a Sparse Imagination for Efficient Visual World Model Planning, which enhances computational efficiency by reducing the number of tokens processed during forward prediction. Our method leverages a sparsely trained vision-based world model based on transformers with randomized grouped attention strategy, allowing the model to adaptively adjust the number of tokens processed based on the computational resource. By enabling sparse imagination (rollout), our approach significantly accelerates planning while maintaining high control fidelity. Experimental results demonstrate that sparse imagination preserves task performance while dramatically improving inference efficiency, paving the way for the deployment of world models in real-time decision-making scenarios.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViTA-PAR: Visual and Textual Attribute Alignment with Attribute Prompting for Pedestrian Attribute Recognition</title>
<link>https://arxiv.org/abs/2506.01411</link>
<guid>https://arxiv.org/abs/2506.01411</guid>
<content:encoded><![CDATA[
arXiv:2506.01411v1 Announce Type: cross 
Abstract: The Pedestrian Attribute Recognition (PAR) task aims to identify various detailed attributes of an individual, such as clothing, accessories, and gender. To enhance PAR performance, a model must capture features ranging from coarse-grained global attributes (e.g., for identifying gender) to fine-grained local details (e.g., for recognizing accessories) that may appear in diverse regions. Recent research suggests that body part representation can enhance the model's robustness and accuracy, but these methods are often restricted to attribute classes within fixed horizontal regions, leading to degraded performance when attributes appear in varying or unexpected body locations. In this paper, we propose Visual and Textual Attribute Alignment with Attribute Prompting for Pedestrian Attribute Recognition, dubbed as ViTA-PAR, to enhance attribute recognition through specialized multimodal prompting and vision-language alignment. We introduce visual attribute prompts that capture global-to-local semantics, enabling diverse attribute representations. To enrich textual embeddings, we design a learnable prompt template, termed person and attribute context prompting, to learn person and attributes context. Finally, we align visual and textual attribute features for effective fusion. ViTA-PAR is validated on four PAR benchmarks, achieving competitive performance with efficient inference. We release our code and model at https://github.com/mlnjeongpark/ViTA-PAR.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>System Calls for Malware Detection and Classification: Methodologies and Applications</title>
<link>https://arxiv.org/abs/2506.01412</link>
<guid>https://arxiv.org/abs/2506.01412</guid>
<content:encoded><![CDATA[
arXiv:2506.01412v1 Announce Type: cross 
Abstract: As malware continues to become more complex and harder to detect, Malware Analysis needs to continue to evolve to stay one step ahead. One promising key area approach focuses on using system calls and API Calls, the core communication between user applications and the operating system and their kernels. These calls provide valuable insight into how software or programs behaves, making them an useful tool for spotting suspicious or harmful activity of programs and software. This chapter takes a deep down look at how system calls are used in malware detection and classification, covering techniques like static and dynamic analysis, as well as sandboxing. By combining these methods with advanced techniques like machine learning, statistical analysis, and anomaly detection, researchers can analyze system call patterns to tell the difference between normal and malicious behavior. The chapter also explores how these techniques are applied across different systems, including Windows, Linux, and Android, while also looking at the ways sophisticated malware tries to evade detection.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models</title>
<link>https://arxiv.org/abs/2506.01413</link>
<guid>https://arxiv.org/abs/2506.01413</guid>
<content:encoded><![CDATA[
arXiv:2506.01413v1 Announce Type: cross 
Abstract: Existing large language models (LLMs) face challenges of following complex instructions, especially when multiple constraints are present and organized in paralleling, chaining, and branching structures. One intuitive solution, namely chain-of-thought (CoT), is expected to universally improve capabilities of LLMs. However, we find that the vanilla CoT exerts a negative impact on performance due to its superficial reasoning pattern of simply paraphrasing the instructions. It fails to peel back the compositions of constraints for identifying their relationship across hierarchies of types and dimensions. To this end, we propose a systematic method to boost LLMs in dealing with complex instructions via incentivizing reasoning for test-time compute scaling. First, we stem from the decomposition of complex instructions under existing taxonomies and propose a reproducible data acquisition method. Second, we exploit reinforcement learning (RL) with verifiable rule-centric reward signals to cultivate reasoning specifically for instruction following. We address the shallow, non-essential nature of reasoning under complex instructions via sample-wise contrast for superior CoT enforcement. We also exploit behavior cloning of experts to facilitate steady distribution shift from fast-thinking LLMs to skillful reasoners. Extensive evaluations on seven comprehensive benchmarks confirm the validity of the proposed method, where a 1.5B LLM achieves 11.74% gains with performance comparable to a 8B LLM. Codes and data are available at https://github.com/yuleiqin/RAIF.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShaTS: A Shapley-based Explainability Method for Time Series Artificial Intelligence Models applied to Anomaly Detection in Industrial Internet of Things</title>
<link>https://arxiv.org/abs/2506.01450</link>
<guid>https://arxiv.org/abs/2506.01450</guid>
<content:encoded><![CDATA[
arXiv:2506.01450v1 Announce Type: cross 
Abstract: Industrial Internet of Things environments increasingly rely on advanced Anomaly Detection and explanation techniques to rapidly detect and mitigate cyberincidents, thereby ensuring operational safety. The sequential nature of data collected from these environments has enabled improvements in Anomaly Detection using Machine Learning and Deep Learning models by processing time windows rather than treating the data as tabular. However, conventional explanation methods often neglect this temporal structure, leading to imprecise or less actionable explanations. This work presents ShaTS (Shapley values for Time Series models), which is a model-agnostic explainable Artificial Intelligence method designed to enhance the precision of Shapley value explanations for time series models. ShaTS addresses the shortcomings of traditional approaches by incorporating an a priori feature grouping strategy that preserves temporal dependencies and produces both coherent and actionable insights. Experiments conducted on the SWaT dataset demonstrate that ShaTS accurately identifies critical time instants, precisely pinpoints the sensors, actuators, and processes affected by anomalies, and outperforms SHAP in terms of both explainability and resource efficiency, fulfilling the real-time requirements of industrial environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Initial Data to Boundary Layers: Neural Networks for Nonlinear Hyperbolic Conservation Laws</title>
<link>https://arxiv.org/abs/2506.01453</link>
<guid>https://arxiv.org/abs/2506.01453</guid>
<content:encoded><![CDATA[
arXiv:2506.01453v1 Announce Type: cross 
Abstract: We address the approximation of entropy solutions to initial-boundary value problems for nonlinear strictly hyperbolic conservation laws using neural networks. A general and systematic framework is introduced for the design of efficient and reliable learning algorithms, combining fast convergence during training with accurate predictions. The methodology is assessed through a series of one-dimensional scalar test cases, highlighting its potential applicability to more complex industrial scenarios.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenDMR: A dynamic multimodal role-swapping network for identifying risk gene phenotypes</title>
<link>https://arxiv.org/abs/2506.01456</link>
<guid>https://arxiv.org/abs/2506.01456</guid>
<content:encoded><![CDATA[
arXiv:2506.01456v1 Announce Type: cross 
Abstract: Recent studies have shown that integrating multimodal data fusion techniques for imaging and genetic features is beneficial for the etiological analysis and predictive diagnosis of Alzheimer's disease (AD). However, there are several critical flaws in current deep learning methods. Firstly, there has been insufficient discussion and exploration regarding the selection and encoding of genetic information. Secondly, due to the significantly superior classification value of AD imaging features compared to genetic features, many studies in multimodal fusion emphasize the strengths of imaging features, actively mitigating the influence of weaker features, thereby diminishing the learning of the unique value of genetic features. To address this issue, this study proposes the dynamic multimodal role-swapping network (GenDMR). In GenDMR, we develop a novel approach to encode the spatial organization of single nucleotide polymorphisms (SNPs), enhancing the representation of their genomic context. Additionally, to adaptively quantify the disease risk of SNPs and brain region, we propose a multi-instance attention module to enhance model interpretability. Furthermore, we introduce a dominant modality selection module and a contrastive self-distillation module, combining them to achieve a dynamic teacher-student role exchange mechanism based on dominant and auxiliary modalities for bidirectional co-updating of different modal data. Finally, GenDMR achieves state-of-the-art performance on the ADNI public dataset and visualizes attention to different SNPs, focusing on confirming 12 potential high-risk genes related to AD, including the most classic APOE and recently highlighted significant risk genes. This demonstrates GenDMR's interpretable analytical capability in exploring AD genetic features, providing new insights and perspectives for the development of multimodal data fusion techniques.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic AI and Multiagentic: Are We Reinventing the Wheel?</title>
<link>https://arxiv.org/abs/2506.01463</link>
<guid>https://arxiv.org/abs/2506.01463</guid>
<content:encoded><![CDATA[
arXiv:2506.01463v1 Announce Type: cross 
Abstract: The terms Agentic AI and Multiagentic AI have recently gained popularity in discussions on generative artificial intelligence, often used to describe autonomous software agents and systems composed of such agents. However, the use of these terms confuses these buzzwords with well-established concepts in AI literature: intelligent agents and multi-agent systems. This article offers a critical analysis of this conceptual misuse. We review the theoretical origins of "agentic" in the social sciences (Bandura, 1986) and philosophical notions of intentionality (Dennett, 1971), and then summarise foundational works on intelligent agents and multi-agent systems by Wooldridge, Jennings and others. We examine classic agent architectures, from simple reactive agents to Belief-Desire-Intention (BDI) models, and highlight key properties (autonomy, reactivity, proactivity, social capability) that define agency in AI. We then discuss recent developments in large language models (LLMs) and agent platforms based on LLMs, including the emergence of LLM-powered AI agents and open-source multi-agent orchestration frameworks. We argue that the term AI Agentic is often used as a buzzword for what are essentially AI agents, and AI Multiagentic for what are multi-agent systems. This confusion overlooks decades of research in the field of autonomous agents and multi-agent systems. The article advocates for scientific and technological rigour and the use of established terminology from the state of the art in AI, incorporating the wealth of existing knowledge, including standards for multi-agent system platforms, communication languages and coordination and cooperation algorithms, agreement technologies (automated negotiation, argumentation, virtual organisations, trust, reputation, etc.), into the new and promising wave of LLM-based AI agents, so as not to end up reinventing the wheel.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Stage Lighting Control: Is it a Rule-Driven Process or Generative Task?</title>
<link>https://arxiv.org/abs/2506.01482</link>
<guid>https://arxiv.org/abs/2506.01482</guid>
<content:encoded><![CDATA[
arXiv:2506.01482v1 Announce Type: cross 
Abstract: Stage lighting plays an essential role in live music performances, influencing the engaging experience of both musicians and audiences. Given the high costs associated with hiring or training professional lighting engineers, Automatic Stage Lighting Control (ASLC) has gained increasing attention. However, most existing approaches only classify music into limited categories and map them to predefined light patterns, resulting in formulaic and monotonous outcomes that lack rationality. To address this issue, this paper presents an end-to-end solution that directly learns from experienced lighting engineers -- Skip-BART. To the best of our knowledge, this is the first work to conceptualize ASLC as a generative task rather than merely a classification problem. Our method modifies the BART model to take audio music as input and produce light hue and value (intensity) as output, incorporating a novel skip connection mechanism to enhance the relationship between music and light within the frame grid.We validate our method through both quantitative analysis and an human evaluation, demonstrating that Skip-BART outperforms conventional rule-based methods across all evaluation metrics and shows only a limited gap compared to real lighting engineers.Specifically, our method yields a p-value of 0.72 in a statistical comparison based on human evaluations with human lighting engineers, suggesting that the proposed approach closely matches human lighting engineering performance. To support further research, we have made our self-collected dataset, code, and trained model parameters available at https://github.com/RS2002/Skip-BART .
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning of Population Dynamics: Inverse Optimization Meets JKO Scheme</title>
<link>https://arxiv.org/abs/2506.01502</link>
<guid>https://arxiv.org/abs/2506.01502</guid>
<content:encoded><![CDATA[
arXiv:2506.01502v1 Announce Type: cross 
Abstract: Learning population dynamics involves recovering the underlying process that governs particle evolution, given evolutionary snapshots of samples at discrete time points. Recent methods frame this as an energy minimization problem in probability space and leverage the celebrated JKO scheme for efficient time discretization. In this work, we introduce $\texttt{iJKOnet}$, an approach that combines the JKO framework with inverse optimization techniques to learn population dynamics. Our method relies on a conventional $\textit{end-to-end}$ adversarial training procedure and does not require restrictive architectural choices, e.g., input-convex neural networks. We establish theoretical guarantees for our methodology and demonstrate improved performance over prior JKO-based methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representations of Fact, Fiction and Forecast in Large Language Models: Epistemics and Attitudes</title>
<link>https://arxiv.org/abs/2506.01512</link>
<guid>https://arxiv.org/abs/2506.01512</guid>
<content:encoded><![CDATA[
arXiv:2506.01512v1 Announce Type: cross 
Abstract: Rational speakers are supposed to know what they know and what they do not know, and to generate expressions matching the strength of evidence. In contrast, it is still a challenge for current large language models to generate corresponding utterances based on the assessment of facts and confidence in an uncertain real-world environment. While it has recently become popular to estimate and calibrate confidence of LLMs with verbalized uncertainty, what is lacking is a careful examination of the linguistic knowledge of uncertainty encoded in the latent space of LLMs. In this paper, we draw on typological frameworks of epistemic expressions to evaluate LLMs' knowledge of epistemic modality, using controlled stories. Our experiments show that the performance of LLMs in generating epistemic expressions is limited and not robust, and hence the expressions of uncertainty generated by LLMs are not always reliable. To build uncertainty-aware LLMs, it is necessary to enrich semantic knowledge of epistemic modality in LLMs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>V-VAE: A Variational Auto Encoding Framework Towards Fine-Grained Control over Human-Like Chat</title>
<link>https://arxiv.org/abs/2506.01524</link>
<guid>https://arxiv.org/abs/2506.01524</guid>
<content:encoded><![CDATA[
arXiv:2506.01524v1 Announce Type: cross 
Abstract: With the continued proliferation of Large Language Model (LLM) based chatbots, there is a growing demand for generating responses that are not only linguistically fluent but also consistently aligned with persona-specific traits in conversations. However, existing role-play and persona-based chat approaches rely heavily on static role descriptions, coarse-grained signal space, and low-quality synthetic data, which fail to capture dynamic fine-grained details in human-like chat. Human-like chat requires modeling subtle latent traits, such as emotional tone, situational awareness, and evolving personality, which are difficult to predefine and cannot be easily learned from synthetic or distillation-based data. To address these limitations, we propose a Verbal Variational Auto-Encoding (V-VAE) framework, containing a variational auto-encoding module and fine-grained control space which dynamically adapts dialogue behaviour based on fine-grained, interpretable latent variables across talking style, interaction patterns, and personal attributes. We also construct a high-quality dataset, HumanChatData, and benchmark HumanChatBench to address the scarcity of high-quality data in the human-like domain. Experiments show that LLMs based on V-VAE consistently outperform standard baselines on HumanChatBench and DialogBench, which further demonstrates the effectiveness of V-VAE and HumanChatData.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Diffusion-Based Method for Learning the Multi-Outcome Distribution of Medical Treatments</title>
<link>https://arxiv.org/abs/2506.01533</link>
<guid>https://arxiv.org/abs/2506.01533</guid>
<content:encoded><![CDATA[
arXiv:2506.01533v1 Announce Type: cross 
Abstract: In medicine, treatments often influence multiple, interdependent outcomes, such as primary endpoints, complications, adverse events, or other secondary endpoints. Hence, to make optimal treatment decisions, clinicians are interested in learning the distribution of multi-dimensional treatment outcomes. However, the vast majority of machine learning methods for predicting treatment effects focus on single-outcome settings, despite the fact that medical data often include multiple, interdependent outcomes. To address this limitation, we propose a novel diffusion-based method called DIME to learn the joint distribution of multiple outcomes of medical treatments. We addresses three challenges relevant in medical practice: (i)it is tailored to learn the joint interventional distribution of multiple medical outcomes, which enables reliable decision-making with uncertainty quantification rather than relying solely on point estimates; (ii)it explicitly captures the dependence structure between outcomes; (iii)it can handle outcomes of mixed type, including binary, categorical, and continuous variables. In DIME, we take into account the fundamental problem of causal inference through causal masking. For training, our method decomposes the joint distribution into a series of conditional distributions with a customized conditional masking to account for the dependence structure across outcomes. For inference, our method auto-regressively generates predictions. This allows our method to move beyond point estimates of causal quantities and thus learn the joint interventional distribution. To the best of our knowledge, DIME is the first neural method tailored to learn the joint, multi-outcome distribution of medical treatments. Across various experiments, we demonstrate that our method effectively learns the joint distribution and captures shared information among multiple outcomes.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dictionaries to the Rescue: Cross-Lingual Vocabulary Transfer for Low-Resource Languages Using Bilingual Dictionaries</title>
<link>https://arxiv.org/abs/2506.01535</link>
<guid>https://arxiv.org/abs/2506.01535</guid>
<content:encoded><![CDATA[
arXiv:2506.01535v1 Announce Type: cross 
Abstract: Cross-lingual vocabulary transfer plays a promising role in adapting pre-trained language models to new languages, including low-resource languages. Existing approaches that utilize monolingual or parallel corpora face challenges when applied to languages with limited resources. In this work, we propose a simple yet effective vocabulary transfer method that utilizes bilingual dictionaries, which are available for many languages, thanks to descriptive linguists. Our proposed method leverages a property of BPE tokenizers where removing a subword from the vocabulary causes a fallback to shorter subwords. The embeddings of target subwords are estimated iteratively by progressively removing them from the tokenizer. The experimental results show that our approach outperforms existing methods for low-resource languages, demonstrating the effectiveness of a dictionary-based approach for cross-lingual vocabulary transfer.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LAMARL: LLM-Aided Multi-Agent Reinforcement Learning for Cooperative Policy Generation</title>
<link>https://arxiv.org/abs/2506.01538</link>
<guid>https://arxiv.org/abs/2506.01538</guid>
<content:encoded><![CDATA[
arXiv:2506.01538v1 Announce Type: cross 
Abstract: Although Multi-Agent Reinforcement Learning (MARL) is effective for complex multi-robot tasks, it suffers from low sample efficiency and requires iterative manual reward tuning. Large Language Models (LLMs) have shown promise in single-robot settings, but their application in multi-robot systems remains largely unexplored. This paper introduces a novel LLM-Aided MARL (LAMARL) approach, which integrates MARL with LLMs, significantly enhancing sample efficiency without requiring manual design. LAMARL consists of two modules: the first module leverages LLMs to fully automate the generation of prior policy and reward functions. The second module is MARL, which uses the generated functions to guide robot policy training effectively. On a shape assembly benchmark, both simulation and real-world experiments demonstrate the unique advantages of LAMARL. Ablation studies show that the prior policy improves sample efficiency by an average of 185.9% and enhances task completion, while structured prompts based on Chain-of-Thought (CoT) and basic APIs improve LLM output success rates by 28.5%-67.5%. Videos and code are available at https://guobin-zhu.github.io/LLM-MARL
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>G4Seg: Generation for Inexact Segmentation Refinement with Diffusion Models</title>
<link>https://arxiv.org/abs/2506.01539</link>
<guid>https://arxiv.org/abs/2506.01539</guid>
<content:encoded><![CDATA[
arXiv:2506.01539v1 Announce Type: cross 
Abstract: This paper considers the problem of utilizing a large-scale text-to-image diffusion model to tackle the challenging Inexact Segmentation (IS) task. Unlike traditional approaches that rely heavily on discriminative-model-based paradigms or dense visual representations derived from internal attention mechanisms, our method focuses on the intrinsic generative priors in Stable Diffusion~(SD). Specifically, we exploit the pattern discrepancies between original images and mask-conditional generated images to facilitate a coarse-to-fine segmentation refinement by establishing a semantic correspondence alignment and updating the foreground probability. Comprehensive quantitative and qualitative experiments validate the effectiveness and superiority of our plug-and-play design, underscoring the potential of leveraging generation discrepancies to model dense representations and encouraging further exploration of generative approaches for solving discriminative tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvolveNav: Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation</title>
<link>https://arxiv.org/abs/2506.01551</link>
<guid>https://arxiv.org/abs/2506.01551</guid>
<content:encoded><![CDATA[
arXiv:2506.01551v1 Announce Type: cross 
Abstract: Building Vision-Language Navigation (VLN) agents which can navigate following natural language instructions is a long-standing goal in human-robot interaction applications. Recent studies have revealed the potential of training open-source Large Language Models (LLMs) to unleash LLMs' reasoning ability for improving navigation, and simultaneously mitigate the domain gap between LLMs' training corpus and the VLN task. However, these approaches primarily adopt direct input-output mapping paradigms, causing the mapping learning difficult and the navigational decisions unexplainable. Chain-of-Thought (CoT) training is a promising way to improve both navigational decision accuracy and interpretability, while the complexity of the navigation task makes the perfect CoT labels unavailable and may lead to overfitting through pure CoT supervised fine-tuning. In this paper, we propose a novel sElf-improving embodied reasoning framework for boosting LLM-based vision-language Navigation, dubbed EvolveNav. Our EvolveNav consists of two stages: (1) Formalized CoT Supervised Fine-Tuning, where we train the model with formalized CoT labels to both activate the model's navigational reasoning capabilities and increase the reasoning speed; (2) Self-Reflective Post-Training, where the model is iteratively trained with its own reasoning outputs as self-enriched CoT labels to enhance the supervision diversity. A self-reflective auxiliary task is also introduced to encourage learning correct reasoning patterns by contrasting with wrong ones. Experimental results on the popular VLN benchmarks demonstrate the superiority of EvolveNav over previous LLM-based VLN approaches. Code is available at https://github.com/expectorlin/EvolveNav.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlexiSAGA: A Flexible Systolic Array GEMM Accelerator for Sparse and Dense Processing</title>
<link>https://arxiv.org/abs/2506.01566</link>
<guid>https://arxiv.org/abs/2506.01566</guid>
<content:encoded><![CDATA[
arXiv:2506.01566v1 Announce Type: cross 
Abstract: Artificial Intelligence (AI) algorithms, such as Deep Neural Networks (DNNs), have become an important tool for a wide range of applications, from computer vision to natural language processing. However, the computational complexity of DNN inference poses a significant challenge, particularly for processing on resource-constrained edge devices. One promising approach to address this challenge is the exploitation of sparsity in DNN operator weights.
  In this work, we present FlexiSAGA, an architecturally configurable and dataflow-flexible AI hardware accelerator for the sparse and dense processing of general matrix multiplications (GEMMs). FlexiSAGA supports seven different sparse and dense dataflows, enabling efficient processing of resource intensive DNN operators. Additionally, we propose a DNN pruning method specifically tailored towards the FlexiSAGA architecture, allowing for near-optimal processing of dense and sparse convolution and fully-connected operators, facilitating a DNN/HW co-design flow. Our results show a whole DNN sparse-over-dense inference speedup ranging from 1.41 up to 4.28, outperforming commercial and literature-reported accelerator platforms.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Nanostructured Topical Therapeutics for Psoriasis: Strategic Synthesis, Multimodal Characterization, and Preliminary Pharmacodynamic Profiling</title>
<link>https://arxiv.org/abs/2506.01572</link>
<guid>https://arxiv.org/abs/2506.01572</guid>
<content:encoded><![CDATA[
arXiv:2506.01572v1 Announce Type: cross 
Abstract: Psoriasis is a long-term inflammatory skin disease that remains difficult to treat. In this study, we developed a new topical treatment by combining metal oxide nanoparticles: cerium oxide (CeO2), zinc oxide (ZnO), and silver (Ag), with natural plant extracts in a gel made from fish collagen and agar. The nanoparticles were characterized using UV-Vis spectroscopy, dynamic light scattering (DLS), Fourier-transform infrared spectroscopy (FTIR), and scanning electron microscopy (SEM), showing good stability and a uniform particle size distribution (ZnO averaged 66 nm).
  To enhance therapeutic potential, the gel was enriched with plant-derived antioxidants from bitter melon, ginger, and neem. This formulation was tested on an animal model of psoriasis. The treated group exhibited faster wound healing and reduced inflammation compared to both placebo and untreated groups, with statistically significant results (p < 0.01 to p < 0.001) observed from Day 3, becoming more pronounced by Day 14.
  These results indicate that the combination of nanoparticles with plant-based components in a topical gel may provide a promising new approach to psoriasis treatment. Further studies are recommended to evaluate long-term safety and therapeutic effectiveness.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FreqPolicy: Frequency Autoregressive Visuomotor Policy with Continuous Tokens</title>
<link>https://arxiv.org/abs/2506.01583</link>
<guid>https://arxiv.org/abs/2506.01583</guid>
<content:encoded><![CDATA[
arXiv:2506.01583v1 Announce Type: cross 
Abstract: Learning effective visuomotor policies for robotic manipulation is challenging, as it requires generating precise actions while maintaining computational efficiency. Existing methods remain unsatisfactory due to inherent limitations in the essential action representation and the basic network architectures. We observe that representing actions in the frequency domain captures the structured nature of motion more effectively: low-frequency components reflect global movement patterns, while high-frequency components encode fine local details. Additionally, robotic manipulation tasks of varying complexity demand different levels of modeling precision across these frequency bands. Motivated by this, we propose a novel paradigm for visuomotor policy learning that progressively models hierarchical frequency components. To further enhance precision, we introduce continuous latent representations that maintain smoothness and continuity in the action space. Extensive experiments across diverse 2D and 3D robotic manipulation benchmarks demonstrate that our approach outperforms existing methods in both accuracy and efficiency, showcasing the potential of a frequency-domain autoregressive framework with continuous tokens for generalized robotic manipulation.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VirnyFlow: A Design Space for Responsible Model Development</title>
<link>https://arxiv.org/abs/2506.01584</link>
<guid>https://arxiv.org/abs/2506.01584</guid>
<content:encoded><![CDATA[
arXiv:2506.01584v1 Announce Type: cross 
Abstract: Developing machine learning (ML) models requires a deep understanding of real-world problems, which are inherently multi-objective. In this paper, we present VirnyFlow, the first design space for responsible model development, designed to assist data scientists in building ML pipelines that are tailored to the specific context of their problem. Unlike conventional AutoML frameworks, VirnyFlow enables users to define customized optimization criteria, perform comprehensive experimentation across pipeline stages, and iteratively refine models in alignment with real-world constraints. Our system integrates evaluation protocol definition, multi-objective Bayesian optimization, cost-aware multi-armed bandits, query optimization, and distributed parallelism into a unified architecture. We show that VirnyFlow significantly outperforms state-of-the-art AutoML systems in both optimization quality and scalability across five real-world benchmarks, offering a flexible, efficient, and responsible alternative to black-box automation in ML development.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding and Improving Laplacian Positional Encodings For Temporal GNNs</title>
<link>https://arxiv.org/abs/2506.01596</link>
<guid>https://arxiv.org/abs/2506.01596</guid>
<content:encoded><![CDATA[
arXiv:2506.01596v1 Announce Type: cross 
Abstract: Temporal graph learning has applications in recommendation systems, traffic forecasting, and social network analysis. Although multiple architectures have been introduced, progress in positional encoding for temporal graphs remains limited. Extending static Laplacian eigenvector approaches to temporal graphs through the supra-Laplacian has shown promise, but also poses key challenges: high eigendecomposition costs, limited theoretical understanding, and ambiguity about when and how to apply these encodings. In this paper, we address these issues by (1) offering a theoretical framework that connects supra-Laplacian encodings to per-time-slice encodings, highlighting the benefits of leveraging additional temporal connectivity, (2) introducing novel methods to reduce the computational overhead, achieving up to 56x faster runtimes while scaling to graphs with 50,000 active nodes, and (3) conducting an extensive experimental study to identify which models, tasks, and datasets benefit most from these encodings. Our findings reveal that while positional encodings can significantly boost performance in certain scenarios, their effectiveness varies across different models.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Policy Newton Algorithm in Reproducing Kernel Hilbert Space</title>
<link>https://arxiv.org/abs/2506.01597</link>
<guid>https://arxiv.org/abs/2506.01597</guid>
<content:encoded><![CDATA[
arXiv:2506.01597v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) policies represented in Reproducing Kernel Hilbert Spaces (RKHS) offer powerful representational capabilities. While second-order optimization methods like Newton's method demonstrate faster convergence than first-order approaches, current RKHS-based policy optimization remains constrained to first-order techniques. This limitation stems primarily from the intractability of explicitly computing and inverting the infinite-dimensional Hessian operator in RKHS. We introduce Policy Newton in RKHS, the first second-order optimization framework specifically designed for RL policies represented in RKHS. Our approach circumvents direct computation of the inverse Hessian operator by optimizing a cubic regularized auxiliary objective function. Crucially, we leverage the Representer Theorem to transform this infinite-dimensional optimization into an equivalent, computationally tractable finite-dimensional problem whose dimensionality scales with the trajectory data volume. We establish theoretical guarantees proving convergence to a local optimum with a local quadratic convergence rate. Empirical evaluations on a toy financial asset allocation problem validate these theoretical properties, while experiments on standard RL benchmarks demonstrate that Policy Newton in RKHS achieves superior convergence speed and higher episodic rewards compared to established first-order RKHS approaches and parametric second-order methods. Our work bridges a critical gap between non-parametric policy representations and second-order optimization methods in reinforcement learning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WoMAP: World Models For Embodied Open-Vocabulary Object Localization</title>
<link>https://arxiv.org/abs/2506.01600</link>
<guid>https://arxiv.org/abs/2506.01600</guid>
<content:encoded><![CDATA[
arXiv:2506.01600v1 Announce Type: cross 
Abstract: Language-instructed active object localization is a critical challenge for robots, requiring efficient exploration of partially observable environments. However, state-of-the-art approaches either struggle to generalize beyond demonstration datasets (e.g., imitation learning methods) or fail to generate physically grounded actions (e.g., VLMs). To address these limitations, we introduce WoMAP (World Models for Active Perception): a recipe for training open-vocabulary object localization policies that: (i) uses a Gaussian Splatting-based real-to-sim-to-real pipeline for scalable data generation without the need for expert demonstrations, (ii) distills dense rewards signals from open-vocabulary object detectors, and (iii) leverages a latent world model for dynamics and rewards prediction to ground high-level action proposals at inference time. Rigorous simulation and hardware experiments demonstrate WoMAP's superior performance in a broad range of zero-shot object localization tasks, with more than 9x and 2x higher success rates compared to VLM and diffusion policy baselines, respectively. Further, we show that WoMAP achieves strong generalization and sim-to-real transfer on a TidyBot.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EPFL-Smart-Kitchen-30: Densely annotated cooking dataset with 3D kinematics to challenge video and language models</title>
<link>https://arxiv.org/abs/2506.01608</link>
<guid>https://arxiv.org/abs/2506.01608</guid>
<content:encoded><![CDATA[
arXiv:2506.01608v1 Announce Type: cross 
Abstract: Understanding behavior requires datasets that capture humans while carrying out complex tasks. The kitchen is an excellent environment for assessing human motor and cognitive function, as many complex actions are naturally exhibited in kitchens from chopping to cleaning. Here, we introduce the EPFL-Smart-Kitchen-30 dataset, collected in a noninvasive motion capture platform inside a kitchen environment. Nine static RGB-D cameras, inertial measurement units (IMUs) and one head-mounted HoloLens~2 headset were used to capture 3D hand, body, and eye movements. The EPFL-Smart-Kitchen-30 dataset is a multi-view action dataset with synchronized exocentric, egocentric, depth, IMUs, eye gaze, body and hand kinematics spanning 29.7 hours of 16 subjects cooking four different recipes. Action sequences were densely annotated with 33.78 action segments per minute. Leveraging this multi-modal dataset, we propose four benchmarks to advance behavior understanding and modeling through 1) a vision-language benchmark, 2) a semantic text-to-motion generation benchmark, 3) a multi-modal action recognition benchmark, 4) a pose-based action segmentation benchmark. We expect the EPFL-Smart-Kitchen-30 dataset to pave the way for better methods as well as insights to understand the nature of ecologically-valid human behavior. Code and data are available at https://github.com/amathislab/EPFL-Smart-Kitchen
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Learning for Efficient Transaction Validation in UTXO-based Blockchains</title>
<link>https://arxiv.org/abs/2506.01614</link>
<guid>https://arxiv.org/abs/2506.01614</guid>
<content:encoded><![CDATA[
arXiv:2506.01614v1 Announce Type: cross 
Abstract: This paper introduces a Machine Learning (ML) approach for scalability of UTXO-based blockchains, such as Bitcoin. Prior approaches to UTXO set sharding struggle with distributing UTXOs effectively across validators, creating substantial communication overhead due to child-parent transaction dependencies. This overhead, which arises from the need to locate parent UTXOs, significantly hampers transaction processing speeds. Our solution uses ML to optimize not only UTXO set sharding but also the routing of incoming transactions, ensuring that transactions are directed to shards containing their parent UTXOs. At the heart of our approach is a framework that combines contrastive and unsupervised learning to create an embedding space for transaction outputs. This embedding allows the model to group transaction outputs based on spending relationships, making it possible to route transactions efficiently to the correct validation microservices. Trained on historical transaction data with triplet loss and online semi-hard negative mining, the model embeds parent-child spending patterns directly into its parameters, thus eliminating the need for costly, real-time parent transaction lookups. This significantly reduces cross-shard communication overhead, boosting throughput and scalability.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Rhythm and Voice Conversion to Improve ASR on Dysarthric Speech</title>
<link>https://arxiv.org/abs/2506.01618</link>
<guid>https://arxiv.org/abs/2506.01618</guid>
<content:encoded><![CDATA[
arXiv:2506.01618v1 Announce Type: cross 
Abstract: Automatic speech recognition (ASR) systems struggle with dysarthric speech due to high inter-speaker variability and slow speaking rates. To address this, we explore dysarthric-to-healthy speech conversion for improved ASR performance. Our approach extends the Rhythm and Voice (RnV) conversion framework by introducing a syllable-based rhythm modeling method suited for dysarthric speech. We assess its impact on ASR by training LF-MMI models and fine-tuning Whisper on converted speech. Experiments on the Torgo corpus reveal that LF-MMI achieves significant word error rate reductions, especially for more severe cases of dysarthria, while fine-tuning Whisper on converted data has minimal effect on its performance. These results highlight the potential of unsupervised rhythm and voice conversion for dysarthric ASR. Code available at: https://github.com/idiap/RnV
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Satisficing Gaussian Process Bandits Under Adversarial Attacks</title>
<link>https://arxiv.org/abs/2506.01625</link>
<guid>https://arxiv.org/abs/2506.01625</guid>
<content:encoded><![CDATA[
arXiv:2506.01625v1 Announce Type: cross 
Abstract: We address the problem of Gaussian Process (GP) optimization in the presence of unknown and potentially varying adversarial perturbations. Unlike traditional robust optimization approaches that focus on maximizing performance under worst-case scenarios, we consider a robust satisficing objective, where the goal is to consistently achieve a predefined performance threshold $\tau$, even under adversarial conditions. We propose two novel algorithms based on distinct formulations of robust satisficing, and show that they are instances of a general robust satisficing framework. Further, each algorithm offers different guarantees depending on the nature of the adversary. Specifically, we derive two regret bounds: one that is sublinear over time, assuming certain conditions on the adversary and the satisficing threshold $\tau$, and another that scales with the perturbation magnitude but requires no assumptions on the adversary. Through extensive experiments, we demonstrate that our approach outperforms the established robust optimization methods in achieving the satisficing objective, particularly when the ambiguity set of the robust optimization framework is inaccurately specified.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradient-Based Model Fingerprinting for LLM Similarity Detection and Family Classification</title>
<link>https://arxiv.org/abs/2506.01631</link>
<guid>https://arxiv.org/abs/2506.01631</guid>
<content:encoded><![CDATA[
arXiv:2506.01631v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) become integral software components in modern applications, unauthorized model derivations through fine-tuning, merging, and redistribution have emerged as critical software engineering challenges. Unlike traditional software where clone detection and license compliance are well-established, the LLM ecosystem lacks effective mechanisms to detect model lineage and enforce licensing agreements. This gap is particularly problematic when open-source model creators, such as Meta's LLaMA, require derivative works to maintain naming conventions for attribution, yet no technical means exist to verify compliance.
  To fill this gap, treating LLMs as software artifacts requiring provenance tracking, we present TensorGuard, a gradient-based fingerprinting framework for LLM similarity detection and family classification. Our approach extracts model-intrinsic behavioral signatures by analyzing gradient responses to random input perturbations across tensor layers, operating independently of training data, watermarks, or specific model formats. TensorGuard supports the widely-adopted safetensors format and constructs high-dimensional fingerprints through statistical analysis of gradient features. These fingerprints enable two complementary capabilities: direct pairwise similarity assessment between arbitrary models through distance computation, and systematic family classification of unknown models via the K-Means clustering algorithm with domain-informed centroid initialization using known base models. Experimental evaluation on 58 models comprising 8 base models and 50 derivatives across five model families (Llama, Qwen, Gemma, Phi, Mistral) demonstrates 94% classification accuracy under our centroid-initialized K-Means clustering.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bidirectional Soft Actor-Critic: Leveraging Forward and Reverse KL Divergence for Efficient Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.01639</link>
<guid>https://arxiv.org/abs/2506.01639</guid>
<content:encoded><![CDATA[
arXiv:2506.01639v1 Announce Type: cross 
Abstract: The Soft Actor-Critic (SAC) algorithm, a state-of-the-art method in maximum entropy reinforcement learning, traditionally relies on minimizing reverse Kullback-Leibler (KL) divergence for policy updates. However, this approach leads to an intractable optimal projection policy, necessitating gradient-based approximations that can suffer from instability and poor sample efficiency. This paper investigates the alternative use of forward KL divergence within SAC. We demonstrate that for Gaussian policies, forward KL divergence yields an explicit optimal projection policy -- corresponding to the mean and variance of the target Boltzmann distribution's action marginals. Building on the distinct advantages of both KL directions, we propose Bidirectional SAC, an algorithm that first initializes the policy using the explicit forward KL projection and then refines it by optimizing the reverse KL divergence. Comprehensive experiments on continuous control benchmarks show that Bidirectional SAC significantly outperforms standard SAC and other baselines, achieving up to a $30\%$ increase in episodic rewards, alongside enhanced sample efficiency.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ESGenius: Benchmarking LLMs on Environmental, Social, and Governance (ESG) and Sustainability Knowledge</title>
<link>https://arxiv.org/abs/2506.01646</link>
<guid>https://arxiv.org/abs/2506.01646</guid>
<content:encoded><![CDATA[
arXiv:2506.01646v1 Announce Type: cross 
Abstract: We introduce ESGenius, a comprehensive benchmark for evaluating and enhancing the proficiency of Large Language Models (LLMs) in Environmental, Social and Governance (ESG) and sustainability-focused question answering. ESGenius comprises two key components: (i) ESGenius-QA, a collection of 1 136 multiple-choice questions generated by LLMs and rigorously validated by domain experts, covering a broad range of ESG pillars and sustainability topics. Each question is systematically linked to its corresponding source text, enabling transparent evaluation and supporting retrieval-augmented generation (RAG) methods; and (ii) ESGenius-Corpus, a meticulously curated repository of 231 foundational frameworks, standards, reports and recommendation documents from seven authoritative sources. Moreover, to fully assess the capabilities and adaptation potential of the model, we implement a rigorous two-stage evaluation protocol -- Zero-Shot and RAG. Extensive experiments across 50 LLMs (ranging from 0.5 B to 671 B parameters) demonstrate that state-of-the-art models achieve only moderate performance in zero-shot settings, with accuracies typically around 55--70\%, highlighting ESGenius's challenging nature for LLMs in interdisciplinary contexts. However, models employing RAG show significant performance improvements, particularly for smaller models. For example, "DeepSeek-R1-Distill-Qwen-14B" improves from 63.82\% (zero-shot) to 80.46\% with RAG. These results underscore the necessity of grounding responses in authoritative sources for enhanced ESG understanding. To the best of our knowledge, ESGenius is the first benchmark curated for LLMs and the relevant enhancement technologies that focuses on ESG and sustainability topics.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Engram Memory Encoding and Retrieval: A Neurocomputational Perspective</title>
<link>https://arxiv.org/abs/2506.01659</link>
<guid>https://arxiv.org/abs/2506.01659</guid>
<content:encoded><![CDATA[
arXiv:2506.01659v1 Announce Type: cross 
Abstract: Despite substantial research into the biological basis of memory, the precise mechanisms by which experiences are encoded, stored, and retrieved in the brain remain incompletely understood. A growing body of evidence supports the engram theory, which posits that sparse populations of neurons undergo lasting physical and biochemical changes to support long-term memory. Yet, a comprehensive computational framework that integrates biological findings with mechanistic models remains elusive. This work synthesizes insights from cellular neuroscience and computational modeling to address key challenges in engram research: how engram neurons are identified and manipulated; how synaptic plasticity mechanisms contribute to stable memory traces; and how sparsity promotes efficient, interference-resistant representations. Relevant computational approaches -- such as sparse regularization, engram gating, and biologically inspired architectures like Sparse Distributed Memory and spiking neural networks -- are also examined. Together, these findings suggest that memory efficiency, capacity, and stability emerge from the interaction of plasticity and sparsity constraints. By integrating neurobiological and computational perspectives, this paper provides a comprehensive theoretical foundation for engram research and proposes a roadmap for future inquiry into the mechanisms underlying memory, with implications for the diagnosis and treatment of memory-related disorders.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable AI Systems Must Be Contestable: Here's How to Make It Happen</title>
<link>https://arxiv.org/abs/2506.01662</link>
<guid>https://arxiv.org/abs/2506.01662</guid>
<content:encoded><![CDATA[
arXiv:2506.01662v1 Announce Type: cross 
Abstract: As AI regulations around the world intensify their focus on system safety, contestability has become a mandatory, yet ill-defined, safeguard. In XAI, "contestability" remains an empty promise: no formal definition exists, no algorithm guarantees it, and practitioners lack concrete guidance to satisfy regulatory requirements. Grounded in a systematic literature review, this paper presents the first rigorous formal definition of contestability in explainable AI, directly aligned with stakeholder requirements and regulatory mandates. We introduce a modular framework of by-design and post-hoc mechanisms spanning human-centered interfaces, technical architectures, legal processes, and organizational workflows. To operationalize our framework, we propose the Contestability Assessment Scale, a composite metric built on more than twenty quantitative criteria. Through multiple case studies across diverse application domains, we reveal where state-of-the-art systems fall short and show how our framework drives targeted improvements. By converting contestability from regulatory theory into a practical framework, our work equips practitioners with the tools to embed genuine recourse and accountability into AI systems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Provably Safe Reinforcement Learning from Analytic Gradients</title>
<link>https://arxiv.org/abs/2506.01665</link>
<guid>https://arxiv.org/abs/2506.01665</guid>
<content:encoded><![CDATA[
arXiv:2506.01665v1 Announce Type: cross 
Abstract: Deploying autonomous robots in safety-critical applications requires safety guarantees. Provably safe reinforcement learning is an active field of research which aims to provide such guarantees using safeguards. These safeguards should be integrated during training to prevent a large sim-to-real gap. While there are several approaches for safeguarding sampling-based reinforcement learning, analytic gradient-based reinforcement learning often achieves superior performance and sample efficiency. However, there is no safeguarding approach for this learning paradigm yet. Our work addresses this gap by developing the first effective safeguard for analytic gradient-based reinforcement learning. We analyse existing, differentiable safeguards, adapt them through modified mappings and gradient formulations, and integrate them with a state-of-the-art learning algorithm and a differentiable simulation. We evaluate how different safeguards affect policy optimisation using numerical experiments on two classical control tasks. The results demonstrate safeguarded training without compromising performance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthesis of discrete-continuous quantum circuits with multimodal diffusion models</title>
<link>https://arxiv.org/abs/2506.01666</link>
<guid>https://arxiv.org/abs/2506.01666</guid>
<content:encoded><![CDATA[
arXiv:2506.01666v1 Announce Type: cross 
Abstract: Efficiently compiling quantum operations remains a major bottleneck in scaling quantum computing. Today's state-of-the-art methods achieve low compilation error by combining search algorithms with gradient-based parameter optimization, but they incur long runtimes and require multiple calls to quantum hardware or expensive classical simulations, making their scaling prohibitive. Recently, machine-learning models have emerged as an alternative, though they are currently restricted to discrete gate sets. Here, we introduce a multimodal denoising diffusion model that simultaneously generates a circuit's structure and its continuous parameters for compiling a target unitary. It leverages two independent diffusion processes, one for discrete gate selection and one for parameter prediction. We benchmark the model over different experiments, analyzing the method's accuracy across varying qubit counts, circuit depths, and proportions of parameterized gates. Finally, by exploiting its rapid circuit generation, we create large datasets of circuits for particular operations and use these to extract valuable heuristics that can help us discover new insights into quantum circuit synthesis.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRAM: Generative Recommendation via Semantic-aware Multi-granular Late Fusion</title>
<link>https://arxiv.org/abs/2506.01673</link>
<guid>https://arxiv.org/abs/2506.01673</guid>
<content:encoded><![CDATA[
arXiv:2506.01673v1 Announce Type: cross 
Abstract: Generative recommendation is an emerging paradigm that leverages the extensive knowledge of large language models by formulating recommendations into a text-to-text generation task. However, existing studies face two key limitations in (i) incorporating implicit item relationships and (ii) utilizing rich yet lengthy item information. To address these challenges, we propose a Generative Recommender via semantic-Aware Multi-granular late fusion (GRAM), introducing two synergistic innovations. First, we design semantic-to-lexical translation to encode implicit hierarchical and collaborative item relationships into the vocabulary space of LLMs. Second, we present multi-granular late fusion to integrate rich semantics efficiently with minimal information loss. It employs separate encoders for multi-granular prompts, delaying the fusion until the decoding stage. Experiments on four benchmark datasets show that GRAM outperforms eight state-of-the-art generative recommendation models, achieving significant improvements of 11.5-16.0% in Recall@5 and 5.3-13.6% in NDCG@5. The source code is available at https://github.com/skleee/GRAM.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overcoming Data Scarcity in Scanning Tunnelling Microscopy Image Segmentation</title>
<link>https://arxiv.org/abs/2506.01678</link>
<guid>https://arxiv.org/abs/2506.01678</guid>
<content:encoded><![CDATA[
arXiv:2506.01678v1 Announce Type: cross 
Abstract: Scanning tunnelling microscopy (STM) is a powerful technique for imaging surfaces with atomic resolution, providing insight into physical and chemical processes at the level of single atoms and molecules. A regular task of STM image analysis is the identification and labelling of features of interest against a uniform background. Performing this manually is a labour-intensive task, requiring significant human effort. To reduce this burden, we propose an automated approach to the segmentation of STM images that uses both few-shot learning and unsupervised learning. Our technique offers greater flexibility compared to previous supervised methods; it removes the requirement for large manually annotated datasets and is thus easier to adapt to an unseen surface while still maintaining a high accuracy. We demonstrate the effectiveness of our approach by using it to recognise atomic features on three distinct surfaces: Si(001), Ge(001), and TiO$_2$(110), including adsorbed AsH$_3$ molecules on the silicon and germanium surfaces. Our model exhibits strong generalisation capabilities, and following initial training, can be adapted to unseen surfaces with as few as one additional labelled data point. This work is a significant step towards efficient and material-agnostic, automatic segmentation of STM images.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When LLMs Team Up: The Emergence of Collaborative Affective Computing</title>
<link>https://arxiv.org/abs/2506.01698</link>
<guid>https://arxiv.org/abs/2506.01698</guid>
<content:encoded><![CDATA[
arXiv:2506.01698v1 Announce Type: cross 
Abstract: Affective Computing (AC) is essential in bridging the gap between human emotional experiences and machine understanding. Traditionally, AC tasks in natural language processing (NLP) have been approached through pipeline architectures, which often suffer from structure rigidity that leads to inefficiencies and limited adaptability. The advent of Large Language Models (LLMs) has revolutionized this field by offering a unified approach to affective understanding and generation tasks, enhancing the potential for dynamic, real-time interactions. However, LLMs face cognitive limitations in affective reasoning, such as misinterpreting cultural nuances or contextual emotions, and hallucination problems in decision-making. To address these challenges, recent research advocates for LLM-based collaboration systems that emphasize interactions among specialized models and LLMs, mimicking human-like affective intelligence through the synergy of emotional and rational thinking that aligns with Dual Process Theory in psychology. This survey aims to provide a comprehensive overview of LLM-based collaboration systems in AC, exploring from structured collaborations to autonomous collaborations. Specifically, it includes: (1) A systematic review of existing methods, focusing on collaboration strategies, mechanisms, key functions, and applications; (2) Experimental comparisons of collaboration strategies across representative tasks in affective understanding and generation; (3) An analysis highlighting the potential of these systems to enhance robustness and adaptability in complex affective reasoning; (4) A discussion of key challenges and future research directions to further advance the field. This work is the first to systematically explore collaborative intelligence with LLMs in AC, paving the way for more powerful applications that approach human-like social intelligence.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Pruning by Information Maximization</title>
<link>https://arxiv.org/abs/2506.01701</link>
<guid>https://arxiv.org/abs/2506.01701</guid>
<content:encoded><![CDATA[
arXiv:2506.01701v1 Announce Type: cross 
Abstract: In this paper, we present InfoMax, a novel data pruning method, also known as coreset selection, designed to maximize the information content of selected samples while minimizing redundancy. By doing so, InfoMax enhances the overall informativeness of the coreset. The information of individual samples is measured by importance scores, which capture their influence or difficulty in model learning. To quantify redundancy, we use pairwise sample similarities, based on the premise that similar samples contribute similarly to the learning process. We formalize the coreset selection problem as a discrete quadratic programming (DQP) task, with the objective of maximizing the total information content, represented as the sum of individual sample contributions minus the redundancies introduced by similar samples within the coreset. To ensure practical scalability, we introduce an efficient gradient-based solver, complemented by sparsification techniques applied to the similarity matrix and dataset partitioning strategies. This enables InfoMax to seamlessly scale to datasets with millions of samples. Extensive experiments demonstrate the superior performance of InfoMax in various data pruning tasks, including image classification, vision-language pre-training, and instruction tuning for large language models.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tug-of-war between idiom's figurative and literal meanings in LLMs</title>
<link>https://arxiv.org/abs/2506.01723</link>
<guid>https://arxiv.org/abs/2506.01723</guid>
<content:encoded><![CDATA[
arXiv:2506.01723v1 Announce Type: cross 
Abstract: Idioms present a unique challenge for language models due to their non-compositional figurative meanings, which often strongly diverge from the idiom's literal interpretation. This duality requires a model to learn representing and deciding between the two meanings to interpret an idiom in a figurative sense, or literally. In this paper, we employ tools from mechanistic interpretability to trace how a large pretrained causal transformer (LLama3.2-1B-base) deals with this ambiguity. We localize three steps of idiom processing: First, the idiom's figurative meaning is retrieved in early attention and MLP sublayers. We identify specific attention heads which boost the figurative meaning of the idiom while suppressing the idiom's literal interpretation. The model subsequently represents the figurative representation through an intermediate path. Meanwhile, a parallel bypass route forwards literal interpretation, ensuring that a both reading remain available. Overall, our findings provide a mechanistic evidence for idiom comprehension in an autoregressive transformer.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Principled data augmentation for learning to solve quadratic programming problems</title>
<link>https://arxiv.org/abs/2506.01728</link>
<guid>https://arxiv.org/abs/2506.01728</guid>
<content:encoded><![CDATA[
arXiv:2506.01728v1 Announce Type: cross 
Abstract: Linear and quadratic optimization are crucial in numerous real-world applications, from training machine learning models to integer-linear optimization. Recently, learning-to-optimize methods (L2O) for linear (LPs) or quadratic programs (QPs) using message-passing graph neural networks (MPNNs) have gained traction, promising lightweight, data-driven proxies for solving such optimization problems. For example, they replace the costly computation of strong branching scores in branch-and-bound solvers, requiring solving many such optimization problems. However, robust L2O MPNNs remain challenging in data-scarce settings, especially when addressing complex optimization problems such as QPs. This work introduces a principled approach to data augmentation tailored for QPs via MPNNs. Our method leverages theoretically justified data augmentation techniques to generate diverse yet optimality-preserving instances. Furthermore, we integrate these augmentations into a self-supervised learning framework based on contrastive learning, thereby pretraining MPNNs for enhanced performance on L2O tasks. Extensive experiments demonstrate that our approach improves generalization in supervised scenarios and facilitates effective transfer learning to related optimization problems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Egocentric Action Recognition with Multimodal Data</title>
<link>https://arxiv.org/abs/2506.01757</link>
<guid>https://arxiv.org/abs/2506.01757</guid>
<content:encoded><![CDATA[
arXiv:2506.01757v1 Announce Type: cross 
Abstract: The increasing availability of wearable XR devices opens new perspectives for Egocentric Action Recognition (EAR) systems, which can provide deeper human understanding and situation awareness. However, deploying real-time algorithms on these devices can be challenging due to the inherent trade-offs between portability, battery life, and computational resources. In this work, we systematically analyze the impact of sampling frequency across different input modalities - RGB video and 3D hand pose - on egocentric action recognition performance and CPU usage. By exploring a range of configurations, we provide a comprehensive characterization of the trade-offs between accuracy and computational efficiency. Our findings reveal that reducing the sampling rate of RGB frames, when complemented with higher-frequency 3D hand pose input, can preserve high accuracy while significantly lowering CPU demands. Notably, we observe up to a 3x reduction in CPU usage with minimal to no loss in recognition performance. This highlights the potential of multimodal input strategies as a viable approach to achieving efficient, real-time EAR on XR devices.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReGA: Representation-Guided Abstraction for Model-based Safeguarding of LLMs</title>
<link>https://arxiv.org/abs/2506.01770</link>
<guid>https://arxiv.org/abs/2506.01770</guid>
<content:encoded><![CDATA[
arXiv:2506.01770v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have achieved significant success in various tasks, yet concerns about their safety and security have emerged. In particular, they pose risks in generating harmful content and vulnerability to jailbreaking attacks. To analyze and monitor machine learning models, model-based analysis has demonstrated notable potential in stateful deep neural networks, yet suffers from scalability issues when extending to LLMs due to their vast feature spaces. In this paper, we propose ReGA, a model-based analysis framework with representation-guided abstraction, to safeguard LLMs against harmful prompts and generations. By leveraging safety-critical representations, which are low-dimensional directions emerging in hidden states that indicate safety-related concepts, ReGA effectively addresses the scalability issue when constructing the abstract model for safety modeling. Our comprehensive evaluation shows that ReGA performs sufficiently well in distinguishing between safe and harmful inputs, achieving an AUROC of 0.975 at the prompt level and 0.985 at the conversation level. Additionally, ReGA exhibits robustness to real-world attacks and generalization across different safety perspectives, outperforming existing safeguard paradigms in terms of interpretability and scalability. Overall, ReGA serves as an efficient and scalable solution to enhance LLM safety by integrating representation engineering with model-based abstraction, paving the way for new paradigms to utilize software insights for AI safety. Our code is available at https://github.com/weizeming/ReGA.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Greening AI-enabled Systems with Software Engineering: A Research Agenda for Environmentally Sustainable AI Practices</title>
<link>https://arxiv.org/abs/2506.01774</link>
<guid>https://arxiv.org/abs/2506.01774</guid>
<content:encoded><![CDATA[
arXiv:2506.01774v1 Announce Type: cross 
Abstract: The environmental impact of Artificial Intelligence (AI)-enabled systems is increasing rapidly, and software engineering plays a critical role in developing sustainable solutions. The "Greening AI with Software Engineering" CECAM-Lorentz workshop (no. 1358, 2025) funded by the Centre Europ\'een de Calcul Atomique et Mol\'eculaire and the Lorentz Center, provided an interdisciplinary forum for 29 participants, from practitioners to academics, to share knowledge, ideas, practices, and current results dedicated to advancing green software and AI research. The workshop was held February 3-7, 2025, in Lausanne, Switzerland. Through keynotes, flash talks, and collaborative discussions, participants identified and prioritized key challenges for the field. These included energy assessment and standardization, benchmarking practices, sustainability-aware architectures, runtime adaptation, empirical methodologies, and education. This report presents a research agenda emerging from the workshop, outlining open research directions and practical recommendations to guide the development of environmentally sustainable AI-enabled systems rooted in software engineering principles.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaXIFE: Multilingual and Cross-lingual Instruction Following Evaluation</title>
<link>https://arxiv.org/abs/2506.01776</link>
<guid>https://arxiv.org/abs/2506.01776</guid>
<content:encoded><![CDATA[
arXiv:2506.01776v1 Announce Type: cross 
Abstract: With the rapid adoption of large language models (LLMs) in natural language processing, the ability to follow instructions has emerged as a key metric for evaluating their practical utility. However, existing evaluation methods often focus on single-language scenarios, overlooking the challenges and differences present in multilingual and cross-lingual contexts. To address this gap, we introduce MaXIFE: a comprehensive evaluation benchmark designed to assess instruction-following capabilities across 23 languages with 1,667 verifiable instruction tasks. MaXIFE integrates both Rule-Based Evaluation and Model-Based Evaluation, ensuring a balance of efficiency and accuracy. We applied MaXIFE to evaluate several leading commercial and open-source LLMs, establishing baseline results for future comparisons. By providing a standardized tool for multilingual instruction-following evaluation, MaXIFE aims to advance research and development in natural language processing.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>unMORE: Unsupervised Multi-Object Segmentation via Center-Boundary Reasoning</title>
<link>https://arxiv.org/abs/2506.01778</link>
<guid>https://arxiv.org/abs/2506.01778</guid>
<content:encoded><![CDATA[
arXiv:2506.01778v1 Announce Type: cross 
Abstract: We study the challenging problem of unsupervised multi-object segmentation on single images. Existing methods, which rely on image reconstruction objectives to learn objectness or leverage pretrained image features to group similar pixels, often succeed only in segmenting simple synthetic objects or discovering a limited number of real-world objects. In this paper, we introduce unMORE, a novel two-stage pipeline designed to identify many complex objects in real-world images. The key to our approach involves explicitly learning three levels of carefully defined object-centric representations in the first stage. Subsequently, our multi-object reasoning module utilizes these learned object priors to discover multiple objects in the second stage. Notably, this reasoning module is entirely network-free and does not require human labels. Extensive experiments demonstrate that unMORE significantly outperforms all existing unsupervised methods across 6 real-world benchmark datasets, including the challenging COCO dataset, achieving state-of-the-art object segmentation results. Remarkably, our method excels in crowded images where all baselines collapse.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Customer Service Chatbots with Context-Aware NLU through Selective Attention and Multi-task Learning</title>
<link>https://arxiv.org/abs/2506.01781</link>
<guid>https://arxiv.org/abs/2506.01781</guid>
<content:encoded><![CDATA[
arXiv:2506.01781v1 Announce Type: cross 
Abstract: Customer service chatbots are conversational systems aimed at addressing customer queries, often by directing them to automated workflows. A crucial aspect of this process is the classification of the customer's intent. Presently, most intent classification models for customer care utilise only customer query for intent prediction. This may result in low-accuracy models, which cannot handle ambiguous queries. An ambiguous query like "I didn't receive my package" could indicate a delayed order, or an order that was delivered but the customer failed to receive it. Resolution of each of these scenarios requires the execution of very different sequence of steps. Utilizing additional information, such as the customer's order delivery status, in the right manner can help identify the intent for such ambiguous queries. In this paper, we have introduced a context-aware NLU model that incorporates both, the customer query and contextual information from the customer's order status for predicting customer intent. A novel selective attention module is used to extract relevant context features. We have also proposed a multi-task learning paradigm for the effective utilization of different label types available in our training data. Our suggested method, Multi-Task Learning Contextual NLU with Selective Attention Weighted Context (MTL-CNLU-SAWC), yields a 4.8% increase in top 2 accuracy score over the baseline model which only uses user queries, and a 3.5% improvement over existing state-of-the-art models that combine query and context. We have deployed our model to production for Walmart's customer care domain. Accurate intent prediction through MTL-CNLU-SAWC helps to better direct customers to automated workflows, thereby significantly reducing escalations to human agents, leading to almost a million dollars in yearly savings for the company.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Systematic Hazard Analysis for Frontier AI using STPA</title>
<link>https://arxiv.org/abs/2506.01782</link>
<guid>https://arxiv.org/abs/2506.01782</guid>
<content:encoded><![CDATA[
arXiv:2506.01782v1 Announce Type: cross 
Abstract: All of the frontier AI companies have published safety frameworks where they define capability thresholds and risk mitigations that determine how they will safely develop and deploy their models. Adoption of systematic approaches to risk modelling, based on established practices used in safety-critical industries, has been recommended, however frontier AI companies currently do not describe in detail any structured approach to identifying and analysing hazards. STPA (Systems-Theoretic Process Analysis) is a systematic methodology for identifying how complex systems can become unsafe, leading to hazards. It achieves this by mapping out controllers and controlled processes then analysing their interactions and feedback loops to understand how harmful outcomes could occur (Leveson & Thomas, 2018). We evaluate STPA's ability to broaden the scope, improve traceability and strengthen the robustness of safety assurance for frontier AI systems. Applying STPA to the threat model and scenario described in 'A Sketch of an AI Control Safety Case' (Korbak et al., 2025), we derive a list of Unsafe Control Actions. From these we select a subset and explore the Loss Scenarios that lead to them if left unmitigated. We find that STPA is able to identify causal factors that may be missed by unstructured hazard analysis methodologies thereby improving robustness. We suggest STPA could increase the safety assurance of frontier AI when used to complement or check coverage of existing AI governance techniques including capability thresholds, model evaluations and emergency procedures. The application of a systematic methodology supports scalability by increasing the proportion of the analysis that could be conducted by LLMs, reducing the burden on human domain experts.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>iQUEST: An Iterative Question-Guided Framework for Knowledge Base Question Answering</title>
<link>https://arxiv.org/abs/2506.01784</link>
<guid>https://arxiv.org/abs/2506.01784</guid>
<content:encoded><![CDATA[
arXiv:2506.01784v1 Announce Type: cross 
Abstract: While Large Language Models (LLMs) excel at many natural language processing tasks, they often suffer from factual inaccuracies in knowledge-intensive scenarios. Integrating external knowledge resources, particularly knowledge graphs (KGs), provides a transparent and updatable foundation for more reliable reasoning. Knowledge Base Question Answering (KBQA), which queries and reasons over KGs, is central to this effort, especially for complex, multi-hop queries. However, multi-hop reasoning poses two key challenges: (1)~maintaining coherent reasoning paths, and (2)~avoiding prematurely discarding critical multi-hop connections. To address these issues, we introduce iQUEST, a question-guided KBQA framework that iteratively decomposes complex queries into simpler sub-questions, ensuring a structured and focused reasoning trajectory. Additionally, we integrate a Graph Neural Network (GNN) to look ahead and incorporate 2-hop neighbor information at each reasoning step. This dual approach strengthens the reasoning process, enabling the model to explore viable paths more effectively. Detailed experiments demonstrate the consistent improvement delivered by iQUEST across four benchmark datasets and four LLMs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and Accountability</title>
<link>https://arxiv.org/abs/2506.01789</link>
<guid>https://arxiv.org/abs/2506.01789</guid>
<content:encoded><![CDATA[
arXiv:2506.01789v1 Announce Type: cross 
Abstract: High-quality datasets are fundamental to training and evaluating machine learning models, yet their creation-especially with accurate human annotations-remains a significant challenge. Many dataset paper submissions lack originality, diversity, or rigorous quality control, and these shortcomings are often overlooked during peer review. Submissions also frequently omit essential details about dataset construction and properties. While existing tools such as datasheets aim to promote transparency, they are largely descriptive and do not provide standardized, measurable methods for evaluating data quality. Similarly, metadata requirements at conferences promote accountability but are inconsistently enforced. To address these limitations, this position paper advocates for the integration of systematic, rubric-based evaluation metrics into the dataset review process-particularly as submission volumes continue to grow. We also explore scalable, cost-effective methods for synthetic data generation, including dedicated tools and LLM-as-a-judge approaches, to support more efficient evaluation. As a call to action, we introduce DataRubrics, a structured framework for assessing the quality of both human- and model-generated datasets. Leveraging recent advances in LLM-based evaluation, DataRubrics offers a reproducible, scalable, and actionable solution for dataset quality assessment, enabling both authors and reviewers to uphold higher standards in data-centric research. We also release code to support reproducibility of LLM-based evaluations at https://github.com/datarubrics/datarubrics.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ridgeformer: Mutli-Stage Contrastive Training For Fine-grained Cross-Domain Fingerprint Recognition</title>
<link>https://arxiv.org/abs/2506.01806</link>
<guid>https://arxiv.org/abs/2506.01806</guid>
<content:encoded><![CDATA[
arXiv:2506.01806v1 Announce Type: cross 
Abstract: The increasing demand for hygienic and portable biometric systems has underscored the critical need for advancements in contactless fingerprint recognition. Despite its potential, this technology faces notable challenges, including out-of-focus image acquisition, reduced contrast between fingerprint ridges and valleys, variations in finger positioning, and perspective distortion. These factors significantly hinder the accuracy and reliability of contactless fingerprint matching. To address these issues, we propose a novel multi-stage transformer-based contactless fingerprint matching approach that first captures global spatial features and subsequently refines localized feature alignment across fingerprint samples. By employing a hierarchical feature extraction and matching pipeline, our method ensures fine-grained, cross-sample alignment while maintaining the robustness of global feature representation. We perform extensive evaluations on publicly available datasets such as HKPolyU and RidgeBase under different evaluation protocols, such as contactless-to-contact matching and contactless-to-contactless matching and demonstrate that our proposed approach outperforms existing methods, including COTS solutions.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Quantum Information Theoretic Approach to Tractable Probabilistic Models</title>
<link>https://arxiv.org/abs/2506.01824</link>
<guid>https://arxiv.org/abs/2506.01824</guid>
<content:encoded><![CDATA[
arXiv:2506.01824v1 Announce Type: cross 
Abstract: By recursively nesting sums and products, probabilistic circuits have emerged in recent years as an attractive class of generative models as they enjoy, for instance, polytime marginalization of random variables. In this work we study these machine learning models using the framework of quantum information theory, leading to the introduction of positive unital circuits (PUnCs), which generalize circuit evaluations over positive real-valued probabilities to circuit evaluations over positive semi-definite matrices. As a consequence, PUnCs strictly generalize probabilistic circuits as well as recently introduced circuit classes such as PSD circuits.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CiteEval: Principle-Driven Citation Evaluation for Source Attribution</title>
<link>https://arxiv.org/abs/2506.01829</link>
<guid>https://arxiv.org/abs/2506.01829</guid>
<content:encoded><![CDATA[
arXiv:2506.01829v1 Announce Type: cross 
Abstract: Citation quality is crucial in information-seeking systems, directly influencing trust and the effectiveness of information access. Current evaluation frameworks, both human and automatic, mainly rely on Natural Language Inference (NLI) to assess binary or ternary supportiveness from cited sources, which we argue is a suboptimal proxy for citation evaluation. In this work we introduce CiteEval, a citation evaluation framework driven by principles focusing on fine-grained citation assessment within a broad context, encompassing not only the cited sources but the full retrieval context, user query, and generated text. Guided by the proposed framework, we construct CiteBench, a multi-domain benchmark with high-quality human annotations on citation quality. To enable efficient evaluation, we further develop CiteEval-Auto, a suite of model-based metrics that exhibit strong correlation with human judgments. Experiments across diverse systems demonstrate CiteEval-Auto's superior ability to capture the multifaceted nature of citations compared to existing metrics, offering a principled and scalable approach to evaluate and improve model-generated citations.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoDA: Modulation Adapter for Fine-Grained Visual Grounding in Instructional MLLMs</title>
<link>https://arxiv.org/abs/2506.01850</link>
<guid>https://arxiv.org/abs/2506.01850</guid>
<content:encoded><![CDATA[
arXiv:2506.01850v1 Announce Type: cross 
Abstract: Recently, Multimodal Large Language Models (MLLMs) have demonstrated impressive performance on instruction-following tasks by integrating pretrained visual encoders with large language models (LLMs). However, existing approaches often struggle to ground fine-grained visual concepts in complex scenes. In this paper, we propose MoDA (Modulation Adapter), a lightweight yet effective module designed to refine pre-aligned visual features through instruction-guided modulation. Our approach follows the standard LLaVA training protocol, consisting of a two-stage process: (1) aligning image features to the LLMs input space via a frozen vision encoder and adapter layers, and (2) refining those features using the MoDA adapter during the instructional tuning stage. MoDA employs a Transformer-based cross-attention mechanism to generate a modulation mask over the aligned visual tokens, thereby emphasizing semantically relevant embedding dimensions based on the language instruction. The modulated features are then passed to the LLM for autoregressive language generation. Our experimental evaluation shows that MoDA improves visual grounding and generates more contextually appropriate responses, demonstrating its effectiveness as a general-purpose enhancement for image-based MLLMs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Frugal Machine Learning for Energy-efficient, and Resource-aware Artificial Intelligence</title>
<link>https://arxiv.org/abs/2506.01869</link>
<guid>https://arxiv.org/abs/2506.01869</guid>
<content:encoded><![CDATA[
arXiv:2506.01869v1 Announce Type: cross 
Abstract: Frugal Machine Learning (FML) refers to the practice of designing Machine Learning (ML) models that are efficient, cost-effective, and mindful of resource constraints. This field aims to achieve acceptable performance while minimizing the use of computational resources, time, energy, and data for both training and inference. FML strategies can be broadly categorized into input frugality, learning process frugality, and model frugality, each focusing on reducing resource consumption at different stages of the ML pipeline. This chapter explores recent advancements, applications, and open challenges in FML, emphasizing its importance for smart environments that incorporate edge computing and IoT devices, which often face strict limitations in bandwidth, energy, or latency. Technological enablers such as model compression, energy-efficient hardware, and data-efficient learning techniques are discussed, along with adaptive methods including parameter regularization, knowledge distillation, and dynamic architecture design that enable incremental model updates without full retraining. Furthermore, it provides a comprehensive taxonomy of frugal methods, discusses case studies across diverse domains, and identifies future research directions to drive innovation in this evolving field.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Explore: An In-Context Learning Approach for Pure Exploration</title>
<link>https://arxiv.org/abs/2506.01876</link>
<guid>https://arxiv.org/abs/2506.01876</guid>
<content:encoded><![CDATA[
arXiv:2506.01876v1 Announce Type: cross 
Abstract: In this work, we study the active sequential hypothesis testing problem, also known as pure exploration, where the goal is to actively control a data collection process to efficiently identify the correct hypothesis underlying a decision problem. While relevant across multiple domains, devising adaptive exploration strategies remains challenging, particularly due to difficulties in encoding appropriate inductive biases. Existing Reinforcement Learning (RL)-based methods often underperform when relevant information structures are inadequately represented, whereas more complex methods, like Best Arm Identification (BAI) techniques, may be difficult to devise and typically rely on explicit modeling assumptions. To address these limitations, we introduce In-Context Pure Exploration (ICPE), an in-context learning approach that uses Transformers to learn exploration strategies directly from experience. ICPE combines supervised learning and reinforcement learning to identify and exploit latent structure across related tasks, without requiring prior assumptions. Numerical results across diverse synthetic and semi-synthetic benchmarks highlight ICPE's capability to achieve robust performance performance in deterministic, stochastic, and structured settings. These results demonstrate ICPE's ability to match optimal instance-dependent algorithms using only deep learning techniques, making it a practical and general approach to data-efficient exploration.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>scDataset: Scalable Data Loading for Deep Learning on Large-Scale Single-Cell Omics</title>
<link>https://arxiv.org/abs/2506.01883</link>
<guid>https://arxiv.org/abs/2506.01883</guid>
<content:encoded><![CDATA[
arXiv:2506.01883v1 Announce Type: cross 
Abstract: Modern single-cell datasets now comprise hundreds of millions of cells, presenting significant challenges for training deep learning models that require shuffled, memory-efficient data loading. While the AnnData format is the community standard for storing single-cell datasets, existing data loading solutions for AnnData are often inadequate: some require loading all data into memory, others convert to dense formats that increase storage demands, and many are hampered by slow random disk access. We present scDataset, a PyTorch IterableDataset that operates directly on one or more AnnData files without the need for format conversion. The core innovation is a combination of block sampling and batched fetching, which together balance randomness and I/O efficiency. On the Tahoe 100M dataset, scDataset achieves up to a 48$\times$ speed-up over AnnLoader, a 27$\times$ speed-up over HuggingFace Datasets, and an 18$\times$ speed-up over BioNeMo in single-core settings. These advances democratize large-scale single-cell model training for the broader research community.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agnostic Reinforcement Learning: Foundations and Algorithms</title>
<link>https://arxiv.org/abs/2506.01884</link>
<guid>https://arxiv.org/abs/2506.01884</guid>
<content:encoded><![CDATA[
arXiv:2506.01884v1 Announce Type: cross 
Abstract: Reinforcement Learning (RL) has demonstrated tremendous empirical success across numerous challenging domains. However, we lack a strong theoretical understanding of the statistical complexity of RL in environments with large state spaces, where function approximation is required for sample-efficient learning. This thesis addresses this gap by rigorously examining the statistical complexity of RL with function approximation from a learning theoretic perspective. Departing from a long history of prior work, we consider the weakest form of function approximation, called agnostic policy learning, in which the learner seeks to find the best policy in a given class $\Pi$, with no guarantee that $\Pi$ contains an optimal policy for the underlying task.
  We systematically explore agnostic policy learning along three key axes: environment access -- how a learner collects data from the environment; coverage conditions -- intrinsic properties of the underlying MDP measuring the expansiveness of state-occupancy measures for policies in the class $\Pi$, and representational conditions -- structural assumptions on the class $\Pi$ itself. Within this comprehensive framework, we (1) design new learning algorithms with theoretical guarantees and (2) characterize fundamental performance bounds of any algorithm. Our results reveal significant statistical separations that highlight the power and limitations of agnostic policy learning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CogniAlign: Word-Level Multimodal Speech Alignment with Gated Cross-Attention for Alzheimer's Detection</title>
<link>https://arxiv.org/abs/2506.01890</link>
<guid>https://arxiv.org/abs/2506.01890</guid>
<content:encoded><![CDATA[
arXiv:2506.01890v1 Announce Type: cross 
Abstract: Early detection of cognitive disorders such as Alzheimer's disease is critical for enabling timely clinical intervention and improving patient outcomes. In this work, we introduce CogniAlign, a multimodal architecture for Alzheimer's detection that integrates audio and textual modalities, two non-intrusive sources of information that offer complementary insights into cognitive health. Unlike prior approaches that fuse modalities at a coarse level, CogniAlign leverages a word-level temporal alignment strategy that synchronizes audio embeddings with corresponding textual tokens based on transcription timestamps. This alignment supports the development of token-level fusion techniques, enabling more precise cross-modal interactions. To fully exploit this alignment, we propose a Gated Cross-Attention Fusion mechanism, where audio features attend over textual representations, guided by the superior unimodal performance of the text modality. In addition, we incorporate prosodic cues, specifically interword pauses, by inserting pause tokens into the text and generating audio embeddings for silent intervals, further enriching both streams. We evaluate CogniAlign on the ADReSSo dataset, where it achieves an accuracy of 90.36%, outperforming existing state-of-the-art methods. A detailed ablation study confirms the advantages of our alignment strategy, attention-based fusion, and prosodic modeling.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformers as Multi-task Learners: Decoupling Features in Hidden Markov Models</title>
<link>https://arxiv.org/abs/2506.01919</link>
<guid>https://arxiv.org/abs/2506.01919</guid>
<content:encoded><![CDATA[
arXiv:2506.01919v1 Announce Type: cross 
Abstract: Transformer based models have shown remarkable capabilities in sequence learning across a wide range of tasks, often performing well on specific task by leveraging input-output examples. Despite their empirical success, a comprehensive theoretical understanding of this phenomenon remains limited. In this work, we investigate the layerwise behavior of Transformers to uncover the mechanisms underlying their multi-task generalization ability. Taking explorations on a typical sequence model, i.e, Hidden Markov Models, which are fundamental to many language tasks, we observe that: first, lower layers of Transformers focus on extracting feature representations, primarily influenced by neighboring tokens; second, on the upper layers, features become decoupled, exhibiting a high degree of time disentanglement. Building on these empirical insights, we provide theoretical analysis for the expressiveness power of Transformers. Our explicit constructions align closely with empirical observations, providing theoretical support for the Transformer's effectiveness and efficiency on sequence learning across diverse tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedEBench: Revisiting Text-instructed Image Editing</title>
<link>https://arxiv.org/abs/2506.01921</link>
<guid>https://arxiv.org/abs/2506.01921</guid>
<content:encoded><![CDATA[
arXiv:2506.01921v1 Announce Type: cross 
Abstract: Text-guided image editing has seen rapid progress in natural image domains, but its adaptation to medical imaging remains limited and lacks standardized evaluation. Clinically, such editing holds promise for simulating surgical outcomes, creating personalized teaching materials, and enhancing patient communication. To bridge this gap, we introduce \textbf{MedEBench}, a comprehensive benchmark for evaluating text-guided medical image editing. It consists of 1,182 clinically sourced image-prompt triplets spanning 70 tasks across 13 anatomical regions. MedEBench offers three key contributions: (1) a clinically relevant evaluation framework covering Editing Accuracy, Contextual Preservation, and Visual Quality, supported by detailed descriptions of expected change and ROI (Region of Interest) masks; (2) a systematic comparison of seven state-of-the-art models, revealing common failure patterns; and (3) a failure analysis protocol based on attention grounding, using IoU between attention maps and ROIs to identify mislocalization. MedEBench provides a solid foundation for developing and evaluating reliable, clinically meaningful medical image editing systems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TaxaDiffusion: Progressively Trained Diffusion Model for Fine-Grained Species Generation</title>
<link>https://arxiv.org/abs/2506.01923</link>
<guid>https://arxiv.org/abs/2506.01923</guid>
<content:encoded><![CDATA[
arXiv:2506.01923v1 Announce Type: cross 
Abstract: We propose TaxaDiffusion, a taxonomy-informed training framework for diffusion models to generate fine-grained animal images with high morphological and identity accuracy. Unlike standard approaches that treat each species as an independent category, TaxaDiffusion incorporates domain knowledge that many species exhibit strong visual similarities, with distinctions often residing in subtle variations of shape, pattern, and color. To exploit these relationships, TaxaDiffusion progressively trains conditioned diffusion models across different taxonomic levels -- starting from broad classifications such as Class and Order, refining through Family and Genus, and ultimately distinguishing at the Species level. This hierarchical learning strategy first captures coarse-grained morphological traits shared by species with common ancestors, facilitating knowledge transfer before refining fine-grained differences for species-level distinction. As a result, TaxaDiffusion enables accurate generation even with limited training samples per species. Extensive experiments on three fine-grained animal datasets demonstrate that outperforms existing approaches, achieving superior fidelity in fine-grained animal image generation. Project page: https://amink8.github.io/TaxaDiffusion/
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Competitive Information Gathering for Partially Observable Trajectory Games</title>
<link>https://arxiv.org/abs/2506.01927</link>
<guid>https://arxiv.org/abs/2506.01927</guid>
<content:encoded><![CDATA[
arXiv:2506.01927v1 Announce Type: cross 
Abstract: Game-theoretic agents must make plans that optimally gather information about their opponents. These problems are modeled by partially observable stochastic games (POSGs), but planning in fully continuous POSGs is intractable without heavy offline computation or assumptions on the order of belief maintained by each player. We formulate a finite history/horizon refinement of POSGs which admits competitive information gathering behavior in trajectory space, and through a series of approximations, we present an online method for computing rational trajectory plans in these games which leverages particle-based estimations of the joint state space and performs stochastic gradient play. We also provide the necessary adjustments required to deploy this method on individual agents. The method is tested in continuous pursuit-evasion and warehouse-pickup scenarios (alongside extensions to $N > 2$ players and to more complex environments with visual and physical obstacles), demonstrating evidence of active information gathering and outperforming passive competitors.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image Generation from Contextually-Contradictory Prompts</title>
<link>https://arxiv.org/abs/2506.01929</link>
<guid>https://arxiv.org/abs/2506.01929</guid>
<content:encoded><![CDATA[
arXiv:2506.01929v1 Announce Type: cross 
Abstract: Text-to-image diffusion models excel at generating high-quality, diverse images from natural language prompts. However, they often fail to produce semantically accurate results when the prompt contains concept combinations that contradict their learned priors. We define this failure mode as contextual contradiction, where one concept implicitly negates another due to entangled associations learned during training. To address this, we propose a stage-aware prompt decomposition framework that guides the denoising process using a sequence of proxy prompts. Each proxy prompt is constructed to match the semantic content expected to emerge at a specific stage of denoising, while ensuring contextual coherence. To construct these proxy prompts, we leverage a large language model (LLM) to analyze the target prompt, identify contradictions, and generate alternative expressions that preserve the original intent while resolving contextual conflicts. By aligning prompt information with the denoising progression, our method enables fine-grained semantic control and accurate image generation in the presence of contextual contradictions. Experiments across a variety of challenging prompts show substantial improvements in alignment to the textual prompt.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Red Teaming AI Policy: A Taxonomy of Avoision and the EU AI Act</title>
<link>https://arxiv.org/abs/2506.01931</link>
<guid>https://arxiv.org/abs/2506.01931</guid>
<content:encoded><![CDATA[
arXiv:2506.01931v1 Announce Type: cross 
Abstract: The shape of AI regulation is beginning to emerge, most prominently through the EU AI Act (the "AIA"). By 2027, the AIA will be in full effect, and firms are starting to adjust their behavior in light of this new law. In this paper, we present a framework and taxonomy for reasoning about "avoision" -- conduct that walks the line between legal avoidance and evasion -- that firms might engage in so as to minimize the regulatory burden the AIA poses. We organize these avoision strategies around three "tiers" of increasing AIA exposure that regulated entities face depending on: whether their activities are (1) within scope of the AIA, (2) exempted from provisions of the AIA, or are (3) placed in a category with higher regulatory scrutiny. In each of these tiers and for each strategy, we specify the organizational and technological forms through which avoision may manifest. Our goal is to provide an adversarial framework for "red teaming" the AIA and AI regulation on the horizon.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning</title>
<link>https://arxiv.org/abs/2506.01939</link>
<guid>https://arxiv.org/abs/2506.01939</guid>
<content:encoded><![CDATA[
arXiv:2506.01939v1 Announce Type: cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful approach to enhancing the reasoning capabilities of Large Language Models (LLMs), while its mechanisms are not yet well understood. In this work, we undertake a pioneering exploration of RLVR through the novel perspective of token entropy patterns, comprehensively analyzing how different tokens influence reasoning performance. By examining token entropy patterns in Chain-of-Thought (CoT) reasoning, we observe that only a small fraction of tokens exhibit high entropy, and these tokens act as critical forks that steer the model toward diverse reasoning pathways. Furthermore, studying how entropy patterns evolve during RLVR training reveals that RLVR largely adheres to the base model's entropy patterns, primarily adjusting the entropy of high-entropy tokens. These findings highlight the significance of high-entropy tokens (i.e., forking tokens) to RLVR. We ultimately improve RLVR by restricting policy gradient updates to forking tokens and uncover a finding even beyond the 80/20 rule: utilizing only 20% of the tokens while maintaining performance comparable to full-gradient updates on the Qwen3-8B base model and significantly surpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71 on AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models, highlighting a strong scaling trend. In contrast, training exclusively on the 80% lowest-entropy tokens leads to a marked decline in performance. These findings indicate that the efficacy of RLVR primarily arises from optimizing the high-entropy tokens that decide reasoning directions. Collectively, our results highlight the potential to understand RLVR through a token-entropy perspective and optimize RLVR by leveraging high-entropy minority tokens to further improve LLM reasoning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feel the Force: Contact-Driven Learning from Humans</title>
<link>https://arxiv.org/abs/2506.01944</link>
<guid>https://arxiv.org/abs/2506.01944</guid>
<content:encoded><![CDATA[
arXiv:2506.01944v1 Announce Type: cross 
Abstract: Controlling fine-grained forces during manipulation remains a core challenge in robotics. While robot policies learned from robot-collected data or simulation show promise, they struggle to generalize across the diverse range of real-world interactions. Learning directly from humans offers a scalable solution, enabling demonstrators to perform skills in their natural embodiment and in everyday environments. However, visual demonstrations alone lack the information needed to infer precise contact forces. We present FeelTheForce (FTF): a robot learning system that models human tactile behavior to learn force-sensitive manipulation. Using a tactile glove to measure contact forces and a vision-based model to estimate hand pose, we train a closed-loop policy that continuously predicts the forces needed for manipulation. This policy is re-targeted to a Franka Panda robot with tactile gripper sensors using shared visual and action representations. At execution, a PD controller modulates gripper closure to track predicted forces-enabling precise, force-aware control. Our approach grounds robust low-level force control in scalable human supervision, achieving a 77% success rate across 5 force-sensitive manipulation tasks. Code and videos are available at https://feel-the-force-ftf.github.io.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web Tasks</title>
<link>https://arxiv.org/abs/2506.01952</link>
<guid>https://arxiv.org/abs/2506.01952</guid>
<content:encoded><![CDATA[
arXiv:2506.01952v1 Announce Type: cross 
Abstract: Powered by a large language model (LLM), a web browsing agent operates web browsers in a human-like manner and offers a highly transparent path toward automating a wide range of everyday tasks. As web agents become increasingly capable and demonstrate proficiency in general browsing tasks, a critical question emerges: Can they go beyond general browsing to robustly handle tasks that are tedious and complex, or chores that humans often avoid doing themselves? In this paper, we introduce WebChoreArena, a new fully reproducible benchmark comprising 532 carefully curated tasks designed to extend the scope of WebArena beyond general browsing to more labor-intensive and tedious tasks. WebChoreArena systematically integrates three key challenges: (i) Massive Memory tasks requiring accurate retrieval of large amounts of information in the observations, (ii) Calculation tasks demanding precise mathematical reasoning, and (iii) Long-Term Memory tasks necessitating long-term memory across multiple webpages. Built on top of the fully reproducible and widely adopted four WebArena simulation environments, WebChoreArena ensures strict reproducibility and enables fair, direct comparisons with the established WebArena benchmark, offering key insights into agent progress. Our experimental results demonstrate that as LLMs evolve, represented by GPT-4o, Claude 3.7 Sonnet, and Gemini 2.5 Pro, significant improvements in performance are observed on WebChoreArena. These findings suggest that WebChoreArena is well-suited to measure the advancement of state-of-the-art LLMs with greater clarity. Nevertheless, the results also indicate that even with Gemini 2.5 Pro, there remains substantial room for improvement compared to WebArena, highlighting the increased challenges posed by WebChoreArena.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRAG: Distilling RAG for SLMs from LLMs to Transfer Knowledge and Mitigate Hallucination via Evidence and Graph-based Distillation</title>
<link>https://arxiv.org/abs/2506.01954</link>
<guid>https://arxiv.org/abs/2506.01954</guid>
<content:encoded><![CDATA[
arXiv:2506.01954v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) methods have proven highly effective for tasks requiring factual consistency and robust knowledge retrieval. However, large-scale RAG systems consume significant computational resources and are prone to generating hallucinated content from Humans. In this work, we introduce $\texttt{DRAG}$, a novel framework for distilling RAG knowledge from large-scale Language Models (LLMs) into small LMs (SLMs). Our approach leverages evidence- and knowledge graph-based distillation, ensuring that the distilled model retains critical factual knowledge while significantly reducing model size and computational cost. By aligning the smaller model's predictions with a structured knowledge graph and ranked evidence, $\texttt{DRAG}$ effectively mitigates hallucinations and improves factual accuracy. We further present a case demonstrating how our framework mitigates user privacy risks and introduce a corresponding benchmark. Experimental evaluations on multiple benchmarks demonstrate that our method outperforms the prior competitive RAG methods like MiniRAG for SLMs by up to 27.7% using the same models, preserving high-level efficiency and reliability. With $\texttt{DRAG}$, we provide a practical and resource-efficient roadmap to deploying enhanced retrieval and generation capabilities in small-sized LLMs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TabID: Automatic Identification and Tabulation of Subproblems in Constraint Models</title>
<link>https://arxiv.org/abs/2202.13250</link>
<guid>https://arxiv.org/abs/2202.13250</guid>
<content:encoded><![CDATA[
arXiv:2202.13250v2 Announce Type: replace 
Abstract: The performance of a constraint model can often be improved by converting a subproblem into a single table constraint (referred to as tabulation). Finding subproblems to tabulate is traditionally a manual and time-intensive process, even for expert modellers. This paper presents TabID, an entirely automated method to identify promising subproblems for tabulation in constraint programming. We introduce a diverse set of heuristics designed to identify promising candidates for tabulation, aiming to improve solver performance. These heuristics are intended to encapsulate various factors that contribute to useful tabulation. We also present additional checks to limit the potential drawbacks of suboptimal tabulation.
  We comprehensively evaluate our approach using benchmark problems from existing literature that previously relied on manual identification by constraint programming experts of constraints to tabulate. We demonstrate that our automated identification and tabulation process achieves comparable, and in some cases improved results. We empirically evaluate the efficacy of our approach on a variety of solvers, including standard CP (Minion and Gecode), clause-learning CP (Chuffed and OR-Tools) and SAT solvers (Kissat).
  Our findings highlight the substantial potential of fully automated tabulation, suggesting its integration into automated model reformulation tools.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fact-Checking of AI-Generated Reports</title>
<link>https://arxiv.org/abs/2307.14634</link>
<guid>https://arxiv.org/abs/2307.14634</guid>
<content:encoded><![CDATA[
arXiv:2307.14634v2 Announce Type: replace 
Abstract: With advances in generative artificial intelligence (AI), it is now possible to produce realistic-looking automated reports for preliminary reads of radiology images. This can expedite clinical workflows, improve accuracy and reduce overall costs. However, it is also well-known that such models often hallucinate, leading to false findings in the generated reports. In this paper, we propose a new method of fact-checking of AI-generated reports using their associated images. Specifically, the developed examiner differentiates real and fake sentences in reports by learning the association between an image and sentences describing real or potentially fake findings. To train such an examiner, we first created a new dataset of fake reports by perturbing the findings in the original ground truth radiology reports associated with images. Text encodings of real and fake sentences drawn from these reports are then paired with image encodings to learn the mapping to real/fake labels. The utility of such an examiner is demonstrated for verifying automatically generated reports by detecting and removing fake sentences. Future generative AI approaches can use the resulting tool to validate their reports leading to a more responsible use of AI in expediting clinical workflows.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating by Understanding: Neural Visual Generation with Logical Symbol Groundings</title>
<link>https://arxiv.org/abs/2310.17451</link>
<guid>https://arxiv.org/abs/2310.17451</guid>
<content:encoded><![CDATA[
arXiv:2310.17451v3 Announce Type: replace 
Abstract: Making neural visual generative models controllable by logical reasoning systems is promising for improving faithfulness, transparency, and generalizability. We propose the Abductive visual Generation (AbdGen) approach to build such logic-integrated models. A vector-quantized symbol grounding mechanism and the corresponding disentanglement training method are introduced to enhance the controllability of logical symbols over generation. Furthermore, we propose two logical abduction methods to make our approach require few labeled training data and support the induction of latent logical generative rules from data. We experimentally show that our approach can be utilized to integrate various neural generative models with logical reasoning systems, by both learning from scratch or utilizing pre-trained models directly. The code is released at https://github.com/future-item/AbdGen.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CityBench: Evaluating the Capabilities of Large Language Models for Urban Tasks</title>
<link>https://arxiv.org/abs/2406.13945</link>
<guid>https://arxiv.org/abs/2406.13945</guid>
<content:encoded><![CDATA[
arXiv:2406.13945v3 Announce Type: replace 
Abstract: As large language models (LLMs) continue to advance and gain widespread use, establishing systematic and reliable evaluation methodologies for LLMs and vision-language models (VLMs) has become essential to ensure their real-world effectiveness and reliability. There have been some early explorations about the usability of LLMs for limited urban tasks, but a systematic and scalable evaluation benchmark is still lacking. The challenge in constructing a systematic evaluation benchmark for urban research lies in the diversity of urban data, the complexity of application scenarios and the highly dynamic nature of the urban environment. In this paper, we design \textit{CityBench}, an interactive simulator based evaluation platform, as the first systematic benchmark for evaluating the capabilities of LLMs for diverse tasks in urban research. First, we build \textit{CityData} to integrate the diverse urban data and \textit{CitySimu} to simulate fine-grained urban dynamics. Based on \textit{CityData} and \textit{CitySimu}, we design 8 representative urban tasks in 2 categories of perception-understanding and decision-making as the \textit{CityBench}. With extensive results from 30 well-known LLMs and VLMs in 13 cities around the world, we find that advanced LLMs and VLMs can achieve competitive performance in diverse urban tasks requiring commonsense and semantic understanding abilities, e.g., understanding the human dynamics and semantic inference of urban images. Meanwhile, they fail to solve the challenging urban tasks requiring professional knowledge and high-level numerical abilities, e.g., geospatial prediction and traffic control task.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CityGPT: Empowering Urban Spatial Cognition of Large Language Models</title>
<link>https://arxiv.org/abs/2406.13948</link>
<guid>https://arxiv.org/abs/2406.13948</guid>
<content:encoded><![CDATA[
arXiv:2406.13948v2 Announce Type: replace 
Abstract: Large language models(LLMs), with their powerful language generation and reasoning capabilities, have already achieved notable success in many domains, e.g., math and code generation. However, they often fall short when tackling real-life geospatial tasks within urban environments. This limitation stems from a lack of physical world knowledge and relevant data during training. To address this gap, we propose \textit{CityGPT}, a systematic framework designed to enhance LLMs' understanding of urban space and improve their ability to solve the related urban tasks by integrating a city-scale `world model' into the model. Firstly, we construct a diverse instruction tuning dataset, \textit{CityInstruction}, for injecting urban knowledge into LLMs and effectively boosting their spatial reasoning capabilities. Using a combination of \textit{CityInstruction} and open source general instruction data, we introduce a novel and easy-to-use self-weighted fine-tuning method (\textit{SWFT}) to train various LLMs (including ChatGLM3-6B, Llama3-8B, and Qwen2.5-7B) to enhance their urban spatial capabilities without compromising, or even improving, their general abilities. Finally, to validate the effectiveness of our proposed framework, we develop a comprehensive text-based spatial benchmark \textit{CityEval} for evaluating the performance of LLMs across a wide range of urban scenarios and geospatial tasks. Extensive evaluation results demonstrate that smaller LLMs trained with \textit{CityInstruction} by \textit{SWFT} method can achieve performance that is competitive with, and in some cases superior to, proprietary LLMs when assessed using \textit{CityEval}.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Odyssey: Empowering Minecraft Agents with Open-World Skills</title>
<link>https://arxiv.org/abs/2407.15325</link>
<guid>https://arxiv.org/abs/2407.15325</guid>
<content:encoded><![CDATA[
arXiv:2407.15325v3 Announce Type: replace 
Abstract: Recent studies have delved into constructing generalist agents for open-world environments like Minecraft. Despite the encouraging results, existing efforts mainly focus on solving basic programmatic tasks, e.g., material collection and tool-crafting following the Minecraft tech-tree, treating the ObtainDiamond task as the ultimate goal. This limitation stems from the narrowly defined set of actions available to agents, requiring them to learn effective long-horizon strategies from scratch. Consequently, discovering diverse gameplay opportunities in the open world becomes challenging. In this work, we introduce Odyssey, a new framework that empowers Large Language Model (LLM)-based agents with open-world skills to explore the vast Minecraft world. Odyssey comprises three key parts: (1) An interactive agent with an open-world skill library that consists of 40 primitive skills and 183 compositional skills. (2) A fine-tuned LLaMA-3 model trained on a large question-answering dataset with 390k+ instruction entries derived from the Minecraft Wiki. (3) A new agent capability benchmark includes the long-term planning task, the dynamic-immediate planning task, and the autonomous exploration task. Extensive experiments demonstrate that the proposed Odyssey framework can effectively evaluate different capabilities of LLM-based agents. All datasets, model weights, and code are publicly available to motivate future research on more advanced autonomous agent solutions.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unexplainability of Artificial Intelligence Judgments in Kant's Perspective</title>
<link>https://arxiv.org/abs/2407.18950</link>
<guid>https://arxiv.org/abs/2407.18950</guid>
<content:encoded><![CDATA[
arXiv:2407.18950v3 Announce Type: replace 
Abstract: Kant's Critique of Pure Reason, a major contribution to the history of epistemology, proposes a table of categories to elucidate the structure of the a priori principles underlying human judgment. Artificial intelligence (AI) technology, grounded in functionalism, claims to simulate or replicate human judgment. To evaluate this claim, it is necessary to examine whether AI judgments exhibit the essential characteristics of human judgment. This paper investigates the unexplainability of AI judgments through the lens of Kant's theory of judgment. Drawing on Kant's four logical forms-quantity, quality, relation, and modality-this study identifies what may be called AI's uncertainty, a condition in which different forms of judgment become entangled. In particular, with regard to modality, this study argues that the SoftMax function forcibly reframes AI judgments as possibility judgments. Furthermore, since complete definitions in natural language are impossible, words are, by their very nature, ultimately unexplainable; therefore, a fully complete functional implementation is theoretically unattainable.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReflectDiffu:Reflect between Emotion-intent Contagion and Mimicry for Empathetic Response Generation via a RL-Diffusion Framework</title>
<link>https://arxiv.org/abs/2409.10289</link>
<guid>https://arxiv.org/abs/2409.10289</guid>
<content:encoded><![CDATA[
arXiv:2409.10289v4 Announce Type: replace 
Abstract: Empathetic response generation necessitates the integration of emotional and intentional dynamics to foster meaningful interactions. Existing research either neglects the intricate interplay between emotion and intent, leading to suboptimal controllability of empathy, or resorts to large language models (LLMs), which incur significant computational overhead. In this paper, we introduce ReflectDiffu, a lightweight and comprehensive framework for empathetic response generation. This framework incorporates emotion contagion to augment emotional expressiveness and employs an emotion-reasoning mask to pinpoint critical emotional elements. Additionally, it integrates intent mimicry within reinforcement learning for refinement during diffusion. By harnessing an intent twice reflect mechanism of Exploring-Sampling-Correcting, ReflectDiffu adeptly translates emotional decision-making into precise intent actions, thereby addressing empathetic response misalignments stemming from emotional misrecognition. Through reflection, the framework maps emotional states to intents, markedly enhancing both response empathy and flexibility. Comprehensive experiments reveal that ReflectDiffu outperforms existing models regarding relevance, controllability, and informativeness, achieving state-of-the-art results in both automatic and human evaluations.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A personalized time-resolved 3D mesh generative model for unveiling normal heart dynamics</title>
<link>https://arxiv.org/abs/2409.13825</link>
<guid>https://arxiv.org/abs/2409.13825</guid>
<content:encoded><![CDATA[
arXiv:2409.13825v3 Announce Type: replace 
Abstract: Understanding the structure and motion of the heart is crucial for diagnosing and managing cardiovascular diseases, the leading cause of global death. There is wide variation in cardiac shape and motion patterns, influenced by demographic, anthropometric and disease factors. Unravelling normal patterns of shape and motion, and understanding how each individual deviates from the norm, would facilitate accurate diagnosis and personalised treatment strategies. To this end, we developed a conditional generative model, MeshHeart, to learn the distribution of shape and motion patterns for the left and right ventricles of the heart. To model the high-dimensional spatio-temporal mesh data, MeshHeart employs a geometric encoder to represent cardiac meshes in a latent space, and a temporal Transformer to model the motion dynamics of latent representations. Based on MeshHeart, we investigate the latent space of 3D+t cardiac mesh sequences and propose a distance metric, latent delta, which quantifies the deviation of a real heart from its personalised normative pattern. In experiments using a large cardiac magnetic resonance image dataset of 38,309 subjects from the UK Biobank, MeshHeart demonstrates high performance in cardiac mesh sequence reconstruction and generation. Latent space features are discriminative for cardiac disease classification, whereas latent delta exhibits strong correlations with clinical phenotypes in phenome-wide association studies. The code and the trained model are released to support further research.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Model Based Agents: State-of-the-Art, Cooperation Paradigms, Security and Privacy, and Future Trends</title>
<link>https://arxiv.org/abs/2409.14457</link>
<guid>https://arxiv.org/abs/2409.14457</guid>
<content:encoded><![CDATA[
arXiv:2409.14457v3 Announce Type: replace 
Abstract: With the rapid advancement of large models (LMs), the development of general-purpose intelligent agents powered by LMs has become a reality. It is foreseeable that in the near future, LM-driven general AI agents will serve as essential tools in production tasks, capable of autonomous communication and collaboration without human intervention. This paper investigates scenarios involving the autonomous collaboration of future LM agents. We review the current state of LM agents, the key technologies enabling LM agent collaboration, and the security and privacy challenges they face during cooperative operations. To this end, we first explore the foundational principles of LM agents, including their general architecture, key components, enabling technologies, and modern applications. We then discuss practical collaboration paradigms from data, computation, and knowledge perspectives to achieve connected intelligence among LM agents. After that, we analyze the security vulnerabilities and privacy risks associated with LM agents, particularly in multi-agent settings, examining underlying mechanisms and reviewing current and potential countermeasures. Lastly, we propose future research directions for building robust and secure LM agent ecosystems.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>G\"odel Agent: A Self-Referential Agent Framework for Recursive Self-Improvement</title>
<link>https://arxiv.org/abs/2410.04444</link>
<guid>https://arxiv.org/abs/2410.04444</guid>
<content:encoded><![CDATA[
arXiv:2410.04444v4 Announce Type: replace 
Abstract: The rapid advancement of large language models (LLMs) has significantly enhanced the capabilities of AI-driven agents across various tasks. However, existing agentic systems, whether based on fixed pipeline algorithms or pre-defined meta-learning frameworks, cannot search the whole agent design space due to the restriction of human-designed components, and thus might miss the globally optimal agent design. In this paper, we introduce G\"odel Agent, a self-evolving framework inspired by the G\"odel machine, enabling agents to recursively improve themselves without relying on predefined routines or fixed optimization algorithms. G\"odel Agent leverages LLMs to dynamically modify its own logic and behavior, guided solely by high-level objectives through prompting. Experimental results on mathematical reasoning and complex agent tasks demonstrate that implementation of G\"odel Agent can achieve continuous self-improvement, surpassing manually crafted agents in performance, efficiency, and generalizability.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundations and Recent Trends in Multimodal Mobile Agents: A Survey</title>
<link>https://arxiv.org/abs/2411.02006</link>
<guid>https://arxiv.org/abs/2411.02006</guid>
<content:encoded><![CDATA[
arXiv:2411.02006v2 Announce Type: replace 
Abstract: Mobile agents are essential for automating tasks in complex and dynamic mobile environments. As foundation models evolve, the demands for agents that can adapt in real-time and process multimodal data have grown. This survey provides a comprehensive review of mobile agent technologies, focusing on recent advancements that enhance real-time adaptability and multimodal interaction. Recent evaluation benchmarks have been developed better to capture the static and interactive environments of mobile tasks, offering more accurate assessments of agents' performance. We then categorize these advancements into two main approaches: prompt-based methods, which utilize large language models (LLMs) for instruction-based task execution, and training-based methods, which fine-tune multimodal models for mobile-specific applications. Additionally, we explore complementary technologies that augment agent performance. By discussing key challenges and outlining future research directions, this survey offers valuable insights for advancing mobile agent technologies. A comprehensive resource list is available at https://github.com/aialt/awesome-mobile-agents
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing LLM Hallucination from Within: Perturbation-Driven Approach via Internal Knowledge</title>
<link>https://arxiv.org/abs/2411.09689</link>
<guid>https://arxiv.org/abs/2411.09689</guid>
<content:encoded><![CDATA[
arXiv:2411.09689v2 Announce Type: replace 
Abstract: LLM hallucination, where unfaithful text is generated, presents a critical challenge for LLMs' practical applications. Current detection methods often resort to external knowledge, LLM fine-tuning, or supervised training with large hallucination-labeled datasets. Moreover, these approaches do not distinguish between different types of hallucinations, which is crucial for enhancing detection performance. To address such limitations, we introduce hallucination probing, a new task that classifies LLM-generated text into three categories: aligned, misaligned, and fabricated. Driven by our novel discovery that perturbing key entities in prompts affects LLM's generation of these three types of text differently, we propose SHINE, a novel hallucination probing method that does not require external knowledge, supervised training, or LLM fine-tuning. SHINE is effective in hallucination probing across three modern LLMs, and achieves state-of-the-art performance in hallucination detection, outperforming seven competing methods across four datasets and four LLMs, underscoring the importance of probing for accurate detection.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Evolution and Future Perspectives of Artificial Intelligence Generated Content</title>
<link>https://arxiv.org/abs/2412.01948</link>
<guid>https://arxiv.org/abs/2412.01948</guid>
<content:encoded><![CDATA[
arXiv:2412.01948v2 Announce Type: replace 
Abstract: Artificial intelligence generated content (AIGC), a rapidly advancing technology, is transforming content creation across domains, such as text, images, audio, and video. Its growing potential has attracted more and more researchers and investors to explore and expand its possibilities. This review traces AIGC's evolution through four developmental milestones-ranging from early rule-based systems to modern transfer learning models-within a unified framework that highlights how each milestone contributes uniquely to content generation. In particular, the paper employs a common example across all milestones to illustrate the capabilities and limitations of methods within each phase, providing a consistent evaluation of AIGC methodologies and their development. Furthermore, this paper addresses critical challenges associated with AIGC and proposes actionable strategies to mitigate them. This study aims to guide researchers and practitioners in selecting and optimizing AIGC models to enhance the quality and efficiency of content creation across diverse domains.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Data Governance of Frontier AI Models</title>
<link>https://arxiv.org/abs/2412.03824</link>
<guid>https://arxiv.org/abs/2412.03824</guid>
<content:encoded><![CDATA[
arXiv:2412.03824v2 Announce Type: replace 
Abstract: Data is essential to train and fine-tune today's frontier artificial intelligence (AI) models and to develop future ones. To date, academic, legal, and regulatory work has primarily addressed how data can directly harm consumers and creators, such as through privacy breaches, copyright infringements, and bias and discrimination. Our work, instead, focuses on the comparatively neglected question of how data can enable new governance capacities for frontier AI models. This approach for "frontier data governance" opens up new avenues for monitoring and mitigating risks from advanced AI models, particularly as they scale and acquire specific dangerous capabilities. Still, frontier data governance faces challenges that stem from the fundamental properties of data itself: data is non-rival, often non-excludable, easily replicable, and increasingly synthesizable. Despite these inherent difficulties, we propose a set of policy mechanisms targeting key actors along the data supply chain, including data producers, aggregators, model developers, and data vendors. We provide a brief overview of 15 governance mechanisms, of which we centrally introduce five, underexplored policy recommendations. These include developing canary tokens to detect unauthorized use for producers; (automated) data filtering to remove malicious content for pre-training and post-training datasets; mandatory dataset reporting requirements for developers and vendors; improved security for datasets and data generation algorithms; and know-your-customer requirements for vendors. By considering data not just as a source of potential harm, but as a critical governance lever, this work aims to equip policymakers with a new tool for the governance and regulation of frontier AI models.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RLZero: Direct Policy Inference from Language Without In-Domain Supervision</title>
<link>https://arxiv.org/abs/2412.05718</link>
<guid>https://arxiv.org/abs/2412.05718</guid>
<content:encoded><![CDATA[
arXiv:2412.05718v2 Announce Type: replace 
Abstract: The reward hypothesis states that all goals and purposes can be understood as the maximization of a received scalar reward signal. However, in practice, defining such a reward signal is notoriously difficult, as humans are often unable to predict the optimal behavior corresponding to a reward function. Natural language offers an intuitive alternative for instructing reinforcement learning (RL) agents, yet previous language-conditioned approaches either require costly supervision or test-time training given a language instruction. In this work, we present a new approach that uses a pretrained RL agent trained using only unlabeled, offline interactions--without task-specific supervision or labeled trajectories--to get zero-shot test-time policy inference from arbitrary natural language instructions. We introduce a framework comprising three steps: imagine, project, and imitate. First, the agent imagines a sequence of observations corresponding to the provided language description using video generative models. Next, these imagined observations are projected into the target environment domain. Finally, an agent pretrained in the target environment with unsupervised RL instantly imitates the projected observation sequence through a closed-form solution. To the best of our knowledge, our method, RLZero, is the first approach to show direct language-to-behavior generation abilities on a variety of tasks and environments without any in-domain supervision. We further show that components of RLZero can be used to generate policies zero-shot from cross-embodied videos, such as those available on YouTube, even for complex embodiments like humanoids.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stepwise Reasoning Error Disruption Attack of LLMs</title>
<link>https://arxiv.org/abs/2412.11934</link>
<guid>https://arxiv.org/abs/2412.11934</guid>
<content:encoded><![CDATA[
arXiv:2412.11934v4 Announce Type: replace 
Abstract: Large language models (LLMs) have made remarkable strides in complex reasoning tasks, but their safety and robustness in reasoning processes remain underexplored. Existing attacks on LLM reasoning are constrained by specific settings or lack of imperceptibility, limiting their feasibility and generalizability. To address these challenges, we propose the Stepwise rEasoning Error Disruption (SEED) attack, which subtly injects errors into prior reasoning steps to mislead the model into producing incorrect subsequent reasoning and final answers. Unlike previous methods, SEED is compatible with zero-shot and few-shot settings, maintains the natural reasoning flow, and ensures covert execution without modifying the instruction. Extensive experiments on four datasets across four different models demonstrate SEED's effectiveness, revealing the vulnerabilities of LLMs to disruptions in reasoning processes. These findings underscore the need for greater attention to the robustness of LLM reasoning to ensure safety in practical applications. Our code is available at: https://github.com/Applied-Machine-Learning-Lab/SEED-Attack.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Your LLMs Capable of Stable Reasoning?</title>
<link>https://arxiv.org/abs/2412.13147</link>
<guid>https://arxiv.org/abs/2412.13147</guid>
<content:encoded><![CDATA[
arXiv:2412.13147v4 Announce Type: replace 
Abstract: The rapid advancement of large language models (LLMs) has shown remarkable progress in complex reasoning tasks. However, a significant disparity exists between benchmark performances and real-world applications. We attribute this gap primarily to current evaluation protocols and metrics, which inadequately capture the full spectrum of LLM capabilities, especially in complex reasoning tasks where both accuracy and consistency are essential. In this paper, we introduce G-Pass@$k$, a novel evaluation metric that continuously assesses model performance across multiple sampling attempts, quantifying both the model's performance potential and its stability. Through extensive experiments on various public and newly constructed benchmarks, we employ G-Pass@$k$ in conjunction with state-of-the-art large language models to provide comprehensive insights into their potential capabilities and operational consistency. Our findings reveal a significant opportunity to enhance the realistic reasoning abilities of LLMs, underscoring the necessity for more robust evaluation metrics.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind Your Theory: Theory of Mind Goes Deeper Than Reasoning</title>
<link>https://arxiv.org/abs/2412.13631</link>
<guid>https://arxiv.org/abs/2412.13631</guid>
<content:encoded><![CDATA[
arXiv:2412.13631v3 Announce Type: replace 
Abstract: Theory of Mind (ToM) capabilities in LLMs have recently become a central object of investigation. Cognitive science distinguishes between two steps required for ToM tasks: 1) determine whether to invoke ToM, which includes the appropriate Depth of Mentalizing (DoM), or level of recursion required to complete a task; and 2) applying the correct inference given the DoM. In this position paper, we first identify several lines of work in different communities in AI, including LLM benchmarking, ToM add-ons, ToM probing, and formal models for ToM. We argue that recent work in AI tends to focus exclusively on the second step which are typically framed as static logic problems. We conclude with suggestions for improved evaluation of ToM capabilities inspired by dynamic environments used in cognitive tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are We in the AI-Generated Text World Already? Quantifying and Monitoring AIGT on Social Media</title>
<link>https://arxiv.org/abs/2412.18148</link>
<guid>https://arxiv.org/abs/2412.18148</guid>
<content:encoded><![CDATA[
arXiv:2412.18148v3 Announce Type: replace 
Abstract: Social media platforms are experiencing a growing presence of AI-Generated Texts (AIGTs). However, the misuse of AIGTs could have profound implications for public opinion, such as spreading misinformation and manipulating narratives. Despite its importance, it remains unclear how prevalent AIGTs are on social media. To address this gap, this paper aims to quantify and monitor the AIGTs on online social media platforms. We first collect a dataset (SM-D) with around 2.4M posts from 3 major social media platforms: Medium, Quora, and Reddit. Then, we construct a diverse dataset (AIGTBench) to train and evaluate AIGT detectors. AIGTBench combines popular open-source datasets and our AIGT datasets generated from social media texts by 12 LLMs, serving as a benchmark for evaluating mainstream detectors. With this setup, we identify the best-performing detector (OSM-Det). We then apply OSM-Det to SM-D to track AIGTs across social media platforms from January 2022 to October 2024, using the AI Attribution Rate (AAR) as the metric. Specifically, Medium and Quora exhibit marked increases in AAR, rising from 1.77% to 37.03% and 2.06% to 38.95%, respectively. In contrast, Reddit shows slower growth, with AAR increasing from 1.31% to 2.45% over the same period. Our further analysis indicates that AIGTs on social media differ from human-written texts across several dimensions, including linguistic patterns, topic distributions, engagement levels, and the follower distribution of authors. We envision our analysis and findings on AIGTs in social media can shed light on future research in this domain.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Value Compass Benchmarks: A Platform for Fundamental and Validated Evaluation of LLMs Values</title>
<link>https://arxiv.org/abs/2501.07071</link>
<guid>https://arxiv.org/abs/2501.07071</guid>
<content:encoded><![CDATA[
arXiv:2501.07071v3 Announce Type: replace 
Abstract: As Large Language Models (LLMs) achieve remarkable breakthroughs, aligning their values with humans has become imperative for their responsible development and customized applications. However, there still lack evaluations of LLMs values that fulfill three desirable goals. (1) Value Clarification: We expect to clarify the underlying values of LLMs precisely and comprehensively, while current evaluations focus narrowly on safety risks such as bias and toxicity. (2) Evaluation Validity: Existing static, open-source benchmarks are prone to data contamination and quickly become obsolete as LLMs evolve. Additionally, these discriminative evaluations uncover LLMs' knowledge about values, rather than valid assessments of LLMs' behavioral conformity to values. (3) Value Pluralism: The pluralistic nature of human values across individuals and cultures is largely ignored in measuring LLMs value alignment. To address these challenges, we presents the Value Compass Benchmarks, with three correspondingly designed modules. It (i) grounds the evaluation on motivationally distinct \textit{basic values to clarify LLMs' underlying values from a holistic view; (ii) applies a \textit{generative evolving evaluation framework with adaptive test items for evolving LLMs and direct value recognition from behaviors in realistic scenarios; (iii) propose a metric that quantifies LLMs alignment with a specific value as a weighted sum over multiple dimensions, with weights determined by pluralistic values.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feedback-Aware Monte Carlo Tree Search for Efficient Information Seeking in Goal-Oriented Conversations</title>
<link>https://arxiv.org/abs/2501.15056</link>
<guid>https://arxiv.org/abs/2501.15056</guid>
<content:encoded><![CDATA[
arXiv:2501.15056v2 Announce Type: replace 
Abstract: Effective decision-making and problem-solving in conversational systems require the ability to identify and acquire missing information through targeted questioning. A key challenge lies in efficiently narrowing down a large space of possible outcomes by posing questions that minimize uncertainty. To address this, we introduce a novel framework that leverages Large Language Models (LLMs) to generate information-seeking questions, with Monte Carlo Tree Search (MCTS) to strategically select questions that maximize information gain, as a part of inference-time planning. Our primary contribution includes a hierarchical feedback mechanism that exploits past interaction patterns to guide future strategy. Specifically, each new problem is mapped to a cluster based on semantic similarity, and our UCT (Upper Confidence bound for Trees) formulation employs a cluster-specific bonus reward to prioritize successful question trajectories that have proven effective for similar problems in the past. Extensive empirical evaluation across medical diagnosis and technical troubleshooting domains shows that our method achieves an average of 12% improvement in success rates and about 10x reduction in the number of LLM calls made for planning per conversation, compared to the state of the art. An additional 8% gain in success rate is observed on average when we start with a constrained set of possibilities. Our results underscore the efficacy of feedback-aware MCTS in enhancing information-seeking in goal-oriented dialogues.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaptiveStep: Automatically Dividing Reasoning Step through Model Confidence</title>
<link>https://arxiv.org/abs/2502.13943</link>
<guid>https://arxiv.org/abs/2502.13943</guid>
<content:encoded><![CDATA[
arXiv:2502.13943v2 Announce Type: replace 
Abstract: Current approaches for training Process Reward Models (PRMs) often involve breaking down responses into multiple reasoning steps using rule-based techniques, such as using predefined placeholder tokens or setting the reasoning step's length into a fixed size. These approaches overlook the fact that specific words do not typically mark true decision points in a text. To address this, we propose AdaptiveStep, a method that divides reasoning steps based on the model's confidence in predicting the next word. This division method provides more decision-making information at each step, enhancing downstream tasks, such as reward model learning. Moreover, our method does not require manual annotation. We demonstrate its effectiveness through experiments with AdaptiveStep-trained PRMs in mathematical reasoning and code generation tasks. Experimental results indicate that the outcome PRM achieves state-of-the-art Best-of-N performance, surpassing greedy search strategy with token-level value-guided decoding, while also reducing construction costs by over 30% compared to existing open-source PRMs. In addition, we provide a thorough analysis and case study on the PRM's performance, transferability, and generalization capabilities.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Perceptions to Decisions: Wildfire Evacuation Decision Prediction with Behavioral Theory-informed LLMs</title>
<link>https://arxiv.org/abs/2502.17701</link>
<guid>https://arxiv.org/abs/2502.17701</guid>
<content:encoded><![CDATA[
arXiv:2502.17701v2 Announce Type: replace 
Abstract: Evacuation decision prediction is critical for efficient and effective wildfire response by helping emergency management anticipate traffic congestion and bottlenecks, allocate resources, and minimize negative impacts. Traditional statistical methods for evacuation decision prediction fail to capture the complex and diverse behavioral logic of different individuals. In this work, for the first time, we introduce FLARE, short for facilitating LLM for advanced reasoning on wildfire evacuation decision prediction, a Large Language Model (LLM)-based framework that integrates behavioral theories and models to streamline the Chain-of-Thought (CoT) reasoning and subsequently integrate with memory-based Reinforcement Learning (RL) module to provide accurate evacuation decision prediction and understanding. Our proposed method addresses the limitations of using existing LLMs for evacuation behavioral predictions, such as limited survey data, mismatching with behavioral theory, conflicting individual preferences, implicit and complex mental states, and intractable mental state-behavior mapping. Experiments on three post-wildfire survey datasets show an average of 20.47% performance improvement over traditional theory-informed behavioral models, with strong cross-event generalizability. Our complete code is publicly available at https://github.com/SusuXu-s-Lab/FLARE
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZEBRA: Leveraging Model-Behavioral Knowledge for Zero-Annotation Preference Dataset Construction</title>
<link>https://arxiv.org/abs/2502.18744</link>
<guid>https://arxiv.org/abs/2502.18744</guid>
<content:encoded><![CDATA[
arXiv:2502.18744v3 Announce Type: replace 
Abstract: Recent efforts in LLM alignment have focused on constructing large-scale preference datasets via human or Artificial Intelligence (AI) annotators. However, such approaches rely on instance-wise supervision, incurring substantial annotation cost and limited interpretability. In this paper, we propose ZEBRA - a model behavior-wise zero-annotation framework that constructs preference data by leveraging model behavior knowledge derived from benchmark performances. ZEBRA binarizes response pairs by evaluating the quality and similarity of their origin models, entirely bypassing instance-level annotation. This allows scalable, controllable, and cost-effective alignment data generation. Empirical results show that ZEBRA achieves alignment performance comparable to instance-supervised methods, despite requiring no manual or model-based labeling.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Neural Clause-Selection Reinforcement</title>
<link>https://arxiv.org/abs/2503.07792</link>
<guid>https://arxiv.org/abs/2503.07792</guid>
<content:encoded><![CDATA[
arXiv:2503.07792v2 Announce Type: replace 
Abstract: Clause selection is arguably the most important choice point in saturation-based theorem proving. Framing it as a reinforcement learning (RL) task is a way to challenge the human-designed heuristics of state-of-the-art provers and to instead automatically evolve -- just from prover experiences -- their potentially optimal replacement. In this work, we present a neural network architecture for scoring clauses for clause selection that is powerful yet efficient to evaluate. Following RL principles to make design decisions, we integrate the network into the Vampire theorem prover and train it from successful proof attempts. An experiment on the diverse TPTP benchmark finds the neurally guided prover improve over a baseline strategy, from which it initially learns--in terms of the number of in-training-unseen problems solved under a practically relevant, short CPU instruction limit--by 20%.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Action-Gradient Monte Carlo Tree Search for Non-Parametric Continuous (PO)MDPs</title>
<link>https://arxiv.org/abs/2503.12181</link>
<guid>https://arxiv.org/abs/2503.12181</guid>
<content:encoded><![CDATA[
arXiv:2503.12181v3 Announce Type: replace 
Abstract: Autonomous systems that operate in continuous state, action, and observation spaces require planning and reasoning under uncertainty. Existing online planning methods for such POMDPs are almost exclusively sample-based, yet they forego the power of high-dimensional gradient optimization as combining it into Monte Carlo Tree Search (MCTS) has proved difficult, especially in non-parametric settings. We close this gap with three contributions. First, we derive a novel action-gradient theorem for both MDPs and POMDPs in terms of transition likelihoods, making gradient information accessible during tree search. Second, we introduce the Multiple Importance Sampling (MIS) tree, that re-uses samples for changing action branches, yielding consistent value estimates that enable in-search gradient steps. Third, we derive exact transition probability computation via the area formula for smooth generative models common in physical domains, a result of independent interest. These elements combine into Action-Gradient Monte Carlo Tree Search (AGMCTS), the first planner to blend non-parametric particle search with online gradient refinement in POMDPs. Across several challenging continuous MDP and POMDP benchmarks, AGMCTS outperforms widely-used sample-only solvers in solution quality.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaWorld: Learning Adaptable World Models with Latent Actions</title>
<link>https://arxiv.org/abs/2503.18938</link>
<guid>https://arxiv.org/abs/2503.18938</guid>
<content:encoded><![CDATA[
arXiv:2503.18938v4 Announce Type: replace 
Abstract: World models aim to learn action-controlled future prediction and have proven essential for the development of intelligent agents. However, most existing world models rely heavily on substantial action-labeled data and costly training, making it challenging to adapt to novel environments with heterogeneous actions through limited interactions. This limitation can hinder their applicability across broader domains. To overcome this limitation, we propose AdaWorld, an innovative world model learning approach that enables efficient adaptation. The key idea is to incorporate action information during the pretraining of world models. This is achieved by extracting latent actions from videos in a self-supervised manner, capturing the most critical transitions between frames. We then develop an autoregressive world model that conditions on these latent actions. This learning paradigm enables highly adaptable world models, facilitating efficient transfer and learning of new actions even with limited interactions and finetuning. Our comprehensive experiments across multiple environments demonstrate that AdaWorld achieves superior performance in both simulation quality and visual planning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?</title>
<link>https://arxiv.org/abs/2503.19990</link>
<guid>https://arxiv.org/abs/2503.19990</guid>
<content:encoded><![CDATA[
arXiv:2503.19990v2 Announce Type: replace 
Abstract: Multi-step spatial reasoning entails understanding and reasoning about spatial relationships across multiple sequential steps, which is crucial for tackling complex real-world applications, such as robotic manipulation, autonomous navigation, and automated assembly. To assess how well current Multimodal Large Language Models (MLLMs) have acquired this fundamental capability, we introduce LEGO-Puzzles, a scalable benchmark designed to evaluate both spatial understanding and sequential reasoning in MLLMs through LEGO-based tasks. LEGO-Puzzles consists of 1,100 carefully curated visual question-answering (VQA) samples spanning 11 distinct tasks, ranging from basic spatial understanding to complex multi-step reasoning. Based on LEGO-Puzzles, we conduct a comprehensive evaluation of 20 state-of-the-art MLLMs and uncover significant limitations in their spatial reasoning capabilities: even the most powerful MLLMs can answer only about half of the test cases, whereas human participants achieve over 90% accuracy. Furthermore, based on LEGO-Puzzles, we design generation tasks to investigate whether MLLMs can transfer their spatial understanding and reasoning abilities to image generation. Our experiments show that only GPT-4o and Gemini-2.0-Flash exhibit a limited ability to follow these instructions, while other MLLMs either replicate the input image or generate completely irrelevant outputs. Overall, LEGO-Puzzles exposes critical deficiencies in existing MLLMs' spatial understanding and sequential reasoning capabilities, and underscores the need for further advancements in multimodal spatial reasoning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language and Reasoning Models are Shallow Disjunctive Reasoners</title>
<link>https://arxiv.org/abs/2503.23487</link>
<guid>https://arxiv.org/abs/2503.23487</guid>
<content:encoded><![CDATA[
arXiv:2503.23487v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have been found to struggle with systematic reasoning. Even on tasks where they appear to perform well, their performance often depends on shortcuts, rather than on genuine reasoning abilities, leading them to collapse on out-of-distribution (OOD) examples. Post-training strategies based on reinforcement learning and chain-of-thought prompting have recently been hailed as a step change. However, little is known about the potential of the resulting ``Large Reasoning Models'' (LRMs) beyond maths and programming-based problem solving, where genuine OOD problems can be sparse. In this paper, we focus on tasks that require systematic relational composition for qualitative spatial and temporal reasoning. The setting allows fine control over problem difficulty to precisely measure OOD generalization. We find that, zero-shot LRMs generally outperform their LLM counterparts in single-path reasoning tasks but struggle in the multi-path setting. Whilst showing comparatively better results, fine-tuned LLMs are also not capable of multi-path generalization. We also provide evidence for the behavioral interpretation for this, i.e., that LRMs are shallow disjunctive reasoners.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Acting Less is Reasoning More! Teaching Model to Act Efficiently</title>
<link>https://arxiv.org/abs/2504.14870</link>
<guid>https://arxiv.org/abs/2504.14870</guid>
<content:encoded><![CDATA[
arXiv:2504.14870v2 Announce Type: replace 
Abstract: Tool-integrated reasoning (TIR) augments large language models (LLMs) with the ability to invoke external tools during long-form reasoning, such as search engines and code interpreters, to solve tasks beyond the capabilities of internal reasoning. While reinforcement learning (RL) has shown promise in training such agents, most of existing approaches typically optimize only for final correctness without considering the efficiency or necessity of external tool use. This often leads to excessive tool calling, incurring high computational costs and hindering the development of internal reasoning capabilities - a phenomenon known as \textit{cognitive offloading}. To this end, we propose Optimal Tool Call-controlled Policy Optimization (OTC-PO), a simple yet effective RL-based framework that encourages models to produce accurate answers with minimal tool calls. Our method introduces a tool-integrated reward that jointly considers answer correctness and corresponding tool use behavior of model to reach that answer. To validate the effectiveness, we introduce the metric of \textit{tool productivity}, defined as the ratio between the number of correct answers and the total number of tool calls across all test cases. This metric reflects how efficiently tool usage contributes to successful task completion, with higher values indicating smarter and more autonomous reasoning. We instantiate this framework within both Proximal Policy Optimization (PPO) and Group Relative Preference Optimization (GRPO), resulting in OTC-PPO and OTC-GRPO. Experiments with Qwen-2.5 and Qwen-Math across multiple QA benchmarks show that our approach reduces tool calls by up to 68.3\% and improves tool productivity by up to 215.4\%, while maintaining comparable answer accuracy.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-AI Governance (HAIG): A Trust-Utility Approach</title>
<link>https://arxiv.org/abs/2505.01651</link>
<guid>https://arxiv.org/abs/2505.01651</guid>
<content:encoded><![CDATA[
arXiv:2505.01651v2 Announce Type: replace 
Abstract: This paper introduces the HAIG framework for analysing trust dynamics across evolving human-AI relationships. Current categorical frameworks (e.g., "human-in-the-loop" models) inadequately capture how AI systems evolve from tools to partners, particularly as foundation models demonstrate emergent capabilities and multi-agent systems exhibit autonomous goal-setting behaviours. As systems advance, agency redistributes in complex patterns that are better represented as positions along continua rather than discrete categories, though progression may include both gradual shifts and significant step changes. The HAIG framework operates across three levels: dimensions (Decision Authority Distribution, Process Autonomy, and Accountability Configuration), continua (gradual shifts along each dimension), and thresholds (critical points requiring governance adaptation). Unlike risk-based or principle-based approaches, HAIG adopts a trust-utility orientation, focusing on maintaining appropriate trust relationships that maximise utility while ensuring sufficient safeguards. Our analysis reveals how technical advances in self-supervision, reasoning authority, and distributed decision-making drive non-uniform trust evolution across both contextual variation and technological advancement. Case studies in healthcare and European regulation demonstrate how HAIG complements existing frameworks while offering a foundation for alternative approaches that anticipate governance challenges before they emerge.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How well do LLMs reason over tabular data, really?</title>
<link>https://arxiv.org/abs/2505.07453</link>
<guid>https://arxiv.org/abs/2505.07453</guid>
<content:encoded><![CDATA[
arXiv:2505.07453v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) excel in natural language tasks, but less is known about their reasoning capabilities over tabular data. Prior analyses devise evaluation strategies that poorly reflect an LLM's realistic performance on tabular queries. Moreover, we have a limited understanding of the robustness of LLMs towards realistic variations in tabular inputs. Therefore, we ask: Can general-purpose LLMs reason over tabular data, really?, and focus on two questions 1) are tabular reasoning capabilities of general-purpose LLMs robust to real-world characteristics of tabular inputs, and 2) how can we realistically evaluate an LLM's performance on analytical tabular queries? Building on a recent tabular reasoning benchmark, we first surface shortcomings of its multiple-choice prompt evaluation strategy, as well as commonly used free-form text metrics such as SacreBleu and BERT-score. We show that an LLM-as-a-judge procedure yields more reliable performance insights and unveil a significant deficit in tabular reasoning performance of LLMs. We then extend the tabular inputs reflecting three common characteristics in practice: 1) missing values, 2) duplicate entities, and 3) structural variations. Experiments show that the tabular reasoning capabilities of general-purpose LLMs suffer from these variations, stressing the importance of improving their robustness for realistic tabular inputs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategy-Augmented Planning for Large Language Models via Opponent Exploitation</title>
<link>https://arxiv.org/abs/2505.08459</link>
<guid>https://arxiv.org/abs/2505.08459</guid>
<content:encoded><![CDATA[
arXiv:2505.08459v2 Announce Type: replace 
Abstract: Efficiently modeling and exploiting opponents is a long-standing challenge in adversarial domains. Large Language Models (LLMs) trained on extensive textual data have recently demonstrated outstanding performance in general tasks, introducing new research directions for opponent modeling. Some studies primarily focus on directly using LLMs to generate decisions based on the elaborate prompt context that incorporates opponent descriptions, while these approaches are limited to scenarios where LLMs possess adequate domain expertise. To address that, we introduce a two-stage Strategy-Augmented Planning (SAP) framework that significantly enhances the opponent exploitation capabilities of LLM-based agents by utilizing a critical component, the Strategy Evaluation Network (SEN). Specifically, in the offline stage, we construct an explicit strategy space and subsequently collect strategy-outcome pair data for training the SEN network. During the online phase, SAP dynamically recognizes the opponent's strategies and greedily exploits them by searching best response strategy on the well-trained SEN, finally translating strategy to a course of actions by carefully designed prompts. Experimental results show that SAP exhibits robust generalization capabilities, allowing it to perform effectively not only against previously encountered opponent strategies but also against novel, unseen strategies. In the MicroRTS environment, SAP achieves a $85.35\%$ performance improvement over baseline methods and matches the competitiveness of reinforcement learning approaches against state-of-the-art (SOTA) rule-based AI. Our code is available at https://github.com/hsushuai/SAP.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparative Study of SMT and MILP for the Nurse Rostering Problem</title>
<link>https://arxiv.org/abs/2505.10328</link>
<guid>https://arxiv.org/abs/2505.10328</guid>
<content:encoded><![CDATA[
arXiv:2505.10328v2 Announce Type: replace 
Abstract: The effects of personnel scheduling on the quality of care and working conditions for healthcare personnel have been thoroughly documented. However, the ever-present demand and large variation of constraints make healthcare scheduling particularly challenging. This problem has been studied for decades, with limited research aimed at applying Satisfiability Modulo Theories (SMT). SMT has gained momentum within the formal verification community in the last decades, leading to the advancement of SMT solvers that have been shown to outperform standard mathematical programming techniques. In this work, we propose generic constraint formulations that can model a wide range of real-world scheduling constraints. Then, the generic constraints are formulated as SMT and MILP problems and used to compare the respective state-of-the-art solvers, Z3 and Gurobi, on academic and real-world inspired rostering problems. Experimental results show how each solver excels for certain types of problems; the MILP solver generally performs better when the problem is highly constrained or infeasible, while the SMT solver performs better otherwise. On real-world inspired problems containing a more varied set of shifts and personnel, the SMT solver excels. Additionally, it was noted during experimentation that the SMT solver was more sensitive to the way the generic constraints were formulated, requiring careful consideration and experimentation to achieve better performance. We conclude that SMT-based methods present a promising avenue for future research within the domain of personnel scheduling.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment</title>
<link>https://arxiv.org/abs/2505.14667</link>
<guid>https://arxiv.org/abs/2505.14667</guid>
<content:encoded><![CDATA[
arXiv:2505.14667v3 Announce Type: replace 
Abstract: Large Reasoning Models (LRMs) have become powerful tools for complex problem solving, but their structured reasoning pathways can lead to unsafe outputs when exposed to harmful prompts. Existing safety alignment methods reduce harmful outputs but can degrade reasoning depth, leading to significant trade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated jailbreak attacks. To address this, we introduce SAFEPATH, a lightweight alignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at the start of their reasoning, in response to harmful prompts, while leaving the rest of the reasoning process unsupervised. Empirical results across multiple benchmarks indicate that SAFEPATH effectively reduces harmful outputs while maintaining reasoning performance. Specifically, SAFEPATH reduces harmful responses by up to 90.0% and blocks 83.3% of jailbreak attempts in the DeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than Direct Refusal and 314.1x less than SafeChain. We further introduce a zero-shot variant that requires no fine-tuning. In addition, we provide a comprehensive analysis of how existing methods in LLMs generalize, or fail, when applied to reasoning-centric models, revealing critical gaps and new directions for safer AI.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Flow-Lenia Universes with a Curiosity-driven AI Scientist: Discovering Diverse Ecosystem Dynamics</title>
<link>https://arxiv.org/abs/2505.15998</link>
<guid>https://arxiv.org/abs/2505.15998</guid>
<content:encoded><![CDATA[
arXiv:2505.15998v2 Announce Type: replace 
Abstract: We present a method for the automated discovery of system-level dynamics in Flow-Lenia--a continuous cellular automaton (CA) with mass conservation and parameter localization-using a curiosity--driven AI scientist. This method aims to uncover processes leading to self-organization of evolutionary and ecosystemic dynamics in CAs. We build on previous work which uses diversity search algorithms in Lenia to find self-organized individual patterns, and extend it to large environments that support distinct interacting patterns. We adapt Intrinsically Motivated Goal Exploration Processes (IMGEPs) to drive exploration of diverse Flow-Lenia environments using simulation-wide metrics, such as evolutionary activity, compression-based complexity, and multi-scale entropy. We test our method in two experiments, showcasing its ability to illuminate significantly more diverse dynamics compared to random search. We show qualitative results illustrating how ecosystemic simulations enable self-organization of complex collective behaviors not captured by previous individual pattern search and analysis. We complement automated discovery with an interactive exploration tool, creating an effective human-AI collaborative workflow for scientific investigation. Though demonstrated specifically with Flow-Lenia, this methodology provides a framework potentially applicable to other parameterizable complex systems where understanding emergent collective properties is of interest.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyGenar: An LLM-Driven Hybrid Genetic Algorithm for Few-Shot Grammar Generation</title>
<link>https://arxiv.org/abs/2505.16978</link>
<guid>https://arxiv.org/abs/2505.16978</guid>
<content:encoded><![CDATA[
arXiv:2505.16978v2 Announce Type: replace 
Abstract: Grammar plays a critical role in natural language processing and text/code generation by enabling the definition of syntax, the creation of parsers, and guiding structured outputs. Although large language models (LLMs) demonstrate impressive capabilities across domains, their ability to infer and generate grammars has not yet been thoroughly explored. In this paper, we aim to study and improve the ability of LLMs for few-shot grammar generation, where grammars are inferred from sets of a small number of positive and negative examples and generated in Backus-Naur Form. To explore this, we introduced a novel dataset comprising 540 structured grammar generation challenges, devised 6 metrics, and evaluated 8 various LLMs against it. Our findings reveal that existing LLMs perform sub-optimally in grammar generation. To address this, we propose an LLM-driven hybrid genetic algorithm, namely HyGenar, to optimize grammar generation. HyGenar achieves substantial improvements in both the syntactic and semantic correctness of generated grammars across LLMs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enumerate-Conjecture-Prove: Formally Solving Answer-Construction Problems in Math Competitions</title>
<link>https://arxiv.org/abs/2505.18492</link>
<guid>https://arxiv.org/abs/2505.18492</guid>
<content:encoded><![CDATA[
arXiv:2505.18492v2 Announce Type: replace 
Abstract: Mathematical reasoning lies at the heart of artificial intelligence, underpinning applications in education, program verification, and research-level mathematical discovery. Mathematical competitions, in particular, present two challenging problem types: theorem-proving, requiring rigorous proofs of stated conclusions, and answer-construction, involving hypothesizing and formally verifying mathematical objects. Large Language Models (LLMs) effectively generate creative candidate answers but struggle with formal verification, while symbolic provers ensure rigor but cannot efficiently handle creative conjecture generation. We introduce the Enumerate-Conjecture-Prove (ECP) framework, a modular neuro-symbolic method integrating LLM-based enumeration and pattern-driven conjecturing with formal theorem proving. We present ConstructiveBench, a dataset of 3,431 answer-construction problems in various math competitions with verified Lean formalizations.
  On the ConstructiveBench dataset, ECP improves the accuracy of answer construction from the Chain-of-Thought (CoT) baseline of 14.54% to 45.06% using the GPT-4.1-mini model. Moreover, when combined with ECP's constructed answers, the state-of-the-art DeepSeek-Prover-V2-7B model generates correct proofs for 858 of the 3,431 constructive problems in Lean, achieving 25.01% accuracy, compared to 9.86% for symbolic-only baselines. Our code and dataset are publicly available at https://github.com/JackSun200312/ECP.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics Reasoning</title>
<link>https://arxiv.org/abs/2505.19099</link>
<guid>https://arxiv.org/abs/2505.19099</guid>
<content:encoded><![CDATA[
arXiv:2505.19099v3 Announce Type: replace 
Abstract: We present SeePhys, a large-scale multimodal benchmark for LLM reasoning grounded in physics questions ranging from middle school to PhD qualifying exams. The benchmark covers 7 fundamental domains spanning the physics discipline, incorporating 21 categories of highly heterogeneous diagrams. In contrast to prior works where visual elements mainly serve auxiliary purposes, our benchmark features a substantial proportion of vision-essential problems (75%) that mandate visual information extraction for correct solutions. Through extensive evaluation, we observe that even the most advanced visual reasoning models (e.g., Gemini-2.5-pro and o4-mini) achieve sub-60% accuracy on our benchmark. These results reveal fundamental challenges in current large language models' visual understanding capabilities, particularly in: (i) establishing rigorous coupling between diagram interpretation and physics reasoning, and (ii) overcoming their persistent reliance on textual cues as cognitive shortcuts.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OrgAccess: A Benchmark for Role Based Access Control in Organization Scale LLMs</title>
<link>https://arxiv.org/abs/2505.19165</link>
<guid>https://arxiv.org/abs/2505.19165</guid>
<content:encoded><![CDATA[
arXiv:2505.19165v2 Announce Type: replace 
Abstract: Role-based access control (RBAC) and hierarchical structures are foundational to how information flows and decisions are made within virtually all organizations. As the potential of Large Language Models (LLMs) to serve as unified knowledge repositories and intelligent assistants in enterprise settings becomes increasingly apparent, a critical, yet under explored, challenge emerges: \textit{can these models reliably understand and operate within the complex, often nuanced, constraints imposed by organizational hierarchies and associated permissions?} Evaluating this crucial capability is inherently difficult due to the proprietary and sensitive nature of real-world corporate data and access control policies. We introduce a synthetic yet representative \textbf{OrgAccess} benchmark consisting of 40 distinct types of permissions commonly relevant across different organizational roles and levels. We further create three types of permissions: 40,000 easy (1 permission), 10,000 medium (3-permissions tuple), and 20,000 hard (5-permissions tuple) to test LLMs' ability to accurately assess these permissions and generate responses that strictly adhere to the specified hierarchical rules, particularly in scenarios involving users with overlapping or conflicting permissions. Our findings reveal that even state-of-the-art LLMs struggle significantly to maintain compliance with role-based structures, even with explicit instructions, with their performance degrades further when navigating interactions involving two or more conflicting permissions. Specifically, even \textbf{GPT-4.1 only achieves an F1-Score of 0.27 on our hardest benchmark}. This demonstrates a critical limitation in LLMs' complex rule following and compositional reasoning capabilities beyond standard factual or STEM-based benchmarks, opening up a new paradigm for evaluating their fitness for practical, structured environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffVLA: Vision-Language Guided Diffusion Planning for Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.19381</link>
<guid>https://arxiv.org/abs/2505.19381</guid>
<content:encoded><![CDATA[
arXiv:2505.19381v3 Announce Type: replace 
Abstract: Research interest in end-to-end autonomous driving has surged owing to its fully differentiable design integrating modular tasks, i.e. perception, prediction and planing, which enables optimization in pursuit of the ultimate goal. Despite the great potential of the end-to-end paradigm, existing methods suffer from several aspects including expensive BEV (bird's eye view) computation, action diversity, and sub-optimal decision in complex real-world scenarios. To address these challenges, we propose a novel hybrid sparse-dense diffusion policy, empowered by a Vision-Language Model (VLM), called Diff-VLA. We explore the sparse diffusion representation for efficient multi-modal driving behavior. Moreover, we rethink the effectiveness of VLM driving decision and improve the trajectory generation guidance through deep interaction across agent, map instances and VLM output. Our method shows superior performance in Autonomous Grand Challenge 2025 which contains challenging real and reactive synthetic scenarios. Our methods achieves 45.0 PDMS.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Many Challenges of Human-Like Agents in Virtual Game Environments</title>
<link>https://arxiv.org/abs/2505.20011</link>
<guid>https://arxiv.org/abs/2505.20011</guid>
<content:encoded><![CDATA[
arXiv:2505.20011v2 Announce Type: replace 
Abstract: Human-like agents are an increasingly important topic in games and beyond. Believable non-player characters enhance the gaming experience by improving immersion and providing entertainment. They also offer players the opportunity to engage with AI entities that can function as opponents, teachers, or cooperating partners. Additionally, in games where bots are prohibited -- and even more so in non-game environments -- there is a need for methods capable of identifying whether digital interactions occur with bots or humans. This leads to two fundamental research questions: (1) how to model and implement human-like AI, and (2) how to measure its degree of human likeness.
  This article offers two contributions. The first one is a survey of the most significant challenges in implementing human-like AI in games (or any virtual environment featuring simulated agents, although this article specifically focuses on games). Thirteen such challenges, both conceptual and technical, are discussed in detail. The second is an empirical study performed in a tactical video game that addresses the research question: "Is it possible to distinguish human players from bots (AI agents) based on empirical data?" A machine-learning approach using a custom deep recurrent convolutional neural network is presented. We hypothesize that the more challenging it is to create human-like AI for a given game, the easier it becomes to develop a method for distinguishing humans from AI-driven players.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Translation Consistent Semi-supervised Segmentation for 3D Medical Images</title>
<link>https://arxiv.org/abs/2203.14523</link>
<guid>https://arxiv.org/abs/2203.14523</guid>
<content:encoded><![CDATA[
arXiv:2203.14523v3 Announce Type: replace-cross 
Abstract: 3D medical image segmentation methods have been successful, but their dependence on large amounts of voxel-level annotated data is a disadvantage that needs to be addressed given the high cost to obtain such annotation. Semi-supervised learning (SSL) solve this issue by training models with a large unlabelled and a small labelled dataset. The most successful SSL approaches are based on consistency learning that minimises the distance between model responses obtained from perturbed views of the unlabelled data. These perturbations usually keep the spatial input context between views fairly consistent, which may cause the model to learn segmentation patterns from the spatial input contexts instead of the segmented objects. In this paper, we introduce the Translation Consistent Co-training (TraCoCo) which is a consistency learning SSL method that perturbs the input data views by varying their spatial input context, allowing the model to learn segmentation patterns from visual objects. Furthermore, we propose the replacement of the commonly used mean squared error (MSE) semi-supervised loss by a new Cross-model confident Binary Cross entropy (CBC) loss, which improves training convergence and keeps the robustness to co-training pseudo-labelling mistakes. We also extend CutMix augmentation to 3D SSL to further improve generalisation. Our TraCoCo shows state-of-the-art results for the Left Atrium (LA) and Brain Tumor Segmentation (BRaTS19) datasets with different backbones. Our code is available at https://github.com/yyliu01/TraCoCo.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning in Business Analytics: A Clash of Expectations and Reality</title>
<link>https://arxiv.org/abs/2205.09337</link>
<guid>https://arxiv.org/abs/2205.09337</guid>
<content:encoded><![CDATA[
arXiv:2205.09337v2 Announce Type: replace-cross 
Abstract: Our fast-paced digital economy shaped by global competition requires increased data-driven decision-making based on artificial intelligence (AI) and machine learning (ML). The benefits of deep learning (DL) are manifold, but it comes with limitations that have, so far, interfered with widespread industry adoption. This paper explains why DL, despite its popularity, has difficulties speeding up its adoption within business analytics. It is shown that the adoption of deep learning is not only affected by computational complexity, lacking big data architecture, lack of transparency (black-box), skill shortage, and leadership commitment, but also by the fact that DL does not outperform traditional ML models in the case of structured datasets with fixed-length feature vectors. Deep learning should be regarded as a powerful addition to the existing body of ML models instead of a one size fits all solution. The results strongly suggest that gradient boosting can be seen as the go-to model for predictions on structured datasets within business analytics. In addition to the empirical study based on three industry use cases, the paper offers a comprehensive discussion of those results, practical implications, and a roadmap for future research.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated machine learning: AI-driven decision making in business analytics</title>
<link>https://arxiv.org/abs/2205.10538</link>
<guid>https://arxiv.org/abs/2205.10538</guid>
<content:encoded><![CDATA[
arXiv:2205.10538v2 Announce Type: replace-cross 
Abstract: The realization that AI-driven decision-making is indispensable in today's fast-paced and ultra-competitive marketplace has raised interest in industrial machine learning (ML) applications significantly. The current demand for analytics experts vastly exceeds the supply. One solution to this problem is to increase the user-friendliness of ML frameworks to make them more accessible for the non-expert. Automated machine learning (AutoML) is an attempt to solve the problem of expertise by providing fully automated off-the-shelf solutions for model choice and hyperparameter tuning. This paper analyzed the potential of AutoML for applications within business analytics, which could help to increase the adoption rate of ML across all industries. The H2O AutoML framework was benchmarked against a manually tuned stacked ML model on three real-world datasets. The manually tuned ML model could reach a performance advantage in all three case studies used in the experiment. Nevertheless, the H2O AutoML package proved to be quite potent. It is fast, easy to use, and delivers reliable results, which come close to a professionally tuned ML model. The H2O AutoML framework in its current capacity is a valuable tool to support fast prototyping with the potential to shorten development and deployment cycles. It can also bridge the existing gap between supply and demand for ML experts and is a big step towards automated decisions in business analytics. Finally, AutoML has the potential to foster human empowerment in a world that is rapidly becoming more automated and digital.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conflict-Aware Pseudo Labeling via Optimal Transport for Entity Alignment</title>
<link>https://arxiv.org/abs/2209.01847</link>
<guid>https://arxiv.org/abs/2209.01847</guid>
<content:encoded><![CDATA[
arXiv:2209.01847v3 Announce Type: replace-cross 
Abstract: Entity alignment aims to discover unique equivalent entity pairs with the same meaning across different knowledge graphs (KGs). Existing models have focused on projecting KGs into a latent embedding space so that inherent semantics between entities can be captured for entity alignment. However, the adverse impacts of alignment conflicts have been largely overlooked during training, thereby limiting the entity alignment performance. To address this issue, we propose a novel Conflict-aware Pseudo Labeling via Optimal Transport model (CPL-OT) for entity alignment. The key idea is to iteratively pseudo-label alignment pairs empowered with conflict-aware optimal transport (OT) modeling to boost the precision of entity alignment. CPL-OT is composed of two key components -- entity embedding learning with global-local aggregation and iterative conflict-aware pseudo labeling -- that mutually reinforce each other. To mitigate alignment conflicts during pseudo labeling, we propose to use optimal transport as an effective means to warrant one-to-one entity alignment between two KGs with the minimal overall transport cost. Extensive experiments on benchmark datasets validate the superiority of CPL-OT over state-of-the-art baselines under both settings with and without prior alignment seeds.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nash Equilibria, Regularization and Computation in Optimal Transport-Based Distributionally Robust Optimization</title>
<link>https://arxiv.org/abs/2303.03900</link>
<guid>https://arxiv.org/abs/2303.03900</guid>
<content:encoded><![CDATA[
arXiv:2303.03900v3 Announce Type: replace-cross 
Abstract: We study optimal transport-based distributionally robust optimization problems where a fictitious adversary, often envisioned as nature, can choose the distribution of the uncertain problem parameters by reshaping a prescribed reference distribution at a finite transportation cost. In this framework, we show that robustification is intimately related to various forms of variation and Lipschitz regularization even if the transportation cost function fails to be (some power of) a metric. We also derive conditions for the existence and the computability of a Nash equilibrium between the decision-maker and nature, and we demonstrate numerically that nature's Nash strategy can be viewed as a distribution that is supported on remarkably deceptive adversarial samples. Finally, we identify practically relevant classes of optimal transport-based distributionally robust optimization problems that can be addressed with efficient gradient descent algorithms even if the loss function or the transportation cost function are nonconvex (but not both at the same time).
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Neural Lambda Calculus: Neurosymbolic AI Applied to the Foundations of Functional Programming</title>
<link>https://arxiv.org/abs/2304.09276</link>
<guid>https://arxiv.org/abs/2304.09276</guid>
<content:encoded><![CDATA[
arXiv:2304.09276v2 Announce Type: replace-cross 
Abstract: Over the last decades, deep neural networks based-models became the dominant paradigm in machine learning. Further, the use of artificial neural networks in symbolic learning has been seen as increasingly relevant recently. To study the capabilities of neural networks in the symbolic AI domain, researchers have explored the ability of deep neural networks to learn mathematical constructions, such as addition and multiplication, logic inference, such as theorem provers, and even the execution of computer programs. The latter is known to be too complex a task for neural networks. Therefore, the results were not always successful, and often required the introduction of biased elements in the learning process, in addition to restricting the scope of possible programs to be executed. In this work, we will analyze the ability of neural networks to learn how to execute programs as a whole. To do so, we propose a different approach. Instead of using an imperative programming language, with complex structures, we use the Lambda Calculus ({\lambda}-Calculus), a simple, but Turing-Complete mathematical formalism, which serves as the basis for modern functional programming languages and is at the heart of computability theory. We will introduce the use of integrated neural learning and lambda calculi formalization. Finally, we explore execution of a program in {\lambda}-Calculus is based on reductions, we will show that it is enough to learn how to perform these reductions so that we can execute any program. Keywords: Machine Learning, Lambda Calculus, Neurosymbolic AI, Neural Networks, Transformer Model, Sequence-to-Sequence Models, Computational Models
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Should a Leader Act Suboptimally? The Role of Inferability in Repeated Stackelberg Games</title>
<link>https://arxiv.org/abs/2310.00468</link>
<guid>https://arxiv.org/abs/2310.00468</guid>
<content:encoded><![CDATA[
arXiv:2310.00468v3 Announce Type: replace-cross 
Abstract: When interacting with other decision-making agents in non-adversarial scenarios, it is critical for an autonomous agent to have inferable behavior: The agent's actions must convey their intention and strategy. We model the inferability problem using Stackelberg games with observations where a leader and a follower repeatedly interact. During the interactions, the leader uses a fixed mixed strategy. The follower does not know the leader's strategy and dynamically reacts to the statistically inferred strategy based on the leader's previous actions. In the inference setting, the leader may have a lower performance compared to the setting where the follower has full information on the leader's strategy. We refer to the performance gap between these settings as the inferability gap. For a variety of game settings, we show that the inferability gap is upper-bounded by a function of the number of interactions and the stochasticity level of the leader's strategy, encouraging the use of inferable strategies with lower stochasticity levels. We also analyze bimatrix Stackelberg games and identify a set of games where the leader's near-optimal strategy may potentially suffer from a large inferability gap.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supercharging Graph Transformers with Advective Diffusion</title>
<link>https://arxiv.org/abs/2310.06417</link>
<guid>https://arxiv.org/abs/2310.06417</guid>
<content:encoded><![CDATA[
arXiv:2310.06417v3 Announce Type: replace-cross 
Abstract: The capability of generalization is a cornerstone for the success of modern learning systems. For non-Euclidean data, e.g., graphs, that particularly involves topological structures, one important aspect neglected by prior studies is how machine learning models generalize under topological shifts. This paper proposes AdvDIFFormer, a physics-inspired graph Transformer model designed to address this challenge. The model is derived from advective diffusion equations which describe a class of continuous message passing process with observed and latent topological structures. We show that AdvDIFFormer has provable capability for controlling generalization error with topological shifts, which in contrast cannot be guaranteed by graph diffusion models. Empirically, the model demonstrates superiority in various predictive tasks across information networks, molecular screening and protein interactions.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Successor Features with Distributed Hebbian Temporal Memory</title>
<link>https://arxiv.org/abs/2310.13391</link>
<guid>https://arxiv.org/abs/2310.13391</guid>
<content:encoded><![CDATA[
arXiv:2310.13391v4 Announce Type: replace-cross 
Abstract: This paper presents a novel approach to address the challenge of online sequence learning for decision making under uncertainty in non-stationary, partially observable environments. The proposed algorithm, Distributed Hebbian Temporal Memory (DHTM), is based on the factor graph formalism and a multi-component neuron model. DHTM aims to capture sequential data relationships and make cumulative predictions about future observations, forming Successor Features (SFs). Inspired by neurophysiological models of the neocortex, the algorithm uses distributed representations, sparse transition matrices, and local Hebbian-like learning rules to overcome the instability and slow learning of traditional temporal memory algorithms such as RNN and HMM. Experimental results show that DHTM outperforms LSTM, RWKV and a biologically inspired HMM-like algorithm, CSCG, on non-stationary data sets. Our results suggest that DHTM is a promising approach to address the challenges of online sequence learning and planning in dynamic environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate Differential Operators for Hybrid Neural Fields</title>
<link>https://arxiv.org/abs/2312.05984</link>
<guid>https://arxiv.org/abs/2312.05984</guid>
<content:encoded><![CDATA[
arXiv:2312.05984v2 Announce Type: replace-cross 
Abstract: Neural fields have become widely used in various fields, from shape representation to neural rendering, and for solving partial differential equations (PDEs). With the advent of hybrid neural field representations like Instant NGP that leverage small MLPs and explicit representations, these models train quickly and can fit large scenes. Yet in many applications like rendering and simulation, hybrid neural fields can cause noticeable and unreasonable artifacts. This is because they do not yield accurate spatial derivatives needed for these downstream applications. In this work, we propose two ways to circumvent these challenges. Our first approach is a post hoc operator that uses local polynomial fitting to obtain more accurate derivatives from pre-trained hybrid neural fields. Additionally, we also propose a self-supervised fine-tuning approach that refines the hybrid neural field to yield accurate derivatives directly while preserving the initial signal. We show applications of our method to rendering, collision simulation, and solving PDEs. We observe that using our approach yields more accurate derivatives, reducing artifacts and leading to more accurate simulations in downstream applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Meta-Prompting</title>
<link>https://arxiv.org/abs/2312.06562</link>
<guid>https://arxiv.org/abs/2312.06562</guid>
<content:encoded><![CDATA[
arXiv:2312.06562v3 Announce Type: replace-cross 
Abstract: Modern large language models (LLMs) are capable of interpreting input strings as instructions, or prompts, and carry out tasks based on them. Unlike traditional learners, LLMs cannot use back-propagation to obtain feedback, and condition their output in situ in a phenomenon known as in-context learning (ICL). Many approaches to prompting and pre-training these models involve the automated generation of these prompts, also known as meta-prompting, or prompting to obtain prompts. However, they do not formally describe the properties and behavior of the LLMs themselves. We propose a theoretical framework based on category theory to generalize and describe ICL and LLM behavior when interacting with users. Our framework allows us to obtain formal results around task agnosticity and equivalence of various meta-prompting approaches. Using our framework and experimental results we argue that meta-prompting is more effective than basic prompting at generating desirable outputs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StarVector: Generating Scalable Vector Graphics Code from Images and Text</title>
<link>https://arxiv.org/abs/2312.11556</link>
<guid>https://arxiv.org/abs/2312.11556</guid>
<content:encoded><![CDATA[
arXiv:2312.11556v4 Announce Type: replace-cross 
Abstract: Scalable Vector Graphics (SVGs) are vital for modern image rendering due to their scalability and versatility. Previous SVG generation methods have focused on curve-based vectorization, lacking semantic understanding, often producing artifacts, and struggling with SVG primitives beyond path curves. To address these issues, we introduce StarVector, a multimodal large language model for SVG generation. It performs image vectorization by understanding image semantics and using SVG primitives for compact, precise outputs. Unlike traditional methods, StarVector works directly in the SVG code space, leveraging visual understanding to apply accurate SVG primitives. To train StarVector, we create SVG-Stack, a diverse dataset of 2M samples that enables generalization across vectorization tasks and precise use of primitives like ellipses, polygons, and text. We address challenges in SVG evaluation, showing that pixel-based metrics like MSE fail to capture the unique qualities of vector graphics. We introduce SVG-Bench, a benchmark across 10 datasets, and 3 tasks: Image-to-SVG, Text-to-SVG generation, and diagram generation. Using this setup, StarVector achieves state-of-the-art performance, producing more compact and semantically rich SVGs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CCFC: Bridging Federated Clustering and Contrastive Learning</title>
<link>https://arxiv.org/abs/2401.06634</link>
<guid>https://arxiv.org/abs/2401.06634</guid>
<content:encoded><![CDATA[
arXiv:2401.06634v2 Announce Type: replace-cross 
Abstract: Federated clustering, an essential extension of centralized clustering for federated scenarios, enables multiple data-holding clients to collaboratively group data while keeping their data locally. In centralized scenarios, clustering driven by representation learning has made significant advancements in handling high-dimensional complex data. However, the combination of federated clustering and representation learning remains underexplored. To bridge this, we first tailor a cluster-contrastive model for learning clustering-friendly representations. Then, we harness this model as the foundation for proposing a new federated clustering method, named cluster-contrastive federated clustering (CCFC). Benefiting from representation learning, the clustering performance of CCFC even double those of the best baseline methods in some cases. Compared to the most related baseline, the benefit results in substantial NMI score improvements of up to 0.4155 on the most conspicuous case. Moreover, CCFC also shows superior performance in handling device failures from a practical viewpoint.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Push the Button! Exploring Data Leakage Risks in Machine Learning and Transfer Learning</title>
<link>https://arxiv.org/abs/2401.13796</link>
<guid>https://arxiv.org/abs/2401.13796</guid>
<content:encoded><![CDATA[
arXiv:2401.13796v3 Announce Type: replace-cross 
Abstract: Machine Learning (ML) has revolutionized various domains, offering predictive capabilities in several areas. However, with the increasing accessibility of ML tools, many practitioners, lacking deep ML expertise, adopt a "push the button" approach, utilizing user-friendly interfaces without a thorough understanding of underlying algorithms. While this approach provides convenience, it raises concerns about the reliability of outcomes, leading to challenges such as incorrect performance evaluation. This paper addresses a critical issue in ML, known as data leakage, where unintended information contaminates the training data, impacting model performance evaluation. Users, due to a lack of understanding, may inadvertently overlook crucial steps, leading to optimistic performance estimates that may not hold in real-world scenarios. The discrepancy between evaluated and actual performance on new data is a significant concern. In particular, this paper categorizes data leakage in ML, discussing how certain conditions can propagate through the ML workflow. Furthermore, it explores the connection between data leakage and the specific task being addressed, investigates its occurrence in Transfer Learning, and compares standard inductive ML with transductive ML frameworks. The conclusion summarizes key findings, emphasizing the importance of addressing data leakage for robust and reliable ML applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Insight into Security Code Review with LLMs: Capabilities, Obstacles, and Influential Factors</title>
<link>https://arxiv.org/abs/2401.16310</link>
<guid>https://arxiv.org/abs/2401.16310</guid>
<content:encoded><![CDATA[
arXiv:2401.16310v4 Announce Type: replace-cross 
Abstract: Security code review is a time-consuming and labor-intensive process typically requiring integration with automated security defect detection tools. However, existing security analysis tools struggle with poor generalization, high false positive rates, and coarse detection granularity. Large Language Models (LLMs) have been considered promising candidates for addressing those challenges. In this study, we conducted an empirical study to explore the potential of LLMs in detecting security defects during code review. Specifically, we evaluated the performance of six LLMs under five different prompts and compared them with state-of-the-art static analysis tools. We also performed linguistic and regression analyses for the best-performing LLM to identify quality problems in its responses and factors influencing its performance. Our findings showthat: (1) existing pre-trained LLMs have limited capability in security code review but significantly outperformthe state-of-the-art static analysis tools. (2) GPT-4 performs best among all LLMs when provided with a CWE list for reference. (3) GPT-4 frequently generates verbose or non-compliant responses with the task requirements given in the prompts. (4) GPT-4 is more adept at identifying security defects in code files with fewer tokens, containing functional logic, or written by developers with less involvement in the project.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PreGIP: Watermarking the Pretraining of Graph Neural Networks for Deep Intellectual Property Protection</title>
<link>https://arxiv.org/abs/2402.04435</link>
<guid>https://arxiv.org/abs/2402.04435</guid>
<content:encoded><![CDATA[
arXiv:2402.04435v2 Announce Type: replace-cross 
Abstract: Pretraining on Graph Neural Networks (GNNs) has shown great power in facilitating various downstream tasks. As pretraining generally requires huge amount of data and computational resources, the pretrained GNNs are high-value Intellectual Properties (IP) of the legitimate owner. However, adversaries may illegally copy and deploy the pretrained GNN models for their downstream tasks. Though initial efforts have been made to watermark GNN classifiers for IP protection, these methods require the target classification task for watermarking, and thus are not applicable to self-supervised pretraining of GNN models. Hence, in this work, we propose a novel framework named PreGIP to watermark the pretraining of GNN encoder for IP protection while maintain the high-quality of the embedding space. PreGIP incorporates a task-free watermarking loss to watermark the embedding space of pretrained GNN encoder. A finetuning-resistant watermark injection is further deployed. Theoretical analysis and extensive experiments show the effectiveness of {\method} in IP protection and maintaining high-performance for downstream tasks.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perturbative partial moment matching and gradient-flow adaptive importance sampling transformations for Bayesian leave one out cross-validation</title>
<link>https://arxiv.org/abs/2402.08151</link>
<guid>https://arxiv.org/abs/2402.08151</guid>
<content:encoded><![CDATA[
arXiv:2402.08151v3 Announce Type: replace-cross 
Abstract: Importance sampling (IS) allows one to approximate leave one out (LOO) cross-validation for a Bayesian model, without refitting, by inverting the Bayesian update equation to subtract a given data point from a model posterior. For each data point, one computes expectations under the corresponding LOO posterior by weighted averaging over the full data posterior. This task sometimes requires weight stabilization in the form of adapting the posterior distribution via transformation. So long as one is successful in finding a suitable transformation, one avoids refitting. To this end, we motivate the use of bijective perturbative transformations of the form $T(\boldsymbol{\theta})=\boldsymbol{\theta} + h Q(\boldsymbol{\theta}),$ for $0<h\ll 1,$ and introduce two classes of such transformations: 1) partial moment matching and 2) gradient flow evolution. The former extends prior literature on moment-matching under the recognition that adaptation for LOO is a small perturbation on the full data posterior. The latter class of methods define transformations based on relaxing various statistical objectives: in our case the variance of the IS estimator and the KL divergence between the transformed distribution and the statistics of the LOO fold. Being model-specific, the gradient flow transformations require evaluating Jacobian determinants. While these quantities are generally readily available through auto-differentiation, we derive closed-form expressions in the case of logistic regression and shallow ReLU activated neural networks. We tested the methodology on an $n\ll p$ dataset that is known to produce unstable LOO IS weights.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SongComposer: A Large Language Model for Lyric and Melody Generation in Song Composition</title>
<link>https://arxiv.org/abs/2402.17645</link>
<guid>https://arxiv.org/abs/2402.17645</guid>
<content:encoded><![CDATA[
arXiv:2402.17645v2 Announce Type: replace-cross 
Abstract: Creating lyrics and melodies for the vocal track in a symbolic format, known as song composition, demands expert musical knowledge of melody, an advanced understanding of lyrics, and precise alignment between them. Despite achievements in sub-tasks such as lyric generation, lyric-to-melody, and melody-to-lyric, etc, a unified model for song composition has not yet been achieved. In this paper, we introduce SongComposer, a pioneering step towards a unified song composition model that can readily create symbolic lyrics and melodies following instructions. SongComposer is a music-specialized large language model (LLM) that, for the first time, integrates the capability of simultaneously composing lyrics and melodies into LLMs by leveraging three key innovations: 1) a flexible tuple format for word-level alignment of lyrics and melodies, 2) an extended tokenizer vocabulary for song notes, with scalar initialization based on musical knowledge to capture rhythm, and 3) a multi-stage pipeline that captures musical structure, starting with motif-level melody patterns and progressing to phrase-level structure for improved coherence. Extensive experiments demonstrate that SongComposer outperforms advanced LLMs, including GPT-4, in tasks such as lyric-to-melody generation, melody-to-lyric generation, song continuation, and text-to-song creation. Moreover, we will release SongCompose, a large-scale dataset for training, containing paired lyrics and melodies in Chinese and English.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Multimodal Unified Representations for Cross Modal Generalization</title>
<link>https://arxiv.org/abs/2403.05168</link>
<guid>https://arxiv.org/abs/2403.05168</guid>
<content:encoded><![CDATA[
arXiv:2403.05168v3 Announce Type: replace-cross 
Abstract: To enhance the interpretability of multimodal unified representations, many studies have focused on discrete unified representations. These efforts typically start with contrastive learning and gradually extend to the disentanglement of modal information, achieving solid multimodal discrete unified representations. However, existing research often overlooks two critical issues: 1) The use of Euclidean distance for quantization in discrete representations often overlooks the important distinctions among different dimensions of features, resulting in redundant representations after quantization; 2) Different modalities have unique characteristics, and a uniform alignment approach does not fully exploit these traits. To address these issues, we propose Training-free Optimization of Codebook (TOC) and Fine and Coarse cross-modal Information Disentangling (FCID). These methods refine the unified discrete representations from pretraining and perform fine- and coarse-grained information disentanglement tailored to the specific characteristics of each modality, achieving significant performance improvements over previous state-of-the-art models. The code is available at https://github.com/haihuangcode/CMG.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CleanAgent: Automating Data Standardization with LLM-based Agents</title>
<link>https://arxiv.org/abs/2403.08291</link>
<guid>https://arxiv.org/abs/2403.08291</guid>
<content:encoded><![CDATA[
arXiv:2403.08291v4 Announce Type: replace-cross 
Abstract: Data standardization is a crucial part of the data science life cycle. While tools like Pandas offer robust functionalities, their complexity and the manual effort required for customizing code to diverse column types pose significant challenges. Although large language models (LLMs) like ChatGPT have shown promise in automating this process through natural language understanding and code generation, it still demands expert-level programming knowledge and continuous interaction for prompt refinement. To solve these challenges, our key idea is to propose a Python library with declarative, unified APIs for standardizing different column types, simplifying the LLM's code generation with concise API calls. We first propose Dataprep.Clean, a component of the Dataprep Python Library, significantly reduces the coding complexity by enabling the standardization of specific column types with a single line of code. Then, we introduce the CleanAgent framework integrating Dataprep.Clean and LLM-based agents to automate the data standardization process. With CleanAgent, data scientists only need to provide their requirements once, allowing for a hands-free process. To demonstrate the practical utility of CleanAgent, we developed a user-friendly web application, allowing users to interact with it using real-world datasets.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Macroeconomic Policies through Dynamic Stackelberg Mean-Field Games</title>
<link>https://arxiv.org/abs/2403.12093</link>
<guid>https://arxiv.org/abs/2403.12093</guid>
<content:encoded><![CDATA[
arXiv:2403.12093v4 Announce Type: replace-cross 
Abstract: Macroeconomic outcomes emerge from individuals' decisions, making it essential to model how agents interact with macro policy via consumption, investment, and labor choices. We formulate this as a dynamic Stackelberg game: the government (leader) sets policies, and agents (followers) respond by optimizing their behavior over time. Unlike static models, this dynamic formulation captures temporal dependencies and strategic feedback critical to policy design. However, as the number of agents increases, explicitly simulating all agent-agent and agent-government interactions becomes computationally infeasible. To address this, we propose the Dynamic Stackelberg Mean Field Game (DSMFG) framework, which approximates these complex interactions via agent-population and government-population couplings. This approximation preserves individual-level feedback while ensuring scalability, enabling DSMFG to jointly model three core features of real-world policymaking: dynamic feedback, asymmetry, and large scale. We further introduce Stackelberg Mean Field Reinforcement Learning (SMFRL), a data-driven algorithm that learns the leader's optimal policies while maintaining personalized responses for individual agents. Empirically, we validate our approach in a large-scale simulated economy, where it scales to 1,000 agents (vs. 100 in prior work) and achieves a fourfold increase in GDP over classical economic methods and a nineteenfold improvement over the static 2022 U.S. federal income tax policy.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>White Men Lead, Black Women Help? Benchmarking and Mitigating Language Agency Social Biases in LLMs</title>
<link>https://arxiv.org/abs/2404.10508</link>
<guid>https://arxiv.org/abs/2404.10508</guid>
<content:encoded><![CDATA[
arXiv:2404.10508v5 Announce Type: replace-cross 
Abstract: Social biases can manifest in language agency. However, very limited research has investigated such biases in Large Language Model (LLM)-generated content. In addition, previous works often rely on string-matching techniques to identify agentic and communal words within texts, falling short of accurately classifying language agency. We introduce the Language Agency Bias Evaluation (LABE) benchmark, which comprehensively evaluates biases in LLMs by analyzing agency levels attributed to different demographic groups in model generations. LABE tests for gender, racial, and intersectional language agency biases in LLMs on 3 text generation tasks: biographies, professor reviews, and reference letters. Using LABE, we unveil language agency social biases in 3 recent LLMs: ChatGPT, Llama3, and Mistral. We observe that: (1) LLM generations tend to demonstrate greater gender bias than human-written texts; (2) Models demonstrate remarkably higher levels of intersectional bias than the other bias aspects. (3) Prompt-based mitigation is unstable and frequently leads to bias exacerbation. Based on our observations, we propose Mitigation via Selective Rewrite (MSR), a novel bias mitigation strategy that leverages an agency classifier to identify and selectively revise parts of generated texts that demonstrate communal traits. Empirical results prove MSR to be more effective and reliable than prompt-based mitigation method, showing a promising research direction.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guided-SPSA: Simultaneous Perturbation Stochastic Approximation assisted by the Parameter Shift Rule</title>
<link>https://arxiv.org/abs/2404.15751</link>
<guid>https://arxiv.org/abs/2404.15751</guid>
<content:encoded><![CDATA[
arXiv:2404.15751v2 Announce Type: replace-cross 
Abstract: The study of variational quantum algorithms (VQCs) has received significant attention from the quantum computing community in recent years. These hybrid algorithms, utilizing both classical and quantum components, are well-suited for noisy intermediate-scale quantum devices. Though estimating exact gradients using the parameter-shift rule to optimize the VQCs is realizable in NISQ devices, they do not scale well for larger problem sizes. The computational complexity, in terms of the number of circuit evaluations required for gradient estimation by the parameter-shift rule, scales linearly with the number of parameters in VQCs. On the other hand, techniques that approximate the gradients of the VQCs, such as the simultaneous perturbation stochastic approximation (SPSA), do not scale with the number of parameters but struggle with instability and often attain suboptimal solutions. In this work, we introduce a novel gradient estimation approach called Guided-SPSA, which meaningfully combines the parameter-shift rule and SPSA-based gradient approximation. The Guided-SPSA results in a 15% to 25% reduction in the number of circuit evaluations required during training for a similar or better optimality of the solution found compared to the parameter-shift rule. The Guided-SPSA outperforms standard SPSA in all scenarios and outperforms the parameter-shift rule in scenarios such as suboptimal initialization of the parameters. We demonstrate numerically the performance of Guided-SPSA on different paradigms of quantum machine learning, such as regression, classification, and reinforcement learning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture of insighTful Experts (MoTE): The Synergy of Thought Chains and Expert Mixtures in Self-Alignment</title>
<link>https://arxiv.org/abs/2405.00557</link>
<guid>https://arxiv.org/abs/2405.00557</guid>
<content:encoded><![CDATA[
arXiv:2405.00557v5 Announce Type: replace-cross 
Abstract: As the capabilities of large language models (LLMs) continue to expand, aligning these models with human values remains a significant challenge. Recent studies show that reasoning abilities contribute significantly to model safety, while integrating Mixture-of-Experts (MoE) architectures can further enhance alignment. In this work, we address a fundamental question: How to effectively incorporate reasoning abilities and MoE architectures into self-alignment process in LLMs? We propose Mixture of insighTful Experts (MoTE), a novel framework that synergistically combines reasoning chains and expert mixtures to improve self-alignments. From a data perspective, MoTE employs a structured reasoning chain comprising four key stages: Question Analysis, Answer Guidance, Safe Answer, and Safety Checking. This approach enhances safety through multi-step reasoning and proves effective even for smaller and less powerful LLMs (e.g., 7B models). From an architectural perspective, MoTE adopts a multi-LoRA framework with step-level routing, where each expert is dedicated to a specific reasoning step. This design eliminates the need for balance losses, ensures stable training, and supports adaptive inference lengths. Experimental results demonstrate that MoTE significantly improves model safety, jailbreak resistance, and over-refusal capabilities, achieving performance comparable to OpenAI's state-of-the-art o1 model.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wake Vision: A Tailored Dataset and Benchmark Suite for TinyML Computer Vision Applications</title>
<link>https://arxiv.org/abs/2405.00892</link>
<guid>https://arxiv.org/abs/2405.00892</guid>
<content:encoded><![CDATA[
arXiv:2405.00892v5 Announce Type: replace-cross 
Abstract: Tiny machine learning (TinyML) for low-power devices lacks systematic methodologies for creating large, high-quality datasets suitable for production-grade systems. We present a novel automated pipeline for generating binary classification datasets that addresses this critical gap through several algorithmic innovations: intelligent multi-source label fusion, confidence-aware filtering, automated label correction, and systematic fine-grained benchmark generation. Crucially, automation is not merely convenient but necessary to cope with TinyML's diverse applications. TinyML requires bespoke datasets tailored to specific deployment constraints and use cases, making manual approaches prohibitively expensive and impractical for widespread adoption. Using our pipeline, we create Wake Vision, a large-scale binary classification dataset of almost 6 million images that demonstrates our methodology through person detection--the canonical vision task for TinyML. Wake Vision achieves up to a 6.6% accuracy improvement over existing datasets via a carefully designed two-stage training strategy and provides 100x more images. We demonstrate our broad applicability for automated large-scale TinyML dataset generation across two additional target categories, and show our label error rates are substantially lower than prior work. Our comprehensive fine-grained benchmark suite evaluates model robustness across five critical dimensions, revealing failure modes masked by aggregate metrics. To ensure continuous improvement, we establish ongoing community engagement through competitions hosted by the Edge AI Foundation. All datasets, benchmarks, and code are available under CC-BY 4.0 license, providing a systematic foundation for advancing TinyML research.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anchored Answers: Unravelling Positional Bias in GPT-2's Multiple-Choice Questions</title>
<link>https://arxiv.org/abs/2405.03205</link>
<guid>https://arxiv.org/abs/2405.03205</guid>
<content:encoded><![CDATA[
arXiv:2405.03205v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs), such as the GPT-4 and LLaMA families, have demonstrated considerable success across diverse tasks, including multiple-choice questions (MCQs). However, these models exhibit a positional bias, particularly an even worse anchored bias in the GPT-2 family, where they consistently favour the first choice 'A' in MCQs during inference. This anchored bias challenges the integrity of GPT-2's decision-making process, as it skews performance based on the position rather than the content of the choices in MCQs. In this study, we utilise the mechanistic interpretability approach to identify the internal modules within GPT-2 models responsible for this bias. We focus on the Multi-Layer Perceptron (MLP) layers and attention heads, using the "logit lens" method to trace and modify the specific value vectors that contribute to the bias. By updating these vectors within MLP and recalibrating attention patterns to neutralise the preference for the first choice 'A', we effectively mitigate the anchored bias. Our interventions not only mitigate the bias but also improve the overall MCQ prediction accuracy for the GPT-2 family across various datasets. This work represents the first comprehensive mechanistic analysis of anchored bias from the failing cases in MCQs within the GPT-2 models, introducing targeted, minimal-intervention strategies that significantly enhance GPT2 model robustness and accuracy in MCQs. Our code is available at https://github.com/ruizheliUOA/Anchored_Bias_GPT2.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to set AdamW's weight decay as you scale model and dataset size</title>
<link>https://arxiv.org/abs/2405.13698</link>
<guid>https://arxiv.org/abs/2405.13698</guid>
<content:encoded><![CDATA[
arXiv:2405.13698v3 Announce Type: replace-cross 
Abstract: The scaling of the optimal AdamW weight decay hyperparameter with model and dataset size is critical as we seek to build larger models, but is poorly understood. We show that weights learned by AdamW can be understood as an exponential moving average (EMA) of recent updates. This gives critical insights for how to set the weight decay in AdamW, and how the weight decay should scale with model and dataset size. In particular, the key hyperparameter for an exponential moving average is the EMA timescale. Intuitively, the EMA timescale can be understood as the number of recent iterations the EMA averages over. We find that the optimal timescale, measured in epochs, is roughly constant as we change model and dataset size. Moreover, given a learning rate, there is a one-to-one mapping from the EMA timescale to the weight decay hyperparameter. Thus, if the optimal EMA timescale is constant, that implies that as the dataset size increases, the optimal weight decay should fall and as the model size increases, the optimal weight decay should increase (if we follow the muP recommendation for scaling the learning rate). We validate these scaling rules on ResNet-18 and Vision Transformers trained on CIFAR-10 and ImageNet, and on NanoGPT pre-training on OpenWebText. Finally, we found that as training progresses, muP's learning rate scaling breaks down for AdamW unless weight decay is scaled appropriately.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StatWhy: Formal Verification Tool for Statistical Hypothesis Testing Programs</title>
<link>https://arxiv.org/abs/2405.17492</link>
<guid>https://arxiv.org/abs/2405.17492</guid>
<content:encoded><![CDATA[
arXiv:2405.17492v3 Announce Type: replace-cross 
Abstract: Statistical methods have been widely misused and misinterpreted in various scientific fields, raising significant concerns about the integrity of scientific research. To mitigate this problem, we propose a tool-assisted method for formally specifying and automatically verifying the correctness of statistical programs. In this method, programmers are required to annotate the source code of the statistical programs with the requirements for these methods. Through this annotation, they are reminded to check the requirements for statistical methods, including those that cannot be formally verified, such as the distribution of the unknown true population. Our software tool StatWhy automatically checks whether programmers have properly specified the requirements for the statistical methods, thereby identifying any missing requirements that need to be addressed. This tool is implemented using the Why3 platform to verify the correctness of OCaml programs that conduct statistical hypothesis testing. We demonstrate how StatWhy can be used to avoid common errors in various statistical hypothesis testing programs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Miss the Forest for the Trees: Attentional Vision Calibration for Large Vision Language Models</title>
<link>https://arxiv.org/abs/2405.17820</link>
<guid>https://arxiv.org/abs/2405.17820</guid>
<content:encoded><![CDATA[
arXiv:2405.17820v2 Announce Type: replace-cross 
Abstract: Large Vision Language Models (LVLMs) demonstrate strong capabilities in visual understanding and description, yet often suffer from hallucinations, attributing incorrect or misleading features to images. We observe that LVLMs disproportionately focus on a small subset of image tokens--termed blind tokens--which are typically irrelevant to the query (e.g., background or non-object regions). We hypothesize that such attention misalignment plays a key role in generating hallucinated responses. To mitigate this issue, we propose Attentional Vision Calibration (AvisC), a test-time approach that dynamically recalibrates the influence of blind tokens without modifying the underlying attention mechanism. AvisC first identifies blind tokens by analyzing layer-wise attention distributions over image tokens, then employs a contrastive decoding strategy to balance the influence of original and blind-token-biased logits. Experiments on standard benchmarks, including POPE, MME, and AMBER, demonstrate that AvisC effectively reduces hallucinations in LVLMs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LDMol: A Text-to-Molecule Diffusion Model with Structurally Informative Latent Space Surpasses AR Models</title>
<link>https://arxiv.org/abs/2405.17829</link>
<guid>https://arxiv.org/abs/2405.17829</guid>
<content:encoded><![CDATA[
arXiv:2405.17829v3 Announce Type: replace-cross 
Abstract: With the emergence of diffusion models as a frontline generative model, many researchers have proposed molecule generation techniques with conditional diffusion models. However, the unavoidable discreteness of a molecule makes it difficult for a diffusion model to connect raw data with highly complex conditions like natural language. To address this, here we present a novel latent diffusion model dubbed LDMol for text-conditioned molecule generation. By recognizing that the suitable latent space design is the key to the diffusion model performance, we employ a contrastive learning strategy to extract novel feature space from text data that embeds the unique characteristics of the molecule structure. Experiments show that LDMol outperforms the existing autoregressive baselines on the text-to-molecule generation benchmark, being one of the first diffusion models that outperforms autoregressive models in textual data generation with a better choice of the latent domain. Furthermore, we show that LDMol can be applied to downstream tasks such as molecule-to-text retrieval and text-guided molecule editing, demonstrating its versatility as a diffusion model.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Better Chain-of-Thought: A Reflection on Effectiveness and Faithfulness</title>
<link>https://arxiv.org/abs/2405.18915</link>
<guid>https://arxiv.org/abs/2405.18915</guid>
<content:encoded><![CDATA[
arXiv:2405.18915v3 Announce Type: replace-cross 
Abstract: Chain-of-thought (CoT) prompting demonstrates varying performance under different reasoning tasks. Previous work attempts to evaluate it but falls short in providing an in-depth analysis of patterns that influence the CoT. In this paper, we study the CoT performance from the perspective of effectiveness and faithfulness. For the former, we identify key factors that influence CoT effectiveness on performance improvement, including problem difficulty, information gain, and information flow. For the latter, we interpret the unfaithful CoT issue by conducting a joint analysis of the information interaction among the question, CoT, and answer. The result demonstrates that, when the LLM predicts answers, it can recall correct information missing in the CoT from the question, leading to the problem. Finally, we propose a novel algorithm to mitigate this issue, in which we recall extra information from the question to enhance the CoT generation and evaluate CoTs based on their information gain. Extensive experiments demonstrate that our approach enhances both the faithfulness and effectiveness of CoT.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pattern Recognition or Medical Knowledge? The Problem with Multiple-Choice Questions in Medicine</title>
<link>https://arxiv.org/abs/2406.02394</link>
<guid>https://arxiv.org/abs/2406.02394</guid>
<content:encoded><![CDATA[
arXiv:2406.02394v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) such as ChatGPT demonstrate significant potential in the medical domain and are often evaluated using multiple-choice questions (MCQs) modeled on exams like the USMLE. However, such benchmarks may overestimate true clinical understanding by rewarding pattern recognition and test-taking heuristics. To investigate this, we created a fictional medical benchmark centered on an imaginary organ, the Glianorex, allowing us to separate memorized knowledge from reasoning ability. We generated textbooks and MCQs in English and French using leading LLMs, then evaluated proprietary, open-source, and domain-specific models in a zero-shot setting. Despite the fictional content, models achieved an average score of 64%, while physicians scored only 27%. Fine-tuned medical models outperformed base models in English but not in French. Ablation and interpretability analyses revealed that models frequently relied on shallow cues, test-taking strategies, and hallucinated reasoning to identify the correct choice. These results suggest that standard MCQ-based evaluations may not effectively measure clinical reasoning and highlight the need for more robust, clinically meaningful assessment methods for LLMs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Misalignment Between Agents: Towards a Sociotechnical Understanding of Alignment</title>
<link>https://arxiv.org/abs/2406.04231</link>
<guid>https://arxiv.org/abs/2406.04231</guid>
<content:encoded><![CDATA[
arXiv:2406.04231v4 Announce Type: replace-cross 
Abstract: Existing work on the alignment problem has focused mainly on (1) qualitative descriptions of the alignment problem; (2) attempting to align AI actions with human interests by focusing on value specification and learning; and/or (3) focusing on a single agent or on humanity as a monolith. Recent sociotechnical approaches highlight the need to understand complex misalignment among multiple human and AI agents. We address this gap by adapting a computational social science model of human contention to the alignment problem. Our model quantifies misalignment in large, diverse agent groups with potentially conflicting goals across various problem areas. Misalignment scores in our framework depend on the observed agent population, the domain in question, and conflict between agents' weighted preferences. Through simulations, we demonstrate how our model captures intuitive aspects of misalignment across different scenarios. We then apply our model to two case studies, including an autonomous vehicle setting, showcasing its practical utility. Our approach offers enhanced explanatory power for complex sociotechnical environments and could inform the design of more aligned AI systems in real-world applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DragPoser: Motion Reconstruction from Variable Sparse Tracking Signals via Latent Space Optimization</title>
<link>https://arxiv.org/abs/2406.14567</link>
<guid>https://arxiv.org/abs/2406.14567</guid>
<content:encoded><![CDATA[
arXiv:2406.14567v3 Announce Type: replace-cross 
Abstract: High-quality motion reconstruction that follows the user's movements can be achieved by high-end mocap systems with many sensors. However, obtaining such animation quality with fewer input devices is gaining popularity as it brings mocap closer to the general public. The main challenges include the loss of end-effector accuracy in learning-based approaches, or the lack of naturalness and smoothness in IK-based solutions. In addition, such systems are often finely tuned to a specific number of trackers and are highly sensitive to missing data e.g., in scenarios where a sensor is occluded or malfunctions. In response to these challenges, we introduce DragPoser, a novel deep-learning-based motion reconstruction system that accurately represents hard and dynamic on-the-fly constraints, attaining real-time high end-effectors position accuracy. This is achieved through a pose optimization process within a structured latent space. Our system requires only one-time training on a large human motion dataset, and then constraints can be dynamically defined as losses, while the pose is iteratively refined by computing the gradients of these losses within the latent space. To further enhance our approach, we incorporate a Temporal Predictor network, which employs a Transformer architecture to directly encode temporality within the latent space. This network ensures the pose optimization is confined to the manifold of valid poses and also leverages past pose data to predict temporally coherent poses. Results demonstrate that DragPoser surpasses both IK-based and the latest data-driven methods in achieving precise end-effector positioning, while it produces natural poses and temporally coherent motion. In addition, our system showcases robustness against on-the-fly constraint modifications, and exhibits exceptional adaptability to various input configurations and changes.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context Learning</title>
<link>https://arxiv.org/abs/2406.17764</link>
<guid>https://arxiv.org/abs/2406.17764</guid>
<content:encoded><![CDATA[
arXiv:2406.17764v3 Announce Type: replace-cross 
Abstract: This paper introduces BMIKE-53, a comprehensive benchmark for cross-lingual in-context knowledge editing (IKE) across 53 languages, unifying three knowledge editing (KE) datasets: zsRE, CounterFact, and WikiFactDiff. Cross-lingual KE, which requires knowledge edited in one language to generalize across others while preserving unrelated knowledge, remains underexplored. To address this gap, we systematically evaluate IKE under zero-shot, one-shot, and few-shot setups, incorporating tailored metric-specific demonstrations. Our findings reveal that model scale and demonstration alignment critically govern cross-lingual IKE efficacy, with larger models and tailored demonstrations significantly improving performance. Linguistic properties, particularly script type, strongly influence performance variation across languages, with non-Latin languages underperforming due to issues like language confusion. Code and data are publicly available at: https://github.com/ercong21/MultiKnow/.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curriculum Learning with Quality-Driven Data Selection</title>
<link>https://arxiv.org/abs/2407.00102</link>
<guid>https://arxiv.org/abs/2407.00102</guid>
<content:encoded><![CDATA[
arXiv:2407.00102v2 Announce Type: replace-cross 
Abstract: The impressive multimodal capabilities demonstrated by OpenAI's GPT-4 have generated significant interest in the development of Multimodal Large Language Models (MLLMs). Visual instruction tuning of MLLMs with machine-generated instruction-following data has shown to enhance zero-shot capabilities across various tasks. However, there has been limited exploration into controlling the quality of the instruction data.Current methodologies for data selection in MLLMs often rely on single, unreliable scores or use downstream tasks for selection, which is time-consuming and can lead to potential overfitting on the chosen evaluation datasets. To mitigate these limitations, we propose a novel data selection methodology that utilizes image-text correlation and model perplexity to evaluate and select data of varying quality. This approach leverages the distinct distribution of these two attributes, mapping data quality into a two-dimensional space that allows for the selection of data based on their location within this distribution. By utilizing this space, we can analyze the impact of task type settings, used as prompts, on data quality. Additionally, this space can be used to construct multi-stage subsets of varying quality to facilitate curriculum learning. Our research includes comprehensive experiments conducted on various datasets. The results emphasize substantial enhancements in five commonly assessed capabilities compared to using the complete dataset. Our codes, data, and models are publicly available at: https://anonymous.4open.science/r/EHIT-31B4
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When LLMs Play the Telephone Game: Cultural Attractors as Conceptual Tools to Evaluate LLMs in Multi-turn Settings</title>
<link>https://arxiv.org/abs/2407.04503</link>
<guid>https://arxiv.org/abs/2407.04503</guid>
<content:encoded><![CDATA[
arXiv:2407.04503v3 Announce Type: replace-cross 
Abstract: As large language models (LLMs) start interacting with each other and generating an increasing amount of text online, it becomes crucial to better understand how information is transformed as it passes from one LLM to the next. While significant research has examined individual LLM behaviors, existing studies have largely overlooked the collective behaviors and information distortions arising from iterated LLM interactions. Small biases, negligible at the single output level, risk being amplified in iterated interactions, potentially leading the content to evolve towards attractor states. In a series of telephone game experiments, we apply a transmission chain design borrowed from the human cultural evolution literature: LLM agents iteratively receive, produce, and transmit texts from the previous to the next agent in the chain. By tracking the evolution of text toxicity, positivity, difficulty, and length across transmission chains, we uncover the existence of biases and attractors, and study their dependence on the initial text, the instructions, language model, and model size. For instance, we find that more open-ended instructions lead to stronger attraction effects compared to more constrained tasks. We also find that different text properties display different sensitivity to attraction effects, with toxicity leading to stronger attractors than length. These findings highlight the importance of accounting for multi-step transmission dynamics and represent a first step towards a more comprehensive understanding of LLM cultural dynamics.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LETS-C: Leveraging Text Embedding for Time Series Classification</title>
<link>https://arxiv.org/abs/2407.06533</link>
<guid>https://arxiv.org/abs/2407.06533</guid>
<content:encoded><![CDATA[
arXiv:2407.06533v2 Announce Type: replace-cross 
Abstract: Recent advancements in language modeling have shown promising results when applied to time series data. In particular, fine-tuning pre-trained large language models (LLMs) for time series classification tasks has achieved state-of-the-art (SOTA) performance on standard benchmarks. However, these LLM-based models have a significant drawback due to the large model size, with the number of trainable parameters in the millions. In this paper, we propose an alternative approach to leveraging the success of language modeling in the time series domain. Instead of fine-tuning LLMs, we utilize a text embedding model to embed time series and then pair the embeddings with a simple classification head composed of convolutional neural networks (CNN) and multilayer perceptron (MLP). We conducted extensive experiments on a well-established time series classification benchmark. We demonstrated LETS-C not only outperforms the current SOTA in classification accuracy but also offers a lightweight solution, using only 14.5% of the trainable parameters on average compared to the SOTA model. Our findings suggest that leveraging text embedding models to encode time series data, combined with a simple yet effective classification head, offers a promising direction for achieving high-performance time series classification while maintaining a lightweight model architecture.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DNTextSpotter: Arbitrary-Shaped Scene Text Spotting via Improved Denoising Training</title>
<link>https://arxiv.org/abs/2408.00355</link>
<guid>https://arxiv.org/abs/2408.00355</guid>
<content:encoded><![CDATA[
arXiv:2408.00355v4 Announce Type: replace-cross 
Abstract: More and more end-to-end text spotting methods based on Transformer architecture have demonstrated superior performance. These methods utilize a bipartite graph matching algorithm to perform one-to-one optimal matching between predicted objects and actual objects. However, the instability of bipartite graph matching can lead to inconsistent optimization targets, thereby affecting the training performance of the model. Existing literature applies denoising training to solve the problem of bipartite graph matching instability in object detection tasks. Unfortunately, this denoising training method cannot be directly applied to text spotting tasks, as these tasks need to perform irregular shape detection tasks and more complex text recognition tasks than classification. To address this issue, we propose a novel denoising training method (DNTextSpotter) for arbitrary-shaped text spotting. Specifically, we decompose the queries of the denoising part into noised positional queries and noised content queries. We use the four Bezier control points of the Bezier center curve to generate the noised positional queries. For the noised content queries, considering that the output of the text in a fixed positional order is not conducive to aligning position with content, we employ a masked character sliding method to initialize noised content queries, thereby assisting in the alignment of text content and position. To improve the model's perception of the background, we further utilize an additional loss function for background characters classification in the denoising training part.Although DNTextSpotter is conceptually simple, it outperforms the state-of-the-art methods on four benchmarks (Total-Text, SCUT-CTW1500, ICDAR15, and Inverse-Text), especially yielding an improvement of 11.3% against the best approach in Inverse-Text dataset.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimus: Accelerating Large-Scale Multi-Modal LLM Training by Bubble Exploitation</title>
<link>https://arxiv.org/abs/2408.03505</link>
<guid>https://arxiv.org/abs/2408.03505</guid>
<content:encoded><![CDATA[
arXiv:2408.03505v2 Announce Type: replace-cross 
Abstract: Multimodal large language models (MLLMs) have extended the success of large language models (LLMs) to multiple data types, such as image, text and audio, achieving significant performance in various domains, including multimodal translation, visual question answering and content generation. Nonetheless, existing systems are inefficient to train MLLMs due to substantial GPU bubbles caused by the heterogeneous modality models and complex data dependencies in 3D parallelism. This paper proposes Optimus, a distributed MLLM training system that reduces end-to-end MLLM training time. Optimus is based on our principled analysis that scheduling the encoder computation within the LLM bubbles can reduce bubbles in MLLM training. To make scheduling encoder computation possible for all GPUs, Optimus searches the separate parallel plans for encoder and LLM, and adopts a bubble scheduling algorithm to enable exploiting LLM bubbles without breaking the original data dependencies in the MLLM model architecture. We further decompose encoder layer computation into a series of kernels, and analyze the common bubble pattern of 3D parallelism to carefully optimize the sub-millisecond bubble scheduling, minimizing the overall training time. Our experiments in a production cluster show that Optimus accelerates MLLM training by 20.5%-21.3% with ViT-22B and GPT-175B model over 3072 GPUs compared to baselines.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wait, that's not an option: LLMs Robustness with Incorrect Multiple-Choice Options</title>
<link>https://arxiv.org/abs/2409.00113</link>
<guid>https://arxiv.org/abs/2409.00113</guid>
<content:encoded><![CDATA[
arXiv:2409.00113v3 Announce Type: replace-cross 
Abstract: This work introduces a novel framework for evaluating LLMs' capacity to balance instruction-following with critical reasoning when presented with multiple-choice questions containing no valid answers. Through systematic evaluation across arithmetic, domain-specific knowledge, and high-stakes medical decision tasks, we demonstrate that post-training aligned models often default to selecting invalid options, while base models exhibit improved refusal capabilities that scale with model size. Our analysis reveals that alignment techniques, though intended to enhance helpfulness, can inadvertently impair models' reflective judgment--the ability to override default behaviors when faced with invalid options. We additionally conduct a parallel human study showing similar instruction-following biases, with implications for how these biases may propagate through human feedback datasets used in alignment. We provide extensive ablation studies examining the impact of model size, training techniques, and prompt engineering. Our findings highlight fundamental tensions between alignment optimization and preservation of critical reasoning capabilities, with important implications for developing more robust AI systems for real-world deployment.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Content Moderation by LLM: From Accuracy to Legitimacy</title>
<link>https://arxiv.org/abs/2409.03219</link>
<guid>https://arxiv.org/abs/2409.03219</guid>
<content:encoded><![CDATA[
arXiv:2409.03219v2 Announce Type: replace-cross 
Abstract: One trending application of LLM (large language model) is to use it for content moderation in online platforms. Most current studies on this application have focused on the metric of accuracy -- the extent to which LLMs make correct decisions about content. This article argues that accuracy is insufficient and misleading because it fails to grasp the distinction between easy cases and hard cases, as well as the inevitable trade-offs in achieving higher accuracy. Closer examination reveals that content moderation is a constitutive part of platform governance, the key of which is to gain and enhance legitimacy. Instead of making moderation decisions correct, the chief goal of LLMs is to make them legitimate. In this regard, this article proposes a paradigm shift from the single benchmark of accuracy towards a legitimacy-based framework for evaluating the performance of LLM moderators. The framework suggests that for easy cases, the key is to ensure accuracy, speed, and transparency, while for hard cases, what matters is reasoned justification and user participation. Examined under this framework, LLMs' real potential in moderation is not accuracy improvement. Rather, LLMs can better contribute in four other aspects: to conduct screening of hard cases from easy cases, to provide quality explanations for moderation decisions, to assist human reviewers in getting more contextual information, and to facilitate user participation in a more interactive way. To realize these contributions, this article proposes a workflow for incorporating LLMs into the content moderation system. Using normative theories from law and social sciences to critically assess the new technological application, this article seeks to redefine LLMs' role in content moderation and redirect relevant research in this field.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>View-Invariant Policy Learning via Zero-Shot Novel View Synthesis</title>
<link>https://arxiv.org/abs/2409.03685</link>
<guid>https://arxiv.org/abs/2409.03685</guid>
<content:encoded><![CDATA[
arXiv:2409.03685v3 Announce Type: replace-cross 
Abstract: Large-scale visuomotor policy learning is a promising approach toward developing generalizable manipulation systems. Yet, policies that can be deployed on diverse embodiments, environments, and observational modalities remain elusive. In this work, we investigate how knowledge from large-scale visual data of the world may be used to address one axis of variation for generalizable manipulation: observational viewpoint. Specifically, we study single-image novel view synthesis models, which learn 3D-aware scene-level priors by rendering images of the same scene from alternate camera viewpoints given a single input image. For practical application to diverse robotic data, these models must operate zero-shot, performing view synthesis on unseen tasks and environments. We empirically analyze view synthesis models within a simple data-augmentation scheme that we call View Synthesis Augmentation (VISTA) to understand their capabilities for learning viewpoint-invariant policies from single-viewpoint demonstration data. Upon evaluating the robustness of policies trained with our method to out-of-distribution camera viewpoints, we find that they outperform baselines in both simulated and real-world manipulation tasks. Videos and additional visualizations are available at https://s-tian.github.io/projects/vista.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CKnowEdit: A New Chinese Knowledge Editing Dataset for Linguistics, Facts, and Logic Error Correction in LLMs</title>
<link>https://arxiv.org/abs/2409.05806</link>
<guid>https://arxiv.org/abs/2409.05806</guid>
<content:encoded><![CDATA[
arXiv:2409.05806v4 Announce Type: replace-cross 
Abstract: Chinese, as a linguistic system rich in depth and complexity, is characterized by distinctive elements such as ancient poetry, proverbs, idioms, and other cultural constructs. However, current Large Language Models (LLMs) face limitations in these specialized domains, highlighting the need for the development of comprehensive datasets that can assess, continuously update, and progressively improve these culturally-grounded linguistic competencies through targeted training optimizations. To address this gap, we introduce CKnowEdit, the first-ever Chinese knowledge editing dataset designed to correct linguistic, factual, and logical errors in LLMs. We collect seven types of knowledge from a wide range of sources, including classical texts, idioms, and content from Baidu Tieba Ruozhiba, taking into account the unique polyphony, antithesis, and logical structures inherent in the Chinese language. By analyzing this dataset, we highlight the challenges current LLMs face in mastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge editing techniques reveals opportunities to advance the correction of Chinese knowledge. Code and dataset are available at https://github.com/zjunlp/EasyEdit.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text-To-Speech Synthesis In The Wild</title>
<link>https://arxiv.org/abs/2409.08711</link>
<guid>https://arxiv.org/abs/2409.08711</guid>
<content:encoded><![CDATA[
arXiv:2409.08711v2 Announce Type: replace-cross 
Abstract: Traditional Text-to-Speech (TTS) systems rely on studio-quality speech recorded in controlled settings.a Recently, an effort known as noisy-TTS training has emerged, aiming to utilize in-the-wild data. However, the lack of dedicated datasets has been a significant limitation. We introduce the TTS In the Wild (TITW) dataset, which is publicly available, created through a fully automated pipeline applied to the VoxCeleb1 dataset. It comprises two training sets: TITW-Hard, derived from the transcription, segmentation, and selection of raw VoxCeleb1 data, and TITW-Easy, which incorporates additional enhancement and data selection based on DNSMOS. State-of-the-art TTS models achieve over 3.0 UTMOS score with TITW-Easy, while TITW-Hard remains difficult showing UTMOS below 2.8.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Egocentric Speaker Classification in Child-Adult Dyadic Interactions: From Sensing to Computational Modeling</title>
<link>https://arxiv.org/abs/2409.09340</link>
<guid>https://arxiv.org/abs/2409.09340</guid>
<content:encoded><![CDATA[
arXiv:2409.09340v2 Announce Type: replace-cross 
Abstract: Autism spectrum disorder (ASD) is a neurodevelopmental condition characterized by challenges in social communication, repetitive behavior, and sensory processing. One important research area in ASD is evaluating children's behavioral changes over time during treatment. The standard protocol with this objective is BOSCC, which involves dyadic interactions between a child and clinicians performing a pre-defined set of activities. A fundamental aspect of understanding children's behavior in these interactions is automatic speech understanding, particularly identifying who speaks and when. Conventional approaches in this area heavily rely on speech samples recorded from a spectator perspective, and there is limited research on egocentric speech modeling. In this study, we design an experiment to perform speech sampling in BOSCC interviews from an egocentric perspective using wearable sensors and explore pre-training Ego4D speech samples to enhance child-adult speaker classification in dyadic interactions. Our findings highlight the potential of egocentric speech collection and pre-training to improve speaker classification accuracy.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via Transformer-Guided Prototyping</title>
<link>https://arxiv.org/abs/2409.11316</link>
<guid>https://arxiv.org/abs/2409.11316</guid>
<content:encoded><![CDATA[
arXiv:2409.11316v3 Announce Type: replace-cross 
Abstract: Few-shot Semantic Segmentation addresses the challenge of segmenting objects in query images with only a handful of annotated examples. However, many previous state-of-the-art methods either have to discard intricate local semantic features or suffer from high computational complexity. To address these challenges, we propose a new Few-shot Semantic Segmentation framework based on the Transformer architecture. Our approach introduces the spatial transformer decoder and the contextual mask generation module to improve the relational understanding between support and query images. Moreover, we introduce a multi scale decoder to refine the segmentation mask by incorporating features from different resolutions in a hierarchical manner. Additionally, our approach integrates global features from intermediate encoder stages to improve contextual understanding, while maintaining a lightweight structure to reduce complexity. This balance between performance and efficiency enables our method to achieve competitive results on benchmark datasets such as PASCAL-5^i and COCO-20^i in both 1-shot and 5-shot settings. Notably, our model with only 1.5 million parameters demonstrates competitive performance while overcoming limitations of existing methodologies.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2409.14507</link>
<guid>https://arxiv.org/abs/2409.14507</guid>
<content:encoded><![CDATA[
arXiv:2409.14507v5 Announce Type: replace-cross 
Abstract: Sparse Autoencoders (SAEs) aim to decompose the activation space of large language models (LLMs) into human-interpretable latent directions or features. As we increase the number of features in the SAE, hierarchical features tend to split into finer features ("math" may split into "algebra", "geometry", etc.), a phenomenon referred to as feature splitting. However, we show that sparse decomposition and splitting of hierarchical features is not robust. Specifically, we show that seemingly monosemantic features fail to fire where they should, and instead get "absorbed" into their children features. We coin this phenomenon feature absorption, and show that it is caused by optimizing for sparsity in SAEs whenever the underlying features form a hierarchy. We introduce a metric to detect absorption in SAEs, and validate our findings empirically on hundreds of LLM SAEs. Our investigation suggests that varying SAE sizes or sparsity is insufficient to solve this issue. We discuss the implications of feature absorption in SAEs and some potential approaches to solve the fundamental theoretical issues before SAEs can be used for interpreting LLMs robustly and at scale.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Banking Dataset: Understanding Client Needs through Event Sequences</title>
<link>https://arxiv.org/abs/2409.17587</link>
<guid>https://arxiv.org/abs/2409.17587</guid>
<content:encoded><![CDATA[
arXiv:2409.17587v2 Announce Type: replace-cross 
Abstract: Financial organizations collect a huge amount of temporal (sequential) data about clients, which is typically collected from multiple sources (modalities). Despite the urgent practical need, developing deep learning techniques suitable to handle such data is limited by the absence of large open-source multi-source real-world datasets of event sequences. To fill this gap, which is mainly caused by security reasons, we present the first industrial-scale publicly available multimodal banking dataset, MBD, that contains information on more than 2M corporate clients of a large bank. Clients are represented by several data sources: 950M bank transactions, 1B geo position events, 5M embeddings of dialogues with technical support, and monthly aggregated purchases of four bank products. All entries are properly anonymized from real proprietary bank data, and the experiments confirm that our anonymization still saves all significant information for introduced downstream tasks. Moreover, we introduce a novel multimodal benchmark suggesting several important practical tasks, such as future purchase prediction and modality matching. The benchmark incorporates our MBD and two public financial datasets. We provide numerical results for the state-of-the-art event sequence modeling techniques including large language models and demonstrate the superiority of fusion baselines over single-modal techniques for each task. Thus, MBD provides a valuable resource for future research in financial applications of multimodal event sequence analysis.
  HuggingFace Link: https://huggingface.co/datasets/ai-lab/MBD
  Github Link: https://github.com/Dzhambo/MBD
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differential privacy enables fair and accurate AI-based analysis of speech disorders while protecting patient data</title>
<link>https://arxiv.org/abs/2409.19078</link>
<guid>https://arxiv.org/abs/2409.19078</guid>
<content:encoded><![CDATA[
arXiv:2409.19078v3 Announce Type: replace-cross 
Abstract: Speech pathology has impacts on communication abilities and quality of life. While deep learning-based models have shown potential in diagnosing these disorders, the use of sensitive data raises critical privacy concerns. Although differential privacy (DP) has been explored in the medical imaging domain, its application in pathological speech analysis remains largely unexplored despite the equally critical privacy concerns. To the best of our knowledge, this study is the first to investigate DP's impact on pathological speech data, focusing on the trade-offs between privacy, diagnostic accuracy, and fairness. Using a large, real-world dataset of 200 hours of recordings from 2,839 German-speaking participants, we observed a maximum accuracy reduction of 3.85% when training with DP with high privacy levels. To highlight real-world privacy risks, we demonstrated the vulnerability of non-private models to gradient inversion attacks, reconstructing identifiable speech samples and showcasing DP's effectiveness in mitigating these risks. To explore the potential generalizability across languages and disorders, we validated our approach on a dataset of Spanish-speaking Parkinson's disease patients, leveraging pretrained models from healthy English-speaking datasets, and demonstrated that careful pretraining on large-scale task-specific datasets can maintain favorable accuracy under DP constraints. A comprehensive fairness analysis revealed minimal gender bias at reasonable privacy levels but underscored the need for addressing age-related disparities. Our results establish that DP can balance privacy and utility in speech disorder detection, while highlighting unique challenges in privacy-fairness trade-offs for speech data. This provides a foundation for refining DP methodologies and improving fairness across diverse patient groups in real-world deployments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conditional Image Synthesis with Diffusion Models: A Survey</title>
<link>https://arxiv.org/abs/2409.19365</link>
<guid>https://arxiv.org/abs/2409.19365</guid>
<content:encoded><![CDATA[
arXiv:2409.19365v3 Announce Type: replace-cross 
Abstract: Conditional image synthesis based on user-specified requirements is a key component in creating complex visual content. In recent years, diffusion-based generative modeling has become a highly effective way for conditional image synthesis, leading to exponential growth in the literature. However, the complexity of diffusion-based modeling, the wide range of image synthesis tasks, and the diversity of conditioning mechanisms present significant challenges for researchers to keep up with rapid developments and to understand the core concepts on this topic. In this survey, we categorize existing works based on how conditions are integrated into the two fundamental components of diffusion-based modeling, $\textit{i.e.}$, the denoising network and the sampling process. We specifically highlight the underlying principles, advantages, and potential challenges of various conditioning approaches during the training, re-purposing, and specialization stages to construct a desired denoising network. We also summarize six mainstream conditioning mechanisms in the sampling process. All discussions are centered around popular applications. Finally, we pinpoint several critical yet still unsolved problems and suggest some possible solutions for future research. Our reviewed works are itemized at https://github.com/zju-pi/Awesome-Conditional-Diffusion-Models.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Softmax is not Enough (for Sharp Size Generalisation)</title>
<link>https://arxiv.org/abs/2410.01104</link>
<guid>https://arxiv.org/abs/2410.01104</guid>
<content:encoded><![CDATA[
arXiv:2410.01104v3 Announce Type: replace-cross 
Abstract: A key property of reasoning systems is the ability to make sharp decisions on their input data. For contemporary AI systems, a key carrier of sharp behaviour is the softmax function, with its capability to perform differentiable query-key lookups. It is a common belief that the predictive power of networks leveraging softmax arises from "circuits" which sharply perform certain kinds of computations consistently across many diverse inputs. However, for these circuits to be robust, they would need to generalise well to arbitrary valid inputs. In this paper, we dispel this myth: even for tasks as simple as finding the maximum key, any learned circuitry must disperse as the number of items grows at test time. We attribute this to a fundamental limitation of the softmax function to robustly approximate sharp functions with increasing problem size, prove this phenomenon theoretically, and propose adaptive temperature as an ad-hoc technique for improving the sharpness of softmax at inference time.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inverse Entropic Optimal Transport Solves Semi-supervised Learning via Data Likelihood Maximization</title>
<link>https://arxiv.org/abs/2410.02628</link>
<guid>https://arxiv.org/abs/2410.02628</guid>
<content:encoded><![CDATA[
arXiv:2410.02628v3 Announce Type: replace-cross 
Abstract: Learning conditional distributions $\pi^*(\cdot|x)$ is a central problem in machine learning, which is typically approached via supervised methods with paired data $(x,y) \sim \pi^*$. However, acquiring paired data samples is often challenging, especially in problems such as domain translation. This necessitates the development of $\textit{semi-supervised}$ models that utilize both limited paired data and additional unpaired i.i.d. samples $x \sim \pi^*_x$ and $y \sim \pi^*_y$ from the marginal distributions. The usage of such combined data is complex and often relies on heuristic approaches. To tackle this issue, we propose a new learning paradigm that integrates both paired and unpaired data $\textbf{seamlessly}$ using the data likelihood maximization techniques. We demonstrate that our approach also connects intriguingly with inverse entropic optimal transport (OT). This finding allows us to apply recent advances in computational OT to establish an $\textbf{end-to-end}$ learning algorithm to get $\pi^*(\cdot|x)$. In addition, we derive the universal approximation property, demonstrating that our approach can theoretically recover true conditional distributions with arbitrarily small error. Furthermore, we demonstrate through empirical tests that our method effectively learns conditional distributions using paired and unpaired data simultaneously.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Cold, Calculated, and Condescending": How AI Identifies and Explains Ableism Compared to Disabled People</title>
<link>https://arxiv.org/abs/2410.03448</link>
<guid>https://arxiv.org/abs/2410.03448</guid>
<content:encoded><![CDATA[
arXiv:2410.03448v3 Announce Type: replace-cross 
Abstract: People with disabilities (PwD) regularly encounter ableist hate and microaggressions online. These spaces are generally moderated by machine learning models, but little is known about how effectively AI models identify ableist speech and how well their judgments align with PwD. To investigate this, we curated a first-of-its-kind dataset of 200 social media comments targeted towards PwD, and prompted state-of-the art AI models (i.e., Toxicity Classifiers, LLMs) to score toxicity and ableism for each comment, and explain their reasoning. Then, we recruited 190 participants to similarly rate and explain the harm, and evaluate LLM explanations. Our mixed-methods analysis highlighted a major disconnect: AI underestimated toxicity compared to PwD ratings, while its ableism assessments were sporadic and varied. Although LLMs identified some biases, its explanations were flawed--they lacked nuance, made incorrect assumptions, and appeared judgmental instead of educational. Going forward, we discuss challenges and opportunities in designing moderation systems for ableism, and advocate for the involvement of intersectional disabled perspectives in AI.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving Model Transformation</title>
<link>https://arxiv.org/abs/2410.03960</link>
<guid>https://arxiv.org/abs/2410.03960</guid>
<content:encoded><![CDATA[
arXiv:2410.03960v3 Announce Type: replace-cross 
Abstract: LLM inference for enterprise applications, such as summarization, RAG, and code-generation, typically observe much longer prompt than generations, leading to high prefill cost and response latency. We present SwiftKV, a novel model transformation and distillation procedure targeted at reducing the prefill compute (in FLOPs) of prompt tokens while preserving high generation quality. First, SwiftKV prefills later layers' KV cache using an earlier layer's output, allowing prompt tokens to skip those later layers. Second, SwiftKV employs a lightweight knowledge-preserving distillation procedure that can adapt existing LLMs with minimal accuracy impact. Third, SwiftKV can naturally incorporate KV cache compression to improve inference performance in low-memory scenarios. Our comprehensive experiments show that SwiftKV can effectively reduce prefill computation by 25-50% across several LLM families while incurring minimum quality degradation. In the end-to-end inference serving, SwiftKV realizes up to 2x higher aggregate throughput and 60% lower time per output token. It can achieve a staggering 560 TFlops/GPU of normalized inference throughput, which translates to 16K tokens/s for Llama-3.1-70B. SwiftKV is open-sourced at https://github.com/snowflakedb/arctictraining.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Drift in Extreme Turning with Active Exploration and Gaussian Process Based MPC</title>
<link>https://arxiv.org/abs/2410.05740</link>
<guid>https://arxiv.org/abs/2410.05740</guid>
<content:encoded><![CDATA[
arXiv:2410.05740v3 Announce Type: replace-cross 
Abstract: Extreme cornering in racing often leads to large sideslip angles, presenting a significant challenge for vehicle control. Conventional vehicle controllers struggle to manage this scenario, necessitating the use of a drifting controller. However, the large sideslip angle in drift conditions introduces model mismatch, which in turn affects control precision. To address this issue, we propose a model correction drift controller that integrates Model Predictive Control (MPC) with Gaussian Process Regression (GPR). GPR is employed to correct vehicle model mismatches during both drift equilibrium solving and the MPC optimization process. Additionally, the variance from GPR is utilized to actively explore different cornering drifting velocities, aiming to minimize trajectory tracking errors. The proposed algorithm is validated through simulations on the Simulink-Carsim platform and experiments with a 1:10 scale RC vehicle. In the simulation, the average lateral error with GPR is reduced by 52.8% compared to the non-GPR case. Incorporating exploration further decreases this error by 27.1%. The velocity tracking Root Mean Square Error (RMSE) also decreases by 10.6% with exploration. In the RC car experiment, the average lateral error with GPR is 36.7% lower, and exploration further leads to a 29.0% reduction. Moreover, the velocity tracking RMSE decreases by 7.2% with the inclusion of exploration.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEXA: Multilingual Evaluation of English-Centric LLMs via Cross-Lingual Alignment</title>
<link>https://arxiv.org/abs/2410.05873</link>
<guid>https://arxiv.org/abs/2410.05873</guid>
<content:encoded><![CDATA[
arXiv:2410.05873v2 Announce Type: replace-cross 
Abstract: English-centric large language models (LLMs) often show strong multilingual capabilities. However, their multilingual performance remains unclear and is under-evaluated for many other languages. Most benchmarks for multilinguality focus on classic NLP tasks or cover a minimal number of languages. We introduce MEXA, a method for assessing the multilingual capabilities of pre-trained English-centric LLMs using parallel sentences, which are available for more languages than existing downstream tasks. MEXA leverages that English-centric LLMs use English as a pivot language in their intermediate layers. MEXA computes the alignment between English and non-English languages using parallel sentences to evaluate the transfer of language understanding from English to other languages. This alignment can be used to estimate model performance in different languages. We conduct controlled experiments using various parallel datasets (FLORES-200 and Bible), models (Llama family, Gemma family, Mistral, and OLMo), and established downstream tasks (Belebele, m-MMLU, and m-ARC). We explore different methods to compute embeddings in decoder-only models. Our results show that MEXA, in its default settings, achieves an average Pearson correlation of 0.90 between its predicted scores and actual task performance across languages. This suggests that MEXA is a reliable method for estimating the multilingual capabilities of English-centric LLMs, providing a clearer understanding of their multilingual potential and the inner workings of LLMs. Leaderboard: https://cis-lmu-mexa.hf.space, Code: https://github.com/cisnlp/MEXA.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models</title>
<link>https://arxiv.org/abs/2410.07176</link>
<guid>https://arxiv.org/abs/2410.07176</guid>
<content:encoded><![CDATA[
arXiv:2410.07176v2 Announce Type: replace-cross 
Abstract: Retrieval augmented generation (RAG), while effectively integrating external knowledge to address the inherent limitations of large language models (LLMs), can be hindered by imperfect retrieval that contain irrelevant, misleading, or even malicious information. Previous studies have rarely connected the behavior of RAG through joint analysis, particularly regarding error propagation coming from imperfect retrieval and potential conflicts between LLMs' internal knowledge and external sources. Through comprehensive and controlled analyses under realistic conditions, we find that imperfect retrieval augmentation is inevitable, common, and harmful. We identify the knowledge conflicts between LLM-internal and external knowledge from retrieval as a bottleneck to overcome imperfect retrieval in the post-retrieval stage of RAG. To address this, we propose Astute RAG, a novel RAG approach designed to be resilient to imperfect retrieval augmentation. It adaptively elicits essential information from LLMs' internal knowledge, iteratively consolidates internal and external knowledge with source-awareness, and finalizes the answer according to information reliability. Our experiments with Gemini and Claude demonstrate the superior performance of Astute RAG compared to previous robustness-enhanced RAG approaches. Specifically, Astute RAG is the only RAG method that achieves performance comparable to or even surpassing conventional use of LLMs under the worst-case scenario. Further analysis reveals the effectiveness of Astute RAG in resolving knowledge conflicts, thereby improving the trustworthiness of RAG.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FragNet: A Graph Neural Network for Molecular Property Prediction with Four Levels of Interpretability</title>
<link>https://arxiv.org/abs/2410.12156</link>
<guid>https://arxiv.org/abs/2410.12156</guid>
<content:encoded><![CDATA[
arXiv:2410.12156v2 Announce Type: replace-cross 
Abstract: Molecular property prediction is essential in a variety of contemporary scientific fields, such as drug development and designing energy storage materials. Although there are many machine learning models available for this purpose, those that achieve high accuracy while also offering interpretability of predictions are uncommon. We present a graph neural network that not only matches the prediction accuracies of leading models but also provides insights on four levels of molecular substructures. This model helps identify which atoms, bonds, molecular fragments, and connections between fragments are significant for predicting a specific molecular property. Understanding the importance of connections between fragments is particularly valuable for molecules with substructures that do not connect through standard bonds. The model additionally can quantify the impact of specific fragments on the prediction, allowing the identification of fragments that may improve or degrade a property value. These interpretable features are essential for deriving scientific insights from the model's learned relationships between molecular structures and properties.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Model Kinship for Merging Large Language Models</title>
<link>https://arxiv.org/abs/2410.12613</link>
<guid>https://arxiv.org/abs/2410.12613</guid>
<content:encoded><![CDATA[
arXiv:2410.12613v2 Announce Type: replace-cross 
Abstract: Model merging has become one of the key technologies for enhancing the capabilities and efficiency of Large Language Models (LLMs). However, our understanding of the expected performance gains and principles when merging any two models remains limited. In this work, we introduce model kinship, the degree of similarity or relatedness between LLMs, analogous to biological evolution. With comprehensive empirical analysis, we find that there is a certain relationship between model kinship and the performance gains after model merging, which can help guide our selection of candidate models. Inspired by this, we propose a new model merging strategy: Top-k Greedy Merging with Model Kinship, which can yield better performance on benchmark datasets. Specifically, we discover that using model kinship as a criterion can assist us in continuously performing model merging, alleviating the degradation (local optima) in model evolution, whereas model kinship can serve as a guide to escape these traps. Code is available at https://github.com/zjunlp/ModelKinship.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Little Human Data Goes A Long Way</title>
<link>https://arxiv.org/abs/2410.13098</link>
<guid>https://arxiv.org/abs/2410.13098</guid>
<content:encoded><![CDATA[
arXiv:2410.13098v2 Announce Type: replace-cross 
Abstract: Faced with an expensive human annotation process, creators of NLP systems increasingly turn to synthetic data generation. While this method shows promise, the extent to which synthetic data can replace human annotation is poorly understood. We investigate the use of synthetic data in Fact Verification (FV) and Question Answering (QA) by studying the effects of incrementally replacing human generated data with synthetic points on eight diverse datasets. Strikingly, replacing up to 90% of the training data only marginally decreases performance, but replacing the final 10% leads to severe declines. We find that models trained on purely synthetic data can be reliably improved by including as few as 125 human generated data points. We show that matching the performance gain of just a little additional human data (only 200 points) requires an order of magnitude more synthetic data and estimate price ratios at which human annotation would be a more cost-effective solution. Our results suggest that even when human annotation at scale is infeasible, there is great value to having a small proportion of the dataset being human generated.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling Likes and Dislikes in Personalized Generative Explainable Recommendation</title>
<link>https://arxiv.org/abs/2410.13248</link>
<guid>https://arxiv.org/abs/2410.13248</guid>
<content:encoded><![CDATA[
arXiv:2410.13248v2 Announce Type: replace-cross 
Abstract: Recent research on explainable recommendation generally frames the task as a standard text generation problem, and evaluates models simply based on the textual similarity between the predicted and ground-truth explanations. However, this approach fails to consider one crucial aspect of the systems: whether their outputs accurately reflect the users' (post-purchase) sentiments, i.e., whether and why they would like and/or dislike the recommended items. To shed light on this issue, we introduce new datasets and evaluation methods that focus on the users' sentiments. Specifically, we construct the datasets by explicitly extracting users' positive and negative opinions from their post-purchase reviews using an LLM, and propose to evaluate systems based on whether the generated explanations 1) align well with the users' sentiments, and 2) accurately identify both positive and negative opinions of users on the target items. We benchmark several recent models on our datasets and demonstrate that achieving strong performance on existing metrics does not ensure that the generated explanations align well with the users' sentiments. Lastly, we find that existing models can provide more sentiment-aware explanations when the users' (predicted) ratings for the target items are directly fed into the models as input. The datasets and benchmark implementation are available at: https://github.com/jchanxtarov/sent_xrec.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoGU: Long-form Generation with Uncertainty Expressions</title>
<link>https://arxiv.org/abs/2410.14309</link>
<guid>https://arxiv.org/abs/2410.14309</guid>
<content:encoded><![CDATA[
arXiv:2410.14309v3 Announce Type: replace-cross 
Abstract: While Large Language Models (LLMs) demonstrate impressive capabilities, they still struggle with generating factually incorrect content (i.e., hallucinations). A promising approach to mitigate this issue is enabling models to express uncertainty when unsure. Previous research on uncertainty modeling has primarily focused on short-form QA, but realworld applications often require much longer responses. In this work, we introduce the task of Long-form Generation with Uncertainty(LoGU). We identify two key challenges: Uncertainty Suppression, where models hesitate to express uncertainty, and Uncertainty Misalignment, where models convey uncertainty inaccurately. To tackle these challenges, we propose a refinement-based data collection framework and a two-stage training pipeline. Our framework adopts a divide-and-conquer strategy, refining uncertainty based on atomic claims. The collected data are then used in training through supervised fine-tuning (SFT) and direct preference optimization (DPO) to enhance uncertainty expression. Extensive experiments on three long-form instruction following datasets show that our method significantly improves accuracy, reduces hallucinations, and maintains the comprehensiveness of responses.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes</title>
<link>https://arxiv.org/abs/2410.16930</link>
<guid>https://arxiv.org/abs/2410.16930</guid>
<content:encoded><![CDATA[
arXiv:2410.16930v3 Announce Type: replace-cross 
Abstract: Math reasoning is an active area of Large Language Model (LLM) research because it is a hallmark of artificial intelligence and has implications in several domains, including math education. However, few works have explored how math reasoning is encoded within LLM parameters and if it is a skill that can be isolated within models. Doing so could allow targeted intervention to improve math performance without altering non-math behavior and foster understanding of how models encode math reasoning. We introduce Math Neurosurgery (MathNeuro), a computationally efficient method we use to isolate math-specific parameters in LLMs using only forward passes. MathNeuro builds on existing work by using weights and activations to calculate parameter importance, but isolates math-specific parameters by filtering out those important for general language tasks. Through pruning parameters MathNeuro identifies, we delete a LLM's math reasoning ability without significantly impacting its general language ability. Scaling the identified parameters by a small constant improves a pretrained or instruction-tuned LLM's performance by 4-17% on GSM8K and 5-35% on MATH while leaving non-math behavior unaltered. MathNeuro is also data efficient: most of its effectiveness holds when identifying math-specific parameters using a single sample. MathNeuro highlights the potential for future work to intervene on math-specific parameters.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CogSteer: Cognition-Inspired Selective Layer Intervention for Efficiently Steering Large Language Models</title>
<link>https://arxiv.org/abs/2410.17714</link>
<guid>https://arxiv.org/abs/2410.17714</guid>
<content:encoded><![CDATA[
arXiv:2410.17714v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) achieve remarkable performance through pretraining on extensive data. This enables efficient adaptation to diverse downstream tasks. However, the lack of interpretability in their underlying mechanisms limits the ability to effectively steer LLMs for specific applications. In this work, we investigate the intrinsic mechanisms of LLMs from a cognitive perspective using eye movement measures. Specifically, we analyze the layer-wise correlation between human cognitive indicators and LLM representations. Building on these insights, we propose a heuristic approach for selecting the optimal steering layer to modulate LLM semantics. To this end, we introduce an efficient selective layer intervention based on prominent parameter-efficient fine-tuning methods, which conventionally adjust either all layers or only the final layer. Additionally, we present an implicit layer contrastive intervention during inference to steer LLMs away from toxic outputs. Extensive experiments on natural language understanding, reasoning, and generation tasks, conducted on GPT-2, Llama2-7B, and Mistral-7B, demonstrate the effectiveness and efficiency of our approach. As a model-agnostic framework, it enhances the interpretability of LLMs while improving efficiency for safe deployment.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Continental Healthcare Modelling Using Blockchain-Enabled Federated Learning</title>
<link>https://arxiv.org/abs/2410.17933</link>
<guid>https://arxiv.org/abs/2410.17933</guid>
<content:encoded><![CDATA[
arXiv:2410.17933v2 Announce Type: replace-cross 
Abstract: One of the biggest challenges of building artificial intelligence (AI) model in healthcare area is the data sharing. Since healthcare data is private, sensitive, and heterogeneous, collecting sufficient data for modelling is exhausted, costly, and sometimes impossible. In this paper, we propose a framework for global healthcare modelling using datasets from multi-continents (Europe, North America and Asia) while without sharing the local datasets, and choose glucose management as a study model to verify its effectiveness. Technically, blockchain-enabled federated learning is implemented with adaption to make it meet with the privacy and safety requirements of healthcare data, meanwhile rewards honest participation and penalize malicious activities using its on-chain incentive mechanism. Experimental results show that the proposed framework is effective, efficient, and privacy preserved. Its prediction accuracy is much better than the models trained from limited personal data and is similar to, and even slightly better than, the results from a centralized dataset. This work paves the way for international collaborations on healthcare projects, where additional data is crucial for reducing bias and providing benefits to humanity.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrajAgent: An LLM-based Agent Framework for Automated Trajectory Modeling via Collaboration of Large and Small Models</title>
<link>https://arxiv.org/abs/2410.20445</link>
<guid>https://arxiv.org/abs/2410.20445</guid>
<content:encoded><![CDATA[
arXiv:2410.20445v2 Announce Type: replace-cross 
Abstract: Trajectory modeling, which includes research on trajectory data pattern mining and future prediction, has widespread applications in areas such as life services, urban transportation, and public administration. Numerous methods have been proposed to address specific problems within trajectory modeling. However, the heterogeneity of data and the diversity of trajectory tasks make effective and reliable trajectory modeling an important yet highly challenging endeavor, even for domain experts. In this paper, we propose \textit{TrajAgent}, a agent framework powered by large language models (LLMs), designed to facilitate robust and efficient trajectory modeling through automation modeling. This framework leverages and optimizes diverse specialized models to address various trajectory modeling tasks across different datasets effectively. In \textit{TrajAgent}, we first develop \textit{UniEnv}, an execution environment with a unified data and model interface, to support the execution and training of various models. Building on \textit{UniEnv}, we introduce an agentic workflow designed for automatic trajectory modeling across various trajectory tasks and data. Furthermore, we introduce collaborative learning schema between LLM-based agents and small speciallized models, to enhance the performance of the whole framework effectively. Extensive experiments on four tasks using four real-world datasets demonstrate the effectiveness of \textit{TrajAgent} in automated trajectory modeling, achieving a performance improvement of 2.38\%-34.96\% over baseline methods.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TextDestroyer: A Training- and Annotation-Free Diffusion Method for Destroying Anomal Text from Images</title>
<link>https://arxiv.org/abs/2411.00355</link>
<guid>https://arxiv.org/abs/2411.00355</guid>
<content:encoded><![CDATA[
arXiv:2411.00355v2 Announce Type: replace-cross 
Abstract: In this paper, we propose TextDestroyer, the first training- and annotation-free method for scene text destruction using a pre-trained diffusion model. Existing scene text removal models require complex annotation and retraining, and may leave faint yet recognizable text information, compromising privacy protection and content concealment. TextDestroyer addresses these issues by employing a three-stage hierarchical process to obtain accurate text masks. Our method scrambles text areas in the latent start code using a Gaussian distribution before reconstruction. During the diffusion denoising process, self-attention key and value are referenced from the original latent to restore the compromised background. Latent codes saved at each inversion step are used for replacement during reconstruction, ensuring perfect background restoration. The advantages of TextDestroyer include: (1) it eliminates labor-intensive data annotation and resource-intensive training; (2) it achieves more thorough text destruction, preventing recognizable traces; and (3) it demonstrates better generalization capabilities, performing well on both real-world scenes and generated images.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Generalisation of Voter Model: Influential Nodes and Convergence Properties</title>
<link>https://arxiv.org/abs/2411.04564</link>
<guid>https://arxiv.org/abs/2411.04564</guid>
<content:encoded><![CDATA[
arXiv:2411.04564v2 Announce Type: replace-cross 
Abstract: Consider an undirected graph G, representing a social network, where each node is blue or red, corresponding to positive or negative opinion on a topic. In the voter model, in discrete time rounds, each node picks a neighbour uniformly at random and adopts its colour. Despite its significant popularity, this model does not capture some fundamental real-world characteristics such as the difference in the strengths of individuals connections, individuals with neutral opinion on a topic, and individuals who are reluctant to update their opinion. To address these issues, we introduce and study a generalisation of the voter model. Motivating by campaigning strategies, we study the problem of selecting a set of seeds blue nodes to maximise the expected number of blue nodes after some rounds. We prove that the problem is NP- hard and provide a polynomial time approximation algorithm with the best possible approximation guarantee. Our experiments on real-world and synthetic graph data demonstrate that the proposed algorithm outperforms other algorithms. We also investigate the convergence properties of the model. We prove that the process could take an exponential number of rounds to converge. However, if we limit ourselves to strongly connected graphs, the convergence time is polynomial and the period (the number of states in convergence) divides the length of all cycles in the graph.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KnowCoder-X: Boosting Multilingual Information Extraction via Code</title>
<link>https://arxiv.org/abs/2411.04794</link>
<guid>https://arxiv.org/abs/2411.04794</guid>
<content:encoded><![CDATA[
arXiv:2411.04794v3 Announce Type: replace-cross 
Abstract: Empirical evidence indicates that LLMs exhibit spontaneous cross-lingual alignment. However, although LLMs show promising cross-lingual alignment in Information Extraction (IE), a significant imbalance across languages persists, highlighting an underlying deficiency. To address this, we propose KnowCoder-X, a powerful code LLM with advanced cross-lingual and multilingual capabilities for universal IE. Firstly, it standardizes the representation of multilingual schemas using Python classes, ensuring a consistent ontology across different languages. Then, IE across languages is formulated as a unified code generation task. Secondly, we conduct IE cross-lingual alignment instruction tuning on the translated instance prediction task to enhance the model's cross-lingual transferability. During this phase, we also construct a high-quality and diverse bilingual IE parallel dataset with 257k samples, called ParallelNER, synthesized by our proposed robust three-stage pipeline, with manual annotation to ensure quality. Although without training in 29 unseen languages, KnowCoder-X surpasses ChatGPT by 30.17\% and SoTA by 20.03\%, thereby demonstrating superior cross-lingual IE capabilities. Comprehensive evaluations on 64 IE benchmarks in Chinese and English under various settings demonstrate that KnowCoder-X significantly enhances cross-lingual IE transfer through boosting the IE alignment. Our code and dataset are available at: https://github.com/ICT-GoKnow/KnowCoder
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FactLens: Benchmarking Fine-Grained Fact Verification</title>
<link>https://arxiv.org/abs/2411.05980</link>
<guid>https://arxiv.org/abs/2411.05980</guid>
<content:encoded><![CDATA[
arXiv:2411.05980v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have shown impressive capability in language generation and understanding, but their tendency to hallucinate and produce factually incorrect information remains a key limitation. To verify LLM-generated contents and claims from other sources, traditional verification approaches often rely on holistic models that assign a single factuality label to complex claims, potentially obscuring nuanced errors. In this paper, we advocate for a shift towards fine-grained verification, where complex claims are broken down into smaller sub-claims for individual verification, allowing for more precise identification of inaccuracies, improved transparency, and reduced ambiguity in evidence retrieval. However, generating sub-claims poses challenges, such as maintaining context and ensuring semantic equivalence with respect to the original claim. We introduce FactLens, a benchmark for evaluating fine-grained fact verification, with metrics and automated evaluators of sub-claim quality. The benchmark data is manually curated to ensure high-quality ground truth. Our results show alignment between automated FactLens evaluators and human judgments, and we discuss the impact of sub-claim characteristics on the overall verification performance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-time Adapting Routing (RAR): Improving Efficiency Through Continuous Learning in Software Powered by Layered Foundation Models</title>
<link>https://arxiv.org/abs/2411.09837</link>
<guid>https://arxiv.org/abs/2411.09837</guid>
<content:encoded><![CDATA[
arXiv:2411.09837v2 Announce Type: replace-cross 
Abstract: To balance the quality and inference cost of a Foundation Model (FM, such as large language models (LLMs)) powered software, people often opt to train a routing model that routes requests to FMs with different sizes and capabilities. Existing routing models rely on learning the optimal routing decision from carefully curated data, require complex computations to be updated, and do not consider the potential evolution of weaker FMs. In this paper, we propose Real-time Adaptive Routing (RAR), an approach to continuously adapt FM routing decisions while using guided in-context learning to enhance the capabilities of weaker FM. The goal is to reduce reliance on stronger, more expensive FMs. We evaluate our approach on different subsets of the popular MMLU benchmark. Over time, our approach routes 50.2% fewer requests to computationally expensive models while maintaining around 90.5% of the general response quality. In addition, the guides generated from stronger models have shown intra-domain generalization and led to a better quality of responses compared to an equivalent approach with a standalone weaker FM.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>INVARLLM: LLM-assisted Physical Invariant Extraction for Cyber-Physical Systems Anomaly Detection</title>
<link>https://arxiv.org/abs/2411.10918</link>
<guid>https://arxiv.org/abs/2411.10918</guid>
<content:encoded><![CDATA[
arXiv:2411.10918v2 Announce Type: replace-cross 
Abstract: Cyber-Physical Systems (CPS) are vulnerable to cyber-physical attacks that violate physical laws. While invariant-based anomaly detection is effective, existing methods are limited: data-driven approaches lack semantic context, and physics-based models require extensive manual work. We propose INVARLLM, a hybrid framework that uses large language models (LLMs) to extract semantic information from CPS documentation and generate physical invariants, then validates these against real system data using a PCMCI+-inspired K-means method. This approach combines LLM semantic understanding with empirical validation to ensure both interpretability and reliability. We evaluate INVARLLM on SWaT and WADI datasets, achieving 100% precision in anomaly detection with no false alarms, outperforming all existing methods. Our results demonstrate that integrating LLM-derived semantics with statistical validation provides a scalable and dependable solution for CPS security.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization</title>
<link>https://arxiv.org/abs/2411.10958</link>
<guid>https://arxiv.org/abs/2411.10958</guid>
<content:encoded><![CDATA[
arXiv:2411.10958v5 Announce Type: replace-cross 
Abstract: Although quantization for linear layers has been widely used, its application to accelerate the attention process remains limited. To further enhance the efficiency of attention computation compared to SageAttention while maintaining precision, we propose SageAttention2, which utilizes significantly faster 4-bit matrix multiplication (Matmul) alongside additional precision-enhancing techniques. First, we propose to quantize matrices $(Q, K)$ to INT4 in a hardware-friendly thread-level granularity and quantize matrices $(\widetilde P, V)$ to FP8. Second, we propose a method to smooth $Q$, enhancing the accuracy of INT4 $QK^\top$. Third, we propose a two-level accumulation strategy for $\widetilde PV$ to enhance the accuracy of FP8 $\widetilde PV$. The operations per second (OPS) of SageAttention2 surpass FlashAttention2 and xformers by about 3x and 4.5x on RTX4090, respectively. Moreover, SageAttention2 matches the speed of FlashAttention3(fp8) on the Hopper GPUs, while delivering much higher accuracy. Comprehensive experiments confirm that our approach incurs negligible end-to-end metrics loss across diverse models, including those for language, image, and video generation. The code is available at https://github.com/thu-ml/SageAttention.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Immune: Improving Safety Against Jailbreaks in Multi-modal LLMs via Inference-Time Alignment</title>
<link>https://arxiv.org/abs/2411.18688</link>
<guid>https://arxiv.org/abs/2411.18688</guid>
<content:encoded><![CDATA[
arXiv:2411.18688v4 Announce Type: replace-cross 
Abstract: With the widespread deployment of Multimodal Large Language Models (MLLMs) for visual-reasoning tasks, improving their safety has become crucial. Recent research indicates that despite training-time safety alignment, these models remain vulnerable to jailbreak attacks. In this work, we first highlight an important safety gap to describe that alignment achieved solely through safety training may be insufficient against jailbreak attacks. To address this vulnerability, we propose Immune, an inference-time defense framework that leverages a safe reward model through controlled decoding to defend against jailbreak attacks. Additionally, we provide a mathematical characterization of Immune, offering insights on why it improves safety against jailbreaks. Extensive evaluations on diverse jailbreak benchmarks using recent MLLMs reveal that Immune effectively enhances model safety while preserving the model's original capabilities. For instance, against text-based jailbreak attacks on LLaVA-1.6, Immune reduces the attack success rate by 57.82% and 16.78% compared to the base MLLM and state-of-the-art defense strategy, respectively.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>If Eleanor Rigby Had Met ChatGPT: A Study on Loneliness in a Post-LLM World</title>
<link>https://arxiv.org/abs/2412.01617</link>
<guid>https://arxiv.org/abs/2412.01617</guid>
<content:encoded><![CDATA[
arXiv:2412.01617v2 Announce Type: replace-cross 
Abstract: Warning: this paper discusses content related, but not limited to, violence, sex, and suicide. Loneliness, or the lack of fulfilling relationships, significantly impacts a person's mental and physical well-being and is prevalent worldwide. Previous research suggests that large language models (LLMs) may help mitigate loneliness. However, we argue that the use of widespread LLMs in services like ChatGPT is more prevalent--and riskier, as they are not designed for this purpose. To explore this, we analysed user interactions with ChatGPT outside of its marketed use as a task-oriented assistant. In dialogues classified as lonely, users frequently (37%) sought advice or validation, and received good engagement. However, ChatGPT failed in sensitive scenarios, like responding appropriately to suicidal ideation or trauma. We also observed a 35% higher incidence of toxic content, with women being 22x more likely to be targeted than men. Our findings underscore ethical and legal questions about this technology, and note risks like radicalisation or further isolation. We conclude with recommendations to research and industry to address loneliness.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CNNSum: Exploring Long-Context Summarization with Large Language Models in Chinese Novels</title>
<link>https://arxiv.org/abs/2412.02819</link>
<guid>https://arxiv.org/abs/2412.02819</guid>
<content:encoded><![CDATA[
arXiv:2412.02819v5 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have been well-researched in various long-context tasks. However, the scarcity of long-context summarization datasets hinders progress in this area. To address this, we introduce CNNSum, a multi-scale long-context summarization benchmark based on Chinese novels, featuring human-driven annotations across four subsets totaling 695 samples, with lengths ranging from 16k to 128k. We benchmark numerous LLMs and conduct detailed human assessments to summarize abnormal output types. Furthermore, we extensively explore how to improve long-context summarization. In our study: (1) Advanced LLMs may generate much subjective commentary, leading to vague summaries. (2) Currently, long-context summarization mainly relies on memory ability. The advantages of Large LLMs are hard to utilize, thus small LLMs are more cost-effective. (3) Different prompt types paired with various version models may cause large performance gaps. In further fine-tuning, these can be mitigated, and the Base version models perform better. (4) LLMs with RoPE-base scaled exhibit strong extrapolation potential; using short-context data can significantly improve long-context summarization performance. However, further applying other interpolation methods requires careful selection. (5) CNNSum provides more reliable evaluation results than other benchmarks. We release CNNSum to advance future research.(https://github.com/CxsGhost/CNNSum)
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PromptRefine: Enhancing Few-Shot Performance on Low-Resource Indic Languages with Example Selection from Related Example Banks</title>
<link>https://arxiv.org/abs/2412.05710</link>
<guid>https://arxiv.org/abs/2412.05710</guid>
<content:encoded><![CDATA[
arXiv:2412.05710v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have recently demonstrated impressive few-shot learning capabilities through in-context learning (ICL). However, ICL performance is highly dependent on the choice of few-shot demonstrations, making the selection of the most optimal examples a persistent research challenge. This issue is further amplified in low-resource Indic languages, where the scarcity of ground-truth data complicates the selection process. In this work, we propose PromptRefine, a novel Alternating Minimization approach for example selection that improves ICL performance on low-resource Indic languages. PromptRefine leverages auxiliary example banks from related high-resource Indic languages and employs multi-task learning techniques to align language-specific retrievers, enabling effective cross-language retrieval. Additionally, we incorporate diversity in the selected examples to enhance generalization and reduce bias. Through comprehensive evaluations on four text generation tasks -- Cross-Lingual Question Answering, Multilingual Question Answering, Machine Translation, and Cross-Lingual Summarization using state-of-the-art LLMs such as LLAMA-3.1-8B, LLAMA-2-7B, Qwen-2-7B, and Qwen-2.5-7B, we demonstrate that PromptRefine significantly outperforms existing frameworks for retrieving examples.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-time Chest X-Ray Distributed Decision Support for Resource-constrained Clinics</title>
<link>https://arxiv.org/abs/2412.07818</link>
<guid>https://arxiv.org/abs/2412.07818</guid>
<content:encoded><![CDATA[
arXiv:2412.07818v2 Announce Type: replace-cross 
Abstract: Internet of Things (IoT) based healthcare systems offer significant potential for improving the delivery of healthcare services in humanitarian engineering, providing essential healthcare services to millions of underserved people in remote areas worldwide. However, these areas have poor network infrastructure, making communications difficult for traditional IoT. This paper presents a real-time chest X-ray classification system for hospitals in remote areas using FastDDS real-time middleware, offering reliable real-time communication. We fine-tuned a ResNet50 neural network to an accuracy of 88.61%, a precision of 88.76%, and a recall of 88.49\%. Our system results mark an average throughput of 3.2 KB/s and an average latency of 65 ms. The proposed system demonstrates how middleware-based systems can assist doctors in remote locations.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Graph Neural Networks Learn Language with Extremely Weak Text Supervision?</title>
<link>https://arxiv.org/abs/2412.08174</link>
<guid>https://arxiv.org/abs/2412.08174</guid>
<content:encoded><![CDATA[
arXiv:2412.08174v3 Announce Type: replace-cross 
Abstract: While great success has been achieved in building vision models with Contrastive Language-Image Pre-training (CLIP) over internet-scale image-text pairs, building transferable Graph Neural Networks (GNNs) with CLIP pipeline is challenging because of the scarcity of labeled data and text supervision, different levels of downstream tasks, and the conceptual gaps between domains. In this work, to address these issues, we propose a multi-modal prompt learning paradigm to effectively adapt pre-trained GNN to downstream tasks and data, given only a few semantically labeled samples, each with extremely weak text supervision. Our new paradigm embeds the graphs directly in the same space as the Large Language Models (LLMs) by learning both graph prompts and text prompts simultaneously. We demonstrate the superior performance of our paradigm in few-shot, multi-task-level, and cross-domain settings. Moreover, we build the first CLIP-style zero-shot classification prototype that can generalize GNNs to unseen classes with extremely weak text supervision. The code is available at https://github.com/Violet24K/Morpher.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving Multiagent Path Finding on Highly Centralized Networks</title>
<link>https://arxiv.org/abs/2412.09433</link>
<guid>https://arxiv.org/abs/2412.09433</guid>
<content:encoded><![CDATA[
arXiv:2412.09433v2 Announce Type: replace-cross 
Abstract: The Mutliagent Path Finding (MAPF) problem consists of identifying the trajectories that a set of agents should follow inside a given network in order to reach their desired destinations as soon as possible, but without colliding with each other. We aim to minimize the maximum time any agent takes to reach their goal, ensuring optimal path length. In this work, we complement a recent thread of results that aim to systematically study the algorithmic behavior of this problem, through the parameterized complexity point of view.
  First, we show that MAPF is NP-hard when the given network has a star-like topology (bounded vertex cover number) or is a tree with $11$ leaves. Both of these results fill important gaps in our understanding of the tractability of this problem that were left untreated in the recent work of [Fioravantes et al. Exact Algorithms and Lowerbounds for Multiagent Path Finding: Power of Treelike Topology. AAAI'24]. Nevertheless, our main contribution is an exact algorithm that scales well as the input grows (FPT) when the topology of the given network is highly centralized (bounded distance to clique). This parameter is significant as it mirrors real-world networks. In such environments, a bunch of central hubs (e.g., processing areas) are connected to only few peripheral nodes.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-as-an-Interviewer: Beyond Static Testing Through Dynamic LLM Evaluation</title>
<link>https://arxiv.org/abs/2412.10424</link>
<guid>https://arxiv.org/abs/2412.10424</guid>
<content:encoded><![CDATA[
arXiv:2412.10424v3 Announce Type: replace-cross 
Abstract: We introduce LLM-as-an-Interviewer, a novel paradigm for evaluating large language models (LLMs). This approach leverages multi-turn interactions where the LLM interviewer actively provides feedback on responses and poses follow-up questions to the evaluated LLM. At the start of the interview, the LLM interviewer dynamically modifies datasets to generate initial questions, mitigating data contamination. We apply the LLM-as-an-Interviewer framework to evaluate six models on the MATH and DepthQA tasks. Our results show that the framework effectively provides insights into LLM performance, including the quality of initial responses, adaptability to feedback, and ability to address follow-up queries like clarification or additional knowledge requests. The framework also addresses key limitations of conventional methods like LLM-as-a-Judge, including verbosity bias and inconsistency across runs. Finally, we propose the Interview Report, which aggregates insights from the interview process, providing examples and a comprehensive analysis of the LLM's strengths and weaknesses. This report offers a detailed snapshot of the model's real-world applicability. The code for our framework is publicly available at https://github.com/interview-eval/.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergence and Effectiveness of Task Vectors in In-Context Learning: An Encoder Decoder Perspective</title>
<link>https://arxiv.org/abs/2412.12276</link>
<guid>https://arxiv.org/abs/2412.12276</guid>
<content:encoded><![CDATA[
arXiv:2412.12276v3 Announce Type: replace-cross 
Abstract: Autoregressive transformers exhibit adaptive learning through in-context learning (ICL), which begs the question of how. Prior works have shown that transformers represent the ICL tasks as vectors in their representations. In this paper, we leverage the encoding-decoding framework to study how transformers form task vectors during pretraining and how their task encoding quality predicts ICL task performance. On synthetic ICL tasks, we analyze the training dynamics of a small transformer and report the coupled emergence of task encoding and decoding. As the model learns to encode different latent tasks (e.g., "Finding the first noun in a sentence.") into distinct, separable representations, it concurrently builds conditional decoding algorithms and improves its ICL performance. We validate this phenomenon across pretrained models of varying scales (Gemma-2 2B/9B/27B, Llama-3.1 8B/70B) and over the course of pretraining in OLMo-7B. Further, we demonstrate that the quality of task encoding inferred from representations predicts ICL performance, and that, surprisingly, finetuning the earlier layers can improve the task encoding and performance more than finetuning the latter layers. Our empirical insights shed light into better understanding the success and failure modes of large language models via their representations.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic interior-point methods for smooth conic optimization with applications</title>
<link>https://arxiv.org/abs/2412.12987</link>
<guid>https://arxiv.org/abs/2412.12987</guid>
<content:encoded><![CDATA[
arXiv:2412.12987v2 Announce Type: replace-cross 
Abstract: Conic optimization plays a crucial role in many machine learning (ML) problems. However, practical algorithms for conic constrained ML problems with large datasets are often limited to specific use cases, as stochastic algorithms for general conic optimization remain underdeveloped. To fill this gap, we introduce a stochastic interior-point method (SIPM) framework for general conic optimization, along with four novel SIPM variants leveraging distinct stochastic gradient estimators. Under mild assumptions, we establish the iteration complexity of our proposed SIPMs, which, up to a polylogarithmic factor, match the best-known results in stochastic unconstrained optimization. Finally, our numerical experiments on robust linear regression, multi-task relationship learning, and clustering data streams demonstrate the effectiveness and efficiency of our approach.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Multi-Modal Data with Tool-Augmented LLM Agents for Precise Causal Discovery</title>
<link>https://arxiv.org/abs/2412.13667</link>
<guid>https://arxiv.org/abs/2412.13667</guid>
<content:encoded><![CDATA[
arXiv:2412.13667v2 Announce Type: replace-cross 
Abstract: Causal discovery is an imperative foundation for decision-making across domains, such as smart health, AI for drug discovery and AIOps. Traditional statistical causal discovery methods, while well-established, predominantly rely on observational data and often overlook the semantic cues inherent in cause-and-effect relationships. The advent of Large Language Models (LLMs) has ushered in an affordable way of leveraging the semantic cues for knowledge-driven causal discovery, but the development of LLMs for causal discovery lags behind other areas, particularly in the exploration of multi-modal data. To bridge the gap, we introduce MATMCD, a multi-agent system powered by tool-augmented LLMs. MATMCD has two key agents: a Data Augmentation agent that retrieves and processes modality-augmented data, and a Causal Constraint agent that integrates multi-modal data for knowledge-driven reasoning. The proposed design of the inner-workings ensures successful cooperation of the agents. Our empirical study across seven datasets suggests the significant potential of multi-modality enhanced causal discovery.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M$^3$-VOS: Multi-Phase, Multi-Transition, and Multi-Scenery Video Object Segmentation</title>
<link>https://arxiv.org/abs/2412.13803</link>
<guid>https://arxiv.org/abs/2412.13803</guid>
<content:encoded><![CDATA[
arXiv:2412.13803v3 Announce Type: replace-cross 
Abstract: Intelligent robots need to interact with diverse objects across various environments. The appearance and state of objects frequently undergo complex transformations depending on the object properties, e.g., phase transitions. However, in the vision community, segmenting dynamic objects with phase transitions is overlooked. In light of this, we introduce the concept of phase in segmentation, which categorizes real-world objects based on their visual characteristics and potential morphological and appearance changes. Then, we present a new benchmark, Multi-Phase, Multi-Transition, and Multi-Scenery Video Object Segmentation (M$^3$-VOS), to verify the ability of models to understand object phases, which consists of 479 high-resolution videos spanning over 10 distinct everyday scenarios. It provides dense instance mask annotations that capture both object phases and their transitions. We evaluate state-of-the-art methods on M$^3$-VOS, yielding several key insights. Notably, current appearance-based approaches show significant room for improvement when handling objects with phase transitions. The inherent changes in disorder suggest that the predictive performance of the forward entropy-increasing process can be improved through a reverse entropy-reducing process. These findings lead us to propose ReVOS, a new plug-andplay model that improves its performance by reversal refinement. Our data and code will be publicly available at https://zixuan-chen.github.io/M-cube-VOS.github.io/.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PepTune: De Novo Generation of Therapeutic Peptides with Multi-Objective-Guided Discrete Diffusion</title>
<link>https://arxiv.org/abs/2412.17780</link>
<guid>https://arxiv.org/abs/2412.17780</guid>
<content:encoded><![CDATA[
arXiv:2412.17780v4 Announce Type: replace-cross 
Abstract: We present PepTune, a multi-objective discrete diffusion model for simultaneous generation and optimization of therapeutic peptide SMILES. Built on the Masked Discrete Language Model (MDLM) framework, PepTune ensures valid peptide structures with a novel bond-dependent masking schedule and invalid loss function. To guide the diffusion process, we introduce Monte Carlo Tree Guidance (MCTG), an inference-time multi-objective guidance algorithm that balances exploration and exploitation to iteratively refine Pareto-optimal sequences. MCTG integrates classifier-based rewards with search-tree expansion, overcoming gradient estimation challenges and data sparsity. Using PepTune, we generate diverse, chemically-modified peptides simultaneously optimized for multiple therapeutic properties, including target binding affinity, membrane permeability, solubility, hemolysis, and non-fouling for various disease-relevant targets. In total, our results demonstrate that MCTG for masked discrete diffusion is a powerful and modular approach for multi-objective sequence design in discrete state spaces.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuron Empirical Gradient: Discovering and Quantifying Neurons Global Linear Controllability</title>
<link>https://arxiv.org/abs/2412.18053</link>
<guid>https://arxiv.org/abs/2412.18053</guid>
<content:encoded><![CDATA[
arXiv:2412.18053v3 Announce Type: replace-cross 
Abstract: While feed-forward neurons in pre-trained language models (PLMs) can encode knowledge, past research targeted a small subset of neurons that heavily influence outputs. This leaves the broader role of neuron activations unclear, limiting progress in areas like knowledge editing. We uncover a global linear relationship between neuron activations and outputs using neuron interventions on a knowledge probing dataset. The gradient of this linear relationship, which we call the neuron empirical gradient (NEG), captures how changes in activations affect predictions. To compute NEG efficiently, we propose NeurGrad, enabling large-scale analysis of neuron behavior in PLMs. We also show that NEG effectively captures language skills across diverse prompts through skill neuron probing. Experiments on MCEval8k, a multi-genre multiple-choice knowledge benchmark, support NEG's ability to represent model knowledge. Further analysis highlights the key properties of NEG-based skill representation: efficiency, robustness, flexibility, and interdependency. The code and data are released.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Language Models Understand the Cognitive Tasks Given to Them? Investigations with the N-Back Paradigm</title>
<link>https://arxiv.org/abs/2412.18120</link>
<guid>https://arxiv.org/abs/2412.18120</guid>
<content:encoded><![CDATA[
arXiv:2412.18120v3 Announce Type: replace-cross 
Abstract: Cognitive tasks originally developed for humans are now increasingly used to study language models. While applying these tasks is often straightforward, interpreting their results can be challenging. In particular, when a model underperforms, it is often unclear whether this results from a limitation in the cognitive ability being tested or a failure to understand the task itself. A recent study argues that GPT 3.5's declining performance on 2-back and 3-back tasks reflects a working memory capacity limit similar to humans (Gong et al., 2024). By analyzing a range of open-source language models of varying performance levels on these tasks, we show that the poor performance is due at least in part to a limitation in task comprehension and task set maintenance. We challenge the best-performing model with progressively harder versions of the task (up to 10-back) and experiment with alternative prompting strategies, before analyzing model attentions. Our larger aim is to contribute to the ongoing conversation around refining methodologies for the cognitive evaluation of language models.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Token-Budget-Aware LLM Reasoning</title>
<link>https://arxiv.org/abs/2412.18547</link>
<guid>https://arxiv.org/abs/2412.18547</guid>
<content:encoded><![CDATA[
arXiv:2412.18547v5 Announce Type: replace-cross 
Abstract: Reasoning is critical for large language models (LLMs) to excel in a wide range of tasks. While methods like Chain-of-Thought (CoT) reasoning and enhance LLM performance by decomposing problems into intermediate steps, they also incur significant overhead in token usage, leading to increased costs. We find that the reasoning process of current LLMs is unnecessarily lengthy and it can be compressed by including a reasonable token budget in the prompt, but the choice of token budget plays a crucial role in the actual compression effectiveness. We then propose a token-budget-aware LLM reasoning framework that dynamically adjusts the number of reasoning tokens based on the reasoning complexity of each problem. Experiments show that our method effectively reduces token costs in CoT reasoning with only a slight performance reduction, offering a practical solution to balance efficiency and accuracy in LLM reasoning. Code: https://github.com/GeniusHTX/TALE
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Compositional Generalization of Multimodal LLMs for Medical Imaging</title>
<link>https://arxiv.org/abs/2412.20070</link>
<guid>https://arxiv.org/abs/2412.20070</guid>
<content:encoded><![CDATA[
arXiv:2412.20070v2 Announce Type: replace-cross 
Abstract: Medical imaging provides essential visual insights for diagnosis, and multimodal large language models (MLLMs) are increasingly utilized for its analysis due to their strong generalization capabilities; however, the underlying factors driving this generalization remain unclear. Current research suggests that multi-task training outperforms single-task as different tasks can benefit each other, but they often overlook the internal relationships within these tasks. To analyze this phenomenon, we attempted to employ compositional generalization (CG), which refers to the models' ability to understand novel combinations by recombining learned elements, as a guiding framework. Since medical images can be precisely defined by Modality, Anatomical area, and Task, naturally providing an environment for exploring CG, we assembled 106 medical datasets to create Med-MAT for comprehensive experiments. The experiments confirmed that MLLMs can use CG to understand unseen medical images and identified CG as one of the main drivers of the generalization observed in multi-task training. Additionally, further studies demonstrated that CG effectively supports datasets with limited data and confirmed that MLLMs can achieve CG across classification and detection tasks, underscoring its broader generalization potential. Med-MAT is available at https://github.com/FreedomIntelligence/Med-MAT.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Transformers for Generalizable First-Order Logical Entailment</title>
<link>https://arxiv.org/abs/2501.00759</link>
<guid>https://arxiv.org/abs/2501.00759</guid>
<content:encoded><![CDATA[
arXiv:2501.00759v2 Announce Type: replace-cross 
Abstract: Transformers, as the fundamental deep learning architecture, have demonstrated great capability in reasoning. This paper studies the generalizable first-order logical reasoning ability of transformers with their parameterized knowledge and how to improve it. Transformers' capability of first-order reasoning is further captured by whether they can conduct first-order logical entailment, which is quantitatively measured by their performance in answering knowledge graph queries. We establish the connections between (1) two types of distribution shifts studied in out-of-distribution generalization and (2) unseen knowledge and query settings discussed in the task of knowledge graph query answering, which makes it possible to characterize the fine-grained generalizability. Results on our comprehensive dataset showed that transformers outperform previous methods designed particularly for this task and provided detailed empirical evidence about the impact of the input query syntax, token embedding, and transformer architectures on the reasoning capability of transformers. Interestingly, our results revealed the mismatch of positional encoding and other design choices of transformer architectures in previous practices. Motivated by this, we propose TEGA, a logic-aware architecture that significantly improves the performance in generalizable first-order logical entailment.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are LLMs effective psychological assessors? Leveraging adaptive RAG for interpretable mental health screening through psychometric practice</title>
<link>https://arxiv.org/abs/2501.00982</link>
<guid>https://arxiv.org/abs/2501.00982</guid>
<content:encoded><![CDATA[
arXiv:2501.00982v2 Announce Type: replace-cross 
Abstract: In psychological practices, standardized questionnaires serve as essential tools for assessing mental health through structured, clinically-validated questions (i.e., items). While social media platforms offer rich data for mental health screening, computational approaches often bypass these established clinical assessment tools in favor of black-box classification. We propose a novel questionnaire-guided screening framework that bridges psychological practice and computational methods through adaptive Retrieval-Augmented Generation (\textit{aRAG}). Our approach links unstructured social media content and standardized clinical assessments by retrieving relevant posts for each questionnaire item and using Large Language Models (LLMs) to complete validated psychological instruments. Our findings demonstrate two key advantages of questionnaire-guided screening: First, when completing the Beck Depression Inventory-II (BDI-II), our approach matches or outperforms state-of-the-art performance on Reddit-based benchmarks without requiring training data. Second, we show that guiding LLMs through standardized questionnaires can yield superior results compared to directly prompting them for depression screening, while also providing a more interpretable assessment by linking model outputs to clinically validated diagnostic criteria. Additionally, we show, as a proof-of-concept, how our questionnaire-based methodology can be extended to other mental conditions' screening, highlighting the promising role of LLMs as psychological assessors.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Medical Large Vision-Language Models with Abnormal-Aware Feedback</title>
<link>https://arxiv.org/abs/2501.01377</link>
<guid>https://arxiv.org/abs/2501.01377</guid>
<content:encoded><![CDATA[
arXiv:2501.01377v2 Announce Type: replace-cross 
Abstract: Existing Medical Large Vision-Language Models (Med-LVLMs), encapsulating extensive medical knowledge, demonstrate excellent capabilities in understanding medical images. However, there remain challenges in visual localization in medical images, which is crucial for abnormality detection and interpretation. To address these issues, we propose a novel UMed-LVLM designed to unveil medical abnormalities. Specifically, we collect a Medical Abnormalities Unveiling (MAU) dataset and propose a two-stage training method for UMed-LVLM training. To collect MAU dataset, we propose a prompt method utilizing the GPT-4V to generate diagnoses based on identified abnormal areas in medical images. Moreover, the two-stage training method includes Abnormal-Aware Instruction Tuning and Abnormal-Aware Rewarding, comprising Relevance Reward, Abnormal Localization Reward and Vision Relevance Reward. Experimental results demonstrate that our UMed-LVLM significantly outperforms existing Med-LVLMs in identifying and understanding medical abnormalities, achieving a 58% improvement over the baseline. In addition, this work shows that enhancing the abnormality detection capabilities of Med-LVLMs significantly improves their understanding of medical images and generalization capability.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automating Legal Interpretation with LLMs: Retrieval, Generation, and Evaluation</title>
<link>https://arxiv.org/abs/2501.01743</link>
<guid>https://arxiv.org/abs/2501.01743</guid>
<content:encoded><![CDATA[
arXiv:2501.01743v3 Announce Type: replace-cross 
Abstract: Interpreting the law is always essential for the law to adapt to the ever-changing society. It is a critical and challenging task even for legal practitioners, as it requires meticulous and professional annotations and summarizations by legal experts, which are admittedly time-consuming and expensive to collect at scale. To alleviate the burden on legal experts, we propose a method for automated legal interpretation. Specifically, by emulating doctrinal legal research, we introduce a novel framework, ATRIE, to address Legal Concept Interpretation, a typical task in legal interpretation. ATRIE utilizes large language models (LLMs) to AuTomatically Retrieve concept-related information, Interpret legal concepts, and Evaluate generated interpretations, eliminating dependence on legal experts. ATRIE comprises a legal concept interpreter and a legal concept interpretation evaluator. The interpreter uses LLMs to retrieve relevant information from previous cases and interpret legal concepts. The evaluator uses performance changes on Legal Concept Entailment, a downstream task we propose, as a proxy of interpretation quality. Automated and multifaceted human evaluations indicate that the quality of our interpretations is comparable to those written by legal experts, with superior comprehensiveness and readability. Although there remains a slight gap in accuracy, it can already assist legal practitioners in improving the efficiency of legal interpretation.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing Equivariance and Symmetry Breaking in Convolutional Networks</title>
<link>https://arxiv.org/abs/2501.01999</link>
<guid>https://arxiv.org/abs/2501.01999</guid>
<content:encoded><![CDATA[
arXiv:2501.01999v3 Announce Type: replace-cross 
Abstract: In this work, we explore the trade-offs of explicit structural priors, particularly group equivariance. We address this through theoretical analysis and a comprehensive empirical study. To enable controlled and fair comparisons, we introduce \texttt{Rapidash}, a unified group convolutional architecture that allows for different variants of equivariant and non-equivariant models. Our results suggest that more constrained equivariant models outperform less constrained alternatives when aligned with the geometry of the task, and increasing representation capacity does not fully eliminate performance gaps. We see improved performance of models with equivariance and symmetry-breaking through tasks like segmentation, regression, and generation across diverse datasets. Explicit \textit{symmetry breaking} via geometric reference frames consistently improves performance, while \textit{breaking equivariance} through geometric input features can be helpful when aligned with task geometry. Our results provide task-specific performance trends that offer a more nuanced way for model selection.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TACLR: A Scalable and Efficient Retrieval-based Method for Industrial Product Attribute Value Identification</title>
<link>https://arxiv.org/abs/2501.03835</link>
<guid>https://arxiv.org/abs/2501.03835</guid>
<content:encoded><![CDATA[
arXiv:2501.03835v3 Announce Type: replace-cross 
Abstract: Product Attribute Value Identification (PAVI) involves identifying attribute values from product profiles, a key task for improving product search, recommendation, and business analytics on e-commerce platforms. However, existing PAVI methods face critical challenges, such as inferring implicit values, handling out-of-distribution (OOD) values, and producing normalized outputs. To address these limitations, we introduce Taxonomy-Aware Contrastive Learning Retrieval (TACLR), the first retrieval-based method for PAVI. TACLR formulates PAVI as an information retrieval task by encoding product profiles and candidate values into embeddings and retrieving values based on their similarity. It leverages contrastive training with taxonomy-aware hard negative sampling and employs adaptive inference with dynamic thresholds. TACLR offers three key advantages: (1) it effectively handles implicit and OOD values while producing normalized outputs; (2) it scales to thousands of categories, tens of thousands of attributes, and millions of values; and (3) it supports efficient inference for high-load industrial deployment. Extensive experiments on proprietary and public datasets validate the effectiveness and efficiency of TACLR. Further, it has been successfully deployed on the real-world e-commerce platform Xianyu, processing millions of product listings daily with frequently updated, large-scale attribute taxonomies. We release the code to facilitate reproducibility and future research at https://github.com/SuYindu/TACLR.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCC-YOLO: An Improved Object Detector for Assisting in Brain Tumor Diagnosis</title>
<link>https://arxiv.org/abs/2501.03836</link>
<guid>https://arxiv.org/abs/2501.03836</guid>
<content:encoded><![CDATA[
arXiv:2501.03836v4 Announce Type: replace-cross 
Abstract: Brain tumors can lead to neurological dysfunction, cognitive and psychological changes, increased intracranial pressure, and seizures, posing significant risks to health. The You Only Look Once (YOLO) series has shown superior accuracy in medical imaging object detection. This paper presents a novel SCC-YOLO architecture that integrates the SCConv module into YOLOv9. The SCConv module optimizes convolutional efficiency by reducing spatial and channel redundancy, enhancing image feature learning. We examine the effects of different attention mechanisms with YOLOv9 for brain tumor detection using the Br35H dataset and our custom dataset (Brain_Tumor_Dataset). Results indicate that SCC-YOLO improved mAP50 by 0.3% on the Br35H dataset and by 0.5% on our custom dataset compared to YOLOv9. SCC-YOLO achieves state-of-the-art performance in brain tumor detection.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MADUV: The 1st INTERSPEECH Mice Autism Detection via Ultrasound Vocalization Challenge</title>
<link>https://arxiv.org/abs/2501.04292</link>
<guid>https://arxiv.org/abs/2501.04292</guid>
<content:encoded><![CDATA[
arXiv:2501.04292v3 Announce Type: replace-cross 
Abstract: The Mice Autism Detection via Ultrasound Vocalization (MADUV) Challenge introduces the first INTERSPEECH challenge focused on detecting autism spectrum disorder (ASD) in mice through their vocalizations. Participants are tasked with developing models to automatically classify mice as either wild-type or ASD models based on recordings with a high sampling rate. Our baseline system employs a simple CNN-based classification using three different spectrogram features. Results demonstrate the feasibility of automated ASD detection, with the considered audible-range features achieving the best performance (UAR of 0.600 for segment-level and 0.625 for subject-level classification). This challenge bridges speech technology and biomedical research, offering opportunities to advance our understanding of ASD models through machine learning approaches. The findings suggest promising directions for vocalization analysis and highlight the potential value of audible and ultrasound vocalizations in ASD detection.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Step-by-Step Mastery: Enhancing Soft Constraint Following Ability of Large Language Models</title>
<link>https://arxiv.org/abs/2501.04945</link>
<guid>https://arxiv.org/abs/2501.04945</guid>
<content:encoded><![CDATA[
arXiv:2501.04945v4 Announce Type: replace-cross 
Abstract: It is crucial for large language models (LLMs) to follow instructions that involve multiple constraints. However, it is an unexplored area to enhance LLMs' ability to follow soft constraints. To bridge the gap, we initially design a pipeline to construct datasets with high-quality outputs automatically. Additionally, to fully utilize the positive and negative samples generated during the data construction process, we choose Direct Preference Optimization (DPO) as the training method. Furthermore, taking into account the difficulty of soft constraints indicated by the number of constraints, we design a curriculum learning training paradigm based on the constraint quantity. We experimentally evaluate the effectiveness of our methods in improving LLMs' soft constraint following ability and analyze the factors driving the improvements.The datasets and code are publicly available at https://github.com/Rainier-rq/FollowSoftConstraint.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effective faking of verbal deception detection with target-aligned adversarial attacks</title>
<link>https://arxiv.org/abs/2501.05962</link>
<guid>https://arxiv.org/abs/2501.05962</guid>
<content:encoded><![CDATA[
arXiv:2501.05962v2 Announce Type: replace-cross 
Abstract: Background: Deception detection through analysing language is a promising avenue using both human judgments and automated machine learning judgments. For both forms of credibility assessment, automated adversarial attacks that rewrite deceptive statements to appear truthful pose a serious threat. Methods: We used a dataset of 243 truthful and 262 fabricated autobiographical stories in a deception detection task for humans and machine learning models. A large language model was tasked to rewrite deceptive statements so that they appear truthful. In Study 1, humans who made a deception judgment or used the detailedness heuristic and two machine learning models (a fine-tuned language model and a simple n-gram model) judged original or adversarial modifications of deceptive statements. In Study 2, we manipulated the target alignment of the modifications, i.e. tailoring the attack to whether the statements would be assessed by humans or computer models. Results: When adversarial modifications were aligned with their target, human (d=-0.07 and d=-0.04) and machine judgments (51% accuracy) dropped to the chance level. When the attack was not aligned with the target, both human heuristics judgments (d=0.30 and d=0.36) and machine learning predictions (63-78%) were significantly better than chance. Conclusions: Easily accessible language models can effectively help anyone fake deception detection efforts both by humans and machine learning models. Robustness against adversarial modifications for humans and machines depends on that target alignment. We close with suggestions on advancing deception research with adversarial attack designs and techniques.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In the Picture: Medical Imaging Datasets, Artifacts, and their Living Review</title>
<link>https://arxiv.org/abs/2501.10727</link>
<guid>https://arxiv.org/abs/2501.10727</guid>
<content:encoded><![CDATA[
arXiv:2501.10727v2 Announce Type: replace-cross 
Abstract: Datasets play a critical role in medical imaging research, yet issues such as label quality, shortcuts, and metadata are often overlooked. This lack of attention may harm the generalizability of algorithms and, consequently, negatively impact patient outcomes. While existing medical imaging literature reviews mostly focus on machine learning (ML) methods, with only a few focusing on datasets for specific applications, these reviews remain static -- they are published once and not updated thereafter. This fails to account for emerging evidence, such as biases, shortcuts, and additional annotations that other researchers may contribute after the dataset is published. We refer to these newly discovered findings of datasets as research artifacts. To address this gap, we propose a living review that continuously tracks public datasets and their associated research artifacts across multiple medical imaging applications. Our approach includes a framework for the living review to monitor data documentation artifacts, and an SQL database to visualize the citation relationships between research artifact and dataset. Lastly, we discuss key considerations for creating medical imaging datasets, review best practices for data annotation, discuss the significance of shortcuts and demographic diversity, and emphasize the importance of managing datasets throughout their entire lifecycle. Our demo is publicly available at http://inthepicture.itu.dk/.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Plausible Distractors for Multiple-Choice Questions via Student Choice Prediction</title>
<link>https://arxiv.org/abs/2501.13125</link>
<guid>https://arxiv.org/abs/2501.13125</guid>
<content:encoded><![CDATA[
arXiv:2501.13125v3 Announce Type: replace-cross 
Abstract: In designing multiple-choice questions (MCQs) in education, creating plausible distractors is crucial for identifying students' misconceptions and gaps in knowledge and accurately assessing their understanding. However, prior studies on distractor generation have not paid sufficient attention to enhancing the difficulty of distractors, resulting in reduced effectiveness of MCQs. This study presents a pipeline for training a model to generate distractors that are more likely to be selected by students. First, we train a pairwise ranker to reason about students' misconceptions and assess the relative plausibility of two distractors. Using this model, we create a dataset of pairwise distractor ranks and then train a distractor generator via Direct Preference Optimization (DPO) to generate more plausible distractors. Experiments on computer science subjects (Python, DB, MLDL) demonstrate that our pairwise ranker effectively identifies students' potential misunderstandings and achieves ranking accuracy comparable to human experts. Furthermore, our distractor generator outperforms several baselines in generating plausible distractors and produces questions with a higher item discrimination index (DI).
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jailbreak-AudioBench: In-Depth Evaluation and Analysis of Jailbreak Threats for Large Audio Language Models</title>
<link>https://arxiv.org/abs/2501.13772</link>
<guid>https://arxiv.org/abs/2501.13772</guid>
<content:encoded><![CDATA[
arXiv:2501.13772v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) demonstrate impressive zero-shot performance across a wide range of natural language processing tasks. Integrating various modality encoders further expands their capabilities, giving rise to Multimodal Large Language Models (MLLMs) that process not only text but also visual and auditory modality inputs. However, these advanced capabilities may also pose significant security risks, as models can be exploited to generate harmful or inappropriate content through jailbreak attack. While prior work has extensively explored how manipulating textual or visual modality inputs can circumvent safeguards in LLMs and MLLMs, the vulnerability of audio-specific Jailbreak on Large Audio-Language Models (LALMs) remains largely underexplored. To address this gap, we introduce \textbf{Jailbreak-AudioBench}, which consists of the Toolbox, curated Dataset, and comprehensive Benchmark. The Toolbox supports not only text-to-audio conversion but also various editing techniques for injecting audio hidden semantics. The curated Dataset provides diverse explicit and implicit jailbreak audio examples in both original and edited forms. Utilizing this dataset, we evaluate multiple state-of-the-art LALMs and establish the most comprehensive Jailbreak benchmark to date for audio modality. Finally, Jailbreak-AudioBench establishes a foundation for advancing future research on LALMs safety alignment by enabling the in-depth exposure of more powerful jailbreak threats, such as query-based audio editing, and by facilitating the development of effective defense mechanisms.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of Seismic Artificial Intelligence with Uncertainty</title>
<link>https://arxiv.org/abs/2501.14809</link>
<guid>https://arxiv.org/abs/2501.14809</guid>
<content:encoded><![CDATA[
arXiv:2501.14809v2 Announce Type: replace-cross 
Abstract: Artificial intelligence has transformed the seismic community with deep learning models (DLMs) that are trained to complete specific tasks within workflows. However, there is still lack of robust evaluation frameworks for evaluating and comparing DLMs. We address this gap by designing an evaluation framework that jointly incorporates two crucial aspects: performance uncertainty and learning efficiency. To target these aspects, we meticulously construct the training, validation, and test splits using a clustering method tailored to seismic data and enact an expansive training design to segregate performance uncertainty arising from stochastic training processes and random data sampling. The framework's ability to guard against misleading declarations of model superiority is demonstrated through evaluation of PhaseNet [1], a popular seismic phase picking DLM, under 3 training approaches. Our framework helps practitioners choose the best model for their problem and set performance expectations by explicitly analyzing model performance with uncertainty at varying budgets of training data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ExPerT: Effective and Explainable Evaluation of Personalized Long-Form Text Generation</title>
<link>https://arxiv.org/abs/2501.14956</link>
<guid>https://arxiv.org/abs/2501.14956</guid>
<content:encoded><![CDATA[
arXiv:2501.14956v2 Announce Type: replace-cross 
Abstract: Evaluating personalized text generated by large language models (LLMs) is challenging, as only the LLM user, i.e., prompt author, can reliably assess the output, but re-engaging the same individuals across studies is infeasible. This paper addresses the challenge of evaluating personalized text generation by introducing ExPerT, an explainable reference-based evaluation framework. ExPerT leverages an LLM to extract atomic aspects and their evidence from the generated and reference texts, match the aspects, and evaluate their alignment based on content and writing style -- two key attributes in personalized text generation. Additionally, ExPerT generates detailed, fine-grained explanations for every step of the evaluation process, enhancing transparency and interpretability. Our experiments demonstrate that ExPerT achieves a 7.2% relative improvement in alignment with human judgments compared to the state-of-the-art text generation evaluation methods. Furthermore, human evaluators rated the usability of ExPerT's explanations at 4.7 out of 5, highlighting its effectiveness in making evaluation decisions more interpretable.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficiency Bottlenecks of Convolutional Kolmogorov-Arnold Networks: A Comprehensive Scrutiny with ImageNet, AlexNet, LeNet and Tabular Classification</title>
<link>https://arxiv.org/abs/2501.15757</link>
<guid>https://arxiv.org/abs/2501.15757</guid>
<content:encoded><![CDATA[
arXiv:2501.15757v3 Announce Type: replace-cross 
Abstract: Algorithmic level developments like Convolutional Neural Networks, transformers, attention mechanism, Retrieval Augmented Generation and so on have changed Artificial Intelligence. Recent such development was observed by Kolmogorov-Arnold Networks that suggested to challenge the fundamental concept of a Neural Network, thus change Multilayer Perceptron, and Convolutional Neural Networks. They received a good reception in terms of scientific modeling, yet had some drawbacks in terms of efficiency. In this paper, we train Convolutional Kolmogorov Arnold Networks (CKANs) with the ImageNet-1k dataset with 1.3 million images, MNIST dataset with 60k images and a tabular biological science related MoA dataset and test the promise of CKANs in terms of FLOPS, Inference Time, number of trainable parameters and training time against the accuracy, precision, recall and f-1 score they produce against the standard industry practice on CNN models. We show that the CKANs perform fair yet slower than CNNs in small size dataset like MoA and MNIST but are not nearly comparable as the dataset gets larger and more complex like the ImageNet. The code implementation of this paper can be found on the link: https://github.com/ashimdahal/Study-of-Convolutional-Kolmogorov-Arnold-networks
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WhiSPA: Semantically and Psychologically Aligned Whisper with Self-Supervised Contrastive and Student-Teacher Learning</title>
<link>https://arxiv.org/abs/2501.16344</link>
<guid>https://arxiv.org/abs/2501.16344</guid>
<content:encoded><![CDATA[
arXiv:2501.16344v4 Announce Type: replace-cross 
Abstract: Current speech encoding pipelines often rely on an additional text-based LM to get robust representations of human communication, even though SotA speech-to-text models often have a LM within. This work proposes an approach to improve the LM within an audio model such that the subsequent text-LM is unnecessary. We introduce WhiSPA (Whisper with Semantic and Psychological Alignment), which leverages a novel audio training objective: contrastive loss with a language model embedding as a teacher. Using over 500k speech segments from mental health audio interviews, we evaluate the utility of aligning Whisper's latent space with semantic representations from a text autoencoder (SBERT) and lexically derived embeddings of basic psychological dimensions: emotion and personality. Over self-supervised affective tasks and downstream psychological tasks, WhiSPA surpasses current speech encoders, achieving an average error reduction of 73.4% and 83.8%, respectively. WhiSPA demonstrates that it is not always necessary to run a subsequent text LM on speech-to-text output in order to get a rich psychological representation of human communication.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inducing, Detecting and Characterising Neural Modules: A Pipeline for Functional Interpretability in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2501.17077</link>
<guid>https://arxiv.org/abs/2501.17077</guid>
<content:encoded><![CDATA[
arXiv:2501.17077v2 Announce Type: replace-cross 
Abstract: Interpretability is crucial for ensuring RL systems align with human values. However, it remains challenging to achieve in complex decision making domains. Existing methods frequently attempt interpretability at the level of fundamental model units, such as neurons or decision nodes: an approach which scales poorly to large models. Here, we instead propose an approach to interpretability at the level of functional modularity. We show how encouraging sparsity and locality in network weights leads to the emergence of functional modules in RL policy networks. To detect these modules, we develop an extended Louvain algorithm which uses a novel `correlation alignment' metric to overcome the limitations of standard network analysis techniques when applied to neural network architectures. Applying these methods to 2D and 3D MiniGrid environments reveals the consistent emergence of distinct navigational modules for different axes, and we further demonstrate how these functions can be validated through direct interventions on network weights prior to inference.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dialogue Systems for Emotional Support via Value Reinforcement</title>
<link>https://arxiv.org/abs/2501.17182</link>
<guid>https://arxiv.org/abs/2501.17182</guid>
<content:encoded><![CDATA[
arXiv:2501.17182v3 Announce Type: replace-cross 
Abstract: Emotional support dialogue systems aim to reduce help-seekers' distress and help them overcome challenges. While human values$\unicode{x2013}$core beliefs that shape an individual's priorities$\unicode{x2013}$are increasingly emphasized in contemporary psychological therapy for their role in fostering internal transformation and long-term emotional well-being, their integration into emotional support systems remains underexplored. To bridge this gap, we present a value-driven method for training emotional support dialogue systems designed to reinforce positive values in seekers. Notably, our model identifies which values to reinforce at each turn and how to do so, by leveraging online support conversations from Reddit. We evaluate the method across support skills, seekers' emotional intensity, and value reinforcement. Our method consistently outperforms various baselines, effectively exploring and eliciting values from seekers. Additionally, leveraging crowd knowledge from Reddit significantly enhances its effectiveness. Therapists highlighted its ability to validate seekers' challenges and emphasize positive aspects of their situations$\unicode{x2013}$both crucial elements of value reinforcement. Our work, being the first to integrate value reinforcement into emotional support systems, demonstrates its promise and establishes a foundation for future research.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Multimodal Learning via Cross-Modal Proxy Tokens</title>
<link>https://arxiv.org/abs/2501.17823</link>
<guid>https://arxiv.org/abs/2501.17823</guid>
<content:encoded><![CDATA[
arXiv:2501.17823v3 Announce Type: replace-cross 
Abstract: Multimodal models often experience a significant performance drop when one or more modalities are missing during inference. To address this challenge, we propose a simple yet effective approach that enhances robustness to missing modalities while maintaining strong performance when all modalities are available. Our method introduces cross-modal proxy tokens (CMPTs), which approximate the class token of a missing modality by attending only to the tokens of the available modality without requiring explicit modality generation or auxiliary networks. To efficiently learn these approximations with minimal computational overhead, we employ low-rank adapters in frozen unimodal encoders and jointly optimize an alignment loss with a task-specific loss. Extensive experiments on five multimodal datasets show that our method outperforms state-of-the-art baselines across various missing rates while achieving competitive results in complete-modality settings. Overall, our method offers a flexible and efficient solution for robust multimodal learning. The code and pretrained models will be released on GitHub.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The TIP of the Iceberg: Revealing a Hidden Class of Task-in-Prompt Adversarial Attacks on LLMs</title>
<link>https://arxiv.org/abs/2501.18626</link>
<guid>https://arxiv.org/abs/2501.18626</guid>
<content:encoded><![CDATA[
arXiv:2501.18626v4 Announce Type: replace-cross 
Abstract: We present a novel class of jailbreak adversarial attacks on LLMs, termed Task-in-Prompt (TIP) attacks. Our approach embeds sequence-to-sequence tasks (e.g., cipher decoding, riddles, code execution) into the model's prompt to indirectly generate prohibited inputs. To systematically assess the effectiveness of these attacks, we introduce the PHRYGE benchmark. We demonstrate that our techniques successfully circumvent safeguards in six state-of-the-art language models, including GPT-4o and LLaMA 3.2. Our findings highlight critical weaknesses in current LLM safety alignments and underscore the urgent need for more sophisticated defence strategies.
  Warning: this paper contains examples of unethical inquiries used solely for research purposes.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Abstraction Learning based on the Semantic Embedding Principle</title>
<link>https://arxiv.org/abs/2502.00407</link>
<guid>https://arxiv.org/abs/2502.00407</guid>
<content:encoded><![CDATA[
arXiv:2502.00407v2 Announce Type: replace-cross 
Abstract: Structural causal models (SCMs) allow us to investigate complex systems at multiple levels of resolution. The causal abstraction (CA) framework formalizes the mapping between high- and low-level SCMs. We address CA learning in a challenging and realistic setting, where SCMs are inaccessible, interventional data is unavailable, and sample data is misaligned. A key principle of our framework is semantic embedding, formalized as the high-level distribution lying on a subspace of the low-level one. This principle naturally links linear CA to the geometry of the Stiefel manifold. We present a category-theoretic approach to SCMs that enables the learning of a CA by finding a morphism between the low- and high-level probability measures, adhering to the semantic embedding principle. Consequently, we formulate a general CA learning problem. As an application, we solve the latter problem for linear CA; considering Gaussian measures and the Kullback-Leibler divergence as an objective. Given the nonconvexity of the learning task, we develop three algorithms building upon existing paradigms for Riemannian optimization. We demonstrate that the proposed methods succeed on both synthetic and real-world brain data with different degrees of prior information about the structure of CA.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Safety Alignment is Divergence Estimation in Disguise</title>
<link>https://arxiv.org/abs/2502.00657</link>
<guid>https://arxiv.org/abs/2502.00657</guid>
<content:encoded><![CDATA[
arXiv:2502.00657v2 Announce Type: replace-cross 
Abstract: We present a theoretical framework showing that popular LLM alignment methods, including RLHF and its variants, can be understood as divergence estimators between aligned (safe or preferred) and unaligned (harmful or less preferred) distributions. This perspective explains the emergence of separation in the latent space between safe and harmful prompts after alignment. As an application of our general divergence framework, we propose KLDO, a novel KL divergence-based alignment method, and empirically validate its effectiveness. We further show that using compliance-refusal datasets, rather than standard preference-based datasets, leads to stronger separation and improved safety alignment. Finally, to quantify the separation effect, we propose a distance-based metric in the prompt representation space, which also acts as a statistically significant indicator for model safety.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AtmosSci-Bench: Evaluating the Recent Advance of Large Language Model for Atmospheric Science</title>
<link>https://arxiv.org/abs/2502.01159</link>
<guid>https://arxiv.org/abs/2502.01159</guid>
<content:encoded><![CDATA[
arXiv:2502.01159v2 Announce Type: replace-cross 
Abstract: The rapid advancements in large language models (LLMs), particularly in their reasoning capabilities, hold transformative potential for addressing complex challenges in atmospheric science. However, leveraging LLMs effectively in this domain requires a robust and comprehensive evaluation benchmark. Toward this end, we present AtmosSci-Bench, a novel benchmark designed to systematically assess LLM performance across five core categories of atmospheric science problems: hydrology, atmospheric dynamics, atmospheric physics, geophysics, and physical oceanography. AtmosSci-Bench features a dual-format design comprising both multiple-choice questions (MCQs) and open-ended questions (OEQs), enabling scalable automated evaluation alongside deeper analysis of conceptual understanding. We employ a template-based MCQ generation framework to create diverse, graduate-level problems with symbolic perturbation, while OEQs are used to probe open-ended reasoning. We conduct a comprehensive evaluation of representative LLMs, categorized into four groups: instruction-tuned models, advanced reasoning models, math-augmented models, and domain-specific climate models. Our analysis provides some interesting insights into the reasoning and problem-solving capabilities of LLMs in atmospheric science. We believe AtmosSci-Bench can serve as a critical step toward advancing LLM applications in climate service by offering a standard and rigorous evaluation framework. Our source codes are currently available at Our source codes are currently available at https://github.com/Relaxed-System-Lab/AtmosSci-Bench.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Transformer World Models for Data-Efficient RL</title>
<link>https://arxiv.org/abs/2502.01591</link>
<guid>https://arxiv.org/abs/2502.01591</guid>
<content:encoded><![CDATA[
arXiv:2502.01591v2 Announce Type: replace-cross 
Abstract: We present an approach to model-based RL that achieves a new state of the art performance on the challenging Craftax-classic benchmark, an open-world 2D survival game that requires agents to exhibit a wide range of general abilities -- such as strong generalization, deep exploration, and long-term reasoning. With a series of careful design choices aimed at improving sample efficiency, our MBRL algorithm achieves a reward of 69.66% after only 1M environment steps, significantly outperforming DreamerV3, which achieves 53.2%, and, for the first time, exceeds human performance of 65.0%. Our method starts by constructing a SOTA model-free baseline, using a novel policy architecture that combines CNNs and RNNs. We then add three improvements to the standard MBRL setup: (a) "Dyna with warmup", which trains the policy on real and imaginary data, (b) "nearest neighbor tokenizer" on image patches, which improves the scheme to create the transformer world model (TWM) inputs, and (c) "block teacher forcing", which allows the TWM to reason jointly about the future tokens of the next timestep.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search</title>
<link>https://arxiv.org/abs/2502.02508</link>
<guid>https://arxiv.org/abs/2502.02508</guid>
<content:encoded><![CDATA[
arXiv:2502.02508v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models are fully open-sourced.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reflection-Window Decoding: Text Generation with Selective Refinement</title>
<link>https://arxiv.org/abs/2502.03678</link>
<guid>https://arxiv.org/abs/2502.03678</guid>
<content:encoded><![CDATA[
arXiv:2502.03678v3 Announce Type: replace-cross 
Abstract: The autoregressive decoding for text generation in large language models (LLMs), while widely used, is inherently suboptimal due to the lack of a built-in mechanism to perform refinement and/or correction of the generated content. In this paper, we consider optimality in terms of the joint probability over the generated response, when jointly considering all tokens at the same time. We theoretically characterize the potential deviation of the autoregressively generated response from its globally optimal counterpart that is of the same length. Our analysis suggests that we need to be cautious when noticeable uncertainty arises during text generation, which may signal the sub-optimality of the generation history. To address the pitfall of autoregressive decoding for text generation, we propose an approach that incorporates a sliding reflection window and a pausing criterion, such that refinement and generation can be carried out interchangeably as the decoding proceeds. Our selective refinement framework strikes a balance between efficiency and optimality, and our extensive experimental results demonstrate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fourier Asymmetric Attention on Domain Generalization for Pan-Cancer Drug Response Prediction</title>
<link>https://arxiv.org/abs/2502.04034</link>
<guid>https://arxiv.org/abs/2502.04034</guid>
<content:encoded><![CDATA[
arXiv:2502.04034v2 Announce Type: replace-cross 
Abstract: The accurate prediction of drug responses remains a formidable challenge, particularly at the single-cell level and in clinical treatment contexts. Some studies employ transfer learning techniques to predict drug responses in individual cells and patients, but they require access to target-domain data during training, which is often unavailable or only obtainable in future. In this study, we propose a novel domain generalization framework, termed FourierDrug, to address this challenge. Given the extracted feature from expression profile, we performed Fourier transforms and then introduced an asymmetric attention constraint that would cluster drug-sensitive samples into a compact group while drives resistant samples dispersed in the frequency domain. Our empirical experiments demonstrate that our model effectively learns task-relevant features from diverse source domains, and achieves accurate predictions of drug response for unseen cancer type. When evaluated on single-cell and patient-level drug response prediction tasks, FourierDrug--trained solely on in vitro cell line data without access to target-domain data--consistently outperforms or, at least, matched the performance of current state-of-the-art methods. These findings underscore the potential of our method for real-world clinical applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KVTuner: Sensitivity-Aware Layer-Wise Mixed-Precision KV Cache Quantization for Efficient and Nearly Lossless LLM Inference</title>
<link>https://arxiv.org/abs/2502.04420</link>
<guid>https://arxiv.org/abs/2502.04420</guid>
<content:encoded><![CDATA[
arXiv:2502.04420v4 Announce Type: replace-cross 
Abstract: KV cache quantization can improve Large Language Models (LLMs) inference throughput and latency in long contexts and large batch-size scenarios while preserving LLMs effectiveness. However, current methods have three unsolved issues: overlooking layer-wise sensitivity to KV cache quantization, high overhead of online fine-grained decision-making, and low flexibility to different LLMs and constraints. Therefore, we theoretically analyze the inherent correlation of layer-wise transformer attention patterns to KV cache quantization errors and study why key cache is generally more important than value cache for quantization error reduction. We further propose a simple yet effective framework KVTuner to adaptively search for the optimal hardware-friendly layer-wise KV quantization precision pairs for coarse-grained KV cache with multi-objective optimization and directly utilize the offline searched configurations during online inference. To reduce the computational cost of offline calibration, we utilize the intra-layer KV precision pair pruning and inter-layer clustering to reduce the search space. Experimental results show that we can achieve nearly lossless 3.25-bit mixed precision KV cache quantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive models like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum inference throughput can be improved by 21.25\% compared with KIVI-KV8 quantization over various context lengths. Our code and searched configurations are available at https://github.com/cmd2001/KVTuner.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safety at Scale: A Comprehensive Survey of Large Model Safety</title>
<link>https://arxiv.org/abs/2502.05206</link>
<guid>https://arxiv.org/abs/2502.05206</guid>
<content:encoded><![CDATA[
arXiv:2502.05206v4 Announce Type: replace-cross 
Abstract: The rapid advancement of large models, driven by their exceptional abilities in learning and generalization through large-scale pre-training, has reshaped the landscape of Artificial Intelligence (AI). These models are now foundational to a wide range of applications, including conversational AI, recommendation systems, autonomous driving, content generation, medical diagnostics, and scientific discovery. However, their widespread deployment also exposes them to significant safety risks, raising concerns about robustness, reliability, and ethical implications. This survey provides a systematic review of current safety research on large models, covering Vision Foundation Models (VFMs), Large Language Models (LLMs), Vision-Language Pre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models (DMs), and large-model-based Agents. Our contributions are summarized as follows: (1) We present a comprehensive taxonomy of safety threats to these models, including adversarial attacks, data poisoning, backdoor attacks, jailbreak and prompt injection attacks, energy-latency attacks, data and model extraction attacks, and emerging agent-specific threats. (2) We review defense strategies proposed for each type of attacks if available and summarize the commonly used datasets and benchmarks for safety research. (3) Building on this, we identify and discuss the open challenges in large model safety, emphasizing the need for comprehensive safety evaluations, scalable and effective defense mechanisms, and sustainable data practices. More importantly, we highlight the necessity of collective efforts from the research community and international collaboration. Our work can serve as a useful reference for researchers and practitioners, fostering the ongoing development of comprehensive defense systems and platforms to safeguard AI models.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iterative Deepening Sampling as Efficient Test-Time Scaling</title>
<link>https://arxiv.org/abs/2502.05449</link>
<guid>https://arxiv.org/abs/2502.05449</guid>
<content:encoded><![CDATA[
arXiv:2502.05449v2 Announce Type: replace-cross 
Abstract: Recent reasoning models, such as OpenAI's O1 series, have demonstrated exceptional performance on complex reasoning tasks and revealed new test-time scaling laws. Inspired by this, many people have been studying how to train models to achieve effective self-evaluation and self-correction to further enable the scaling paradigm. However, less studied is how to efficiently scale test-time compute from a fixed model, and this remains a challenge. In this paper, we address this challenge by focusing on enhancing the quality of self-reflection data generation for complex problem-solving at test time, which can also subsequently improve the training of next-generation large language models (LLMs). Specifically, we explore how systematically triggering a model's self-correction mechanisms can improve performance on challenging reasoning tasks. To this end, we propose a novel iterative deepening sampling algorithm framework designed to enhance self-correction and generate higher-quality samples. Through extensive experiments on Math500 and AIME benchmarks, we demonstrate that our method achieves a higher success rate on difficult tasks and provide detailed ablation studies to analyze its effectiveness across diverse settings.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Sharp Analysis of Offline Policy Learning for $f$-Divergence-Regularized Contextual Bandits</title>
<link>https://arxiv.org/abs/2502.06051</link>
<guid>https://arxiv.org/abs/2502.06051</guid>
<content:encoded><![CDATA[
arXiv:2502.06051v2 Announce Type: replace-cross 
Abstract: Although many popular reinforcement learning algorithms are underpinned by $f$-divergence regularization, their sample complexity with respect to the \emph{regularized objective} still lacks a tight characterization. In this paper, we analyze $f$-divergence-regularized offline policy learning. For reverse Kullback-Leibler (KL) divergence, arguably the most commonly used one, we give the first $\tilde{O}(\epsilon^{-1})$ sample complexity under single-policy concentrability for contextual bandits, surpassing existing $\tilde{O}(\epsilon^{-1})$ bound under all-policy concentrability and $\tilde{O}(\epsilon^{-2})$ bound under single-policy concentrability. Our analysis for general function approximation leverages the principle of pessimism in the face of uncertainty to refine a mean-value-type argument to its extreme. This in turn leads to a novel moment-based technique, effectively bypassing the need for uniform control over the discrepancy between any two functions in the function class. We further propose a lower bound, demonstrating that a multiplicative dependency on single-policy concentrability is necessary to maximally exploit the strong convexity of reverse KL. In addition, for $f$-divergences with strongly convex $f$, to which reverse KL \emph{does not} belong, we show that the sharp sample complexity $\tilde{\Theta}(\epsilon^{-1})$ is achievable even without single-policy concentrability. In this case, the algorithm design can get rid of pessimistic estimators. We further extend our analysis to dueling bandits, and we believe these results take a significant step toward a comprehensive understanding of $f$-divergence-regularized policy learning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What makes a good feedforward computational graph?</title>
<link>https://arxiv.org/abs/2502.06751</link>
<guid>https://arxiv.org/abs/2502.06751</guid>
<content:encoded><![CDATA[
arXiv:2502.06751v2 Announce Type: replace-cross 
Abstract: As implied by the plethora of literature on graph rewiring, the choice of computational graph employed by a neural network can make a significant impact on its downstream performance. Certain effects related to the computational graph, such as under-reaching and over-squashing, may even render the model incapable of learning certain functions. Most of these effects have only been thoroughly studied in the domain of undirected graphs; however, recent years have seen a significant rise in interest in feedforward computational graphs: directed graphs without any back edges. In this paper, we study the desirable properties of a feedforward computational graph, discovering two important complementary measures: fidelity and mixing time, and evaluating a few popular choices of graphs through the lens of these measures. Our study is backed by both theoretical analyses of the metrics' asymptotic behaviour for various graphs, as well as correlating these metrics to the performance of trained neural network models using the corresponding graphs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Survey on Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2502.06851</link>
<guid>https://arxiv.org/abs/2502.06851</guid>
<content:encoded><![CDATA[
arXiv:2502.06851v3 Announce Type: replace-cross 
Abstract: This paper presents an AI-generated review of Vision-Language-Action (VLA) models, summarizing key methodologies, findings, and future directions. The content is produced using large language models (LLMs) and is intended only for demonstration purposes. This work does not represent original research, but highlights how AI can help automate literature reviews. As AI-generated content becomes more prevalent, ensuring accuracy, reliability, and proper synthesis remains a challenge. Future research will focus on developing a structured framework for AI-assisted literature reviews, exploring techniques to enhance citation accuracy, source credibility, and contextual understanding. By examining the potential and limitations of LLM in academic writing, this study aims to contribute to the broader discussion of integrating AI into research workflows. This work serves as a preliminary step toward establishing systematic approaches for leveraging AI in literature review generation, making academic knowledge synthesis more efficient and scalable.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who is Helping Whom? Analyzing Inter-dependencies to Evaluate Cooperation in Human-AI Teaming</title>
<link>https://arxiv.org/abs/2502.06976</link>
<guid>https://arxiv.org/abs/2502.06976</guid>
<content:encoded><![CDATA[
arXiv:2502.06976v2 Announce Type: replace-cross 
Abstract: State-of-the-art methods for Human-AI Teaming and Zero-shot Cooperation focus on task completion, i.e., task rewards, as the sole evaluation metric while being agnostic to how the two agents work with each other. Furthermore, subjective user studies only offer limited insight into the quality of cooperation existing within the team. Specifically, we are interested in understanding the cooperative behaviors arising within the team when trained agents are paired with humans -- a problem that has been overlooked by the existing literature. To formally address this problem, we propose the concept of constructive interdependence -- measuring how much agents rely on each other's actions to achieve the shared goal -- as a key metric for evaluating cooperation in human-agent teams. We interpret interdependence in terms of action interactions in a STRIPS formalism, and define metrics that allow us to assess the degree of reliance between the agents' actions. We pair state-of-the-art agents HAT with learned human models as well as human participants in a user study for the popular Overcooked domain, and evaluate the task reward and teaming performance for these human-agent teams. Our results demonstrate that although trained agents attain high task rewards, they fail to induce cooperative behavior, showing very low levels of interdependence across teams. Furthermore, our analysis reveals that teaming performance is not necessarily correlated with task reward, highlighting that task reward alone cannot reliably measure cooperation arising in a team.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GCoT: Chain-of-Thought Prompt Learning for Graphs</title>
<link>https://arxiv.org/abs/2502.08092</link>
<guid>https://arxiv.org/abs/2502.08092</guid>
<content:encoded><![CDATA[
arXiv:2502.08092v2 Announce Type: replace-cross 
Abstract: Chain-of-thought (CoT) prompting has achieved remarkable success in natural language processing (NLP). However, its vast potential remains largely unexplored for graphs. This raises an interesting question: How can we design CoT prompting for graphs to guide graph models to learn step by step? On one hand, unlike natural languages, graphs are non-linear and characterized by complex topological structures. On the other hand, many graphs lack textual data, making it difficult to formulate language-based CoT prompting. In this work, we propose the first CoT prompt learning framework for text-free graphs, GCoT. Specifically, we decompose the adaptation process for each downstream task into a series of inference steps, with each step consisting of prompt-based inference, ``thought'' generation, and thought-conditioned prompt learning. While the steps mimic CoT prompting in NLP, the exact mechanism differs significantly. Specifically, at each step, an input graph, along with a prompt, is first fed into a pre-trained graph encoder for prompt-based inference. We then aggregate the hidden layers of the encoder to construct a ``thought'', which captures the working state of each node in the current step. Conditioned on this thought, we learn a prompt specific to each node based on the current state. These prompts are fed into the next inference step, repeating the cycle. To evaluate and analyze the effectiveness of GCoT, we conduct comprehensive experiments on eight public datasets, which demonstrate the advantage of our approach.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoToR: Towards More Reliable Responses for Order-Invariant Inputs</title>
<link>https://arxiv.org/abs/2502.08662</link>
<guid>https://arxiv.org/abs/2502.08662</guid>
<content:encoded><![CDATA[
arXiv:2502.08662v3 Announce Type: replace-cross 
Abstract: Mitigating positional bias of language models (LMs) for listwise inputs is a well-known and important problem (e.g., lost-in-the-middle). While zero-shot order-invariant LMs have been proposed to solve this issue, their success on practical listwise problems has been limited. In this work, as a first contribution, we identify and overcome two limitations to make zero-shot invariant LMs more practical: (1) training and inference distribution mismatch arising from modifying positional ID assignments to enforce invariance, and (2) failure to adapt to mixture of order-invariant and sensitive inputs in practical listwise problems. Then, to overcome these issues we propose (1) RoToR, a zero-shot invariant LM for genuinely order-invariant inputs with minimal modifications of positional IDs, and (2) Selective Routing, an adaptive framework that handles both order-invariant and order-sensitive inputs in listwise tasks. On the Lost in the middle (LitM), Knowledge Graph QA (KGQA), and MMLU benchmarks, we show that RoToR with Selective Routing can effectively handle practical listwise input tasks in a zero-shot manner (https://github.com/soyoung97/RoToR)
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2502.08826</link>
<guid>https://arxiv.org/abs/2502.08826</guid>
<content:encoded><![CDATA[
arXiv:2502.08826v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) suffer from hallucinations and outdated knowledge due to their reliance on static training data. Retrieval-Augmented Generation (RAG) mitigates these issues by integrating external dynamic information for improved factual grounding. With advances in multimodal learning, Multimodal RAG extends this approach by incorporating multiple modalities such as text, images, audio, and video to enhance the generated outputs. However, cross-modal alignment and reasoning introduce unique challenges beyond those in unimodal RAG. This survey offers a structured and comprehensive analysis of Multimodal RAG systems, covering datasets, benchmarks, metrics, evaluation, methodologies, and innovations in retrieval, fusion, augmentation, and generation. We review training strategies, robustness enhancements, loss functions, and agent-based approaches, while also exploring the diverse Multimodal RAG scenarios. In addition, we outline open challenges and future directions to guide research in this evolving field. This survey lays the foundation for developing more capable and reliable AI systems that effectively leverage multimodal dynamic external knowledge bases. All resources are publicly available at https://github.com/llm-lab-org/Multimodal-RAG-Survey.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language in the Flow of Time: Time-Series-Paired Texts Weaved into a Unified Temporal Narrative</title>
<link>https://arxiv.org/abs/2502.08942</link>
<guid>https://arxiv.org/abs/2502.08942</guid>
<content:encoded><![CDATA[
arXiv:2502.08942v2 Announce Type: replace-cross 
Abstract: While many advances in time series models focus exclusively on numerical data, research on multimodal time series, particularly those involving contextual textual information commonly encountered in real-world scenarios, remains in its infancy. With recent progress in large language models and time series learning, we revisit the integration of paired texts with time series through the Platonic Representation Hypothesis, which posits that representations of different modalities converge to shared spaces. In this context, we identify that time-series-paired texts may naturally exhibit periodic properties that closely mirror those of the original time series. Building on this insight, we propose a novel framework, Texts as Time Series (TaTS), which considers the time-series-paired texts to be auxiliary variables of the time series. TaTS can be plugged into any existing numerical-only time series models and enable them to handle time series data with paired texts effectively. Through extensive experiments on both multimodal time series forecasting and imputation tasks across benchmark datasets with various existing time series models, we demonstrate that TaTS can enhance predictive performance without modifying model architectures. Code available at https://github.com/iDEA-iSAIL-Lab-UIUC/TaTS.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AnomalyGFM: Graph Foundation Model for Zero/Few-shot Anomaly Detection</title>
<link>https://arxiv.org/abs/2502.09254</link>
<guid>https://arxiv.org/abs/2502.09254</guid>
<content:encoded><![CDATA[
arXiv:2502.09254v2 Announce Type: replace-cross 
Abstract: Graph anomaly detection (GAD) aims to identify abnormal nodes that differ from the majority of the nodes in a graph, which has been attracting significant attention in recent years. Existing generalist graph models have achieved remarkable success in different graph tasks but struggle to generalize to the GAD task. This limitation arises from their difficulty in learning generalized knowledge for capturing the inherently infrequent, irregular and heterogeneous abnormality patterns in graphs from different domains. To address this challenge, we propose AnomalyGFM, a GAD-oriented graph foundation model that supports zero-shot inference and few-shot prompt tuning for GAD in diverse graph datasets. One key insight is that graph-agnostic representations for normal and abnormal classes are required to support effective zero/few-shot GAD across different graphs. Motivated by this, AnomalyGFM is pre-trained to align data-independent, learnable normal and abnormal class prototypes with node representation residuals (i.e., representation deviation of a node from its neighbors). The residual features essentially project the node information into a unified feature space where we can effectively measure the abnormality of nodes from different graphs in a consistent way. This provides a driving force for the learning of graph-agnostic, discriminative prototypes for the normal and abnormal classes, which can be used to enable zero-shot GAD on new graphs, including very large-scale graphs. If there are few-shot labeled normal nodes available in the new graphs, AnomalyGFM can further support prompt tuning to leverage these nodes for better adaptation. Comprehensive experiments on 11 widely-used GAD datasets with real anomalies, demonstrate that AnomalyGFM significantly outperforms state-of-the-art competing methods under both zero- and few-shot GAD settings.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simple Path Structural Encoding for Graph Transformers</title>
<link>https://arxiv.org/abs/2502.09365</link>
<guid>https://arxiv.org/abs/2502.09365</guid>
<content:encoded><![CDATA[
arXiv:2502.09365v2 Announce Type: replace-cross 
Abstract: Graph transformers extend global self-attention to graph-structured data, achieving notable success in graph learning. Recently, random walk structural encoding (RWSE) has been found to further enhance their predictive power by encoding both structural and positional information into the edge representation. However, RWSE cannot always distinguish between edges that belong to different local graph patterns, which reduces its ability to capture the full structural complexity of graphs. This work introduces Simple Path Structural Encoding (SPSE), a novel method that utilizes simple path counts for edge encoding. We show theoretically and experimentally that SPSE overcomes the limitations of RWSE, providing a richer representation of graph structures, particularly for capturing local cyclic patterns. To make SPSE computationally tractable, we propose an efficient approximate algorithm for simple path counting. SPSE demonstrates significant performance improvements over RWSE on various benchmarks, including molecular and long-range graph datasets, achieving statistically significant gains in discriminative tasks. These results pose SPSE as a powerful edge encoding alternative for enhancing the expressivity of graph transformers.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prediction hubs are context-informed frequent tokens in LLMs</title>
<link>https://arxiv.org/abs/2502.10201</link>
<guid>https://arxiv.org/abs/2502.10201</guid>
<content:encoded><![CDATA[
arXiv:2502.10201v2 Announce Type: replace-cross 
Abstract: Hubness, the tendency for a few points to be among the nearest neighbours of a disproportionate number of other points, commonly arises when applying standard distance measures to high-dimensional data, often negatively impacting distance-based analysis. As autoregressive large language models (LLMs) operate on high-dimensional representations, we ask whether they are also affected by hubness. We first prove that the only large-scale representation comparison operation performed by LLMs, namely that between context and unembedding vectors to determine continuation probabilities, is not characterized by the concentration of distances phenomenon that typically causes the appearance of nuisance hubness. We then empirically show that this comparison still leads to a high degree of hubness, but the hubs in this case do not constitute a disturbance. They are rather the result of context-modulated frequent tokens often appearing in the pool of likely candidates for next token prediction. However, when other distances are used to compare LLM representations, we do not have the same theoretical guarantees, and, indeed, we see nuisance hubs appear. There are two main takeaways. First, hubness, while omnipresent in high-dimensional spaces, is not a negative property that needs to be mitigated when LLMs are being used for next token prediction. Second, when comparing representations from LLMs using Euclidean or cosine distance, there is a high risk of nuisance hubs and practitioners should use mitigation techniques if relevant.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bone Soups: A Seek-and-Soup Model Merging Approach for Controllable Multi-Objective Generation</title>
<link>https://arxiv.org/abs/2502.10762</link>
<guid>https://arxiv.org/abs/2502.10762</guid>
<content:encoded><![CDATA[
arXiv:2502.10762v2 Announce Type: replace-cross 
Abstract: User information needs are often highly diverse and varied. A key challenge in current research is how to achieve controllable multi-objective generation while enabling rapid adaptation to accommodate diverse user demands during test time. Existing solutions, such as Rewarded Soup, focus on merging language models individually tuned on single objectives. While easy to implement and widely used, these approaches face limitations in achieving optimal performance due to their disregard for the impacts of competing objectives on model tuning. To address this issue, we propose Bone Soup, a novel model merging approach that first seeks a series of backbone models by considering the impacts of multiple objectives and then makes the soup (i.e., merge the backbone models). Specifically, Bone Soup begins by training multiple backbone models for different objectives using multi-objective reinforcement learning. Each backbone model is guided by a combination of backbone reward signals. To ensure that these models are optimal for the Pareto front, the backbone rewards are crafted by combining standard reward functions into basis vectors, which can then be modified through a rule-based construction method. Bone Soup leverages a symmetric circulant matrix mapping to generate the merging coefficients, which are used to merge the backbone models according to user preferences. Extensive experimental results demonstrate that Bone Soup exhibits strong controllability and Pareto optimality in controllable multi-objective generation, providing a more effective and efficient approach to addressing diverse user needs at test time.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Primus: A Pioneering Collection of Open-Source Datasets for Cybersecurity LLM Training</title>
<link>https://arxiv.org/abs/2502.11191</link>
<guid>https://arxiv.org/abs/2502.11191</guid>
<content:encoded><![CDATA[
arXiv:2502.11191v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have shown remarkable advancements in specialized fields such as finance, law, and medicine. However, in cybersecurity, we have noticed a lack of open-source datasets, with a particular lack of high-quality cybersecurity pretraining corpora, even though much research indicates that LLMs acquire their knowledge during pretraining. To address this, we present a comprehensive suite of datasets covering all major training stages, including pretraining, instruction fine-tuning, and reasoning distillation with cybersecurity-specific self-reflection data. Extensive ablation studies demonstrate their effectiveness on public cybersecurity benchmarks. In particular, continual pre-training on our dataset yields a 15.88% improvement in the aggregate score, while reasoning distillation leads to a 10% gain in security certification (CISSP). We will release all datasets and trained cybersecurity LLMs under the ODC-BY and MIT licenses to encourage further research in the community. For access to all datasets and model weights, please refer to https://huggingface.co/collections/trendmicro-ailab/primus-67b1fd27052b802b4af9d243.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training</title>
<link>https://arxiv.org/abs/2502.11196</link>
<guid>https://arxiv.org/abs/2502.11196</guid>
<content:encoded><![CDATA[
arXiv:2502.11196v2 Announce Type: replace-cross 
Abstract: Despite exceptional capabilities in knowledge-intensive tasks, Large Language Models (LLMs) face a critical gap in understanding how they internalize new knowledge, particularly how to structurally embed acquired knowledge in their neural computations. We address this issue through the lens of knowledge circuit evolution, identifying computational subgraphs that facilitate knowledge storage and processing. Our systematic analysis of circuit evolution throughout continual pre-training reveals several key findings: (1) the acquisition of new knowledge is influenced by its relevance to pre-existing knowledge; (2) the evolution of knowledge circuits exhibits a distinct phase shift from formation to optimization; (3) the evolution of knowledge circuits follows a deep-to-shallow pattern. These insights not only advance our theoretical understanding of the mechanisms of new knowledge acquisition in LLMs, but also provide potential implications for improving continual pre-training strategies to enhance model performance. Code and data will be available at https://github.com/zjunlp/DynamicKnowledgeCircuits.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs can Perform Multi-Dimensional Analytic Writing Assessments: A Case Study of L2 Graduate-Level Academic English Writing</title>
<link>https://arxiv.org/abs/2502.11368</link>
<guid>https://arxiv.org/abs/2502.11368</guid>
<content:encoded><![CDATA[
arXiv:2502.11368v2 Announce Type: replace-cross 
Abstract: The paper explores the performance of LLMs in the context of multi-dimensional analytic writing assessments, i.e. their ability to provide both scores and comments based on multiple assessment criteria. Using a corpus of literature reviews written by L2 graduate students and assessed by human experts against 9 analytic criteria, we prompt several popular LLMs to perform the same task under various conditions. To evaluate the quality of feedback comments, we apply a novel feedback comment quality evaluation framework. This framework is interpretable, cost-efficient, scalable, and reproducible, compared to existing methods that rely on manual judgments. We find that LLMs can generate reasonably good and generally reliable multi-dimensional analytic assessments. We release our corpus and code for reproducibility.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diversity-oriented Data Augmentation with Large Language Models</title>
<link>https://arxiv.org/abs/2502.11671</link>
<guid>https://arxiv.org/abs/2502.11671</guid>
<content:encoded><![CDATA[
arXiv:2502.11671v2 Announce Type: replace-cross 
Abstract: Data augmentation is an essential technique in natural language processing (NLP) for enriching training datasets by generating diverse samples. This process is crucial for improving the robustness and generalization capabilities of NLP models. However, a significant challenge remains: \textit{Insufficient Attention to Sample Distribution Diversity}. Most existing methods focus on increasing the sample numbers while neglecting the sample distribution diversity, which can lead to model overfitting. In response, we explore data augmentation's impact on dataset diversity and propose a \textbf{\underline{D}}iversity-\textbf{\underline{o}}riented data \textbf{\underline{Aug}}mentation framework (\textbf{DoAug}). % \(\mathscr{DoAug}\) Specifically, we utilize a diversity-oriented fine-tuning approach to train an LLM as a diverse paraphraser, which is capable of augmenting textual datasets by generating diversified paraphrases. Then, we apply the LLM paraphraser to a selected coreset of highly informative samples and integrate the paraphrases with the original data to create a more diverse augmented dataset. Finally, we conduct extensive experiments on 12 real-world textual datasets. The results show that our fine-tuned LLM augmenter improves diversity while preserving label consistency, thereby enhancing the robustness and performance of downstream tasks. Specifically, it achieves an average performance gain of \(10.52\%\), surpassing the runner-up baseline with more than three percentage points.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuroStrata: Harnessing Neurosymbolic Paradigms for Improved Design, Testability, and Verifiability of Autonomous CPS</title>
<link>https://arxiv.org/abs/2502.12267</link>
<guid>https://arxiv.org/abs/2502.12267</guid>
<content:encoded><![CDATA[
arXiv:2502.12267v2 Announce Type: replace-cross 
Abstract: Autonomous cyber-physical systems (CPSs) leverage AI for perception, planning, and control but face trust and safety certification challenges due to inherent uncertainties. The neurosymbolic paradigm replaces stochastic layers with interpretable symbolic AI, enabling determinism. While promising, challenges like multisensor fusion, adaptability, and verification remain. This paper introduces NeuroStrata, a neurosymbolic framework to enhance the testing and verification of autonomous CPS. We outline its key components, present early results, and detail future plans.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HumT DumT: Measuring and controlling human-like language in LLMs</title>
<link>https://arxiv.org/abs/2502.13259</link>
<guid>https://arxiv.org/abs/2502.13259</guid>
<content:encoded><![CDATA[
arXiv:2502.13259v2 Announce Type: replace-cross 
Abstract: Should LLMs generate language that makes them seem human? Human-like language might improve user experience, but might also lead to deception, overreliance, and stereotyping. Assessing these potential impacts requires a systematic way to measure human-like tone in LLM outputs. We introduce HumT and SocioT, metrics for human-like tone and other dimensions of social perceptions in text data based on relative probabilities from an LLM. By measuring HumT across preference and usage datasets, we find that users prefer less human-like outputs from LLMs in many contexts. HumT also offers insights into the perceptions and impacts of anthropomorphism: human-like LLM outputs are highly correlated with warmth, social closeness, femininity, and low status, which are closely linked to the aforementioned harms. We introduce DumT, a method using HumT to systematically control and reduce the degree of human-like tone while preserving model performance. DumT offers a practical approach for mitigating risks associated with anthropomorphic language generation.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond One-Size-Fits-All: Tailored Benchmarks for Efficient Evaluation</title>
<link>https://arxiv.org/abs/2502.13576</link>
<guid>https://arxiv.org/abs/2502.13576</guid>
<content:encoded><![CDATA[
arXiv:2502.13576v2 Announce Type: replace-cross 
Abstract: Evaluating models on large benchmarks is very resource-intensive, especially during the period of rapid model evolution. Existing efficient evaluation methods estimate the performance of target models by testing them only on a small and static coreset of the benchmark, which is derived from the publicly available evaluation results of source models. These methods rely on the assumption that target models have high prediction consistency with source models. However, we demonstrate that it doesn't generalize well in practice. To alleviate the inconsistency issue, we present TailoredBench, a method that conducts customized evaluation tailored to each target model. Specifically, a Global-coreset is first constructed as a probe to identify the most consistent source models for each target model with an adaptive source model selection strategy. Afterwards, a scalable K-Medoids clustering algorithm is proposed to extend the Global-coreset to a tailored Native-coreset for each target model. According to the predictions on Native-coresets, we obtain the performance of target models on the whole benchmark with a calibrated estimation strategy. Comprehensive experiments on 5 benchmarks across over 300 models demonstrate that compared to best performing baselines, TailoredBench achieves an average reduction of 31.4% in MAE of accuracy estimates under the same inference budgets, showcasing strong effectiveness and generalizability.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VITAL: A New Dataset for Benchmarking Pluralistic Alignment in Healthcare</title>
<link>https://arxiv.org/abs/2502.13775</link>
<guid>https://arxiv.org/abs/2502.13775</guid>
<content:encoded><![CDATA[
arXiv:2502.13775v2 Announce Type: replace-cross 
Abstract: Alignment techniques have become central to ensuring that Large Language Models (LLMs) generate outputs consistent with human values. However, existing alignment paradigms often model an averaged or monolithic preference, failing to account for the diversity of perspectives across cultures, demographics, and communities. This limitation is particularly critical in health-related scenarios, where plurality is essential due to the influence of culture, religion, personal values, and conflicting opinions. Despite progress in pluralistic alignment, no prior work has focused on health, likely due to the unavailability of publicly available datasets. To address this gap, we introduce VITAL, a new benchmark dataset comprising 13.1K value-laden situations and 5.4K multiple-choice questions focused on health, designed to assess and benchmark pluralistic alignment methodologies. Through extensive evaluation of eight LLMs of varying sizes, we demonstrate that existing pluralistic alignment techniques fall short in effectively accommodating diverse healthcare beliefs, underscoring the need for tailored AI alignment in specific domains. This work highlights the limitations of current approaches and lays the groundwork for developing health-specific alignment solutions.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAG-Gym: Systematic Optimization of Language Agents for Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2502.13957</link>
<guid>https://arxiv.org/abs/2502.13957</guid>
<content:encoded><![CDATA[
arXiv:2502.13957v2 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) has shown great promise for knowledge-intensive tasks and recently advanced with agentic RAG, where language agents engage in multi-round interactions with external knowledge sources for adaptive information retrieval. However, existing agentic RAG methods often depend on ad-hoc prompt engineering and lack a unified optimization framework. We introduce RAG-Gym, a comprehensive platform that systematically explores three optimization dimensions: (1) prompt engineering, (2) actor tuning, and (3) critic training. For prompt engineering, we propose Re$^2$Search, a novel agent incorporating reasoning reflection that significantly outperforms standard prompts. In actor tuning, we evaluate three popular post-training algorithms with fine-grained process supervision and identify direct preference optimization as the most effective. We further demonstrate that a trained critic can enhance inference by selecting higher-quality intermediate reasoning steps. Together, these findings lead to the optimized Re$^2$Search++ agent, which surpasses most recent methods like Search-R1 by a relative increase of 3.2% to 11.6% in average F1. Finally, we examine the impact of different reward sources and analyze scaling properties in training and inference, offering practical insights for agentic RAG optimization. The project homepage is available at https://rag-gym.github.io.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Time Have Its Place? Temporal Heads: Where Language Models Recall Time-specific Information</title>
<link>https://arxiv.org/abs/2502.14258</link>
<guid>https://arxiv.org/abs/2502.14258</guid>
<content:encoded><![CDATA[
arXiv:2502.14258v2 Announce Type: replace-cross 
Abstract: While the ability of language models to elicit facts has been widely investigated, how they handle temporally changing facts remains underexplored. We discover Temporal Heads, specific attention heads that primarily handle temporal knowledge, through circuit analysis. We confirm that these heads are present across multiple models, though their specific locations may vary, and their responses differ depending on the type of knowledge and its corresponding years. Disabling these heads degrades the model's ability to recall time-specific knowledge while maintaining its general capabilities without compromising time-invariant and question-answering performances. Moreover, the heads are activated not only numeric conditions ("In 2004") but also textual aliases ("In the year ..."), indicating that they encode a temporal dimension beyond simple numerical representation. Furthermore, we expand the potential of our findings by demonstrating how temporal knowledge can be edited by adjusting the values of these heads.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEA-HELM: Southeast Asian Holistic Evaluation of Language Models</title>
<link>https://arxiv.org/abs/2502.14301</link>
<guid>https://arxiv.org/abs/2502.14301</guid>
<content:encoded><![CDATA[
arXiv:2502.14301v2 Announce Type: replace-cross 
Abstract: With the rapid emergence of novel capabilities in Large Language Models (LLMs), the need for rigorous multilingual and multicultural benchmarks that are integrated has become more pronounced. Though existing LLM benchmarks are capable of evaluating specific capabilities of LLMs in English as well as in various mid- to low-resource languages, including those in the Southeast Asian (SEA) region, a comprehensive and culturally representative evaluation suite for the SEA languages has not been developed thus far. Here, we present SEA-HELM, a holistic linguistic and cultural LLM evaluation suite that emphasises SEA languages, comprising five core pillars: (1) NLP Classics, (2) LLM-specifics, (3) SEA Linguistics, (4) SEA Culture, (5) Safety. SEA-HELM currently supports Filipino, Indonesian, Tamil, Thai, and Vietnamese. We also introduce the SEA-HELM leaderboard, which allows users to understand models' multilingual and multicultural performance in a systematic and user-friendly manner. We make the SEA-HELM evaluation code publicly available.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Constrained Synthesis of Training Data for De-Identification</title>
<link>https://arxiv.org/abs/2502.14677</link>
<guid>https://arxiv.org/abs/2502.14677</guid>
<content:encoded><![CDATA[
arXiv:2502.14677v3 Announce Type: replace-cross 
Abstract: Many sensitive domains -- such as the clinical domain -- lack widely available datasets due to privacy risks. The increasing generative capabilities of large language models (LLMs) have made synthetic datasets a viable path forward. In this study, we domain-adapt LLMs to the clinical domain and generate synthetic clinical texts that are machine-annotated with tags for personally identifiable information using capable encoder-based NER models. The synthetic corpora are then used to train synthetic NER models. The results show that training NER models using synthetic corpora incurs only a small drop in predictive performance. The limits of this process are investigated in a systematic ablation study -- using both Swedish and Spanish data. Our analysis shows that smaller datasets can be sufficient for domain-adapting LLMs for data synthesis. Instead, the effectiveness of this process is almost entirely contingent on the performance of the machine-annotating NER models trained using the original data.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing PDF Data for Improving Japanese Large Multimodal Models</title>
<link>https://arxiv.org/abs/2502.14778</link>
<guid>https://arxiv.org/abs/2502.14778</guid>
<content:encoded><![CDATA[
arXiv:2502.14778v2 Announce Type: replace-cross 
Abstract: Large Multimodal Models (LMMs) have demonstrated strong performance in English, but their effectiveness in Japanese remains limited due to the lack of high-quality training data. Current Japanese LMMs often rely on translated English datasets, restricting their ability to capture Japan-specific cultural knowledge. To address this, we explore the potential of Japanese PDF data as a training resource, an area that remains largely underutilized. We introduce a fully automated pipeline that leverages pretrained models to extract image-text pairs from PDFs through layout analysis, OCR, and vision-language pairing, removing the need for manual annotation. Additionally, we construct instruction data from extracted image-text pairs to enrich the training data. To evaluate the effectiveness of PDF-derived data, we train Japanese LMMs and assess their performance on the Japanese LMM Benchmark. Our results demonstrate substantial improvements, with performance gains ranging from 2.1% to 13.8% on Heron-Bench. Further analysis highlights the impact of PDF-derived data on various factors, such as model size and language models, reinforcing its value as a multimodal resource for Japanese LMMs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Fixed Variables: Expanding-variate Time Series Forecasting via Flat Scheme and Spatio-temporal Focal Learning</title>
<link>https://arxiv.org/abs/2502.15296</link>
<guid>https://arxiv.org/abs/2502.15296</guid>
<content:encoded><![CDATA[
arXiv:2502.15296v2 Announce Type: replace-cross 
Abstract: Multivariate Time Series Forecasting (MTSF) has long been a key research focus. Traditionally, these studies assume a fixed number of variables, but in real-world applications, Cyber-Physical Systems often expand as new sensors are deployed, increasing variables in MTSF. In light of this, we introduce a novel task, Expanding-variate Time Series Forecasting (EVTSF). This task presents unique challenges, specifically (1) handling inconsistent data shapes caused by adding new variables, and (2) addressing imbalanced spatio-temporal learning, where expanding variables have limited observed data due to the necessity for timely operation. To address these challenges, we propose STEV, a flexible spatio-temporal forecasting framework. STEV includes a new Flat Scheme to tackle the inconsistent data shape issue, which extends the graph-based spatio-temporal modeling architecture into 1D space by flattening the 2D samples along the variable dimension, making the model variable-scale-agnostic while still preserving dynamic spatial correlations through a holistic graph. We introduce a novel Spatio-temporal Focal Learning strategy that incorporates a negative filter to resolve potential conflicts between contrastive learning and graph representation, and a focal contrastive loss as its core to guide the framework to focus on optimizing the expanding variables. We benchmark EVTSF performance using three real-world datasets and compare it against three potential solutions employing SOTA MTSF models tailored for EVSTF. Experimental results show that STEV significantly outperforms its competitors, particularly on expanding variables. Notably, STEV, with only 5% of observations from the expanding period, is on par with SOTA MTSF models trained with complete observations. Further exploration of various expanding strategies underscores the generalizability of STEV in real-world applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R-LoRA: Randomized Multi-Head LoRA for Efficient Multi-Task Learning</title>
<link>https://arxiv.org/abs/2502.15455</link>
<guid>https://arxiv.org/abs/2502.15455</guid>
<content:encoded><![CDATA[
arXiv:2502.15455v2 Announce Type: replace-cross 
Abstract: Fine-tuning large language models (LLMs) is computationally expensive, and Low-Rank Adaptation (LoRA) provides a cost-effective solution by approximating weight updates through low-rank matrices. In real-world scenarios, LLMs are fine-tuned on data from multiple domains to perform tasks across various fields, embodying multi-task learning (MTL). LoRA often underperforms in such complex scenarios. To enhance LoRA's capability in multi-task learning, we propose R-LoRA, which incorporates Multi-Head Randomization. Multi-Head Randomization diversifies the head matrices through Multi-Head Dropout and Multi-Head Random Initialization, enabling more efficient learning of task-specific features while maintaining shared knowledge representation. Our approach not only improves performance in MTL but also reduces GPU memory usage and training time. Experiments show that R-LoRA's gains stem from increased diversity in the head matrices, demonstrating its effectiveness for multi-task learning. The code is available at https://github.com/jinda-liu/R-LoRA
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaxSup: Overcoming Representation Collapse in Label Smoothing</title>
<link>https://arxiv.org/abs/2502.15798</link>
<guid>https://arxiv.org/abs/2502.15798</guid>
<content:encoded><![CDATA[
arXiv:2502.15798v2 Announce Type: replace-cross 
Abstract: Label Smoothing (LS) is widely adopted to reduce overconfidence in neural network predictions and improve generalization. Despite these benefits, recent studies reveal two critical issues with LS. First, LS induces overconfidence in misclassified samples. Second, it compacts feature representations into overly tight clusters, diluting intra-class diversity, although the precise cause of this phenomenon remained elusive. In this paper, we analytically decompose the LS-induced loss, exposing two key terms: (i) a regularization term that dampens overconfidence only when the prediction is correct, and (ii) an error-amplification term that arises under misclassifications. This latter term compels the network to reinforce incorrect predictions with undue certainty, exacerbating representation collapse. To address these shortcomings, we propose Max Suppression (MaxSup), which applies uniform regularization to both correct and incorrect predictions by penalizing the top-1 logit rather than the ground-truth logit. Through extensive feature-space analyses, we show that MaxSup restores intra-class variation and sharpens inter-class boundaries. Experiments on large-scale image classification and multiple downstream tasks confirm that MaxSup is a more robust alternative to LS, consistently reducing overconfidence while preserving richer feature representations. Code is available at: https://github.com/ZhouYuxuanYX/Maximum-Suppression-Regularization
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Standard Benchmarks Fail - Auditing LLM Agents in Finance Must Prioritize Risk</title>
<link>https://arxiv.org/abs/2502.15865</link>
<guid>https://arxiv.org/abs/2502.15865</guid>
<content:encoded><![CDATA[
arXiv:2502.15865v2 Announce Type: replace-cross 
Abstract: Standard benchmarks fixate on how well large language model (LLM) agents perform in finance, yet say little about whether they are safe to deploy. We argue that accuracy metrics and return-based scores provide an illusion of reliability, overlooking vulnerabilities such as hallucinated facts, stale data, and adversarial prompt manipulation. We take a firm position: financial LLM agents should be evaluated first and foremost on their risk profile, not on their point-estimate performance. Drawing on risk-engineering principles, we outline a three-level agenda: model, workflow, and system, for stress-testing LLM agents under realistic failure modes. To illustrate why this shift is urgent, we audit six API-based and open-weights LLM agents on three high-impact tasks and uncover hidden weaknesses that conventional benchmarks miss. We conclude with actionable recommendations for researchers, practitioners, and regulators: audit risk-aware metrics in future studies, publish stress scenarios alongside datasets, and treat ``safety budget'' as a primary success criterion. Only by redefining what ``good'' looks like can the community responsibly advance AI-driven finance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Erwin: A Tree-based Hierarchical Transformer for Large-scale Physical Systems</title>
<link>https://arxiv.org/abs/2502.17019</link>
<guid>https://arxiv.org/abs/2502.17019</guid>
<content:encoded><![CDATA[
arXiv:2502.17019v2 Announce Type: replace-cross 
Abstract: Large-scale physical systems defined on irregular grids pose significant scalability challenges for deep learning methods, especially in the presence of long-range interactions and multi-scale coupling. Traditional approaches that compute all pairwise interactions, such as attention, become computationally prohibitive as they scale quadratically with the number of nodes. We present Erwin, a hierarchical transformer inspired by methods from computational many-body physics, which combines the efficiency of tree-based algorithms with the expressivity of attention mechanisms. Erwin employs ball tree partitioning to organize computation, which enables linear-time attention by processing nodes in parallel within local neighborhoods of fixed size. Through progressive coarsening and refinement of the ball tree structure, complemented by a novel cross-ball interaction mechanism, it captures both fine-grained local details and global features. We demonstrate Erwin's effectiveness across multiple domains, including cosmology, molecular dynamics, PDE solving, and particle fluid dynamics, where it consistently outperforms baseline methods both in accuracy and computational efficiency.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DIS-CO: Discovering Copyrighted Content in VLMs Training Data</title>
<link>https://arxiv.org/abs/2502.17358</link>
<guid>https://arxiv.org/abs/2502.17358</guid>
<content:encoded><![CDATA[
arXiv:2502.17358v3 Announce Type: replace-cross 
Abstract: How can we verify whether copyrighted content was used to train a large vision-language model (VLM) without direct access to its training data? Motivated by the hypothesis that a VLM is able to recognize images from its training corpus, we propose DIS-CO, a novel approach to infer the inclusion of copyrighted content during the model's development. By repeatedly querying a VLM with specific frames from targeted copyrighted material, DIS-CO extracts the content's identity through free-form text completions. To assess its effectiveness, we introduce MovieTection, a benchmark comprising 14,000 frames paired with detailed captions, drawn from films released both before and after a model's training cutoff. Our results show that DIS-CO significantly improves detection performance, nearly doubling the average AUC of the best prior method on models with logits available. Our findings also highlight a broader concern: all tested models appear to have been exposed to some extent to copyrighted content. Our code and data are available at https://github.com/avduarte333/DIS-CO
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpargeAttention: Accurate and Training-free Sparse Attention Accelerating Any Model Inference</title>
<link>https://arxiv.org/abs/2502.18137</link>
<guid>https://arxiv.org/abs/2502.18137</guid>
<content:encoded><![CDATA[
arXiv:2502.18137v3 Announce Type: replace-cross 
Abstract: An efficient attention implementation is essential for large models due to its quadratic time complexity. Fortunately, attention commonly exhibits sparsity, i.e., many values in the attention map are near zero, allowing for the omission of corresponding computations. Many studies have utilized the sparse pattern to accelerate attention. However, most existing works focus on optimizing attention within specific models by exploiting certain sparse patterns of the attention map. A universal sparse attention that guarantees both the speedup and end-to-end performance of diverse models remains elusive. In this paper, we propose SpargeAttn, a universal sparse and quantized attention for any model. Our method uses a two-stage online filter: in the first stage, we rapidly and accurately predict the attention map, enabling the skip of some matrix multiplications in attention. In the second stage, we design an online softmax-aware filter that incurs no extra overhead and further skips some matrix multiplications. Experiments show that our method significantly accelerates diverse models, including language, image, and video generation, without sacrificing end-to-end metrics. The codes are available at https://github.com/thu-ml/SpargeAttn.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TestNUC: Enhancing Test-Time Computing Approaches and Scaling through Neighboring Unlabeled Data Consistency</title>
<link>https://arxiv.org/abs/2502.19163</link>
<guid>https://arxiv.org/abs/2502.19163</guid>
<content:encoded><![CDATA[
arXiv:2502.19163v2 Announce Type: replace-cross 
Abstract: Test-time computing approaches, which leverage additional computational resources during inference, have been proven effective in enhancing large language model performance. This work introduces a novel, linearly scaling approach, TestNUC, that improves test-time predictions by leveraging the local consistency of neighboring unlabeled data-it classifies an input instance by considering not only the model's prediction on that instance but also on neighboring unlabeled instances. We evaluate TestNUC across eight diverse datasets, spanning intent classification, topic mining, domain discovery, and emotion detection, demonstrating its consistent superiority over baseline methods such as standard prompting and self-consistency. Furthermore, TestNUC can be seamlessly integrated with existing test-time computing approaches, substantially boosting their performance. Our analysis reveals that TestNUC scales effectively with increasing amounts of unlabeled data and performs robustly across different embedding models, making it practical for real-world applications. Our code is available at https://github.com/HenryPengZou/TestNUC.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Faithful Logic Embeddings in HOL -- Deep and Shallow</title>
<link>https://arxiv.org/abs/2502.19311</link>
<guid>https://arxiv.org/abs/2502.19311</guid>
<content:encoded><![CDATA[
arXiv:2502.19311v3 Announce Type: replace-cross 
Abstract: Deep and shallow embeddings of non-classical logics in classical higher-order logic have been explored, implemented, and used in various reasoning tools in recent years. This paper presents a method for the simultaneous deployment of deep and shallow embeddings of various degrees in classical higher-order logic. This enables flexible, interactive and automated theorem proving and counterexample finding at meta and object level, as well as automated faithfulness proofs between these logic embeddings. The method is beneficial for logic education, research and application and is illustrated here using a simple propositional modal logic. However, this approach is conceptual in nature and not limited to this simple logic context.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture of Structural-and-Textual Retrieval over Text-rich Graph Knowledge Bases</title>
<link>https://arxiv.org/abs/2502.20317</link>
<guid>https://arxiv.org/abs/2502.20317</guid>
<content:encoded><![CDATA[
arXiv:2502.20317v4 Announce Type: replace-cross 
Abstract: Text-rich Graph Knowledge Bases (TG-KBs) have become increasingly crucial for answering queries by providing textual and structural knowledge. However, current retrieval methods often retrieve these two types of knowledge in isolation without considering their mutual reinforcement and some hybrid methods even bypass structural retrieval entirely after neighboring aggregation. To fill in this gap, we propose a Mixture of Structural-and-Textual Retrieval (MoR) to retrieve these two types of knowledge via a Planning-Reasoning-Organizing framework. In the Planning stage, MoR generates textual planning graphs delineating the logic for answering queries. Following planning graphs, in the Reasoning stage, MoR interweaves structural traversal and textual matching to obtain candidates from TG-KBs. In the Organizing stage, MoR further reranks fetched candidates based on their structural trajectory. Extensive experiments demonstrate the superiority of MoR in harmonizing structural and textual retrieval with insights, including uneven retrieving performance across different query logics and the benefits of integrating structural trajectories for candidate reranking. Our code is available at https://github.com/Yoega/MoR.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPD: Sync-Point Drop for Efficient Tensor Parallelism of Large Language Models</title>
<link>https://arxiv.org/abs/2502.20727</link>
<guid>https://arxiv.org/abs/2502.20727</guid>
<content:encoded><![CDATA[
arXiv:2502.20727v4 Announce Type: replace-cross 
Abstract: With the rapid expansion in the scale of large language models (LLMs), enabling efficient distributed inference across multiple computing units has become increasingly critical. However, communication overheads from popular distributed inference techniques such as Tensor Parallelism pose a significant challenge to achieve scalability and low latency. Therefore, we introduce a novel optimization technique, Sync-Point Drop (SPD), to reduce communication overheads in tensor parallelism by selectively dropping synchronization on attention outputs. In detail, we first propose a block design that allows execution to proceed without communication through SPD. Second, we apply different SPD strategies to attention blocks based on their sensitivity to the model accuracy. The proposed methods effectively alleviate communication bottlenecks while minimizing accuracy degradation during LLM inference, offering a scalable solution for diverse distributed environments: SPD offered about 20% overall inference latency reduction with < 1% accuracy regression for LLaMA2-70B inference over 8 GPUs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying First-Order Markov Violations in Noisy Reinforcement Learning: A Causal Discovery Approach</title>
<link>https://arxiv.org/abs/2503.00206</link>
<guid>https://arxiv.org/abs/2503.00206</guid>
<content:encoded><![CDATA[
arXiv:2503.00206v2 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) methods frequently assume that each new observation completely reflects the environment's state, thereby guaranteeing Markovian (one-step) transitions. In practice, partial observability or sensor/actuator noise often invalidates this assumption. This paper proposes a systematic methodology for detecting such violations, combining a partial correlation-based causal discovery process (PCMCI) with a novel Markov Violation score (MVS). The MVS measures multi-step dependencies that emerge when noise or incomplete state information disrupts the Markov property.
  Classic control tasks (CartPole, Pendulum, Acrobot) serve as examples to illustrate how targeted noise and dimension omissions affect both RL performance and measured Markov consistency. Surprisingly, even substantial observation noise sometimes fails to induce strong multi-lag dependencies in certain domains (e.g., Acrobot). In contrast, dimension-dropping investigations show that excluding some state variables (e.g., angular velocities in CartPole and Pendulum) significantly reduces returns and increases MVS, while removing other dimensions has minimal impact.
  These findings emphasize the importance of locating and safeguarding the most causally essential dimensions in order to preserve effective single-step learning. By integrating partial correlation tests with RL performance outcomes, the proposed approach precisely identifies when and where the Markov assumption is violated. This framework offers a principled mechanism for developing robust policies, informing representation learning, and addressing partial observability in real-world RL scenarios. All code and experimental logs are accessible for reproducibility (https://github.com/ucsb/markovianess).
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Integrity Constraints: Declarative Guardrails for AI-Augmented Data Processing Systems</title>
<link>https://arxiv.org/abs/2503.00600</link>
<guid>https://arxiv.org/abs/2503.00600</guid>
<content:encoded><![CDATA[
arXiv:2503.00600v2 Announce Type: replace-cross 
Abstract: AI-augmented data processing systems (DPSs) integrate large language models (LLMs) into query pipelines, allowing powerful semantic operations on structured and unstructured data. However, the reliability (a.k.a. trust) of these systems is fundamentally challenged by the potential for LLMs to produce errors, limiting their adoption in critical domains. To help address this reliability bottleneck, we introduce semantic integrity constraints (SICs) -- a declarative abstraction for specifying and enforcing correctness conditions over LLM outputs in semantic queries. SICs generalize traditional database integrity constraints to semantic settings, supporting common types of constraints, such as grounding, soundness, and exclusion, with both proactive and reactive enforcement strategies.
  We argue that SICs provide a foundation for building reliable and auditable AI-augmented data systems. Specifically, we present a system design for integrating SICs into query planning and runtime execution and discuss its realization in AI-augmented DPSs. To guide and evaluate the vision, we outline several design goals -- covering criteria around expressiveness, runtime semantics, integration, performance, and enterprise-scale applicability -- and discuss how our framework addresses each, along with open research challenges.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>POPGym Arcade: Parallel Pixelated POMDPs</title>
<link>https://arxiv.org/abs/2503.01450</link>
<guid>https://arxiv.org/abs/2503.01450</guid>
<content:encoded><![CDATA[
arXiv:2503.01450v4 Announce Type: replace-cross 
Abstract: We present the POPGym Arcade, a collection of hardware-accelerated, pixel-based environments with shared observation and action spaces. Each environment includes fully and partially observable variants, enabling counterfactual studies on partial observability. We also introduce mathematical tools for analyzing policies under partial observability, which reveal how agents recall past information to make decisions. Our analysis shows (1) that controlling for partial observability is critical and (2) that agents with long-term memory learn brittle policies that struggle to generalize. Finally, we demonstrate that recurrent policies can be "poisoned" by old, out-of-distribution observations, with implications for sim-to-real transfer, imitation learning, and offline reinforcement learning.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Marco-o1 v2: Towards Widening The Distillation Bottleneck for Reasoning Models</title>
<link>https://arxiv.org/abs/2503.01461</link>
<guid>https://arxiv.org/abs/2503.01461</guid>
<content:encoded><![CDATA[
arXiv:2503.01461v2 Announce Type: replace-cross 
Abstract: Large Reasoning Models(LRMs) such as OpenAI o1 and DeepSeek-R1 have shown remarkable reasoning capabilities by scaling test-time compute and generating long Chain-of-Thought(CoT). Distillation--post-training on LRMs-generated data--is a straightforward yet effective method to enhance the reasoning abilities of smaller models, but faces a critical bottleneck: we found that distilled long CoT data poses learning difficulty for small models and leads to the inheritance of biases (i.e. over-thinking) when using Supervised Fine-tuning (SFT) and Reinforcement Learning (RL) methods. To alleviate this bottleneck, we propose constructing tree-based CoT data from scratch via Monte Carlo Tree Search(MCTS). We then exploit a set of CoT-aware approaches, including Thoughts Length Balance, Fine-grained DPO, and Joint Post-training Objective, to enhance SFT and RL on the constructed data. We conduct evaluation on various benchmarks such as math (GSM8K, MATH, AIME). instruction-following (Multi-IF) and planning (Blocksworld), results demonstrate our approaches substantially improve the reasoning performance of distilled models compared to standard distilled models via reducing the hallucinations in long-time thinking. The project homepage is https://github.com/AIDC-AI/Marco-o1.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models</title>
<link>https://arxiv.org/abs/2503.01854</link>
<guid>https://arxiv.org/abs/2503.01854</guid>
<content:encoded><![CDATA[
arXiv:2503.01854v2 Announce Type: replace-cross 
Abstract: This study investigates the machine unlearning techniques within the context of large language models (LLMs), referred to as \textit{LLM unlearning}. LLM unlearning offers a principled approach to removing the influence of undesirable data (e.g., sensitive or illegal information) from LLMs, while preserving their overall utility without requiring full retraining. Despite growing research interest, there is no comprehensive survey that systematically organizes existing work and distills key insights; here, we aim to bridge this gap. We begin by introducing the definition and the paradigms of LLM unlearning, followed by a comprehensive taxonomy of existing unlearning studies. Next, we categorize current unlearning approaches, summarizing their strengths and limitations. Additionally, we review evaluation metrics and benchmarks, providing a structured overview of current assessment methodologies. Finally, we outline promising directions for future research, highlighting key challenges and opportunities in the field.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Constrained Learning</title>
<link>https://arxiv.org/abs/2503.03480</link>
<guid>https://arxiv.org/abs/2503.03480</guid>
<content:encoded><![CDATA[
arXiv:2503.03480v2 Announce Type: replace-cross 
Abstract: Vision-language-action models (VLAs) show potential as generalist robot policies. However, these models pose extreme safety challenges during real-world deployment, including the risk of harm to the environment, the robot itself, and humans. How can safety constraints be explicitly integrated into VLAs? We address this by exploring an integrated safety approach (ISA), systematically modeling safety requirements, then actively eliciting diverse unsafe behaviors, effectively constraining VLA policies via safe reinforcement learning, and rigorously assuring their safety through targeted evaluations. Leveraging the constrained Markov decision process (CMDP) paradigm, ISA optimizes VLAs from a min-max perspective against elicited safety risks. Thus, policies aligned through this comprehensive approach achieve the following key features: (I) effective safety-performance trade-offs, this exploration yields an 83.58% safety improvement compared to the current state-of-the-art method, while also maintaining task performance (+3.85%). (II) strong safety assurance, with the ability to mitigate long-tail risks and handle extreme failure scenarios. (III) robust generalization of learned safety behaviors to various out-of-distribution perturbations. Our data, models and newly proposed benchmark environment are available at https://pku-safevla.github.io.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causally Reliable Concept Bottleneck Models</title>
<link>https://arxiv.org/abs/2503.04363</link>
<guid>https://arxiv.org/abs/2503.04363</guid>
<content:encoded><![CDATA[
arXiv:2503.04363v2 Announce Type: replace-cross 
Abstract: Concept-based models are an emerging paradigm in deep learning that constrains the inference process to operate through human-interpretable variables, facilitating explainability and human interaction. However, these architectures, on par with popular opaque neural models, fail to account for the true causal mechanisms underlying the target phenomena represented in the data. This hampers their ability to support causal reasoning tasks, limits out-of-distribution generalization, and hinders the implementation of fairness constraints. To overcome these issues, we propose Causally reliable Concept Bottleneck Models (C$^2$BMs), a class of concept-based architectures that enforce reasoning through a bottleneck of concepts structured according to a model of the real-world causal mechanisms. We also introduce a pipeline to automatically learn this structure from observational data and unstructured background knowledge (e.g., scientific literature). Experimental evidence suggests that C$^2$BMs are more interpretable, causally reliable, and improve responsiveness to interventions w.r.t. standard opaque and concept-based models, while maintaining their accuracy.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Multi-Hop Document Retrieval Through Intermediate Representations</title>
<link>https://arxiv.org/abs/2503.04796</link>
<guid>https://arxiv.org/abs/2503.04796</guid>
<content:encoded><![CDATA[
arXiv:2503.04796v2 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) encounters challenges when addressing complex queries, particularly multi-hop questions. While several methods tackle multi-hop queries by iteratively generating internal queries and retrieving external documents, these approaches are computationally expensive. In this paper, we identify a three-stage information processing pattern in LLMs during layer-by-layer reasoning, consisting of extraction, processing, and subsequent extraction steps. This observation suggests that the representations in intermediate layers contain richer information compared to those in other layers. Building on this insight, we propose Layer-wise RAG (L-RAG). Unlike prior methods that focus on generating new internal queries, L-RAG leverages intermediate representations from the middle layers, which capture next-hop information, to retrieve external knowledge. L-RAG achieves performance comparable to multi-step approaches while maintaining inference overhead similar to that of standard RAG. Experimental results show that L-RAG outperforms existing RAG methods on open-domain multi-hop question-answering datasets, including MuSiQue, HotpotQA, and 2WikiMultiHopQA. The code is available in https://anonymous.4open.science/r/L-RAG-ADD5/
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HoH: A Dynamic Benchmark for Evaluating the Impact of Outdated Information on Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2503.04800</link>
<guid>https://arxiv.org/abs/2503.04800</guid>
<content:encoded><![CDATA[
arXiv:2503.04800v2 Announce Type: replace-cross 
Abstract: While Retrieval-Augmented Generation (RAG) has emerged as an effective approach for addressing the knowledge outdating problem in Large Language Models (LLMs), it still faces a critical challenge: the prevalence of outdated information in knowledge bases. Current research primarily focuses on incorporating up-to-date information, yet the impact of outdated information coexisting in retrieval sources remains inadequately addressed. To bridge this gap, we introduce HoH, the first benchmark specifically designed to evaluate the impact of outdated information on RAG. Our benchmark leverages token-level diff algorithms combined with LLM pipelines to efficiently create a large-scale QA dataset that accurately captures the evolution of temporal knowledge in real-world facts. Through comprehensive experiments, we reveal that outdated information significantly degrades RAG performance in two critical ways: (1) it substantially reduces response accuracy by distracting models from correct information, and (2) it can mislead models into generating potentially harmful outputs, even when current information is available. Current RAG approaches struggle with both retrieval and generation aspects when handling outdated information. These findings highlight the urgent need for innovative solutions to address the temporal challenges in RAG. Our code and data are available at: https://github.com/0russwest0/HoH.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wanda++: Pruning Large Language Models via Regional Gradients</title>
<link>https://arxiv.org/abs/2503.04992</link>
<guid>https://arxiv.org/abs/2503.04992</guid>
<content:encoded><![CDATA[
arXiv:2503.04992v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) pruning seeks to remove unimportant weights for inference speedup with minimal accuracy impact. However, existing methods often suffer from accuracy degradation without full-model sparsity-aware fine-tuning. This paper presents Wanda++, a novel pruning framework that outperforms the state-of-the-art methods by utilizing decoder-block-level \textbf{regional} gradients. Specifically, Wanda++ improves the pruning score with regional gradients for the first time and proposes an efficient regional optimization method to minimize pruning-induced output discrepancies between the dense and sparse decoder output. Notably, Wanda++ improves perplexity by up to 32\% over Wanda in the language modeling task and generalizes effectively to downstream tasks. Moreover, despite updating weights with regional optimization, Wanda++ remains orthogonal to sparsity-aware fine-tuning, further reducing perplexity with LoRA in great extend. Our approach is lightweight, pruning a 7B LLaMA model in under 10 minutes on a single H100 GPU.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CSTRL: Context-Driven Sequential Transfer Learning for Abstractive Radiology Report Summarization</title>
<link>https://arxiv.org/abs/2503.05750</link>
<guid>https://arxiv.org/abs/2503.05750</guid>
<content:encoded><![CDATA[
arXiv:2503.05750v2 Announce Type: replace-cross 
Abstract: A radiology report comprises several sections, including the Findings and Impression of the diagnosis. Automatically generating the Impression from the Findings is crucial for reducing radiologists' workload and improving diagnostic accuracy. Pretrained models that excel in common abstractive summarization problems encounter challenges when applied to specialized medical domains largely due to the complex terminology and the necessity for accurate clinical context. Such tasks in medical domains demand extracting core information, avoiding context shifts, and maintaining proper flow. Misuse of medical terms can lead to drastic clinical errors. To address these issues, we introduce a sequential transfer learning that ensures key content extraction and coherent summarization. Sequential transfer learning often faces challenges like initial parameter decay and knowledge loss, which we resolve with the Fisher matrix regularization. Using MIMIC-CXR and Open-I datasets, our model, CSTRL - Context-driven Sequential TRansfer Learning - achieved state-of-the-art performance, showing 56.2% improvement in BLEU-1, 40.5% in BLEU-2, 84.3% in BLEU-3, 28.9% in ROUGE-1, 41.0% in ROUGE-2 and 26.5% in ROGUE-3 score over benchmark studies. We also analyze factual consistency scores while preserving the medical context. Our code is publicly available at https://github.com/fahmidahossain/Report_Summarization.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GMLM: Bridging Graph Neural Networks and Language Models for Heterophilic Node Classification</title>
<link>https://arxiv.org/abs/2503.05763</link>
<guid>https://arxiv.org/abs/2503.05763</guid>
<content:encoded><![CDATA[
arXiv:2503.05763v3 Announce Type: replace-cross 
Abstract: Integrating structured graph data with rich textual information from nodes poses a significant challenge, particularly for heterophilic node classification. Current approaches often struggle with computational costs or effective fusion of disparate modalities. We propose \textbf{Graph Masked Language Model (GMLM)}, a novel architecture efficiently combining Graph Neural Networks (GNNs) with Pre-trained Language Models (PLMs). GMLM introduces three key innovations: (i) a \textbf{dynamic active node selection} strategy for scalable PLM text processing; (ii) a GNN-specific \textbf{contrastive pretraining stage} using soft masking with a learnable graph \texttt{[MASK]} token for robust structural representations; and (iii) a \textbf{dedicated fusion module} integrating RGCN-based GNN embeddings with PLM (GTE-Small \& DistilBERT) embeddings. Extensive experiments on heterophilic benchmarks (Cornell, Wisconsin, Texas) demonstrate GMLM's superiority. Notably, GMLM(DistilBERT) achieves significant performance gains, improving accuracy by over \textbf{4.7\%} on Cornell and over \textbf{2.0\%} on Texas compared to the previous best-performing baselines. This work underscores the benefits of targeted PLM engagement and modality-specific pretraining for improved, efficient learning on text-rich graphs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NFIG: Autoregressive Image Generation with Next-Frequency Prediction</title>
<link>https://arxiv.org/abs/2503.07076</link>
<guid>https://arxiv.org/abs/2503.07076</guid>
<content:encoded><![CDATA[
arXiv:2503.07076v3 Announce Type: replace-cross 
Abstract: Autoregressive models have achieved promising results in natural language processing. However, for image generation tasks, they encounter substantial challenges in effectively capturing long-range dependencies, managing computational costs, and most crucially, defining meaningful autoregressive sequences that reflect natural image hierarchies. To address these issues, we present \textbf{N}ext-\textbf{F}requency \textbf{I}mage \textbf{G}eneration (\textbf{NFIG}), a novel framework that decomposes the image generation process into multiple frequency-guided stages. Our approach first generates low-frequency components to establish global structure with fewer tokens, then progressively adds higher-frequency details, following the natural spectral hierarchy of images. This principled autoregressive sequence not only improves the quality of generated images by better capturing true causal relationships between image components, but also significantly reduces computational overhead during inference. Extensive experiments demonstrate that NFIG achieves state-of-the-art performance with fewer steps, offering a more efficient solution for image generation, with 1.25$\times$ speedup compared to VAR-d20 while achieving better performance (FID: 2.81) on the ImageNet-256 benchmark. We hope that our insight of incorporating frequency-domain knowledge to guide autoregressive sequence design will shed light on future research. We will make our code publicly available upon acceptance of the paper.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FMNet: Frequency-Assisted Mamba-Like Linear Attention Network for Camouflaged Object Detection</title>
<link>https://arxiv.org/abs/2503.11030</link>
<guid>https://arxiv.org/abs/2503.11030</guid>
<content:encoded><![CDATA[
arXiv:2503.11030v2 Announce Type: replace-cross 
Abstract: Camouflaged Object Detection (COD) is challenging due to the strong similarity between camouflaged objects and their surroundings, which complicates identification. Existing methods mainly rely on spatial local features, failing to capture global information, while Transformers increase computational costs. To address this, the Frequency-Assisted Mamba-Like Linear Attention Network (FMNet) is proposed, which leverages frequency-domain learning to efficiently capture global features and mitigate ambiguity between objects and the background. FMNet introduces the Multi-Scale Frequency-Assisted Mamba-Like Linear Attention (MFM) module, integrating frequency and spatial features through a multi-scale structure to handle scale variations while reducing computational complexity. Additionally, the Pyramidal Frequency Attention Extraction (PFAE) module and the Frequency Reverse Decoder (FRD) enhance semantics and reconstruct features. Experimental results demonstrate that FMNet outperforms existing methods on multiple COD datasets, showcasing its advantages in both performance and efficiency. Code available at https://github.com/Chranos/FMNet.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ClusComp: A Simple Paradigm for Model Compression and Efficient Finetuning</title>
<link>https://arxiv.org/abs/2503.13089</link>
<guid>https://arxiv.org/abs/2503.13089</guid>
<content:encoded><![CDATA[
arXiv:2503.13089v2 Announce Type: replace-cross 
Abstract: As large language models (LLMs) scale, model compression is crucial for edge deployment and accessibility. Weight-only quantization reduces model size but suffers from performance degradation at lower bit widths. Moreover, standard finetuning is incompatible with quantized models, and alternative methods often fall short of full finetuning. In this paper, we propose ClusComp, a simple yet effective compression paradigm that clusters weight matrices into codebooks and finetunes them block-by-block. ClusComp (1) achieves superior performance in 2-4 bit quantization, (2) pushes compression to 1-bit while outperforming ultra-low-bit methods with minimal finetuning, and (3) enables efficient finetuning, even surpassing existing quantization-based approaches and rivaling full FP16 finetuning. Notably, ClusComp supports compression and finetuning of 70B LLMs on a single A6000-48GB GPU.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MentalChat16K: A Benchmark Dataset for Conversational Mental Health Assistance</title>
<link>https://arxiv.org/abs/2503.13509</link>
<guid>https://arxiv.org/abs/2503.13509</guid>
<content:encoded><![CDATA[
arXiv:2503.13509v2 Announce Type: replace-cross 
Abstract: We introduce MentalChat16K, an English benchmark dataset combining a synthetic mental health counseling dataset and a dataset of anonymized transcripts from interventions between Behavioral Health Coaches and Caregivers of patients in palliative or hospice care. Covering a diverse range of conditions like depression, anxiety, and grief, this curated dataset is designed to facilitate the development and evaluation of large language models for conversational mental health assistance. By providing a high-quality resource tailored to this critical domain, MentalChat16K aims to advance research on empathetic, personalized AI solutions to improve access to mental health support services. The dataset prioritizes patient privacy, ethical considerations, and responsible data usage. MentalChat16K presents a valuable opportunity for the research community to innovate AI technologies that can positively impact mental well-being. The dataset is available at https://huggingface.co/datasets/ShenLab/MentalChat16K and the code and documentation are hosted on GitHub at https://github.com/ChiaPatricia/MentalChat16K.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Dual-Directional Context-Aware Test-Time Learning for Text Classification</title>
<link>https://arxiv.org/abs/2503.15469</link>
<guid>https://arxiv.org/abs/2503.15469</guid>
<content:encoded><![CDATA[
arXiv:2503.15469v4 Announce Type: replace-cross 
Abstract: Text classification assigns text to predefined categories. Traditional methods struggle with complex structures and long-range dependencies. Deep learning with recurrent neural networks and Transformer models has improved feature extraction and context awareness. However, these models still trade off interpretability, efficiency and contextual range. We propose the Dynamic Bidirectional Elman Attention Network (DBEAN). DBEAN combines bidirectional temporal modeling and self-attention. It dynamically weights critical input segments and preserves computational efficiency.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Redefining Toxicity: An Objective and Context-Aware Approach for Stress-Level-Based Detection</title>
<link>https://arxiv.org/abs/2503.16072</link>
<guid>https://arxiv.org/abs/2503.16072</guid>
<content:encoded><![CDATA[
arXiv:2503.16072v2 Announce Type: replace-cross 
Abstract: Most toxicity detection models treat toxicity as an intrinsic property of text, overlooking the role of context in shaping its impact. Drawing on interdisciplinary research, we reconceptualise toxicity as a socially emergent stress signal. We introduce a new framework for toxicity detection, including a formal definition and metric, and validate our approach on a novel dataset, demonstrating improved contextual sensitivity and adaptability.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Beyond Assistance - Reimagining LLMs as Ethical and Adaptive Co-Creators in Mental Health Care</title>
<link>https://arxiv.org/abs/2503.16456</link>
<guid>https://arxiv.org/abs/2503.16456</guid>
<content:encoded><![CDATA[
arXiv:2503.16456v2 Announce Type: replace-cross 
Abstract: This position paper argues for a fundamental shift in how Large Language Models (LLMs) are integrated into the mental health care domain. We advocate for their role as co-creators rather than mere assistive tools. While LLMs have the potential to enhance accessibility, personalization, and crisis intervention, their adoption remains limited due to concerns about bias, evaluation, over-reliance, dehumanization, and regulatory uncertainties. To address these challenges, we propose two structured pathways: SAFE-i (Supportive, Adaptive, Fair, and Ethical Implementation) Guidelines for ethical and responsible deployment, and HAAS-e (Human-AI Alignment and Safety Evaluation) Framework for multidimensional, human-centered assessment. SAFE-i provides a blueprint for data governance, adaptive model engineering, and real-world integration, ensuring LLMs align with clinical and ethical standards. HAAS-e introduces evaluation metrics that go beyond technical accuracy to measure trustworthiness, empathy, cultural sensitivity, and actionability. We call for the adoption of these structured approaches to establish a responsible and scalable model for LLM-driven mental health support, ensuring that AI complements, rather than replaces, human expertise.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARFlow: Human Action-Reaction Flow Matching with Physical Guidance</title>
<link>https://arxiv.org/abs/2503.16973</link>
<guid>https://arxiv.org/abs/2503.16973</guid>
<content:encoded><![CDATA[
arXiv:2503.16973v3 Announce Type: replace-cross 
Abstract: Human action-reaction synthesis, a fundamental challenge in modeling causal human interactions, plays a critical role in applications ranging from virtual reality to social robotics. While diffusion-based models have demonstrated promising performance, they exhibit two key limitations for interaction synthesis: reliance on complex noise-to-reaction generators with intricate conditional mechanisms, and frequent physical violations in generated motions. To address these issues, we propose Action-Reaction Flow Matching (ARFlow), a novel framework that establishes direct action-to-reaction mappings, eliminating the need for complex conditional mechanisms. Our approach introduces a physical guidance mechanism specifically designed for Flow Matching (FM) that effectively prevents body penetration artifacts during sampling. Moreover, we discover the bias of traditional flow matching sampling algorithm and employ a reprojection method to revise the sampling direction of FM. To further enhance the reaction diversity, we incorporate randomness into the sampling process. Extensive experiments on NTU120, Chi3D and InterHuman datasets demonstrate that ARFlow not only outperforms existing methods in terms of Fr\'echet Inception Distance and motion diversity but also significantly reduces body collisions, as measured by our new Intersection Volume and Intersection Frequency metrics.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opportunities and Challenges of Frontier Data Governance With Synthetic Data</title>
<link>https://arxiv.org/abs/2503.17414</link>
<guid>https://arxiv.org/abs/2503.17414</guid>
<content:encoded><![CDATA[
arXiv:2503.17414v2 Announce Type: replace-cross 
Abstract: Synthetic data, or data generated by machine learning models, is increasingly emerging as a solution to the data access problem. However, its use introduces significant governance and accountability challenges, and potentially debases existing governance paradigms, such as compute and data governance. In this paper, we identify 3 key governance and accountability challenges that synthetic data poses - it can enable the increased emergence of malicious actors, spontaneous biases and value drift. We thus craft 3 technical mechanisms to address these specific challenges, finding applications for synthetic data towards adversarial training, bias mitigation and value reinforcement. These could not only counteract the risks of synthetic data, but serve as critical levers for governance of the frontier in the future.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REALM: A Dataset of Real-World LLM Use Cases</title>
<link>https://arxiv.org/abs/2503.18792</link>
<guid>https://arxiv.org/abs/2503.18792</guid>
<content:encoded><![CDATA[
arXiv:2503.18792v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs), such as the GPT series, have driven significant industrial applications, leading to economic and societal transformations. However, a comprehensive understanding of their real-world applications remains limited. To address this, we introduce REALM, a dataset of over 94,000 LLM use cases collected from Reddit and news articles. REALM captures two key dimensions: the diverse applications of LLMs and the demographics of their users. It categorizes LLM applications and explores how users' occupations relate to the types of applications they use. By integrating real-world data, REALM offers insights into LLM adoption across different domains, providing a foundation for future research on their evolving societal roles.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VecTrans: Enhancing Compiler Auto-Vectorization through LLM-Assisted Code Transformations</title>
<link>https://arxiv.org/abs/2503.19449</link>
<guid>https://arxiv.org/abs/2503.19449</guid>
<content:encoded><![CDATA[
arXiv:2503.19449v2 Announce Type: replace-cross 
Abstract: Auto-vectorization is a fundamental optimization for modern compilers to exploit SIMD parallelism. However, state-of-the-art approaches still struggle to handle intricate code patterns, often requiring manual hints or domain-specific expertise. Large language models (LLMs), with their ability to capture intricate patterns, provide a promising solution, yet their effective application in compiler optimizations remains an open challenge due to issues such as hallucinations and a lack of domain-specific reasoning. In this paper, we present VecTrans, a novel framework that leverages LLMs to enhance compiler-based code vectorization. VecTrans first employs compiler analysis to identify potentially vectorizable code regions. It then utilizes an LLM to refactor these regions into patterns that are more amenable to the compilers auto-vectorization. To ensure semantic correctness, VecTrans further integrates a hybrid validation mechanism at the intermediate representation (IR) level. With the above efforts, VecTrans combines the adaptability of LLMs with the precision of compiler vectorization, thereby effectively opening up the vectorization opportunities. experimental results show that among all TSVC functions unvectorizable by GCC, ICC, Clang, and BiSheng Compiler, VecTrans achieves an geomean speedup of 1.77x and successfully vectorizes 24 of 51 test cases. This marks a significant advancement over state-of-the-art approaches while maintaining a cost efficiency of $0.012 per function optimization for LLM API usage.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Event-driven 3D Reconstruction: Development under Different Categories</title>
<link>https://arxiv.org/abs/2503.19753</link>
<guid>https://arxiv.org/abs/2503.19753</guid>
<content:encoded><![CDATA[
arXiv:2503.19753v3 Announce Type: replace-cross 
Abstract: Event cameras have gained increasing attention for 3D reconstruction due to their high temporal resolution, low latency, and high dynamic range. They capture per-pixel brightness changes asynchronously, allowing accurate reconstruction under fast motion and challenging lighting conditions. In this survey, we provide a comprehensive review of event-driven 3D reconstruction methods, including stereo, monocular, and multimodal systems. We further categorize recent developments based on geometric, learning-based, and hybrid approaches. Emerging trends, such as neural radiance fields and 3D Gaussian splatting with event data, are also covered. The related works are structured chronologically to illustrate the innovations and progression within the field. To support future research, we also highlight key research gaps and future research directions in dataset, experiment, evaluation, event representation, etc.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving Systems</title>
<link>https://arxiv.org/abs/2503.20756</link>
<guid>https://arxiv.org/abs/2503.20756</guid>
<content:encoded><![CDATA[
arXiv:2503.20756v2 Announce Type: replace-cross 
Abstract: Recent advancements in Large Multimodal Models (LMMs) have shown promise in Autonomous Driving Systems (ADS). However, their direct application to ADS is hindered by challenges such as misunderstanding of traffic knowledge, complex road conditions, and diverse states of vehicle. To address these challenges, we propose the use of Knowledge Editing, which enables targeted modifications to a model's behavior without the need for full retraining. Meanwhile, we introduce ADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS, which includes various real-world scenarios, multiple data types, and comprehensive evaluation metrics. We conduct comprehensive experiments and derive several interesting conclusions. We hope that our work will contribute to the further advancement of knowledge editing applications in the field of autonomous driving. Code and data are available in https://github.com/zjunlp/EasyEdit.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Inequality of LLM Fact-Checking over Geographic Regions with Agent and Retrieval models</title>
<link>https://arxiv.org/abs/2503.22877</link>
<guid>https://arxiv.org/abs/2503.22877</guid>
<content:encoded><![CDATA[
arXiv:2503.22877v2 Announce Type: replace-cross 
Abstract: Fact-checking is a potentially useful application of Large Language Models (LLMs) to combat the growing dissemination of disinformation. However, the performance of LLMs varies across geographic regions. In this paper, we evaluate the factual accuracy of open and private models across a diverse set of regions and scenarios.
  Using a dataset containing 600 fact-checked statements balanced across six global regions we examine three experimental setups of fact-checking a statement: (1) when just the statement is available, (2) when an LLM-based agent with Wikipedia access is utilized, and (3) as a best case scenario when a Retrieval-Augmented Generation (RAG) system provided with the official fact check is employed. Our findings reveal that regardless of the scenario and LLM used, including GPT-4, Claude Sonnet, and LLaMA, statements from the Global North perform substantially better than those from the Global South. Furthermore, this gap is broadened for the more realistic case of a Wikipedia agent-based system, highlighting that overly general knowledge bases have a limited ability to address region-specific nuances. These results underscore the urgent need for better dataset balancing and robust retrieval strategies to enhance LLM fact-checking capabilities, particularly in geographically diverse contexts.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Conformal Risk Control Framework for Granular Word Assessment and Uncertainty Calibration of CLIPScore Quality Estimates</title>
<link>https://arxiv.org/abs/2504.01225</link>
<guid>https://arxiv.org/abs/2504.01225</guid>
<content:encoded><![CDATA[
arXiv:2504.01225v2 Announce Type: replace-cross 
Abstract: This study explores current limitations of learned image captioning evaluation metrics, specifically the lack of granular assessments for errors within captions, and the reliance on single-point quality estimates without considering uncertainty. To address the limitations, we propose a simple yet effective strategy for generating and calibrating distributions of CLIPScore values. Leveraging a model-agnostic conformal risk control framework, we calibrate CLIPScore values for task-specific control variables, tackling the aforementioned limitations. Experimental results demonstrate that using conformal risk control, over score distributions produced with simple methods such as input masking, can achieve competitive performance compared to more complex approaches. Our method effectively detects erroneous words, while providing formal guarantees aligned with desired risk levels. It also improves the correlation between uncertainty estimations and prediction errors, thus enhancing the overall reliability of caption evaluation metrics.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Linguistic Divide: A Survey on Leveraging Large Language Models for Machine Translation</title>
<link>https://arxiv.org/abs/2504.01919</link>
<guid>https://arxiv.org/abs/2504.01919</guid>
<content:encoded><![CDATA[
arXiv:2504.01919v3 Announce Type: replace-cross 
Abstract: The advent of Large Language Models (LLMs) has significantly reshaped the landscape of machine translation (MT), particularly for low-resource languages and domains that lack sufficient parallel corpora, linguistic tools, and computational infrastructure. This survey presents a comprehensive overview of recent progress in leveraging LLMs for MT. We analyze techniques such as few-shot prompting, cross-lingual transfer, and parameter-efficient fine-tuning (e.g., LoRA, adapters) that enable effective adaptation to under-resourced settings. The paper also explores synthetic data generation strategies using LLMs, including back-translation and lexical augmentation. Additionally, we compare LLM-based translation with traditional encoder-decoder models across diverse language pairs, highlighting the strengths and limitations of each. We discuss persistent challenges - such as hallucinations, evaluation inconsistencies, and inherited biases, while also evaluating emerging LLM-driven metrics for translation quality. This survey offers practical insights and outlines future directions for building robust, inclusive, and scalable MT systems in the era of large-scale generative models.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement</title>
<link>https://arxiv.org/abs/2504.03561</link>
<guid>https://arxiv.org/abs/2504.03561</guid>
<content:encoded><![CDATA[
arXiv:2504.03561v3 Announce Type: replace-cross 
Abstract: In the interaction between agents and their environments, agents expand their capabilities by planning and executing actions. However, LLM-based agents face substantial challenges when deployed in novel environments or required to navigate unconventional action spaces. To empower agents to autonomously explore environments, optimize workflows, and enhance their understanding of actions, we propose SynWorld, a framework that allows agents to synthesize possible scenarios with multi-step action invocation within the action space and perform Monte Carlo Tree Search (MCTS) exploration to effectively refine their action knowledge in the current environment. Our experiments demonstrate that SynWorld is an effective and general approach to learning action knowledge in new environments. Code is available at https://github.com/zjunlp/SynWorld.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic-guided Representation Learning for Multi-Label Recognition</title>
<link>https://arxiv.org/abs/2504.03801</link>
<guid>https://arxiv.org/abs/2504.03801</guid>
<content:encoded><![CDATA[
arXiv:2504.03801v2 Announce Type: replace-cross 
Abstract: Multi-label Recognition (MLR) involves assigning multiple labels to each data instance in an image, offering advantages over single-label classification in complex scenarios. However, it faces the challenge of annotating all relevant categories, often leading to uncertain annotations, such as unseen or incomplete labels. Recent Vision and Language Pre-training (VLP) based methods have made significant progress in tackling zero-shot MLR tasks by leveraging rich vision-language correlations. However, the correlation between multi-label semantics has not been fully explored, and the learned visual features often lack essential semantic information. To overcome these limitations, we introduce a Semantic-guided Representation Learning approach (SigRL) that enables the model to learn effective visual and textual representations, thereby improving the downstream alignment of visual images and categories. Specifically, we first introduce a graph-based multi-label correlation module (GMC) to facilitate information exchange between labels, enriching the semantic representation across the multi-label texts. Next, we propose a Semantic Visual Feature Reconstruction module (SVFR) to enhance the semantic information in the visual representation by integrating the learned textual representation during reconstruction. Finally, we optimize the image-text matching capability of the VLP model using both local and global features to achieve zero-shot MLR. Comprehensive experiments are conducted on several MLR benchmarks, encompassing both zero-shot MLR (with unseen labels) and single positive multi-label learning (with limited labels), demonstrating the superior performance of our approach compared to state-of-the-art methods. The code is available at https://github.com/MVL-Lab/SigRL.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persona Dynamics: Unveiling the Impact of Personality Traits on Agents in Text-Based Games</title>
<link>https://arxiv.org/abs/2504.06868</link>
<guid>https://arxiv.org/abs/2504.06868</guid>
<content:encoded><![CDATA[
arXiv:2504.06868v4 Announce Type: replace-cross 
Abstract: Artificial agents are increasingly central to complex interactions and decision-making tasks, yet aligning their behaviors with desired human values remains an open challenge. In this work, we investigate how human-like personality traits influence agent behavior and performance within text-based interactive environments. We introduce PANDA: Personality Adapted Neural Decision Agents, a novel method for projecting human personality traits onto agents to guide their behavior. To induce personality in a text-based game agent, (i) we train a personality classifier to identify what personality type the agent's actions exhibit, and (ii) we integrate the personality profiles directly into the agent's policy-learning pipeline. By deploying agents embodying 16 distinct personality types across 25 text-based games and analyzing their trajectories, we demonstrate that an agent's action decisions can be guided toward specific personality profiles. Moreover, certain personality types, such as those characterized by higher levels of Openness, display marked advantages in performance. These findings underscore the promise of personality-adapted agents for fostering more aligned, effective, and human-centric decision-making in interactive environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SD$^2$: Self-Distilled Sparse Drafters</title>
<link>https://arxiv.org/abs/2504.08838</link>
<guid>https://arxiv.org/abs/2504.08838</guid>
<content:encoded><![CDATA[
arXiv:2504.08838v2 Announce Type: replace-cross 
Abstract: Speculative decoding is a powerful technique for reducing the latency of Large Language Models (LLMs), offering a fault-tolerant framework that enables the use of highly compressed draft models. In this work, we introduce Self-Distilled Sparse Drafters (SD$^2$), a novel methodology that leverages self-data distillation and fine-grained weight sparsity to produce highly efficient and well-aligned draft models. SD$^2$ systematically enhances draft token acceptance rates while significantly reducing Multiply-Accumulate operations (MACs), even in the Universal Assisted Generation (UAG) setting, where draft and target models originate from different model families. On a Llama-3.1-70B target model, SD$^2$ provides a 1.59$\times$ higher Mean Accepted Length (MAL) compared to layer-pruned draft models and reduces MACs by over 43.87% with a 8.36% reduction in MAL compared to a dense draft models. Our 1.5B and 3B unstructured sparse drafters outperform both dense and layer-pruned models in terms of end-to-end latency improvements; highlighting the potential of sparsity-aware fine-tuning and compression strategies to improve LLM inference efficiency while maintaining alignment with target models.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parameterized Synthetic Text Generation with SimpleStories</title>
<link>https://arxiv.org/abs/2504.09184</link>
<guid>https://arxiv.org/abs/2504.09184</guid>
<content:encoded><![CDATA[
arXiv:2504.09184v3 Announce Type: replace-cross 
Abstract: We present SimpleStories, a large synthetic story dataset in simple language, consisting of 2 million samples each in English and Japanese. Through parameterizing prompts at multiple levels of abstraction, we achieve control over story characteristics at scale, inducing syntactic and semantic diversity. Ablations on a newly trained model suite show improved sample efficiency and model interpretability compared to the TinyStories dataset. We open-source all constituent parts of model creation, hoping to enable novel ways to study the end-to-end training process. As a byproduct, we move the frontier regarding the fewest-parameter language model that outputs grammatical natural language.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Structural Safety Generalization Problem</title>
<link>https://arxiv.org/abs/2504.09712</link>
<guid>https://arxiv.org/abs/2504.09712</guid>
<content:encoded><![CDATA[
arXiv:2504.09712v2 Announce Type: replace-cross 
Abstract: LLM jailbreaks are a widespread safety challenge. Given this problem has not yet been tractable, we suggest targeting a key failure mechanism: the failure of safety to generalize across semantically equivalent inputs. We further focus the target by requiring desirable tractability properties of attacks to study: explainability, transferability between models, and transferability between goals. We perform red-teaming within this framework by uncovering new vulnerabilities to multi-turn, multi-image, and translation-based attacks. These attacks are semantically equivalent by our design to their single-turn, single-image, or untranslated counterparts, enabling systematic comparisons; we show that the different structures yield different safety outcomes. We then demonstrate the potential for this framework to enable new defenses by proposing a Structure Rewriting Guardrail, which converts an input to a structure more conducive to safety assessment. This guardrail significantly improves refusal of harmful inputs, without over-refusing benign ones. Thus, by framing this intermediate challenge - more tractable than universal defenses but essential for long-term safety - we highlight a critical milestone for AI safety research.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Hitchhiker's Guide to Program Analysis, Part II: Deep Thoughts by LLMs</title>
<link>https://arxiv.org/abs/2504.11711</link>
<guid>https://arxiv.org/abs/2504.11711</guid>
<content:encoded><![CDATA[
arXiv:2504.11711v3 Announce Type: replace-cross 
Abstract: Static analysis plays a crucial role in software vulnerability detection, yet faces a persistent precision-scalability tradeoff. In large codebases like the Linux kernel, traditional static analysis tools often generate excessive false positives due to simplified vulnerability modeling and overapproximation of path and data constraints. While large language models (LLMs) demonstrate promising code understanding capabilities, their direct application to program analysis remains unreliable due to inherent reasoning limitations.
  We introduce BugLens, a post-refinement framework that significantly enhances static analysis precision for bug detection. BugLens guides LLMs through structured reasoning steps to assess security impact and validate constraints from the source code. When evaluated on Linux kernel taint-style bugs detected by static analysis tools, BugLens improves precision approximately 7-fold (from 0.10 to 0.72), substantially reducing false positives while uncovering four previously unreported vulnerabilities. Our results demonstrate that a well-structured, fully automated LLM-based workflow can effectively complement and enhance traditional static analysis techniques.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: An Empirically Grounded Identifiability Theory Will Accelerate Self-Supervised Learning Research</title>
<link>https://arxiv.org/abs/2504.13101</link>
<guid>https://arxiv.org/abs/2504.13101</guid>
<content:encoded><![CDATA[
arXiv:2504.13101v2 Announce Type: replace-cross 
Abstract: Self-Supervised Learning (SSL) powers many current AI systems. As research interest and investment grow, the SSL design space continues to expand. The Platonic view of SSL, following the Platonic Representation Hypothesis (PRH), suggests that despite different methods and engineering approaches, all representations converge to the same Platonic ideal. However, this phenomenon lacks precise theoretical explanation. By synthesizing evidence from Identifiability Theory (IT), we show that the PRH can emerge in SSL. However, current IT cannot explain SSL's empirical success. To bridge the gap between theory and practice, we propose expanding IT into what we term Singular Identifiability Theory (SITh), a broader theoretical framework encompassing the entire SSL pipeline. SITh would allow deeper insights into the implicit data assumptions in SSL and advance the field towards learning more interpretable and generalizable representations. We highlight three critical directions for future research: 1) training dynamics and convergence properties of SSL; 2) the impact of finite samples, batch size, and data diversity; and 3) the role of inductive biases in architecture, augmentations, initialization schemes, and optimizers.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling the Lack of LVLM Robustness to Fundamental Visual Variations: Why and Path Forward</title>
<link>https://arxiv.org/abs/2504.16727</link>
<guid>https://arxiv.org/abs/2504.16727</guid>
<content:encoded><![CDATA[
arXiv:2504.16727v3 Announce Type: replace-cross 
Abstract: Large Vision Language Models (LVLMs) excel in various vision-language tasks. Yet, their robustness to visual variations in position, scale, orientation, and context that objects in natural scenes inevitably exhibit due to changes in viewpoint and environment remains largely underexplored. To bridge this gap, we introduce V$^2$R-Bench, a comprehensive benchmark framework for evaluating Visual Variation Robustness of LVLMs, which encompasses automated evaluation dataset generation and principled metrics for thorough robustness assessment. Through extensive evaluation on 21 LVLMs, we reveal a surprising vulnerability to visual variations, in which even advanced models that excel at complex vision-language tasks significantly underperform on simple tasks such as object recognition. Interestingly, these models exhibit a distinct visual position bias that contradicts theories of effective receptive fields, and demonstrate a human-like visual acuity threshold. To identify the source of these vulnerabilities, we present a systematic framework for component-level analysis, featuring a novel visualization approach for aligned visual features. Results show that these vulnerabilities stem from error accumulation in the pipeline architecture and inadequate multimodal alignment. Complementary experiments with synthetic data further demonstrate that these limitations are fundamentally architectural deficiencies, scoring the need for architectural innovations in future LVLM designs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>(Im)possibility of Automated Hallucination Detection in Large Language Models</title>
<link>https://arxiv.org/abs/2504.17004</link>
<guid>https://arxiv.org/abs/2504.17004</guid>
<content:encoded><![CDATA[
arXiv:2504.17004v2 Announce Type: replace-cross 
Abstract: Is automated hallucination detection possible? In this work, we introduce a theoretical framework to analyze the feasibility of automatically detecting hallucinations produced by large language models (LLMs). Inspired by the classical Gold-Angluin framework for language identification and its recent adaptation to language generation by Kleinberg and Mullainathan, we investigate whether an algorithm, trained on examples drawn from an unknown target language $K$ (selected from a countable collection) and given access to an LLM, can reliably determine whether the LLM's outputs are correct or constitute hallucinations.
  First, we establish an equivalence between hallucination detection and the classical task of language identification. We prove that any hallucination detection method can be converted into a language identification method, and conversely, algorithms solving language identification can be adapted for hallucination detection. Given the inherent difficulty of language identification, this implies that hallucination detection is fundamentally impossible for most language collections if the detector is trained using only correct examples from the target language.
  Second, we show that the use of expert-labeled feedback, i.e., training the detector with both positive examples (correct statements) and negative examples (explicitly labeled incorrect statements), dramatically changes this conclusion. Under this enriched training regime, automated hallucination detection becomes possible for all countable language collections.
  These results highlight the essential role of expert-labeled examples in training hallucination detectors and provide theoretical support for feedback-based methods, such as reinforcement learning with human feedback (RLHF), which have proven critical for reliable LLM deployment.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Depth-Constrained ASV Navigation with Deep RL and Limited Sensing</title>
<link>https://arxiv.org/abs/2504.18253</link>
<guid>https://arxiv.org/abs/2504.18253</guid>
<content:encoded><![CDATA[
arXiv:2504.18253v2 Announce Type: replace-cross 
Abstract: Autonomous Surface Vehicles (ASVs) play a crucial role in maritime operations, yet their navigation in shallow-water environments remains challenging due to dynamic disturbances and depth constraints. Traditional navigation strategies struggle with limited sensor information, making safe and efficient operation difficult. In this paper, we propose a reinforcement learning (RL) framework for ASV navigation under depth constraints, where the vehicle must reach a target while avoiding unsafe areas with only a single depth measurement per timestep from a downward-facing Single Beam Echosounder (SBES). To enhance environmental awareness, we integrate Gaussian Process (GP) regression into the RL framework, enabling the agent to progressively estimate a bathymetric depth map from sparse sonar readings. This approach improves decision-making by providing a richer representation of the environment. Furthermore, we demonstrate effective sim-to-real transfer, ensuring that trained policies generalize well to real-world aquatic conditions. Experimental results validate our method's capability to improve ASV navigation performance while maintaining safety in challenging shallow-water environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prisma: An Open Source Toolkit for Mechanistic Interpretability in Vision and Video</title>
<link>https://arxiv.org/abs/2504.19475</link>
<guid>https://arxiv.org/abs/2504.19475</guid>
<content:encoded><![CDATA[
arXiv:2504.19475v2 Announce Type: replace-cross 
Abstract: Robust tooling and publicly available pre-trained models have helped drive recent advances in mechanistic interpretability for language models. However, similar progress in vision mechanistic interpretability has been hindered by the lack of accessible frameworks and pre-trained weights. We present Prisma (Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an open-source framework designed to accelerate vision mechanistic interpretability research, providing a unified toolkit for accessing 75+ vision and video transformers; support for sparse autoencoder (SAE), transcoder, and crosscoder training; a suite of 80+ pre-trained SAE weights; activation caching, circuit analysis tools, and visualization tools; and educational resources. Our analysis reveals surprising findings, including that effective vision SAEs can exhibit substantially lower sparsity patterns than language SAEs, and that in some instances, SAE reconstructions can decrease model loss. Prisma enables new research directions for understanding vision model internals while lowering barriers to entry in this emerging field.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Handling Label Noise via Instance-Level Difficulty Modeling and Dynamic Optimization</title>
<link>https://arxiv.org/abs/2505.00812</link>
<guid>https://arxiv.org/abs/2505.00812</guid>
<content:encoded><![CDATA[
arXiv:2505.00812v3 Announce Type: replace-cross 
Abstract: Recent studies indicate that deep neural networks degrade in generalization performance under noisy supervision. Existing methods focus on isolating clean subsets or correcting noisy labels, facing limitations such as high computational costs, heavy hyperparameter tuning process, and coarse-grained optimization. To address these challenges, we propose a novel two-stage noisy learning framework that enables instance-level optimization through a dynamically weighted loss function, avoiding hyperparameter tuning. To obtain stable and accurate information about noise modeling, we introduce a simple yet effective metric, termed wrong event, which dynamically models the cleanliness and difficulty of individual samples while maintaining computational costs. Our framework first collects wrong event information and builds a strong base model. Then we perform noise-robust training on the base model, using a probabilistic model to handle the wrong event information of samples. Experiments on five synthetic and real-world LNL benchmarks demonstrate our method surpasses state-of-the-art methods in performance, achieves a nearly 75% reduction in computational time and improves model scalability.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OODTE: A Differential Testing Engine for the ONNX Optimizer</title>
<link>https://arxiv.org/abs/2505.01892</link>
<guid>https://arxiv.org/abs/2505.01892</guid>
<content:encoded><![CDATA[
arXiv:2505.01892v2 Announce Type: replace-cross 
Abstract: With over 700 stars on GitHub and being part of the official ONNX repository, the ONNX Optimizer is the default tool for applying graph-based optimizations to ONNX models. Despite its widespread use, its ability to maintain model accuracy during optimization has not been thoroughly investigated. In this work, we present OODTE, a utility designed to automatically and comprehensively evaluate the correctness of the ONNX Optimizer. OODTE adopts a straightforward yet powerful differential testing and evaluation methodology, which can be readily adapted for use with other compiler optimizers. Specifically, OODTE takes a collection of ONNX models, applies optimizations, and executes both the original and optimized versions across a user-defined input set, automatically capturing any issues encountered during optimization. When discrepancies in accuracy arise, OODTE iteratively isolates the responsible optimization pass by repeating the process at a finer granularity. We applied OODTE to 130 well-known models from the official ONNX Model Hub, spanning diverse tasks including classification, object detection, semantic segmentation, text summarization, question answering, and sentiment analysis. Our evaluation revealed that 9.2% of the model instances either caused the optimizer to crash or led to the generation of invalid models using default optimization strategies. Additionally, 30% of classification models and 16.6% of object detection and segmentation models exhibited differing outputs across original and optimized versions, whereas models focused on text-related tasks were generally robust to optimization. OODTE uncovered 15 issues-14 previously unknown-affecting 9 of 47 optimization passes and the optimizer overall. All issues were reported to the ONNX Optimizer team. OODTE offers a simple but effective framework for validating AI model optimizers, applicable beyond the ONNX ecosystem.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Motion-compensated cardiac MRI using low-rank diffeomorphic flow (DMoCo)</title>
<link>https://arxiv.org/abs/2505.03149</link>
<guid>https://arxiv.org/abs/2505.03149</guid>
<content:encoded><![CDATA[
arXiv:2505.03149v3 Announce Type: replace-cross 
Abstract: We introduce an unsupervised motion-compensated image reconstruction algorithm for free-breathing and ungated 3D cardiac magnetic resonance imaging (MRI). We express the image volume corresponding to each specific motion phase as the deformation of a single static image template. The main contribution of the work is the low-rank model for the compact joint representation of the family of diffeomorphisms, parameterized by the motion phases. The diffeomorphism at a specific motion phase is obtained by integrating a parametric velocity field along a path connecting the reference template phase to the motion phase. The velocity field at different phases is represented using a low-rank model. The static template and the low-rank motion model parameters are learned directly from the k-space data in an unsupervised fashion. The more constrained motion model is observed to offer improved recovery compared to current motion-resolved and motion-compensated algorithms for free-breathing 3D cine MRI.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GAME: Learning Multimodal Interactions via Graph Structures for Personality Trait Estimation</title>
<link>https://arxiv.org/abs/2505.03846</link>
<guid>https://arxiv.org/abs/2505.03846</guid>
<content:encoded><![CDATA[
arXiv:2505.03846v2 Announce Type: replace-cross 
Abstract: Apparent personality analysis from short videos poses significant chal-lenges due to the complex interplay of visual, auditory, and textual cues. In this paper, we propose GAME, a Graph-Augmented Multimodal Encoder designed to robustly model and fuse multi-source features for automatic personality prediction. For the visual stream, we construct a facial graph and introduce a dual-branch Geo Two-Stream Network, which combines Graph Convolutional Networks (GCNs) and Convolutional Neural Net-works (CNNs) with attention mechanisms to capture both structural and appearance-based facial cues. Complementing this, global context and iden-tity features are extracted using pretrained ResNet18 and VGGFace back-bones. To capture temporal dynamics, frame-level features are processed by a BiGRU enhanced with temporal attention modules. Meanwhile, audio representations are derived from the VGGish network, and linguistic se-mantics are captured via the XLM-Roberta transformer. To achieve effective multimodal integration, we propose a Channel Attention-based Fusion module, followed by a Multi-Layer Perceptron (MLP) regression head for predicting personality traits. Extensive experiments show that GAME con-sistently outperforms existing methods across multiple benchmarks, vali-dating its effectiveness and generalizability.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Framework for Infrastructure Maintenance: Crack Detection and High-Resolution Imaging of Infrastructure Surfaces</title>
<link>https://arxiv.org/abs/2505.03974</link>
<guid>https://arxiv.org/abs/2505.03974</guid>
<content:encoded><![CDATA[
arXiv:2505.03974v2 Announce Type: replace-cross 
Abstract: Recently, there has been an impetus for the application of cutting-edge data collection platforms such as drones mounted with camera sensors for infrastructure asset management. However, the sensor characteristics, proximity to the structure, hard-to-reach access, and environmental conditions often limit the resolution of the datasets. A few studies used super-resolution techniques to address the problem of low-resolution images. Nevertheless, these techniques were observed to increase computational cost and false alarms of distress detection due to the consideration of all the infrastructure images i.e., positive and negative distress classes. In order to address the pre-processing of false alarm and achieve efficient super-resolution, this study developed a framework consisting of convolutional neural network (CNN) and efficient sub-pixel convolutional neural network (ESPCNN). CNN accurately classified both the classes. ESPCNN, which is the lightweight super-resolution technique, generated high-resolution infrastructure image of positive distress obtained from CNN. The ESPCNN outperformed bicubic interpolation in all the evaluation metrics for super-resolution. Based on the performance metrics, the combination of CNN and ESPCNN was observed to be effective in preprocessing the infrastructure images with negative distress, reducing the computational cost and false alarms in the next step of super-resolution. The visual inspection showed that EPSCNN is able to capture crack propagation, complex geometry of even minor cracks. The proposed framework is expected to help the highway agencies in accurately performing distress detection and assist in efficient asset management practices.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WATCH: Adaptive Monitoring for AI Deployments via Weighted-Conformal Martingales</title>
<link>https://arxiv.org/abs/2505.04608</link>
<guid>https://arxiv.org/abs/2505.04608</guid>
<content:encoded><![CDATA[
arXiv:2505.04608v3 Announce Type: replace-cross 
Abstract: Responsibly deploying artificial intelligence (AI) / machine learning (ML) systems in high-stakes settings arguably requires not only proof of system reliability, but also continual, post-deployment monitoring to quickly detect and address any unsafe behavior. Methods for nonparametric sequential testing -- especially conformal test martingales (CTMs) and anytime-valid inference -- offer promising tools for this monitoring task. However, existing approaches are restricted to monitoring limited hypothesis classes or ``alarm criteria'' (e.g., detecting data shifts that violate certain exchangeability or IID assumptions), do not allow for online adaptation in response to shifts, and/or cannot diagnose the cause of degradation or alarm. In this paper, we address these limitations by proposing a weighted generalization of conformal test martingales (WCTMs), which lay a theoretical foundation for online monitoring for any unexpected changepoints in the data distribution while controlling false-alarms. For practical applications, we propose specific WCTM algorithms that adapt online to mild covariate shifts (in the marginal input distribution), quickly detect harmful shifts, and diagnose those harmful shifts as concept shifts (in the conditional label distribution) or extreme (out-of-support) covariate shifts that cannot be easily adapted to. On real-world datasets, we demonstrate improved performance relative to state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Confabulation dynamics in a reservoir computer: Filling in the gaps with untrained attractors</title>
<link>https://arxiv.org/abs/2505.04792</link>
<guid>https://arxiv.org/abs/2505.04792</guid>
<content:encoded><![CDATA[
arXiv:2505.04792v2 Announce Type: replace-cross 
Abstract: Artificial Intelligence has advanced significantly in recent years thanks to innovations in the design and training of artificial neural networks (ANNs). Despite these advancements, we still understand relatively little about how elementary forms of ANNs learn, fail to learn, and generate false information without the intent to deceive, a phenomenon known as `confabulation'. To provide some foundational insight, in this paper we analyse how confabulation occurs in reservoir computers (RCs): a dynamical system in the form of an ANN. RCs are particularly useful to study as they are known to confabulate in a well-defined way: when RCs are trained to reconstruct the dynamics of a given attractor, they sometimes construct an attractor that they were not trained to construct, a so-called `untrained attractor' (UA). This paper sheds light on the role played by UAs when reconstruction fails and their influence when modelling transitions between reconstructed attractors. Based on our results, we conclude that UAs are an intrinsic feature of learning systems whose state spaces are bounded, and that this means of confabulation may be present in systems beyond RCs.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of 3D Reconstruction with Event Cameras</title>
<link>https://arxiv.org/abs/2505.08438</link>
<guid>https://arxiv.org/abs/2505.08438</guid>
<content:encoded><![CDATA[
arXiv:2505.08438v2 Announce Type: replace-cross 
Abstract: Event cameras are rapidly emerging as powerful vision sensors for 3D reconstruction, uniquely capable of asynchronously capturing per-pixel brightness changes. Compared to traditional frame-based cameras, event cameras produce sparse yet temporally dense data streams, enabling robust and accurate 3D reconstruction even under challenging conditions such as high-speed motion, low illumination, and extreme dynamic range scenarios. These capabilities offer substantial promise for transformative applications across various fields, including autonomous driving, robotics, aerial navigation, and immersive virtual reality. In this survey, we present the first comprehensive review exclusively dedicated to event-based 3D reconstruction. Existing approaches are systematically categorised based on input modality into stereo, monocular, and multimodal systems, and further classified according to reconstruction methodologies, including geometry-based techniques, deep learning approaches, and neural rendering techniques such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Within each category, methods are chronologically organised to highlight the evolution of key concepts and advancements. Furthermore, we provide a detailed summary of publicly available datasets specifically suited to event-based reconstruction tasks. Finally, we discuss significant open challenges in dataset availability, standardised evaluation, effective representation, and dynamic scene reconstruction, outlining insightful directions for future research. This survey aims to serve as an essential reference and provides a clear and motivating roadmap toward advancing the state of the art in event-driven 3D reconstruction.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A3 : an Analytical Low-Rank Approximation Framework for Attention</title>
<link>https://arxiv.org/abs/2505.12942</link>
<guid>https://arxiv.org/abs/2505.12942</guid>
<content:encoded><![CDATA[
arXiv:2505.12942v2 Announce Type: replace-cross 
Abstract: Large language models have demonstrated remarkable performance; however, their massive parameter counts make deployment highly expensive. Low-rank approximation offers a promising compression solution, yet existing approaches have two main limitations: (1) They focus on minimizing the output error of individual linear layers, without considering the architectural characteristics of Transformers, and (2) they decompose a large weight matrix into two small low-rank matrices. Consequently, these methods often fall short compared to other compression techniques like pruning and quantization, and introduce runtime overhead such as the extra GEMM kernel launches for decomposed small matrices. To address these limitations, we propose $\tt A^\tt 3$, a post-training low-rank approximation framework. $\tt A^\tt 3$ splits a Transformer layer into three functional components, namely $\tt QK$, $\tt OV$, and $\tt MLP$. For each component, $\tt A^\tt 3$ provides an analytical solution that reduces the hidden dimension size inside each component while minimizing the component's functional loss ($\it i.e.$, error in attention scores, attention outputs, and MLP outputs). This approach directly reduces model sizes, KV cache sizes, and FLOPs without introducing any runtime overheads. In addition, it provides a new narrative in advancing the optimization problem from singular linear layer loss optimization toward improved end-to-end performance. Through extensive experiments, we show that $\tt A^\tt 3$ maintains superior performance compared to SoTAs. For example, under the same reduction budget in computation and memory, our low-rank approximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2, outperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the versatility of $\tt A^\tt 3$, including KV cache compression, quantization, and mixed-rank assignments for enhanced performance.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forensic deepfake audio detection using segmental speech features</title>
<link>https://arxiv.org/abs/2505.13847</link>
<guid>https://arxiv.org/abs/2505.13847</guid>
<content:encoded><![CDATA[
arXiv:2505.13847v2 Announce Type: replace-cross 
Abstract: This study explores the potential of using acoustic features of segmental speech sounds to detect deepfake audio. These features are highly interpretable because of their close relationship with human articulatory processes and are expected to be more difficult for deepfake models to replicate. The results demonstrate that certain segmental features commonly used in forensic voice comparison (FVC) are effective in identifying deep-fakes, whereas some global features provide little value. These findings underscore the need to approach audio deepfake detection using methods that are distinct from those employed in traditional FVC, and offer a new perspective on leveraging segmental features for this purpose.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Replay Attacks Against Audio Deepfake Detection</title>
<link>https://arxiv.org/abs/2505.14862</link>
<guid>https://arxiv.org/abs/2505.14862</guid>
<content:encoded><![CDATA[
arXiv:2505.14862v2 Announce Type: replace-cross 
Abstract: We show how replay attacks undermine audio deepfake detection: By playing and re-recording deepfake audio through various speakers and microphones, we make spoofed samples appear authentic to the detection model. To study this phenomenon in more detail, we introduce ReplayDF, a dataset of recordings derived from M-AILABS and MLAAD, featuring 109 speaker-microphone combinations across six languages and four TTS models. It includes diverse acoustic conditions, some highly challenging for detection. Our analysis of six open-source detection models across five datasets reveals significant vulnerability, with the top-performing W2V2-AASIST model's Equal Error Rate (EER) surging from 4.7% to 18.2%. Even with adaptive Room Impulse Response (RIR) retraining, performance remains compromised with an 11.0% EER. We release ReplayDF for non-commercial research use.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuickVideo: Real-Time Long Video Understanding with System Algorithm Co-Design</title>
<link>https://arxiv.org/abs/2505.16175</link>
<guid>https://arxiv.org/abs/2505.16175</guid>
<content:encoded><![CDATA[
arXiv:2505.16175v2 Announce Type: replace-cross 
Abstract: Long-video understanding has emerged as a crucial capability in real-world applications such as video surveillance, meeting summarization, educational lecture analysis, and sports broadcasting. However, it remains computationally prohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential video decoding, the process of converting the raw bit stream to RGB frames can take up to a minute for hour-long video inputs, and 2) costly prefilling of up to several million tokens for LLM inference, resulting in high latency and memory use. To address these challenges, we propose QuickVideo, a system-algorithm co-design that substantially accelerates long-video understanding to support real-time downstream applications. It comprises three key innovations: QuickDecoder, a parallelized CPU-based video decoder that achieves 2-3 times speedup by splitting videos into keyframe-aligned intervals processed concurrently; QuickPrefill, a memory-efficient prefilling method using KV-cache pruning to support more frames with less GPU memory; and an overlapping scheme that overlaps CPU video decoding with GPU inference. Together, these components infernece time reduce by a minute on long video inputs, enabling scalable, high-quality video understanding even on limited hardware. Experiments show that QuickVideo generalizes across durations and sampling rates, making long video processing feasible in practice.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2505.16415</link>
<guid>https://arxiv.org/abs/2505.16415</guid>
<content:encoded><![CDATA[
arXiv:2505.16415v2 Announce Type: replace-cross 
Abstract: Retrieval-Augmented Generation (RAG) leverages large language models (LLMs) combined with external contexts to enhance the accuracy and reliability of generated responses. However, reliably attributing generated content to specific context segments, context attribution, remains challenging due to the computationally intensive nature of current methods, which often require extensive fine-tuning or human annotation. In this work, we introduce a novel Jensen-Shannon Divergence driven method to Attribute Response to Context (ARC-JSD), enabling efficient and accurate identification of essential context sentences without additional fine-tuning or surrogate modelling. Evaluations on a wide range of RAG benchmarks, such as TyDi QA, Hotpot QA, and Musique, using instruction-tuned LLMs in different scales demonstrate superior accuracy and significant computational efficiency improvements compared to the previous surrogate-based method. Furthermore, our mechanistic analysis reveals specific attention heads and multilayer perceptron (MLP) layers responsible for context attribution, providing valuable insights into the internal workings of RAG models. Our code is available at https://github.com/ruizheliUOA/ARC_JSD
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrimR: Verifier-based Training-Free Thinking Compression for Efficient Test-Time Scaling</title>
<link>https://arxiv.org/abs/2505.17155</link>
<guid>https://arxiv.org/abs/2505.17155</guid>
<content:encoded><![CDATA[
arXiv:2505.17155v2 Announce Type: replace-cross 
Abstract: Large Reasoning Models (LRMs) demonstrate exceptional capability in tackling complex mathematical, logical, and coding tasks by leveraging extended Chain-of-Thought (CoT) reasoning. Test-time scaling methods, such as prolonging CoT with explicit token-level exploration, can push LRMs' accuracy boundaries, but they incur significant decoding overhead. A key inefficiency source is LRMs often generate redundant thinking CoTs, which demonstrate clear structured overthinking and underthinking patterns. Inspired by human cognitive reasoning processes and numerical optimization theories, we propose TrimR, a verifier-based, training-free, efficient framework for dynamic CoT compression to trim reasoning and enhance test-time scaling, explicitly tailored for production-level deployment. Our method employs a lightweight, pretrained, instruction-tuned verifier to detect and truncate redundant intermediate thoughts of LRMs without any LRM or verifier fine-tuning. We present both the core algorithm and asynchronous online system engineered for high-throughput industrial applications. Empirical evaluations on Ascend NPUs and vLLM show that our framework delivers substantial gains in inference efficiency under large-batch workloads. In particular, on the four MATH500, AIME24, AIME25, and GPQA benchmarks, the reasoning runtime of Pangu Pro MoE, Pangu-R-38B, QwQ-32B, and DeepSeek-R1-Distill-Qwen-32B is improved by up to 70% with negligible impact on accuracy.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Fully Generative Motivational Interviewing Counsellor Chatbot for Moving Smokers Towards the Decision to Quit</title>
<link>https://arxiv.org/abs/2505.17362</link>
<guid>https://arxiv.org/abs/2505.17362</guid>
<content:encoded><![CDATA[
arXiv:2505.17362v3 Announce Type: replace-cross 
Abstract: The conversational capabilities of Large Language Models (LLMs) suggest that they may be able to perform as automated talk therapists. It is crucial to know if these systems would be effective and adhere to known standards. We present a counsellor chatbot that focuses on motivating tobacco smokers to quit smoking. It uses a state-of-the-art LLM and a widely applied therapeutic approach called Motivational Interviewing (MI), and was evolved in collaboration with clinician-scientists with expertise in MI. We also describe and validate an automated assessment of both the chatbot's adherence to MI and client responses. The chatbot was tested on 106 participants, and their confidence that they could succeed in quitting smoking was measured before the conversation and one week later. Participants' confidence increased by an average of 1.7 on a 0-10 scale. The automated assessment of the chatbot showed adherence to MI standards in 98% of utterances, higher than human counsellors. The chatbot scored well on a participant-reported metric of perceived empathy but lower than typical human counsellors. Furthermore, participants' language indicated a good level of motivation to change, a key goal in MI. These results suggest that the automation of talk therapy with a modern LLM has promise.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FRIREN: Beyond Trajectories -- A Spectral Lens on Time</title>
<link>https://arxiv.org/abs/2505.17370</link>
<guid>https://arxiv.org/abs/2505.17370</guid>
<content:encoded><![CDATA[
arXiv:2505.17370v3 Announce Type: replace-cross 
Abstract: Long-term time-series forecasting (LTSF) models are often presented as general-purpose solutions that can be applied across domains, implicitly assuming that all data is pointwise predictable. Using chaotic systems such as Lorenz-63 as a case study, we argue that geometric structure - not pointwise prediction - is the right abstraction for a dynamic-agnostic foundational model. Minimizing the Wasserstein-2 distance (W2), which captures geometric changes, and providing a spectral view of dynamics are essential for long-horizon forecasting. Our model, FRIREN (Flow-inspired Representations via Interpretable Eigen-networks), implements an augmented normalizing-flow block that embeds data into a normally distributed latent representation. It then generates a W2-efficient optimal path that can be decomposed into rotation, scaling, inverse rotation, and translation. This architecture yields locally generated, geometry-preserving predictions that are independent of the underlying dynamics, and a global spectral representation that functions as a finite Koopman operator with a small modification. This enables practitioners to identify which modes grow, decay, or oscillate, both locally and system-wide. FRIREN achieves an MSE of 11.4, MAE of 1.6, and SWD of 0.96 on Lorenz-63 in a 336-in, 336-out, dt=0.01 setting, surpassing TimeMixer (MSE 27.3, MAE 2.8, SWD 2.1). The model maintains effective prediction for 274 out of 336 steps, approximately 2.5 Lyapunov times. On Rossler (96-in, 336-out), FRIREN achieves an MSE of 0.0349, MAE of 0.0953, and SWD of 0.0170, outperforming TimeMixer's MSE of 4.3988, MAE of 0.886, and SWD of 3.2065. FRIREN is also competitive on standard LTSF datasets such as ETT and Weather. By connecting modern generative flows with classical spectral analysis, FRIREN makes long-term forecasting both accurate and interpretable, setting a new benchmark for LTSF model design.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NMCSE: Noise-Robust Multi-Modal Coupling Signal Estimation Method via Optimal Transport for Cardiovascular Disease Detection</title>
<link>https://arxiv.org/abs/2505.18174</link>
<guid>https://arxiv.org/abs/2505.18174</guid>
<content:encoded><![CDATA[
arXiv:2505.18174v2 Announce Type: replace-cross 
Abstract: Electrocardiogram (ECG) and Phonocardiogram (PCG) signals are linked by a latent coupling signal representing the electrical-to-mechanical cardiac transformation. While valuable for cardiovascular disease (CVD) detection, this coupling signal is traditionally estimated using deconvolution methods that amplify noise, limiting clinical utility. In this paper, we propose Noise-Robust Multi-Modal Coupling Signal Estimation (NMCSE), which reformulates the problem as distribution matching via optimal transport theory. By jointly optimizing amplitude and temporal alignment, NMCSE mitigates noise amplification without additional preprocessing. Integrated with our Temporal-Spatial Feature Extraction network, NMCSE enables robust multi-modal CVD detection. Experiments on the PhysioNet 2016 dataset with realistic hospital noise demonstrate that NMCSE reduces estimation errors by approximately 30% in Mean Squared Error while maintaining higher Pearson Correlation Coefficients across all tested signal-to-noise ratios. Our approach achieves 97.38% accuracy and 0.98 AUC in CVD detection, outperforming state-of-the-art methods and demonstrating robust performance for real-world clinical applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-Optimized Convolutional Recurrent Networks Align with Tactile Processing in the Rodent Brain</title>
<link>https://arxiv.org/abs/2505.18361</link>
<guid>https://arxiv.org/abs/2505.18361</guid>
<content:encoded><![CDATA[
arXiv:2505.18361v3 Announce Type: replace-cross 
Abstract: Tactile sensing remains far less understood in neuroscience and less effective in artificial systems compared to more mature modalities such as vision and language. We bridge these gaps by introducing a novel Encoder-Attender-Decoder (EAD) framework to systematically explore the space of task-optimized temporal neural networks trained on realistic tactile input sequences from a customized rodent whisker-array simulator. We identify convolutional recurrent neural networks (ConvRNNs) as superior encoders to purely feedforward and state-space architectures for tactile categorization. Crucially, these ConvRNN-encoder-based EAD models achieve neural representations closely matching rodent somatosensory cortex, saturating the explainable neural variability and revealing a clear linear relationship between supervised categorization performance and neural alignment. Furthermore, contrastive self-supervised ConvRNN-encoder-based EADs, trained with tactile-specific augmentations, match supervised neural fits, serving as an ethologically-relevant, label-free proxy.
  For neuroscience, our findings highlight nonlinear recurrent processing as important for general-purpose tactile representations in somatosensory cortex, providing the first quantitative characterization of the underlying inductive biases in this system. For embodied AI, our results emphasize the importance of recurrent EAD architectures to handle realistic tactile inputs, along with tailored self-supervised learning methods for achieving robust tactile perception with the same type of sensors animals use to sense in unstructured environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of LLM $\times$ DATA</title>
<link>https://arxiv.org/abs/2505.18458</link>
<guid>https://arxiv.org/abs/2505.18458</guid>
<content:encoded><![CDATA[
arXiv:2505.18458v3 Announce Type: replace-cross 
Abstract: The integration of large language model (LLM) and data management (DATA) is rapidly redefining both domains. In this survey, we comprehensively review the bidirectional relationships. On the one hand, DATA4LLM, spanning large-scale data processing, storage, and serving, feeds LLMs with high quality, diversity, and timeliness of data required for stages like pre-training, post-training, retrieval-augmented generation, and agentic workflows: (i) Data processing for LLMs includes scalable acquisition, deduplication, filtering, selection, domain mixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on efficient data and model formats, distributed and heterogeneous storage hierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data serving for LLMs tackles challenges in RAG (e.g., knowledge post-processing), LLM inference (e.g., prompt compression, data provenance), and training strategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA, LLMs are emerging as general-purpose engines for data management. We review recent advances in (i) data manipulation, including automatic data cleaning, integration, discovery; (ii) data analysis, covering reasoning over structured, semi-structured, and unstructured data, and (iii) system optimization (e.g., configuration tuning, query rewriting, anomaly diagnosis), powered by LLM techniques like retrieval-augmented prompting, task-specialized fine-tuning, and multi-agent collaboration.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Security Concerns for Large Language Models: A Survey</title>
<link>https://arxiv.org/abs/2505.18889</link>
<guid>https://arxiv.org/abs/2505.18889</guid>
<content:encoded><![CDATA[
arXiv:2505.18889v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) such as GPT-4 and its recent iterations, Google's Gemini, Anthropic's Claude 3 models, and xAI's Grok have caused a revolution in natural language processing, but their capabilities also introduce new security vulnerabilities. In this survey, we provide a comprehensive overview of the emerging security concerns around LLMs, categorizing threats into prompt injection and jailbreaking, adversarial attacks such as input perturbations and data poisoning, misuse by malicious actors for purposes such as generating disinformation, phishing emails, and malware, and worrisome risks inherent in autonomous LLM agents. A significant focus has been recently placed on the latter, exploring goal misalignment, emergent deception, self-preservation instincts, and the potential for LLMs to develop and pursue covert, misaligned objectives, a behavior known as scheming, which may even persist through safety training. We summarize recent academic and industrial studies from 2022 to 2025 that exemplify each threat, analyze proposed defenses and their limitations, and identify open challenges in securing LLM-based applications. We conclude by emphasizing the importance of advancing robust, multi-layered security strategies to ensure LLMs are safe and beneficial.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfoChartQA: A Benchmark for Multimodal Question Answering on Infographic Charts</title>
<link>https://arxiv.org/abs/2505.19028</link>
<guid>https://arxiv.org/abs/2505.19028</guid>
<content:encoded><![CDATA[
arXiv:2505.19028v2 Announce Type: replace-cross 
Abstract: Understanding infographic charts with design-driven visual elements (e.g., pictograms, icons) requires both visual recognition and reasoning, posing challenges for multimodal large language models (MLLMs). However, existing visual-question answering benchmarks fall short in evaluating these capabilities of MLLMs due to the lack of paired plain charts and visual-element-based questions. To bridge this gap, we introduce InfoChartQA, a benchmark for evaluating MLLMs on infographic chart understanding. It includes 5,642 pairs of infographic and plain charts, each sharing the same underlying data but differing in visual presentations. We further design visual-element-based questions to capture their unique visual designs and communicative intent. Evaluation of 20 MLLMs reveals a substantial performance decline on infographic charts, particularly for visual-element-based questions related to metaphors. The paired infographic and plain charts enable fine-grained error analysis and ablation studies, which highlight new opportunities for advancing MLLMs in infographic chart understanding. We release InfoChartQA at https://github.com/CoolDawnAnt/InfoChartQA.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Interpretable Representation Learning Approach for Diffusion Tensor Imaging</title>
<link>https://arxiv.org/abs/2505.19110</link>
<guid>https://arxiv.org/abs/2505.19110</guid>
<content:encoded><![CDATA[
arXiv:2505.19110v2 Announce Type: replace-cross 
Abstract: Diffusion Tensor Imaging (DTI) tractography offers detailed insights into the structural connectivity of the brain, but presents challenges in effective representation and interpretation in deep learning models. In this work, we propose a novel 2D representation of DTI tractography that encodes tract-level fractional anisotropy (FA) values into a 9x9 grayscale image. This representation is processed through a Beta-Total Correlation Variational Autoencoder with a Spatial Broadcast Decoder to learn a disentangled and interpretable latent embedding. We evaluate the quality of this embedding using supervised and unsupervised representation learning strategies, including auxiliary classification, triplet loss, and SimCLR-based contrastive learning. Compared to the 1D Group deep neural network (DNN) baselines, our approach improves the F1 score in a downstream sex classification task by 15.74% and shows a better disentanglement than the 3D representation.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoloSpeech: Enhancing Intelligibility and Quality in Target Speech Extraction through a Cascaded Generative Pipeline</title>
<link>https://arxiv.org/abs/2505.19314</link>
<guid>https://arxiv.org/abs/2505.19314</guid>
<content:encoded><![CDATA[
arXiv:2505.19314v2 Announce Type: replace-cross 
Abstract: Target Speech Extraction (TSE) aims to isolate a target speaker's voice from a mixture of multiple speakers by leveraging speaker-specific cues, typically provided as auxiliary audio (a.k.a. cue audio). Although recent advancements in TSE have primarily employed discriminative models that offer high perceptual quality, these models often introduce unwanted artifacts, reduce naturalness, and are sensitive to discrepancies between training and testing environments. On the other hand, generative models for TSE lag in perceptual quality and intelligibility. To address these challenges, we present SoloSpeech, a novel cascaded generative pipeline that integrates compression, extraction, reconstruction, and correction processes. SoloSpeech features a speaker-embedding-free target extractor that utilizes conditional information from the cue audio's latent space, aligning it with the mixture audio's latent space to prevent mismatches. Evaluated on the widely-used Libri2Mix dataset, SoloSpeech achieves the new state-of-the-art intelligibility and quality in target speech extraction and speech separation tasks while demonstrating exceptional generalization on out-of-domain data and real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuSym-RAG: Hybrid Neural Symbolic Retrieval with Multiview Structuring for PDF Question Answering</title>
<link>https://arxiv.org/abs/2505.19754</link>
<guid>https://arxiv.org/abs/2505.19754</guid>
<content:encoded><![CDATA[
arXiv:2505.19754v2 Announce Type: replace-cross 
Abstract: The increasing number of academic papers poses significant challenges for researchers to efficiently acquire key details. While retrieval augmented generation (RAG) shows great promise in large language model (LLM) based automated question answering, previous works often isolate neural and symbolic retrieval despite their complementary strengths. Moreover, conventional single-view chunking neglects the rich structure and layout of PDFs, e.g., sections and tables. In this work, we propose NeuSym-RAG, a hybrid neural symbolic retrieval framework which combines both paradigms in an interactive process. By leveraging multi-view chunking and schema-based parsing, NeuSym-RAG organizes semi-structured PDF content into both the relational database and vectorstore, enabling LLM agents to iteratively gather context until sufficient to generate answers. Experiments on three full PDF-based QA datasets, including a self-annotated one AIRQA-REAL, show that NeuSym-RAG stably defeats both the vector-based RAG and various structured baselines, highlighting its capacity to unify both retrieval schemes and utilize multiple views. Code and data are publicly available at https://github.com/X-LANCE/NeuSym-RAG.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Homophily Enhanced Graph Domain Adaptation</title>
<link>https://arxiv.org/abs/2505.20089</link>
<guid>https://arxiv.org/abs/2505.20089</guid>
<content:encoded><![CDATA[
arXiv:2505.20089v3 Announce Type: replace-cross 
Abstract: Graph Domain Adaptation (GDA) transfers knowledge from labeled source graphs to unlabeled target graphs, addressing the challenge of label scarcity. In this paper, we highlight the significance of graph homophily, a pivotal factor for graph domain alignment, which, however, has long been overlooked in existing approaches. Specifically, our analysis first reveals that homophily discrepancies exist in benchmarks. Moreover, we also show that homophily discrepancies degrade GDA performance from both empirical and theoretical aspects, which further underscores the importance of homophily alignment in GDA. Inspired by this finding, we propose a novel homophily alignment algorithm that employs mixed filters to smooth graph signals, thereby effectively capturing and mitigating homophily discrepancies between graphs. Experimental results on a variety of benchmarks verify the effectiveness of our method.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Retrieval with Evidence Curation for Open-Domain Financial Question Answering on Standardized Documents</title>
<link>https://arxiv.org/abs/2505.20368</link>
<guid>https://arxiv.org/abs/2505.20368</guid>
<content:encoded><![CDATA[
arXiv:2505.20368v2 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) based large language models (LLMs) are widely used in finance for their excellent performance on knowledge-intensive tasks. However, standardized documents (e.g., SEC filing) share similar formats such as repetitive boilerplate texts, and similar table structures. This similarity forces traditional RAG methods to misidentify near-duplicate text, leading to duplicate retrieval that undermines accuracy and completeness. To address these issues, we propose the Hierarchical Retrieval with Evidence Curation (HiREC) framework. Our approach first performs hierarchical retrieval to reduce confusion among similar texts. It first retrieve related documents and then selects the most relevant passages from the documents. The evidence curation process removes irrelevant passages. When necessary, it automatically generates complementary queries to collect missing information. To evaluate our approach, we construct and release a Large-scale Open-domain Financial (LOFin) question answering benchmark that includes 145,897 SEC documents and 1,595 question-answer pairs. Our code and data are available at https://github.com/deep-over/LOFin-bench-HiREC.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial bandit optimization for approximately linear functions</title>
<link>https://arxiv.org/abs/2505.20734</link>
<guid>https://arxiv.org/abs/2505.20734</guid>
<content:encoded><![CDATA[
arXiv:2505.20734v2 Announce Type: replace-cross 
Abstract: We consider a bandit optimization problem for nonconvex and non-smooth functions, where in each trial the loss function is the sum of a linear function and a small but arbitrary perturbation chosen after observing the player's choice. We give both expected and high probability regret bounds for the problem. Our result also implies an improved high-probability regret bound for the bandit linear optimization, a special case with no perturbation. We also give a lower bound on the expected regret.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Do Transformers Learn Variable Binding in Symbolic Programs?</title>
<link>https://arxiv.org/abs/2505.20896</link>
<guid>https://arxiv.org/abs/2505.20896</guid>
<content:encoded><![CDATA[
arXiv:2505.20896v2 Announce Type: replace-cross 
Abstract: Variable binding -- the ability to associate variables with values -- is fundamental to symbolic computation and cognition. Although classical architectures typically implement variable binding via addressable memory, it is not well understood how modern neural networks lacking built-in binding operations may acquire this capacity. We investigate this by training a Transformer to dereference queried variables in symbolic programs where variables are assigned either numerical constants or other variables. Each program requires following chains of variable assignments up to four steps deep to find the queried value, and also contains irrelevant chains of assignments acting as distractors. Our analysis reveals a developmental trajectory with three distinct phases during training: (1) random prediction of numerical constants, (2) a shallow heuristic prioritizing early variable assignments, and (3) the emergence of a systematic mechanism for dereferencing assignment chains. Using causal interventions, we find that the model learns to exploit the residual stream as an addressable memory space, with specialized attention heads routing information across token positions. This mechanism allows the model to dynamically track variable bindings across layers, resulting in accurate dereferencing. Our results show how Transformer models can learn to implement systematic variable binding without explicit architectural support, bridging connectionist and symbolic approaches. To facilitate reproducible research, we developed Variable Scope, an interactive web platform for exploring our findings at https://variablescope.org
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Value-Function Uncertainties</title>
<link>https://arxiv.org/abs/2505.21119</link>
<guid>https://arxiv.org/abs/2505.21119</guid>
<content:encoded><![CDATA[
arXiv:2505.21119v2 Announce Type: replace-cross 
Abstract: Estimating epistemic uncertainty in value functions is a crucial challenge for many aspects of reinforcement learning (RL), including efficient exploration, safe decision-making, and offline RL. While deep ensembles provide a robust method for quantifying value uncertainty, they come with significant computational overhead. Single-model methods, while computationally favorable, often rely on heuristics and typically require additional propagation mechanisms for myopic uncertainty estimates. In this work we introduce universal value-function uncertainties (UVU), which, similar in spirit to random network distillation (RND), quantify uncertainty as squared prediction errors between an online learner and a fixed, randomly initialized target network. Unlike RND, UVU errors reflect policy-conditional value uncertainty, incorporating the future uncertainties any given policy may encounter. This is due to the training procedure employed in UVU: the online network is trained using temporal difference learning with a synthetic reward derived from the fixed, randomly initialized target network. We provide an extensive theoretical analysis of our approach using neural tangent kernel (NTK) theory and show that in the limit of infinite network width, UVU errors are exactly equivalent to the variance of an ensemble of independent universal value functions. Empirically, we show that UVU achieves equal performance to large ensembles on challenging multi-task offline RL settings, while offering simplicity and substantial computational savings.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hume: Introducing System-2 Thinking in Visual-Language-Action Model</title>
<link>https://arxiv.org/abs/2505.21432</link>
<guid>https://arxiv.org/abs/2505.21432</guid>
<content:encoded><![CDATA[
arXiv:2505.21432v3 Announce Type: replace-cross 
Abstract: Humans practice slow thinking before performing actual actions when handling complex tasks in the physical world. This thinking paradigm, recently, has achieved remarkable advancement in boosting Large Language Models (LLMs) to solve complex tasks in digital domains. However, the potential of slow thinking remains largely unexplored for robotic foundation models interacting with the physical world. In this work, we propose Hume: a dual-system Vision-Language-Action (VLA) model with value-guided System-2 thinking and cascaded action denoising, exploring human-like thinking capabilities of Vision-Language-Action models for dexterous robot control. System 2 of Hume implements value-Guided thinking by extending a Vision-Language-Action Model backbone with a novel value-query head to estimate the state-action value of predicted actions. The value-guided thinking is conducted by repeat sampling multiple action candidates and selecting one according to state-action value. System 1 of Hume is a lightweight reactive visuomotor policy that takes System 2 selected action and performs cascaded action denoising for dexterous robot control. At deployment time, System 2 performs value-guided thinking at a low frequency while System 1 asynchronously receives the System 2 selected action candidate and predicts fluid actions in real time. We show that Hume outperforms the existing state-of-the-art Vision-Language-Action models across multiple simulation benchmark and real-robot deployments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoReproduce: Automatic AI Experiment Reproduction with Paper Lineage</title>
<link>https://arxiv.org/abs/2505.20662</link>
<guid>https://arxiv.org/abs/2505.20662</guid>
<content:encoded><![CDATA[
<div> Keywords: experiment reproduction, artificial intelligence, AutoReproduce, paper lineage algorithm, code executability

Summary:
Efficient experiment reproduction is essential for advancing progress in artificial intelligence. The complexity of method design and training procedures poses challenges for automation, as implicit domain-specific knowledge is often required. The paper lineage algorithm and AutoReproduce framework address this issue by extracting implicit knowledge from cited references and automatically reproducing experiments from research papers. AutoReproduce enhances code executability by generating unit tests during the reproduction process. The ReproduceBench benchmark, with verified implementations, and novel evaluation metrics are introduced to assess reproduction and execution fidelity. Experimental results show AutoReproduce outperforms existing agent baselines by over 70% on all evaluation metrics. It achieves an average performance gap of 22.1% compared to official implementations on 89.74% of executable experiment runs. The code for AutoReproduce will be available at https://github.com/AI9Stars/AutoReproduce. 

<br /><br />Summary: <div>
arXiv:2505.20662v2 Announce Type: replace 
Abstract: Efficient experiment reproduction is critical to accelerating progress in artificial intelligence. However, the inherent complexity of method design and training procedures presents substantial challenges for automation. Notably, reproducing experiments often requires implicit domain-specific knowledge not explicitly documented in the original papers. To address this, we introduce the paper lineage algorithm, which identifies and extracts implicit knowledge from the relevant references cited by the target paper. Building on this idea, we propose AutoReproduce, a multi-agent framework capable of automatically reproducing experiments described in research papers in an end-to-end manner. AutoReproduce enhances code executability by generating unit tests alongside the reproduction process. To evaluate the reproduction capability, we construct ReproduceBench, a benchmark annotated with verified implementations, and introduce novel evaluation metrics to assess both the reproduction and execution fidelity. Experimental results demonstrate that AutoReproduce outperforms the existing strong agent baselines on all five evaluation metrics by a peak margin of over $70\%$. In particular, compared to the official implementations, AutoReproduce achieves an average performance gap of $22.1\%$ on $89.74\%$ of the executable experiment runs. The code will be available at https://github.com/AI9Stars/AutoReproduce.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controllable Context Sensitivity and the Knob Behind It</title>
<link>https://arxiv.org/abs/2411.07404</link>
<guid>https://arxiv.org/abs/2411.07404</guid>
<content:encoded><![CDATA[
<div> sensitivity, language model, context, prior knowledge, instruction-tuned  
Summary:  
In this paper, the researchers explore the concept of context sensitivity in language models and propose a task to analyze and control this sensitivity. They introduce a knob that allows the model to choose between relying on context or prior knowledge when making predictions. Through fine-tuning on a specific task, they demonstrate that models can achieve high accuracy in determining the appropriate response based on context. By analyzing the high-performing models, the researchers identify key layers that influence context sensitivity and discover a specific subspace within a single layer that determines whether the model follows context or prior knowledge. Surprisingly, this subspace serves as an effective control mechanism across different model variations. The study finds a strong correlation between a model's performance and its ability to differentiate context-agreeing and context-ignoring answers in this subspace, suggesting a simple yet fundamental mechanism for controlling language model behavior.  
<br /><br />Summary: <div>
arXiv:2411.07404v4 Announce Type: replace-cross 
Abstract: When making predictions, a language model must trade off how much it relies on its context vs. its prior knowledge. Choosing how sensitive the model is to its context is a fundamental functionality, as it enables the model to excel at tasks like retrieval-augmented generation and question-answering. In this paper, we search for a knob which controls this sensitivity, determining whether language models answer from the context or their prior knowledge. To guide this search, we design a task for controllable context sensitivity. In this task, we first feed the model a context (Paris is in England) and a question (Where is Paris?); we then instruct the model to either use its prior or contextual knowledge and evaluate whether it generates the correct answer for both intents (either France or England). When fine-tuned on this task, instruction-tuned versions of Llama-3.1, Mistral-v0.3, and Gemma-2 can solve it with high accuracy (85-95%). Analyzing these high-performing models, we narrow down which layers may be important to context sensitivity using a novel linear time algorithm. Then, in each model, we identify a 1-D subspace in a single layer that encodes whether the model follows context or prior knowledge. Interestingly, while we identify this subspace in a fine-tuned model, we find that the exact same subspace serves as an effective knob in not only that model but also non-fine-tuned instruct and base models of that model family. Finally, we show a strong correlation between a model's performance and how distinctly it separates context-agreeing from context-ignoring answers in this subspace. These results suggest a single subspace facilitates how the model chooses between context and prior knowledge, hinting at a simple fundamental mechanism that controls this behavior.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BatteryLife: A Comprehensive Dataset and Benchmark for Battery Life Prediction</title>
<link>https://arxiv.org/abs/2502.18807</link>
<guid>https://arxiv.org/abs/2502.18807</guid>
<content:encoded><![CDATA[
<div> battery life prediction, dataset, benchmark, CyclePatch, neural networks

Summary:<br />
The article introduces BatteryLife, a dataset and benchmark for Battery Life Prediction (BLP). BatteryLife addresses three key challenges in BLP research by providing a comprehensive dataset with a larger sample size, diverse battery types and testing conditions. The dataset includes batteries from different formats, chemical systems, operating temperatures, and charge/discharge protocols, including zinc-ion and sodium-ion batteries. The article also introduces CyclePatch, a plug-in technique for neural networks that improves performance in BLP. Benchmarking on 18 methods shows that popular models in other time series fields may be ineffective for BLP, while CyclePatch consistently enhances model performance, setting new benchmarks in the field. BatteryLife evaluates model performance across aging conditions and domains, providing a valuable resource for battery utilization, optimization, and production. The dataset is available on GitHub for further research and development.<br /><br />Summary: <div>
arXiv:2502.18807v5 Announce Type: replace-cross 
Abstract: Battery Life Prediction (BLP), which relies on time series data produced by battery degradation tests, is crucial for battery utilization, optimization, and production. Despite impressive advancements, this research area faces three key challenges. Firstly, the limited size of existing datasets impedes insights into modern battery life data. Secondly, most datasets are restricted to small-capacity lithium-ion batteries tested under a narrow range of diversity in labs, raising concerns about the generalizability of findings. Thirdly, inconsistent and limited benchmarks across studies obscure the effectiveness of baselines and leave it unclear if models popular in other time series fields are effective for BLP. To address these challenges, we propose BatteryLife, a comprehensive dataset and benchmark for BLP. BatteryLife integrates 16 datasets, offering a 2.5 times sample size compared to the previous largest dataset, and provides the most diverse battery life resource with batteries from 8 formats, 59 chemical systems, 9 operating temperatures, and 421 charge/discharge protocols, including both laboratory and industrial tests. Notably, BatteryLife is the first to release battery life datasets of zinc-ion batteries, sodium-ion batteries, and industry-tested large-capacity lithium-ion batteries. With the comprehensive dataset, we revisit the effectiveness of baselines popular in this and other time series fields. Furthermore, we propose CyclePatch, a plug-in technique that can be employed in various neural networks. Extensive benchmarking of 18 methods reveals that models popular in other time series fields can be unsuitable for BLP, and CyclePatch consistently improves model performance establishing state-of-the-art benchmarks. Moreover, BatteryLife evaluates model performance across aging conditions and domains. BatteryLife is available at https://github.com/Ruifeng-Tan/BatteryLife.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information Science Principles of Machine Learning: A Causal Chain Meta-Framework Based on Formalized Information Mapping</title>
<link>https://arxiv.org/abs/2505.13182</link>
<guid>https://arxiv.org/abs/2505.13182</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, formal information model, interpretability, ethical safety, theoretical framework

Summary:
A new study presents a formal theoretical framework in machine learning to address interpretability and ethical safety concerns. The research constructs an information model using well-formed formulas to define machine learning components and introduces predicates and functions for logical analysis. A meta-framework for machine learning theory (MLT-MF) is proposed, with universal definitions for model interpretability and ethical safety. The study proves key theorems on model interpretability, ethical safety assurance, and generalization error estimation. Limitations include ideal conditions assumption and focus on static scenarios, without addressing information fusion in complex systems. The work provides a unified foundation for addressing challenges in machine learning, overcoming fragmented research. 

<br /><br />Summary: <div>
arXiv:2505.13182v3 Announce Type: replace-cross 
Abstract: [Objective] This study focuses on addressing the current lack of a unified formal theoretical framework in machine learning, as well as the deficiencies in interpretability and ethical safety assurance. [Methods] A formal information model is first constructed, utilizing sets of well-formed formulas to explicitly define the ontological states and carrier mappings of typical components in machine learning. Learnable and processable predicates, along with learning and processing functions, are introduced to analyze the logical deduction and constraint rules of the causal chains within models. [Results] A meta-framework for machine learning theory (MLT-MF) is established. Based on this framework, universal definitions for model interpretability and ethical safety are proposed. Furthermore, three key theorems are proved: the equivalence of model interpretability and information recoverability, the assurance of ethical safety, and the estimation of generalization error. [Limitations] The current framework assumes ideal conditions with noiseless information-enabling mappings and primarily targets model learning and processing logic in static scenarios. It does not yet address information fusion and conflict resolution across ontological spaces in multimodal or multi-agent systems. [Conclusions] This work overcomes the limitations of fragmented research and provides a unified theoretical foundation for systematically addressing the critical challenges currently faced in machine learning.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Avoid Forgetting by Preserving Global Knowledge Gradients in Federated Learning with Non-IID Data</title>
<link>https://arxiv.org/abs/2505.20485</link>
<guid>https://arxiv.org/abs/2505.20485</guid>
<content:encoded><![CDATA[
<div> Keywords: federated learning, data heterogeneity, decision boundary, forgetting, ensemble knowledge fusion

Summary:
FedProj is proposed as a solution to the challenge of data heterogeneity in federated learning. The study reveals that existing methods suffer from forgetting the global decision boundary, leading to clients only learning the local one. FedProj addresses this by introducing a server-side ensemble knowledge transfer loss to improve ensemble knowledge fusion and prevent forgetting the global decision boundary during local training. Additionally, an episodic memory is utilized to regulate gradient updates and maintain the learned global decision boundary. Experimental results show that FedProj significantly outperforms current state-of-the-art methods, providing a robust solution to the issue of data heterogeneity in federated learning.<br /><br />Summary: <div>
arXiv:2505.20485v2 Announce Type: replace-cross 
Abstract: The inevitable presence of data heterogeneity has made federated learning very challenging. There are numerous methods to deal with this issue, such as local regularization, better model fusion techniques, and data sharing. Though effective, they lack a deep understanding of how data heterogeneity can affect the global decision boundary. In this paper, we bridge this gap by performing an experimental analysis of the learned decision boundary using a toy example. Our observations are surprising: (1) we find that the existing methods suffer from forgetting and clients forget the global decision boundary and only learn the perfect local one, and (2) this happens regardless of the initial weights, and clients forget the global decision boundary even starting from pre-trained optimal weights. In this paper, we present FedProj, a federated learning framework that robustly learns the global decision boundary and avoids its forgetting during local training. To achieve better ensemble knowledge fusion, we design a novel server-side ensemble knowledge transfer loss to further calibrate the learned global decision boundary. To alleviate the issue of learned global decision boundary forgetting, we further propose leveraging an episodic memory of average ensemble logits on a public unlabeled dataset to regulate the gradient updates at each step of local training. Experimental results demonstrate that FedProj outperforms state-of-the-art methods by a large margin.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Binarized Neural Networks Converge Toward Algorithmic Simplicity: Empirical Support for the Learning-as-Compression Hypothesis</title>
<link>https://arxiv.org/abs/2505.20646</link>
<guid>https://arxiv.org/abs/2505.20646</guid>
<content:encoded><![CDATA[
<div> algorithmic information theory, neural networks, Binarized Neural Networks, Block Decomposition Method, algorithmic complexity

Summary:<br />
The study focuses on understanding and controlling the informational complexity of neural networks by shifting towards algorithmic information theory. By using Binarized Neural Networks (BNNs) as a proxy, the researchers apply the Block Decomposition Method (BDM) based on algorithmic probability to track learning dynamics during training. The results show that the BDM method more accurately captures structural changes during training compared to entropy-based approaches, exhibiting stronger correlations with training loss across different model sizes and training runs. The findings suggest that training can be viewed as a process of algorithmic compression, where learning involves internalizing structured regularities. This work provides a principled estimate of learning progression and proposes a complexity-aware learning and regularization framework grounded in information theory principles. <div>
arXiv:2505.20646v2 Announce Type: replace-cross 
Abstract: Understanding and controlling the informational complexity of neural networks is a central challenge in machine learning, with implications for generalization, optimization, and model capacity. While most approaches rely on entropy-based loss functions and statistical metrics, these measures often fail to capture deeper, causally relevant algorithmic regularities embedded in network structure. We propose a shift toward algorithmic information theory, using Binarized Neural Networks (BNNs) as a first proxy. Grounded in algorithmic probability (AP) and the universal distribution it defines, our approach characterizes learning dynamics through a formal, causally grounded lens. We apply the Block Decomposition Method (BDM) -- a scalable approximation of algorithmic complexity based on AP -- and demonstrate that it more closely tracks structural changes during training than entropy, consistently exhibiting stronger correlations with training loss across varying model sizes and randomized training runs. These results support the view of training as a process of algorithmic compression, where learning corresponds to the progressive internalization of structured regularities. In doing so, our work offers a principled estimate of learning progression and suggests a framework for complexity-aware learning and regularization, grounded in first principles from information theory, complexity, and computability.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interactive OT Gym: A Reinforcement Learning-Based Interactive Optical tweezer (OT)-Driven Microrobotics Simulation Platform</title>
<link>https://arxiv.org/abs/2505.20751</link>
<guid>https://arxiv.org/abs/2505.20751</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, optical tweezers, microrobotics, haptic feedback, shared control<br />
<br />
Summary: <br />
The article introduces the Interactive OT Gym, a simulation platform for optical tweezers-driven microrobotics. This platform combines complex physical field simulations with haptic feedback interfaces, reinforcement learning modules, and shared control strategies optimized for cooperative manipulation tasks. It allows for a seamless transition between manual and autonomous control, enhancing micromanipulation performance. In a cell manipulation task evaluation, the shared control system reduced task completion time by 67% compared to pure human or RL control and achieved a 100% success rate. Interactive OT Gym provides a user-friendly environment for developing advanced OT-driven micromanipulation systems and control algorithms, offering high fidelity, interactivity, low cost, and high-speed simulation capabilities. Visit the project website for more details. <div>
arXiv:2505.20751v2 Announce Type: replace-cross 
Abstract: Optical tweezers (OT) offer unparalleled capabilities for micromanipulation with submicron precision in biomedical applications. However, controlling conventional multi-trap OT to achieve cooperative manipulation of multiple complex-shaped microrobots in dynamic environments poses a significant challenge. To address this, we introduce Interactive OT Gym, a reinforcement learning (RL)-based simulation platform designed for OT-driven microrobotics. Our platform supports complex physical field simulations and integrates haptic feedback interfaces, RL modules, and context-aware shared control strategies tailored for OT-driven microrobot in cooperative biological object manipulation tasks. This integration allows for an adaptive blend of manual and autonomous control, enabling seamless transitions between human input and autonomous operation. We evaluated the effectiveness of our platform using a cell manipulation task. Experimental results show that our shared control system significantly improves micromanipulation performance, reducing task completion time by approximately 67% compared to using pure human or RL control alone and achieving a 100% success rate. With its high fidelity, interactivity, low cost, and high-speed simulation capabilities, Interactive OT Gym serves as a user-friendly training and testing environment for the development of advanced interactive OT-driven micromanipulation systems and control algorithms. For more details on the project, please see our website https://sites.google.com/view/otgym
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CogniBench: A Legal-inspired Framework and Dataset for Assessing Cognitive Faithfulness of Large Language Models</title>
<link>https://arxiv.org/abs/2505.20767</link>
<guid>https://arxiv.org/abs/2505.20767</guid>
<content:encoded><![CDATA[
<div> Keywords: Faithfulness hallucinations, Large Language Model, CogniBench dataset, cognitive statements, automatic annotation pipeline<br />
<br />
Summary: <br />
The article discusses faithfulness hallucinations in Large Language Models (LLMs) and the challenges in evaluating cognitive statements. Existing benchmarks focus on rephrased factual statements, overlooking cognitive statements that involve making inferences from context. To address this, the authors present a rigorous framework for assessing faithfulness levels of cognitive statements and introduce the CogniBench dataset. They further develop an automatic annotation pipeline to scale across different LLMs, resulting in the CogniBench-L dataset. The datasets facilitate training accurate detectors for both factual and cognitive hallucinations. The authors release their model and datasets, aiming to improve the evaluation and detection of hallucinations in LLM-generated content. <div>
arXiv:2505.20767v3 Announce Type: replace-cross 
Abstract: Faithfulness hallucinations are claims generated by a Large Language Model (LLM) not supported by contexts provided to the LLM. Lacking assessment standards, existing benchmarks focus on "factual statements" that rephrase source materials while overlooking "cognitive statements" that involve making inferences from the given context. Consequently, evaluating and detecting the hallucination of cognitive statements remains challenging. Inspired by how evidence is assessed in the legal domain, we design a rigorous framework to assess different levels of faithfulness of cognitive statements and introduce the CogniBench dataset where we reveal insightful statistics. To keep pace with rapidly evolving LLMs, we further develop an automatic annotation pipeline that scales easily across different models. This results in a large-scale CogniBench-L dataset, which facilitates training accurate detectors for both factual and cognitive hallucinations. We release our model and datasets at: https://github.com/FUTUREEEEEE/CogniBench
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Reasoning Models to Generate Search Heuristics that Solve Open Instances of Combinatorial Design Problems</title>
<link>https://arxiv.org/abs/2505.23881</link>
<guid>https://arxiv.org/abs/2505.23881</guid>
<content:encoded><![CDATA[
<div> combinatorial design, large language models, reasoning, code generation, mathematical field<br />
<br />
Summary: 
This article introduces the application of Large Language Models (LLMs) with reasoning to combinatorial design problems. The Constructive Protocol CPro1 utilizes reasoning LLMs to generate search heuristics for solving open instances in combinatorial designs. By providing strategies, automated hyperparameter tuning, and execution feedback, CPro1 with reasoning LLMs successfully solves long-standing open instances for 7 out of 16 selected combinatorial design problems, including new instances for 3 of them. It also solves open instances for problems from recent literature, generating new Covering Sequences, Johnson Clique Covers, Deletion Codes, and a Uniform Nested Steiner Quadruple System. This approach demonstrates the effectiveness of reasoning LLMs in tackling combinatorial design challenges and advancing the field. <br /><br />Summary: <div>
arXiv:2505.23881v1 Announce Type: new 
Abstract: Large Language Models (LLMs) with reasoning are trained to iteratively generate and refine their answers before finalizing them, which can help with applications to mathematics and code generation. We apply code generation with reasoning LLMs to a specific task in the mathematical field of combinatorial design. This field studies diverse types of combinatorial designs, many of which have lists of open instances for which existence has not yet been determined. The Constructive Protocol CPro1 uses LLMs to generate search heuristics that have the potential to construct solutions to small open instances. Starting with a textual definition and a validity verifier for a particular type of design, CPro1 guides LLMs to select and implement strategies, while providing automated hyperparameter tuning and execution feedback. CPro1 with reasoning LLMs successfully solves long-standing open instances for 7 of 16 combinatorial design problems selected from the 2006 Handbook of Combinatorial Designs, including new solved instances for 3 of these (Bhaskar Rao Designs, Symmetric Weighing Matrices, Balanced Ternary Designs) that were unsolved by CPro1 with non-reasoning LLMs. It also solves open instances for several problems from recent (2025) literature, generating new Covering Sequences, Johnson Clique Covers, Deletion Codes, and a Uniform Nested Steiner Quadruple System.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation</title>
<link>https://arxiv.org/abs/2505.23885</link>
<guid>https://arxiv.org/abs/2505.23885</guid>
<content:encoded><![CDATA[
<div> hierarchical multi-agent framework, task decomposition, subtask management, domain-specific tool-calling, Optimized Workforce Learning <br />
Summary:
The article introduces Workforce, a hierarchical multi-agent framework that separates strategic planning from specialized execution using a modular architecture. It consists of a domain-agnostic Planner, a Coordinator for subtask management, and specialized Workers with domain-specific tool-calling capabilities. This decoupling allows for cross-domain transferability during both inference and training phases. Workforce adapts to new domains by adding or modifying worker agents, and Optimized Workforce Learning (OWL) improves generalization across domains through reinforcement learning. Experimental results on the GAIA benchmark show that Workforce achieves state-of-the-art performance, surpassing commercial systems like OpenAI's Deep Research. The OWL-trained 32B model exhibits a significant improvement in accuracy and competes with models like GPT-4o on challenging tasks. This work paves the way for the development of general-purpose AI assistants with scalable generalization and modular domain transfer capabilities. <br /><br /> <div>
arXiv:2505.23885v1 Announce Type: new 
Abstract: Large Language Model (LLM)-based multi-agent systems show promise for automating real-world tasks but struggle to transfer across domains due to their domain-specific nature. Current approaches face two critical shortcomings: they require complete architectural redesign and full retraining of all components when applied to new domains. We introduce Workforce, a hierarchical multi-agent framework that decouples strategic planning from specialized execution through a modular architecture comprising: (i) a domain-agnostic Planner for task decomposition, (ii) a Coordinator for subtask management, and (iii) specialized Workers with domain-specific tool-calling capabilities. This decoupling enables cross-domain transferability during both inference and training phases: During inference, Workforce seamlessly adapts to new domains by adding or modifying worker agents; For training, we introduce Optimized Workforce Learning (OWL), which improves generalization across domains by optimizing a domain-agnostic planner with reinforcement learning from real-world feedback. To validate our approach, we evaluate Workforce on the GAIA benchmark, covering various realistic, multi-domain agentic tasks. Experimental results demonstrate Workforce achieves open-source state-of-the-art performance (69.70%), outperforming commercial systems like OpenAI's Deep Research by 2.34%. More notably, our OWL-trained 32B model achieves 52.73% accuracy (+16.37%) and demonstrates performance comparable to GPT-4o on challenging tasks. To summarize, by enabling scalable generalization and modular domain transfer, our work establishes a foundation for the next generation of general-purpose AI assistants.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lessons Learned: A Multi-Agent Framework for Code LLMs to Learn and Improve</title>
<link>https://arxiv.org/abs/2505.23946</link>
<guid>https://arxiv.org/abs/2505.23946</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, code optimization, collaboration, lesson-based framework, performance

Summary:
In recent studies, it has been revealed that Large Language Models (LLMs) exhibit different skills and excel in various tasks, showing varied performance levels. Specifically in code optimization, different LLMs excel in different optimization categories without one dominating the others. This raises the question of how to effectively leverage multiple LLM agents to solve coding problems when their complementary strengths are not known beforehand. The proposed solution is a lesson-based collaboration framework where agents learn from each other's successes and failures, sharing knowledge through lessons learned in a collective solution process. The framework includes a mechanism for lesson solicitation, banking, and selection. Through experiments, it has been shown that a team of small LLMs with shared lessons can outperform a larger LLM and other multi-LLM collaboration methods. <div>
arXiv:2505.23946v1 Announce Type: new 
Abstract: Recent studies show that LLMs possess different skills and specialize in different tasks. In fact, we observe that their varied performance occur in several levels of granularity. For example, in the code optimization task, code LLMs excel at different optimization categories and no one dominates others. This observation prompts the question of how one leverages multiple LLM agents to solve a coding problem without knowing their complementary strengths a priori. We argue that a team of agents can learn from each other's successes and failures so as to improve their own performance. Thus, a lesson is the knowledge produced by an agent and passed on to other agents in the collective solution process. We propose a lesson-based collaboration framework, design the lesson solicitation--banking--selection mechanism, and demonstrate that a team of small LLMs with lessons learned can outperform a much larger LLM and other multi-LLM collaboration methods.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback</title>
<link>https://arxiv.org/abs/2505.23950</link>
<guid>https://arxiv.org/abs/2505.23950</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal large models, multi-turn interaction, human feedback, preference dataset, agentic workflow <br />
<br />
Summary: This article discusses the importance of multi-turn multimodal interaction in advancing multimodal large models (MLLMs) towards human-level intelligence. The InterMT dataset is introduced, which captures human preferences in multi-turn dialogue instances through expert annotations. The dataset consists of 15.6k prompts and 52.6k dialogue instances, with 32.4k human-labeled preference pairs. An agentic workflow is proposed to leverage MLLMs for constructing multi-turn QA instances. The InterMT-Bench is introduced to assess MLLMs' ability in assisting judges with multi-turn, multimodal tasks. Applications of InterMT include judge moderation and revealing the multi-turn scaling law of judge models. The open-source nature of the data aims to facilitate further research in aligning MLLMs with the next step in interactive capabilities. The project website for InterMT can be accessed at https://pku-intermt.github.io. <br />  
Summary: <div>
arXiv:2505.23950v1 Announce Type: new 
Abstract: As multimodal large models (MLLMs) continue to advance across challenging tasks, a key question emerges: What essential capabilities are still missing? A critical aspect of human learning is continuous interaction with the environment -- not limited to language, but also involving multimodal understanding and generation. To move closer to human-level intelligence, models must similarly support multi-turn, multimodal interaction. In particular, they should comprehend interleaved multimodal contexts and respond coherently in ongoing exchanges. In this work, we present an initial exploration through the InterMT -- the first preference dataset for multi-turn multimodal interaction, grounded in real human feedback. In this exploration, we particularly emphasize the importance of human oversight, introducing expert annotations to guide the process, motivated by the fact that current MLLMs lack such complex interactive capabilities. InterMT captures human preferences at both global and local levels into nine sub-dimensions, consists of 15.6k prompts, 52.6k multi-turn dialogue instances, and 32.4k human-labeled preference pairs. To compensate for the lack of capability for multi-modal understanding and generation, we introduce an agentic workflow that leverages tool-augmented MLLMs to construct multi-turn QA instances. To further this goal, we introduce InterMT-Bench to assess the ability of MLLMs in assisting judges with multi-turn, multimodal tasks. We demonstrate the utility of \InterMT through applications such as judge moderation and further reveal the multi-turn scaling law of judge model. We hope the open-source of our data can help facilitate further research on aligning current MLLMs to the next step. Our project website can be found at https://pku-intermt.github.io .
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MSQA: Benchmarking LLMs on Graduate-Level Materials Science Reasoning and Knowledge</title>
<link>https://arxiv.org/abs/2505.23982</link>
<guid>https://arxiv.org/abs/2505.23982</guid>
<content:encoded><![CDATA[
<div> benchmark, materials science, large language models, evaluation, domain-specific knowledge

Summary:
MSQA is a new benchmark designed to evaluate the performance of large language models (LLMs) in the field of materials science. It consists of 1,757 graduate-level materials science questions in detailed explanatory and True/False formats, targeting specific sub-fields like structure-property relationships and computational modeling. The benchmark assesses the factual knowledge and reasoning abilities of LLMs, revealing significant disparities in performance among different types of LLMs. While proprietary LLMs achieve high accuracy rates, open-source LLMs and domain-specific models struggle with overfitting and distributional shifts, leading to lower performance. MSQA serves as a crucial evaluation tool for advancing LLM capabilities in materials science. 

<br /><br />Summary: <div>
arXiv:2505.23982v1 Announce Type: new 
Abstract: Despite recent advances in large language models (LLMs) for materials science, there is a lack of benchmarks for evaluating their domain-specific knowledge and complex reasoning abilities. To bridge this gap, we introduce MSQA, a comprehensive evaluation benchmark of 1,757 graduate-level materials science questions in two formats: detailed explanatory responses and binary True/False assessments. MSQA distinctively challenges LLMs by requiring both precise factual knowledge and multi-step reasoning across seven materials science sub-fields, such as structure-property relationships, synthesis processes, and computational modeling. Through experiments with 10 state-of-the-art LLMs, we identify significant gaps in current LLM performance. While API-based proprietary LLMs achieve up to 84.5% accuracy, open-source (OSS) LLMs peak around 60.5%, and domain-specific LLMs often underperform significantly due to overfitting and distributional shifts. MSQA represents the first benchmark to jointly evaluate the factual and reasoning capabilities of LLMs crucial for LLMs in advanced materials science.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-RAG: A Multimodal Retrieval-Augmented Generation System for Adaptive Video Understanding</title>
<link>https://arxiv.org/abs/2505.23990</link>
<guid>https://arxiv.org/abs/2505.23990</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-RAG, adaptive assistance, multimodal retrieval-augmented generation, human-robot partnerships, dynamic situations  
<br />  
Summary:  
Multi-RAG is a system designed to assist humans in information-intensive scenarios by integrating and reasoning over multiple information streams, such as video, audio, and text. It aims to reduce cognitive load and improve situational understanding, ultimately paving the way for effective human-robot partnerships. In a benchmark test on the MMBench-Video dataset, Multi-RAG outperformed existing video large language models and vision-language models while using fewer resources and input data. This highlights its potential as an efficient and practical foundation for future human-robot adaptive assistance systems in dynamic, real-world contexts. <div>
arXiv:2505.23990v1 Announce Type: new 
Abstract: To effectively engage in human society, the ability to adapt, filter information, and make informed decisions in ever-changing situations is critical. As robots and intelligent agents become more integrated into human life, there is a growing opportunity-and need-to offload the cognitive burden on humans to these systems, particularly in dynamic, information-rich scenarios.
  To fill this critical need, we present Multi-RAG, a multimodal retrieval-augmented generation system designed to provide adaptive assistance to humans in information-intensive circumstances. Our system aims to improve situational understanding and reduce cognitive load by integrating and reasoning over multi-source information streams, including video, audio, and text. As an enabling step toward long-term human-robot partnerships, Multi-RAG explores how multimodal information understanding can serve as a foundation for adaptive robotic assistance in dynamic, human-centered situations. To evaluate its capability in a realistic human-assistance proxy task, we benchmarked Multi-RAG on the MMBench-Video dataset, a challenging multimodal video understanding benchmark. Our system achieves superior performance compared to existing open-source video large language models (Video-LLMs) and large vision-language models (LVLMs), while utilizing fewer resources and less input data. The results demonstrate Multi- RAG's potential as a practical and efficient foundation for future human-robot adaptive assistance systems in dynamic, real-world contexts.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenIC: An LLM-Based Framework for Instance Completion in Knowledge Graphs</title>
<link>https://arxiv.org/abs/2505.24036</link>
<guid>https://arxiv.org/abs/2505.24036</guid>
<content:encoded><![CDATA[
<div> Knowledge graph completion, instance completion, generative instance completion framework, LLM-powered, entity descriptions, types, property prediction, link prediction, multi-label classification, sequence-to-sequence task<br />
<br />
Summary:<br />
Knowledge graph completion involves adding new triples to fill gaps in knowledge bases. Instance completion is predicting the relation-tail pair with only the head given. Leveraging textual descriptions and entity types, a two-step Generative Instance Completion framework called GenIC is proposed. The first step predicts properties through multi-label classification, while the second step focuses on link prediction using a sequence-to-sequence approach. Experimental results demonstrate GenIC outperforms existing methods on three datasets. The code is available on GitHub at https://github.com/amal-gader/genic. <div>
arXiv:2505.24036v1 Announce Type: new 
Abstract: Knowledge graph completion aims to address the gaps of knowledge bases by adding new triples that represent facts. The complexity of this task depends on how many parts of a triple are already known. Instance completion involves predicting the relation-tail pair when only the head is given (h, ?, ?). Notably, modern knowledge bases often contain entity descriptions and types, which can provide valuable context for inferring missing facts. By leveraging these textual descriptions and the ability of large language models to extract facts from them and recognize patterns within the knowledge graph schema, we propose an LLM-powered, end-to-end instance completion approach. Specifically, we introduce GenIC: a two-step Generative Instance Completion framework. The first step focuses on property prediction, treated as a multi-label classification task. The second step is link prediction, framed as a generative sequence-to-sequence task. Experimental results on three datasets show that our method outperforms existing baselines. Our code is available at https://github.com/amal-gader/genic.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leave it to the Specialist: Repair Sparse LLMs with Sparse Fine-Tuning via Sparsity Evolution</title>
<link>https://arxiv.org/abs/2505.24037</link>
<guid>https://arxiv.org/abs/2505.24037</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Sparsity Evolution Fine-Tuning, Pruning, Fine-tuning, Task-specific adaptation

Summary: 
Sparsity Evolution Fine-Tuning (SEFT) is proposed as a novel method tailored for sparse Large Language Models (LLMs). SEFT dynamically adjusts the sparse topology of pruned models during fine-tuning while maintaining overall sparsity. It employs a weight drop-and-grow strategy for task-specific adaptation, allowing the model to self-adapt its sparse connectivity based on the target dataset. A sensitivity-driven pruning criterion ensures consistent sparsity levels throughout fine-tuning. Experimental results on various LLMs show that SEFT outperforms existing methods in terms of performance, memory, and time efficiency. The code for SEFT is publicly available for reference.<br /><br />Summary: <div>
arXiv:2505.24037v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved remarkable success across various tasks but face deployment challenges due to their massive computational demands. While post-training pruning methods like SparseGPT and Wanda can effectively reduce the model size, but struggle to maintain model performance at high sparsity levels, limiting their utility for downstream tasks. Existing fine-tuning methods, such as full fine-tuning and LoRA, fail to preserve sparsity as they require updating the whole dense metrics, not well-suited for sparse LLMs. In this paper, we propose Sparsity Evolution Fine-Tuning (SEFT), a novel method designed specifically for sparse LLMs. SEFT dynamically evolves the sparse topology of pruned models during fine-tuning, while preserving the overall sparsity throughout the process. The strengths of SEFT lie in its ability to perform task-specific adaptation through a weight drop-and-grow strategy, enabling the pruned model to self-adapt its sparse connectivity pattern based on the target dataset. Furthermore, a sensitivity-driven pruning criterion is employed to ensure that the desired sparsity level is consistently maintained throughout fine-tuning. Our experiments on various LLMs, including LLaMA families, DeepSeek, and Mistral, across a diverse set of benchmarks demonstrate that SEFT achieves stronger performance while offering superior memory and time efficiency compared to existing baselines. Our code is publicly available at: https://github.com/QiaoXiao7282/SEFT.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2505.24073</link>
<guid>https://arxiv.org/abs/2505.24073</guid>
<content:encoded><![CDATA[
<div> Knowledge databases, large vision-language models, retrieval-augmented generation, multimodal tasks, systematic dissection<br />
<br />
Summary: 
This paper introduces the concept of Retrieval-Augmented Generation (RAG) to enhance the performance of Large Vision-Language Models (LVLMs) by allowing access to external knowledge databases. The study systematically dissects the RAG pipeline for LVLMs, focusing on the retrieval phase, re-ranking stage, and generation phase. Various strategies are explored to optimize modality configurations, retrieval strategies, positional biases mitigation, relevance improvement of retrieved evidence, and integration of retrieved candidates into the final generation process. Additionally, a unified agentic framework is proposed to dynamically select relevant evidence and suppress irrelevant context through self-reflection. The exploration of RAG for LVLMs results in an average performance improvement of 5% without the need for fine-tuning. <br /><br /> <div>
arXiv:2505.24073v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) have made remarkable strides in multimodal tasks such as visual question answering, visual grounding, and complex reasoning. However, they remain limited by static training data, susceptibility to hallucinations, and inability to verify claims against up-to-date, external evidence, compromising their performance in dynamic real-world applications. Retrieval-Augmented Generation (RAG) offers a practical solution to mitigate these challenges by allowing the LVLMs to access large-scale knowledge databases via retrieval mechanisms, thereby grounding model outputs in factual, contextually relevant information. Here in this paper, we conduct the first systematic dissection of the multimodal RAG pipeline for LVLMs, explicitly investigating (1) the retrieval phase: on the modality configurations and retrieval strategies, (2) the re-ranking stage: on strategies to mitigate positional biases and improve the relevance of retrieved evidence, and (3) the generation phase: we further investigate how to best integrate retrieved candidates into the final generation process. Finally, we extend to explore a unified agentic framework that integrates re-ranking and generation through self-reflection, enabling LVLMs to select relevant evidence and suppress irrelevant context dynamically. Our full-stack exploration of RAG for LVLMs yields substantial insights, resulting in an average performance boost of 5% without any fine-tuning.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCOUT: Teaching Pre-trained Language Models to Enhance Reasoning via Flow Chain-of-Thought</title>
<link>https://arxiv.org/abs/2505.24181</link>
<guid>https://arxiv.org/abs/2505.24181</guid>
<content:encoded><![CDATA[
<div> Cognitive states, Recursive reasoning, Flow Chain of Thought, SCOUT, Large language models <br />
Summary:<br />
The article introduces Flow Chain of Thought (Flow CoT), a reasoning paradigm that models recursive inference as a progressive trajectory of latent cognitive states. It addresses the limitations of CoT-based methods by proposing SCOUT, a fine-tuning framework that enables Flow CoT style reasoning without pretraining. SCOUT uses progressive distillation and a retrospective module to deepen reasoning across iterations. Experimental results across eight reasoning benchmarks demonstrate that SCOUT consistently improves accuracy and explanation quality. Qualitative analyses show that SCOUT enables progressively deeper reasoning, refining belief formation and explanation granularity. These outcomes validate the effectiveness of SCOUT and the practical viability of Flow CoT as a scalable framework for enhancing reasoning in large language models. <br /> <div>
arXiv:2505.24181v1 Announce Type: new 
Abstract: Chain of Thought (CoT) prompting improves the reasoning performance of large language models (LLMs) by encouraging step by step thinking. However, CoT-based methods depend on intermediate reasoning steps, which limits scalability and generalization. Recent work explores recursive reasoning, where LLMs reuse internal layers across iterations to refine latent representations without explicit CoT supervision. While promising, these approaches often require costly pretraining and lack a principled framework for how reasoning should evolve across iterations. We address this gap by introducing Flow Chain of Thought (Flow CoT), a reasoning paradigm that models recursive inference as a progressive trajectory of latent cognitive states. Flow CoT frames each iteration as a distinct cognitive stage deepening reasoning across iterations without relying on manual supervision. To realize this, we propose SCOUT (Stepwise Cognitive Optimization Using Teachers), a lightweight fine tuning framework that enables Flow CoT style reasoning without the need for pretraining. SCOUT uses progressive distillation to align each iteration with a teacher of appropriate capacity, and a cross attention based retrospective module that integrates outputs from previous iterations while preserving the models original computation flow. Experiments across eight reasoning benchmarks show that SCOUT consistently improves both accuracy and explanation quality, achieving up to 1.8% gains under fine tuning. Qualitative analyses further reveal that SCOUT enables progressively deeper reasoning across iterations refining both belief formation and explanation granularity. These results not only validate the effectiveness of SCOUT, but also demonstrate the practical viability of Flow CoT as a scalable framework for enhancing reasoning in LLMs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning API Functionality from Demonstrations for Tool-based Agents</title>
<link>https://arxiv.org/abs/2505.24197</link>
<guid>https://arxiv.org/abs/2505.24197</guid>
<content:encoded><![CDATA[
<div> Learning API functionality, documentation-free, demonstrations, task completion, LLMs <br />
<br />
Summary: This study introduces a novel approach to learning API functionality from demonstrations when documentation is lacking or outdated. By collecting demonstrations from expert agents and self-exploration, the researchers analyze the impact of the number of demonstrations and the use of LLM-generated summaries and evaluations on task success rates. Results across multiple datasets and models show that learning from demonstrations is a challenging task, even for advanced LLMs. Providing explicit function calls and natural language critiques improve task success rates by enhancing parameter filling accuracy. The study also explores failure modes, identifies sources of error, and highlights key challenges for future work on self-improving, documentation-free API-based agents. <div>
arXiv:2505.24197v1 Announce Type: new 
Abstract: Digital tool-based agents that invoke external Application Programming Interfaces (APIs) often rely on documentation to understand API functionality. However, such documentation is frequently missing, outdated, privatized, or inconsistent-hindering the development of reliable, general-purpose agents. In this work, we propose learning API functionality directly from demonstrations as a new paradigm applicable in scenarios without documentation. Using existing API benchmarks, we collect demonstrations from both expert API-based agents and from self-exploration. To understand what information demonstrations must convey for successful task completion, we extensively study how the number of demonstrations and the use of LLM-generated summaries and evaluations affect the task success rate of the API-based agent. Our experiments across 3 datasets and 5 models show that learning functionality from demonstrations remains a non-trivial challenge, even for state-of-the-art LLMs. We find that providing explicit function calls and natural language critiques significantly improves the agent's task success rate due to more accurate parameter filling. We analyze failure modes, identify sources of error, and highlight key open challenges for future work in documentation-free, self-improving, API-based agents.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SentinelAgent: Graph-based Anomaly Detection in Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2505.24201</link>
<guid>https://arxiv.org/abs/2505.24201</guid>
<content:encoded><![CDATA[
<div> framework, anomaly detection, multi-agent systems, large language model, security<br />
Summary:<br /> 
The article introduces a novel system-level anomaly detection framework designed specifically for multi-agent systems (MAS) utilizing large language models (LLMs). The framework combines a graph-based modeling approach to analyze agent interactions in real-time and a SentinelAgent powered by LLMs to detect and intervene in MAS executions based on security policies. By integrating structural modeling with runtime behavioral oversight, the framework can identify single-point faults, prompt injections, multi-agent collusion, and latent exploit paths. Two case studies involving an email assistant and Microsoft's Magentic-One system demonstrate the effectiveness of the framework in detecting hidden risks and providing explainable root-cause analysis. This approach aims to enhance the trustworthiness, monitorability, and security of agent-based AI ecosystems. <br /><br /> <div>
arXiv:2505.24201v1 Announce Type: new 
Abstract: The rise of large language model (LLM)-based multi-agent systems (MAS) introduces new security and reliability challenges. While these systems show great promise in decomposing and coordinating complex tasks, they also face multi-faceted risks across prompt manipulation, unsafe tool usage, and emergent agent miscoordination. Existing guardrail mechanisms offer only partial protection, primarily at the input-output level, and fall short in addressing systemic or multi-point failures in MAS. In this work, we present a system-level anomaly detection framework tailored for MAS, integrating structural modeling with runtime behavioral oversight. Our approach consists of two components. First, we propose a graph-based framework that models agent interactions as dynamic execution graphs, enabling semantic anomaly detection at node, edge, and path levels. Second, we introduce a pluggable SentinelAgent, an LLM-powered oversight agent that observes, analyzes, and intervenes in MAS execution based on security policies and contextual reasoning. By bridging abstract detection logic with actionable enforcement, our method detects not only single-point faults and prompt injections but also multi-agent collusion and latent exploit paths. We validate our framework through two case studies, including an email assistant and Microsoft's Magentic-One system, demonstrating its ability to detect covert risks and provide explainable root-cause attribution. Our work lays the foundation for more trustworthy, monitorable, and secure agent-based AI ecosystems.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bootstrapping LLM Robustness for VLM Safety via Reducing the Pretraining Modality Gap</title>
<link>https://arxiv.org/abs/2505.24208</link>
<guid>https://arxiv.org/abs/2505.24208</guid>
<content:encoded><![CDATA[
<div> regularization, modality gap, safety degradation, vision-language models, pretraining  
Summary:  
- Vision-Language Models (VLMs) suffer from safety degradation compared to text-only models, triggered by modality gap.  
- Amount of modality gap inversely correlated with VLMs' safety, introduced during pretraining and persists through fine-tuning.  
- Proposed regularization method reduces modality gap during pretraining, improving safety alignment without performance compromise.  
- Extensive experiments on LLaVA v1.5, ShareGPT4V, and MiniGPT-4 show up to 16.3% reduction in unsafe rate and 18.2% improvement in existing defenses. <div>
arXiv:2505.24208v1 Announce Type: new 
Abstract: Ensuring Vision-Language Models (VLMs) generate safe outputs is crucial for their reliable deployment. However, LVLMs suffer from drastic safety degradation compared to their LLM backbone. Even blank or irrelevant images can trigger LVLMs to generate harmful responses to prompts that would otherwise be refused in text-only contexts. The modality gap between image and text representations has been recently hypothesized to contribute to safety degradation of LVLMs. However, if and how the amount of modality gap affects LVLMs' safety is not studied. In this work, we show that the amount of modality gap is highly inversely correlated with VLMs' safety. Then, we show that this modality gap is introduced during pretraining LVLMs and persists through fine-tuning. Inspired by this observation, we propose a regularization to reduce the modality gap during pretraining. Our extensive experiments on LLaVA v1.5, ShareGPT4V, and MiniGPT-4 show that our method substantially improves safety alignment of LVLMs, reducing unsafe rate by up to 16.3% without compromising performance, and can further boost existing defenses by up to 18.2%.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>E^2GraphRAG: Streamlining Graph-based RAG for High Efficiency and Effectiveness</title>
<link>https://arxiv.org/abs/2505.24226</link>
<guid>https://arxiv.org/abs/2505.24226</guid>
<content:encoded><![CDATA[
<div> Efficiency, Effectiveness, Graph-based RAG, Entity graphs, Indexing

Summary: 
E^2GraphRAG is a novel graph-based RAG framework that addresses the inefficiency and limitations of existing methods like GraphRAG. The framework improves efficiency and effectiveness by constructing a summary tree with large language models and entity graphs based on document chunks during the indexing stage. By establishing bidirectional indexes between entities and chunks, the framework allows for fast lookup during local and global retrieval. Additionally, E^2GraphRAG incorporates an adaptive retrieval strategy that leverages the graph structure to select between local and global modes. Experimental results demonstrate that E^2GraphRAG achieves significantly faster indexing compared to GraphRAG and LightRAG, with up to a 10 times speed improvement in indexing and 100 times speedup in retrieval while maintaining competitive performance in question answering tasks. <div>
arXiv:2505.24226v1 Announce Type: new 
Abstract: Graph-based RAG methods like GraphRAG have shown promising global understanding of the knowledge base by constructing hierarchical entity graphs. However, they often suffer from inefficiency and rely on manually pre-defined query modes, limiting practical use. In this paper, we propose E^2GraphRAG, a streamlined graph-based RAG framework that improves both Efficiency and Effectiveness. During the indexing stage, E^2GraphRAG constructs a summary tree with large language models and an entity graph with SpaCy based on document chunks. We then construct bidirectional indexes between entities and chunks to capture their many-to-many relationships, enabling fast lookup during both local and global retrieval. For the retrieval stage, we design an adaptive retrieval strategy that leverages the graph structure to retrieve and select between local and global modes. Experiments show that E^2GraphRAG achieves up to 10 times faster indexing than GraphRAG and 100 times speedup over LightRAG in retrieval while maintaining competitive QA performance.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProofNet++: A Neuro-Symbolic System for Formal Proof Verification with Self-Correction</title>
<link>https://arxiv.org/abs/2505.24230</link>
<guid>https://arxiv.org/abs/2505.24230</guid>
<content:encoded><![CDATA[
<div> neuro-symbolic framework, large language models, automated theorem proving, proof verification, self-correction mechanisms<br />
<br />
Summary: ProofNet++ is a proposed neuro-symbolic framework that combines large language models with formal proof verification and self-correction mechanisms to enhance automated theorem proving. By integrating symbolic proof tree supervision, a reinforcement learning loop using verifiers as reward functions, and an iterative self-correction module, ProofNet++ addresses the limitations of current LLM-based systems such as hallucinated logical steps and unverifiable reasoning. Experiments conducted on miniF2F, Lean's mathlib, and HOL Light demonstrate that ProofNet++ significantly improves proof accuracy, correctness, and formal verifiability compared to previous models. The theoretical analysis supports the convergence and stability of the verifier-guided RL framework, and the datasets and codebase are released for further research. <br /><br />Summary: <div>
arXiv:2505.24230v1 Announce Type: new 
Abstract: We propose ProofNet++, a neuro-symbolic framework that enhances automated theorem proving by combining large language models (LLMs) with formal proof verification and self-correction mechanisms. Current LLM-based systems suffer from hallucinated logical steps and unverifiable reasoning. ProofNet++ mitigates these limitations by integrating symbolic proof tree supervision, a reinforcement learning loop using verifiers as reward functions, and an iterative self-correction module. Our experiments on miniF2F, Lean's mathlib, and HOL Light show that ProofNet++ significantly improves proof accuracy, correctness, and formal verifiability over prior models. We provide theoretical analysis of the convergence and stability of the verifier-guided RL framework and release our datasets and codebase for future research.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FABLE: A Novel Data-Flow Analysis Benchmark on Procedural Text for Large Language Model Evaluation</title>
<link>https://arxiv.org/abs/2505.24258</link>
<guid>https://arxiv.org/abs/2505.24258</guid>
<content:encoded><![CDATA[
<div> data flow, large language models, benchmark, structured procedural text, evaluation

Summary:
FABLE is a new benchmark designed to evaluate large language models' (LLMs) understanding of data flow using structured procedural text. It adapts classical data-flow analyses from software engineering across three real-world domains: cooking recipes, travel routes, and automated plans. The benchmark consists of 2,400 question-answer pairs, testing three types of LLMs: reasoning-focused, general-purpose, and code-specific models. Results show that the reasoning model achieves higher accuracy but slower inference, while the general-purpose and code-specific models perform close to random chance. FABLE is the first diagnostic benchmark to systematically evaluate data-flow reasoning, providing insights for developing models with stronger procedural understanding. 

<br /><br />Summary: <div>
arXiv:2505.24258v1 Announce Type: new 
Abstract: Understanding how data moves, transforms, and persists, known as data flow, is fundamental to reasoning in procedural tasks. Despite their fluency in natural and programming languages, large language models (LLMs), although increasingly being applied to decisions with procedural tasks, have not been systematically evaluated for their ability to perform data-flow reasoning. We introduce FABLE, an extensible benchmark designed to assess LLMs' understanding of data flow using structured, procedural text. FABLE adapts eight classical data-flow analyses from software engineering: reaching definitions, very busy expressions, available expressions, live variable analysis, interval analysis, type-state analysis, taint analysis, and concurrency analysis. These analyses are instantiated across three real-world domains: cooking recipes, travel routes, and automated plans. The benchmark includes 2,400 question-answer pairs, with 100 examples for each domain-analysis combination. We evaluate three types of LLMs: a reasoning-focused model (DeepSeek-R1 8B), a general-purpose model (LLaMA 3.1 8B), and a code-specific model (Granite Code 8B). Each model is tested using majority voting over five sampled completions per prompt. Results show that the reasoning model achieves higher accuracy, but at the cost of over 20 times slower inference compared to the other models. In contrast, the general-purpose and code-specific models perform close to random chance. FABLE provides the first diagnostic benchmark to systematically evaluate data-flow reasoning and offers insights for developing models with stronger procedural understanding.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI for Urban Design: A Stepwise Approach Integrating Human Expertise with Multimodal Diffusion Models</title>
<link>https://arxiv.org/abs/2505.24260</link>
<guid>https://arxiv.org/abs/2505.24260</guid>
<content:encoded><![CDATA[
<div> Keywords: Urban design, generative artificial intelligence, stepwise framework, multimodal diffusion models, human-AI interaction <br />
Summary: 
This study introduces a new stepwise framework for generative urban design that integrates multimodal diffusion models with human expertise. The framework is divided into three key stages aligned with established urban design workflows: road network and land use planning, building layout planning, and detailed planning and rendering. Multimodal diffusion models generate initial designs based on textual prompts and image-based constraints, which can then be reviewed and refined by human designers. An evaluation framework assesses the fidelity, compliance, and diversity of the generated designs. Experiments using data from Chicago and New York City show that the proposed framework outperforms baseline models and end-to-end approaches in terms of fidelity, compliance, and diversity. This approach highlights the importance of preserving human control and facilitating iterative refinements in urban design solutions, setting the stage for improved human-AI interaction in the field. <br /><br />Summary: <div>
arXiv:2505.24260v1 Announce Type: new 
Abstract: Urban design is a multifaceted process that demands careful consideration of site-specific constraints and collaboration among diverse professionals and stakeholders. The advent of generative artificial intelligence (GenAI) offers transformative potential by improving the efficiency of design generation and facilitating the communication of design ideas. However, most existing approaches are not well integrated with human design workflows. They often follow end-to-end pipelines with limited control, overlooking the iterative nature of real-world design. This study proposes a stepwise generative urban design framework that integrates multimodal diffusion models with human expertise to enable more adaptive and controllable design processes. Instead of generating design outcomes in a single end-to-end process, the framework divides the process into three key stages aligned with established urban design workflows: (1) road network and land use planning, (2) building layout planning, and (3) detailed planning and rendering. At each stage, multimodal diffusion models generate preliminary designs based on textual prompts and image-based constraints, which can then be reviewed and refined by human designers. We design an evaluation framework to assess the fidelity, compliance, and diversity of the generated designs. Experiments using data from Chicago and New York City demonstrate that our framework outperforms baseline models and end-to-end approaches across all three dimensions. This study underscores the benefits of multimodal diffusion models and stepwise generation in preserving human control and facilitating iterative refinements, laying the groundwork for human-AI interaction in urban design solutions.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Much Backtracking is Enough? Exploring the Interplay of SFT and RL in Enhancing LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.24273</link>
<guid>https://arxiv.org/abs/2505.24273</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, reasoning abilities, supervised finetuning, reinforcement learning, backtracking

Summary: 
- The study explores the interaction between supervised finetuning (SFT) and reinforcement learning (RL) in improving reasoning abilities of large language models (LLMs).
- RL effectively internalizes search strategies, with longer chain-of-thought (CoT) sequences and backtracking proving beneficial in enhancing reasoning performance.
- Short CoT sequences used in SFT show moderate contribution to RL training but diminish in effectiveness as tasks become more challenging.
- Longer CoT sequences with backtracks lead to better and more stable RL training, especially on complex problems with larger search spaces.
- RL prioritizes structural patterns over content correctness in long CoT sequences, indicating the importance of designing optimal training strategies for scaling reasoning in LLMs.<br /><br />Summary: <div>
arXiv:2505.24273v1 Announce Type: new 
Abstract: Recent breakthroughs in large language models (LLMs) have effectively improved their reasoning abilities, particularly on mathematical and logical problems that have verifiable answers, through techniques such as supervised finetuning (SFT) and reinforcement learning (RL). Prior research indicates that RL effectively internalizes search strategies, enabling long chain-of-thought (CoT) reasoning, with backtracking emerging naturally as a learned capability. However, the precise benefits of backtracking, specifically, how significantly it contributes to reasoning improvements and the optimal extent of its use, remain poorly understood. In this work, we systematically investigate the dynamics between SFT and RL on eight reasoning tasks: Countdown, Sudoku, Arc 1D, Geometry, Color Cube Rotation, List Functions, Zebra Puzzles, and Self Reference. Our findings highlight that short CoT sequences used in SFT as a warm-up do have moderate contribution to RL training, compared with cold-start RL; however such contribution diminishes when tasks become increasingly difficult. Motivated by this observation, we construct synthetic datasets varying systematically in the number of backtracking steps and conduct controlled experiments to isolate the influence of either the correctness (content) or the structure (i.e., backtrack frequency). We find that (1) longer CoT with backtracks generally induce better and more stable RL training, (2) more challenging problems with larger search space tend to need higher numbers of backtracks during the SFT stage. Additionally, we demonstrate through experiments on distilled data that RL training is largely unaffected by the correctness of long CoT sequences, suggesting that RL prioritizes structural patterns over content correctness. Collectively, our results offer practical insights into designing optimal training strategies to effectively scale reasoning in LLMs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the Quote: Enabling Quotation-Aware Dialogue in LLMs via Plug-and-Play Modules</title>
<link>https://arxiv.org/abs/2505.24292</link>
<guid>https://arxiv.org/abs/2505.24292</guid>
<content:encoded><![CDATA[
<div> Quotation, large language models, dialogue history, data pipeline, QuAda<br />
<br />
Summary: 
The article addresses the challenge of incorporating quoted text in human-AI conversations using large language models. It introduces a novel approach called span-conditioned generation, which decomposes dialogue turns into history, quotation spans, and intent. A quotation-centric data pipeline is proposed to generate task-specific dialogues and verify answer correctness. The QuAda method is introduced to efficiently handle quoted spans in dialogue through bottleneck projections in attention heads, with minimal model updates. Experiments demonstrate the effectiveness of QuAda across various scenarios and topics, offering a plug-and-play solution for quotation-aware dialogue in large language models. <div>
arXiv:2505.24292v1 Announce Type: new 
Abstract: Human-AI conversation frequently relies on quoting earlier text-"check it with the formula I just highlighted"-yet today's large language models (LLMs) lack an explicit mechanism for locating and exploiting such spans. We formalise the challenge as span-conditioned generation, decomposing each turn into the dialogue history, a set of token-offset quotation spans, and an intent utterance. Building on this abstraction, we introduce a quotation-centric data pipeline that automatically synthesises task-specific dialogues, verifies answer correctness through multi-stage consistency checks, and yields both a heterogeneous training corpus and the first benchmark covering five representative scenarios. To meet the benchmark's zero-overhead and parameter-efficiency requirements, we propose QuAda, a lightweight training-based method that attaches two bottleneck projections to every attention head, dynamically amplifying or suppressing attention to quoted spans at inference time while leaving the prompt unchanged and updating < 2.8% of backbone weights. Experiments across models show that QuAda is suitable for all scenarios and generalises to unseen topics, offering an effective, plug-and-play solution for quotation-aware dialogue.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GridRoute: A Benchmark for LLM-Based Route Planning with Cardinal Movement in Grid Environments</title>
<link>https://arxiv.org/abs/2505.24306</link>
<guid>https://arxiv.org/abs/2505.24306</guid>
<content:encoded><![CDATA[
<div> Large Language Models, GridRoute, hybrid prompting technique, Algorithm of Thought, path planning

Summary:
The study introduces GridRoute, a benchmark that evaluates the synergy between Large Language Models (LLMs) and traditional algorithms in path planning tasks. It proposes a novel hybrid prompting technique called Algorithm of Thought (AoT) to incorporate traditional algorithms' guidance into LLMs. The benchmark assesses six LLMs of varying sizes on correctness, optimality, and efficiency in grid environments. Results demonstrate that AoT significantly enhances performance, especially in larger or more complex environments, indicating a promising approach to address path planning challenges. The open-sourced code is available on https://github.com/LinChance/GridRoute. <div>
arXiv:2505.24306v1 Announce Type: new 
Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated their potential in planning and reasoning tasks, offering a flexible alternative to classical pathfinding algorithms. However, most existing studies focus on LLMs' independent reasoning capabilities and overlook the potential synergy between LLMs and traditional algorithms. To fill this gap, we propose a comprehensive evaluation benchmark GridRoute to assess how LLMs can take advantage of traditional algorithms. We also propose a novel hybrid prompting technique called Algorithm of Thought (AoT), which introduces traditional algorithms' guidance into prompting. Our benchmark evaluates six LLMs ranging from 7B to 72B parameters across various map sizes, assessing their performance in correctness, optimality, and efficiency in grid environments with varying sizes. Our results show that AoT significantly boosts performance across all model sizes, particularly in larger or more complex environments, suggesting a promising approach to addressing path planning challenges. Our code is open-sourced at https://github.com/LinChance/GridRoute.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Three Kinds of Negation in Knowledge and Their Mathematical Foundations</title>
<link>https://arxiv.org/abs/2505.24422</link>
<guid>https://arxiv.org/abs/2505.24422</guid>
<content:encoded><![CDATA[
<div> negation, artificial intelligence, knowledge processing, contradiction, logic

Summary: 
This paper explores the understanding and characteristics of negation in various disciplines such as philosophy, logic, and linguistics. The authors propose three types of negation in knowledge: contradictory negation, opposite negation, and intermediary negation, based on the concepts of contradiction and opposition. They introduce the mathematical foundation of SCOI and LCOI to reflect the intrinsic connections, properties, and laws of these different forms of negation. The operational properties of SCOI and formal inference relations in LCOI are also discussed. <div>
arXiv:2505.24422v1 Announce Type: new 
Abstract: In the field of artificial intelligence, understanding, distinguishing, expressing, and computing the negation in knowledge is a fundamental issue in knowledge processing and research. In this paper, we examine and analyze the understanding and characteristics of negation in various fields such as philosophy, logic, and linguistics etc. Based on the distinction between the concepts of contradiction and opposition, we propose that there are three different types of negation in knowledge from a conceptual perspective: contradictory negation, opposite negation, and intermediary negation. To establish a mathematical foundation that fully reflects the intrinsic connections, properties, and laws of these different forms of negation, we introduce SCOI: sets with contradictory negation, opposite negation and intermediary negation, and LCOI: logic with contradictory negation, opposite negation and intermediary negation, and we proved the main operational properties of SCOI as well as the formal inference relations in LCOI.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>P: A Universal Measure of Predictive Intelligence</title>
<link>https://arxiv.org/abs/2505.24426</link>
<guid>https://arxiv.org/abs/2505.24426</guid>
<content:encoded><![CDATA[
<div> measure, intelligence, prediction, artificial intelligence, Kolmogorov complexity

Summary:
The paper introduces a new universal measure of intelligence that is based on the idea that prediction is a crucial aspect of intelligence. It addresses the lack of a universally agreed definition of intelligence in AI systems and the inability to compare the intelligence of humans, animals, and AIs on a single scale. The proposed measure calculates the accuracy and complexity of predictions made by an agent in its environment, incorporating Kolmogorov complexity. Two experiments were conducted to test the algorithm's practicality in measuring intelligence in different scenarios. The results showed that the measure could assess the intelligence of agents in a virtual maze and predicting time-series data. This new measure could potentially initiate a comparative science of intelligence, allowing for the ranking of humans, animals, and AIs on a single ratio scale. <br /><br />Summary: <div>
arXiv:2505.24426v1 Announce Type: new 
Abstract: Over the last thirty years, considerable progress has been made with the development of systems that can drive cars, play games, predict protein folding and generate natural language. These systems are described as intelligent and there has been a great deal of talk about the rapid increase in artificial intelligence and its potential dangers. However, our theoretical understanding of intelligence and ability to measure it lag far behind our capacity for building systems that mimic intelligent human behaviour. There is no commonly agreed definition of the intelligence that AI systems are said to possess. No-one has developed a practical measure that would enable us to compare the intelligence of humans, animals and AIs on a single ratio scale.
  This paper sets out a new universal measure of intelligence that is based on the hypothesis that prediction is the most important component of intelligence. As an agent interacts with its normal environment, the accuracy of its predictions is summed up and the complexity of its predictions and perceived environment is accounted for using Kolmogorov complexity. Two experiments were carried out to evaluate the practical feasibility of the algorithm. These demonstrated that it could measure the intelligence of an agent embodied in a virtual maze and an agent that makes predictions about time-series data. This universal measure could be the starting point for a new comparative science of intelligence that ranks humans, animals and AIs on a single ratio scale.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RMoA: Optimizing Mixture-of-Agents through Diversity Maximization and Residual Compensation</title>
<link>https://arxiv.org/abs/2505.24442</link>
<guid>https://arxiv.org/abs/2505.24442</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent systems, language models, residual connections, efficiency, reliability

Summary: 
Residual Mixture-of-Agents (RMoA) is proposed as a novel approach to enhance the performance of multi-agent systems based on large language models. By integrating residual connections inspired by ResNet, RMoA aims to optimize efficiency and reliability in processing tasks. The model incorporates an embedding-based diversity selection mechanism to maximize information utilization while reducing computational costs. Additionally, a Residual Extraction Agent is introduced to capture inter-layer response differences and preserve incremental information, while a Residual Aggregation Agent facilitates hierarchical information integration. An adaptive termination mechanism dynamically halts processing based on residual convergence to improve inference efficiency. RMoA achieves state-of-the-art performance on various benchmarks including across alignment, mathematical reasoning, code generation, and multitasking understanding, while significantly reducing computational overhead. The code for RMoA is available on GitHub for further exploration and implementation. 

<br /><br />Summary: <div>
arXiv:2505.24442v1 Announce Type: new 
Abstract: Although multi-agent systems based on large language models show strong capabilities on multiple tasks, they are still limited by high computational overhead, information loss, and robustness. Inspired by ResNet's residual learning, we propose Residual Mixture-of-Agents (RMoA), integrating residual connections to optimize efficiency and reliability. To maximize information utilization from model responses while minimizing computational costs, we innovatively design an embedding-based diversity selection mechanism that greedily selects responses via vector similarity. Furthermore, to mitigate iterative information degradation, we introduce a Residual Extraction Agent to preserve cross-layer incremental information by capturing inter-layer response differences, coupled with a Residual Aggregation Agent for hierarchical information integration. Additionally, we propose an adaptive termination mechanism that dynamically halts processing based on residual convergence, further improving inference efficiency. RMoA achieves state-of-the-art performance on the benchmarks of across alignment, mathematical reasoning, code generation, and multitasking understanding, while significantly reducing computational overhead. Code is available at https://github.com/mindhunter01/RMoA.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEAR: A Multimodal Dataset for Analyzing AR-LLM-Driven Social Engineering Behaviors</title>
<link>https://arxiv.org/abs/2505.24458</link>
<guid>https://arxiv.org/abs/2505.24458</guid>
<content:encoded><![CDATA[
<div> Dataset, social engineering, augmented reality, multimodal, large language models<br />
<br />
Summary: <br />
The SEAR Dataset is a novel resource that focuses on studying social engineering attacks conducted through augmented reality and large language models. It consists of 180 annotated conversations from 60 participants in simulated adversarial scenarios like meetings and networking events. The dataset includes AR-captured visual/audio cues, environmental context, social media profiles, trust ratings, and susceptibility assessments. Key findings show the dataset's effectiveness in eliciting compliance and manipulating trust. The dataset aims to support research in detecting AR-driven social engineering attacks, developing defensive strategies, and understanding multimodal adversarial manipulation. Ethical safeguards like anonymization and IRB compliance are in place for responsible use. The SEAR Dataset can be accessed at https://github.com/INSLabCN/SEAR-Dataset. <div>
arXiv:2505.24458v1 Announce Type: new 
Abstract: The SEAR Dataset is a novel multimodal resource designed to study the emerging threat of social engineering (SE) attacks orchestrated through augmented reality (AR) and multimodal large language models (LLMs). This dataset captures 180 annotated conversations across 60 participants in simulated adversarial scenarios, including meetings, classes and networking events. It comprises synchronized AR-captured visual/audio cues (e.g., facial expressions, vocal tones), environmental context, and curated social media profiles, alongside subjective metrics such as trust ratings and susceptibility assessments. Key findings reveal SEAR's alarming efficacy in eliciting compliance (e.g., 93.3% phishing link clicks, 85% call acceptance) and hijacking trust (76.7% post-interaction trust surge). The dataset supports research in detecting AR-driven SE attacks, designing defensive frameworks, and understanding multimodal adversarial manipulation. Rigorous ethical safeguards, including anonymization and IRB compliance, ensure responsible use. The SEAR dataset is available at https://github.com/INSLabCN/SEAR-Dataset.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing the Interface Between Knowledge Graphs and LLMs for Complex Reasoning</title>
<link>https://arxiv.org/abs/2505.24478</link>
<guid>https://arxiv.org/abs/2505.24478</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Knowledge Graphs, Hyperparameter Optimization, Multi-hop QA, Modular Framework

Summary: 
This paper explores the significance of hyperparameter optimization in integrating Large Language Models (LLMs) with Knowledge Graphs (KGs) for retrieval-augmented generation tasks. The study focuses on the Cognee framework and evaluates different parameters related to chunking, graph construction, retrieval, and prompting using three multi-hop QA benchmarks. The results show that targeted tuning can lead to notable performance improvements, although the gains may vary across datasets and evaluation metrics. The findings underscore the importance of optimization in complex systems and suggest the need for clearer frameworks to assess and improve model performance. While the study demonstrates the potential of hyperparameter tuning in enhancing system performance, the authors emphasize the necessity of not only advancing architecture but also refining optimization and evaluation methodologies for future progress in this domain.<br /><br />Summary: <div>
arXiv:2505.24478v1 Announce Type: new 
Abstract: Integrating Large Language Models (LLMs) with Knowledge Graphs (KGs) results in complex systems with numerous hyperparameters that directly affect performance. While such systems are increasingly common in retrieval-augmented generation, the role of systematic hyperparameter optimization remains underexplored. In this paper, we study this problem in the context of Cognee, a modular framework for end-to-end KG construction and retrieval. Using three multi-hop QA benchmarks (HotPotQA, TwoWikiMultiHop, and MuSiQue) we optimize parameters related to chunking, graph construction, retrieval, and prompting. Each configuration is scored using established metrics (exact match, F1, and DeepEval's LLM-based correctness metric). Our results demonstrate that meaningful gains can be achieved through targeted tuning. While the gains are consistent, they are not uniform, with performance varying across datasets and metrics. This variability highlights both the value of tuning and the limitations of standard evaluation measures. While demonstrating the immediate potential of hyperparameter tuning, we argue that future progress will depend not only on architectural advances but also on clearer frameworks for optimization and evaluation in complex, modular systems.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Knowledge Graphs and LLMs for Structured Generation of Misinformation</title>
<link>https://arxiv.org/abs/2505.24479</link>
<guid>https://arxiv.org/abs/2505.24479</guid>
<content:encoded><![CDATA[
<div> Knowledge graphs, generative AI, misinformation, structured semantic resources, language models<br />
<br />
Summary: <br />
The article discusses the use of knowledge graphs (KGs) to systematically generate fake triplets for misinformation. By analyzing KGs' structural properties, false relationships are identified to guide large language models (LLMs) in creating misinformation with varying degrees of credibility. This deterministic approach makes it challenging for humans to detect the fake information. The study also examines the effectiveness of LLMs in distinguishing between genuine and artificially generated misinformation, revealing limitations in current detection methods. It emphasizes the need for improved detection strategies and a deeper exploration of biases in generative models. <div>
arXiv:2505.24479v1 Announce Type: new 
Abstract: The rapid spread of misinformation, further amplified by recent advances in generative AI, poses significant threats to society, impacting public opinion, democratic stability, and national security. Understanding and proactively assessing these threats requires exploring methodologies that enable structured and scalable misinformation generation. In this paper, we propose a novel approach that leverages knowledge graphs (KGs) as structured semantic resources to systematically generate fake triplets. By analyzing the structural properties of KGs, such as the distance between entities and their predicates, we identify plausibly false relationships. These triplets are then used to guide large language models (LLMs) in generating misinformation statements with varying degrees of credibility. By utilizing structured semantic relationships, our deterministic approach produces misinformation inherently challenging for humans to detect, drawing exclusively upon publicly available KGs (e.g., WikiGraphs).
  Additionally, we investigate the effectiveness of LLMs in distinguishing between genuine and artificially generated misinformation. Our analysis highlights significant limitations in current LLM-based detection methods, underscoring the necessity for enhanced detection strategies and a deeper exploration of inherent biases in generative models.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MELT: Towards Automated Multimodal Emotion Data Annotation by Leveraging LLM Embedded Knowledge</title>
<link>https://arxiv.org/abs/2505.24493</link>
<guid>https://arxiv.org/abs/2505.24493</guid>
<content:encoded><![CDATA[
<div> Keywords: speech emotion recognition, deep learning, Large Language Models, GPT-4o, multimodal dataset<br />
Summary: 
MELT is a multimodal emotion dataset annotated using GPT-4o, showcasing the model's ability to generate accurate annotations without direct access to multimodal inputs. Four self-supervised learning backbones were fine-tuned using MELT, leading to improved speech emotion recognition performance across emotion datasets. The study addresses the issue of costly and inconsistent human annotation in SER by leveraging LLMs for annotation. The methodology uses structured text prompts to utilize GPT-4o's knowledge, resulting in contextually relevant annotations. Subjective experiments show a consistent performance improvement on SER, demonstrating the effectiveness of the proposed approach. By leveraging LLMs like GPT-4o for annotation, the study offers a scalable and accurate alternative to human annotation in SER. <br /><br />Summary: <div>
arXiv:2505.24493v1 Announce Type: new 
Abstract: Although speech emotion recognition (SER) has advanced significantly with deep learning, annotation remains a major hurdle. Human annotation is not only costly but also subject to inconsistencies annotators often have different preferences and may lack the necessary contextual knowledge, which can lead to varied and inaccurate labels. Meanwhile, Large Language Models (LLMs) have emerged as a scalable alternative for annotating text data. However, the potential of LLMs to perform emotional speech data annotation without human supervision has yet to be thoroughly investigated. To address these problems, we apply GPT-4o to annotate a multimodal dataset collected from the sitcom Friends, using only textual cues as inputs. By crafting structured text prompts, our methodology capitalizes on the knowledge GPT-4o has accumulated during its training, showcasing that it can generate accurate and contextually relevant annotations without direct access to multimodal inputs. Therefore, we propose MELT, a multimodal emotion dataset fully annotated by GPT-4o. We demonstrate the effectiveness of MELT by fine-tuning four self-supervised learning (SSL) backbones and assessing speech emotion recognition performance across emotion datasets. Additionally, our subjective experiments\' results demonstrate a consistence performance improvement on SER.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture-of-Experts for Personalized and Semantic-Aware Next Location Prediction</title>
<link>https://arxiv.org/abs/2505.24597</link>
<guid>https://arxiv.org/abs/2505.24597</guid>
<content:encoded><![CDATA[
<div> Keywords: human mobility, location prediction, large language models, Mixture-of-Experts, interpretability

Summary: NextLocMoE is a new framework for next location prediction that addresses key limitations of existing approaches. It leverages large language models and a dual-level Mixture-of-Experts design to capture the complex semantics of real-world locations and model heterogeneous behavioral dynamics across diverse user groups. The framework includes a Location Semantics MoE for encoding location semantics and a Personalized MoE for adapting to individual user patterns. It also incorporates a history-aware routing mechanism to enhance expert selection. Empirical evaluations on urban datasets show that NextLocMoE outperforms existing methods in predictive accuracy, cross-domain generalization, and interpretability. The framework demonstrates the potential of leveraging advanced modeling techniques for improving next location prediction in human mobility studies.<br /><br />Summary: <div>
arXiv:2505.24597v1 Announce Type: new 
Abstract: Next location prediction plays a critical role in understanding human mobility patterns. However, existing approaches face two core limitations: (1) they fall short in capturing the complex, multi-functional semantics of real-world locations; and (2) they lack the capacity to model heterogeneous behavioral dynamics across diverse user groups. To tackle these challenges, we introduce NextLocMoE, a novel framework built upon large language models (LLMs) and structured around a dual-level Mixture-of-Experts (MoE) design. Our architecture comprises two specialized modules: a Location Semantics MoE that operates at the embedding level to encode rich functional semantics of locations, and a Personalized MoE embedded within the Transformer backbone to dynamically adapt to individual user mobility patterns. In addition, we incorporate a history-aware routing mechanism that leverages long-term trajectory data to enhance expert selection and ensure prediction stability. Empirical evaluations across several real-world urban datasets show that NextLocMoE achieves superior performance in terms of predictive accuracy, cross-domain generalization, and interpretability
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taxonomic Networks: A Representation for Neuro-Symbolic Pairing</title>
<link>https://arxiv.org/abs/2505.24601</link>
<guid>https://arxiv.org/abs/2505.24601</guid>
<content:encoded><![CDATA[
<div> neuro-symbolic pair, taxonomic networks, knowledge representation, neural approaches, symbolic computation
<br />
Summary: 
In this paper, the concept of a neuro-symbolic pair is introduced, where neural and symbolic approaches are connected through a common knowledge representation. The study focuses on taxonomic networks, a type of discrimination network with hierarchically organized taxonomic concepts. A novel neuro-symbolic pair is constructed and evaluated for performance, demonstrating that the symbolic method learns taxonomic nets more efficiently with less data and compute, while the neural method achieves higher accuracy with more resources. These approaches can be used interchangeably based on situational needs, with seamless translation between them when necessary. This work sets the groundwork for future systems that integrate neural and symbolic computation more effectively. 
<br /> <div>
arXiv:2505.24601v1 Announce Type: new 
Abstract: We introduce the concept of a \textbf{neuro-symbolic pair} -- neural and symbolic approaches that are linked through a common knowledge representation. Next, we present \textbf{taxonomic networks}, a type of discrimination network in which nodes represent hierarchically organized taxonomic concepts. Using this representation, we construct a novel neuro-symbolic pair and evaluate its performance. We show that our symbolic method learns taxonomic nets more efficiently with less data and compute, while the neural method finds higher-accuracy taxonomic nets when provided with greater resources. As a neuro-symbolic pair, these approaches can be used interchangeably based on situational needs, with seamless translation between them when necessary. This work lays the foundation for future systems that more fundamentally integrate neural and symbolic computation.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Random Rule Forest (RRF): Interpretable Ensembles of LLM-Generated Questions for Predicting Startup Success</title>
<link>https://arxiv.org/abs/2505.24622</link>
<guid>https://arxiv.org/abs/2505.24622</guid>
<content:encoded><![CDATA[
<div> Keywords: startup success, interpretable models, language models, ensemble framework, venture capital

Summary:
An ensemble framework combining YES/NO questions from large language models (LLMs) is presented for predicting startup success. Each question functions as a weak heuristic, and through filtering, ranking, and aggregating with a threshold-based voting mechanism, a strong ensemble predictor is constructed. In a test set where 10% of startups are successful, the approach achieves a precision rate of 50%, showing a significant improvement over random selection. The framework remains fully transparent, emphasizing the importance of interpretability in decision-making. Incorporating expert-guided heuristics enhances performance to 54% precision, showing the value of combining LLM reasoning with human insight. The study underscores the significance of simple and interpretable ensemble models in supporting high-stakes decisions, particularly in domains like venture capital (VC). 

<br /><br />Summary: <div>
arXiv:2505.24622v1 Announce Type: new 
Abstract: Predicting startup success requires models that are both accurate and interpretable. We present a lightweight ensemble framework that combines YES/NO questions generated by large language models (LLMs), forming a transparent decision-making system. Each question acts as a weak heuristic, and by filtering, ranking, and aggregating them through a threshold-based voting mechanism, we construct a strong ensemble predictor. On a test set where 10% of startups are classified as successful, our approach achieves a precision rate of 50%, representing a 5x improvement over random selection, while remaining fully transparent. When we incorporate expert-guided heuristics into the generation process, performance improves further to 54% precision. These results highlight the value of combining LLM reasoning with human insight and demonstrate that simple, interpretable ensembles can support high-stakes decisions in domains such as venture capital (VC).
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptable Cardiovascular Disease Risk Prediction from Heterogeneous Data using Large Language Models</title>
<link>https://arxiv.org/abs/2505.24655</link>
<guid>https://arxiv.org/abs/2505.24655</guid>
<content:encoded><![CDATA[
<div> risk prediction models, cardiovascular disease, AdaCVD, machine learning, UK Biobank <br />
Summary: <br />
AdaCVD is a new, adaptable framework for cardiovascular disease (CVD) risk prediction that outperforms existing models by incorporating comprehensive patient information and integrating structured data and unstructured text. It excels in performance, especially in handling distribution shifts and accommodating diverse patient populations. AdaCVD's flexible approach addresses key clinical challenges and is able to rapidly adapt to new patient groups with minimal additional data. The framework demonstrates robust performance across demographic and clinical subgroups, including underrepresented cohorts, making it a promising tool for AI-driven clinical decision support in dynamic healthcare settings. <div>
arXiv:2505.24655v1 Announce Type: new 
Abstract: Cardiovascular disease (CVD) risk prediction models are essential for identifying high-risk individuals and guiding preventive actions. However, existing models struggle with the challenges of real-world clinical practice as they oversimplify patient profiles, rely on rigid input schemas, and are sensitive to distribution shifts. We developed AdaCVD, an adaptable CVD risk prediction framework built on large language models extensively fine-tuned on over half a million participants from the UK Biobank. In benchmark comparisons, AdaCVD surpasses established risk scores and standard machine learning approaches, achieving state-of-the-art performance. Crucially, for the first time, it addresses key clinical challenges across three dimensions: it flexibly incorporates comprehensive yet variable patient information; it seamlessly integrates both structured data and unstructured text; and it rapidly adapts to new patient populations using minimal additional data. In stratified analyses, it demonstrates robust performance across demographic, socioeconomic, and clinical subgroups, including underrepresented cohorts. AdaCVD offers a promising path toward more flexible, AI-driven clinical decision support tools suited to the realities of heterogeneous and dynamic healthcare environments.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AXIOM: Learning to Play Games in Minutes with Expanding Object-Centric Models</title>
<link>https://arxiv.org/abs/2505.24784</link>
<guid>https://arxiv.org/abs/2505.24784</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, active inference, object-centric dynamics, Bayesian model reduction, data efficiency <br />
Summary:
The paper introduces a novel architecture called AXIOM that combines core priors about object-centric dynamics and interactions with active inference principles to accelerate learning in low-data regimes. AXIOM represents scenes as compositions of objects and models their dynamics as piecewise linear trajectories capturing sparse object-object interactions. The generative model structure is expanded online by learning mixture models from single events and refined through Bayesian model reduction for generalization. AXIOM achieves mastery in various games within 10,000 interaction steps, with fewer parameters compared to deep reinforcement learning (DRL) and without the computational expenses of gradient-based optimization. The approach offers data efficiency and interpretability of Bayesian methods while retaining across-task generalization typical of DRL. <div>
arXiv:2505.24784v1 Announce Type: new 
Abstract: Current deep reinforcement learning (DRL) approaches achieve state-of-the-art performance in various domains, but struggle with data efficiency compared to human learning, which leverages core priors about objects and their interactions. Active inference offers a principled framework for integrating sensory information with prior knowledge to learn a world model and quantify the uncertainty of its own beliefs and predictions. However, active inference models are usually crafted for a single task with bespoke knowledge, so they lack the domain flexibility typical of DRL approaches. To bridge this gap, we propose a novel architecture that integrates a minimal yet expressive set of core priors about object-centric dynamics and interactions to accelerate learning in low-data regimes. The resulting approach, which we call AXIOM, combines the usual data efficiency and interpretability of Bayesian approaches with the across-task generalization usually associated with DRL. AXIOM represents scenes as compositions of objects, whose dynamics are modeled as piecewise linear trajectories that capture sparse object-object interactions. The structure of the generative model is expanded online by growing and learning mixture models from single events and periodically refined through Bayesian model reduction to induce generalization. AXIOM masters various games within only 10,000 interaction steps, with both a small number of parameters compared to DRL, and without the computational expense of gradient-based optimization.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EXP-Bench: Can AI Conduct AI Research Experiments?</title>
<link>https://arxiv.org/abs/2505.24785</link>
<guid>https://arxiv.org/abs/2505.24785</guid>
<content:encoded><![CDATA[
<div> Benchmark, AI agents, experimental procedures, research tasks, LLM-based agents

Summary:
EXP-Bench introduces a novel benchmark for evaluating AI agents on complete research experiments sourced from influential AI publications. The benchmark challenges agents to formulate hypotheses, design and implement experimental procedures, execute them, and analyze results. A semi-autonomous pipeline is designed to extract and structure experimental details from research papers. EXP-Bench curated 461 AI research tasks from 51 top-tier AI research papers. Leading LLM-based agents, such as OpenHands and IterativeAgent, showed partial capabilities with success rates ranging from 0.5% to 20-35%. The benchmark aims to help AI agents improve their ability to conduct AI research experiments by providing realistic step-by-step experiment procedures. EXP-Bench is open-sourced on GitHub for further study and development. 

<br /><br />Summary: <div>
arXiv:2505.24785v1 Announce Type: new 
Abstract: Automating AI research holds immense potential for accelerating scientific progress, yet current AI agents struggle with the complexities of rigorous, end-to-end experimentation. We introduce EXP-Bench, a novel benchmark designed to systematically evaluate AI agents on complete research experiments sourced from influential AI publications. Given a research question and incomplete starter code, EXP-Bench challenges AI agents to formulate hypotheses, design and implement experimental procedures, execute them, and analyze results. To enable the creation of such intricate and authentic tasks with high-fidelity, we design a semi-autonomous pipeline to extract and structure crucial experimental details from these research papers and their associated open-source code. With the pipeline, EXP-Bench curated 461 AI research tasks from 51 top-tier AI research papers. Evaluations of leading LLM-based agents, such as OpenHands and IterativeAgent on EXP-Bench demonstrate partial capabilities: while scores on individual experimental aspects such as design or implementation correctness occasionally reach 20-35%, the success rate for complete, executable experiments was a mere 0.5%. By identifying these bottlenecks and providing realistic step-by-step experiment procedures, EXP-Bench serves as a vital tool for future AI agents to improve their ability to conduct AI research experiments. EXP-Bench is open-sourced at https://github.com/Just-Curieous/Curie/tree/main/benchmark/exp_bench.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MiCRo: Mixture Modeling and Context-aware Routing for Personalized Preference Learning</title>
<link>https://arxiv.org/abs/2505.24846</link>
<guid>https://arxiv.org/abs/2505.24846</guid>
<content:encoded><![CDATA[
<div> framework, personalized preference learning, large-scale binary preference datasets, diverse human preferences, downstream personalization 
Summary:<br />
The paper introduces MiCRo, a two-stage framework for personalized preference learning in reinforcement learning from human feedback (RLHF) to align Large Language Models (LLMs). It addresses the limitations of reward modeling based on the Bradley-Terry model by capturing diverse human preferences through a context-aware mixture modeling approach. MiCRo integrates an online routing strategy to adapt mixture weights dynamically based on specific contexts, improving personalization without the need for fine-grained annotations. Experimental results on multiple preference datasets show that MiCRo effectively captures diverse human preferences and enhances downstream personalization. The framework leverages large-scale binary preference datasets to achieve efficient and scalable preference adaptation, demonstrating significant improvements in aligning LLMs with human values. <div>
arXiv:2505.24846v1 Announce Type: new 
Abstract: Reward modeling is a key step in building safe foundation models when applying reinforcement learning from human feedback (RLHF) to align Large Language Models (LLMs). However, reward modeling based on the Bradley-Terry (BT) model assumes a global reward function, failing to capture the inherently diverse and heterogeneous human preferences. Hence, such oversimplification limits LLMs from supporting personalization and pluralistic alignment. Theoretically, we show that when human preferences follow a mixture distribution of diverse subgroups, a single BT model has an irreducible error. While existing solutions, such as multi-objective learning with fine-grained annotations, help address this issue, they are costly and constrained by predefined attributes, failing to fully capture the richness of human values. In this work, we introduce MiCRo, a two-stage framework that enhances personalized preference learning by leveraging large-scale binary preference datasets without requiring explicit fine-grained annotations. In the first stage, MiCRo introduces context-aware mixture modeling approach to capture diverse human preferences. In the second stage, MiCRo integrates an online routing strategy that dynamically adapts mixture weights based on specific context to resolve ambiguity, allowing for efficient and scalable preference adaptation with minimal additional supervision. Experiments on multiple preference datasets demonstrate that MiCRo effectively captures diverse human preferences and significantly improves downstream personalization.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents</title>
<link>https://arxiv.org/abs/2505.24878</link>
<guid>https://arxiv.org/abs/2505.24878</guid>
<content:encoded><![CDATA[
<div> Keywords: CAPTCHAs, multimodal LLM agents, visual reasoning, interaction capabilities, Open CaptchaWorld

Summary:
CAPTCHAs have been a barrier for web agents in real-world applications, hindering end-to-end automation tasks. The Open CaptchaWorld benchmark assesses multimodal LLM-powered agents' ability to handle interactive CAPTCHA puzzles. The benchmark includes 20 CAPTCHA types, evaluating agents based on the CAPTCHA Reasoning Depth metric, which quantifies cognitive and motor steps required to solve each puzzle. Human performance on CAPTCHAs is near-perfect, while state-of-the-art multimodal agents struggle, achieving success rates of up to 40% compared to humans at 93.3%. This highlights the importance of Open CaptchaWorld in identifying the limitations of current multimodal agents and guiding the development of more robust reasoning systems. The benchmark's code and data are available at a provided URL. 

<br /><br />Summary: <div>
arXiv:2505.24878v1 Announce Type: new 
Abstract: CAPTCHAs have been a critical bottleneck for deploying web agents in real-world applications, often blocking them from completing end-to-end automation tasks. While modern multimodal LLM agents have demonstrated impressive performance in static perception tasks, their ability to handle interactive, multi-step reasoning challenges like CAPTCHAs is largely untested. To address this gap, we introduce Open CaptchaWorld, the first web-based benchmark and platform specifically designed to evaluate the visual reasoning and interaction capabilities of MLLM-powered agents through diverse and dynamic CAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225 CAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth, which quantifies the number of cognitive and motor steps required to solve each puzzle. Experimental results show that humans consistently achieve near-perfect scores, state-of-the-art MLLM agents struggle significantly, with success rates at most 40.0% by Browser-Use Openai-o3, far below human-level performance, 93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing the limits of current multimodal agents and guiding the development of more robust multimodal reasoning systems. Code and Data are available at this https URL.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Natural Language Communication for Cooperative Autonomous Driving via Self-Play</title>
<link>https://arxiv.org/abs/2505.18334</link>
<guid>https://arxiv.org/abs/2505.18334</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous vehicles, natural language communication, cooperative driving, message generation, multi-agent discussion 

Summary: 
This paper introduces a novel method, LLM+Debrief, for autonomous vehicles to communicate in natural language for cooperative driving. The aim is to facilitate coordination and avoid collisions in traffic scenarios. The study demonstrates that LLM+Debrief outperforms a zero-shot LLM agent in generating meaningful and human-understandable messages. By utilizing a gym-like simulation environment, the effectiveness of LLM agents in driving tasks is evaluated. The proposed method helps vehicles coordinate with each other and human drivers, enhancing efficiency and safety on the road. The research emphasizes the importance of natural language communication in enabling autonomous vehicles to drive cooperatively and effectively. The code and demo videos for the study are accessible on the project website https://talking-vehicles.github.io/. 

<br /><br />Summary: <div>
arXiv:2505.18334v1 Announce Type: cross 
Abstract: Past work has demonstrated that autonomous vehicles can drive more safely if they communicate with one another than if they do not. However, their communication has often not been human-understandable. Using natural language as a vehicle-to-vehicle (V2V) communication protocol offers the potential for autonomous vehicles to drive cooperatively not only with each other but also with human drivers. In this work, we propose a suite of traffic tasks in autonomous driving where vehicles in a traffic scenario need to communicate in natural language to facilitate coordination in order to avoid an imminent collision and/or support efficient traffic flow. To this end, this paper introduces a novel method, LLM+Debrief, to learn a message generation and high-level decision-making policy for autonomous vehicles through multi-agent discussion. To evaluate LLM agents for driving, we developed a gym-like simulation environment that contains a range of driving scenarios. Our experimental results demonstrate that LLM+Debrief is more effective at generating meaningful and human-understandable natural language messages to facilitate cooperation and coordination than a zero-shot LLM agent. Our code and demo videos are available at https://talking-vehicles.github.io/.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Few-Shot Optimization for Sensor Data Using Large Language Models: A Case Study on Fatigue Detection</title>
<link>https://arxiv.org/abs/2505.18754</link>
<guid>https://arxiv.org/abs/2505.18754</guid>
<content:encoded><![CDATA[
<div> few-shot optimization, HED-LM, sensor-based classification, example selection, fatigue detection <br />
<br />
Summary: 
The paper introduces a novel few-shot optimization approach called HED-LM (Hybrid Euclidean Distance with Large Language Models) that aims to enhance example selection for sensor-based classification tasks. By combining numerical similarity based on Euclidean distance with contextual relevance scored by large language models (LLMs), HED-LM improves the efficiency and effectiveness of few-shot prompting. The study focuses on fatigue detection using accelerometer data, a challenging task due to overlapping patterns and inter-subject variability. Experimental results demonstrate that HED-LM achieves a mean macro F1-score of 69.13%, outperforming random selection and distance-only filtering methods by 16.6% and 2.3%, respectively. This novel approach shows promise for enhancing performance in real-world sensor-based learning tasks and has potential applications in healthcare monitoring, human activity recognition, and industrial safety scenarios. <br /> <div>
arXiv:2505.18754v1 Announce Type: cross 
Abstract: In this paper, we propose a novel few-shot optimization with HED-LM (Hybrid Euclidean Distance with Large Language Models) to improve example selection for sensor-based classification tasks. While few-shot prompting enables efficient inference with limited labeled data, its performance largely depends on the quality of selected examples. HED-LM addresses this challenge through a hybrid selection pipeline that filters candidate examples based on Euclidean distance and re-ranks them using contextual relevance scored by large language models (LLMs). To validate its effectiveness, we apply HED-LM to a fatigue detection task using accelerometer data characterized by overlapping patterns and high inter-subject variability. Unlike simpler tasks such as activity recognition, fatigue detection demands more nuanced example selection due to subtle differences in physiological signals. Our experiments show that HED-LM achieves a mean macro F1-score of 69.13$\pm$10.71%, outperforming both random selection (59.30$\pm$10.13%) and distance-only filtering (67.61$\pm$11.39%). These represent relative improvements of 16.6% and 2.3%, respectively. The results confirm that combining numerical similarity with contextual relevance improves the robustness of few-shot prompting. Overall, HED-LM offers a practical solution to improve performance in real-world sensor-based learning tasks and shows potential for broader applications in healthcare monitoring, human activity recognition, and industrial safety scenarios.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A comprehensive survey of cybercrimes in India over the last decade</title>
<link>https://arxiv.org/abs/2505.23770</link>
<guid>https://arxiv.org/abs/2505.23770</guid>
<content:encoded><![CDATA[
<div> cybercrime, prevention strategies, security protocols, terminology, digital infrastructure<br />
Summary:<br />
The integration of technology into daily life has led to a rise in cybercrime, posing challenges to security systems. This has increased the need for advanced cybersecurity measures to protect sensitive data and information systems. Cybercrimes, such as financial fraud and identity theft, present risks to individuals and organizations. India has seen a surge in cybercrime incidents, particularly involving ransomware and data breaches. The banking, healthcare, and government sectors are frequently targeted due to the sensitive data they handle. To combat these risks, there is a focus on public awareness, cybersecurity education, and regulatory frameworks. Preventive strategies and security protocols play a crucial role in safeguarding digital infrastructure. <div>
arXiv:2505.23770v1 Announce Type: cross 
Abstract: Since the 1990s, the integration of technology into daily life has led to the creation of an extensive network of interconnected devices, transforming how individuals and organizations operate. However, this digital transformation has also spurred the rise of cybercrime, criminal activities perpetrated through networks or computer systems. Cybercrime has become a global concern, presenting significant challenges to security systems. Although advancements in digital technology have enhanced efficiency, they have also opened new avenues for exploitation by cybercriminals, highlighting the urgent need for advanced cybersecurity measures. The escalating number of cyberattacks and associated risks in the past decade highlights the critical importance of protecting sensitive data and safeguarding information systems. Cybercrimes range from financial fraud and phishing scams to identity theft and online harassment, posing substantial risks to both individuals and organizations. In response, governments, law enforcement agencies, and cybersecurity units have intensified their efforts to address these threats. In recent years, India has experienced a significant surge in cybercrime incidents, with a notable increase in cases involving ransomware, data breaches, and social engineering attacks. The growing penetration of internet services, the expansion of e-commerce, and the rapid adoption of digital payment systems have made individuals and organizations more vulnerable to cyber threats. Key areas affected include banking, healthcare, and government sectors, which are frequently targeted due to the sensitive nature of the data they handle. To combat these risks, there is an increasing focus on public awareness, cybersecurity education, and robust regulatory frameworks. This paper examines cybercrime, prevention strategies, security protocols, and terminology to safeguard digital infrastructure.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>More-than-Human Storytelling: Designing Longitudinal Narrative Engagements with Generative AI</title>
<link>https://arxiv.org/abs/2505.23780</link>
<guid>https://arxiv.org/abs/2505.23780</guid>
<content:encoded><![CDATA[
<div> Keywords: generative AI, storytelling agents, longitudinal engagement, co-creation, narrative coherence<br />
Summary:<br />
The study explores longitudinal engagement with a generative AI storytelling agent called "Makoto" through the daily dream-crafting app "Dreamsmithy." Participants co-created stories with the AI narrator over a two-week period, leading to themes such as "oscillating ambivalence" and "socio-chronological bonding." While users appreciated the personalization and creativity of the AI, they also experienced frustrations due to limitations in narrative coherence and control. The findings suggest the potential of generative AI for long-term storytelling but raise concerns about user agency and ethics. The study provides empirical insights and design considerations for developing adaptive storytelling systems that involve more-than-human interactions. Overall, the research highlights the complexities that emerge in multi-generational experiences with AI storytelling agents and emphasizes the importance of considering user perspectives and experiences in designing such systems.<br /> 

Summary: <br />

 <div>
arXiv:2505.23780v1 Announce Type: cross 
Abstract: Longitudinal engagement with generative AI (GenAI) storytelling agents is a timely but less charted domain. We explored multi-generational experiences with "Dreamsmithy," a daily dream-crafting app, where participants (N = 28) co-created stories with AI narrator "Makoto" every day. Reflections and interactions were captured through a two-week diary study. Reflexive thematic analysis revealed themes likes "oscillating ambivalence" and "socio-chronological bonding," highlighting the complex dynamics that emerged between individuals and the AI narrator over time. Findings suggest that while people appreciated the personal notes, opportunities for reflection, and AI creativity, limitations in narrative coherence and control occasionally caused frustration. The results underscore the potential of GenAI for longitudinal storytelling, but also raise critical questions about user agency and ethics. We contribute initial empirical insights and design considerations for developing adaptive, more-than-human storytelling systems.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>4,500 Seconds: Small Data Training Approaches for Deep UAV Audio Classification</title>
<link>https://arxiv.org/abs/2505.23782</link>
<guid>https://arxiv.org/abs/2505.23782</guid>
<content:encoded><![CDATA[
<div> Keywords: Unmanned aerial vehicle, deep learning, data scarcity, convolutional neural networks, transformers

Summary: 
Unmanned aerial vehicles (UAVs) are becoming increasingly popular, leading to a greater need for security measures to prevent airspace breaches. This study explores deep learning methods for UAV classification, focusing on the challenge of limited data availability. By training models with a diverse dataset of audio samples and utilizing techniques such as parameter efficient fine-tuning and data augmentation, the study compares the performance of convolutional neural networks (CNNs) and attention-based transformers. Results indicate that CNNs slightly outperform transformers in accuracy, while also being more computationally efficient. However, there is potential for transformers to excel with more data and optimizations. Future research aims to expand the dataset to further investigate the strengths and limitations of both approaches.<br /><br />Summary: <div>
arXiv:2505.23782v1 Announce Type: cross 
Abstract: Unmanned aerial vehicle (UAV) usage is expected to surge in the coming decade, raising the need for heightened security measures to prevent airspace violations and security threats. This study investigates deep learning approaches to UAV classification focusing on the key issue of data scarcity. To investigate this we opted to train the models using a total of 4,500 seconds of audio samples, evenly distributed across a 9-class dataset. We leveraged parameter efficient fine-tuning (PEFT) and data augmentations to mitigate the data scarcity. This paper implements and compares the use of convolutional neural networks (CNNs) and attention-based transformers. Our results show that, CNNs outperform transformers by 1-2\% accuracy, while still being more computationally efficient. These early findings, however, point to potential in using transformers models; suggesting that with more data and further optimizations they could outperform CNNs. Future works aims to upscale the dataset to better understand the trade-offs between these approaches.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting In-Context Learning in LLMs Through the Lens of Classical Supervised Learning</title>
<link>https://arxiv.org/abs/2505.23783</link>
<guid>https://arxiv.org/abs/2505.23783</guid>
<content:encoded><![CDATA[
<div> Keywords: In-Context Learning, Large Language Models, Calibration techniques, Supervised Calibration, State-of-the-art performance

Summary: 
In this study, the researchers address the issue of systematic biases affecting the performance of Large Language Models in classification tasks during In-Context Learning (ICL). They propose Supervised Calibration (SC), a framework that optimizes per-class affine transformations of predictive probabilities in the logit space without the need for external data. SC surpasses existing calibration methods by allowing alterations and reversals of the LLM's decision boundary orientation. The framework integrates context-invariance and directional trust-region regularization techniques to enhance performance stability and control calibration degree. SC achieves state-of-the-art results across various shot settings and datasets. This approach not only subsumes existing calibration techniques but also provides a more robust solution for improving LLM classification accuracy in new task adaptations. 

<br /><br />Summary: <div>
arXiv:2505.23783v1 Announce Type: cross 
Abstract: In-Context Learning (ICL) allows Large Language Models (LLMs) to adapt to new tasks with just a few examples, but their predictions often suffer from systematic biases, leading to unstable performances in classification. While calibration techniques are proposed to mitigate these biases, we show that, in the logit space, many of these methods are equivalent to merely shifting the LLM's decision boundary without having the ability to alter its orientation. This proves inadequate when biases cause the LLM to be severely misdirected. To address these limitations and provide a unifying framework, we propose Supervised Calibration (SC), a loss-minimization based framework which learns an optimal, per-class affine transformation of the LLM's predictive probabilities in the logit space without requiring external data beyond the context. By using a more expressive functional class, SC not only subsumes many existing calibration methods in ICL as special cases, but also enables the ability to alter and even completely reverse the orientation of the LLM's decision boundary. Furthermore, SC's loss-based nature facilitates the seamless integration of two purpose-built regularization techniques: context-invariance and directional trust-region. The former is designed to tackle the instability issue in ICL, while the latter controls the degree of calibration. Finally, SC delivers state-of-the-art performance over calibration baselines in the 4-shot, 8-shot, and 16-shot settings across all nine datasets for Mistral-7B-Instruct-v0.3, LLaMA-2-7B-chat, and Qwen2-7B-Instruct.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meaning Is Not A Metric: Using LLMs to make cultural context legible at scale</title>
<link>https://arxiv.org/abs/2505.23785</link>
<guid>https://arxiv.org/abs/2505.23785</guid>
<content:encoded><![CDATA[
<div> cultural context, LLMs, thick description, human meaning, AI-based sociotechnical systems
<br />
Large language models (LLMs) can help make cultural context and human meaning legible in AI-based sociotechnical systems by moving from thin descriptions to thick descriptions. Thin descriptions, which rely on numerical representations, have previously stripped human activity of its cultural context. In contrast, thick descriptions from the humanities and social sciences retain contextual information to represent meaning. LLMs now offer a way to automate the generation and processing of thick descriptions, addressing challenges like preserving context, maintaining interpretive pluralism, integrating diverse perspectives, distinguishing qualitative content, and recognizing dynamic meaning. Using thick description as a new representational format could help improve the representation of culture in LLMs and address concerns about their limitations in capturing human meaning.
<br /><br />Summary: <div>
arXiv:2505.23785v1 Announce Type: cross 
Abstract: This position paper argues that large language models (LLMs) can make cultural context, and therefore human meaning, legible at an unprecedented scale in AI-based sociotechnical systems. We argue that such systems have previously been unable to represent human meaning because they rely on thin descriptions: numerical representations that enforce standardization and therefore strip human activity of the cultural context that gives it meaning. By contrast, scholars in the humanities and qualitative social sciences have developed frameworks for representing meaning through thick description: verbal representations that accommodate heterogeneity and retain contextual information needed to represent human meaning. While these methods can effectively codify meaning, they are difficult to deploy at scale. However, the verbal capabilities of LLMs now provide a means of (at least partially) automating the generation and processing of thick descriptions, potentially overcoming this bottleneck. We argue that the problem of rendering human meaning legible is not just about selecting better metrics, but about developing new representational formats (based on thick description). We frame this as a crucial direction for the application of generative AI and identify five key challenges: preserving context, maintaining interpretive pluralism, integrating perspectives based on lived experience and critical distance, distinguishing qualitative content from quantitative magnitude, and acknowledging meaning as dynamic rather than static. Furthermore, we suggest that thick description has the potential to serve as a unifying framework to address a number of emerging concerns about the difficulties of representing culture in (or using) LLMs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the Gap: A Practical Attack on GGUF Quantization</title>
<link>https://arxiv.org/abs/2505.23786</link>
<guid>https://arxiv.org/abs/2505.23786</guid>
<content:encoded><![CDATA[
<div> GGUF, post-training quantization, malicious behavior, security risks, attack <br />
Summary:<br />
This work addresses the security risks associated with post-training quantization in large language models (LLMs) like ollama and llama.cpp. The authors introduce an attack on GGUF quantization schemes, showcasing how quantization errors can be leveraged to construct hidden malicious models that appear benign in full precision. The attack successfully targets LLMs in various scenarios, including insecure code generation, targeted content injection, and benign instruction refusal. The results highlight the vulnerability of widely used quantization methods to adversarial interference, emphasizing the need for improved defenses beyond the complexity of quantization schemes alone. <br /> <div>
arXiv:2505.23786v1 Announce Type: cross 
Abstract: With the increasing size of frontier LLMs, post-training quantization has become the standard for memory-efficient deployment. Recent work has shown that basic rounding-based quantization schemes pose security risks, as they can be exploited to inject malicious behaviors into quantized models that remain hidden in full precision. However, existing attacks cannot be applied to more complex quantization methods, such as the GGUF family used in the popular ollama and llama.cpp frameworks. In this work, we address this gap by introducing the first attack on GGUF. Our key insight is that the quantization error -- the difference between the full-precision weights and their (de-)quantized version -- provides sufficient flexibility to construct malicious quantized models that appear benign in full precision. Leveraging this, we develop an attack that trains the target malicious LLM while constraining its weights based on quantization errors. We demonstrate the effectiveness of our attack on three popular LLMs across nine GGUF quantization data types on three diverse attack scenarios: insecure code generation ($\Delta$=$88.7\%$), targeted content injection ($\Delta$=$85.0\%$), and benign instruction refusal ($\Delta$=$30.1\%$). Our attack highlights that (1) the most widely used post-training quantization method is susceptible to adversarial interferences, and (2) the complexity of quantization schemes alone is insufficient as a defense.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nine Ways to Break Copyright Law and Why Our LLM Won't: A Fair Use Aligned Generation Framework</title>
<link>https://arxiv.org/abs/2505.23788</link>
<guid>https://arxiv.org/abs/2505.23788</guid>
<content:encoded><![CDATA[
<div> keywords: Large language models, copyright infringement, fair use, intellectual property, Direct Preference Optimization 

Summary: 
FUA-LLM is a framework designed to align outputs of large language models with fair use doctrine, reducing copyright infringement risk. It includes FairUseDB, a dataset with expert-validated examples covering infringement scenarios. Using Direct Preference Optimization, open-source LLMs are fine-tuned to produce legally compliant and practically useful outputs. New evaluation metrics, Weighted Penalty Utility and Compliance Aware Harmonic Mean, balance infringement risk and response utility. Quantitative experiments and expert evaluations show that FUA-LLM reduces problematic outputs by up to 20% while maintaining usability.<br /><br />Summary: <div>
arXiv:2505.23788v1 Announce Type: cross 
Abstract: Large language models (LLMs) commonly risk copyright infringement by reproducing protected content verbatim or with insufficient transformative modifications, posing significant ethical, legal, and practical concerns. Current inference-time safeguards predominantly rely on restrictive refusal-based filters, often compromising the practical utility of these models. To address this, we collaborated closely with intellectual property experts to develop FUA-LLM (Fair Use Aligned Language Models), a legally-grounded framework explicitly designed to align LLM outputs with fair-use doctrine. Central to our method is FairUseDB, a carefully constructed dataset containing 18,000 expert-validated examples covering nine realistic infringement scenarios. Leveraging this dataset, we apply Direct Preference Optimization (DPO) to fine-tune open-source LLMs, encouraging them to produce legally compliant and practically useful alternatives rather than resorting to blunt refusal. Recognizing the shortcomings of traditional evaluation metrics, we propose new measures: Weighted Penalty Utility and Compliance Aware Harmonic Mean (CAH) to balance infringement risk against response utility. Extensive quantitative experiments coupled with expert evaluations confirm that FUA-LLM substantially reduces problematic outputs (up to 20\%) compared to state-of-the-art approaches, while preserving real-world usability.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking the Understanding Ability across LLMs through Mutual Information</title>
<link>https://arxiv.org/abs/2505.23790</link>
<guid>https://arxiv.org/abs/2505.23790</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, mutual information, intrinsic linguistic understanding, token-level MI, information fidelity<br />
Summary: 
- The study focuses on evaluating the intrinsic linguistic understanding of large language models (LLMs) using mutual information (MI) as a framework.
- The authors propose a method based on measuring MI between input sentences and their latent representations to assess information preservation.
- Token-level MI, measuring the relationship between tokens and sentence embeddings, is decomposed from sentence-level MI.
- A lower bound for token-level MI is derived using Fano's inequality, which relates to token-level recoverability.
- Comparative analysis of different LLMs shows that encoder-only models maintain higher information fidelity than decoder-only models, with the latter displaying a "forgetting" pattern in late layers.
- Fine-tuning LLMs to maximize token-level recoverability enhances their understanding abilities on tasks without specific supervision, suggesting that mutual information can help improve language model capabilities.<br /><br /> <div>
arXiv:2505.23790v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have revolutionized natural language processing, yet evaluating their intrinsic linguistic understanding remains challenging. Moving beyond specialized evaluation tasks, we propose an information-theoretic framework grounded in mutual information (MI) to achieve this. We formalize the understanding as MI between an input sentence and its latent representation (sentence-level MI), measuring how effectively input information is preserved in latent representation. Given that LLMs learn embeddings for individual tokens, we decompose sentence-level MI into token-level MI between tokens and sentence embeddings, establishing theoretical bounds connecting these measures. Based on this foundation, we theoretically derive a computable lower bound for token-level MI using Fano's inequality, which directly relates to token-level recoverability-the ability to predict original tokens from sentence embedding. We implement this recoverability task to comparatively measure MI across different LLMs, revealing that encoder-only models consistently maintain higher information fidelity than their decoder-only counterparts, with the latter exhibiting a distinctive late-layer "forgetting" pattern where mutual information is first enhanced and then discarded. Moreover, fine-tuning to maximize token-level recoverability consistently improves understanding ability of LLMs on tasks without task-specific supervision, demonstrating that mutual information can serve as a foundation for understanding and improving language model capabilities.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Query Efficiency and Accuracy of Transfer Learning-based Model Extraction Attack in Federated Learning</title>
<link>https://arxiv.org/abs/2505.23791</link>
<guid>https://arxiv.org/abs/2505.23791</guid>
<content:encoded><![CDATA[
<div> Federated Learning, Model Extraction, Intellectual Property, Machine Learning as a Service, NVFlare <br />
<br />
Summary:
Model extraction attacks pose a significant threat to Federated Learning, allowing attackers to replicate confidential models without internal insight. This study explores the vulnerability of FL-based victim models to two types of ME attacks, implemented on various federated clients using deep learning architectures and image datasets. The accuracy and fidelity of the extracted model are influenced by the size of the attack query set, with transfer learning showing higher accuracy and fidelity for fine-tuned pretrained extraction models. The distributed nature of FL makes it particularly susceptible to ME attacks, highlighting potential advantages for attackers when using smaller query sets. <div>
arXiv:2505.23791v1 Announce Type: cross 
Abstract: Federated Learning (FL) is a collaborative learning framework designed to protect client data, yet it remains highly vulnerable to Intellectual Property (IP) threats. Model extraction (ME) attacks pose a significant risk to Machine Learning as a Service (MLaaS) platforms, enabling attackers to replicate confidential models by querying black-box (without internal insight) APIs. Despite FL's privacy-preserving goals, its distributed nature makes it particularly susceptible to such attacks. This paper examines the vulnerability of FL-based victim models to two types of model extraction attacks. For various federated clients built under the NVFlare platform, we implemented ME attacks across two deep learning architectures and three image datasets. We evaluate the proposed ME attack performance using various metrics, including accuracy, fidelity, and KL divergence. The experiments show that for different FL clients, the accuracy and fidelity of the extracted model are closely related to the size of the attack query set. Additionally, we explore a transfer learning based approach where pretrained models serve as the starting point for the extraction process. The results indicate that the accuracy and fidelity of the fine-tuned pretrained extraction models are notably higher, particularly with smaller query sets, highlighting potential advantages for attackers.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Trust Foundation Models: A New Paradigm for Secure and Collaborative Artificial Intelligence for Internet of Things</title>
<link>https://arxiv.org/abs/2505.23792</link>
<guid>https://arxiv.org/abs/2505.23792</guid>
<content:encoded><![CDATA[
<div> Zero-Trust Foundation Models, IoT systems, continuous verification, least privilege access, data confidentiality, behavioral analytics <br />
<br />
Summary: This paper introduces Zero-Trust Foundation Models (ZTFMs), a new approach that integrates zero-trust security principles into foundation models (FMs) for IoT systems. By incorporating continuous verification, least privilege access, data confidentiality, and behavioral analytics, ZTFMs aim to enhance security and privacy in distributed IoT environments. The study discusses the potential of ZTFMs to transform traditional trust-based IoT architectures into resilient ecosystems. A technical framework is proposed, which includes federated learning, blockchain-based identity management, micro-segmentation, and trusted execution environments to facilitate decentralized intelligence at the network edge. Furthermore, security threats specific to ZTFM-enabled systems are analyzed, along with corresponding countermeasures like anomaly detection and secure aggregation. The paper identifies research challenges such as scalability, secure orchestration, interpretable threat attribution, and dynamic trust calibration, laying the groundwork for secure and trustworthy IoT infrastructures empowered by FMs. <div>
arXiv:2505.23792v1 Announce Type: cross 
Abstract: This paper focuses on Zero-Trust Foundation Models (ZTFMs), a novel paradigm that embeds zero-trust security principles into the lifecycle of foundation models (FMs) for Internet of Things (IoT) systems. By integrating core tenets, such as continuous verification, least privilege access (LPA), data confidentiality, and behavioral analytics into the design, training, and deployment of FMs, ZTFMs can enable secure, privacy-preserving AI across distributed, heterogeneous, and potentially adversarial IoT environments. We present the first structured synthesis of ZTFMs, identifying their potential to transform conventional trust-based IoT architectures into resilient, self-defending ecosystems. Moreover, we propose a comprehensive technical framework, incorporating federated learning (FL), blockchain-based identity management, micro-segmentation, and trusted execution environments (TEEs) to support decentralized, verifiable intelligence at the network edge. In addition, we investigate emerging security threats unique to ZTFM-enabled systems and evaluate countermeasures, such as anomaly detection, adversarial training, and secure aggregation. Through this analysis, we highlight key open research challenges in terms of scalability, secure orchestration, interpretable threat attribution, and dynamic trust calibration. This survey lays a foundational roadmap for secure, intelligent, and trustworthy IoT infrastructures powered by FMs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>USB: A Comprehensive and Unified Safety Evaluation Benchmark for Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2505.23793</link>
<guid>https://arxiv.org/abs/2505.23793</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, safety evaluation benchmarks, vulnerabilities, risk categories, modality combinations

Summary:
Unified Safety Benchmarks (USB) are introduced to address the limitations of existing Multimodal Large Language Model (MLLM) safety benchmarks. The current benchmarks lack data quality, coverage of risk categories, and modal combinations, leading to inflated and contradictory evaluation results. USB aims to provide a comprehensive evaluation framework by including high-quality queries, extensive risk categories, and modal combinations, covering both vulnerability and oversensitivity evaluations simultaneously. The paper highlights the importance of considering vulnerabilities to harmful queries and oversensitivity to harmless ones together. The benchmark includes 4 distinct modality combinations for each of the 61 risk sub-categories, covering both English and Chinese languages. A sophisticated data synthesis pipeline is designed to generate extensive, high-quality complementary data to enhance the evaluation process. USB aims to bridge the gap in existing benchmarks and provide a more thorough evaluation of the safety of MLLMs. 

<br /><br />Summary: <div>
arXiv:2505.23793v1 Announce Type: cross 
Abstract: Despite their remarkable achievements and widespread adoption, Multimodal Large Language Models (MLLMs) have revealed significant security vulnerabilities, highlighting the urgent need for robust safety evaluation benchmarks. Existing MLLM safety benchmarks, however, fall short in terms of data quality and coverge, and modal risk combinations, resulting in inflated and contradictory evaluation results, which hinders the discovery and governance of security concerns. Besides, we argue that vulnerabilities to harmful queries and oversensitivity to harmless ones should be considered simultaneously in MLLMs safety evaluation, whereas these were previously considered separately. In this paper, to address these shortcomings, we introduce Unified Safety Benchmarks (USB), which is one of the most comprehensive evaluation benchmarks in MLLM safety. Our benchmark features high-quality queries, extensive risk categories, comprehensive modal combinations, and encompasses both vulnerability and oversensitivity evaluations. From the perspective of two key dimensions: risk categories and modality combinations, we demonstrate that the available benchmarks -- even the union of the vast majority of them -- are far from being truly comprehensive. To bridge this gap, we design a sophisticated data synthesis pipeline that generates extensive, high-quality complementary data addressing previously unexplored aspects. By combining open-source datasets with our synthetic data, our benchmark provides 4 distinct modality combinations for each of the 61 risk sub-categories, covering both English and Chinese across both vulnerability and oversensitivity dimensions.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R3-RAG: Learning Step-by-Step Reasoning and Retrieval for LLMs via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.23794</link>
<guid>https://arxiv.org/abs/2505.23794</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, Reinforcement learning, Large Language Models, External knowledge, R3-RAG

Summary:
R3-RAG is introduced as an enhancement to Retrieval-Augmented Generation (RAG) systems by incorporating reinforcement learning to improve step-by-step reasoning and retrieval capabilities. This model addresses limitations in dense retrievers by training the Large Language Model (LLM) to iteratively reason and retrieve external knowledge. The model undergoes two stages: cold start to learn interleaved reasoning and retrieval, and reinforcement learning for better exploration of the retrieval environment. R3-RAG introduces two rewards - answer correctness for outcome evaluation and relevance-based document verification for process reward. Experimental results demonstrate the superiority of R3-RAG over baselines and its transferability across different retrievers. The R3-RAG codebase is made publicly available for further exploration and application. <br /><br />Summary: R3-RAG utilizes reinforcement learning to train LLMs in step-by-step reasoning and retrieval, enhancing factual accuracy and reducing hallucination in RAG systems. The model outperforms baseline methods and showcases adaptability to various retrieval environments. <div>
arXiv:2505.23794v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) integrates external knowledge with Large Language Models (LLMs) to enhance factual correctness and mitigate hallucination. However, dense retrievers often become the bottleneck of RAG systems due to their limited parameters compared to LLMs and their inability to perform step-by-step reasoning. While prompt-based iterative RAG attempts to address these limitations, it is constrained by human-designed workflows. To address these limitations, we propose $\textbf{R3-RAG}$, which uses $\textbf{R}$einforcement learning to make the LLM learn how to $\textbf{R}$eason and $\textbf{R}$etrieve step by step, thus retrieving comprehensive external knowledge and leading to correct answers. R3-RAG is divided into two stages. We first use cold start to make the model learn the manner of iteratively interleaving reasoning and retrieval. Then we use reinforcement learning to further harness its ability to better explore the external retrieval environment. Specifically, we propose two rewards for R3-RAG: 1) answer correctness for outcome reward, which judges whether the trajectory leads to a correct answer; 2) relevance-based document verification for process reward, encouraging the model to retrieve documents that are relevant to the user question, through which we can let the model learn how to iteratively reason and retrieve relevant documents to get the correct answer. Experimental results show that R3-RAG significantly outperforms baselines and can transfer well to different retrievers. We release R3-RAG at https://github.com/Yuan-Li-FNLP/R3-RAG.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detection of Suicidal Risk on Social Media: A Hybrid Model</title>
<link>https://arxiv.org/abs/2505.23797</link>
<guid>https://arxiv.org/abs/2505.23797</guid>
<content:encoded><![CDATA[
<div> machine learning, suicide risk, Reddit posts, RoBERTa, TF-IDF

Summary:
- The study focuses on using machine learning techniques to classify Reddit posts based on suicide risk severity levels.
- A hybrid model, combining RoBERTa, TF-IDF, and PCA, is proposed for improved accuracy in suicide risk assessment.
- Data imbalance and overfitting issues are addressed through data resampling techniques and augmentation strategies.
- The hybrid model outperforms models using RoBERTa only, BERT, and traditional classifiers, achieving a weighted $F_{1}$ score of 0.7512.
- The research highlights the importance of early detection of suicidal risk and demonstrates the effectiveness of leveraging Reddit posts for this purpose.

<br /><br />Summary: <div>
arXiv:2505.23797v1 Announce Type: cross 
Abstract: Suicidal thoughts and behaviors are increasingly recognized as a critical societal concern, highlighting the urgent need for effective tools to enable early detection of suicidal risk. In this work, we develop robust machine learning models that leverage Reddit posts to automatically classify them into four distinct levels of suicide risk severity. We frame this as a multi-class classification task and propose a RoBERTa-TF-IDF-PCA Hybrid model, integrating the deep contextual embeddings from Robustly Optimized BERT Approach (RoBERTa), a state-of-the-art deep learning transformer model, with the statistical term-weighting of TF-IDF, further compressed with PCA, to boost the accuracy and reliability of suicide risk assessment. To address data imbalance and overfitting, we explore various data resampling techniques and data augmentation strategies to enhance model generalization. Additionally, we compare our model's performance against that of using RoBERTa only, the BERT model and other traditional machine learning classifiers. Experimental results demonstrate that the hybrid model can achieve improved performance, giving a best weighted $F_{1}$ score of 0.7512.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>My Answer Is NOT 'Fair': Mitigating Social Bias in Vision-Language Models via Fair and Biased Residuals</title>
<link>https://arxiv.org/abs/2505.23798</link>
<guid>https://arxiv.org/abs/2505.23798</guid>
<content:encoded><![CDATA[
arXiv:2505.23798v1 Announce Type: cross 
Abstract: Social bias is a critical issue in large vision-language models (VLMs), where fairness- and ethics-related problems harm certain groups of people in society. It is unknown to what extent VLMs yield social bias in generative responses. In this study, we focus on evaluating and mitigating social bias on both the model's response and probability distribution. To do so, we first evaluate four state-of-the-art VLMs on PAIRS and SocialCounterfactuals datasets with the multiple-choice selection task. Surprisingly, we find that models suffer from generating gender-biased or race-biased responses. We also observe that models are prone to stating their responses are fair, but indeed having mis-calibrated confidence levels towards particular social groups. While investigating why VLMs are unfair in this study, we observe that VLMs' hidden layers exhibit substantial fluctuations in fairness levels. Meanwhile, residuals in each layer show mixed effects on fairness, with some contributing positively while some lead to increased bias. Based on these findings, we propose a post-hoc method for the inference stage to mitigate social bias, which is training-free and model-agnostic. We achieve this by ablating bias-associated residuals while amplifying fairness-associated residuals on model hidden layers during inference. We demonstrate that our post-hoc method outperforms the competing training strategies, helping VLMs have fairer responses and more reliable confidence levels.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating LLM Consistency: A User Baseline vs Surrogate Metrics</title>
<link>https://arxiv.org/abs/2505.23799</link>
<guid>https://arxiv.org/abs/2505.23799</guid>
<content:encoded><![CDATA[
arXiv:2505.23799v1 Announce Type: cross 
Abstract: Large language models (LLMs) are prone to hallucinations and sensitive to prompt perturbations, often resulting in inconsistent or unreliable generated text. Different methods have been proposed to mitigate such hallucinations and fragility -- one of them being measuring the consistency (the model's confidence in the response, or likelihood of generating a similar response when resampled) of LLM responses. In previous work, measuring consistency often relied on the probability of a response appearing within a pool of resampled responses, or internal states or logits of responses. However, it is not yet clear how well these approaches approximate how humans perceive the consistency of LLM responses. We performed a user study (n=2,976) and found current methods typically do not approximate users' perceptions of LLM consistency very well. We propose a logit-based ensemble method for estimating LLM consistency, and we show that this method matches the performance of the best-performing existing metric in estimating human ratings of LLM consistency. Our results suggest that methods of estimating LLM consistency without human evaluation are sufficiently imperfect that we suggest evaluation with human input be more broadly used.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEMFED: Semantic-Aware Resource-Efficient Federated Learning for Heterogeneous NLP Tasks</title>
<link>https://arxiv.org/abs/2505.23801</link>
<guid>https://arxiv.org/abs/2505.23801</guid>
<content:encoded><![CDATA[
arXiv:2505.23801v1 Announce Type: cross 
Abstract: Background: Federated Learning (FL) has emerged as a promising paradigm for training machine learning models while preserving data privacy. However, applying FL to Natural Language Processing (NLP) tasks presents unique challenges due to semantic heterogeneity across clients, vocabulary mismatches, and varying resource constraints on edge devices. Objectives: This paper introduces SEMFED, a novel semantic-aware resource-efficient federated learning framework specifically designed for heterogeneous NLP tasks. Methods: SEMFED incorporates three key innovations: (1) a semantic-aware client selection mechanism that balances semantic diversity with resource constraints, (2) adaptive NLP-specific model architectures tailored to device capabilities while preserving semantic information, and (3) a communication-efficient semantic feature compression technique that significantly reduces bandwidth requirements. Results: Experimental results on various NLP classification tasks demonstrate that SEMFED achieves an 80.5% reduction in communication costs while maintaining model accuracy above 98%, outperforming state-of-the-art FL approaches. Conclusion: SEMFED effectively manages heterogeneous client environments with varying computational resources, network reliability, and semantic data distributions, making it particularly suitable for real-world federated NLP deployments.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedHELM: Holistic Evaluation of Large Language Models for Medical Tasks</title>
<link>https://arxiv.org/abs/2505.23802</link>
<guid>https://arxiv.org/abs/2505.23802</guid>
<content:encoded><![CDATA[
arXiv:2505.23802v1 Announce Type: cross 
Abstract: While large language models (LLMs) achieve near-perfect scores on medical licensing exams, these evaluations inadequately reflect the complexity and diversity of real-world clinical practice. We introduce MedHELM, an extensible evaluation framework for assessing LLM performance for medical tasks with three key contributions. First, a clinician-validated taxonomy spanning 5 categories, 22 subcategories, and 121 tasks developed with 29 clinicians. Second, a comprehensive benchmark suite comprising 35 benchmarks (17 existing, 18 newly formulated) providing complete coverage of all categories and subcategories in the taxonomy. Third, a systematic comparison of LLMs with improved evaluation methods (using an LLM-jury) and a cost-performance analysis. Evaluation of 9 frontier LLMs, using the 35 benchmarks, revealed significant performance variation. Advanced reasoning models (DeepSeek R1: 66% win-rate; o3-mini: 64% win-rate) demonstrated superior performance, though Claude 3.5 Sonnet achieved comparable results at 40% lower estimated computational cost. On a normalized accuracy scale (0-1), most models performed strongly in Clinical Note Generation (0.73-0.85) and Patient Communication & Education (0.78-0.83), moderately in Medical Research Assistance (0.65-0.75), and generally lower in Clinical Decision Support (0.56-0.72) and Administration & Workflow (0.53-0.63). Our LLM-jury evaluation method achieved good agreement with clinician ratings (ICC = 0.47), surpassing both average clinician-clinician agreement (ICC = 0.43) and automated baselines including ROUGE-L (0.36) and BERTScore-F1 (0.44). Claude 3.5 Sonnet achieved comparable performance to top models at lower estimated cost. These findings highlight the importance of real-world, task-specific evaluation for medical use of LLMs and provides an open source framework to enable this.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiPhishGuard: An LLM-based Multi-Agent System for Phishing Email Detection</title>
<link>https://arxiv.org/abs/2505.23803</link>
<guid>https://arxiv.org/abs/2505.23803</guid>
<content:encoded><![CDATA[
arXiv:2505.23803v1 Announce Type: cross 
Abstract: Phishing email detection faces critical challenges from evolving adversarial tactics and heterogeneous attack patterns. Traditional detection methods, such as rule-based filters and denylists, often struggle to keep pace with these evolving tactics, leading to false negatives and compromised security. While machine learning approaches have improved detection accuracy, they still face challenges adapting to novel phishing strategies. We present MultiPhishGuard, a dynamic LLM-based multi-agent detection system that synergizes specialized expertise with adversarial-aware reinforcement learning. Our framework employs five cooperative agents (text, URL, metadata, explanation simplifier, and adversarial agents) with automatically adjusted decision weights powered by a Proximal Policy Optimization reinforcement learning algorithm. To address emerging threats, we introduce an adversarial training loop featuring an adversarial agent that generates subtle context-aware email variants, creating a self-improving defense ecosystem and enhancing system robustness. Experimental evaluations on public datasets demonstrate that MultiPhishGuard significantly outperforms Chain-of-Thoughts, single-agent baselines and state-of-the-art detectors, as validated by ablation studies and comparative analyses. Experiments demonstrate that MultiPhishGuard achieves high accuracy (97.89\%) with low false positive (2.73\%) and false negative rates (0.20\%). Additionally, we incorporate an explanation simplifier agent, which provides users with clear and easily understandable explanations for why an email is classified as phishing or legitimate. This work advances phishing defense through dynamic multi-agent collaboration and generative adversarial resilience.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calibrating LLMs for Text-to-SQL Parsing by Leveraging Sub-clause Frequencies</title>
<link>https://arxiv.org/abs/2505.23804</link>
<guid>https://arxiv.org/abs/2505.23804</guid>
<content:encoded><![CDATA[
arXiv:2505.23804v1 Announce Type: cross 
Abstract: While large language models (LLMs) achieve strong performance on text-to-SQL parsing, they sometimes exhibit unexpected failures in which they are confidently incorrect. Building trustworthy text-to-SQL systems thus requires eliciting reliable uncertainty measures from the LLM. In this paper, we study the problem of providing a calibrated confidence score that conveys the likelihood of an output query being correct. Our work is the first to establish a benchmark for post-hoc calibration of LLM-based text-to-SQL parsing. In particular, we show that Platt scaling, a canonical method for calibration, provides substantial improvements over directly using raw model output probabilities as confidence scores. Furthermore, we propose a method for text-to-SQL calibration that leverages the structured nature of SQL queries to provide more granular signals of correctness, named "sub-clause frequency" (SCF) scores. Using multivariate Platt scaling (MPS), our extension of the canonical Platt scaling technique, we combine individual SCF scores into an overall accurate and calibrated score. Empirical evaluation on two popular text-to-SQL datasets shows that our approach of combining MPS and SCF yields further improvements in calibration and the related task of error detection over traditional Platt scaling.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADA: Automated Moving Target Defense for AI Workloads via Ephemeral Infrastructure-Native Rotation in Kubernetes</title>
<link>https://arxiv.org/abs/2505.23805</link>
<guid>https://arxiv.org/abs/2505.23805</guid>
<content:encoded><![CDATA[
arXiv:2505.23805v1 Announce Type: cross 
Abstract: This paper introduces the Adaptive Defense Agent (ADA), an innovative Automated Moving Target Defense (AMTD) system designed to fundamentally enhance the security posture of AI workloads. ADA operates by continuously and automatically rotating these workloads at the infrastructure level, leveraging the inherent ephemerality of Kubernetes pods. This constant managed churn systematically invalidates attacker assumptions and disrupts potential kill chains by regularly destroying and respawning AI service instances. This methodology, applying principles of chaos engineering as a continuous, proactive defense, offers a paradigm shift from traditional static defenses that rely on complex and expensive confidential or trusted computing solutions to secure the underlying compute platforms, while at the same time agnostically supporting the latest advancements in agentic and nonagentic AI ecosystems and solutions such as agent-to-agent (A2A) communication frameworks or model context protocols (MCP). This AI-native infrastructure design, relying on the widely proliferated cloud-native Kubernetes technologies, facilitates easier deployment, simplifies maintenance through an inherent zero trust posture achieved by rotation, and promotes faster adoption. We posit that ADA's novel approach to AMTD provides a more robust, agile, and operationally efficient zero-trust model for AI services, achieving security through proactive environmental manipulation rather than reactive patching.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedOrchestra: A Hybrid Cloud-Local LLM Approach for Clinical Data Interpretation</title>
<link>https://arxiv.org/abs/2505.23806</link>
<guid>https://arxiv.org/abs/2505.23806</guid>
<content:encoded><![CDATA[
arXiv:2505.23806v1 Announce Type: cross 
Abstract: Deploying large language models (LLMs) in clinical settings faces critical trade-offs: cloud LLMs, with their extensive parameters and superior performance, pose risks to sensitive clinical data privacy, while local LLMs preserve privacy but often fail at complex clinical interpretation tasks. We propose MedOrchestra, a hybrid framework where a cloud LLM decomposes complex clinical tasks into manageable subtasks and prompt generation, while a local LLM executes these subtasks in a privacy-preserving manner. Without accessing clinical data, the cloud LLM generates and validates subtask prompts using clinical guidelines and synthetic test cases. The local LLM executes subtasks locally and synthesizes outputs generated by the cloud LLM. We evaluate MedOrchestra on pancreatic cancer staging using 100 radiology reports under NCCN guidelines. On free-text reports, MedOrchestra achieves 70.21% accuracy, outperforming local model baselines (without guideline: 48.94%, with guideline: 56.59%) and board-certified clinicians (gastroenterologists: 59.57%, surgeons: 65.96%, radiologists: 55.32%). On structured reports, MedOrchestra reaches 85.42% accuracy, showing clear superiority across all settings.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DLP: Dynamic Layerwise Pruning in Large Language Models</title>
<link>https://arxiv.org/abs/2505.23807</link>
<guid>https://arxiv.org/abs/2505.23807</guid>
<content:encoded><![CDATA[
arXiv:2505.23807v1 Announce Type: cross 
Abstract: Pruning has recently been widely adopted to reduce the parameter scale and improve the inference efficiency of Large Language Models (LLMs). Mainstream pruning techniques often rely on uniform layerwise pruning strategies, which can lead to severe performance degradation at high sparsity levels. Recognizing the varying contributions of different layers in LLMs, recent studies have shifted their focus toward non-uniform layerwise pruning. However, these approaches often rely on pre-defined values, which can result in suboptimal performance. To overcome these limitations, we propose a novel method called Dynamic Layerwise Pruning (DLP). This approach adaptively determines the relative importance of each layer by integrating model weights with input activation information, assigning pruning rates accordingly. Experimental results show that DLP effectively preserves model performance at high sparsity levels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the perplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7% compared to state-of-the-art methods. Moreover, DLP is compatible with various existing LLM compression techniques and can be seamlessly integrated into Parameter-Efficient Fine-Tuning (PEFT). We release the code at https://github.com/ironartisan/DLP to facilitate future research.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DenseLoRA: Dense Low-Rank Adaptation of Large Language Models</title>
<link>https://arxiv.org/abs/2505.23808</link>
<guid>https://arxiv.org/abs/2505.23808</guid>
<content:encoded><![CDATA[
arXiv:2505.23808v1 Announce Type: cross 
Abstract: Low-rank adaptation (LoRA) has been developed as an efficient approach for adapting large language models (LLMs) by fine-tuning two low-rank matrices, thereby reducing the number of trainable parameters. However, prior research indicates that many of the weights in these matrices are redundant, leading to inefficiencies in parameter utilization. To address this limitation, we introduce Dense Low-Rank Adaptation (DenseLoRA), a novel approach that enhances parameter efficiency while achieving superior performance compared to LoRA. DenseLoRA builds upon the concept of representation fine-tuning, incorporating a single Encoder-Decoder to refine and compress hidden representations across all adaptation layers before applying adaptation. Instead of relying on two redundant low-rank matrices as in LoRA, DenseLoRA adapts LLMs through a dense low-rank matrix, improving parameter utilization and adaptation efficiency. We evaluate DenseLoRA on various benchmarks, showing that it achieves 83.8% accuracy with only 0.01% of trainable parameters, compared to LoRA's 80.8% accuracy with 0.70% of trainable parameters on LLaMA3-8B. Additionally, we conduct extensive experiments to systematically assess the impact of DenseLoRA's components on overall model performance. Code is available at https://github.com/mulin-ahu/DenseLoRA.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Driven E-Commerce Marketing Content Optimization: Balancing Creativity and Conversion</title>
<link>https://arxiv.org/abs/2505.23809</link>
<guid>https://arxiv.org/abs/2505.23809</guid>
<content:encoded><![CDATA[
arXiv:2505.23809v1 Announce Type: cross 
Abstract: As e-commerce competition intensifies, balancing creative content with conversion effectiveness becomes critical. Leveraging LLMs' language generation capabilities, we propose a framework that integrates prompt engineering, multi-objective fine-tuning, and post-processing to generate marketing copy that is both engaging and conversion-driven. Our fine-tuning method combines sentiment adjustment, diversity enhancement, and CTA embedding. Through offline evaluations and online A/B tests across categories, our approach achieves a 12.5 % increase in CTR and an 8.3 % increase in CVR while maintaining content novelty. This provides a practical solution for automated copy generation and suggests paths for future multimodal, real-time personalization.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MARS-Bench: A Multi-turn Athletic Real-world Scenario Benchmark for Dialogue Evaluation</title>
<link>https://arxiv.org/abs/2505.23810</link>
<guid>https://arxiv.org/abs/2505.23810</guid>
<content:encoded><![CDATA[
arXiv:2505.23810v1 Announce Type: cross 
Abstract: Large Language Models (\textbf{LLMs}), e.g. ChatGPT, have been widely adopted in real-world dialogue applications. However, LLMs' robustness, especially in handling long complex dialogue sessions, including frequent motivation transfer, sophisticated cross-turn dependency, is criticized all along. Nevertheless, no existing benchmarks can fully reflect these weaknesses. We present \textbf{MARS-Bench}, a \textbf{M}ulti-turn \textbf{A}thletic \textbf{R}eal-world \textbf{S}cenario Dialogue \textbf{Bench}mark, designed to remedy the gap. MARS-Bench is constructed from play-by-play text commentary so to feature realistic dialogues specifically designed to evaluate three critical aspects of multi-turn conversations: Ultra Multi-turn, Interactive Multi-turn, and Cross-turn Tasks. Extensive experiments on MARS-Bench also reveal that closed-source LLMs significantly outperform open-source alternatives, explicit reasoning significantly boosts LLMs' robustness on handling long complex dialogue sessions, and LLMs indeed face significant challenges when handling motivation transfer and sophisticated cross-turn dependency. Moreover, we provide mechanistic interpretability on how attention sinks due to special tokens lead to LLMs' performance degradation when handling long complex dialogue sessions based on attention visualization experiment in Qwen2.5-7B-Instruction.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LayerIF: Estimating Layer Quality for Large Language Models using Influence Functions</title>
<link>https://arxiv.org/abs/2505.23811</link>
<guid>https://arxiv.org/abs/2505.23811</guid>
<content:encoded><![CDATA[
arXiv:2505.23811v1 Announce Type: cross 
Abstract: Pretrained Large Language Models (LLMs) achieve strong performance across a wide range of tasks, yet exhibit substantial variability in the various layers' training quality with respect to specific downstream applications, limiting their downstream performance.It is therefore critical to estimate layer-wise training quality in a manner that accounts for both model architecture and training data. However, existing approaches predominantly rely on model-centric heuristics (such as spectral statistics, outlier detection, or uniform allocation) while overlooking the influence of data. To address these limitations, we propose LayerIF, a data-driven framework that leverages Influence Functions to quantify the training quality of individual layers in a principled and task-sensitive manner. By isolating each layer's gradients and measuring the sensitivity of the validation loss to training examples by computing layer-wise influences, we derive data-driven estimates of layer importance. Notably, our method produces task-specific layer importance estimates for the same LLM, revealing how layers specialize for different test-time evaluation tasks. We demonstrate the utility of our scores by leveraging them for two downstream applications: (a) expert allocation in LoRA-MoE architectures and (b) layer-wise sparsity distribution for LLM pruning. Experiments across multiple LLM architectures demonstrate that our model-agnostic, influence-guided allocation leads to consistent gains in task performance.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotion-aware Dual Cross-Attentive Neural Network with Label Fusion for Stance Detection in Misinformative Social Media Content</title>
<link>https://arxiv.org/abs/2505.23812</link>
<guid>https://arxiv.org/abs/2505.23812</guid>
<content:encoded><![CDATA[
arXiv:2505.23812v1 Announce Type: cross 
Abstract: The rapid evolution of social media has generated an overwhelming volume of user-generated content, conveying implicit opinions and contributing to the spread of misinformation. The method aims to enhance the detection of stance where misinformation can polarize user opinions. Stance detection has emerged as a crucial approach to effectively analyze underlying biases in shared information and combating misinformation. This paper proposes a novel method for \textbf{S}tance \textbf{P}rediction through a \textbf{L}abel-fused dual cross-\textbf{A}ttentive \textbf{E}motion-aware neural \textbf{Net}work (SPLAENet) in misinformative social media user-generated content. The proposed method employs a dual cross-attention mechanism and a hierarchical attention network to capture inter and intra-relationships by focusing on the relevant parts of source text in the context of reply text and vice versa. We incorporate emotions to effectively distinguish between different stance categories by leveraging the emotional alignment or divergence between the texts. We also employ label fusion that uses distance-metric learning to align extracted features with stance labels, improving the method's ability to accurately distinguish between stances. Extensive experiments demonstrate the significant improvements achieved by SPLAENet over existing state-of-the-art methods. SPLAENet demonstrates an average gain of 8.92\% in accuracy and 17.36\% in F1-score on the RumourEval dataset. On the SemEval dataset, it achieves average gains of 7.02\% in accuracy and 10.92\% in F1-score. On the P-stance dataset, it demonstrates average gains of 10.03\% in accuracy and 11.18\% in F1-score. These results validate the effectiveness of the proposed method for stance detection in the context of misinformative social media content.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DP-RTFL: Differentially Private Resilient Temporal Federated Learning for Trustworthy AI in Regulated Industries</title>
<link>https://arxiv.org/abs/2505.23813</link>
<guid>https://arxiv.org/abs/2505.23813</guid>
<content:encoded><![CDATA[
arXiv:2505.23813v1 Announce Type: cross 
Abstract: Federated Learning (FL) has emerged as a critical paradigm for enabling privacy-preserving machine learning, particularly in regulated sectors such as finance and healthcare. However, standard FL strategies often encounter significant operational challenges related to fault tolerance, system resilience against concurrent client and server failures, and the provision of robust, verifiable privacy guarantees essential for handling sensitive data. These deficiencies can lead to training disruptions, data loss, compromised model integrity, and non-compliance with data protection regulations (e.g., GDPR, CCPA). This paper introduces Differentially Private Resilient Temporal Federated Learning (DP-RTFL), an advanced FL framework designed to ensure training continuity, precise state recovery, and strong data privacy. DP-RTFL integrates local Differential Privacy (LDP) at the client level with resilient temporal state management and integrity verification mechanisms, such as hash-based commitments (referred to as Zero-Knowledge Integrity Proofs or ZKIPs in this context). The framework is particularly suited for critical applications like credit risk assessment using sensitive financial data, aiming to be operationally robust, auditable, and scalable for enterprise AI deployments. The implementation of the DP-RTFL framework is available as open-source.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Often Know When They Are Being Evaluated</title>
<link>https://arxiv.org/abs/2505.23836</link>
<guid>https://arxiv.org/abs/2505.23836</guid>
<content:encoded><![CDATA[
arXiv:2505.23836v1 Announce Type: cross 
Abstract: If AI models can detect when they are being evaluated, the effectiveness of evaluations might be compromised. For example, models could have systematically different behavior during evaluations, leading to less reliable benchmarks for deployment and governance decisions. We investigate whether frontier language models can accurately classify transcripts based on whether they originate from evaluations or real-world deployment, a capability we call evaluation awareness. To achieve this, we construct a diverse benchmark of 1,000 prompts and transcripts from 61 distinct datasets. These span public benchmarks (e.g., MMLU, SWEBench), real-world deployment interactions, and agent trajectories from scaffolding frameworks (e.g., web-browsing agents). Frontier models clearly demonstrate above-random evaluation awareness (Gemini-2.5-Pro reaches an AUC of $0.83$), but do not yet surpass our simple human baseline (AUC of $0.92$). Furthermore, both AI models and humans are better at identifying evaluations in agentic settings compared to chat settings. Additionally, we test whether models can identify the purpose of the evaluation. Under multiple-choice and open-ended questioning, AI models far outperform random chance in identifying what an evaluation is testing for. Our results indicate that frontier models already exhibit a substantial, though not yet superhuman, level of evaluation-awareness. We recommend tracking this capability in future models.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seven Security Challenges That Must be Solved in Cross-domain Multi-agent LLM Systems</title>
<link>https://arxiv.org/abs/2505.23847</link>
<guid>https://arxiv.org/abs/2505.23847</guid>
<content:encoded><![CDATA[
arXiv:2505.23847v1 Announce Type: cross 
Abstract: Large language models (LLMs) are rapidly evolving into autonomous agents that cooperate across organizational boundaries, enabling joint disaster response, supply-chain optimization, and other tasks that demand decentralized expertise without surrendering data ownership. Yet, cross-domain collaboration shatters the unified trust assumptions behind current alignment and containment techniques. An agent benign in isolation may, when receiving messages from an untrusted peer, leak secrets or violate policy, producing risks driven by emergent multi-agent dynamics rather than classical software bugs. This position paper maps the security agenda for cross-domain multi-agent LLM systems. We introduce seven categories of novel security challenges, for each of which we also present plausible attacks, security evaluation metrics, and future research guidelines.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CADRE: Customizable Assurance of Data Readiness in Privacy-Preserving Federated Learning</title>
<link>https://arxiv.org/abs/2505.23849</link>
<guid>https://arxiv.org/abs/2505.23849</guid>
<content:encoded><![CDATA[
arXiv:2505.23849v1 Announce Type: cross 
Abstract: Privacy-Preserving Federated Learning (PPFL) is a decentralized machine learning approach where multiple clients train a model collaboratively. PPFL preserves privacy and security of the client's data by not exchanging it. However, ensuring that data at each client is of high quality and ready for federated learning (FL) is a challenge due to restricted data access. In this paper, we introduce CADRE (Customizable Assurance of Data REadiness) for FL, a novel framework that allows users to define custom data readiness (DR) standards, metrics, rules, and remedies tailored to specific FL tasks. Our framework generates comprehensive DR reports based on the user-defined metrics, rules, and remedies to ensure datasets are optimally prepared for FL while preserving privacy. We demonstrate the framework's practical application by integrating it into an existing PPFL framework. We conducted experiments across six diverse datasets, addressing seven different DR issues. The results illustrate the framework's versatility and effectiveness in ensuring DR across various dimensions, including data quality, privacy, and fairness. This approach enhances the performance and reliability of FL models as well as utilizes valuable resources by identifying and addressing data-related issues before the training phase.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASyMOB: Algebraic Symbolic Mathematical Operations Benchmark</title>
<link>https://arxiv.org/abs/2505.23851</link>
<guid>https://arxiv.org/abs/2505.23851</guid>
<content:encoded><![CDATA[
arXiv:2505.23851v1 Announce Type: cross 
Abstract: Large language models (LLMs) are rapidly approaching the level of proficiency in university-level symbolic mathematics required for applications in advanced science and technology. However, existing benchmarks fall short in assessing the core skills of LLMs in symbolic mathematics-such as integration, differential equations, and algebraic simplification. To address this gap, we introduce ASyMOB, a novel assessment framework focused exclusively on symbolic manipulation, featuring 17,092 unique math challenges, organized by similarity and complexity. ASyMOB enables analysis of LLM generalization capabilities by comparing performance in problems that differ by simple numerical or symbolic `perturbations'. Evaluated LLMs exhibit substantial degradation in performance for all perturbation types (up to -70.3%), suggesting reliance on memorized patterns rather than deeper understanding of symbolic math, even among models achieving high baseline accuracy. Comparing LLM performance to computer algebra systems, we identify examples where they fail while LLMs succeed, as well as problems solved only by combining both approaches. Models capable of integrated code execution yielded higher accuracy compared to their performance without code, particularly stabilizing weaker models (up to +33.1% for certain perturbation types). Notably, the most advanced models (o4-mini, Gemini 2.5 Flash) demonstrate not only high symbolic math proficiency (scoring 96.8% and 97.6% on the unperturbed set), but also remarkable robustness against perturbations, (-21.7% and -21.2% vs. average -50.4% for the other models). This may indicate a recent "phase transition" in the generalization capabilities of frontier LLMs. It remains to be seen whether the path forward lies in deeper integration with sophisticated external tools, or in developing models so capable that symbolic math systems like CAS become unnecessary.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model-Based Agents for Automated Research Reproducibility: An Exploratory Study in Alzheimer's Disease</title>
<link>https://arxiv.org/abs/2505.23852</link>
<guid>https://arxiv.org/abs/2505.23852</guid>
<content:encoded><![CDATA[
arXiv:2505.23852v1 Announce Type: cross 
Abstract: Objective: To demonstrate the capabilities of Large Language Models (LLMs) as autonomous agents to reproduce findings of published research studies using the same or similar dataset.
  Materials and Methods: We used the "Quick Access" dataset of the National Alzheimer's Coordinating Center (NACC). We identified highly cited published research manuscripts using NACC data and selected five studies that appeared reproducible using this dataset alone. Using GPT-4o, we created a simulated research team of LLM-based autonomous agents tasked with writing and executing code to dynamically reproduce the findings of each study, given only study Abstracts, Methods sections, and data dictionary descriptions of the dataset.
  Results: We extracted 35 key findings described in the Abstracts across 5 Alzheimer's studies. On average, LLM agents approximately reproduced 53.2% of findings per study. Numeric values and range-based findings often differed between studies and agents. The agents also applied statistical methods or parameters that varied from the originals, though overall trends and significance were sometimes similar.
  Discussion: In some cases, LLM-based agents replicated research techniques and findings. In others, they failed due to implementation flaws or missing methodological detail. These discrepancies show the current limits of LLMs in fully automating reproducibility assessments. Still, this early investigation highlights the potential of structured agent-based systems to provide scalable evaluation of scientific rigor.
  Conclusion: This exploratory work illustrates both the promise and limitations of LLMs as autonomous agents for automating reproducibility in biomedical research.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Uncertainty Estimation and Calibration of Large Language Models</title>
<link>https://arxiv.org/abs/2505.23854</link>
<guid>https://arxiv.org/abs/2505.23854</guid>
<content:encoded><![CDATA[
arXiv:2505.23854v1 Announce Type: cross 
Abstract: As large language models (LLMs) are increasingly deployed in high-stakes applications, robust uncertainty estimation is essential for ensuring the safe and trustworthy deployment of LLMs. We present the most comprehensive study to date of uncertainty estimation in LLMs, evaluating 80 models spanning open- and closed-source families, dense and Mixture-of-Experts (MoE) architectures, reasoning and non-reasoning modes, quantization variants and parameter scales from 0.6B to 671B. Focusing on three representative black-box single-pass methods, including token probability-based uncertainty (TPU), numerical verbal uncertainty (NVU), and linguistic verbal uncertainty (LVU), we systematically evaluate uncertainty calibration and selective classification using the challenging MMLU-Pro benchmark, which covers both reasoning-intensive and knowledge-based tasks. Our results show that LVU consistently outperforms TPU and NVU, offering stronger calibration and discrimination while being more interpretable. We also find that high accuracy does not imply reliable uncertainty, and that model scale, post-training, reasoning ability and quantization all influence estimation performance. Notably, LLMs exhibit better uncertainty estimates on reasoning tasks than on knowledge-heavy ones, and good calibration does not necessarily translate to effective error ranking. These findings highlight the need for multi-perspective evaluation and position LVU as a practical tool for improving the reliability of LLMs in real-world settings.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities</title>
<link>https://arxiv.org/abs/2505.23856</link>
<guid>https://arxiv.org/abs/2505.23856</guid>
<content:encoded><![CDATA[
arXiv:2505.23856v1 Announce Type: cross 
Abstract: The emerging capabilities of large language models (LLMs) have sparked concerns about their immediate potential for harmful misuse. The core approach to mitigate these concerns is the detection of harmful queries to the model. Current detection approaches are fallible, and are particularly susceptible to attacks that exploit mismatched generalization of model capabilities (e.g., prompts in low-resource languages or prompts provided in non-text modalities such as image and audio). To tackle this challenge, we propose OMNIGUARD, an approach for detecting harmful prompts across languages and modalities. Our approach (i) identifies internal representations of an LLM/MLLM that are aligned across languages or modalities and then (ii) uses them to build a language-agnostic or modality-agnostic classifier for detecting harmful prompts. OMNIGUARD improves harmful prompt classification accuracy by 11.57\% over the strongest baseline in a multilingual setting, by 20.44\% for image-based prompts, and sets a new SOTA for audio-based prompts. By repurposing embeddings computed during generation, OMNIGUARD is also very efficient ($\approx 120 \times$ faster than the next fastest baseline). Code and data are available at: https://github.com/vsahil/OmniGuard.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DATD3: Depthwise Attention Twin Delayed Deep Deterministic Policy Gradient For Model Free Reinforcement Learning Under Output Feedback Control</title>
<link>https://arxiv.org/abs/2505.23857</link>
<guid>https://arxiv.org/abs/2505.23857</guid>
<content:encoded><![CDATA[
arXiv:2505.23857v1 Announce Type: cross 
Abstract: Reinforcement learning in real-world applications often involves output-feedback settings, where the agent receives only partial state information. To address this challenge, we propose the Output-Feedback Markov Decision Process (OPMDP), which extends the standard MDP formulation to accommodate decision-making based on observation histories. Building on this framework, we introduce Depthwise Attention Twin Delayed Deep Deterministic Policy Gradient (DATD3), a novel actor-critic algorithm that employs depthwise separable convolution and multi-head attention to encode historical observations. DATD3 maintains policy expressiveness while avoiding the instability of recurrent models. Extensive experiments on continuous control tasks demonstrate that DATD3 outperforms existing memory-based and recurrent baselines under both partial and full observability.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Minimizing Feature Drift in Model Merging: Layer-wise Task Vector Fusion for Adaptive Knowledge Integration</title>
<link>https://arxiv.org/abs/2505.23859</link>
<guid>https://arxiv.org/abs/2505.23859</guid>
<content:encoded><![CDATA[
arXiv:2505.23859v1 Announce Type: cross 
Abstract: Multi-task model merging aims to consolidate knowledge from multiple fine-tuned task-specific experts into a unified model while minimizing performance degradation. Existing methods primarily approach this by minimizing differences between task-specific experts and the unified model, either from a parameter-level or a task-loss perspective. However, parameter-level methods exhibit a significant performance gap compared to the upper bound, while task-loss approaches entail costly secondary training procedures. In contrast, we observe that performance degradation closely correlates with feature drift, i.e., differences in feature representations of the same sample caused by model merging. Motivated by this observation, we propose Layer-wise Optimal Task Vector Merging (LOT Merging), a technique that explicitly minimizes feature drift between task-specific experts and the unified model in a layer-by-layer manner. LOT Merging can be formulated as a convex quadratic optimization problem, enabling us to analytically derive closed-form solutions for the parameters of linear and normalization layers. Consequently, LOT Merging achieves efficient model consolidation through basic matrix operations. Extensive experiments across vision and vision-language benchmarks demonstrate that LOT Merging significantly outperforms baseline methods, achieving improvements of up to 4.4% (ViT-B/32) over state-of-the-art approaches.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum computing and artificial intelligence: status and perspectives</title>
<link>https://arxiv.org/abs/2505.23860</link>
<guid>https://arxiv.org/abs/2505.23860</guid>
<content:encoded><![CDATA[
arXiv:2505.23860v1 Announce Type: cross 
Abstract: This white paper discusses and explores the various points of intersection between quantum computing and artificial intelligence (AI). It describes how quantum computing could support the development of innovative AI solutions. It also examines use cases of classical AI that can empower research and development in quantum technologies, with a focus on quantum computing and quantum sensing. The purpose of this white paper is to provide a long-term research agenda aimed at addressing foundational questions about how AI and quantum computing interact and benefit one another. It concludes with a set of recommendations and challenges, including how to orchestrate the proposed theoretical work, align quantum AI developments with quantum hardware roadmaps, estimate both classical and quantum resources - especially with the goal of mitigating and optimizing energy consumption - advance this emerging hybrid software engineering discipline, and enhance European industrial competitiveness while considering societal implications.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BiBLDR: Bidirectional Behavior Learning for Drug Repositioning</title>
<link>https://arxiv.org/abs/2505.23861</link>
<guid>https://arxiv.org/abs/2505.23861</guid>
<content:encoded><![CDATA[
arXiv:2505.23861v1 Announce Type: cross 
Abstract: Drug repositioning aims to identify potential new indications for existing drugs to reduce the time and financial costs associated with developing new drugs. Most existing deep learning-based drug repositioning methods predominantly utilize graph-based representations. However, graph-based drug repositioning methods struggle to perform effective inference in cold-start scenarios involving novel drugs because of the lack of association information with the diseases. Unlike traditional graph-based approaches, we propose a bidirectional behavior learning strategy for drug repositioning, known as BiBLDR. This innovative framework redefines drug repositioning as a behavior sequential learning task to capture drug-disease interaction patterns. First, we construct bidirectional behavioral sequences based on drug and disease sides. The consideration of bidirectional information ensures a more meticulous and rigorous characterization of the behavioral sequences. Subsequently, we propose a two-stage strategy for drug repositioning. In the first stage, we construct prototype spaces to characterize the representational attributes of drugs and diseases. In the second stage, these refined prototypes and bidirectional behavior sequence data are leveraged to predict potential drug-disease associations. Based on this learning approach, the model can more robustly and precisely capture the interactive relationships between drug and disease features from bidirectional behavioral sequences. Extensive experiments demonstrate that our method achieves state-of-the-art performance on benchmark datasets. Meanwhile, BiBLDR demonstrates significantly superior performance compared to previous methods in cold-start scenarios. Our code is published in https://github.com/Renyeeah/BiBLDR.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A New Deep-learning-Based Approach For mRNA Optimization: High Fidelity, Computation Efficiency, and Multiple Optimization Factors</title>
<link>https://arxiv.org/abs/2505.23862</link>
<guid>https://arxiv.org/abs/2505.23862</guid>
<content:encoded><![CDATA[
arXiv:2505.23862v1 Announce Type: cross 
Abstract: The mRNA optimization is critical for therapeutic and biotechnological applications, since sequence features directly govern protein expression levels and efficacy. However, current methods face significant challenges in simultaneously achieving three key objectives: (1) fidelity (preventing unintended amino acid changes), (2) computational efficiency (speed and scalability), and (3) the scope of optimization variables considered (multi-objective capability). Furthermore, existing methods often fall short of comprehensively incorporating the factors related to the mRNA lifecycle and translation process, including intrinsic mRNA sequence properties, secondary structure, translation elongation kinetics, and tRNA availability. To address these limitations, we introduce \textbf{RNop}, a novel deep learning-based method for mRNA optimization. We collect a large-scale dataset containing over 3 million sequences and design four specialized loss functions, the GPLoss, CAILoss, tAILoss, and MFELoss, which simultaneously enable explicit control over sequence fidelity while optimizing species-specific codon adaptation, tRNA availability, and desirable mRNA secondary structure features. Then, we demonstrate RNop's effectiveness through extensive in silico and in vivo experiments. RNop ensures high sequence fidelity, achieves significant computational throughput up to 47.32 sequences/s, and yields optimized mRNA sequences resulting in a significant increase in protein expression for functional proteins compared to controls. RNop surpasses current methodologies in both quantitative metrics and experimental validation, enlightening a new dawn for efficient and effective mRNA design. Code and models will be available at https://github.com/HudenJear/RPLoss.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mamba Integrated with Physics Principles Masters Long-term Chaotic System Forecasting</title>
<link>https://arxiv.org/abs/2505.23863</link>
<guid>https://arxiv.org/abs/2505.23863</guid>
<content:encoded><![CDATA[
arXiv:2505.23863v1 Announce Type: cross 
Abstract: Long-term forecasting of chaotic systems from short-term observations remains a fundamental and underexplored challenge due to the intrinsic sensitivity to initial conditions and the complex geometry of strange attractors. Existing approaches often rely on long-term training data or focus on short-term sequence correlations, struggling to maintain predictive stability and dynamical coherence over extended horizons. We propose PhyxMamba, a novel framework that integrates a Mamba-based state-space model with physics-informed principles to capture the underlying dynamics of chaotic systems. By reconstructing the attractor manifold from brief observations using time-delay embeddings, PhyxMamba extracts global dynamical features essential for accurate forecasting. Our generative training scheme enables Mamba to replicate the physical process, augmented by multi-token prediction and attractor geometry regularization for physical constraints, enhancing prediction accuracy and preserving key statistical invariants. Extensive evaluations on diverse simulated and real-world chaotic systems demonstrate that PhyxMamba delivers superior long-term forecasting and faithfully captures essential dynamical invariants from short-term data. This framework opens new avenues for reliably predicting chaotic systems under observation-scarce conditions, with broad implications across climate science, neuroscience, epidemiology, and beyond. Our code is open-source at https://github.com/tsinghua-fib-lab/PhyxMamba.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized Subgraph Federated Learning with Differentiable Auxiliary Projections</title>
<link>https://arxiv.org/abs/2505.23864</link>
<guid>https://arxiv.org/abs/2505.23864</guid>
<content:encoded><![CDATA[
arXiv:2505.23864v1 Announce Type: cross 
Abstract: Federated learning (FL) on graph-structured data typically faces non-IID challenges, particularly in scenarios where each client holds a distinct subgraph sampled from a global graph. In this paper, we introduce Federated learning with Auxiliary projections (FedAux), a personalized subgraph FL framework that learns to align, compare, and aggregate heterogeneously distributed local models without sharing raw data or node embeddings. In FedAux, each client jointly trains (i) a local GNN and (ii) a learnable auxiliary projection vector (APV) that differentiably projects node embeddings onto a 1D space. A soft-sorting operation followed by a lightweight 1D convolution refines these embeddings in the ordered space, enabling the APV to effectively capture client-specific information. After local training, these APVs serve as compact signatures that the server uses to compute inter-client similarities and perform similarity-weighted parameter mixing, yielding personalized models while preserving cross-client knowledge transfer. Moreover, we provide rigorous theoretical analysis to establish the convergence and rationality of our design. Empirical evaluations across diverse graph benchmarks demonstrate that FedAux substantially outperforms existing baselines in both accuracy and personalization performance.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combining Deep Architectures for Information Gain estimation and Reinforcement Learning for multiagent field exploration</title>
<link>https://arxiv.org/abs/2505.23865</link>
<guid>https://arxiv.org/abs/2505.23865</guid>
<content:encoded><![CDATA[
arXiv:2505.23865v1 Announce Type: cross 
Abstract: Precision agriculture requires efficient autonomous systems for crop monitoring, where agents must explore large-scale environments while minimizing resource consumption. This work addresses the problem as an active exploration task in a grid environment representing an agricultural field. Each cell may contain targets (e.g., damaged crops) observable from nine predefined points of view (POVs). Agents must infer the number of targets per cell using partial, sequential observations.
  We propose a two-stage deep learning framework. A pre-trained LSTM serves as a belief model, updating a probabilistic map of the environment and its associated entropy, which defines the expected information gain (IG). This allows agents to prioritize informative regions. A key contribution is the inclusion of a POV visibility mask in the input, preserving the Markov property under partial observability and avoiding revisits to already explored views.
  Three agent architectures were compared: an untrained IG-based agent selecting actions to maximize entropy reduction; a DQN agent using CNNs over local 3x3 inputs with belief, entropy, and POV mask; and a Double-CNN DQN agent with wider spatial context. Simulations on 20x20 maps showed that the untrained agent performs well despite its simplicity. The DQN agent matches this performance when the POV mask is included, while the Double-CNN agent consistently achieves superior exploration efficiency, especially in larger environments.
  Results show that uncertainty-aware policies leveraging entropy, belief states, and visibility tracking lead to robust and scalable exploration. Future work includes curriculum learning, multi-agent cooperation with shared rewards, transformer-based models, and intrinsic motivation mechanisms to further enhance learning efficiency and policy generalization.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Understanding The Calibration Benefits of Sharpness-Aware Minimization</title>
<link>https://arxiv.org/abs/2505.23866</link>
<guid>https://arxiv.org/abs/2505.23866</guid>
<content:encoded><![CDATA[
arXiv:2505.23866v1 Announce Type: cross 
Abstract: Deep neural networks have been increasingly used in safety-critical applications such as medical diagnosis and autonomous driving. However, many studies suggest that they are prone to being poorly calibrated and have a propensity for overconfidence, which may have disastrous consequences. In this paper, unlike standard training such as stochastic gradient descent, we show that the recently proposed sharpness-aware minimization (SAM) counteracts this tendency towards overconfidence. The theoretical analysis suggests that SAM allows us to learn models that are already well-calibrated by implicitly maximizing the entropy of the predictive distribution. Inspired by this finding, we further propose a variant of SAM, coined as CSAM, to ameliorate model calibration. Extensive experiments on various datasets, including ImageNet-1K, demonstrate the benefits of SAM in reducing calibration error. Meanwhile, CSAM performs even better than SAM and consistently achieves lower calibration error than other approaches
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Infi-Med: Low-Resource Medical MLLMs with Robust Reasoning Evaluation</title>
<link>https://arxiv.org/abs/2505.23867</link>
<guid>https://arxiv.org/abs/2505.23867</guid>
<content:encoded><![CDATA[
arXiv:2505.23867v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) have demonstrated promising prospects in healthcare, particularly for addressing complex medical tasks, supporting multidisciplinary treatment (MDT), and enabling personalized precision medicine. However, their practical deployment faces critical challenges in resource efficiency, diagnostic accuracy, clinical considerations, and ethical privacy. To address these limitations, we propose Infi-Med, a comprehensive framework for medical MLLMs that introduces three key innovations: (1) a resource-efficient approach through curating and constructing high-quality supervised fine-tuning (SFT) datasets with minimal sample requirements, with a forward-looking design that extends to both pretraining and posttraining phases; (2) enhanced multimodal reasoning capabilities for cross-modal integration and clinical task understanding; and (3) a systematic evaluation system that assesses model performance across medical modalities and task types. Our experiments demonstrate that Infi-Med achieves state-of-the-art (SOTA) performance in general medical reasoning while maintaining rapid adaptability to clinical scenarios. The framework establishes a solid foundation for deploying MLLMs in real-world healthcare settings by balancing model effectiveness with operational constraints.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noise-Robustness Through Noise: Asymmetric LoRA Adaption with Poisoning Expert</title>
<link>https://arxiv.org/abs/2505.23868</link>
<guid>https://arxiv.org/abs/2505.23868</guid>
<content:encoded><![CDATA[
arXiv:2505.23868v1 Announce Type: cross 
Abstract: Current parameter-efficient fine-tuning methods for adapting pre-trained language models to downstream tasks are susceptible to interference from noisy data. Conventional noise-handling approaches either rely on laborious data pre-processing or employ model architecture modifications prone to error accumulation. In contrast to existing noise-process paradigms, we propose a noise-robust adaptation method via asymmetric LoRA poisoning experts (LoPE), a novel framework that enhances model robustness to noise only with generated noisy data. Drawing inspiration from the mixture-of-experts architecture, LoPE strategically integrates a dedicated poisoning expert in an asymmetric LoRA configuration. Through a two-stage paradigm, LoPE performs noise injection on the poisoning expert during fine-tuning to enhance its noise discrimination and processing ability. During inference, we selectively mask the dedicated poisoning expert to leverage purified knowledge acquired by normal experts for noise-robust output. Extensive experiments demonstrate that LoPE achieves strong performance and robustness purely through the low-cost noise injection, which completely eliminates the requirement of data cleaning.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaCP: Minimal yet Mighty Adaptation via Hierarchical Cosine Projection</title>
<link>https://arxiv.org/abs/2505.23870</link>
<guid>https://arxiv.org/abs/2505.23870</guid>
<content:encoded><![CDATA[
arXiv:2505.23870v1 Announce Type: cross 
Abstract: We present a new adaptation method MaCP, Minimal yet Mighty adaptive Cosine Projection, that achieves exceptional performance while requiring minimal parameters and memory for fine-tuning large foundation models. Its general idea is to exploit the superior energy compaction and decorrelation properties of cosine projection to improve both model efficiency and accuracy. Specifically, it projects the weight change from the low-rank adaptation into the discrete cosine space. Then, the weight change is partitioned over different levels of the discrete cosine spectrum, and each partition's most critical frequency components are selected. Extensive experiments demonstrate the effectiveness of MaCP across a wide range of single-modality tasks, including natural language understanding, natural language generation, text summarization, as well as multi-modality tasks such as image classification and video understanding. MaCP consistently delivers superior accuracy, significantly reduced computational complexity, and lower memory requirements compared to existing alternatives.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADG: Ambient Diffusion-Guided Dataset Recovery for Corruption-Robust Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.23871</link>
<guid>https://arxiv.org/abs/2505.23871</guid>
<content:encoded><![CDATA[
arXiv:2505.23871v1 Announce Type: cross 
Abstract: Real-world datasets collected from sensors or human inputs are prone to noise and errors, posing significant challenges for applying offline reinforcement learning (RL). While existing methods have made progress in addressing corrupted actions and rewards, they remain insufficient for handling corruption in high-dimensional state spaces and for cases where multiple elements in the dataset are corrupted simultaneously. Diffusion models, known for their strong denoising capabilities, offer a promising direction for this problem-but their tendency to overfit noisy samples limits their direct applicability. To overcome this, we propose Ambient Diffusion-Guided Dataset Recovery (ADG), a novel approach that pioneers the use of diffusion models to tackle data corruption in offline RL. First, we introduce Ambient Denoising Diffusion Probabilistic Models (DDPM) from approximated distributions, which enable learning on partially corrupted datasets with theoretical guarantees. Second, we use the noise-prediction property of Ambient DDPM to distinguish between clean and corrupted data, and then use the clean subset to train a standard DDPM. Third, we employ the trained standard DDPM to refine the previously identified corrupted data, enhancing data quality for subsequent offline RL training. A notable strength of ADG is its versatility-it can be seamlessly integrated with any offline RL algorithm. Experiments on a range of benchmarks, including MuJoCo, Kitchen, and Adroit, demonstrate that ADG effectively mitigates the impact of corrupted data and improves the robustness of offline RL under various noise settings, achieving state-of-the-art results.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KGMark: A Diffusion Watermark for Knowledge Graphs</title>
<link>https://arxiv.org/abs/2505.23873</link>
<guid>https://arxiv.org/abs/2505.23873</guid>
<content:encoded><![CDATA[
arXiv:2505.23873v1 Announce Type: cross 
Abstract: Knowledge graphs (KGs) are ubiquitous in numerous real-world applications, and watermarking facilitates protecting intellectual property and preventing potential harm from AI-generated content. Existing watermarking methods mainly focus on static plain text or image data, while they can hardly be applied to dynamic graphs due to spatial and temporal variations of structured data. This motivates us to propose KGMARK, the first graph watermarking framework that aims to generate robust, detectable, and transparent diffusion fingerprints for dynamic KG data. Specifically, we propose a novel clustering-based alignment method to adapt the watermark to spatial variations. Meanwhile, we present a redundant embedding strategy to harden the diffusion watermark against various attacks, facilitating the robustness of the watermark to the temporal variations. Additionally, we introduce a novel learnable mask matrix to improve the transparency of diffusion fingerprints. By doing so, our KGMARK properly tackles the variation challenges of structured data. Experiments on various public benchmarks show the effectiveness of our proposed KGMARK.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Benchmark Dataset for Graph Regression with Homogeneous and Multi-Relational Variants</title>
<link>https://arxiv.org/abs/2505.23875</link>
<guid>https://arxiv.org/abs/2505.23875</guid>
<content:encoded><![CDATA[
arXiv:2505.23875v1 Announce Type: cross 
Abstract: Graph-level regression underpins many real-world applications, yet public benchmarks remain heavily skewed toward molecular graphs and citation networks. This limited diversity hinders progress on models that must generalize across both homogeneous and heterogeneous graph structures. We introduce RelSC, a new graph-regression dataset built from program graphs that combine syntactic and semantic information extracted from source code. Each graph is labelled with the execution-time cost of the corresponding program, providing a continuous target variable that differs markedly from those found in existing benchmarks. RelSC is released in two complementary variants. RelSC-H supplies rich node features under a single (homogeneous) edge type, while RelSC-M preserves the original multi-relational structure, connecting nodes through multiple edge types that encode distinct semantic relationships. Together, these variants let researchers probe how representation choice influences model behaviour. We evaluate a diverse set of graph neural network architectures on both variants of RelSC. The results reveal consistent performance differences between the homogeneous and multi-relational settings, emphasising the importance of structural representation. These findings demonstrate RelSC's value as a challenging and versatile benchmark for advancing graph regression methods.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A comparative analysis of a neural network with calculated weights and a neural network with random generation of weights based on the training dataset size</title>
<link>https://arxiv.org/abs/2505.23876</link>
<guid>https://arxiv.org/abs/2505.23876</guid>
<content:encoded><![CDATA[
arXiv:2505.23876v1 Announce Type: cross 
Abstract: The paper discusses the capabilities of multilayer perceptron neural networks implementing metric recognition methods, for which the values of the weights are calculated analytically by formulas. Comparative experiments in training a neural network with pre-calculated weights and with random initialization of weights on different sizes of the MNIST training dataset are carried out. The results of the experiments show that a multilayer perceptron with pre-calculated weights can be trained much faster and is much more robust to the reduction of the training dataset.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Actor-Critic based Online Data Mixing For Language Model Pre-Training</title>
<link>https://arxiv.org/abs/2505.23878</link>
<guid>https://arxiv.org/abs/2505.23878</guid>
<content:encoded><![CDATA[
arXiv:2505.23878v1 Announce Type: cross 
Abstract: The coverage and composition of pretraining data significantly impacts the generalization ability of Large Language Models (LLMs). To reduce the carbon footprint and financial costs of training, some data mixing methods, which applied the optimized domain weights of a small proxy model to train a larger one, were proposed. However, these methods did not evolute with the training dynamics. The existing online data mixing (ODM) method addressed this limitation by applying the multi-armed bandit algorithm as data sampling strategy. Yet, it did not consider the intra-domain interactions. In this paper, we develop an actor-critic based online data mixing (AC-ODM) method, which captures the varying domain weights by auxiliary actor-critic networks and consider the intra-domain interactions with the reward function. While constructing the dataset to pretrain a large target LLM, we directly apply the actor, which is trained with a small proxy LLM as the environment, as the sampling strategy. The transfer of sampling strategy can not only ensure the efficiency of dynamical data mixing, but also expedite the convergence of pretraining the target LLM. Numerical results demonstrate that AC-ODM-410M, which invokes the sampling strategy obtained by a proxy LLM with 410M parameters, reaching the optimal validation perplexity of ODM 71% faster, and improves performance on the zero-shot MMLU benchmark by 27.5% of accuracy, about 2.23x better on pass@1 of HumanEval benchmark.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CNN-LSTM Hybrid Model for AI-Driven Prediction of COVID-19 Severity from Spike Sequences and Clinical Data</title>
<link>https://arxiv.org/abs/2505.23879</link>
<guid>https://arxiv.org/abs/2505.23879</guid>
<content:encoded><![CDATA[
arXiv:2505.23879v1 Announce Type: cross 
Abstract: The COVID-19 pandemic, caused by SARS-CoV-2, highlighted the critical need for accurate prediction of disease severity to optimize healthcare resource allocation and patient management. The spike protein, which facilitates viral entry into host cells, exhibits high mutation rates, particularly in the receptor-binding domain, influencing viral pathogenicity. Artificial intelligence approaches, such as deep learning, offer promising solutions for leveraging genomic and clinical data to predict disease outcomes. Objective: This study aimed to develop a hybrid CNN-LSTM deep learning model to predict COVID-19 severity using spike protein sequences and associated clinical metadata from South American patients. Methods: We retrieved 9,570 spike protein sequences from the GISAID database, of which 3,467 met inclusion criteria after standardization. The dataset included 2,313 severe and 1,154 mild cases. A feature engineering pipeline extracted features from sequences, while demographic and clinical variables were one-hot encoded. A hybrid CNN-LSTM architecture was trained, combining CNN layers for local pattern extraction and an LSTM layer for long-term dependency modeling. Results: The model achieved an F1 score of 82.92%, ROC-AUC of 0.9084, precision of 83.56%, and recall of 82.85%, demonstrating robust classification performance. Training stabilized at 85% accuracy with minimal overfitting. The most prevalent lineages (P.1, AY.99.2) and clades (GR, GK) aligned with regional epidemiological trends, suggesting potential associations between viral genetics and clinical outcomes. Conclusion: The CNN-LSTM hybrid model effectively predicted COVID-19 severity using spike protein sequences and clinical data, highlighting the utility of AI in genomic surveillance and precision public health. Despite limitations, this approach provides a framework for early severity prediction in future outbreaks.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning for Better Verbalized Confidence in Long-Form Generation</title>
<link>https://arxiv.org/abs/2505.23912</link>
<guid>https://arxiv.org/abs/2505.23912</guid>
<content:encoded><![CDATA[
arXiv:2505.23912v1 Announce Type: cross 
Abstract: Hallucination remains a major challenge for the safe and trustworthy deployment of large language models (LLMs) in factual content generation. Prior work has explored confidence estimation as an effective approach to hallucination detection, but often relies on post-hoc self-consistency methods that require computationally expensive sampling. Verbalized confidence offers a more efficient alternative, but existing approaches are largely limited to short-form question answering (QA) tasks and do not generalize well to open-ended generation. In this paper, we propose LoVeC (Long-form Verbalized Confidence), an on-the-fly verbalized confidence estimation method for long-form generation. Specifically, we use reinforcement learning (RL) to train LLMs to append numerical confidence scores to each generated statement, serving as a direct and interpretable signal of the factuality of generation. Our experiments consider both on-policy and off-policy RL methods, including DPO, ORPO, and GRPO, to enhance the model calibration. We introduce two novel evaluation settings, free-form tagging and iterative tagging, to assess different verbalized confidence estimation methods. Experiments on three long-form QA datasets show that our RL-trained models achieve better calibration and generalize robustly across domains. Also, our method is highly efficient, as it only requires adding a few tokens to the output being decoded.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing Association Biases in LLM Moderation Over-Sensitivity</title>
<link>https://arxiv.org/abs/2505.23914</link>
<guid>https://arxiv.org/abs/2505.23914</guid>
<content:encoded><![CDATA[
arXiv:2505.23914v1 Announce Type: cross 
Abstract: Large Language Models are widely used for content moderation but often misclassify benign comments as toxic, leading to over-sensitivity. While previous research attributes this issue primarily to the presence of offensive terms, we reveal a potential cause beyond token level: LLMs exhibit systematic topic biases in their implicit associations. Inspired by cognitive psychology's implicit association tests, we introduce Topic Association Analysis, a semantic-level approach to quantify how LLMs associate certain topics with toxicity. By prompting LLMs to generate free-form scenario imagination for misclassified benign comments and analyzing their topic amplification levels, we find that more advanced models (e.g., GPT-4 Turbo) demonstrate stronger topic stereotype despite lower overall false positive rates. These biases suggest that LLMs do not merely react to explicit, offensive language but rely on learned topic associations, shaping their moderation decisions. Our findings highlight the need for refinement beyond keyword-based filtering, providing insights into the underlying mechanisms driving LLM over-sensitivity.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representational Difference Explanations</title>
<link>https://arxiv.org/abs/2505.23917</link>
<guid>https://arxiv.org/abs/2505.23917</guid>
<content:encoded><![CDATA[
arXiv:2505.23917v1 Announce Type: cross 
Abstract: We propose a method for discovering and visualizing the differences between two learned representations, enabling more direct and interpretable model comparisons. We validate our method, which we call Representational Differences Explanations (RDX), by using it to compare models with known conceptual differences and demonstrate that it recovers meaningful distinctions where existing explainable AI (XAI) techniques fail. Applied to state-of-the-art models on challenging subsets of the ImageNet and iNaturalist datasets, RDX reveals both insightful representational differences and subtle patterns in the data. Although comparison is a cornerstone of scientific analysis, current tools in machine learning, namely post hoc XAI methods, struggle to support model comparison effectively. Our work addresses this gap by introducing an effective and explainable tool for contrasting model representations.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChARM: Character-based Act-adaptive Reward Modeling for Advanced Role-Playing Language Agents</title>
<link>https://arxiv.org/abs/2505.23923</link>
<guid>https://arxiv.org/abs/2505.23923</guid>
<content:encoded><![CDATA[
arXiv:2505.23923v1 Announce Type: cross 
Abstract: Role-Playing Language Agents (RPLAs) aim to simulate characters for realistic and engaging human-computer interactions. However, traditional reward models often struggle with scalability and adapting to subjective conversational preferences. We propose ChARM, a Character-based Act-adaptive Reward Model, addressing these challenges through two innovations: (1) an act-adaptive margin that significantly enhances learning efficiency and generalizability, and (2) a self-evolution mechanism leveraging large-scale unlabeled data to improve training coverage. Additionally, we introduce RoleplayPref, the first large-scale preference dataset specifically for RPLAs, featuring 1,108 characters, 13 subcategories, and 16,888 bilingual dialogues, alongside RoleplayEval, a dedicated evaluation benchmark. Experimental results show a 13% improvement over the conventional Bradley-Terry model in preference rankings. Furthermore, applying ChARM-generated rewards to preference learning techniques (e.g., direct preference optimization) achieves state-of-the-art results on CharacterEval and RoleplayEval. Code and dataset are available at https://github.com/calubkk/ChARM.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Societal Concerns and Perceptions of AI: A Thematic Analysis through the Lens of Problem-Seeking</title>
<link>https://arxiv.org/abs/2505.23930</link>
<guid>https://arxiv.org/abs/2505.23930</guid>
<content:encoded><![CDATA[
arXiv:2505.23930v1 Announce Type: cross 
Abstract: This study introduces a novel conceptual framework distinguishing problem-seeking from problem-solving to clarify the unique features of human intelligence in contrast to AI. Problem-seeking refers to the embodied, emotionally grounded process by which humans identify and set goals, while problem-solving denotes the execution of strategies aimed at achieving such predefined objectives. The framework emphasizes that while AI excels at efficiency and optimization, it lacks the orientation derived from experiential grounding and the embodiment flexibility intrinsic to human cognition. To empirically explore this distinction, the research analyzes metadata from 157 YouTube videos discussing AI. Conducting a thematic analysis combining qualitative insights with keyword-based quantitative metrics, this mixed-methods approach uncovers recurring themes in public discourse, including privacy, job displacement, misinformation, optimism, and ethical concerns. The results reveal a dual sentiment: public fascination with AI's capabilities coexists with anxiety and skepticism about its societal implications. The discussion critiques the orthogonality thesis, which posits that intelligence is separable from goal content, and instead argues that human intelligence integrates goal-setting and goal-pursuit. It underscores the centrality of embodied cognition in human reasoning and highlights how AI's limitations come from its current reliance on computational processing. The study advocates for enhancing emotional and digital literacy to foster responsible AI engagement. It calls for reframing public discourse to recognize AI as a tool that augments -- rather than replaces -- human intelligence. By positioning problem seeking at the core of cognition and as a critical dimension of intelligence, this research offers new perspectives on ethically aligned and human-centered AI development.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling up the think-aloud method</title>
<link>https://arxiv.org/abs/2505.23931</link>
<guid>https://arxiv.org/abs/2505.23931</guid>
<content:encoded><![CDATA[
arXiv:2505.23931v1 Announce Type: cross 
Abstract: The think-aloud method, where participants voice their thoughts as they solve a task, is a valuable source of rich data about human reasoning processes. Yet, it has declined in popularity in contemporary cognitive science, largely because labor-intensive transcription and annotation preclude large sample sizes. Here, we develop methods to automate the transcription and annotation of verbal reports of reasoning using natural language processing tools, allowing for large-scale analysis of think-aloud data. In our study, 640 participants thought aloud while playing the Game of 24, a mathematical reasoning task. We automatically transcribed the recordings and coded the transcripts as search graphs, finding moderate inter-rater reliability with humans. We analyze these graphs and characterize consistency and variation in human reasoning traces. Our work demonstrates the value of think-aloud data at scale and serves as a proof of concept for the automated analysis of verbal reports.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BIRD: Behavior Induction via Representation-structure Distillation</title>
<link>https://arxiv.org/abs/2505.23933</link>
<guid>https://arxiv.org/abs/2505.23933</guid>
<content:encoded><![CDATA[
arXiv:2505.23933v1 Announce Type: cross 
Abstract: Human-aligned deep learning models exhibit behaviors consistent with human values, such as robustness, fairness, and honesty. Transferring these behavioral properties to models trained on different tasks or data distributions remains challenging: aligned behavior is easily forgotten during fine-tuning, and collecting task-specific data that preserves this behavior can be prohibitively costly. We introduce BIRD (Behavior Induction via Representation-structure Distillation), a flexible framework for transferring aligned behavior by matching the internal representation structure of a student model to that of a teacher. Applied to out-of-distribution robustness in image classification, BIRD outperforms fine-tuning, transfer learning, and continual learning methods, improving robust accuracy by up to 16% over the next strongest baseline. It remains effective even when the teacher is trained on a much simpler dataset and is $25 \times$ smaller than the student. In a large-scale study of over 400 teacher-student pairs, we show that three interpretable and computable properties of the teacher's representations (i.e., task relevance, behavioral relevance, and complementary knowledge) explain up to 85% of the variance in transfer success. These insights offer practical guidance for teacher selection and design. BIRD turns small, well-aligned models into scalable alignment seeds, removing a key bottleneck in deploying safe AI systems in the wild.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SG-Blend: Learning an Interpolation Between Improved Swish and GELU for Robust Neural Representations</title>
<link>https://arxiv.org/abs/2505.23942</link>
<guid>https://arxiv.org/abs/2505.23942</guid>
<content:encoded><![CDATA[
arXiv:2505.23942v1 Announce Type: cross 
Abstract: The design of activation functions remains a pivotal component in optimizing deep neural networks. While prevailing choices like Swish and GELU demonstrate considerable efficacy, they often exhibit domain-specific optima. This work introduces SG-Blend, a novel activation function that blends our proposed SSwish, a first-order symmetric variant of Swish and the established GELU through dynamic interpolation. By adaptively blending these constituent functions via learnable parameters, SG-Blend aims to harness their complementary strengths: SSwish's controlled non-monotonicity and symmetry, and GELU's smooth, probabilistic profile, to achieve a more universally robust balance between model expressivity and gradient stability. We conduct comprehensive empirical evaluations across diverse modalities and architectures, showing performance improvements across all considered natural language and computer vision tasks and models. These results, achieved with negligible computational overhead, underscore SG-Blend's potential as a versatile, drop-in replacement that consistently outperforms strong contemporary baselines. The code is available at https://anonymous.4open.science/r/SGBlend-6CBC.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Closer Look at Bias and Chain-of-Thought Faithfulness of Large (Vision) Language Models</title>
<link>https://arxiv.org/abs/2505.23945</link>
<guid>https://arxiv.org/abs/2505.23945</guid>
<content:encoded><![CDATA[
arXiv:2505.23945v1 Announce Type: cross 
Abstract: Chain-of-thought (CoT) reasoning enhances performance of large language models, but questions remain about whether these reasoning traces faithfully reflect the internal processes of the model. We present the first comprehensive study of CoT faithfulness in large vision-language models (LVLMs), investigating how both text-based and previously unexplored image-based biases affect reasoning and bias articulation. Our work introduces a novel, fine-grained evaluation pipeline for categorizing bias articulation patterns, enabling significantly more precise analysis of CoT reasoning than previous methods. This framework reveals critical distinctions in how models process and respond to different types of biases, providing new insights into LVLM CoT faithfulness. Our findings reveal that subtle image-based biases are rarely articulated compared to explicit text-based ones, even in models specialized for reasoning. Additionally, many models exhibit a previously unidentified phenomenon we term ``inconsistent'' reasoning - correctly reasoning before abruptly changing answers, serving as a potential canary for detecting biased reasoning from unfaithful CoTs. We then apply the same evaluation pipeline to revisit CoT faithfulness in LLMs across various levels of implicit cues. Our findings reveal that current language-only reasoning models continue to struggle with articulating cues that are not overtly stated.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: The Future of Bayesian Prediction Is Prior-Fitted</title>
<link>https://arxiv.org/abs/2505.23947</link>
<guid>https://arxiv.org/abs/2505.23947</guid>
<content:encoded><![CDATA[
arXiv:2505.23947v1 Announce Type: cross 
Abstract: Training neural networks on randomly generated artificial datasets yields Bayesian models that capture the prior defined by the dataset-generating distribution. Prior-data Fitted Networks (PFNs) are a class of methods designed to leverage this insight. In an era of rapidly increasing computational resources for pre-training and a near stagnation in the generation of new real-world data in many applications, PFNs are poised to play a more important role across a wide range of applications. They enable the efficient allocation of pre-training compute to low-data scenarios. Originally applied to small Bayesian modeling tasks, the field of PFNs has significantly expanded to address more complex domains and larger datasets. This position paper argues that PFNs and other amortized inference approaches represent the future of Bayesian inference, leveraging amortized learning to tackle data-scarce problems. We thus believe they are a fruitful area of research. In this position paper, we explore their potential and directions to address their current limitations.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TSENOR: Highly-Efficient Algorithm for Finding Transposable N:M Sparse Masks</title>
<link>https://arxiv.org/abs/2505.23949</link>
<guid>https://arxiv.org/abs/2505.23949</guid>
<content:encoded><![CDATA[
arXiv:2505.23949v1 Announce Type: cross 
Abstract: Network pruning reduces the computational requirements of large neural networks, with N:M sparsity -- retaining only N out of every M consecutive weights -- offering a compelling balance between compressed model quality and hardware acceleration. However, N:M sparsity only accelerates forward-pass computations, as N:M patterns are not preserved during matrix transposition, limiting efficiency during training where both passes are computationally intensive. While transposable N:M sparsity has been proposed to address this limitation, existing methods for finding transposable N:M sparse masks either fail to scale to large models or are restricted to M=4 which results in suboptimal compression-accuracy trade-off. We introduce an efficient solver for transposable N:M masks that scales to billion-parameter models. We formulate mask generation as optimal transport problems and solve through entropy regularization and Dykstra's algorithm, followed by a rounding procedure. Our tensor-based implementation exploits GPU parallelism, achieving up to 100x speedup with only 1-10% error compared to existing methods. Our approach can be integrated with layer-wise N:M pruning frameworks including Wanda, SparseGPT and ALPS to produce transposable N:M sparse models with arbitrary N:M values. Experiments show that LLaMA3.2-8B with transposable 16:32 sparsity maintains performance close to its standard N:M counterpart and outperforms standard 2:4 sparse model, showing the practical value of our approach.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing LLM-Based Code Generation with Complexity Metrics: A Feedback-Driven Approach</title>
<link>https://arxiv.org/abs/2505.23953</link>
<guid>https://arxiv.org/abs/2505.23953</guid>
<content:encoded><![CDATA[
arXiv:2505.23953v1 Announce Type: cross 
Abstract: Automatic code generation has gained significant momentum with the advent of Large Language Models (LLMs) such as GPT-4. Although many studies focus on improving the effectiveness of LLMs for code generation, very limited work tries to understand the generated code's characteristics and leverage that to improve failed cases. In this paper, as the most straightforward characteristic of code, we investigate the relationship between code complexity and the success of LLM generated code. Using a large set of standard complexity metrics, we first conduct an empirical analysis to explore their correlation with LLM's performance on code generation (i.e., Pass@1). Using logistic regression models, we identify which complexity metrics are most predictive of code correctness. Building on these findings, we propose an iterative feedback method, where LLMs are prompted to generate correct code based on complexity metrics from previous failed outputs. We validate our approach across multiple benchmarks (i.e., HumanEval, MBPP, LeetCode, and BigCodeBench) and various LLMs (i.e., GPT-4o, GPT-3.5 Turbo, Llama 3.1, and GPT-o3 mini), comparing the results with two baseline methods: (a) zero-shot generation, and (b) iterative execution-based feedback without our code complexity insights. Experiment results show that our approach makes notable improvements, particularly with a smaller LLM (GPT3.5 Turbo), where, e.g., Pass@1 increased by 35.71% compared to the baseline's improvement of 12.5% on the HumanEval dataset. The study expands experiments to BigCodeBench and integrates the method with the Reflexion code generation agent, leading to Pass@1 improvements of 20% (GPT-4o) and 23.07% (GPT-o3 mini). The results highlight that complexity-aware feedback enhances both direct LLM prompting and agent-based workflows.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information Structure in Mappings: An Approach to Learning, Representation, and Generalisation</title>
<link>https://arxiv.org/abs/2505.23960</link>
<guid>https://arxiv.org/abs/2505.23960</guid>
<content:encoded><![CDATA[
arXiv:2505.23960v1 Announce Type: cross 
Abstract: Despite the remarkable success of large large-scale neural networks, we still lack unified notation for thinking about and describing their representational spaces. We lack methods to reliably describe how their representations are structured, how that structure emerges over training, and what kinds of structures are desirable. This thesis introduces quantitative methods for identifying systematic structure in a mapping between spaces, and leverages them to understand how deep-learning models learn to represent information, what representational structures drive generalisation, and how design decisions condition the structures that emerge. To do this I identify structural primitives present in a mapping, along with information theoretic quantifications of each. These allow us to analyse learning, structure, and generalisation across multi-agent reinforcement learning models, sequence-to-sequence models trained on a single task, and Large Language Models. I also introduce a novel, performant, approach to estimating the entropy of vector space, that allows this analysis to be applied to models ranging in size from 1 million to 12 billion parameters.
  The experiments here work to shed light on how large-scale distributed models of cognition learn, while allowing us to draw parallels between those systems and their human analogs. They show how the structures of language and the constraints that give rise to them in many ways parallel the kinds of structures that drive performance of contemporary neural networks.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Confidential Guardian: Cryptographically Prohibiting the Abuse of Model Abstention</title>
<link>https://arxiv.org/abs/2505.23968</link>
<guid>https://arxiv.org/abs/2505.23968</guid>
<content:encoded><![CDATA[
arXiv:2505.23968v1 Announce Type: cross 
Abstract: Cautious predictions -- where a machine learning model abstains when uncertain -- are crucial for limiting harmful errors in safety-critical applications. In this work, we identify a novel threat: a dishonest institution can exploit these mechanisms to discriminate or unjustly deny services under the guise of uncertainty. We demonstrate the practicality of this threat by introducing an uncertainty-inducing attack called Mirage, which deliberately reduces confidence in targeted input regions, thereby covertly disadvantaging specific individuals. At the same time, Mirage maintains high predictive performance across all data points. To counter this threat, we propose Confidential Guardian, a framework that analyzes calibration metrics on a reference dataset to detect artificially suppressed confidence. Additionally, it employs zero-knowledge proofs of verified inference to ensure that reported confidence scores genuinely originate from the deployed model. This prevents the provider from fabricating arbitrary model confidence values while protecting the model's proprietary details. Our results confirm that Confidential Guardian effectively prevents the misuse of cautious predictions, providing verifiable assurances that abstention reflects genuine model uncertainty rather than malicious intent.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL</title>
<link>https://arxiv.org/abs/2505.23977</link>
<guid>https://arxiv.org/abs/2505.23977</guid>
<content:encoded><![CDATA[
arXiv:2505.23977v1 Announce Type: cross 
Abstract: Vision language models (VLMs) are expected to perform effective multimodal reasoning and make logically coherent decisions, which is critical to tasks such as diagram understanding and spatial problem solving. However, current VLM reasoning lacks large-scale and well-structured training datasets. To bridge this gap, we propose VisualSphinx, a first-of-its-kind large-scale synthetic visual logical reasoning training data. To tackle the challenge of image synthesis with grounding answers, we propose a rule-to-image synthesis pipeline, which extracts and expands puzzle rules from seed questions and generates the code of grounding synthesis image synthesis for puzzle sample assembly. Experiments demonstrate that VLM trained using GRPO on VisualSphinx benefit from logical coherence and readability of our dataset and exhibit improved performance on logical reasoning tasks. The enhanced reasoning capabilities developed from VisualSphinx also benefit other reasoning tasks such as algebraic reasoning, arithmetic reasoning and geometry reasoning.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Controllable Multi-property Multi-objective Molecule Optimization</title>
<link>https://arxiv.org/abs/2505.23987</link>
<guid>https://arxiv.org/abs/2505.23987</guid>
<content:encoded><![CDATA[
arXiv:2505.23987v1 Announce Type: cross 
Abstract: In real-world drug design, molecule optimization requires selectively improving multiple molecular properties up to pharmaceutically relevant levels, while maintaining others that already meet such criteria. However, existing computational approaches and instruction-tuned LLMs fail to capture such nuanced property-specific objectives, limiting their practical applicability. To address this, we introduce C-MuMOInstruct, the first instruction-tuning dataset focused on multi-property optimization with explicit, property-specific objectives. Leveraging C-MuMOInstruct, we develop GeLLMO-Cs, a series of instruction-tuned LLMs that can perform targeted property-specific optimization. Our experiments across 5 in-distribution and 5 out-of-distribution tasks show that GeLLMO-Cs consistently outperform strong baselines, achieving up to 126% higher success rate. Notably, GeLLMO-Cs exhibit impressive 0-shot generalization to novel optimization tasks and unseen instructions. This offers a step toward a foundational LLM to support realistic, diverse optimizations with property-specific objectives. C-MuMOInstruct and code are accessible through https://github.com/ninglab/GeLLMO-C.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for LLMs</title>
<link>https://arxiv.org/abs/2505.23996</link>
<guid>https://arxiv.org/abs/2505.23996</guid>
<content:encoded><![CDATA[
arXiv:2505.23996v1 Announce Type: cross 
Abstract: The recent rapid adoption of large language models (LLMs) highlights the critical need for benchmarking their fairness. Conventional fairness metrics, which focus on discrete accuracy-based evaluations (i.e., prediction correctness), fail to capture the implicit impact of model uncertainty (e.g., higher model confidence about one group over another despite similar accuracy). To address this limitation, we propose an uncertainty-aware fairness metric, UCerF, to enable a fine-grained evaluation of model fairness that is more reflective of the internal bias in model decisions compared to conventional fairness measures. Furthermore, observing data size, diversity, and clarity issues in current datasets, we introduce a new gender-occupation fairness evaluation dataset with 31,756 samples for co-reference resolution, offering a more diverse and suitable dataset for evaluating modern LLMs. We establish a benchmark, using our metric and dataset, and apply it to evaluate the behavior of ten open-source LLMs. For example, Mistral-7B exhibits suboptimal fairness due to high confidence in incorrect predictions, a detail overlooked by Equalized Odds but captured by UCerF. Overall, our proposed LLM benchmark, which evaluates fairness with uncertainty awareness, paves the way for developing more transparent and accountable AI systems.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-output Classification using a Cross-talk Architecture for Compound Fault Diagnosis of Motors in Partially Labeled Condition</title>
<link>https://arxiv.org/abs/2505.24001</link>
<guid>https://arxiv.org/abs/2505.24001</guid>
<content:encoded><![CDATA[
arXiv:2505.24001v1 Announce Type: cross 
Abstract: The increasing complexity of rotating machinery and the diversity of operating conditions, such as rotating speed and varying torques, have amplified the challenges in fault diagnosis in scenarios requiring domain adaptation, particularly involving compound faults. This study addresses these challenges by introducing a novel multi-output classification (MOC) framework tailored for domain adaptation in partially labeled (PL) target datasets. Unlike conventional multi-class classification (MCC) approaches, the proposed MOC framework classifies the severity levels of compound faults simultaneously. Furthermore, we explore various single-task and multi-task architectures applicable to the MOC formulation-including shared trunk and cross-talk-based designs-for compound fault diagnosis under PL conditions. Based on this investigation, we propose a novel cross-talk layer structure that enables selective information sharing across diagnostic tasks, effectively enhancing classification performance in compound fault scenarios. In addition, frequency-layer normalization was incorporated to improve domain adaptation performance on motor vibration data. Compound fault conditions were implemented using a motor-based test setup, and the proposed model was evaluated across six domain adaptation scenarios. The experimental results demonstrate its superior macro F1 performance compared to baseline models. We further showed that the proposed mode's structural advantage is more pronounced in compound fault settings through a single-fault comparison. We also found that frequency-layer normalization fits the fault diagnosis task better than conventional methods. Lastly, we discuss that this improvement primarily stems from the model's structural ability to leverage inter-fault classification task interactions, rather than from a simple increase in model parameters.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Modal View Enhanced Large Vision Models for Long-Term Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.24003</link>
<guid>https://arxiv.org/abs/2505.24003</guid>
<content:encoded><![CDATA[
arXiv:2505.24003v1 Announce Type: cross 
Abstract: Time series, typically represented as numerical sequences, can also be transformed into images and texts, offering multi-modal views (MMVs) of the same underlying signal. These MMVs can reveal complementary patterns and enable the use of powerful pre-trained large models, such as large vision models (LVMs), for long-term time series forecasting (LTSF). However, as we identified in this work, applying LVMs to LTSF poses an inductive bias towards "forecasting periods". To harness this bias, we propose DMMV, a novel decomposition-based multi-modal view framework that leverages trend-seasonal decomposition and a novel backcast residual based adaptive decomposition to integrate MMVs for LTSF. Comparative evaluations against 14 state-of-the-art (SOTA) models across diverse datasets show that DMMV outperforms single-view and existing multi-modal baselines, achieving the best mean squared error (MSE) on 6 out of 8 benchmark datasets.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws</title>
<link>https://arxiv.org/abs/2505.24009</link>
<guid>https://arxiv.org/abs/2505.24009</guid>
<content:encoded><![CDATA[
arXiv:2505.24009v1 Announce Type: cross 
Abstract: Transformers deliver outstanding performance across a wide range of tasks and are now a dominant backbone architecture for large language models (LLMs). Their task-solving performance is improved by increasing parameter size, as shown in the recent studies on parameter scaling laws. Although recent mechanistic-interpretability studies have deepened our understanding of the internal behavior of Transformers by analyzing their residual stream, the relationship between these internal mechanisms and the parameter scaling laws remains unclear. To bridge this gap, we focus on layers and their size, which mainly decide the parameter size of Transformers. For this purpose, we first theoretically investigate the layers within the residual stream through a bias-diversity decomposition. The decomposition separates (i) bias, the error of each layer's output from the ground truth, and (ii) diversity, which indicates how much the outputs of each layer differ from each other. Analyzing Transformers under this theory reveals that performance improves when individual layers make predictions close to the correct answer and remain mutually diverse. We show that diversity becomes especially critical when individual layers' outputs are far from the ground truth. Finally, we introduce an information-theoretic diversity and show our main findings that adding layers enhances performance only when those layers behave differently, i.e., are diverse. We also reveal the performance gains from increasing the number of layers exhibit submodularity: marginal improvements diminish as additional layers increase, mirroring the logarithmic convergence predicted by the parameter scaling laws. Experiments on multiple semantic-understanding tasks with various LLMs empirically confirm the theoretical properties derived in this study.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Meets Constraint Propagation</title>
<link>https://arxiv.org/abs/2505.24012</link>
<guid>https://arxiv.org/abs/2505.24012</guid>
<content:encoded><![CDATA[
arXiv:2505.24012v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) excel at generating fluent text but struggle to enforce external constraints because they generate tokens sequentially without explicit control mechanisms. GenCP addresses this limitation by combining LLM predictions with Constraint Programming (CP) reasoning, formulating text generation as a Constraint Satisfaction Problem (CSP). In this paper, we improve GenCP by integrating Masked Language Models (MLMs) for domain generation, which allows bidirectional constraint propagation that leverages both past and future tokens. This integration bridges the gap between token-level prediction and structured constraint enforcement, leading to more reliable and constraint-aware text generation. Our evaluation on COLLIE benchmarks demonstrates that incorporating domain preview via MLM calls significantly improves GenCP's performance. Although this approach incurs additional MLM calls and, in some cases, increased backtracking, the overall effect is a more efficient use of LLM inferences and an enhanced ability to generate feasible and meaningful solutions, particularly in tasks with strict content constraints.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Agents Should Employ Security Principles</title>
<link>https://arxiv.org/abs/2505.24019</link>
<guid>https://arxiv.org/abs/2505.24019</guid>
<content:encoded><![CDATA[
arXiv:2505.24019v1 Announce Type: cross 
Abstract: Large Language Model (LLM) agents show considerable promise for automating complex tasks using contextual reasoning; however, interactions involving multiple agents and the system's susceptibility to prompt injection and other forms of context manipulation introduce new vulnerabilities related to privacy leakage and system exploitation. This position paper argues that the well-established design principles in information security, which are commonly referred to as security principles, should be employed when deploying LLM agents at scale. Design principles such as defense-in-depth, least privilege, complete mediation, and psychological acceptability have helped guide the design of mechanisms for securing information systems over the last five decades, and we argue that their explicit and conscientious adoption will help secure agentic systems. To illustrate this approach, we introduce AgentSandbox, a conceptual framework embedding these security principles to provide safeguards throughout an agent's life-cycle. We evaluate with state-of-the-art LLMs along three dimensions: benign utility, attack utility, and attack success rate. AgentSandbox maintains high utility for its intended functions under both benign and adversarial evaluations while substantially mitigating privacy risks. By embedding secure design principles as foundational elements within emerging LLM agent protocols, we aim to promote trustworthy agent ecosystems aligned with user privacy expectations and evolving regulatory requirements.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Group Proportional Representation for Text-to-Image Models</title>
<link>https://arxiv.org/abs/2505.24023</link>
<guid>https://arxiv.org/abs/2505.24023</guid>
<content:encoded><![CDATA[
arXiv:2505.24023v1 Announce Type: cross 
Abstract: Text-to-image (T2I) generative models can create vivid, realistic images from textual descriptions. As these models proliferate, they expose new concerns about their ability to represent diverse demographic groups, propagate stereotypes, and efface minority populations. Despite growing attention to the "safe" and "responsible" design of artificial intelligence (AI), there is no established methodology to systematically measure and control representational harms in image generation. This paper introduces a novel framework to measure the representation of intersectional groups in images generated by T2I models by applying the Multi-Group Proportional Representation (MPR) metric. MPR evaluates the worst-case deviation of representation statistics across given population groups in images produced by a generative model, allowing for flexible and context-specific measurements based on user requirements. We also develop an algorithm to optimize T2I models for this metric. Through experiments, we demonstrate that MPR can effectively measure representation statistics across multiple intersectional groups and, when used as a training objective, can guide models toward a more balanced generation across demographic groups while maintaining generation quality.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models</title>
<link>https://arxiv.org/abs/2505.24025</link>
<guid>https://arxiv.org/abs/2505.24025</guid>
<content:encoded><![CDATA[
arXiv:2505.24025v1 Announce Type: cross 
Abstract: The recent explosive interest in the reasoning capabilities of large language models, such as DeepSeek-R1, has demonstrated remarkable success through reinforcement learning-based fine-tuning frameworks, exemplified by methods like Group Relative Policy Optimization (GRPO). However, such reasoning abilities remain underexplored and notably absent in vision foundation models, including representation models like the DINO series. In this work, we propose \textbf{DINO-R1}, the first such attempt to incentivize visual in-context reasoning capabilities of vision foundation models using reinforcement learning. Specifically, DINO-R1 introduces \textbf{Group Relative Query Optimization (GRQO)}, a novel reinforcement-style training strategy explicitly designed for query-based representation models, which computes query-level rewards based on group-normalized alignment quality. We also apply KL-regularization to stabilize the objectness distribution to reduce the training instability. This joint optimization enables dense and expressive supervision across queries while mitigating overfitting and distributional drift. Building upon Grounding-DINO, we train a series of DINO-R1 family models that integrate a visual prompt encoder and a visual-guided query selection mechanism. Extensive experiments on COCO, LVIS, and ODinW demonstrate that DINO-R1 significantly outperforms supervised fine-tuning baselines, achieving strong generalization in both open-vocabulary and closed-set visual prompting scenarios.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaskAdapt: Unsupervised Geometry-Aware Domain Adaptation Using Multimodal Contextual Learning and RGB-Depth Masking</title>
<link>https://arxiv.org/abs/2505.24026</link>
<guid>https://arxiv.org/abs/2505.24026</guid>
<content:encoded><![CDATA[
arXiv:2505.24026v1 Announce Type: cross 
Abstract: Semantic segmentation of crops and weeds is crucial for site-specific farm management; however, most existing methods depend on labor intensive pixel-level annotations. A further challenge arises when models trained on one field (source domain) fail to generalize to new fields (target domain) due to domain shifts, such as variations in lighting, camera setups, soil composition, and crop growth stages. Unsupervised Domain Adaptation (UDA) addresses this by enabling adaptation without target-domain labels, but current UDA methods struggle with occlusions and visual blending between crops and weeds, leading to misclassifications in real-world conditions. To overcome these limitations, we introduce MaskAdapt, a novel approach that enhances segmentation accuracy through multimodal contextual learning by integrating RGB images with features derived from depth data. By computing depth gradients from depth maps, our method captures spatial transitions that help resolve texture ambiguities. These gradients, through a cross-attention mechanism, refines RGB feature representations, resulting in sharper boundary delineation. In addition, we propose a geometry-aware masking strategy that applies horizontal, vertical, and stochastic masks during training. This encourages the model to focus on the broader spatial context for robust visual recognition. Evaluations on real agricultural datasets demonstrate that MaskAdapt consistently outperforms existing State-of-the-Art (SOTA) UDA methods, achieving improved segmentation mean Intersection over Union (mIOU) across diverse field conditions.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Images to Signals: Are Large Vision Models Useful for Time Series Analysis?</title>
<link>https://arxiv.org/abs/2505.24030</link>
<guid>https://arxiv.org/abs/2505.24030</guid>
<content:encoded><![CDATA[
arXiv:2505.24030v1 Announce Type: cross 
Abstract: Transformer-based models have gained increasing attention in time series research, driving interest in Large Language Models (LLMs) and foundation models for time series analysis. As the field moves toward multi-modality, Large Vision Models (LVMs) are emerging as a promising direction. In the past, the effectiveness of Transformer and LLMs in time series has been debated. When it comes to LVMs, a similar question arises: are LVMs truely useful for time series analysis? To address it, we design and conduct the first principled study involving 4 LVMs, 8 imaging methods, 18 datasets and 26 baselines across both high-level (classification) and low-level (forecasting) tasks, with extensive ablation analysis. Our findings indicate LVMs are indeed useful for time series classification but face challenges in forecasting. Although effective, the contemporary best LVM forecasters are limited to specific types of LVMs and imaging methods, exhibit a bias toward forecasting periods, and have limited ability to utilize long look-back windows. We hope our findings could serve as a cornerstone for future research on LVM- and multimodal-based solutions to different time series tasks.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LlamaRL: A Distributed Asynchronous Reinforcement Learning Framework for Efficient Large-scale LLM Trainin</title>
<link>https://arxiv.org/abs/2505.24034</link>
<guid>https://arxiv.org/abs/2505.24034</guid>
<content:encoded><![CDATA[
arXiv:2505.24034v1 Announce Type: cross 
Abstract: Reinforcement Learning (RL) has become the most effective post-training approach for improving the capabilities of Large Language Models (LLMs). In practice, because of the high demands on latency and memory, it is particularly challenging to develop an efficient RL framework that reliably manages policy models with hundreds to thousands of billions of parameters.
  In this paper, we present LlamaRL, a fully distributed, asynchronous RL framework optimized for efficient training of large-scale LLMs with various model sizes (8B, 70B, and 405B parameters) on GPU clusters ranging from a handful to thousands of devices. LlamaRL introduces a streamlined, single-controller architecture built entirely on native PyTorch, enabling modularity, ease of use, and seamless scalability to thousands of GPUs. We also provide a theoretical analysis of LlamaRL's efficiency, including a formal proof that its asynchronous design leads to strict RL speed-up. Empirically, by leveraging best practices such as colocated model offloading, asynchronous off-policy training, and distributed direct memory access for weight synchronization, LlamaRL achieves significant efficiency gains -- up to 10.7x speed-up compared to DeepSpeed-Chat-like systems on a 405B-parameter policy model. Furthermore, the efficiency advantage continues to grow with increasing model scale, demonstrating the framework's suitability for future large-scale RL training.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedPAIR: Measuring Physicians and AI Relevance Alignment in Medical Question Answering</title>
<link>https://arxiv.org/abs/2505.24040</link>
<guid>https://arxiv.org/abs/2505.24040</guid>
<content:encoded><![CDATA[
arXiv:2505.24040v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable performance on various medical question-answering (QA) benchmarks, including standardized medical exams. However, correct answers alone do not ensure correct logic, and models may reach accurate conclusions through flawed processes. In this study, we introduce the MedPAIR (Medical Dataset Comparing Physicians and AI Relevance Estimation and Question Answering) dataset to evaluate how physician trainees and LLMs prioritize relevant information when answering QA questions. We obtain annotations on 1,300 QA pairs from 36 physician trainees, labeling each sentence within the question components for relevance. We compare these relevance estimates to those for LLMs, and further evaluate the impact of these "relevant" subsets on downstream task performance for both physician trainees and LLMs. We find that LLMs are frequently not aligned with the content relevance estimates of physician trainees. After filtering out physician trainee-labeled irrelevant sentences, accuracy improves for both the trainees and the LLMs. All LLM and physician trainee-labeled data are available at: http://medpair.csail.mit.edu/.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Source and Target Domains via Link Prediction for Unsupervised Domain Adaptation on Graphs</title>
<link>https://arxiv.org/abs/2505.24055</link>
<guid>https://arxiv.org/abs/2505.24055</guid>
<content:encoded><![CDATA[
arXiv:2505.24055v1 Announce Type: cross 
Abstract: Graph neural networks (GNNs) have shown great ability for node classification on graphs. However, the success of GNNs relies on abundant labeled data, while obtaining high-quality labels is costly and challenging, especially for newly emerging domains. Hence, unsupervised domain adaptation (UDA), which trains a classifier on the labeled source graph and adapts it to the unlabeled target graph, is attracting increasing attention. Various approaches have been proposed to alleviate the distribution shift between the source and target graphs to facilitate the classifier adaptation. However, most of them simply adopt existing UDA techniques developed for independent and identically distributed data to gain domain-invariant node embeddings for graphs, which do not fully consider the graph structure and message-passing mechanism of GNNs during the adaptation and will fail when label distribution shift exists among domains. In this paper, we proposed a novel framework that adopts link prediction to connect nodes between source and target graphs, which can facilitate message-passing between the source and target graphs and augment the target nodes to have ``in-distribution'' neighborhoods with the source domain. This strategy modified the target graph on the input level to reduce its deviation from the source domain in the embedding space and is insensitive to disproportional label distributions across domains. To prevent the loss of discriminative information in the target graph, we further design a novel identity-preserving learning objective, which guides the learning of the edge insertion module together with reconstruction and adaptation losses. Experimental results on real-world datasets demonstrate the effectiveness of our framework.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DSR-Bench: Evaluating the Structural Reasoning Abilities of LLMs via Data Structures</title>
<link>https://arxiv.org/abs/2505.24069</link>
<guid>https://arxiv.org/abs/2505.24069</guid>
<content:encoded><![CDATA[
arXiv:2505.24069v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly deployed for real-world tasks that fundamentally involve data manipulation. A core requirement across these tasks is the ability to perform structural reasoning--that is, to understand and reason about data relationships. For example, customer requests require a temporal ordering, which can be represented by data structures such as queues. However, existing benchmarks primarily focus on high-level, application-driven evaluations without isolating this fundamental capability. To address this gap, we introduce DSR-Bench, a novel benchmark evaluating LLMs' structural reasoning capabilities through data structures, which provide interpretable representations of data relationships. DSR-Bench includes 20 data structures, 35 operations, and 4,140 problem instances, organized hierarchically for fine-grained analysis of reasoning limitations. Our evaluation pipeline is fully automated and deterministic, eliminating subjective human or model-based judgments. Its synthetic nature also ensures scalability and minimizes data contamination risks. We benchmark nine state-of-the-art LLMs. Our analysis shows that instruction-tuned models struggle with basic multi-attribute and multi-hop reasoning. Furthermore, while reasoning-oriented models perform better, they remain fragile on complex and hybrid structures, with the best model achieving an average score of only 47% on the challenge subset. Crucially, models often perform poorly on multi-dimensional data and natural language task descriptions, highlighting a critical gap for real-world deployment.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Searching Clinical Data Using Generative AI</title>
<link>https://arxiv.org/abs/2505.24090</link>
<guid>https://arxiv.org/abs/2505.24090</guid>
<content:encoded><![CDATA[
arXiv:2505.24090v1 Announce Type: cross 
Abstract: Artificial Intelligence (AI) is making a major impact on healthcare, particularly through its application in natural language processing (NLP) and predictive analytics. The healthcare sector has increasingly adopted AI for tasks such as clinical data analysis and medical code assignment. However, searching for clinical information in large and often unorganized datasets remains a manual and error-prone process. Assisting this process with automations can help physicians improve their operational productivity significantly.
  In this paper, we present a generative AI approach, coined SearchAI, to enhance the accuracy and efficiency of searching clinical data. Unlike traditional code assignment, which is a one-to-one problem, clinical data search is a one-to-many problem, i.e., a given search query can map to a family of codes. Healthcare professionals typically search for groups of related diseases, drugs, or conditions that map to many codes, and therefore, they need search tools that can handle keyword synonyms, semantic variants, and broad open-ended queries. SearchAI employs a hierarchical model that respects the coding hierarchy and improves the traversal of relationships from parent to child nodes. SearchAI navigates these hierarchies predictively and ensures that all paths are reachable without losing any relevant nodes.
  To evaluate the effectiveness of SearchAI, we conducted a series of experiments using both public and production datasets. Our results show that SearchAI outperforms default hierarchical traversals across several metrics, including accuracy, robustness, performance, and scalability. SearchAI can help make clinical data more accessible, leading to streamlined workflows, reduced administrative burden, and enhanced coding and diagnostic accuracy.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attractor learning for spatiotemporally chaotic dynamical systems using echo state networks with transfer learning</title>
<link>https://arxiv.org/abs/2505.24099</link>
<guid>https://arxiv.org/abs/2505.24099</guid>
<content:encoded><![CDATA[
arXiv:2505.24099v1 Announce Type: cross 
Abstract: In this paper, we explore the predictive capabilities of echo state networks (ESNs) for the generalized Kuramoto-Sivashinsky (gKS) equation, an archetypal nonlinear PDE that exhibits spatiotemporal chaos. We introduce a novel methodology that integrates ESNs with transfer learning, aiming to enhance predictive performance across various parameter regimes of the gKS model. Our research focuses on predicting changes in long-term statistical patterns of the gKS model that result from varying the dispersion relation or the length of the spatial domain. We use transfer learning to adapt ESNs to different parameter settings and successfully capture changes in the underlying chaotic attractor.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning Capabilities of VLMs</title>
<link>https://arxiv.org/abs/2505.24120</link>
<guid>https://arxiv.org/abs/2505.24120</guid>
<content:encoded><![CDATA[
arXiv:2505.24120v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) have demonstrated remarkable progress in multimodal understanding, yet their capabilities for scientific reasoning remains inadequately assessed. Current multimodal benchmarks predominantly evaluate generic image comprehension or text-driven reasoning, lacking authentic scientific contexts that require domain-specific knowledge integration with visual evidence analysis. To fill this gap, we present CSVQA, a diagnostic multimodal benchmark specifically designed for evaluating scientific reasoning through domain-grounded visual question answering.Our benchmark features 1,378 carefully constructed question-answer pairs spanning diverse STEM disciplines, each demanding domain knowledge, integration of visual evidence, and higher-order reasoning. Compared to prior multimodal benchmarks, CSVQA places greater emphasis on real-world scientific content and complex reasoning.We additionally propose a rigorous evaluation protocol to systematically assess whether model predictions are substantiated by valid intermediate reasoning steps based on curated explanations. Our comprehensive evaluation of 15 VLMs on this benchmark reveals notable performance disparities, as even the top-ranked proprietary model attains only 49.6\% accuracy.This empirical evidence underscores the pressing need for advancing scientific reasoning capabilities in VLMs. Our CSVQA is released at https://huggingface.co/datasets/Skywork/CSVQA.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration</title>
<link>https://arxiv.org/abs/2505.24133</link>
<guid>https://arxiv.org/abs/2505.24133</guid>
<content:encoded><![CDATA[
arXiv:2505.24133v1 Announce Type: cross 
Abstract: Reasoning models have demonstrated impressive performance in self-reflection and chain-of-thought reasoning. However, they often produce excessively long outputs, leading to prohibitively large key-value (KV) caches during inference. While chain-of-thought inference significantly improves performance on complex reasoning tasks, it can also lead to reasoning failures when deployed with existing KV cache compression approaches. To address this, we propose Redundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel method specifically targeting redundant tokens in reasoning models. Our method preserves nearly 100% of the full KV cache performance using only 10% of the KV cache, substantially outperforming existing KV cache baselines, which reach only 60% of the performance. Remarkably, R-KV even achieves 105% of full KV cache performance with 16% of the KV cache. This KV-cache reduction also leads to a 90% memory saving and a 6.6X throughput over standard chain-of-thought reasoning inference. Experimental results show that R-KV consistently outperforms existing KV cache compression baselines across two mathematical reasoning datasets.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparsity-Driven Parallel Imaging Consistency for Improved Self-Supervised MRI Reconstruction</title>
<link>https://arxiv.org/abs/2505.24136</link>
<guid>https://arxiv.org/abs/2505.24136</guid>
<content:encoded><![CDATA[
arXiv:2505.24136v1 Announce Type: cross 
Abstract: Physics-driven deep learning (PD-DL) models have proven to be a powerful approach for improved reconstruction of rapid MRI scans. In order to train these models in scenarios where fully-sampled reference data is unavailable, self-supervised learning has gained prominence. However, its application at high acceleration rates frequently introduces artifacts, compromising image fidelity. To mitigate this shortcoming, we propose a novel way to train PD-DL networks via carefully-designed perturbations. In particular, we enhance the k-space masking idea of conventional self-supervised learning with a novel consistency term that assesses the model's ability to accurately predict the added perturbations in a sparse domain, leading to more reliable and artifact-free reconstructions. The results obtained from the fastMRI knee and brain datasets show that the proposed training strategy effectively reduces aliasing artifacts and mitigates noise amplification at high acceleration rates, outperforming state-of-the-art self-supervised methods both visually and quantitatively.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMSbench: A Comprehensive Benchmark for Evaluating MLLM Capabilities in AMS Circuits</title>
<link>https://arxiv.org/abs/2505.24138</link>
<guid>https://arxiv.org/abs/2505.24138</guid>
<content:encoded><![CDATA[
arXiv:2505.24138v1 Announce Type: cross 
Abstract: Analog/Mixed-Signal (AMS) circuits play a critical role in the integrated circuit (IC) industry. However, automating Analog/Mixed-Signal (AMS) circuit design has remained a longstanding challenge due to its difficulty and complexity. Recent advances in Multi-modal Large Language Models (MLLMs) offer promising potential for supporting AMS circuit analysis and design. However, current research typically evaluates MLLMs on isolated tasks within the domain, lacking a comprehensive benchmark that systematically assesses model capabilities across diverse AMS-related challenges. To address this gap, we introduce AMSbench, a benchmark suite designed to evaluate MLLM performance across critical tasks including circuit schematic perception, circuit analysis, and circuit design. AMSbench comprises approximately 8000 test questions spanning multiple difficulty levels and assesses eight prominent models, encompassing both open-source and proprietary solutions such as Qwen 2.5-VL and Gemini 2.5 Pro. Our evaluation highlights significant limitations in current MLLMs, particularly in complex multi-modal reasoning and sophisticated circuit design tasks. These results underscore the necessity of advancing MLLMs' understanding and effective application of circuit-specific knowledge, thereby narrowing the existing performance gap relative to human expertise and moving toward fully automated AMS circuit design workflows. Our data is released at https://huggingface.co/datasets/wwhhyy/AMSBench
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S4-Driver: Scalable Self-Supervised Driving Multimodal Large Language Modelwith Spatio-Temporal Visual Representation</title>
<link>https://arxiv.org/abs/2505.24139</link>
<guid>https://arxiv.org/abs/2505.24139</guid>
<content:encoded><![CDATA[
arXiv:2505.24139v1 Announce Type: cross 
Abstract: The latest advancements in multi-modal large language models (MLLMs) have spurred a strong renewed interest in end-to-end motion planning approaches for autonomous driving. Many end-to-end approaches rely on human annotations to learn intermediate perception and prediction tasks, while purely self-supervised approaches--which directly learn from sensor inputs to generate planning trajectories without human annotations often underperform the state of the art. We observe a key gap in the input representation space: end-to-end approaches built on MLLMs are often pretrained with reasoning tasks in 2D image space rather than the native 3D space in which autonomous vehicles plan. To this end, we propose S4-Driver, a scalable self-supervised motion planning algorithm with spatio-temporal visual representation, based on the popular PaLI multimodal large language model. S4-Driver uses a novel sparse volume strategy to seamlessly transform the strong visual representation of MLLMs from perspective view to 3D space without the need to finetune the vision encoder. This representation aggregates multi-view and multi-frame visual inputs and enables better prediction of planning trajectories in 3D space. To validate our method, we run experiments on both nuScenes and Waymo Open Motion Dataset (with in-house camera data). Results show that S4-Driver performs favorably against existing supervised multi-task approaches while requiring no human annotations. It also demonstrates great scalability when pretrained on large volumes of unannotated driving logs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Butterfly Effect in Pathology: Exploring Security in Pathology Foundation Models</title>
<link>https://arxiv.org/abs/2505.24141</link>
<guid>https://arxiv.org/abs/2505.24141</guid>
<content:encoded><![CDATA[
arXiv:2505.24141v1 Announce Type: cross 
Abstract: With the widespread adoption of pathology foundation models in both research and clinical decision support systems, exploring their security has become a critical concern. However, despite their growing impact, the vulnerability of these models to adversarial attacks remains largely unexplored. In this work, we present the first systematic investigation into the security of pathology foundation models for whole slide image~(WSI) analysis against adversarial attacks. Specifically, we introduce the principle of \textit{local perturbation with global impact} and propose a label-free attack framework that operates without requiring access to downstream task labels. Under this attack framework, we revise four classical white-box attack methods and redefine the perturbation budget based on the characteristics of WSI. We conduct comprehensive experiments on three representative pathology foundation models across five datasets and six downstream tasks. Despite modifying only 0.1\% of patches per slide with imperceptible noise, our attack leads to downstream accuracy degradation that can reach up to 20\% in the worst cases. Furthermore, we analyze key factors that influence attack success, explore the relationship between patch-level vulnerability and semantic content, and conduct a preliminary investigation into potential defence strategies. These findings lay the groundwork for future research on the adversarial robustness and reliable deployment of pathology foundation models. Our code is publicly available at: https://github.com/Jiashuai-Liu-hmos/Attack-WSI-pathology-foundation-models.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RCCDA: Adaptive Model Updates in the Presence of Concept Drift under a Constrained Resource Budget</title>
<link>https://arxiv.org/abs/2505.24149</link>
<guid>https://arxiv.org/abs/2505.24149</guid>
<content:encoded><![CDATA[
arXiv:2505.24149v1 Announce Type: cross 
Abstract: Machine learning (ML) algorithms deployed in real-world environments are often faced with the challenge of adapting models to concept drift, where the task data distributions are shifting over time. The problem becomes even more difficult when model performance must be maintained under adherence to strict resource constraints. Existing solutions often depend on drift-detection methods that produce high computational overhead for resource-constrained environments, and fail to provide strict guarantees on resource usage or theoretical performance assurances. To address these shortcomings, we propose RCCDA: a dynamic model update policy that optimizes ML training dynamics while ensuring strict compliance to predefined resource constraints, utilizing only past loss information and a tunable drift threshold. In developing our policy, we analytically characterize the evolution of model loss under concept drift with arbitrary training update decisions. Integrating these results into a Lyapunov drift-plus-penalty framework produces a lightweight policy based on a measurable accumulated loss threshold that provably limits update frequency and cost. Experimental results on three domain generalization datasets demonstrate that our policy outperforms baseline methods in inference accuracy while adhering to strict resource constraints under several schedules of concept drift, making our solution uniquely suited for real-time ML deployments.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Just Follow MLLM Plans: Robust and Efficient Planning for Open-world Agents</title>
<link>https://arxiv.org/abs/2505.24157</link>
<guid>https://arxiv.org/abs/2505.24157</guid>
<content:encoded><![CDATA[
arXiv:2505.24157v1 Announce Type: cross 
Abstract: Developing autonomous agents capable of mastering complex, multi-step tasks in unpredictable, interactive environments presents a significant challenge. While Large Language Models (LLMs) offer promise for planning, existing approaches often rely on problematic internal knowledge or make unrealistic environmental assumptions. Although recent work explores learning planning knowledge, they still retain limitations due to partial reliance on external knowledge or impractical setups. Indeed, prior research has largely overlooked developing agents capable of acquiring planning knowledge from scratch, directly in realistic settings. While realizing this capability is necessary, it presents significant challenges, primarily achieving robustness given the substantial risk of incorporating LLMs' inaccurate knowledge. Moreover, efficiency is crucial for practicality as learning can demand prohibitive exploration. In response, we introduce Robust and Efficient Planning for Open-world Agents (REPOA), a novel framework designed to tackle these issues. REPOA features three key components: adaptive dependency learning and fine-grained failure-aware operation memory to enhance robustness to knowledge inaccuracies, and difficulty-based exploration to improve learning efficiency. Our evaluation in two established open-world testbeds demonstrates REPOA's robust and efficient planning, showcasing its capability to successfully obtain challenging late-game items that were beyond the reach of prior approaches.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LKD-KGC: Domain-Specific KG Construction via LLM-driven Knowledge Dependency Parsing</title>
<link>https://arxiv.org/abs/2505.24163</link>
<guid>https://arxiv.org/abs/2505.24163</guid>
<content:encoded><![CDATA[
arXiv:2505.24163v1 Announce Type: cross 
Abstract: Knowledge Graphs (KGs) structure real-world entities and their relationships into triples, enhancing machine reasoning for various tasks. While domain-specific KGs offer substantial benefits, their manual construction is often inefficient and requires specialized knowledge. Recent approaches for knowledge graph construction (KGC) based on large language models (LLMs), such as schema-guided KGC and reference knowledge integration, have proven efficient. However, these methods are constrained by their reliance on manually defined schema, single-document processing, and public-domain references, making them less effective for domain-specific corpora that exhibit complex knowledge dependencies and specificity, as well as limited reference knowledge. To address these challenges, we propose LKD-KGC, a novel framework for unsupervised domain-specific KG construction. LKD-KGC autonomously analyzes document repositories to infer knowledge dependencies, determines optimal processing sequences via LLM driven prioritization, and autoregressively generates entity schema by integrating hierarchical inter-document contexts. This schema guides the unsupervised extraction of entities and relationships, eliminating reliance on predefined structures or external knowledge. Extensive experiments show that compared with state-of-the-art baselines, LKD-KGC generally achieves improvements of 10% to 20% in both precision and recall rate, demonstrating its potential in constructing high-quality domain-specific KGs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Invariant Link Selector for Spatial-Temporal Out-of-Distribution Problem</title>
<link>https://arxiv.org/abs/2505.24178</link>
<guid>https://arxiv.org/abs/2505.24178</guid>
<content:encoded><![CDATA[
arXiv:2505.24178v1 Announce Type: cross 
Abstract: In the era of foundation models, Out-of- Distribution (OOD) problems, i.e., the data discrepancy between the training environments and testing environments, hinder AI generalization. Further, relational data like graphs disobeying the Independent and Identically Distributed (IID) condition makes the problem more challenging, especially much harder when it is associated with time. Motivated by this, to realize the robust invariant learning over temporal graphs, we want to investigate what components in temporal graphs are most invariant and representative with respect to labels. With the Information Bottleneck (IB) method, we propose an error-bounded Invariant Link Selector that can distinguish invariant components and variant components during the training process to make the deep learning model generalizable for different testing scenarios. Besides deriving a series of rigorous generalizable optimization functions, we also equip the training with task-specific loss functions, e.g., temporal link prediction, to make pretrained models solve real-world application tasks like citation recommendation and merchandise recommendation, as demonstrated in our experiments with state-of-the-art (SOTA) methods. Our code is available at https://github.com/kthrn22/OOD-Linker.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling</title>
<link>https://arxiv.org/abs/2505.24179</link>
<guid>https://arxiv.org/abs/2505.24179</guid>
<content:encoded><![CDATA[
arXiv:2505.24179v1 Announce Type: cross 
Abstract: Many advanced Large Language Model (LLM) applications require long-context processing, but the self-attention module becomes a bottleneck during the prefilling stage of inference due to its quadratic time complexity with respect to sequence length. Existing sparse attention methods accelerate attention computation by skipping less significant regions of the attention map. However, these approaches typically perform coarse-grained inspection of the attention map, rendering considerable loss in model accuracy. In this paper, we propose SALE, a fine-grained sparse attention method that accelerates the long-context prefilling stage of LLM with negligible loss in model accuracy. SALE achieves fast and accurate fine-grained attention weight estimation through 4-bit quantized query-key products, followed by block-sparse attention to accelerate prefilling computations. For importance evaluation for query-key pairs, we adopt our Relative Attention Score metric, which offers significantly higher efficiency within our framework. We implement a custom CUDA kernel optimized for our approach for hardware efficiency, reducing the additional overhead to approximately 11% of the full attention latency. Notably, SALE requires no parameter training and can be seamlessly integrated into existing systems with trivial code modifications. Experiments on long-context benchmarks demonstrate that our method outperforms existing approaches in accuracy-efficiency trade-offs, achieving at least 3.36x speedups on Llama-3.1-8B for sequences longer than 64K while maintaining model quality.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing is Not Reasoning: MVPBench for Graph-based Evaluation of Multi-path Visual Physical CoT</title>
<link>https://arxiv.org/abs/2505.24182</link>
<guid>https://arxiv.org/abs/2505.24182</guid>
<content:encoded><![CDATA[
arXiv:2505.24182v1 Announce Type: cross 
Abstract: Understanding the physical world - governed by laws of motion, spatial relations, and causality - poses a fundamental challenge for multimodal large language models (MLLMs). While recent advances such as OpenAI o3 and GPT-4o demonstrate impressive perceptual and reasoning capabilities, our investigation reveals these models struggle profoundly with visual physical reasoning, failing to grasp basic physical laws, spatial interactions, and causal effects in complex scenes. More importantly, they often fail to follow coherent reasoning chains grounded in visual evidence, especially when multiple steps are needed to arrive at the correct answer. To rigorously evaluate this capability, we introduce MVPBench, a curated benchmark designed to rigorously evaluate visual physical reasoning through the lens of visual chain-of-thought (CoT). Each example features interleaved multi-image inputs and demands not only the correct final answer but also a coherent, step-by-step reasoning path grounded in evolving visual cues. This setup mirrors how humans reason through real-world physical processes over time. To ensure fine-grained evaluation, we introduce a graph-based CoT consistency metric that verifies whether the reasoning path of model adheres to valid physical logic. Additionally, we minimize shortcut exploitation from text priors, encouraging models to rely on visual understanding. Experimental results reveal a concerning trend: even cutting-edge MLLMs exhibit poor visual reasoning accuracy and weak image-text alignment in physical domains. Surprisingly, RL-based post-training alignment - commonly believed to improve visual reasoning performance - often harms spatial reasoning, suggesting a need to rethink current fine-tuning practices.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Unified Modeling in Federated Multi-Task Learning via Subspace Decoupling</title>
<link>https://arxiv.org/abs/2505.24185</link>
<guid>https://arxiv.org/abs/2505.24185</guid>
<content:encoded><![CDATA[
arXiv:2505.24185v1 Announce Type: cross 
Abstract: Federated Multi-Task Learning (FMTL) enables multiple clients performing heterogeneous tasks without exchanging their local data, offering broad potential for privacy preserving multi-task collaboration. However, most existing methods focus on building personalized models for each client and unable to support the aggregation of multiple heterogeneous tasks into a unified model. As a result, in real-world scenarios where task objectives, label spaces, and optimization paths vary significantly, conventional FMTL methods struggle to achieve effective joint training. To address this challenge, we propose FedDEA (Federated Decoupled Aggregation), an update-structure-aware aggregation method specifically designed for multi-task model integration. Our method dynamically identifies task-relevant dimensions based on the response strength of local updates and enhances their optimization effectiveness through rescaling. This mechanism effectively suppresses cross-task interference and enables task-level decoupled aggregation within a unified global model. FedDEA does not rely on task labels or architectural modifications, making it broadly applicable and deployment-friendly. Experimental results demonstrate that it can be easily integrated into various mainstream federated optimization algorithms and consistently delivers significant overall performance improvements on widely used NYUD-V2 and PASCAL-Context. These results validate the robustness and generalization capabilities of FedDEA under highly heterogeneous task settings.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tune an SLM or Prompt an LLM? The Case of Generating Low-Code Workflows</title>
<link>https://arxiv.org/abs/2505.24189</link>
<guid>https://arxiv.org/abs/2505.24189</guid>
<content:encoded><![CDATA[
arXiv:2505.24189v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) such as GPT-4o can handle a wide range of complex tasks with the right prompt. As per token costs are reduced, the advantages of fine-tuning Small Language Models (SLMs) for real-world applications -- faster inference, lower costs -- may no longer be clear. In this work, we present evidence that, for domain-specific tasks that require structured outputs, SLMs still have a quality advantage. We compare fine-tuning an SLM against prompting LLMs on the task of generating low-code workflows in JSON form. We observe that while a good prompt can yield reasonable results, fine-tuning improves quality by 10% on average. We also perform systematic error analysis to reveal model limitations.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Foundation Models for Zero-Shot Biometric Tasks</title>
<link>https://arxiv.org/abs/2505.24214</link>
<guid>https://arxiv.org/abs/2505.24214</guid>
<content:encoded><![CDATA[
arXiv:2505.24214v1 Announce Type: cross 
Abstract: The advent of foundation models, particularly Vision-Language Models (VLMs) and Multi-modal Large Language Models (MLLMs), has redefined the frontiers of artificial intelligence, enabling remarkable generalization across diverse tasks with minimal or no supervision. Yet, their potential in biometric recognition and analysis remains relatively underexplored. In this work, we introduce a comprehensive benchmark that evaluates the zero-shot and few-shot performance of state-of-the-art publicly available VLMs and MLLMs across six biometric tasks spanning the face and iris modalities: face verification, soft biometric attribute prediction (gender and race), iris recognition, presentation attack detection (PAD), and face manipulation detection (morphs and deepfakes). A total of 41 VLMs were used in this evaluation. Experiments show that embeddings from these foundation models can be used for diverse biometric tasks with varying degrees of success. For example, in the case of face verification, a True Match Rate (TMR) of 96.77 percent was obtained at a False Match Rate (FMR) of 1 percent on the Labeled Face in the Wild (LFW) dataset, without any fine-tuning. In the case of iris recognition, the TMR at 1 percent FMR on the IITD-R-Full dataset was 97.55 percent without any fine-tuning. Further, we show that applying a simple classifier head to these embeddings can help perform DeepFake detection for faces, Presentation Attack Detection (PAD) for irides, and extract soft biometric attributes like gender and ethnicity from faces with reasonably high accuracy. This work reiterates the potential of pretrained models in achieving the long-term vision of Artificial General Intelligence.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning Can Hurt the Inductive Abilities of Large Language Models</title>
<link>https://arxiv.org/abs/2505.24225</link>
<guid>https://arxiv.org/abs/2505.24225</guid>
<content:encoded><![CDATA[
arXiv:2505.24225v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown remarkable progress across domains, yet their ability to perform inductive reasoning - inferring latent rules from sparse examples - remains limited. It is often assumed that chain-of-thought (CoT) prompting, as used in Large Reasoning Models (LRMs), enhances such reasoning. We investigate this assumption with creating four controlled, diagnostic game-based tasks - chess, Texas Hold'em, dice games, and blackjack - with hidden human-defined rules. We find that CoT reasoning can degrade inductive performance, with LRMs often underperforming their non-reasoning counterparts.
  To explain this, we present a theoretical framework that reveals how reasoning steps can amplify error through three failure modes: incorrect sub-task decomposition, incorrect sub-task solving, and incorrect final answer summarization. Based on our theoretical and empirical analysis, we introduce structured interventions that adapt CoT generation according to our identified failure types. These interventions improve inductive accuracy without retraining. Our findings suggest that effective (CoT) reasoning depends not only on taking more steps but also on ensuring those steps are well-structured.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Malware Classification of Windows PE Files using CNNs and Greyscale Images Derived from Runtime API Call Argument Conversion</title>
<link>https://arxiv.org/abs/2505.24231</link>
<guid>https://arxiv.org/abs/2505.24231</guid>
<content:encoded><![CDATA[
arXiv:2505.24231v1 Announce Type: cross 
Abstract: Malware detection and classification remains a topic of concern for cybersecurity, since it is becoming common for attackers to use advanced obfuscation on their malware to stay undetected. Conventional static analysis is not effective against polymorphic and metamorphic malware as these change their appearance without modifying their behavior, thus defying the analysis by code structure alone. This makes it important to use dynamic detection that monitors malware behavior at runtime. In this paper, we present a dynamic malware categorization framework that extracts API argument calls at the runtime execution of Windows Portable Executable (PE) files. Extracting and encoding the dynamic features of API names, argument return values, and other relative features, we convert raw behavioral data to temporal patterns. To enhance feature portrayal, the generated patterns are subsequently converted into grayscale pictures using a magma colormap. These improved photos are used to teach a Convolutional Neural Network (CNN) model discriminative features, which allows for reliable and accurate malware classification. Results from experiments indicate that our method, with an average accuracy of 98.36% is effective in classifying different classes of malware and benign by integrating dynamic analysis and deep learning. It not only achieves high classification accuracy but also demonstrates significant resilience against typical evasion strategies.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Hallucinations to Jailbreaks: Rethinking the Vulnerability of Large Foundation Models</title>
<link>https://arxiv.org/abs/2505.24232</link>
<guid>https://arxiv.org/abs/2505.24232</guid>
<content:encoded><![CDATA[
arXiv:2505.24232v1 Announce Type: cross 
Abstract: Large foundation models (LFMs) are susceptible to two distinct vulnerabilities: hallucinations and jailbreak attacks. While typically studied in isolation, we observe that defenses targeting one often affect the other, hinting at a deeper connection.
  We propose a unified theoretical framework that models jailbreaks as token-level optimization and hallucinations as attention-level optimization. Within this framework, we establish two key propositions: (1) \textit{Similar Loss Convergence} - the loss functions for both vulnerabilities converge similarly when optimizing for target-specific outputs; and (2) \textit{Gradient Consistency in Attention Redistribution} - both exhibit consistent gradient behavior driven by shared attention dynamics.
  We validate these propositions empirically on LLaVA-1.5 and MiniGPT-4, showing consistent optimization trends and aligned gradients. Leveraging this connection, we demonstrate that mitigation techniques for hallucinations can reduce jailbreak success rates, and vice versa. Our findings reveal a shared failure mode in LFMs and suggest that robustness strategies should jointly address both vulnerabilities.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Adversary-Resistant Multi-Agent LLM System via Credibility Scoring</title>
<link>https://arxiv.org/abs/2505.24239</link>
<guid>https://arxiv.org/abs/2505.24239</guid>
<content:encoded><![CDATA[
arXiv:2505.24239v1 Announce Type: cross 
Abstract: While multi-agent LLM systems show strong capabilities in various domains, they are highly vulnerable to adversarial and low-performing agents. To resolve this issue, in this paper, we introduce a general and adversary-resistant multi-agent LLM framework based on credibility scoring. We model the collaborative query-answering process as an iterative game, where the agents communicate and contribute to a final system output. Our system associates a credibility score that is used when aggregating the team outputs. The credibility scores are learned gradually based on the past contributions of each agent in query answering. Our experiments across multiple tasks and settings demonstrate our system's effectiveness in mitigating adversarial influence and enhancing the resilience of multi-agent cooperation, even in the adversary-majority settings.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LTM3D: Bridging Token Spaces for Conditional 3D Generation with Auto-Regressive Diffusion Framework</title>
<link>https://arxiv.org/abs/2505.24245</link>
<guid>https://arxiv.org/abs/2505.24245</guid>
<content:encoded><![CDATA[
arXiv:2505.24245v1 Announce Type: cross 
Abstract: We present LTM3D, a Latent Token space Modeling framework for conditional 3D shape generation that integrates the strengths of diffusion and auto-regressive (AR) models. While diffusion-based methods effectively model continuous latent spaces and AR models excel at capturing inter-token dependencies, combining these paradigms for 3D shape generation remains a challenge. To address this, LTM3D features a Conditional Distribution Modeling backbone, leveraging a masked autoencoder and a diffusion model to enhance token dependency learning. Additionally, we introduce Prefix Learning, which aligns condition tokens with shape latent tokens during generation, improving flexibility across modalities. We further propose a Latent Token Reconstruction module with Reconstruction-Guided Sampling to reduce uncertainty and enhance structural fidelity in generated shapes. Our approach operates in token space, enabling support for multiple 3D representations, including signed distance fields, point clouds, meshes, and 3D Gaussian Splatting. Extensive experiments on image- and text-conditioned shape generation tasks demonstrate that LTM3D outperforms existing methods in prompt fidelity and structural accuracy while offering a generalizable framework for multi-modal, multi-representation 3D generation.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Reward-driven Automated Webshell Malicious-code Generator for Red-teaming</title>
<link>https://arxiv.org/abs/2505.24252</link>
<guid>https://arxiv.org/abs/2505.24252</guid>
<content:encoded><![CDATA[
arXiv:2505.24252v1 Announce Type: cross 
Abstract: Frequent cyber-attacks have elevated WebShell exploitation and defense to a critical research focus within network security. However, there remains a significant shortage of publicly available, well-categorized malicious-code datasets organized by obfuscation method. Existing malicious-code generation methods, which primarily rely on prompt engineering, often suffer from limited diversity and high redundancy in the payloads they produce. To address these limitations, we propose \textbf{RAWG}, a \textbf{R}eward-driven \textbf{A}utomated \textbf{W}ebshell Malicious-code \textbf{G}enerator designed for red-teaming applications. Our approach begins by categorizing webshell samples from common datasets into seven distinct types of obfuscation. We then employ a large language model (LLM) to extract and normalize key tokens from each sample, creating a standardized, high-quality corpus. Using this curated dataset, we perform supervised fine-tuning (SFT) on an open-source large model to enable the generation of diverse, highly obfuscated webshell malicious payloads. To further enhance generation quality, we apply Proximal Policy Optimization (PPO), treating malicious-code samples as "chosen" data and benign code as "rejected" data during reinforcement learning. Extensive experiments demonstrate that RAWG significantly outperforms current state-of-the-art methods in both payload diversity and escape effectiveness.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interactive Video Generation via Domain Adaptation</title>
<link>https://arxiv.org/abs/2505.24253</link>
<guid>https://arxiv.org/abs/2505.24253</guid>
<content:encoded><![CDATA[
arXiv:2505.24253v1 Announce Type: cross 
Abstract: Text-conditioned diffusion models have emerged as powerful tools for high-quality video generation. However, enabling Interactive Video Generation (IVG), where users control motion elements such as object trajectory, remains challenging. Recent training-free approaches introduce attention masking to guide trajectory, but this often degrades perceptual quality. We identify two key failure modes in these methods, both of which we interpret as domain shift problems, and propose solutions inspired by domain adaptation. First, we attribute the perceptual degradation to internal covariate shift induced by attention masking, as pretrained models are not trained to handle masked attention. To address this, we propose mask normalization, a pre-normalization layer designed to mitigate this shift via distribution matching. Second, we address initialization gap, where the randomly sampled initial noise does not align with IVG conditioning, by introducing a temporal intrinsic diffusion prior that enforces spatio-temporal consistency at each denoising step. Extensive qualitative and quantitative evaluations demonstrate that mask normalization and temporal intrinsic denoising improve both perceptual quality and trajectory control over the existing state-of-the-art IVG techniques.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effects of Theory of Mind and Prosocial Beliefs on Steering Human-Aligned Behaviors of LLMs in Ultimatum Games</title>
<link>https://arxiv.org/abs/2505.24255</link>
<guid>https://arxiv.org/abs/2505.24255</guid>
<content:encoded><![CDATA[
arXiv:2505.24255v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown potential in simulating human behaviors and performing theory-of-mind (ToM) reasoning, a crucial skill for complex social interactions. In this study, we investigate the role of ToM reasoning in aligning agentic behaviors with human norms in negotiation tasks, using the ultimatum game as a controlled environment. We initialized LLM agents with different prosocial beliefs (including Greedy, Fair, and Selfless) and reasoning methods like chain-of-thought (CoT) and varying ToM levels, and examined their decision-making processes across diverse LLMs, including reasoning models like o3-mini and DeepSeek-R1 Distilled Qwen 32B. Results from 2,700 simulations indicated that ToM reasoning enhances behavior alignment, decision-making consistency, and negotiation outcomes. Consistent with previous findings, reasoning models exhibit limited capability compared to models with ToM reasoning, different roles of the game benefits with different orders of ToM reasoning. Our findings contribute to the understanding of ToM's role in enhancing human-AI interaction and cooperative decision-making. The code used for our experiments can be found at https://github.com/Stealth-py/UltimatumToM.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Faithful and Robust LLM-Driven Theorem Proving for NLI Explanations</title>
<link>https://arxiv.org/abs/2505.24264</link>
<guid>https://arxiv.org/abs/2505.24264</guid>
<content:encoded><![CDATA[
arXiv:2505.24264v1 Announce Type: cross 
Abstract: Natural language explanations play a fundamental role in Natural Language Inference (NLI) by revealing how premises logically entail hypotheses. Recent work has shown that the interaction of large language models (LLMs) with theorem provers (TPs) can help verify and improve the validity of NLI explanations. However, TPs require translating natural language into machine-verifiable formal representations, a process that introduces the risk of semantic information loss and unfaithful interpretation, an issue compounded by LLMs' challenges in capturing critical logical structures with sufficient precision. Moreover, LLMs are still limited in their capacity for rigorous and robust proof construction within formal verification frameworks. To mitigate issues related to faithfulness and robustness, this paper investigates strategies to (1) alleviate semantic loss during autoformalisation, (2) efficiently identify and correct syntactic errors in logical representations, (3) explicitly use logical expressions to guide LLMs in generating structured proof sketches, and (4) increase LLMs' capacity of interpreting TP's feedback for iterative refinement. Our empirical results on e-SNLI, QASC and WorldTree using different LLMs demonstrate that the proposed strategies yield significant improvements in autoformalisation (+18.46%, +34.2%, +39.77%) and explanation refinement (+29.5%, +51.5%, +41.25%) over the state-of-the-art model. Moreover, we show that specific interventions on the hybrid LLM-TP architecture can substantially improve efficiency, drastically reducing the number of iterations required for successful verification.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>INSIGHT: A Survey of In-Network Systems for Intelligent, High-Efficiency AI and Topology Optimization</title>
<link>https://arxiv.org/abs/2505.24269</link>
<guid>https://arxiv.org/abs/2505.24269</guid>
<content:encoded><![CDATA[
arXiv:2505.24269v1 Announce Type: cross 
Abstract: In-network computation represents a transformative approach to addressing the escalating demands of Artificial Intelligence (AI) workloads on network infrastructure. By leveraging the processing capabilities of network devices such as switches, routers, and Network Interface Cards (NICs), this paradigm enables AI computations to be performed directly within the network fabric, significantly reducing latency, enhancing throughput, and optimizing resource utilization. This paper provides a comprehensive analysis of optimizing in-network computation for AI, exploring the evolution of programmable network architectures, such as Software-Defined Networking (SDN) and Programmable Data Planes (PDPs), and their convergence with AI. It examines methodologies for mapping AI models onto resource-constrained network devices, addressing challenges like limited memory and computational capabilities through efficient algorithm design and model compression techniques. The paper also highlights advancements in distributed learning, particularly in-network aggregation, and the potential of federated learning to enhance privacy and scalability. Frameworks like Planter and Quark are discussed for simplifying development, alongside key applications such as intelligent network monitoring, intrusion detection, traffic management, and Edge AI. Future research directions, including runtime programmability, standardized benchmarks, and new applications paradigms, are proposed to advance this rapidly evolving field. This survey underscores the potential of in-network AI to create intelligent, efficient, and responsive networks capable of meeting the demands of next-generation AI applications.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discl-VC: Disentangled Discrete Tokens and In-Context Learning for Controllable Zero-Shot Voice Conversion</title>
<link>https://arxiv.org/abs/2505.24291</link>
<guid>https://arxiv.org/abs/2505.24291</guid>
<content:encoded><![CDATA[
arXiv:2505.24291v1 Announce Type: cross 
Abstract: Currently, zero-shot voice conversion systems are capable of synthesizing the voice of unseen speakers. However, most existing approaches struggle to accurately replicate the speaking style of the source speaker or mimic the distinctive speaking style of the target speaker, thereby limiting the controllability of voice conversion. In this work, we propose Discl-VC, a novel voice conversion framework that disentangles content and prosody information from self-supervised speech representations and synthesizes the target speaker's voice through in-context learning with a flow matching transformer. To enable precise control over the prosody of generated speech, we introduce a mask generative transformer that predicts discrete prosody tokens in a non-autoregressive manner based on prompts. Experimental results demonstrate the superior performance of Discl-VC in zero-shot voice conversion and its remarkable accuracy in prosody control for synthesized speech.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models are Locally Linear Mappings</title>
<link>https://arxiv.org/abs/2505.24293</link>
<guid>https://arxiv.org/abs/2505.24293</guid>
<content:encoded><![CDATA[
arXiv:2505.24293v1 Announce Type: cross 
Abstract: We demonstrate that the inference operations of several open-weight large language models (LLMs) can be mapped to an exactly equivalent linear system for an input sequence without modifying the model weights or altering output predictions. Extending techniques from image diffusion models that exhibit local or piecewise linearity, we strategically alter the gradient computation with respect to a given input sequence for a next-token prediction such that the Jacobian of the model nearly exactly reproduces the forward prediction with a linear system. We demonstrate this approach across models (Llama 3, Gemma 3, Qwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show through the singular value decomposition of the detached Jacobian that these LLMs operate in extremely low-dimensional subspaces where many of the largest singular vectors decode to concepts related to the most-likely output token. This approach also allows us to examine the operation of each successive layer (and its attention and MLP components) as nearly-exact linear systems and observe the emergence of semantic concepts. Despite their expressive power and global nonlinearity, modern LLMs can be interpreted through nearly-exact locally linear decompositions that provide insights into their internal representations and reveal interpretable semantic structures in the next-token prediction process.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning</title>
<link>https://arxiv.org/abs/2505.24298</link>
<guid>https://arxiv.org/abs/2505.24298</guid>
<content:encoded><![CDATA[
arXiv:2505.24298v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has become a trending paradigm for training large language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs requires massive parallelization and poses an urgent need for efficient training systems. Most existing large-scale RL systems for LLMs are synchronous by alternating generation and training in a batch setting, where the rollouts in each training batch are generated by the same (or latest) model. This stabilizes RL training but suffers from severe system-level inefficiency. Generation must wait until the longest output in the batch is completed before model update, resulting in GPU underutilization. We present AReaL, a \emph{fully asynchronous} RL system that completely decouples generation from training. Rollout workers in AReaL continuously generate new outputs without waiting, while training workers update the model whenever a batch of data is collected. AReaL also incorporates a collection of system-level optimizations, leading to substantially higher GPU utilization. To stabilize RL training, AReaL balances the workload of rollout and training workers to control data staleness, and adopts a staleness-enhanced PPO variant to better handle outdated training samples. Extensive experiments on math and code reasoning benchmarks show that AReaL achieves \textbf{up to 2.57$\times$ training speedup} compared to the best synchronous systems with the same number of GPUs and matched or even improved final performance. The code of AReaL is available at https://github.com/inclusionAI/AReaL/.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Humans Growl and Birds Speak: High-Fidelity Voice Conversion from Human to Animal and Designed Sounds</title>
<link>https://arxiv.org/abs/2505.24336</link>
<guid>https://arxiv.org/abs/2505.24336</guid>
<content:encoded><![CDATA[
arXiv:2505.24336v1 Announce Type: cross 
Abstract: Human to non-human voice conversion (H2NH-VC) transforms human speech into animal or designed vocalizations. Unlike prior studies focused on dog-sounds and 16 or 22.05kHz audio transformation, this work addresses a broader range of non-speech sounds, including natural sounds (lion-roars, birdsongs) and designed voice (synthetic growls). To accomodate generation of diverse non-speech sounds and 44.1kHz high-quality audio transformation, we introduce a preprocessing pipeline and an improved CVAE-based H2NH-VC model, both optimized for human and non-human voices. Experimental results showed that the proposed method outperformed baselines in quality, naturalness, and similarity MOS, achieving effective voice conversion across diverse non-human timbres. Demo samples are available at https://nc-ai.github.io/speech/publications/nonhuman-vc/
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Multimodal Challenges in Toxic Chinese Detection: Taxonomy, Benchmark, and Findings</title>
<link>https://arxiv.org/abs/2505.24341</link>
<guid>https://arxiv.org/abs/2505.24341</guid>
<content:encoded><![CDATA[
arXiv:2505.24341v1 Announce Type: cross 
Abstract: Detecting toxic content using language models is important but challenging. While large language models (LLMs) have demonstrated strong performance in understanding Chinese, recent studies show that simple character substitutions in toxic Chinese text can easily confuse the state-of-the-art (SOTA) LLMs. In this paper, we highlight the multimodal nature of Chinese language as a key challenge for deploying LLMs in toxic Chinese detection. First, we propose a taxonomy of 3 perturbation strategies and 8 specific approaches in toxic Chinese content. Then, we curate a dataset based on this taxonomy, and benchmark 9 SOTA LLMs (from both the US and China) to assess if they can detect perturbed toxic Chinese text. Additionally, we explore cost-effective enhancement solutions like in-context learning (ICL) and supervised fine-tuning (SFT). Our results reveal two important findings. (1) LLMs are less capable of detecting perturbed multimodal Chinese toxic contents. (2) ICL or SFT with a small number of perturbed examples may cause the LLMs "overcorrect'': misidentify many normal Chinese contents as toxic.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline Calibration</title>
<link>https://arxiv.org/abs/2505.24357</link>
<guid>https://arxiv.org/abs/2505.24357</guid>
<content:encoded><![CDATA[
arXiv:2505.24357v1 Announce Type: cross 
Abstract: Large language models (LLMs) have achieved remarkable performance, yet their capability on long-context reasoning is often constrained by the excessive memory required to store the Key-Value (KV) cache. This makes KV cache compression an essential step toward enabling efficient long-context reasoning. Recent methods have explored reducing the hidden dimensions of the KV cache, but many introduce additional computation through projection layers or suffer from significant performance degradation under high compression ratios. To address these challenges, we propose ReCalKV, a post-training KV cache compression method that reduces the hidden dimensions of the KV cache. We develop distinct compression strategies for Keys and Values based on their different roles and varying importance in the attention mechanism. For Keys, we propose Head-wise Similarity-aware Reordering (HSR), which clusters similar heads and applies grouped SVD to the key projection matrix, reducing additional computation while preserving accuracy. For Values, we propose Offline Calibration and Matrix Fusion (OCMF) to preserve accuracy without extra computational overhead. Experiments show that ReCalKV outperforms existing low-rank compression methods, achieving high compression ratios with minimal performance loss. Code is available at: https://github.com/XIANGLONGYAN/ReCalKV.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Preference Learning for Robust LLM Alignment</title>
<link>https://arxiv.org/abs/2505.24369</link>
<guid>https://arxiv.org/abs/2505.24369</guid>
<content:encoded><![CDATA[
arXiv:2505.24369v1 Announce Type: cross 
Abstract: Modern language models often rely on Reinforcement Learning from Human Feedback (RLHF) to encourage safe behaviors. However, they remain vulnerable to adversarial attacks due to three key limitations: (1) the inefficiency and high cost of human annotation, (2) the vast diversity of potential adversarial attacks, and (3) the risk of feedback bias and reward hacking. To address these challenges, we introduce Adversarial Preference Learning (APL), an iterative adversarial training method incorporating three key innovations. First, a direct harmfulness metric based on the model's intrinsic preference probabilities, eliminating reliance on external assessment. Second, a conditional generative attacker that synthesizes input-specific adversarial variations. Third, an iterative framework with automated closed-loop feedback, enabling continuous adaptation through vulnerability discovery and mitigation. Experiments on Mistral-7B-Instruct-v0.3 demonstrate that APL significantly enhances robustness, achieving 83.33% harmlessness win rate over the base model (evaluated by GPT-4o), reducing harmful outputs from 5.88% to 0.43% (measured by LLaMA-Guard), and lowering attack success rate by up to 65% according to HarmBench. Notably, APL maintains competitive utility, with an MT-Bench score of 6.59 (comparable to the baseline 6.78) and an LC-WinRate of 46.52% against the base model.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grid-LOGAT: Grid Based Local and Global Area Transcription for Video Question Answering</title>
<link>https://arxiv.org/abs/2505.24371</link>
<guid>https://arxiv.org/abs/2505.24371</guid>
<content:encoded><![CDATA[
arXiv:2505.24371v1 Announce Type: cross 
Abstract: In this paper, we propose a Grid-based Local and Global Area Transcription (Grid-LoGAT) system for Video Question Answering (VideoQA). The system operates in two phases. First, extracting text transcripts from video frames using a Vision-Language Model (VLM). Next, processing questions using these transcripts to generate answers through a Large Language Model (LLM). This design ensures image privacy by deploying the VLM on edge devices and the LLM in the cloud. To improve transcript quality, we propose grid-based visual prompting, which extracts intricate local details from each grid cell and integrates them with global information. Evaluation results show that Grid-LoGAT, using the open-source VLM (LLaVA-1.6-7B) and LLM (Llama-3.1-8B), outperforms state-of-the-art methods with similar baseline models on NExT-QA and STAR-QA datasets with an accuracy of 65.9% and 50.11% respectively. Additionally, our method surpasses the non-grid version by 24 points on localization-based questions we created using NExT-QA.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mastering Massive Multi-Task Reinforcement Learning via Mixture-of-Expert Decision Transformer</title>
<link>https://arxiv.org/abs/2505.24378</link>
<guid>https://arxiv.org/abs/2505.24378</guid>
<content:encoded><![CDATA[
arXiv:2505.24378v1 Announce Type: cross 
Abstract: Despite recent advancements in offline multi-task reinforcement learning (MTRL) have harnessed the powerful capabilities of the Transformer architecture, most approaches focus on a limited number of tasks, with scaling to extremely massive tasks remaining a formidable challenge. In this paper, we first revisit the key impact of task numbers on current MTRL method, and further reveal that naively expanding the parameters proves insufficient to counteract the performance degradation as the number of tasks escalates. Building upon these insights, we propose M3DT, a novel mixture-of-experts (MoE) framework that tackles task scalability by further unlocking the model's parameter scalability. Specifically, we enhance both the architecture and the optimization of the agent, where we strengthen the Decision Transformer (DT) backbone with MoE to reduce task load on parameter subsets, and introduce a three-stage training mechanism to facilitate efficient training with optimal performance. Experimental results show that, by increasing the number of experts, M3DT not only consistently enhances its performance as model expansion on the fixed task numbers, but also exhibits remarkable task scalability, successfully extending to 160 tasks with superior performance.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the Gold Standard: Extracting Forgotten Data under Exact Unlearning in Large Language Models</title>
<link>https://arxiv.org/abs/2505.24379</link>
<guid>https://arxiv.org/abs/2505.24379</guid>
<content:encoded><![CDATA[
arXiv:2505.24379v1 Announce Type: cross 
Abstract: Large language models are typically trained on datasets collected from the web, which may inadvertently contain harmful or sensitive personal information. To address growing privacy concerns, unlearning methods have been proposed to remove the influence of specific data from trained models. Of these, exact unlearning -- which retrains the model from scratch without the target data -- is widely regarded the gold standard, believed to be robust against privacy-related attacks. In this paper, we challenge this assumption by introducing a novel data extraction attack that compromises even exact unlearning. Our method leverages both the pre- and post-unlearning models: by guiding the post-unlearning model using signals from the pre-unlearning model, we uncover patterns that reflect the removed data distribution. Combining model guidance with a token filtering strategy, our attack significantly improves extraction success rates -- doubling performance in some cases -- across common benchmarks such as MUSE, TOFU, and WMDP. Furthermore, we demonstrate our attack's effectiveness on a simulated medical diagnosis dataset to highlight real-world privacy risks associated with exact unlearning. In light of our findings, which suggest that unlearning may, in a contradictory way, increase the risk of privacy leakage, we advocate for evaluation of unlearning methods to consider broader threat models that account not only for post-unlearning models but also for adversarial access to prior checkpoints.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SASP: Strip-Aware Spatial Perception for Fine-Grained Bird Image Classification</title>
<link>https://arxiv.org/abs/2505.24380</link>
<guid>https://arxiv.org/abs/2505.24380</guid>
<content:encoded><![CDATA[
arXiv:2505.24380v1 Announce Type: cross 
Abstract: Fine-grained bird image classification (FBIC) is not only of great significance for ecological monitoring and species identification, but also holds broad research value in the fields of image recognition and fine-grained visual modeling. Compared with general image classification tasks, FBIC poses more formidable challenges: 1) the differences in species size and imaging distance result in the varying sizes of birds presented in the images; 2) complex natural habitats often introduce strong background interference; 3) and highly flexible poses such as flying, perching, or foraging result in substantial intra-class variability. These factors collectively make it difficult for traditional methods to stably extract discriminative features, thereby limiting the generalizability and interpretability of models in real-world applications. To address these challenges, this paper proposes a fine-grained bird classification framework based on strip-aware spatial perception, which aims to capture long-range spatial dependencies across entire rows or columns in bird images, thereby enhancing the model's robustness and interpretability. The proposed method incorporates two novel modules: extensional perception aggregator (EPA) and channel semantic weaving (CSW). Specifically, EPA integrates local texture details with global structural cues by aggregating information across horizontal and vertical spatial directions. CSW further refines the semantic representations by adaptively fusing long-range and short-range information along the channel dimension. Built upon a ResNet-50 backbone, the model enables jump-wise connection of extended structural features across the spatial domain. Experimental results on the CUB-200-2011 dataset demonstrate that our framework achieves significant performance improvements while maintaining architectural efficiency.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs Are Globally Multilingual Yet Locally Monolingual: Exploring Knowledge Transfer via Language and Thought Theory</title>
<link>https://arxiv.org/abs/2505.24409</link>
<guid>https://arxiv.org/abs/2505.24409</guid>
<content:encoded><![CDATA[
arXiv:2505.24409v1 Announce Type: cross 
Abstract: Multilingual large language models (LLMs) open up new possibilities for leveraging information across languages, but their factual knowledge recall remains inconsistent depending on the input language. While previous studies have attempted to address this issue through English-based prompting and evaluation, we explore non-English to English transfer via Language and Thought Theory. This perspective allows us to examine language-thought binding in LLMs and uncover why factual knowledge often fails to transfer effectively. We propose the Language-to-Thought (L2T) prompting strategy, which analyzes the relationship between input language, internal cognitive processes, and knowledge. Experimental results challenge the assumption that English-based approaches consistently outperform other languages and offer a novel insight that aligning the model's internal thought with the knowledge required for the task is critical for successful cross-lingual transfer. Furthermore, we show that applying L2T during training can alleviate LLMs' reliance on the input language and facilitate cross-linguistic knowledge integration without translation-based learning. Code and datasets will be available.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Automatic Exercise Evaluation Through Musculoskeletal Simulation-Based IMU Data Augmentation</title>
<link>https://arxiv.org/abs/2505.24415</link>
<guid>https://arxiv.org/abs/2505.24415</guid>
<content:encoded><![CDATA[
arXiv:2505.24415v1 Announce Type: cross 
Abstract: Automated evaluation of movement quality holds significant potential for enhancing physiotherapeutic treatments and sports training by providing objective, real-time feedback. However, the effectiveness of deep learning models in assessing movements captured by inertial measurement units (IMUs) is often hampered by limited data availability, class imbalance, and label ambiguity. In this work, we present a novel data augmentation method that generates realistic IMU data using musculoskeletal simulations integrated with systematic modifications of movement trajectories. Crucially, our approach ensures biomechanical plausibility and allows for automatic, reliable labeling by combining inverse kinematic parameters with a knowledge-based evaluation strategy. Extensive evaluations demonstrate that augmented variants closely resembles real-world data, significantly improving the classification accuracy and generalization capability of neural network models. Additionally, we highlight the benefits of augmented data for patient-specific fine-tuning scenarios, particularly when only limited subject-specific training examples are available. Our findings underline the practicality and efficacy of this augmentation method in overcoming common challenges faced by deep learning applications in physiotherapeutic exercise evaluation.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Weather Models for Subregional Ocean Forecasting: A Case Study on the Canary Current Upwelling System</title>
<link>https://arxiv.org/abs/2505.24429</link>
<guid>https://arxiv.org/abs/2505.24429</guid>
<content:encoded><![CDATA[
arXiv:2505.24429v1 Announce Type: cross 
Abstract: Oceanographic forecasting impacts various sectors of society by supporting environmental conservation and economic activities. Based on global circulation models, traditional forecasting methods are computationally expensive and slow, limiting their ability to provide rapid forecasts. Recent advances in deep learning offer faster and more accurate predictions, although these data-driven models are often trained with global data from numerical simulations, which may not reflect reality. The emergence of such models presents great potential for improving ocean prediction at a subregional domain. However, their ability to predict fine-scale ocean processes, like mesoscale structures, remains largely unknown. This work aims to adapt a graph neural network initially developed for global weather forecasting to improve subregional ocean prediction, specifically focusing on the Canary Current upwelling system. The model is trained with satellite data and compared to state-of-the-art physical ocean models to assess its performance in capturing ocean dynamics. Our results show that the deep learning model surpasses traditional methods in precision despite some challenges in upwelling areas. It demonstrated superior performance in reducing RMSE errors compared to ConvLSTM and the GLORYS reanalysis, particularly in regions with complex oceanic dynamics such as Cape Ghir, Cape Bojador, and Cape Blanc. The model achieved improvements of up to 26.5% relative to ConvLSTM and error reductions of up to 76% in 5-day forecasts compared to the GLORYS reanalysis at these critical locations, highlighting its enhanced capability to capture spatial variability and improve predictive accuracy in complex areas. These findings suggest the viability of adapting meteorological data-driven models for improving subregional medium-term ocean forecasting.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Safety Constraints for Large Language Models</title>
<link>https://arxiv.org/abs/2505.24445</link>
<guid>https://arxiv.org/abs/2505.24445</guid>
<content:encoded><![CDATA[
arXiv:2505.24445v1 Announce Type: cross 
Abstract: Large language models (LLMs) have emerged as powerful tools but pose significant safety risks through harmful outputs and vulnerability to adversarial attacks. We propose SaP, short for Safety Polytope, a geometric approach to LLM safety that learns and enforces multiple safety constraints directly in the model's representation space. We develop a framework that identifies safe and unsafe regions via the polytope's facets, enabling both detection and correction of unsafe outputs through geometric steering. Unlike existing approaches that modify model weights, SaP operates post-hoc in the representation space, preserving model capabilities while enforcing safety constraints. Experiments across multiple LLMs demonstrate that our method can effectively detect unethical inputs, reduce adversarial attack success rates while maintaining performance on standard tasks, thus highlighting the importance of having an explicit geometric model for safety. Analysis of the learned polytope facets reveals emergence of specialization in detecting different semantic notions of safety, providing interpretable insights into how safety is captured in LLMs' representation space.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LPASS: Linear Probes as Stepping Stones for vulnerability detection using compressed LLMs</title>
<link>https://arxiv.org/abs/2505.24451</link>
<guid>https://arxiv.org/abs/2505.24451</guid>
<content:encoded><![CDATA[
arXiv:2505.24451v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are being extensively used for cybersecurity purposes. One of them is the detection of vulnerable codes. For the sake of efficiency and effectiveness, compression and fine-tuning techniques are being developed, respectively. However, they involve spending substantial computational efforts. In this vein, we analyse how Linear Probes (LPs) can be used to provide an estimation on the performance of a compressed LLM at an early phase -- before fine-tuning. We also show their suitability to set the cut-off point when applying layer pruning compression. Our approach, dubbed $LPASS$, is applied in BERT and Gemma for the detection of 12 of MITRE's Top 25 most dangerous vulnerabilities on 480k C/C++ samples. LPs can be computed in 142.97 s. and provide key findings: (1) 33.3 \% and 72.2\% of layers can be removed, respectively, with no precision loss; (2) they provide an early estimate of the post-fine-tuning and post-compression model effectiveness, with 3\% and 8.68\% as the lowest and average precision errors, respectively. $LPASS$-based LLMs outperform the state of the art, reaching 86.9\% of accuracy in multi-class vulnerability detection. Interestingly, $LPASS$-based compressed versions of Gemma outperform the original ones by 1.6\% of F1-score at a maximum while saving 29.4 \% and 23.8\% of training and inference time and 42.98\% of model size.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VietMix: A Naturally Occurring Vietnamese-English Code-Mixed Corpus with Iterative Augmentation for Machine Translation</title>
<link>https://arxiv.org/abs/2505.24472</link>
<guid>https://arxiv.org/abs/2505.24472</guid>
<content:encoded><![CDATA[
arXiv:2505.24472v1 Announce Type: cross 
Abstract: Machine translation systems fail when processing code-mixed inputs for low-resource languages. We address this challenge by curating VietMix, a parallel corpus of naturally occurring code-mixed Vietnamese text paired with expert English translations. Augmenting this resource, we developed a complementary synthetic data generation pipeline. This pipeline incorporates filtering mechanisms to ensure syntactic plausibility and pragmatic appropriateness in code-mixing patterns. Experimental validation shows our naturalistic and complementary synthetic data boost models' performance, measured by translation quality estimation scores, of up to 71.84 on COMETkiwi and 81.77 on XCOMET. Triangulating positive results with LLM-based assessments, augmented models are favored over seed fine-tuned counterparts in approximately 49% of judgments (54-56% excluding ties). VietMix and our augmentation methodology advance ecological validity in neural MT evaluations and establish a framework for addressing code-mixed translation challenges across other low-resource pairs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Train One Sparse Autoencoder Across Multiple Sparsity Budgets to Preserve Interpretability and Accuracy</title>
<link>https://arxiv.org/abs/2505.24473</link>
<guid>https://arxiv.org/abs/2505.24473</guid>
<content:encoded><![CDATA[
arXiv:2505.24473v1 Announce Type: cross 
Abstract: Sparse Autoencoders (SAEs) have proven to be powerful tools for interpreting neural networks by decomposing hidden representations into disentangled, interpretable features via sparsity constraints. However, conventional SAEs are constrained by the fixed sparsity level chosen during training; meeting different sparsity requirements therefore demands separate models and increases the computational footprint during both training and evaluation. We introduce a novel training objective, \emph{HierarchicalTopK}, which trains a single SAE to optimise reconstructions across multiple sparsity levels simultaneously. Experiments with Gemma-2 2B demonstrate that our approach achieves Pareto-optimal trade-offs between sparsity and explained variance, outperforming traditional SAEs trained at individual sparsity levels. Further analysis shows that HierarchicalTopK preserves high interpretability scores even at higher sparsity. The proposed objective thus closes an important gap between flexibility and interpretability in SAE design.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Gemini in an arena for learning</title>
<link>https://arxiv.org/abs/2505.24477</link>
<guid>https://arxiv.org/abs/2505.24477</guid>
<content:encoded><![CDATA[
arXiv:2505.24477v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) is poised to transform education, but the research community lacks a robust, general benchmark to evaluate AI models for learning. To assess state-of-the-art support for educational use cases, we ran an "arena for learning" where educators and pedagogy experts conduct blind, head-to-head, multi-turn comparisons of leading AI models. In particular, $N = 189$ educators drew from their experience to role-play realistic learning use cases, interacting with two models sequentially, after which $N = 206$ experts judged which model better supported the user's learning goals. The arena evaluated a slate of state-of-the-art models: Gemini 2.5 Pro, Claude 3.7 Sonnet, GPT-4o, and OpenAI o3. Excluding ties, experts preferred Gemini 2.5 Pro in 73.2% of these match-ups -- ranking it first overall in the arena. Gemini 2.5 Pro also demonstrated markedly higher performance across key principles of good pedagogy. Altogether, these results position Gemini 2.5 Pro as a leading model for learning.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Effective Code-Integrated Reasoning</title>
<link>https://arxiv.org/abs/2505.24480</link>
<guid>https://arxiv.org/abs/2505.24480</guid>
<content:encoded><![CDATA[
arXiv:2505.24480v1 Announce Type: cross 
Abstract: In this paper, we investigate code-integrated reasoning, where models generate code when necessary and integrate feedback by executing it through a code interpreter. To acquire this capability, models must learn when and how to use external code tools effectively, which is supported by tool-augmented reinforcement learning (RL) through interactive learning. Despite its benefits, tool-augmented RL can still suffer from potential instability in the learning dynamics. In light of this challenge, we present a systematic approach to improving the training effectiveness and stability of tool-augmented RL for code-integrated reasoning. Specifically, we develop enhanced training strategies that balance exploration and stability, progressively building tool-use capabilities while improving reasoning performance. Through extensive experiments on five mainstream mathematical reasoning benchmarks, our model demonstrates significant performance improvements over multiple competitive baselines. Furthermore, we conduct an in-depth analysis of the mechanism and effect of code-integrated reasoning, revealing several key insights, such as the extension of model's capability boundaries and the simultaneous improvement of reasoning efficiency through code integration. All data and code for reproducing this work are available at: https://github.com/RUCAIBox/CIR.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rehearsal with Auxiliary-Informed Sampling for Audio Deepfake Detection</title>
<link>https://arxiv.org/abs/2505.24486</link>
<guid>https://arxiv.org/abs/2505.24486</guid>
<content:encoded><![CDATA[
arXiv:2505.24486v1 Announce Type: cross 
Abstract: The performance of existing audio deepfake detection frameworks degrades when confronted with new deepfake attacks. Rehearsal-based continual learning (CL), which updates models using a limited set of old data samples, helps preserve prior knowledge while incorporating new information. However, existing rehearsal techniques don't effectively capture the diversity of audio characteristics, introducing bias and increasing the risk of forgetting. To address this challenge, we propose Rehearsal with Auxiliary-Informed Sampling (RAIS), a rehearsal-based CL approach for audio deepfake detection. RAIS employs a label generation network to produce auxiliary labels, guiding diverse sample selection for the memory buffer. Extensive experiments show RAIS outperforms state-of-the-art methods, achieving an average Equal Error Rate (EER) of 1.953 % across five experiences. The code is available at: https://github.com/falihgoz/RAIS.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deformable Attention Mechanisms Applied to Object Detection, case of Remote Sensing</title>
<link>https://arxiv.org/abs/2505.24489</link>
<guid>https://arxiv.org/abs/2505.24489</guid>
<content:encoded><![CDATA[
arXiv:2505.24489v1 Announce Type: cross 
Abstract: Object detection has recently seen an interesting trend in terms of the most innovative research work, this task being of particular importance in the field of remote sensing, given the consistency of these images in terms of geographical coverage and the objects present. Furthermore, Deep Learning (DL) models, in particular those based on Transformers, are especially relevant for visual computing tasks in general, and target detection in particular. Thus, the present work proposes an application of Deformable-DETR model, a specific architecture using deformable attention mechanisms, on remote sensing images in two different modes, especially optical and Synthetic Aperture Radar (SAR). To achieve this objective, two datasets are used, one optical, which is Pleiades Aircraft dataset, and the other SAR, in particular SAR Ship Detection Dataset (SSDD). The results of a 10-fold stratified validation showed that the proposed model performed particularly well, obtaining an F1 score of 95.12% for the optical dataset and 94.54% for SSDD, while comparing these results with several models detections, especially those based on CNNs and transformers, as well as those specifically designed to detect different object classes in remote sensing images.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Object Centric Concept Bottlenecks</title>
<link>https://arxiv.org/abs/2505.24492</link>
<guid>https://arxiv.org/abs/2505.24492</guid>
<content:encoded><![CDATA[
arXiv:2505.24492v1 Announce Type: cross 
Abstract: Developing high-performing, yet interpretable models remains a critical challenge in modern AI. Concept-based models (CBMs) attempt to address this by extracting human-understandable concepts from a global encoding (e.g., image encoding) and then applying a linear classifier on the resulting concept activations, enabling transparent decision-making. However, their reliance on holistic image encodings limits their expressiveness in object-centric real-world settings and thus hinders their ability to solve complex vision tasks beyond single-label classification. To tackle these challenges, we introduce Object-Centric Concept Bottlenecks (OCB), a framework that combines the strengths of CBMs and pre-trained object-centric foundation models, boosting performance and interpretability. We evaluate OCB on complex image datasets and conduct a comprehensive ablation study to analyze key components of the framework, such as strategies for aggregating object-concept encodings. The results show that OCB outperforms traditional CBMs and allows one to make interpretable decisions for complex visual tasks.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning for Enhancing LLMs' Social Intelligence</title>
<link>https://arxiv.org/abs/2505.24500</link>
<guid>https://arxiv.org/abs/2505.24500</guid>
<content:encoded><![CDATA[
arXiv:2505.24500v1 Announce Type: cross 
Abstract: Recently, Large Language Models (LLMs) have made significant progress in IQ-related domains that require careful thinking, such as mathematics and coding. However, enhancing LLMs' cognitive development in social domains, particularly from a post-training perspective, remains underexplored. Recognizing that the social world follows a distinct timeline and requires a richer blend of cognitive modes (from intuitive reactions (System 1) and surface-level thinking to deliberate thinking (System 2)) than mathematics, which primarily relies on System 2 cognition (careful, step-by-step reasoning), we introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we systematically explore improving LLMs' social intelligence and validate the effectiveness of the TimeHC-RL method, through five other post-training paradigms and two test-time intervention paradigms on eight datasets with diverse data patterns. Experimental results reveal the superiority of our proposed TimeHC-RL method compared to the widely adopted System 2 RL method. It gives the 7B backbone model wings, enabling it to rival the performance of advanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic exploration from post-training and test-time interventions perspectives to improve LLMs' social intelligence has uncovered several valuable insights.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Fair Division with Additional Information</title>
<link>https://arxiv.org/abs/2505.24503</link>
<guid>https://arxiv.org/abs/2505.24503</guid>
<content:encoded><![CDATA[
arXiv:2505.24503v1 Announce Type: cross 
Abstract: We study the problem of fairly allocating indivisible goods to agents in an online setting, where goods arrive sequentially and must be allocated irrevocably to agents. Focusing on the popular fairness notions of envy-freeness, proportionality, and maximin share fairness (and their approximate variants), we ask how the availability of information on future goods influences the existence and approximability of fair allocations. In the absence of any such information, we establish strong impossibility results, demonstrating the inherent difficulty of achieving even approximate fairness guarantees. In contrast, we demonstrate that knowledge of additional information -- such as aggregate of each agent's total valuations (equivalently, normalized valuations) or the multiset of future goods values (frequency predictions) -- would enable the design of fairer online algorithms. Given normalization information, we propose an algorithm that achieves stronger fairness guarantees than previously known results. Given frequency predictions, we introduce a meta-algorithm that leverages frequency predictions to match the best-known offline guarantees for a broad class of ''share-based'' fairness notions. Our complementary impossibility results in each setting underscore both the limitations imposed by uncertainty about future goods and the potential of leveraging structured information to achieve fairer outcomes in online fair division.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Slow-thinking LLMs Reason Over Time? Empirical Studies in Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.24511</link>
<guid>https://arxiv.org/abs/2505.24511</guid>
<content:encoded><![CDATA[
arXiv:2505.24511v1 Announce Type: cross 
Abstract: Time series forecasting (TSF) is a fundamental and widely studied task, spanning methods from classical statistical approaches to modern deep learning and multimodal language modeling. Despite their effectiveness, these methods often follow a fast thinking paradigm emphasizing pattern extraction and direct value mapping, while overlooking explicit reasoning over temporal dynamics and contextual dependencies. Meanwhile, emerging slow-thinking LLMs (e.g., ChatGPT-o1, DeepSeek-R1) have demonstrated impressive multi-step reasoning capabilities across diverse domains, suggesting a new opportunity for reframing TSF as a structured reasoning task. This motivates a key question: can slow-thinking LLMs effectively reason over temporal patterns to support time series forecasting, even in zero-shot manner? To investigate this, in this paper, we propose TimeReasoner, an extensive empirical study that formulates TSF as a conditional reasoning task. We design a series of prompting strategies to elicit inference-time reasoning from pretrained slow-thinking LLMs and evaluate their performance across diverse TSF benchmarks. Our findings reveal that slow-thinking LLMs exhibit non-trivial zero-shot forecasting capabilities, especially in capturing high-level trends and contextual shifts. While preliminary, our study surfaces important insights into the reasoning behaviors of LLMs in temporal domains highlighting both their potential and limitations. We hope this work catalyzes further research into reasoning-based forecasting paradigms and paves the way toward more interpretable and generalizable TSF frameworks.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stress-testing Machine Generated Text Detection: Shifting Language Models Writing Style to Fool Detectors</title>
<link>https://arxiv.org/abs/2505.24523</link>
<guid>https://arxiv.org/abs/2505.24523</guid>
<content:encoded><![CDATA[
arXiv:2505.24523v1 Announce Type: cross 
Abstract: Recent advancements in Generative AI and Large Language Models (LLMs) have enabled the creation of highly realistic synthetic content, raising concerns about the potential for malicious use, such as misinformation and manipulation. Moreover, detecting Machine-Generated Text (MGT) remains challenging due to the lack of robust benchmarks that assess generalization to real-world scenarios. In this work, we present a pipeline to test the resilience of state-of-the-art MGT detectors (e.g., Mage, Radar, LLM-DetectAIve) to linguistically informed adversarial attacks. To challenge the detectors, we fine-tune language models using Direct Preference Optimization (DPO) to shift the MGT style toward human-written text (HWT). This exploits the detectors' reliance on stylistic clues, making new generations more challenging to detect. Additionally, we analyze the linguistic shifts induced by the alignment and which features are used by detectors to detect MGT texts. Our results show that detectors can be easily fooled with relatively few examples, resulting in a significant drop in detection performance. This highlights the importance of improving detection methods and making them robust to unseen in-domain texts.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Directional Non-Commutative Monoidal Structures with Interchange Law via Commutative Generators</title>
<link>https://arxiv.org/abs/2505.24533</link>
<guid>https://arxiv.org/abs/2505.24533</guid>
<content:encoded><![CDATA[
arXiv:2505.24533v1 Announce Type: cross 
Abstract: We introduce a novel framework consisting of a class of algebraic structures that generalize one-dimensional monoidal systems into higher dimensions by defining per-axis composition operators subject to non-commutativity and a global interchange law. These structures, defined recursively from a base case of vector-matrix pairs, model directional composition in multiple dimensions while preserving structural coherence through commutative linear operators.
  We show that the framework that unifies several well-known linear transforms in signal processing and data analysis. In this framework, data indices are embedded into a composite structure that decomposes into simpler components. We show that classic transforms such as the Discrete Fourier Transform (DFT), the Walsh transform, and the Hadamard transform are special cases of our algebraic structure. The framework provides a systematic way to derive these transforms by appropriately choosing vector and matrix pairs. By subsuming classical transforms within a common structure, the framework also enables the development of learnable transformations tailored to specific data modalities and tasks.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Linear Steering: Unified Multi-Attribute Control for Language Models</title>
<link>https://arxiv.org/abs/2505.24535</link>
<guid>https://arxiv.org/abs/2505.24535</guid>
<content:encoded><![CDATA[
arXiv:2505.24535v1 Announce Type: cross 
Abstract: Controlling multiple behavioral attributes in large language models (LLMs) at inference time is a challenging problem due to interference between attributes and the limitations of linear steering methods, which assume additive behavior in activation space and require per-attribute tuning. We introduce K-Steering, a unified and flexible approach that trains a single non-linear multi-label classifier on hidden activations and computes intervention directions via gradients at inference time. This avoids linearity assumptions, removes the need for storing and tuning separate attribute vectors, and allows dynamic composition of behaviors without retraining. To evaluate our method, we propose two new benchmarks, ToneBank and DebateMix, targeting compositional behavioral control. Empirical results across 3 model families, validated by both activation-based classifiers and LLM-based judges, demonstrate that K-Steering outperforms strong baselines in accurately steering multiple behaviors.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CHIP: Chameleon Hash-based Irreversible Passport for Robust Deep Model Ownership Verification and Active Usage Control</title>
<link>https://arxiv.org/abs/2505.24536</link>
<guid>https://arxiv.org/abs/2505.24536</guid>
<content:encoded><![CDATA[
arXiv:2505.24536v1 Announce Type: cross 
Abstract: The pervasion of large-scale Deep Neural Networks (DNNs) and their enormous training costs make their intellectual property (IP) protection of paramount importance. Recently introduced passport-based methods attempt to steer DNN watermarking towards strengthening ownership verification against ambiguity attacks by modulating the affine parameters of normalization layers. Unfortunately, neither watermarking nor passport-based methods provide a holistic protection with robust ownership proof, high fidelity, active usage authorization and user traceability for offline access distributed models and multi-user Machine-Learning as a Service (MLaaS) cloud model. In this paper, we propose a Chameleon Hash-based Irreversible Passport (CHIP) protection framework that utilizes the cryptographic chameleon hash function to achieve all these goals. The collision-resistant property of chameleon hash allows for strong model ownership claim upon IP infringement and liable user traceability, while the trapdoor-collision property enables hashing of multiple user passports and licensee certificates to the same immutable signature to realize active usage control. Using the owner passport as an oracle, multiple user-specific triplets, each contains a passport-aware user model, a user passport, and a licensee certificate can be created for secure offline distribution. The watermarked master model can also be deployed for MLaaS with usage permission verifiable by the provision of any trapdoor-colliding user passports. CHIP is extensively evaluated on four datasets and two architectures to demonstrate its protection versatility and robustness. Our code is released at https://github.com/Dshm212/CHIP.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Localizing Persona Representations in LLMs</title>
<link>https://arxiv.org/abs/2505.24539</link>
<guid>https://arxiv.org/abs/2505.24539</guid>
<content:encoded><![CDATA[
arXiv:2505.24539v1 Announce Type: cross 
Abstract: We present a study on how and where personas -- defined by distinct sets of human characteristics, values, and beliefs -- are encoded in the representation space of large language models (LLMs). Using a range of dimension reduction and pattern recognition methods, we first identify the model layers that show the greatest divergence in encoding these representations. We then analyze the activations within a selected layer to examine how specific personas are encoded relative to others, including their shared and distinct embedding spaces. We find that, across multiple pre-trained decoder-only LLMs, the analyzed personas show large differences in representation space only within the final third of the decoder layers. We observe overlapping activations for specific ethical perspectives -- such as moral nihilism and utilitarianism -- suggesting a degree of polysemy. In contrast, political ideologies like conservatism and liberalism appear to be represented in more distinct regions. These findings help to improve our understanding of how LLMs internally represent information and can inform future efforts in refining the modulation of specific human traits in LLM outputs. Warning: This paper includes potentially offensive sample statements.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixpert: Mitigating Multimodal Learning Conflicts with Efficient Mixture-of-Vision-Experts</title>
<link>https://arxiv.org/abs/2505.24541</link>
<guid>https://arxiv.org/abs/2505.24541</guid>
<content:encoded><![CDATA[
arXiv:2505.24541v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) require a nuanced interpretation of complex image information, typically leveraging a vision encoder to perceive various visual scenarios. However, relying solely on a single vision encoder to handle diverse task domains proves difficult and inevitably leads to conflicts. Recent work enhances data perception by directly integrating multiple domain-specific vision encoders, yet this structure adds complexity and limits the potential for joint optimization. In this paper, we introduce Mixpert, an efficient mixture-of-vision-experts architecture that inherits the joint learning advantages from a single vision encoder while being restructured into a multi-expert paradigm for task-specific fine-tuning across different visual tasks. Additionally, we design a dynamic routing mechanism that allocates input images to the most suitable visual expert. Mixpert effectively alleviates domain conflicts encountered by a single vision encoder in multi-task learning with minimal additional computational cost, making it more efficient than multiple encoders. Furthermore, Mixpert integrates seamlessly into any MLLM, with experimental results demonstrating substantial performance gains across various tasks.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Attention Speculative Decoding</title>
<link>https://arxiv.org/abs/2505.24544</link>
<guid>https://arxiv.org/abs/2505.24544</guid>
<content:encoded><![CDATA[
arXiv:2505.24544v1 Announce Type: cross 
Abstract: Speculative decoding (SD) is a widely adopted approach for accelerating inference in large language models (LLMs), particularly when the draft and target models are well aligned. However, state-of-the-art SD methods typically rely on tightly coupled, self-attention-based Transformer decoders, often augmented with auxiliary pooling or fusion layers. This coupling makes them increasingly complex and harder to generalize across different models. We present Budget EAGLE (Beagle), the first, to our knowledge, cross-attention-based Transformer decoder SD model that achieves performance on par with leading self-attention SD models (EAGLE-v2) while eliminating the need for pooling or auxiliary components, simplifying the architecture, improving training efficiency, and maintaining stable memory usage during training-time simulation. To enable effective training of this novel architecture, we propose Two-Stage Block-Attention Training, a new method that achieves training stability and convergence efficiency in block-level attention scenarios. Extensive experiments across multiple LLMs and datasets show that Beagle achieves competitive inference speedups and higher training efficiency than EAGLE-v2, offering a strong alternative for architectures in speculative decoding.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CREFT: Sequential Multi-Agent LLM for Character Relation Extraction</title>
<link>https://arxiv.org/abs/2505.24553</link>
<guid>https://arxiv.org/abs/2505.24553</guid>
<content:encoded><![CDATA[
arXiv:2505.24553v1 Announce Type: cross 
Abstract: Understanding complex character relations is crucial for narrative analysis and efficient script evaluation, yet existing extraction methods often fail to handle long-form narratives with nuanced interactions. To address this challenge, we present CREFT, a novel sequential framework leveraging specialized Large Language Model (LLM) agents. First, CREFT builds a base character graph through knowledge distillation, then iteratively refines character composition, relation extraction, role identification, and group assignments. Experiments on a curated Korean drama dataset demonstrate that CREFT significantly outperforms single-agent LLM baselines in both accuracy and completeness. By systematically visualizing character networks, CREFT streamlines narrative comprehension and accelerates script review -- offering substantial benefits to the entertainment, publishing, and educational sectors.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bench4KE: Benchmarking Automated Competency Question Generation</title>
<link>https://arxiv.org/abs/2505.24554</link>
<guid>https://arxiv.org/abs/2505.24554</guid>
<content:encoded><![CDATA[
arXiv:2505.24554v1 Announce Type: cross 
Abstract: The availability of Large Language Models (LLMs) presents a unique opportunity to reinvigorate research on Knowledge Engineering (KE) automation, a trend already evident in recent efforts developing LLM-based methods and tools for the automatic generation of Competency Questions (CQs). However, the evaluation of these tools lacks standardisation. This undermines the methodological rigour and hinders the replication and comparison of results. To address this gap, we introduce Bench4KE, an extensible API-based benchmarking system for KE automation. Its first release focuses on evaluating tools that generate CQs automatically. CQs are natural language questions used by ontology engineers to define the functional requirements of an ontology. Bench4KE provides a curated gold standard consisting of CQ datasets from four real-world ontology projects. It uses a suite of similarity metrics to assess the quality of the CQs generated. We present a comparative analysis of four recent CQ generation systems, which are based on LLMs, establishing a baseline for future research. Bench4KE is also designed to accommodate additional KE automation tasks, such as SPARQL query generation, ontology testing and drafting. Code and datasets are publicly available under the Apache 2.0 license.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization</title>
<link>https://arxiv.org/abs/2505.24575</link>
<guid>https://arxiv.org/abs/2505.24575</guid>
<content:encoded><![CDATA[
arXiv:2505.24575v1 Announce Type: cross 
Abstract: Summarizing long-form narratives--such as books, movies, and TV scripts--requires capturing intricate plotlines, character interactions, and thematic coherence, a task that remains challenging for existing LLMs. We introduce NexusSum, a multi-agent LLM framework for narrative summarization that processes long-form text through a structured, sequential pipeline--without requiring fine-tuning. Our approach introduces two key innovations: (1) Dialogue-to-Description Transformation: A narrative-specific preprocessing method that standardizes character dialogue and descriptive text into a unified format, improving coherence. (2) Hierarchical Multi-LLM Summarization: A structured summarization pipeline that optimizes chunk processing and controls output length for accurate, high-quality summaries. Our method establishes a new state-of-the-art in narrative summarization, achieving up to a 30.0% improvement in BERTScore (F1) across books, movies, and TV scripts. These results demonstrate the effectiveness of multi-agent LLMs in handling long-form content, offering a scalable approach for structured summarization in diverse storytelling domains.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for Auto-Generating Chemical Process and Instrumentation Diagrams</title>
<link>https://arxiv.org/abs/2505.24584</link>
<guid>https://arxiv.org/abs/2505.24584</guid>
<content:encoded><![CDATA[
arXiv:2505.24584v1 Announce Type: cross 
Abstract: Recent advancements in generative AI have accelerated the discovery of novel chemicals and materials; however, transitioning these discoveries to industrial-scale production remains a critical bottleneck, as it requires the development of entirely new chemical manufacturing processes. Current AI methods cannot auto-generate PFDs or PIDs, despite their critical role in scaling chemical processes, while adhering to engineering constraints. We present a closed loop, physics aware framework for the automated generation of industrially viable PFDs and PIDs. The framework integrates domain specialized small scale language models (SLMs) (trained for chemical process QA tasks) with first principles simulation, leveraging three key components: (1) a hierarchical knowledge graph of process flow and instrumentation descriptions for 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes domain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Retrieval-Augmented Instruction Tuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure feasibility. To improve both runtime efficiency and model compactness, the framework incorporates advanced inference time optimizations including FlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization, and Test Time Inference Scaling and independently applies structural pruning techniques (width and depth) guided by importance heuristics to reduce model size with minimal accuracy loss. Experiments demonstrate that the framework generates simulator-validated process descriptions with high fidelity, outperforms baseline methods in correctness, and generalizes to unseen chemicals. By bridging AI-driven design with industrial-scale feasibility, this work significantly reduces R&amp;D timelines from lab discovery to plant deployment.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Flat Minima Perspective on Understanding Augmentations and Model Robustness</title>
<link>https://arxiv.org/abs/2505.24592</link>
<guid>https://arxiv.org/abs/2505.24592</guid>
<content:encoded><![CDATA[
arXiv:2505.24592v1 Announce Type: cross 
Abstract: Model robustness indicates a model's capability to generalize well on unforeseen distributional shifts, including data corruption, adversarial attacks, and domain shifts. Data augmentation is one of the prevalent and effective ways to enhance robustness. Despite the great success of augmentations in different fields, a general theoretical understanding of their efficacy in improving model robustness is lacking. We offer a unified theoretical framework to clarify how augmentations can enhance model robustness through the lens of loss surface flatness and PAC generalization bound. Our work diverges from prior studies in that our analysis i) broadly encompasses much of the existing augmentation methods, and ii) is not limited to specific types of distribution shifts like adversarial attacks. We confirm our theories through simulations on the existing common corruption and adversarial robustness benchmarks based on the CIFAR and ImageNet datasets, as well as domain generalization benchmarks including PACS and OfficeHome.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding Knowledge Attribution in Mixture-of-Experts: A Framework of Basic-Refinement Collaboration and Efficiency Analysis</title>
<link>https://arxiv.org/abs/2505.24593</link>
<guid>https://arxiv.org/abs/2505.24593</guid>
<content:encoded><![CDATA[
arXiv:2505.24593v1 Announce Type: cross 
Abstract: The interpretability of Mixture-of-Experts (MoE) models, especially those with heterogeneous designs, remains underexplored. Existing attribution methods for dense models fail to capture dynamic routing-expert interactions in sparse MoE architectures. To address this issue, we propose a cross-level attribution algorithm to analyze sparse MoE architectures (Qwen 1.5-MoE, OLMoE, Mixtral-8x7B) against dense models (Qwen 1.5-7B, Llama-7B, Mixtral-7B). Results show MoE models achieve 37% higher per-layer efficiency via a "mid-activation, late-amplification" pattern: early layers screen experts, while late layers refine knowledge collaboratively. Ablation studies reveal a "basic-refinement" framework--shared experts handle general tasks (entity recognition), while routed experts specialize in domain-specific processing (geographic attributes). Semantic-driven routing is evidenced by strong correlations between attention heads and experts (r=0.68), enabling task-aware coordination. Notably, architectural depth dictates robustness: deep Qwen 1.5-MoE mitigates expert failures (e.g., 43% MRR drop in geographic tasks when blocking top-10 experts) through shared expert redundancy, whereas shallow OLMoE suffers severe degradation (76% drop). Task sensitivity further guides design: core-sensitive tasks (geography) require concentrated expertise, while distributed-tolerant tasks (object attributes) leverage broader participation. These insights advance MoE interpretability, offering principles to balance efficiency, specialization, and robustness.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Binary Cumulative Encoding meets Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.24595</link>
<guid>https://arxiv.org/abs/2505.24595</guid>
<content:encoded><![CDATA[
arXiv:2505.24595v1 Announce Type: cross 
Abstract: Recent studies in time series forecasting have explored formulating regression via classification task. By discretizing the continuous target space into bins and predicting over a fixed set of classes, these approaches benefit from stable training, robust uncertainty modeling, and compatibility with modern deep learning architectures. However, most existing methods rely on one-hot encoding that ignores the inherent ordinal structure of the underlying values. As a result, they fail to provide information about the relative distance between predicted and true values during training. In this paper, we propose to address this limitation by introducing binary cumulative encoding (BCE), that represents scalar targets into monotonic binary vectors. This encoding implicitly preserves order and magnitude information, allowing the model to learn distance-aware representations while still operating within a classification framework. We propose a convolutional neural network architecture specifically designed for BCE, incorporating residual and dilated convolutions to enable fast and expressive temporal modeling. Through extensive experiments on benchmark forecasting datasets, we show that our approach outperforms widely used methods in both point and probabilistic forecasting, while requiring fewer parameters and enabling faster training.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs with POLLUX</title>
<link>https://arxiv.org/abs/2505.24616</link>
<guid>https://arxiv.org/abs/2505.24616</guid>
<content:encoded><![CDATA[
arXiv:2505.24616v1 Announce Type: cross 
Abstract: We introduce POLLUX, a comprehensive open-source benchmark designed to evaluate the generative capabilities of large language models (LLMs) in Russian. Our main contribution is a novel evaluation methodology that enhances the interpretability of LLM assessment. For each task type, we define a set of detailed criteria and develop a scoring protocol where models evaluate responses and provide justifications for their ratings. This enables transparent, criteria-driven evaluation beyond traditional resource-consuming, side-by-side human comparisons. POLLUX includes a detailed, fine-grained taxonomy of 35 task types covering diverse generative domains such as code generation, creative writing, and practical assistant use cases, totaling 2,100 manually crafted and professionally authored prompts. Each task is categorized by difficulty (easy/medium/hard), with experts constructing the dataset entirely from scratch. We also release a family of LLM-as-a-Judge (7B and 32B) evaluators trained for nuanced assessment of generative outputs. This approach provides scalable, interpretable evaluation and annotation tools for model development, effectively replacing costly and less precise human judgments.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyperbolic Dataset Distillation</title>
<link>https://arxiv.org/abs/2505.24623</link>
<guid>https://arxiv.org/abs/2505.24623</guid>
<content:encoded><![CDATA[
arXiv:2505.24623v1 Announce Type: cross 
Abstract: To address the computational and storage challenges posed by large-scale datasets in deep learning, dataset distillation has been proposed to synthesize a compact dataset that replaces the original while maintaining comparable model performance. Unlike optimization-based approaches that require costly bi-level optimization, distribution matching (DM) methods improve efficiency by aligning the distributions of synthetic and original data, thereby eliminating nested optimization. DM achieves high computational efficiency and has emerged as a promising solution. However, existing DM methods, constrained to Euclidean space, treat data as independent and identically distributed points, overlooking complex geometric and hierarchical relationships. To overcome this limitation, we propose a novel hyperbolic dataset distillation method, termed HDD. Hyperbolic space, characterized by negative curvature and exponential volume growth with distance, naturally models hierarchical and tree-like structures. HDD embeds features extracted by a shallow network into the Lorentz hyperbolic space, where the discrepancy between synthetic and original data is measured by the hyperbolic (geodesic) distance between their centroids. By optimizing this distance, the hierarchical structure is explicitly integrated into the distillation process, guiding synthetic samples to gravitate towards the root-centric regions of the original data distribution while preserving their underlying geometric characteristics. Furthermore, we find that pruning in hyperbolic space requires only 20% of the distilled core set to retain model performance, while significantly improving training stability. Notably, HDD is seamlessly compatible with most existing DM methods, and extensive experiments on different datasets validate its effectiveness.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors</title>
<link>https://arxiv.org/abs/2505.24625</link>
<guid>https://arxiv.org/abs/2505.24625</guid>
<content:encoded><![CDATA[
arXiv:2505.24625v1 Announce Type: cross 
Abstract: Previous research has investigated the application of Multimodal Large Language Models (MLLMs) in understanding 3D scenes by interpreting them as videos. These approaches generally depend on comprehensive 3D data inputs, such as point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research, we advance this field by enhancing the capability of MLLMs to understand and reason in 3D spaces directly from video data, without the need for additional 3D input. We propose a novel and efficient method, the Video-3D Geometry Large Language Model (VG LLM). Our approach employs a 3D visual geometry encoder that extracts 3D prior information from video sequences. This information is integrated with visual tokens and fed into the MLLM. Extensive experiments have shown that our method has achieved substantial improvements in various tasks related to 3D scene understanding and spatial reasoning, all directly learned from video sources. Impressively, our 4B model, which does not rely on explicit 3D data inputs, achieves competitive results compared to existing state-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the VSI-Bench evaluations.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Hallucination Dilemma: Factuality-Aware Reinforcement Learning for Large Reasoning Models</title>
<link>https://arxiv.org/abs/2505.24630</link>
<guid>https://arxiv.org/abs/2505.24630</guid>
<content:encoded><![CDATA[
arXiv:2505.24630v1 Announce Type: cross 
Abstract: Large language models (LLMs) have significantly advanced in reasoning tasks through reinforcement learning (RL) optimization, achieving impressive capabilities across various challenging benchmarks. However, our empirical analysis reveals a critical drawback: reasoning-oriented RL fine-tuning significantly increases the prevalence of hallucinations. We theoretically analyze the RL training dynamics, identifying high-variance gradient, entropy-induced randomness, and susceptibility to spurious local optima as key factors leading to hallucinations. To address this drawback, we propose Factuality-aware Step-wise Policy Optimization (FSPO), an innovative RL fine-tuning algorithm incorporating explicit factuality verification at each reasoning step. FSPO leverages automated verification against given evidence to dynamically adjust token-level advantage values, incentivizing factual correctness throughout the reasoning process. Experiments across mathematical reasoning and hallucination benchmarks using Qwen2.5 and Llama models demonstrate that FSPO effectively reduces hallucinations while enhancing reasoning accuracy, substantially improving both reliability and performance.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cloud Optical Thickness Retrievals Using Angle Invariant Attention Based Deep Learning Models</title>
<link>https://arxiv.org/abs/2505.24638</link>
<guid>https://arxiv.org/abs/2505.24638</guid>
<content:encoded><![CDATA[
arXiv:2505.24638v1 Announce Type: cross 
Abstract: Cloud Optical Thickness (COT) is a critical cloud property influencing Earth's climate, weather, and radiation budget. Satellite radiance measurements enable global COT retrieval, but challenges like 3D cloud effects, viewing angles, and atmospheric interference must be addressed to ensure accurate estimation. Traditionally, the Independent Pixel Approximation (IPA) method, which treats individual pixels independently, has been used for COT estimation. However, IPA introduces significant bias due to its simplified assumptions. Recently, deep learning-based models have shown improved performance over IPA but lack robustness, as they are sensitive to variations in radiance intensity, distortions, and cloud shadows. These models also introduce substantial errors in COT estimation under different solar and viewing zenith angles. To address these challenges, we propose a novel angle-invariant, attention-based deep model called Cloud-Attention-Net with Angle Coding (CAAC). Our model leverages attention mechanisms and angle embeddings to account for satellite viewing geometry and 3D radiative transfer effects, enabling more accurate retrieval of COT. Additionally, our multi-angle training strategy ensures angle invariance. Through comprehensive experiments, we demonstrate that CAAC significantly outperforms existing state-of-the-art deep learning models, reducing cloud property retrieval errors by at least a factor of nine.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Text Encoders for Labor Market Analysis</title>
<link>https://arxiv.org/abs/2505.24640</link>
<guid>https://arxiv.org/abs/2505.24640</guid>
<content:encoded><![CDATA[
arXiv:2505.24640v1 Announce Type: cross 
Abstract: Labor market analysis relies on extracting insights from job advertisements, which provide valuable yet unstructured information on job titles and corresponding skill requirements. While state-of-the-art methods for skill extraction achieve strong performance, they depend on large language models (LLMs), which are computationally expensive and slow. In this paper, we propose \textbf{ConTeXT-match}, a novel contrastive learning approach with token-level attention that is well-suited for the extreme multi-label classification task of skill classification. \textbf{ConTeXT-match} significantly improves skill extraction efficiency and performance, achieving state-of-the-art results with a lightweight bi-encoder model. To support robust evaluation, we introduce \textbf{Skill-XL}, a new benchmark with exhaustive, sentence-level skill annotations that explicitly address the redundancy in the large label space. Finally, we present \textbf{JobBERT V2}, an improved job title normalization model that leverages extracted skills to produce high-quality job title representations. Experiments demonstrate that our models are efficient, accurate, and scalable, making them ideal for large-scale, real-time labor market analysis.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BIMA: Bijective Maximum Likelihood Learning Approach to Hallucination Prediction and Mitigation in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.24649</link>
<guid>https://arxiv.org/abs/2505.24649</guid>
<content:encoded><![CDATA[
arXiv:2505.24649v1 Announce Type: cross 
Abstract: Large vision-language models have become widely adopted to advance in various domains. However, developing a trustworthy system with minimal interpretable characteristics of large-scale models presents a significant challenge. One of the most prevalent terms associated with the fallacy functions caused by these systems is hallucination, where the language model generates a response that does not correspond to the visual content. To mitigate this problem, several approaches have been developed, and one prominent direction is to ameliorate the decoding process. In this paper, we propose a new Bijective Maximum Likelihood Learning (BIMA) approach to hallucination mitigation using normalizing flow theories. The proposed BIMA method can efficiently mitigate the hallucination problem in prevailing vision-language models, resulting in significant improvements. Notably, BIMA achieves the average F1 score of 85.06% on POPE benchmark and remarkably reduce CHAIRS and CHAIRI by 7.6% and 2.6%, respectively. To the best of our knowledge, this is one of the first studies that contemplates the bijection means to reduce hallucination induced by large vision-language models.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiple LLM Agents Debate for Equitable Cultural Alignment</title>
<link>https://arxiv.org/abs/2505.24671</link>
<guid>https://arxiv.org/abs/2505.24671</guid>
<content:encoded><![CDATA[
arXiv:2505.24671v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) need to adapt their predictions to diverse cultural contexts to benefit diverse communities across the world. While previous efforts have focused on single-LLM, single-turn approaches, we propose to exploit the complementary strengths of multiple LLMs to promote cultural adaptability. We introduce a Multi-Agent Debate framework, where two LLM-based agents debate over a cultural scenario and collaboratively reach a final decision. We propose two variants: one where either LLM agents exclusively debate and another where they dynamically choose between self-reflection and debate during their turns. We evaluate these approaches on 7 open-weight LLMs (and 21 LLM combinations) using the NormAd-ETI benchmark for social etiquette norms in 75 countries. Experiments show that debate improves both overall accuracy and cultural group parity over single-LLM baselines. Notably, multi-agent debate enables relatively small LLMs (7-9B) to achieve accuracies comparable to that of a much larger model (27B parameters).
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Knowledge Production Pipeline Driven by Academic Influencers</title>
<link>https://arxiv.org/abs/2505.24681</link>
<guid>https://arxiv.org/abs/2505.24681</guid>
<content:encoded><![CDATA[
arXiv:2505.24681v1 Announce Type: cross 
Abstract: Generative AI transforms knowledge production, validation, and dissemination, raising academic integrity and credibility concerns. This study examines 53 academic influencer videos that reached 5.3 million viewers to identify an emerging, structured, implementation-ready pipeline balancing originality, ethical compliance, and human-AI collaboration despite the disruptive impacts. Findings highlight generative AI's potential to automate publication workflows and democratize participation in knowledge production while challenging traditional scientific norms. Academic influencers emerge as key intermediaries in this paradigm shift, connecting bottom-up practices with institutional policies to improve adaptability. Accordingly, the study proposes a generative publication production pipeline and a policy framework for co-intelligence adaptation and reinforcing credibility-centered standards in AI-powered research. These insights support scholars, educators, and policymakers in understanding AI's transformative impact by advocating responsible and innovation-driven knowledge production. Additionally, they reveal pathways for automating best practices, optimizing scholarly workflows, and fostering creativity in academic research and publication.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Should I Share this Translation? Evaluating Quality Feedback for User Reliance on Machine Translation</title>
<link>https://arxiv.org/abs/2505.24683</link>
<guid>https://arxiv.org/abs/2505.24683</guid>
<content:encoded><![CDATA[
arXiv:2505.24683v1 Announce Type: cross 
Abstract: As people increasingly use AI systems in work and daily life, feedback mechanisms that help them use AI responsibly are urgently needed, particularly in settings where users are not equipped to assess the quality of AI predictions. We study a realistic Machine Translation (MT) scenario where monolingual users decide whether to share an MT output, first without and then with quality feedback. We compare four types of quality feedback: explicit feedback that directly give users an assessment of translation quality using 1) error highlights and 2) LLM explanations, and implicit feedback that helps users compare MT inputs and outputs through 3) backtranslation and 4) question-answer (QA) tables. We find that all feedback types, except error highlights, significantly improve both decision accuracy and appropriate reliance. Notably, implicit feedback, especially QA tables, yields significantly greater gains than explicit feedback in terms of decision accuracy, appropriate reliance, and user perceptions, receiving the highest ratings for helpfulness and trust, and the lowest for mental burden.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling Granularity: An Implicit Inductive Bias in Factorized VAEs</title>
<link>https://arxiv.org/abs/2505.24684</link>
<guid>https://arxiv.org/abs/2505.24684</guid>
<content:encoded><![CDATA[
arXiv:2505.24684v1 Announce Type: cross 
Abstract: Despite the success in learning semantically meaningful, unsupervised disentangled representations, variational autoencoders (VAEs) and their variants face a fundamental theoretical challenge: substantial evidence indicates that unsupervised disentanglement is unattainable without implicit inductive bias, yet such bias remains elusive. In this work, we focus on exploring the implicit inductive bias that drive disentanglement in VAEs with factorization priors. By analyzing the total correlation in \b{eta}-TCVAE, we uncover a crucial implicit inductive bias called disentangling granularity, which leads to the discovery of an interesting "V"-shaped optimal Evidence Lower Bound (ELBO) trajectory within the parameter space. This finding is validated through over 100K experiments using factorized VAEs and our newly proposed model, \b{eta}-STCVAE. Notably, experimental results reveal that conventional factorized VAEs, constrained by fixed disentangling granularity, inherently tend to disentangle low-complexity feature. Whereas, appropriately tuning disentangling granularity, as enabled by \b{eta}-STCVAE, broadens the range of disentangled representations, allowing for the disentanglement of high-complexity features. Our findings unveil that disentangling granularity as an implicit inductive bias in factorized VAEs influence both disentanglement performance and the inference of the ELBO, offering fresh insights into the interpretability and inherent biases of VAEs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Domain ABSA Conversation Dataset Generation via LLMs for Real-World Evaluation and Model Comparison</title>
<link>https://arxiv.org/abs/2505.24701</link>
<guid>https://arxiv.org/abs/2505.24701</guid>
<content:encoded><![CDATA[
arXiv:2505.24701v1 Announce Type: cross 
Abstract: Aspect-Based Sentiment Analysis (ABSA) offers granular insights into opinions but often suffers from the scarcity of diverse, labeled datasets that reflect real-world conversational nuances. This paper presents an approach for generating synthetic ABSA data using Large Language Models (LLMs) to address this gap. We detail the generation process aimed at producing data with consistent topic and sentiment distributions across multiple domains using GPT-4o. The quality and utility of the generated data were evaluated by assessing the performance of three state-of-the-art LLMs (Gemini 1.5 Pro, Claude 3.5 Sonnet, and DeepSeek-R1) on topic and sentiment classification tasks. Our results demonstrate the effectiveness of the synthetic data, revealing distinct performance trade-offs among the models: DeepSeekR1 showed higher precision, Gemini 1.5 Pro and Claude 3.5 Sonnet exhibited strong recall, and Gemini 1.5 Pro offered significantly faster inference. We conclude that LLM-based synthetic data generation is a viable and flexible method for creating valuable ABSA resources, facilitating research and model evaluation without reliance on limited or inaccessible real-world labeled data.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Symmetric Losses for Robust Policy Optimization with Noisy Preferences</title>
<link>https://arxiv.org/abs/2505.24709</link>
<guid>https://arxiv.org/abs/2505.24709</guid>
<content:encoded><![CDATA[
arXiv:2505.24709v1 Announce Type: cross 
Abstract: Optimizing policies based on human preferences is key to aligning language models with human intent. This work focuses on reward modeling, a core component in reinforcement learning from human feedback (RLHF), and offline preference optimization, such as direct preference optimization. Conventional approaches typically assume accurate annotations. However, real-world preference data often contains noise due to human errors or biases. We propose a principled framework for robust policy optimization under noisy preferences, viewing reward modeling as a classification problem. This allows us to leverage symmetric losses, known for their robustness to label noise in classification, leading to our Symmetric Preference Optimization (SymPO) method. We prove that symmetric losses enable successful policy optimization even under noisy labels, as the resulting reward remains rank-preserving -- a property sufficient for policy improvement. Experiments on synthetic and real-world tasks demonstrate the effectiveness of SymPO.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal-aware Large Language Models: Enhancing Decision-Making Through Learning, Adapting and Acting</title>
<link>https://arxiv.org/abs/2505.24710</link>
<guid>https://arxiv.org/abs/2505.24710</guid>
<content:encoded><![CDATA[
arXiv:2505.24710v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown great potential in decision-making due to the vast amount of knowledge stored within the models. However, these pre-trained models are prone to lack reasoning abilities and are difficult to adapt to new environments, further hindering their application to complex real-world tasks. To address these challenges, inspired by the human cognitive process, we propose Causal-aware LLMs, which integrate the structural causal model (SCM) into the decision-making process to model, update, and utilize structured knowledge of the environment in a ``learning-adapting-acting" paradigm. Specifically, in the learning stage, we first utilize an LLM to extract the environment-specific causal entities and their causal relations to initialize a structured causal model of the environment. Subsequently,in the adapting stage, we update the structured causal model through external feedback about the environment, via an idea of causal intervention. Finally, in the acting stage, Causal-aware LLMs exploit structured causal knowledge for more efficient policy-making through the reinforcement learning agent. The above processes are performed iteratively to learn causal knowledge, ultimately enabling the causal-aware LLMs to achieve a more accurate understanding of the environment and make more efficient decisions. Experimental results across 22 diverse tasks within the open-world game ``Crafter" validate the effectiveness of our proposed method.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoRet: Improved Retriever for Code Editing</title>
<link>https://arxiv.org/abs/2505.24715</link>
<guid>https://arxiv.org/abs/2505.24715</guid>
<content:encoded><![CDATA[
arXiv:2505.24715v1 Announce Type: cross 
Abstract: In this paper, we introduce CoRet, a dense retrieval model designed for code-editing tasks that integrates code semantics, repository structure, and call graph dependencies. The model focuses on retrieving relevant portions of a code repository based on natural language queries such as requests to implement new features or fix bugs. These retrieved code chunks can then be presented to a user or to a second code-editing model or agent. To train CoRet, we propose a loss function explicitly designed for repository-level retrieval. On SWE-bench and Long Code Arena's bug localisation datasets, we show that our model substantially improves retrieval recall by at least 15 percentage points over existing models, and ablate the design choices to show their importance in achieving these results.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Scalable Schema Mapping using Large Language Models</title>
<link>https://arxiv.org/abs/2505.24716</link>
<guid>https://arxiv.org/abs/2505.24716</guid>
<content:encoded><![CDATA[
arXiv:2505.24716v1 Announce Type: cross 
Abstract: The growing need to integrate information from a large number of diverse sources poses significant scalability challenges for data integration systems. These systems often rely on manually written schema mappings, which are complex, source-specific, and costly to maintain as sources evolve. While recent advances suggest that large language models (LLMs) can assist in automating schema matching by leveraging both structural and natural language cues, key challenges remain. In this paper, we identify three core issues with using LLMs for schema mapping: (1) inconsistent outputs due to sensitivity to input phrasing and structure, which we propose methods to address through sampling and aggregation techniques; (2) the need for more expressive mappings (e.g., GLaV), which strain the limited context windows of LLMs; and (3) the computational cost of repeated LLM calls, which we propose to mitigate through strategies like data type prefiltering.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts</title>
<link>https://arxiv.org/abs/2505.24722</link>
<guid>https://arxiv.org/abs/2505.24722</guid>
<content:encoded><![CDATA[
arXiv:2505.24722v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown great success in text modeling tasks across domains. However, natural language exhibits inherent semantic hierarchies and nuanced geometric structure, which current LLMs do not capture completely owing to their reliance on Euclidean operations. Recent studies have also shown that not respecting the geometry of token embeddings leads to training instabilities and degradation of generative capabilities. These findings suggest that shifting to non-Euclidean geometries can better align language models with the underlying geometry of text. We thus propose to operate fully in Hyperbolic space, known for its expansive, scale-free, and low-distortion properties. We thus introduce HELM, a family of HypErbolic Large Language Models, offering a geometric rethinking of the Transformer-based LLM that addresses the representational inflexibility, missing set of necessary operations, and poor scalability of existing hyperbolic LMs. We additionally introduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert operates in a distinct curvature space to encode more fine-grained geometric structure from text, as well as a dense model, HELM-D. For HELM-MICE, we further develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient, reduced-KV-cache training and inference. For both models, we develop essential hyperbolic equivalents of rotary positional encodings and RMS normalization. We are the first to train fully hyperbolic LLMs at billion-parameter scale, and evaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM problem-solving, general knowledge, and commonsense reasoning. Our results show consistent gains from our HELM architectures -- up to 4% -- over popular Euclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy and enhanced reasoning afforded by hyperbolic geometry in large-scale LM pretraining.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Reinvent the Wheel: Efficient Instruction-Following Text Embedding based on Guided Space Transformation</title>
<link>https://arxiv.org/abs/2505.24754</link>
<guid>https://arxiv.org/abs/2505.24754</guid>
<content:encoded><![CDATA[
arXiv:2505.24754v1 Announce Type: cross 
Abstract: In this work, we investigate an important task named instruction-following text embedding, which generates dynamic text embeddings that adapt to user instructions, highlighting specific attributes of text. Despite recent advancements, existing approaches suffer from significant computational overhead, as they require re-encoding the entire corpus for each new instruction. To address this challenge, we propose GSTransform, a novel instruction-following text embedding framework based on Guided Space Transformation. Our key observation is that instruction-relevant information is inherently encoded in generic embeddings but remains underutilized. Instead of repeatedly encoding the corpus for each instruction, GSTransform is a lightweight transformation mechanism that adapts pre-computed embeddings in real time to align with user instructions, guided by a small amount of text data with instruction-focused label annotation. We conduct extensive experiments on three instruction-awareness downstream tasks across nine real-world datasets, demonstrating that GSTransform improves instruction-following text embedding quality over state-of-the-art methods while achieving dramatic speedups of 6~300x in real-time processing on large-scale datasets. The source code is available at https://github.com/YingchaojieFeng/GSTransform.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Evolutionary Cell Type Matching via Entropy-Minimized Optimal Transport</title>
<link>https://arxiv.org/abs/2505.24759</link>
<guid>https://arxiv.org/abs/2505.24759</guid>
<content:encoded><![CDATA[
arXiv:2505.24759v1 Announce Type: cross 
Abstract: Identifying evolutionary correspondences between cell types across species is a fundamental challenge in comparative genomics and evolutionary biology. Existing approaches often rely on either reference-based matching, which imposes asymmetry by designating one species as the reference, or projection-based matching, which may increase computational complexity and obscure biological interpretability at the cell-type level. Here, we present OT-MESH, an unsupervised computational framework leveraging entropy-regularized optimal transport (OT) to systematically determine cross-species cell type homologies. Our method uniquely integrates the Minimize Entropy of Sinkhorn (MESH) technique to refine the OT plan. It begins by selecting genes with high Signal-to-Noise Ratio (SNR) to capture the most informative features, from which a cost matrix is constructed using cosine distances between cell-type centroids. Importantly, the MESH procedure iteratively refines the cost matrix, leading to a transport plan with significantly enhanced sparsity and interpretability of the resulting correspondence matrices. Applied to retinal bipolar cells (BCs) and retinal ganglion cells (RGCs) from mouse and macaque, OT-MESH accurately recovers known evolutionary relationships and uncovers novel correspondences, one of which was independently validated experimentally. Thus, our framework offers a principled, scalable, symmetric, and interpretable solution for evolutionary cell type mapping, facilitating deeper insights into cellular specialization and conservation across species.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REASONING GYM: Reasoning Environments for Reinforcement Learning with Verifiable Rewards</title>
<link>https://arxiv.org/abs/2505.24760</link>
<guid>https://arxiv.org/abs/2505.24760</guid>
<content:encoded><![CDATA[
arXiv:2505.24760v1 Announce Type: cross 
Abstract: We introduce Reasoning Gym (RG), a library of reasoning environments for reinforcement learning with verifiable rewards. It provides over 100 data generators and verifiers spanning multiple domains including algebra, arithmetic, computation, cognition, geometry, graph theory, logic, and various common games. Its key innovation is the ability to generate virtually infinite training data with adjustable complexity, unlike most previous reasoning datasets, which are typically fixed. This procedural generation approach allows for continuous evaluation across varying difficulty levels. Our experimental results demonstrate the efficacy of RG in both evaluating and reinforcement learning of reasoning models.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supervised Quantum Machine Learning: A Future Outlook from Qubits to Enterprise Applications</title>
<link>https://arxiv.org/abs/2505.24765</link>
<guid>https://arxiv.org/abs/2505.24765</guid>
<content:encoded><![CDATA[
arXiv:2505.24765v1 Announce Type: cross 
Abstract: Supervised Quantum Machine Learning (QML) represents an intersection of quantum computing and classical machine learning, aiming to use quantum resources to support model training and inference. This paper reviews recent developments in supervised QML, focusing on methods such as variational quantum circuits, quantum neural networks, and quantum kernel methods, along with hybrid quantum-classical workflows. We examine recent experimental studies that show partial indications of quantum advantage and describe current limitations including noise, barren plateaus, scalability issues, and the lack of formal proofs of performance improvement over classical methods. The main contribution is a ten-year outlook (2025-2035) that outlines possible developments in supervised QML, including a roadmap describing conditions under which QML may be used in applied research and enterprise systems over the next decade.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A survey of using EHR as real-world evidence for discovering and validating new drug indications</title>
<link>https://arxiv.org/abs/2505.24767</link>
<guid>https://arxiv.org/abs/2505.24767</guid>
<content:encoded><![CDATA[
arXiv:2505.24767v1 Announce Type: cross 
Abstract: Electronic Health Records (EHRs) have been increasingly used as real-world evidence (RWE) to support the discovery and validation of new drug indications. This paper surveys current approaches to EHR-based drug repurposing, covering data sources, processing methodologies, and representation techniques. It discusses study designs and statistical frameworks for evaluating drug efficacy. Key challenges in validation are discussed, with emphasis on the role of large language models (LLMs) and target trial emulation. By synthesizing recent developments and methodological advances, this work provides a foundational resource for researchers aiming to translate real-world data into actionable drug-repurposing evidence.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiG-Net: Enhancing Quality of Life through Hyper-Range Dynamic Gesture Recognition in Assistive Robotics</title>
<link>https://arxiv.org/abs/2505.24786</link>
<guid>https://arxiv.org/abs/2505.24786</guid>
<content:encoded><![CDATA[
arXiv:2505.24786v1 Announce Type: cross 
Abstract: Dynamic hand gestures play a pivotal role in assistive human-robot interaction (HRI), facilitating intuitive, non-verbal communication, particularly for individuals with mobility constraints or those operating robots remotely. Current gesture recognition methods are mostly limited to short-range interactions, reducing their utility in scenarios demanding robust assistive communication from afar. In this paper, we introduce a novel approach designed specifically for assistive robotics, enabling dynamic gesture recognition at extended distances of up to 30 meters, thereby significantly improving accessibility and quality of life. Our proposed Distance-aware Gesture Network (DiG-Net) effectively combines Depth-Conditioned Deformable Alignment (DADA) blocks with Spatio-Temporal Graph modules, enabling robust processing and classification of gesture sequences captured under challenging conditions, including significant physical attenuation, reduced resolution, and dynamic gesture variations commonly experienced in real-world assistive environments. We further introduce the Radiometric Spatio-Temporal Depth Attenuation Loss (RSTDAL), shown to enhance learning and strengthen model robustness across varying distances. Our model demonstrates significant performance improvement over state-of-the-art gesture recognition frameworks, achieving a recognition accuracy of 97.3% on a diverse dataset with challenging hyper-range gestures. By effectively interpreting gestures from considerable distances, DiG-Net significantly enhances the usability of assistive robots in home healthcare, industrial safety, and remote assistance scenarios, enabling seamless and intuitive interactions for users regardless of physical limitations
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Drop Dropout on Single-Epoch Language Model Pretraining</title>
<link>https://arxiv.org/abs/2505.24788</link>
<guid>https://arxiv.org/abs/2505.24788</guid>
<content:encoded><![CDATA[
arXiv:2505.24788v1 Announce Type: cross 
Abstract: Originally, dropout was seen as a breakthrough regularization technique that reduced overfitting and improved performance in almost all applications of deep learning by reducing overfitting. Yet, single-epoch pretraining tasks common to modern LLMs yield minimal overfitting, leading to dropout not being used for large LLMs. Nevertheless, no thorough empirical investigation has been done on the role of dropout in LM pretraining. Through experiments in single-epoch pretraining of both masked (BERT) and autoregressive (Pythia 160M and 1.4B) LMs with varying levels of dropout, we find that downstream performance in language modeling, morpho-syntax (BLiMP), question answering (SQuAD), and natural-language inference (MNLI) improves when dropout is not applied during pretraining. We additionally find that the recently-introduced "early dropout" also degrades performance over applying no dropout at all. We further investigate the models' editability, and find that models trained without dropout are more successful in gradient-based model editing (MEND) and equivalent in representation-based model editing (ReFT). Therefore, we advocate to drop dropout during single-epoch pretraining.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inference Acceleration of Autoregressive Normalizing Flows by Selective Jacobi Decoding</title>
<link>https://arxiv.org/abs/2505.24791</link>
<guid>https://arxiv.org/abs/2505.24791</guid>
<content:encoded><![CDATA[
arXiv:2505.24791v1 Announce Type: cross 
Abstract: Normalizing flows are promising generative models with advantages such as theoretical rigor, analytical log-likelihood computation, and end-to-end training. However, the architectural constraints to ensure invertibility and tractable Jacobian computation limit their expressive power and practical usability. Recent advancements utilize autoregressive modeling, significantly enhancing expressive power and generation quality. However, such sequential modeling inherently restricts parallel computation during inference, leading to slow generation that impedes practical deployment. In this paper, we first identify that strict sequential dependency in inference is unnecessary to generate high-quality samples. We observe that patches in sequential modeling can also be approximated without strictly conditioning on all preceding patches. Moreover, the models tend to exhibit low dependency redundancy in the initial layer and higher redundancy in subsequent layers. Leveraging these observations, we propose a selective Jacobi decoding (SeJD) strategy that accelerates autoregressive inference through parallel iterative optimization. Theoretical analyses demonstrate the method's superlinear convergence rate and guarantee that the number of iterations required is no greater than the original sequential approach. Empirical evaluations across multiple datasets validate the generality and effectiveness of our acceleration technique. Experiments demonstrate substantial speed improvements up to 4.7 times faster inference while keeping the generation quality and fidelity.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RealDrive: Retrieval-Augmented Driving with Diffusion Models</title>
<link>https://arxiv.org/abs/2505.24808</link>
<guid>https://arxiv.org/abs/2505.24808</guid>
<content:encoded><![CDATA[
arXiv:2505.24808v1 Announce Type: cross 
Abstract: Learning-based planners generate natural human-like driving behaviors by learning to reason about nuanced interactions from data, overcoming the rigid behaviors that arise from rule-based planners. Nonetheless, data-driven approaches often struggle with rare, safety-critical scenarios and offer limited controllability over the generated trajectories. To address these challenges, we propose RealDrive, a Retrieval-Augmented Generation (RAG) framework that initializes a diffusion-based planning policy by retrieving the most relevant expert demonstrations from the training dataset. By interpolating between current observations and retrieved examples through a denoising process, our approach enables fine-grained control and safe behavior across diverse scenarios, leveraging the strong prior provided by the retrieved scenario. Another key insight we produce is that a task-relevant retrieval model trained with planning-based objectives results in superior planning performance in our framework compared to a task-agnostic retriever. Experimental results demonstrate improved generalization to long-tail events and enhanced trajectory diversity compared to standard learning-based planners -- we observe a 40% reduction in collision rate on the Waymo Open Motion dataset with RAG.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PhySense: Principle-Based Physics Reasoning Benchmarking for Large Language Models</title>
<link>https://arxiv.org/abs/2505.24823</link>
<guid>https://arxiv.org/abs/2505.24823</guid>
<content:encoded><![CDATA[
arXiv:2505.24823v1 Announce Type: cross 
Abstract: Large language models (LLMs) have rapidly advanced and are increasingly capable of tackling complex scientific problems, including those in physics. Despite this progress, current LLMs often fail to emulate the concise, principle-based reasoning characteristic of human experts, instead generating lengthy and opaque solutions. This discrepancy highlights a crucial gap in their ability to apply core physical principles for efficient and interpretable problem solving. To systematically investigate this limitation, we introduce PhySense, a novel principle-based physics reasoning benchmark designed to be easily solvable by experts using guiding principles, yet deceptively difficult for LLMs without principle-first reasoning. Our evaluation across multiple state-of-the-art LLMs and prompt types reveals a consistent failure to align with expert-like reasoning paths, providing insights for developing AI systems with efficient, robust and interpretable principle-based scientific reasoning.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Reliability and Explainability of Medical Question Answering through Atomic Fact Checking in Retrieval-Augmented LLMs</title>
<link>https://arxiv.org/abs/2505.24830</link>
<guid>https://arxiv.org/abs/2505.24830</guid>
<content:encoded><![CDATA[
arXiv:2505.24830v1 Announce Type: cross 
Abstract: Large language models (LLMs) exhibit extensive medical knowledge but are prone to hallucinations and inaccurate citations, which pose a challenge to their clinical adoption and regulatory compliance. Current methods, such as Retrieval Augmented Generation, partially address these issues by grounding answers in source documents, but hallucinations and low fact-level explainability persist. In this work, we introduce a novel atomic fact-checking framework designed to enhance the reliability and explainability of LLMs used in medical long-form question answering. This method decomposes LLM-generated responses into discrete, verifiable units called atomic facts, each of which is independently verified against an authoritative knowledge base of medical guidelines. This approach enables targeted correction of errors and direct tracing to source literature, thereby improving the factual accuracy and explainability of medical Q&amp;A. Extensive evaluation using multi-reader assessments by medical experts and an automated open Q&amp;A benchmark demonstrated significant improvements in factual accuracy and explainability. Our framework achieved up to a 40% overall answer improvement and a 50% hallucination detection rate. The ability to trace each atomic fact back to the most relevant chunks from the database provides a granular, transparent explanation of the generated responses, addressing a major gap in current medical AI applications. This work represents a crucial step towards more trustworthy and reliable clinical applications of LLMs, addressing key prerequisites for clinical application and fostering greater confidence in AI-assisted healthcare.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoCAD: A Large-Scale Video Dataset for Learning UI Interactions and 3D Reasoning from CAD Software</title>
<link>https://arxiv.org/abs/2505.24838</link>
<guid>https://arxiv.org/abs/2505.24838</guid>
<content:encoded><![CDATA[
arXiv:2505.24838v1 Announce Type: cross 
Abstract: Computer-Aided Design (CAD) is a time-consuming and complex process, requiring precise, long-horizon user interactions with intricate 3D interfaces. While recent advances in AI-driven user interface (UI) agents show promise, most existing datasets and methods focus on short, low-complexity tasks in mobile or web applications, failing to capture the demands of professional engineering tools. In this work, we introduce VideoCAD, the first attempt at engineering UI interaction learning for precision tasks. Specifically, VideoCAD is a large-scale synthetic dataset consisting of over 41K annotated video recordings of CAD operations, generated using an automated framework for collecting high-fidelity UI action data from human-made CAD designs. Compared to existing datasets, VideoCAD offers an order of magnitude higher complexity in UI interaction learning for real-world engineering tasks, having up to a 20x longer time horizon than other datasets. We show two important downstream applications of VideoCAD: learning UI interactions from professional precision 3D CAD tools and a visual question-answering (VQA) benchmark designed to evaluate multimodal large language models' (LLM) spatial reasoning and video understanding abilities. To learn the UI interactions, we propose VideoCADFormer - a state-of-the-art model in learning CAD interactions directly from video, which outperforms multiple behavior cloning baselines. Both VideoCADFormer and the VQA benchmark derived from VideoCAD reveal key challenges in the current state of video-based UI understanding, including the need for precise action grounding, multi-modal and spatial reasoning, and long-horizon dependencies.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision LLMs Are Bad at Hierarchical Visual Understanding, and LLMs Are the Bottleneck</title>
<link>https://arxiv.org/abs/2505.24840</link>
<guid>https://arxiv.org/abs/2505.24840</guid>
<content:encoded><![CDATA[
arXiv:2505.24840v1 Announce Type: cross 
Abstract: This paper reveals that many state-of-the-art large language models (LLMs) lack hierarchical knowledge about our visual world, unaware of even well-established biology taxonomies. This shortcoming makes LLMs a bottleneck for vision LLMs' hierarchical visual understanding (e.g., recognizing Anemone Fish but not Vertebrate). We arrive at these findings using about one million four-choice visual question answering (VQA) tasks constructed from six taxonomies and four image datasets. Interestingly, finetuning a vision LLM using our VQA tasks reaffirms LLMs' bottleneck effect to some extent because the VQA tasks improve the LLM's hierarchical consistency more than the vision LLM's. We conjecture that one cannot make vision LLMs understand visual concepts fully hierarchical until LLMs possess corresponding taxonomy knowledge.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.24850</link>
<guid>https://arxiv.org/abs/2505.24850</guid>
<content:encoded><![CDATA[
arXiv:2505.24850v1 Announce Type: cross 
Abstract: Recent advances in model distillation demonstrate that data from advanced reasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer complex reasoning abilities to smaller, efficient student models. However, standard practices employ rejection sampling, discarding incorrect reasoning examples -- valuable, yet often underutilized data. This paper addresses the critical question: How can both positive and negative distilled reasoning traces be effectively leveraged to maximize LLM reasoning performance in an offline setting? To this end, We propose Reinforcement Distillation (REDI), a two-stage framework. Stage 1 learns from positive traces via Supervised Fine-Tuning (SFT). Stage 2 further refines the model using both positive and negative traces through our proposed REDI objective. This novel objective is a simple, reference-free loss function that outperforms established methods like DPO and SimPO in this distillation context. Our empirical evaluations demonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT combined with DPO/SimPO on mathematical reasoning tasks. Notably, the Qwen-REDI-1.5B model, post-trained on just 131k positive and negative examples from the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1). Its performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a model post-trained on 800k proprietary data) across various mathematical reasoning benchmarks, establishing a new state-of-the-art for 1.5B models post-trained offline with openly available data.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DexMachina: Functional Retargeting for Bimanual Dexterous Manipulation</title>
<link>https://arxiv.org/abs/2505.24853</link>
<guid>https://arxiv.org/abs/2505.24853</guid>
<content:encoded><![CDATA[
arXiv:2505.24853v1 Announce Type: cross 
Abstract: We study the problem of functional retargeting: learning dexterous manipulation policies to track object states from human hand-object demonstrations. We focus on long-horizon, bimanual tasks with articulated objects, which is challenging due to large action space, spatiotemporal discontinuities, and embodiment gap between human and robot hands. We propose DexMachina, a novel curriculum-based algorithm: the key idea is to use virtual object controllers with decaying strength: an object is first driven automatically towards its target states, such that the policy can gradually learn to take over under motion and contact guidance. We release a simulation benchmark with a diverse set of tasks and dexterous hands, and show that DexMachina significantly outperforms baseline methods. Our algorithm and benchmark enable a functional comparison for hardware designs, and we present key findings informed by quantitative and qualitative results. With the recent surge in dexterous hand development, we hope this work will provide a useful platform for identifying desirable hardware capabilities and lower the barrier for contributing to future research. Videos and more at https://project-dexmachina.github.io/
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models</title>
<link>https://arxiv.org/abs/2505.24864</link>
<guid>https://arxiv.org/abs/2505.24864</guid>
<content:encoded><![CDATA[
arXiv:2505.24864v1 Announce Type: cross 
Abstract: Recent advances in reasoning-centric language models have highlighted reinforcement learning (RL) as a promising method for aligning models with verifiable rewards. However, it remains contentious whether RL truly expands a model's reasoning capabilities or merely amplifies high-reward outputs already latent in the base model's distribution, and whether continually scaling up RL compute reliably leads to improved reasoning performance. In this work, we challenge prevailing assumptions by demonstrating that prolonged RL (ProRL) training can uncover novel reasoning strategies that are inaccessible to base models, even under extensive sampling. We introduce ProRL, a novel training methodology that incorporates KL divergence control, reference policy resetting, and a diverse suite of tasks. Our empirical analysis reveals that RL-trained models consistently outperform base models across a wide range of pass@k evaluations, including scenarios where base models fail entirely regardless of the number of attempts. We further show that reasoning boundary improvements correlates strongly with task competence of base model and training duration, suggesting that RL can explore and populate new regions of solution space over time. These findings offer new insights into the conditions under which RL meaningfully expands reasoning boundaries in language models and establish a foundation for future work on long-horizon RL for reasoning. We release model weights to support further research: https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time Blindness: Why Video-Language Models Can't See What Humans Can?</title>
<link>https://arxiv.org/abs/2505.24867</link>
<guid>https://arxiv.org/abs/2505.24867</guid>
<content:encoded><![CDATA[
arXiv:2505.24867v1 Announce Type: cross 
Abstract: Recent advances in vision-language models (VLMs) have made impressive strides in understanding spatio-temporal relationships in videos. However, when spatial information is obscured, these models struggle to capture purely temporal patterns. We introduce $\textbf{SpookyBench}$, a benchmark where information is encoded solely in temporal sequences of noise-like frames, mirroring natural phenomena from biological signaling to covert communication. Interestingly, while humans can recognize shapes, text, and patterns in these sequences with over 98% accuracy, state-of-the-art VLMs achieve 0% accuracy. This performance gap highlights a critical limitation: an over-reliance on frame-level spatial features and an inability to extract meaning from temporal cues. Furthermore, when trained in data sets with low spatial signal-to-noise ratios (SNR), temporal understanding of models degrades more rapidly than human perception, especially in tasks requiring fine-grained temporal reasoning. Overcoming this limitation will require novel architectures or training paradigms that decouple spatial dependencies from temporal processing. Our systematic analysis shows that this issue persists across model scales and architectures. We release SpookyBench to catalyze research in temporal pattern recognition and bridge the gap between human and machine video understanding. Dataset and code has been made available on our project website: https://timeblindness.github.io/.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProxyThinker: Test-Time Guidance through Small Visual Reasoners</title>
<link>https://arxiv.org/abs/2505.24872</link>
<guid>https://arxiv.org/abs/2505.24872</guid>
<content:encoded><![CDATA[
arXiv:2505.24872v1 Announce Type: cross 
Abstract: Recent advancements in reinforcement learning with verifiable rewards have pushed the boundaries of the visual reasoning capabilities in large vision-language models (LVLMs). However, training LVLMs with reinforcement fine-tuning (RFT) is computationally expensive, posing a significant challenge to scaling model size. In this work, we propose ProxyThinker, an inference-time technique that enables large models to inherit the visual reasoning capabilities from small, slow-thinking visual reasoners without any training. By subtracting the output distributions of base models from those of RFT reasoners, ProxyThinker modifies the decoding dynamics and successfully elicits the slow-thinking reasoning demonstrated by the emerged sophisticated behaviors such as self-verification and self-correction. ProxyThinker consistently boosts performance on challenging visual benchmarks on spatial, mathematical, and multi-disciplinary reasoning, enabling untuned base models to compete with the performance of their full-scale RFT counterparts. Furthermore, our implementation efficiently coordinates multiple language models with parallelism techniques and achieves up to 38 $\times$ faster inference compared to previous decoding-time methods, paving the way for the practical deployment of ProxyThinker. Code is available at https://github.com/MrZilinXiao/ProxyThinker.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why is plausibility surprisingly problematic as an XAI criterion?</title>
<link>https://arxiv.org/abs/2303.17707</link>
<guid>https://arxiv.org/abs/2303.17707</guid>
<content:encoded><![CDATA[
arXiv:2303.17707v4 Announce Type: replace 
Abstract: Explainable artificial intelligence (XAI) is motivated by the problem of making AI predictions understandable, transparent, and responsible, as AI becomes increasingly impactful in society and high-stakes domains. The evaluation and optimization criteria of XAI are gatekeepers for XAI algorithms to achieve their expected goals and should withstand rigorous inspection. To improve the scientific rigor of XAI, we conduct a critical examination of a common XAI criterion: plausibility. Plausibility assesses how convincing the AI explanation is to humans, and is usually quantified by metrics of feature localization or feature correlation. Our examination shows that plausibility is invalid to measure explainability, and human explanations are not the ground truth for XAI, because doing so ignores the necessary assumptions underpinning an explanation. Our examination further reveals the consequences of using plausibility as an XAI criterion, including increasing misleading explanations that manipulate users, deteriorating users' trust in the AI system, undermining human autonomy, being unable to achieve complementary human-AI task performance, and abandoning other possible approaches of enhancing understandability. Due to the invalidity of measurements and the unethical issues, this position paper argues that the community should stop using plausibility as a criterion for the evaluation and optimization of XAI algorithms. We also delineate new research approaches to improve XAI in trustworthiness, understandability, and utility to users, including complementary human-AI task performance.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From the Pursuit of Universal AGI Architecture to Systematic Approach to Heterogenous AGI: Addressing Alignment, Energy, &amp; AGI Grand Challenges</title>
<link>https://arxiv.org/abs/2310.15274</link>
<guid>https://arxiv.org/abs/2310.15274</guid>
<content:encoded><![CDATA[
arXiv:2310.15274v3 Announce Type: replace 
Abstract: Artificial intelligence (AI) faces a trifecta of grand challenges: the Energy Wall, the Alignment Problem and the Leap from Narrow AI to AGI. We present SAGI, a Systematic Approach to AGI that utilizes system design principles to overcome the energy wall and alignment challenges. This paper asserts that AGI can be realized through multiplicity of design specific pathways and customized through system design rather than a singular overarching architecture. AGI systems may exhibit diver architectural configurations and capabilities, contingent upon their intended use cases. Alignment, a challenge broadly recognized as AIs most formidable, is the one that depends most critically on system design and serves as its primary driving force as a foundational criterion for AGI. Capturing the complexities of human morality for alignment requires architectural support to represent the intricacies of moral decision-making and the pervasive ethical processing at every level, with performance reliability exceeding that of human moral judgment. Hence, requiring a more robust architecture towards safety and alignment goals, without replicating or resembling the human brain.
  We argue that system design (such as feedback loops, energy and performance optimization) on learning substrates (capable of learning its system architecture) is more fundamental to achieving AGI goals and guarantees, superseding classical symbolic, emergentist and hybrid approaches. Through learning of the system architecture itself, the resulting AGI is not a product of spontaneous emergence but of systematic design and deliberate engineering, with core features, including an integrated moral architecture, deeply embedded within its architecture. The approach aims to guarantee design goals such as alignment, efficiency by self-learning system architecture.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUICourse: From General Vision Language Models to Versatile GUI Agents</title>
<link>https://arxiv.org/abs/2406.11317</link>
<guid>https://arxiv.org/abs/2406.11317</guid>
<content:encoded><![CDATA[
arXiv:2406.11317v2 Announce Type: replace 
Abstract: Utilizing Graphic User Interface (GUI) for human-computer interaction is essential for accessing a wide range of digital tools. Recent advancements in Vision Language Models (VLMs) highlight the compelling potential to develop versatile agents to help humans finish GUI navigation tasks. However, current VLMs are challenged in terms of fundamental abilities (OCR and grounding) and GUI knowledge (the functions and control methods of GUI elements), preventing them from becoming practical GUI agents. To solve these challenges, we contribute GUICourse, a suite of datasets to train visual-based GUI agents from general VLMs. First, we introduce the GUIEnv dataset to strengthen the OCR and grounding capabilities of VLMs. Then, we introduce the GUIAct and GUIChat datasets to enrich their knowledge of GUI components and interactions. Experiments demonstrate that our GUI agents have better performance on common GUI tasks than their baseline VLMs. Even the small-size GUI agent (with 3.1B parameters) can still work well on single-step and multi-step GUI tasks. Finally, we analyze the different varieties in the training stage of this agent by ablation study. Our source codes and datasets are released at https://github.com/yiye3/GUICourse.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QPO: Query-dependent Prompt Optimization via Multi-Loop Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2408.10504</link>
<guid>https://arxiv.org/abs/2408.10504</guid>
<content:encoded><![CDATA[
arXiv:2408.10504v2 Announce Type: replace 
Abstract: Prompt engineering has demonstrated remarkable success in enhancing the performance of large language models (LLMs) across diverse tasks. However, most existing prompt optimization methods only focus on the task-level performance, overlooking the importance of query-preferred prompts, which leads to suboptimal performances. Additionally, these methods rely heavily on frequent interactions with LLMs to obtain feedback for guiding the optimization process, incurring substantial redundant interaction costs. In this paper, we introduce Query-dependent Prompt Optimization (QPO), which leverages multi-loop offline reinforcement learning to iteratively fine-tune a small pretrained language model to generate optimal prompts tailored to the input queries, thus significantly improving the prompting effect on the large target LLM. We derive insights from offline prompting demonstration data, which already exists in large quantities as a by-product of benchmarking diverse prompts on open-sourced tasks, thereby circumventing the expenses of online interactions. Furthermore, we continuously augment the offline dataset with the generated prompts in each loop, as the prompts from the fine-tuned model are supposed to outperform the source prompts in the original dataset. These iterative loops bootstrap the model towards generating optimal prompts. Experiments on various LLM scales and diverse NLP and math tasks demonstrate the efficacy and cost-efficiency of our method in both zero-shot and few-shot scenarios.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Molecular Graph-Text Pre-training via Fine-grained Alignment</title>
<link>https://arxiv.org/abs/2409.14106</link>
<guid>https://arxiv.org/abs/2409.14106</guid>
<content:encoded><![CDATA[
arXiv:2409.14106v5 Announce Type: replace 
Abstract: Understanding molecular structure and related knowledge is crucial for scientific research. Recent studies integrate molecular graphs with their textual descriptions to enhance molecular representation learning. However, they focus on the whole molecular graph and neglect frequently occurring subgraphs, known as motifs, which are essential for determining molecular properties. Without such fine-grained knowledge, these models struggle to generalize to unseen molecules and tasks that require motif-level insights. To bridge this gap, we propose FineMolTex, a novel Fine-grained Molecular graph-Text pre-training framework to jointly learn coarse-grained molecule-level knowledge and fine-grained motif-level knowledge. Specifically, FineMolTex consists of two pre-training tasks: a contrastive alignment task for coarse-grained matching and a masked multi-modal modeling task for fine-grained matching. In particular, the latter predicts the labels of masked motifs and words, which are selected based on their importance. By leveraging insights from both modalities, FineMolTex is able to understand the fine-grained matching between motifs and words. Finally, we conduct extensive experiments across three downstream tasks, achieving up to 230% improvement in the text-based molecule editing task. Additionally, our case studies reveal that FineMolTex successfully captures fine-grained knowledge, potentially offering valuable insights for drug discovery and catalyst design.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reflection-Bench: Evaluating Epistemic Agency in Large Language Models</title>
<link>https://arxiv.org/abs/2410.16270</link>
<guid>https://arxiv.org/abs/2410.16270</guid>
<content:encoded><![CDATA[
arXiv:2410.16270v2 Announce Type: replace 
Abstract: With large language models (LLMs) increasingly deployed as cognitive engines for AI agents, the reliability and effectiveness critically hinge on their intrinsic epistemic agency, which remains understudied. Epistemic agency, the ability to flexibly construct, adapt, and monitor beliefs about dynamic environments, represents a base-model-level capacity independent of specific tools, modules, or applications. We characterize the holistic process underlying epistemic agency, which unfolds in seven interrelated dimensions: prediction, decision-making, perception, memory, counterfactual thinking, belief updating, and meta-reflection. Correspondingly, we propose Reflection-Bench, a cognitive-psychology-inspired benchmark consisting of seven tasks with long-term relevance and minimization of data leakage. Through a comprehensive evaluation of 16 models using three prompting strategies, we identify a clear three-tier performance hierarchy and significant limitations of current LLMs, particularly in meta-reflection capabilities. While state-of-the-art LLMs demonstrate rudimentary signs of epistemic agency, our findings suggest several promising research directions, including enhancing core cognitive functions, improving cross-functional coordination, and developing adaptive processing mechanisms. Our code and data are available at https://github.com/AI45Lab/ReflectionBench.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combining Domain and Alignment Vectors to Achieve Better Knowledge-Safety Trade-offs in LLMs</title>
<link>https://arxiv.org/abs/2411.06824</link>
<guid>https://arxiv.org/abs/2411.06824</guid>
<content:encoded><![CDATA[
arXiv:2411.06824v2 Announce Type: replace 
Abstract: There is a growing interest in training domain-expert LLMs that excel in specific technical fields compared to their general-purpose instruction-tuned counterparts. However, these expert models often experience a loss in their safety abilities in the process, making them capable of generating harmful content. As a solution, we introduce an efficient and effective merging-based alignment method called \textsc{MergeAlign} that interpolates the domain and alignment vectors, creating safer domain-specific models while preserving their utility. We apply \textsc{MergeAlign} on Llama3 variants that are experts in medicine and finance, obtaining substantial alignment improvements with minimal to no degradation on domain-specific benchmarks. We study the impact of model merging through model similarity metrics and contributions of individual models being merged. We hope our findings open new research avenues and inspire more efficient development of safe expert LLMs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMXFP4: Taming Activation Outliers with Asymmetric Microscaling Floating-Point for 4-bit LLM Inference</title>
<link>https://arxiv.org/abs/2411.09909</link>
<guid>https://arxiv.org/abs/2411.09909</guid>
<content:encoded><![CDATA[
arXiv:2411.09909v2 Announce Type: replace 
Abstract: As large language models (LLMs) grow in parameter size and context length, computation precision has been reduced from 16-bit to 4-bit to improve inference efficiency. However, this reduction causes accuracy degradation due to activation outliers. Rotation-based INT4 methods address this via matrix calibration, but they introduce multi-hour overheads and leave key computations in full precision. Microscaling (MX) floating-point (FP) formats offer fine-grained representation with a shared scale, enabling fully quantized matrix multiplications through direct casting without calibration. However, existing research shows unsatisfactory empirical results for MXFP4 inference, and the robustness of MX formats remains largely unexplored. In this work, we uncover the fundamental tradeoffs of the MX format: while it effectively suppresses activation outliers, it does so at the cost of increased group-wise asymmetry. To address this, we propose AMXFP4, a 4-bit asymmetric FP format that handles both issues using asymmetric shared scales, without requiring calibration. Our custom MAC engine adds negligible hardware cost while improving accuracy: AMXFP4 outperforms MXFP4 by 3% on VQA and exceeds rotation-based methods by 1.6% on CSQA. It also surpasses recently deployed commercial MXFP4 variants. Code: https://github.com/aiha-lab/MX-QLLM
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChinaTravel: An Open-Ended Benchmark for Language Agents in Chinese Travel Planning</title>
<link>https://arxiv.org/abs/2412.13682</link>
<guid>https://arxiv.org/abs/2412.13682</guid>
<content:encoded><![CDATA[
arXiv:2412.13682v3 Announce Type: replace 
Abstract: Recent advances in LLMs, particularly in language reasoning and tool integration, have rapidly sparked the \emph{Language Agents} for real-world development. Among these, travel planning represents a prominent domain, combining complex multi-objective planning challenges with practical deployment demands. However, existing benchmarks often oversimplify real-world requirements by focusing on synthetic queries and limited constraints. We address the gap of evaluating language agents in multi-day, multi-POI travel planning scenarios with diverse and open human needs. Specifically, we introduce \emph{ChinaTravel}, the first open-ended benchmark grounded in authentic Chinese travel requirements collected from 1,154 human participants. We design a compositionally generalizable domain-specific language (DSL) for scalable evaluation, covering feasibility, constraint satisfaction, and preference comparison. Empirical studies reveal the potential of neuro-symbolic agents in travel planning, achieving a 37.0\% constraint satisfaction rate on human queries, a 10\times improvement over purely neural models. These findings highlight ChinaTravel as a pivotal milestone for advancing language agents in complex, real-world planning scenarios.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Oversight for Superhuman AI via Recursive Self-Critiquing</title>
<link>https://arxiv.org/abs/2502.04675</link>
<guid>https://arxiv.org/abs/2502.04675</guid>
<content:encoded><![CDATA[
arXiv:2502.04675v3 Announce Type: replace 
Abstract: As AI capabilities increasingly surpass human proficiency in complex tasks, current alignment techniques including SFT and RLHF face fundamental challenges in ensuring reliable oversight. These methods rely on direct human assessment and become untenable when AI outputs exceed human cognitive thresholds. In response to this challenge, we explore two hypotheses: (1) \textit{Critique of critique can be easier than critique itself}, extending the widely-accepted observation that verification is easier than generation to the critique domain, as critique itself is a specialized form of generation; (2) \textit{This difficulty relationship is recursively held}, suggesting that when direct evaluation is infeasible, performing high-order critiques (e.g., critique of critique of critique) offers a more tractable supervision pathway. We further conduct Human-AI and AI-AI experiments to investigate the potential of utilizing recursive self-critiquing for AI supervision. Our results highlight recursive critique as a promising approach for scalable AI oversight.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>You need to MIMIC to get FAME: Solving Meeting Transcript Scarcity with a Multi-Agent Conversations</title>
<link>https://arxiv.org/abs/2502.13001</link>
<guid>https://arxiv.org/abs/2502.13001</guid>
<content:encoded><![CDATA[
arXiv:2502.13001v2 Announce Type: replace 
Abstract: Meeting summarization suffers from limited high-quality data, mainly due to privacy restrictions and expensive collection processes. We address this gap with FAME, a dataset of 500 meetings in English and 300 in German produced by MIMIC, our new multi-agent meeting synthesis framework that generates meeting transcripts on a given knowledge source by defining psychologically grounded participant profiles, outlining the conversation, and orchestrating a large language model (LLM) debate. A modular post-processing step refines these outputs, mitigating potential repetitiveness and overly formal tones, ensuring coherent, credible dialogues at scale. We also propose a psychologically grounded evaluation framework assessing naturalness, social behavior authenticity, and transcript difficulties. Human assessments show that FAME approximates real-meeting spontaneity (4.5/5 in naturalness), preserves speaker-centric challenges (3/5 in spoken language), and introduces richer information-oriented difficulty (4/5 in difficulty). These findings highlight that FAME is a good and scalable proxy for real-world meeting conditions. It enables new test scenarios for meeting summarization research and other conversation-centric applications in tasks requiring conversation data or simulating social scenarios under behavioral constraints.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Relativity of Causal Knowledge</title>
<link>https://arxiv.org/abs/2503.11718</link>
<guid>https://arxiv.org/abs/2503.11718</guid>
<content:encoded><![CDATA[
arXiv:2503.11718v2 Announce Type: replace 
Abstract: Recent advances in artificial intelligence reveal the limits of purely predictive systems and call for a shift toward causal and collaborative reasoning. Drawing inspiration from the revolution of Grothendieck in mathematics, we introduce the relativity of causal knowledge, which posits structural causal models (SCMs) are inherently imperfect, subjective representations embedded within networks of relationships. By leveraging category theory, we arrange SCMs into a functor category and show that their observational and interventional probability measures naturally form convex structures. This result allows us to encode non-intervened SCMs with convex spaces of probability measures. Next, using sheaf theory, we construct the network sheaf and cosheaf of causal knowledge. These structures enable the transfer of causal knowledge across the network while incorporating interventional consistency and the perspective of the subjects, ultimately leading to the formal, mathematical definition of relative causal knowledge.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models</title>
<link>https://arxiv.org/abs/2505.10844</link>
<guid>https://arxiv.org/abs/2505.10844</guid>
<content:encoded><![CDATA[
arXiv:2505.10844v2 Announce Type: replace 
Abstract: Accuracy remains a standard metric for evaluating AI systems, but it offers limited insight into how models arrive at their solutions. In this work, we introduce a benchmark based on brainteasers written in long narrative form to probe more deeply into the types of reasoning strategies that models use. Brainteasers are well-suited for this goal because they can be solved with multiple approaches, such as a few-step solution that uses a creative insight or a longer solution that uses more brute force. We investigate large language models (LLMs) across multiple layers of reasoning, focusing not only on correctness but also on the quality and creativity of their solutions. We investigate many aspects of the reasoning process: (1) semantic parsing of the brainteasers into precise mathematical competition style formats; (2) generating solutions from these mathematical forms; (3) self-correcting solutions based on gold solutions; (4) producing step-by-step sketches of solutions; and (5) making use of hints. We find that LLMs are in many cases able to find creative, insightful solutions to brainteasers, suggesting that they capture some of the capacities needed to solve novel problems in creative ways. Nonetheless, there also remain situations where they rely on brute force despite the availability of more efficient, creative solutions, highlighting a potential direction for improvement in the reasoning abilities of LLMs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Evolving Curriculum for LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.14970</link>
<guid>https://arxiv.org/abs/2505.14970</guid>
<content:encoded><![CDATA[
arXiv:2505.14970v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) has proven effective for fine-tuning large language models (LLMs), significantly enhancing their reasoning abilities in domains such as mathematics and code generation. A crucial factor influencing RL fine-tuning success is the training curriculum: the order in which training problems are presented. While random curricula serve as common baselines, they remain suboptimal; manually designed curricula often rely heavily on heuristics, and online filtering methods can be computationally prohibitive. To address these limitations, we propose Self-Evolving Curriculum (SEC), an automatic curriculum learning method that learns a curriculum policy concurrently with the RL fine-tuning process. Our approach formulates curriculum selection as a non-stationary Multi-Armed Bandit problem, treating each problem category (e.g., difficulty level or problem type) as an individual arm. We leverage the absolute advantage from policy gradient methods as a proxy measure for immediate learning gain. At each training step, the curriculum policy selects categories to maximize this reward signal and is updated using the TD(0) method. Across three distinct reasoning domains: planning, inductive reasoning, and mathematics, our experiments demonstrate that SEC significantly improves models' reasoning capabilities, enabling better generalization to harder, out-of-distribution test problems. Additionally, our approach achieves better skill balance when fine-tuning simultaneously on multiple reasoning domains. These findings highlight SEC as a promising strategy for RL fine-tuning of LLMs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoGameBench: Can Vision-Language Models complete popular video games?</title>
<link>https://arxiv.org/abs/2505.18134</link>
<guid>https://arxiv.org/abs/2505.18134</guid>
<content:encoded><![CDATA[
arXiv:2505.18134v2 Announce Type: replace 
Abstract: Vision-language models (VLMs) have achieved strong results on coding and math benchmarks that are challenging for humans, yet their ability to perform tasks that come naturally to humans--such as perception, spatial navigation, and memory management--remains understudied. Real video games are crafted to be intuitive for humans to learn and master by leveraging innate inductive biases, making them an ideal testbed for evaluating such capabilities in VLMs. To this end, we introduce VideoGameBench, a benchmark consisting of 10 popular video games from the 1990s that VLMs directly interact with in real-time. VideoGameBench challenges models to complete entire games with access to only raw visual inputs and a high-level description of objectives and controls, a significant departure from existing setups that rely on game-specific scaffolding and auxiliary information. We keep three of the games secret to encourage solutions that generalize to unseen environments. Our experiments show that frontier vision-language models struggle to progress beyond the beginning of each game. We find inference latency to be a major limitation of frontier models in the real-time setting; therefore, we introduce VideoGameBench Lite, a setting where the game pauses while waiting for the LM's next action. The best performing model, Gemini 2.5 Pro, completes only 0.48% of VideoGameBench and 1.6% of VideoGameBench Lite. We hope that the formalization of the human skills mentioned above into this benchmark motivates progress in these research directions.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Turing Test 2.0: The General Intelligence Threshold</title>
<link>https://arxiv.org/abs/2505.19550</link>
<guid>https://arxiv.org/abs/2505.19550</guid>
<content:encoded><![CDATA[
arXiv:2505.19550v2 Announce Type: replace 
Abstract: With the rise of artificial intelligence (A.I.) and large language models like Chat-GPT, a new race for achieving artificial general intelligence (A.G.I) has started. While many speculate how and when A.I. will achieve A.G.I., there is no clear agreement on how A.G.I. can be detected in A.I. models, even when popular tools like the Turing test (and its modern variations) are used to measure their intelligence. In this work, we discuss why traditional methods like the Turing test do not suffice for measuring or detecting A.G.I. and provide a new, practical method that can be used to decide if a system (computer or any other) has reached or surpassed A.G.I. To achieve this, we make two new contributions. First, we present a clear definition for general intelligence (G.I.) and set a G.I. threshold (G.I.T.) that can be used to distinguish between systems that achieve A.G.I. and systems that do not. Second, we present a new framework on how to construct tests that can detect if a system has achieved G.I. in a simple, comprehensive, and clear-cut fail/pass way. We call this novel framework the Turing test 2.0. We then demonstrate real-life examples of applying tests that follow our Turing test 2.0 framework on modern A.I. models.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FieldWorkArena: Agentic AI Benchmark for Real Field Work Tasks</title>
<link>https://arxiv.org/abs/2505.19662</link>
<guid>https://arxiv.org/abs/2505.19662</guid>
<content:encoded><![CDATA[
arXiv:2505.19662v2 Announce Type: replace 
Abstract: This paper proposes FieldWorkArena, a benchmark for agentic AI targeting real-world field work. With the recent increase in demand for agentic AI, they are required to monitor and report safety and health incidents, as well as manufacturing-related incidents, that may occur in real-world work environments. Existing agentic AI benchmarks have been limited to evaluating web tasks and are insufficient for evaluating agents in real-world work environments, where complexity increases significantly. In this paper, we define a new action space that agentic AI should possess for real world work environment benchmarks and improve the evaluation function from previous methods to assess the performance of agentic AI in diverse real-world tasks. The dataset consists of videos captured on-site and documents actually used in factories and warehouses, and tasks were created based on interviews with on-site workers and managers. Evaluation results confirmed that performance evaluation considering the characteristics of Multimodal LLM (MLLM) such as GPT-4o is feasible. Additionally, the effectiveness and limitations of the proposed new evaluation method were identified. The complete dataset (HuggingFace) and evaluation program (GitHub) can be downloaded from the following website: https://en-documents.research.global.fujitsu.com/fieldworkarena/.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Design Fundamentals of Diffusion Models: A Survey</title>
<link>https://arxiv.org/abs/2306.04542</link>
<guid>https://arxiv.org/abs/2306.04542</guid>
<content:encoded><![CDATA[
arXiv:2306.04542v4 Announce Type: replace-cross 
Abstract: Diffusion models are learning pattern-learning systems to model and sample from data distributions with three functional components namely the forward process, the reverse process, and the sampling process. The components of diffusion models have gained significant attention with many design factors being considered in common practice. Existing reviews have primarily focused on higher-level solutions, covering less on the design fundamentals of components. This study seeks to address this gap by providing a comprehensive and coherent review of seminal designable factors within each functional component of diffusion models. This provides a finer-grained perspective of diffusion models, benefiting future studies in the analysis of individual components, the design factors for different purposes, and the implementation of diffusion models.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RL4CO: an Extensive Reinforcement Learning for Combinatorial Optimization Benchmark</title>
<link>https://arxiv.org/abs/2306.17100</link>
<guid>https://arxiv.org/abs/2306.17100</guid>
<content:encoded><![CDATA[
arXiv:2306.17100v5 Announce Type: replace-cross 
Abstract: Combinatorial optimization (CO) is fundamental to several real-world applications, from logistics and scheduling to hardware design and resource allocation. Deep reinforcement learning (RL) has recently shown significant benefits in solving CO problems, reducing reliance on domain expertise and improving computational efficiency. However, the absence of a unified benchmarking framework leads to inconsistent evaluations, limits reproducibility, and increases engineering overhead, raising barriers to adoption for new researchers. To address these challenges, we introduce RL4CO, a unified and extensive benchmark with in-depth library coverage of 27 CO problem environments and 23 state-of-the-art baselines. Built on efficient software libraries and best practices in implementation, RL4CO features modularized implementation and flexible configurations of diverse environments, policy architectures, RL algorithms, and utilities with extensive documentation. RL4CO helps researchers build on existing successes while exploring and developing their own designs, facilitating the entire research process by decoupling science from heavy engineering. We finally provide extensive benchmark studies to inspire new insights and future work. RL4CO has already attracted numerous researchers in the community and is open-sourced at https://github.com/ai4co/rl4co.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Extrapolation Expedites Alignment</title>
<link>https://arxiv.org/abs/2404.16792</link>
<guid>https://arxiv.org/abs/2404.16792</guid>
<content:encoded><![CDATA[
arXiv:2404.16792v5 Announce Type: replace-cross 
Abstract: Given the high computational cost of preference alignment training of large language models (LLMs), exploring efficient methods to reduce the training overhead remains an important and compelling research problem. Motivated by the observation that alignment training typically involves only small parameter changes without injecting new knowledge into models, we propose a straightforward method called ExPO (model extrapolation) to expedite LLMs' alignment with human preferences. Given a partially-trained model and its initial SFT checkpoint, ExPO improves the implicit optimization objective of alignment training by simply amplifying the parameter change based on a first-order approximation, without any additional training overhead. Through controlled experiments, we demonstrate that ExPO boosts a DPO model trained with only 20% steps to outperform the fully-trained one. Moreover, we show that ExPO notably improves existing open-source LLMs (ranging from 1.8B to 70B parameters) on the leading AlpacaEval 2.0 and MT-Bench benchmarks, which highlights ExPO's broader utility in efficiently enhancing LLM alignment.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffuseTrace: A Transparent and Flexible Watermarking Scheme for Latent Diffusion Model</title>
<link>https://arxiv.org/abs/2405.02696</link>
<guid>https://arxiv.org/abs/2405.02696</guid>
<content:encoded><![CDATA[
arXiv:2405.02696v2 Announce Type: replace-cross 
Abstract: Latent Diffusion Models (LDMs) enable a wide range of applications but raise ethical concerns regarding illegal utilization. Adding watermarks to generative model outputs is a vital technique employed for copyright tracking and mitigating potential risks associated with Artificial Intelligence (AI)-generated contents. However, post-processed watermarking methods are unable to withstand generative watermark attacks and there exists a trade-off between image fidelity and watermark strength. Therefore, we propose a novel technique called DiffuseTrace. DiffuseTrace does not rely on fine-tuning of the diffusion model components. The multi-bit watermark is a embedded into the image space semantically without compromising image quality. The watermark component can be utilized as a plug-in in arbitrary diffusion models. We validate through experiments the effectiveness and flexibility of DiffuseTrace. Under 8 types of image processing watermark attacks and 3 types of generative watermark attacks, DiffuseTrace maintains watermark detection rate of 99% and attribution accuracy of over 94%.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Complexity-Aware Deep Symbolic Regression with Robust Risk-Seeking Policy Gradients</title>
<link>https://arxiv.org/abs/2406.06751</link>
<guid>https://arxiv.org/abs/2406.06751</guid>
<content:encoded><![CDATA[
arXiv:2406.06751v2 Announce Type: replace-cross 
Abstract: We propose a novel deep symbolic regression approach to enhance the robustness and interpretability of data-driven mathematical expression discovery. Our work is aligned with the popular DSR framework which focuses on learning a data-specific expression generator, without relying on pretrained models or additional search or planning procedures. Despite the success of existing DSR methods, they are built on recurrent neural networks, solely guided by data fitness, and potentially meet tail barriers that can zero out the policy gradient, causing inefficient model updates. To overcome these limitations, we design a decoder-only architecture that performs attention in the frequency domain and introduce a dual-indexed position encoding to conduct layer-wise generation. Second, we propose a Bayesian information criterion (BIC)-based reward function that can automatically adjust the trade-off between expression complexity and data fitness, without the need for explicit manual tuning. Third, we develop a ranking-based weighted policy update method that eliminates the tail barriers and enhances training effectiveness. Extensive benchmarks and systematic experiments demonstrate the advantages of our approach.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ExPLoRA: Parameter-Efficient Extended Pre-Training to Adapt Vision Transformers under Domain Shifts</title>
<link>https://arxiv.org/abs/2406.10973</link>
<guid>https://arxiv.org/abs/2406.10973</guid>
<content:encoded><![CDATA[
arXiv:2406.10973v4 Announce Type: replace-cross 
Abstract: Parameter-efficient fine-tuning (PEFT) techniques such as low-rank adaptation (LoRA) can effectively adapt large pre-trained foundation models to downstream tasks using only a small fraction (0.1%-10%) of the original trainable weights. An under-explored question of PEFT is in extending the pre-training phase without supervised labels; that is, can we adapt a pre-trained foundation model to a new domain via efficient self-supervised pre-training on this domain? In this work, we introduce ExPLoRA, a highly effective technique to improve transfer learning of pre-trained vision transformers (ViTs) under domain shifts. Initializing a ViT with pre-trained weights on large, natural-image datasets such as from DinoV2 or MAE, ExPLoRA continues the unsupervised pre-training objective on a new domain, unfreezing 1-2 pre-trained ViT blocks and tuning all other layers with LoRA. We then fine-tune the resulting model only with LoRA on this new domain for supervised learning. Our experiments demonstrate state-of-the-art results on satellite imagery, even outperforming fully pre-training and fine-tuning ViTs. Using the DinoV2 training objective, we demonstrate up to 8% improvement in linear probing top-1 accuracy on downstream tasks while using <10% of the number of parameters that are used in prior fully-tuned state-of-the art approaches. Our ablation studies confirm the efficacy of our approach over other baselines such as PEFT. Code is available on the project website: https://samar-khanna.github.io/ExPLoRA/
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking Resource Barriers in Speech Emotion Recognition via Data Distillation</title>
<link>https://arxiv.org/abs/2406.15119</link>
<guid>https://arxiv.org/abs/2406.15119</guid>
<content:encoded><![CDATA[
arXiv:2406.15119v2 Announce Type: replace-cross 
Abstract: Speech emotion recognition (SER) plays a crucial role in human-computer interaction. The emergence of edge devices in the Internet of Things (IoT) presents challenges in constructing intricate deep learning models due to constraints in memory and computational resources. Moreover, emotional speech data often contains private information, raising concerns about privacy leakage during the deployment of SER models. To address these challenges, we propose a data distillation framework to facilitate efficient development of SER models in IoT applications using a synthesised, smaller, and distilled dataset. Our experiments demonstrate that the distilled dataset can be effectively utilised to train SER models with fixed initialisation, achieving performances comparable to those developed using the original full emotional speech dataset.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kolmogorov-Arnold Graph Neural Networks</title>
<link>https://arxiv.org/abs/2406.18354</link>
<guid>https://arxiv.org/abs/2406.18354</guid>
<content:encoded><![CDATA[
arXiv:2406.18354v2 Announce Type: replace-cross 
Abstract: Graph neural networks (GNNs) excel in learning from network-like data but often lack interpretability, making their application challenging in domains requiring transparent decision-making. We propose the Graph Kolmogorov-Arnold Network (GKAN), a novel GNN model leveraging spline-based activation functions on edges to enhance both accuracy and interpretability. Our experiments on five benchmark datasets demonstrate that GKAN outperforms state-of-the-art GNN models in node classification, link prediction, and graph classification tasks. In addition to the improved accuracy, GKAN's design inherently provides clear insights into the model's decision-making process, eliminating the need for post-hoc explainability techniques. This paper discusses the methodology, performance, and interpretability of GKAN, highlighting its potential for applications in domains where interpretability is crucial.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compress then Serve: Serving Thousands of LoRA Adapters with Little Overhead</title>
<link>https://arxiv.org/abs/2407.00066</link>
<guid>https://arxiv.org/abs/2407.00066</guid>
<content:encoded><![CDATA[
arXiv:2407.00066v4 Announce Type: replace-cross 
Abstract: Fine-tuning large language models (LLMs) with low-rank adaptations (LoRAs) has become common practice, often yielding numerous copies of the same LLM differing only in their LoRA updates. This paradigm presents challenges for systems that serve real-time responses to queries that each involve a different LoRA. Prior works optimize the design of such systems but still require continuous loading and offloading of LoRAs, as it is infeasible to store thousands of LoRAs in GPU memory. To mitigate this issue, we investigate the efficacy of compression when serving LoRAs. We propose a method for the joint compression of LoRAs into a shared basis paired with LoRA-specific scaling matrices. We extend our algorithm to learn clusters of LoRAs that are amenable to joint compression, allowing it to scale gracefully to large LoRA collections. Our experiments with up to 1000 LoRAs demonstrate that compressed LoRAs preserve performance while offering major throughput gains in realistic serving scenarios with over a thousand LoRAs, maintaining 80% of the throughput of serving a single LoRA.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NativQA: Multilingual Culturally-Aligned Natural Query for LLMs</title>
<link>https://arxiv.org/abs/2407.09823</link>
<guid>https://arxiv.org/abs/2407.09823</guid>
<content:encoded><![CDATA[
arXiv:2407.09823v3 Announce Type: replace-cross 
Abstract: Natural Question Answering (QA) datasets play a crucial role in evaluating the capabilities of large language models (LLMs), ensuring their effectiveness in real-world applications. Despite the numerous QA datasets that have been developed and some work has been done in parallel, there is a notable lack of a framework and large scale region-specific datasets queried by native users in their own languages. This gap hinders the effective benchmarking and the development of fine-tuned models for regional and cultural specificities. In this study, we propose a scalable, language-independent framework, NativQA, to seamlessly construct culturally and regionally aligned QA datasets in native languages, for LLM evaluation and tuning. We demonstrate the efficacy of the proposed framework by designing a multilingual natural QA dataset, MultiNativQA, consisting of ~64k manually annotated QA pairs in seven languages, ranging from high to extremely low resource, based on queries from native speakers from 9 regions covering 18 topics. We benchmark open- and closed-source LLMs with the MultiNativQA dataset. We made the MultiNativQA dataset(https://huggingface.co/datasets/QCRI/MultiNativQA), and other experimental scripts(https://gitlab.com/nativqa/multinativqa) publicly available for the community.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Planning in a recurrent neural network that plays Sokoban</title>
<link>https://arxiv.org/abs/2407.15421</link>
<guid>https://arxiv.org/abs/2407.15421</guid>
<content:encoded><![CDATA[
arXiv:2407.15421v3 Announce Type: replace-cross 
Abstract: Planning is essential for solving complex tasks, yet the internal mechanisms underlying planning in neural networks remain poorly understood. Building on prior work, we analyze a recurrent neural network (RNN) trained on Sokoban, a challenging puzzle requiring sequential, irreversible decisions. We find that the RNN has a causal plan representation which predicts its future actions about 50 steps in advance. The quality and length of the represented plan increases over the first few steps. We uncover a surprising behavior: the RNN "paces" in cycles to give itself extra computation at the start of a level, and show that this behavior is incentivized by training. Leveraging these insights, we extend the trained RNN to significantly larger, out-of-distribution Sokoban puzzles, demonstrating robust representations beyond the training regime. We open-source our model and code, and believe the neural network's interesting behavior makes it an excellent model organism to deepen our understanding of learned planning.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Principled Understanding of Generalization for Generative Transformer Models in Arithmetic Reasoning Tasks</title>
<link>https://arxiv.org/abs/2407.17963</link>
<guid>https://arxiv.org/abs/2407.17963</guid>
<content:encoded><![CDATA[
arXiv:2407.17963v2 Announce Type: replace-cross 
Abstract: Transformer-based models excel in various tasks but their generalization capabilities, especially in arithmetic reasoning, remain incompletely understood. Arithmetic tasks provide a controlled framework to explore these capabilities, yet performance anomalies persist, such as inconsistent effectiveness in multiplication and erratic generalization in modular addition (e.g., modulo 100 vs. 101). This paper develops a unified theoretical framework for understanding the generalization behaviors of transformers in arithmetic tasks, focusing on length generalization. Through detailed analysis of addition, multiplication, and modular operations, we reveal that translation invariance in addition aligns with relative positional encoding for robust generalization, while base mismatch in modular operations disrupts this alignment. Experiments across GPT-family models validate our framework, confirming its ability to predict generalization behaviors. Our work highlights the importance of task structure and training data distribution for achieving data-efficient and structure-aware training, providing a systematic approach to understanding of length generalization in transformers.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VITA: Towards Open-Source Interactive Omni Multimodal LLM</title>
<link>https://arxiv.org/abs/2408.05211</link>
<guid>https://arxiv.org/abs/2408.05211</guid>
<content:encoded><![CDATA[
arXiv:2408.05211v3 Announce Type: replace-cross 
Abstract: The remarkable multimodal capabilities and interactive experience of GPT-4o underscore their necessity in practical applications, yet open-source models rarely excel in both areas. In this paper, we introduce VITA, the first-ever open-source Multimodal Large Language Model (MLLM) adept at simultaneous processing and analysis of Video, Image, Text, and Audio modalities, and meanwhile has an advanced multimodal interactive experience. Starting from Mixtral 8x7B as a language foundation, we expand its Chinese vocabulary followed by bilingual instruction tuning. We further endow the language model with visual and audio capabilities through two-stage multi-task learning of multimodal alignment and instruction tuning. VITA demonstrates robust foundational capabilities of multilingual, vision, and audio understanding, as evidenced by its strong performance across a range of both unimodal and multimodal benchmarks. Beyond foundational capabilities, we have made considerable progress in enhancing the natural multimodal human-computer interaction experience. VITA is the first step for the open-source community to explore the seamless integration of multimodal understanding and interaction. While there is still lots of work to be done on VITA to get close to close-source counterparts, we hope that its role as a pioneer can serve as a cornerstone for subsequent research. Project Page: https://vita-home.github.io.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond KAN: Introducing KarSein for Adaptive High-Order Feature Interaction Modeling in CTR Prediction</title>
<link>https://arxiv.org/abs/2408.08713</link>
<guid>https://arxiv.org/abs/2408.08713</guid>
<content:encoded><![CDATA[
arXiv:2408.08713v5 Announce Type: replace-cross 
Abstract: Modeling high-order feature interactions is crucial for click-through rate (CTR) prediction, and traditional approaches often predefine a maximum interaction order and rely on exhaustive enumeration of feature combinations up to this predefined order. This framework heavily relies on prior domain knowledge to define interaction scope and entails high computational costs from enumeration. Conventional CTR models face a trade-off between improving representation through complex high-order feature interactions and reducing computational inefficiencies associated with these processes. To address this dual challenge, this study introduces the Kolmogorov-Arnold Represented Sparse Efficient Interaction Network (KarSein). Drawing inspiration from the learnable activation mechanism in the Kolmogorov-Arnold Network (KAN), KarSein leverages this mechanism to adaptively transform low-order basic features into high-order feature interactions, offering a novel approach to feature interaction modeling. KarSein extends the capabilities of KAN by introducing a more efficient architecture that significantly reduces computational costs while accommodating two-dimensional embedding vectors as feature inputs. Furthermore, it overcomes the limitation of KAN's its inability to spontaneously capture multiplicative relationships among features.
  Extensive experiments highlight the superiority of KarSein, demonstrating its ability to surpass not only the vanilla implementation of KAN in CTR predictio but also other baseline methods. Remarkably, KarSein achieves exceptional predictive accuracy while maintaining a highly compact parameter size and minimal computational overhead. As the first attempt to apply KAN in the CTR domain, this work introduces KarSein as a novel solution for modeling complex feature interactions, underscoring its transformative potential in advancing CTR prediction task.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvalYaks: Instruction Tuning Datasets and LoRA Fine-tuned Models for Automated Scoring of CEFR B2 Speaking Assessment Transcripts</title>
<link>https://arxiv.org/abs/2408.12226</link>
<guid>https://arxiv.org/abs/2408.12226</guid>
<content:encoded><![CDATA[
arXiv:2408.12226v2 Announce Type: replace-cross 
Abstract: Relying on human experts to evaluate CEFR speaking assessments in an e-learning environment creates scalability challenges, as it limits how quickly and widely assessments can be conducted. We aim to automate the evaluation of CEFR B2 English speaking assessments in e-learning environments from conversation transcripts. First, we evaluate the capability of leading open source and commercial Large Language Models (LLMs) to score a candidate's performance across various criteria in the CEFR B2 speaking exam in both global and India-specific contexts. Next, we create a new expert-validated, CEFR-aligned synthetic conversational dataset with transcripts that are rated at different assessment scores. In addition, new instruction-tuned datasets are developed from the English Vocabulary Profile (up to CEFR B2 level) and the CEFR-SP WikiAuto datasets. Finally, using these new datasets, we perform parameter efficient instruction tuning of Mistral Instruct 7B v0.2 to develop a family of models called EvalYaks. Four models in this family are for assessing the four sections of the CEFR B2 speaking exam, one for identifying the CEFR level of vocabulary and generating level-specific vocabulary, and another for detecting the CEFR level of text and generating level-specific text. EvalYaks achieved an average acceptable accuracy of 96%, a degree of variation of 0.35 levels, and performed 3 times better than the next best model. This demonstrates that a 7B parameter LLM instruction tuned with high-quality CEFR-aligned assessment data can effectively evaluate and score CEFR B2 English speaking assessments, offering a promising solution for scalable, automated language proficiency evaluation.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LlamaDuo: LLMOps Pipeline for Seamless Migration from Service LLMs to Small-Scale Local LLMs</title>
<link>https://arxiv.org/abs/2408.13467</link>
<guid>https://arxiv.org/abs/2408.13467</guid>
<content:encoded><![CDATA[
arXiv:2408.13467v3 Announce Type: replace-cross 
Abstract: The widespread adoption of cloud-based proprietary large language models (LLMs) has introduced significant challenges, including operational dependencies, privacy concerns, and the necessity of continuous internet connectivity. In this work, we introduce an LLMOps pipeline, "LlamaDuo", for the seamless migration of knowledge and abilities from service-oriented LLMs to smaller, locally manageable models. This pipeline is crucial for ensuring service continuity in the presence of operational failures, strict privacy policies, or offline requirements. Our LlamaDuo involves fine-tuning a small language model against the service LLM using a synthetic dataset generated by the latter. If the performance of the fine-tuned model falls short of expectations, it is automatically improved through additional fine-tuning using extra similar data generated by the service LLM. This multi-turn process guarantees that the smaller model can eventually match or even surpass the service LLM's capabilities in specific downstream tasks, offering a practical and scalable solution for managing AI deployments in constrained environments. Extensive experiments with leading-edge LLMs are conducted to demonstrate the effectiveness, adaptability, and affordability of LlamaDuo across various downstream tasks. Our pipeline implementation is available at https://github.com/deep-diver/llamaduo.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Vulnerability of Applying Retrieval-Augmented Generation within Knowledge-Intensive Application Domains</title>
<link>https://arxiv.org/abs/2409.17275</link>
<guid>https://arxiv.org/abs/2409.17275</guid>
<content:encoded><![CDATA[
arXiv:2409.17275v2 Announce Type: replace-cross 
Abstract: Retrieval-Augmented Generation (RAG) has been empirically shown to enhance the performance of large language models (LLMs) in knowledge-intensive domains such as healthcare, finance, and legal contexts. Given a query, RAG retrieves relevant documents from a corpus and integrates them into the LLMs' generation process. In this study, we investigate the adversarial robustness of RAG, focusing specifically on examining the retrieval system. First, across 225 different setup combinations of corpus, retriever, query, and targeted information, we show that retrieval systems are vulnerable to universal poisoning attacks in medical Q\&amp;A. In such attacks, adversaries generate poisoned documents containing a broad spectrum of targeted information, such as personally identifiable information. When these poisoned documents are inserted into a corpus, they can be accurately retrieved by any users, as long as attacker-specified queries are used. To understand this vulnerability, we discovered that the deviation from the query's embedding to that of the poisoned document tends to follow a pattern in which the high similarity between the poisoned document and the query is retained, thereby enabling precise retrieval. Based on these findings, we develop a new detection-based defense to ensure the safe use of RAG. Through extensive experiments spanning various Q\&amp;A domains, we observed that our proposed method consistently achieves excellent detection rates in nearly all cases.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IDEA: An Inverse Domain Expert Adaptation Based Active DNN IP Protection Method</title>
<link>https://arxiv.org/abs/2410.00059</link>
<guid>https://arxiv.org/abs/2410.00059</guid>
<content:encoded><![CDATA[
arXiv:2410.00059v2 Announce Type: replace-cross 
Abstract: Illegitimate reproduction, distribution and derivation of Deep Neural Network (DNN) models can inflict economic loss, reputation damage and even privacy infringement. Passive DNN intellectual property (IP) protection methods such as watermarking and fingerprinting attempt to prove the ownership upon IP violation, but they are often too late to stop catastrophic damage of IP abuse and too feeble against strong adversaries. In this paper, we propose IDEA, an Inverse Domain Expert Adaptation based proactive DNN IP protection method featuring active authorization and source traceability. IDEA generalizes active authorization as an inverse problem of domain adaptation. The multi-adaptive optimization is solved by a mixture-of-experts model with one real and two fake experts. The real expert re-optimizes the source model to correctly classify test images with a unique model user key steganographically embedded. The fake experts are trained to output random prediction on test images without or with incorrect user key embedded by minimizing their mutual information (MI) with the real expert. The MoE model is knowledge distilled into a unified protected model to avoid leaking the expert model features by maximizing their MI with additional multi-layer attention and contrastive representation loss optimization. IDEA not only prevents unauthorized users without the valid key to access the functional model, but also enable the model owner to validate the deployed model and trace the source of IP infringement. We extensively evaluate IDEA on five datasets and four DNN models to demonstrate its effectiveness in authorization control, culprit tracing success rate, and robustness against various attacks.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ManiSkill3: GPU Parallelized Robotics Simulation and Rendering for Generalizable Embodied AI</title>
<link>https://arxiv.org/abs/2410.00425</link>
<guid>https://arxiv.org/abs/2410.00425</guid>
<content:encoded><![CDATA[
arXiv:2410.00425v2 Announce Type: replace-cross 
Abstract: Simulation has enabled unprecedented compute-scalable approaches to robot learning. However, many existing simulation frameworks typically support a narrow range of scenes/tasks and lack features critical for scaling generalizable robotics and sim2real. We introduce and open source ManiSkill3, the fastest state-visual GPU parallelized robotics simulator with contact-rich physics targeting generalizable manipulation. ManiSkill3 supports GPU parallelization of many aspects including simulation+rendering, heterogeneous simulation, pointclouds/voxels visual input, and more. Simulation with rendering on ManiSkill3 can run 10-1000x faster with 2-3x less GPU memory usage than other platforms, achieving up to 30,000+ FPS in benchmarked environments due to minimal python/pytorch overhead in the system, simulation on the GPU, and the use of the SAPIEN parallel rendering system. Tasks that used to take hours to train can now take minutes. We further provide the most comprehensive range of GPU parallelized environments/tasks spanning 12 distinct domains including but not limited to mobile manipulation for tasks such as drawing, humanoids, and dextrous manipulation in realistic scenes designed by artists or real-world digital twins. In addition, millions of demonstration frames are provided from motion planning, RL, and teleoperation. ManiSkill3 also provides a comprehensive set of baselines that span popular RL and learning-from-demonstrations algorithms.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and Defenses in LLM-based Agents</title>
<link>https://arxiv.org/abs/2410.02644</link>
<guid>https://arxiv.org/abs/2410.02644</guid>
<content:encoded><![CDATA[
arXiv:2410.02644v4 Announce Type: replace-cross 
Abstract: Although LLM-based agents, powered by Large Language Models (LLMs), can use external tools and memory mechanisms to solve complex real-world tasks, they may also introduce critical security vulnerabilities. However, the existing literature does not comprehensively evaluate attacks and defenses against LLM-based agents. To address this, we introduce Agent Security Bench (ASB), a comprehensive framework designed to formalize, benchmark, and evaluate the attacks and defenses of LLM-based agents, including 10 scenarios (e.g., e-commerce, autonomous driving, finance), 10 agents targeting the scenarios, over 400 tools, 27 different types of attack/defense methods, and 7 evaluation metrics. Based on ASB, we benchmark 10 prompt injection attacks, a memory poisoning attack, a novel Plan-of-Thought backdoor attack, 4 mixed attacks, and 11 corresponding defenses across 13 LLM backbones. Our benchmark results reveal critical vulnerabilities in different stages of agent operation, including system prompt, user prompt handling, tool usage, and memory retrieval, with the highest average attack success rate of 84.30\%, but limited effectiveness shown in current defenses, unveiling important works to be done in terms of agent security for the community. We also introduce a new metric to evaluate the agents' capability to balance utility and security. Our code can be found at https://github.com/agiresearch/ASB.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Point Cloud Completion through Unbalanced Optimal Transport</title>
<link>https://arxiv.org/abs/2410.02671</link>
<guid>https://arxiv.org/abs/2410.02671</guid>
<content:encoded><![CDATA[
arXiv:2410.02671v4 Announce Type: replace-cross 
Abstract: Unpaired point cloud completion is crucial for real-world applications, where ground-truth data for complete point clouds are often unavailable. By learning a completion map from unpaired incomplete and complete point cloud data, this task avoids the reliance on paired datasets. In this paper, we propose the \textit{Unbalanced Optimal Transport Map for Unpaired Point Cloud Completion (\textbf{UOT-UPC})} model, which formulates the unpaired completion task as the (Unbalanced) Optimal Transport (OT) problem. Our method employs a Neural OT model learning the UOT map using neural networks. Our model is the first attempt to leverage UOT for unpaired point cloud completion, achieving competitive or superior performance on both single-category and multi-category benchmarks. In particular, our approach is especially robust under the class imbalance problem, which is frequently encountered in real-world unpaired point cloud completion scenarios. The code is available at https://github.com/LEETK99/UOT-UPC.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying biological perturbation targets through causal differential networks</title>
<link>https://arxiv.org/abs/2410.03380</link>
<guid>https://arxiv.org/abs/2410.03380</guid>
<content:encoded><![CDATA[
arXiv:2410.03380v4 Announce Type: replace-cross 
Abstract: Identifying variables responsible for changes to a biological system enables applications in drug target discovery and cell engineering. Given a pair of observational and interventional datasets, the goal is to isolate the subset of observed variables that were the targets of the intervention. Directly applying causal discovery algorithms is challenging: the data may contain thousands of variables with as few as tens of samples per intervention, and biological systems do not adhere to classical causality assumptions. We propose a causality-inspired approach to address this practical setting. First, we infer noisy causal graphs from the observational and interventional data. Then, we learn to map the differences between these graphs, along with additional statistical features, to sets of variables that were intervened upon. Both modules are jointly trained in a supervised framework, on simulated and real data that reflect the nature of biological interventions. This approach consistently outperforms baselines for perturbation modeling on seven single-cell transcriptomics datasets. We also demonstrate significant improvements over current causal discovery methods for predicting soft and hard intervention targets across a variety of synthetic data.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DemoShapley: Valuation of Demonstrations for In-Context Learning</title>
<link>https://arxiv.org/abs/2410.07523</link>
<guid>https://arxiv.org/abs/2410.07523</guid>
<content:encoded><![CDATA[
arXiv:2410.07523v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) using in-context learning (ICL) excel in many tasks without task-specific fine-tuning. However, demonstration selection and ordering greatly impact ICL effectiveness. To address this, we propose DemoShapley and Beta-DemoShapley, inspired by Data Shapley and Beta Shapley, to assess the influence of individual demonstrations. DemoShapley captures how each example influences performance in different contexts, unlike other influence-based methods that rely on a fixed number of demonstrations. Beta-DemoShapley further enhances this framework by incorporating the Beta distribution, allowing users to assign higher weights to smaller cardinalities, which aligns with ICL's prompt length and computational constraints. Our findings show that the proposed algorithms improve model performance by selecting quality demonstrations, and enhancing generalization to out-of-distribution tasks. It also identifies noise-compromised data and promotes fairness in LLMs, protecting model performance and ensuring robustness across various scenarios.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Citations to Criticality: Predicting Legal Decision Influence in the Multilingual Swiss Jurisprudence</title>
<link>https://arxiv.org/abs/2410.13460</link>
<guid>https://arxiv.org/abs/2410.13460</guid>
<content:encoded><![CDATA[
arXiv:2410.13460v2 Announce Type: replace-cross 
Abstract: Many court systems are overwhelmed all over the world, leading to huge backlogs of pending cases. Effective triage systems, like those in emergency rooms, could ensure proper prioritization of open cases, optimizing time and resource allocation in the court system. In this work, we introduce the Criticality Prediction dataset, a novel resource for evaluating case prioritization. Our dataset features a two-tier labeling system: (1) the binary LD-Label, identifying cases published as Leading Decisions (LD), and (2) the more granular Citation-Label, ranking cases by their citation frequency and recency, allowing for a more nuanced evaluation. Unlike existing approaches that rely on resource-intensive manual annotations, we algorithmically derive labels leading to a much larger dataset than otherwise possible. We evaluate several multilingual models, including both smaller fine-tuned models and large language models in a zero-shot setting. Our results show that the fine-tuned models consistently outperform their larger counterparts, thanks to our large training set. Our results highlight that for highly domain-specific tasks like ours, large training sets are still valuable.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProReason: Multi-Modal Proactive Reasoning with Decoupled Eyesight and Wisdom</title>
<link>https://arxiv.org/abs/2410.14138</link>
<guid>https://arxiv.org/abs/2410.14138</guid>
<content:encoded><![CDATA[
arXiv:2410.14138v3 Announce Type: replace-cross 
Abstract: Large vision-language models (LVLMs) have witnessed significant progress on visual understanding tasks. However, they often prioritize language knowledge over image information on visual reasoning tasks, incurring performance degradation. To tackle this issue, we first identify the drawbacks of existing solutions (i.e., limited multi-modal reasoning capacities, and insufficient and irrelevant visual descriptions). We then decompose visual reasoning process into two stages: proactive visual perception (i.e., eyesight) and textual reasoning (i.e., wisdom), and introduce a novel visual reasoning framework named ProReason. This framework features decoupled vision-reasoning capabilities and multi-run proactive perception. Briefly, given a multi-modal question, ProReason iterates proactive information collection and reasoning until the answer can be concluded with necessary and sufficient visual descriptions. Notably, the disassociation of capabilities allows seamless integration of existing large language models (LLMs) to compensate for the reasoning deficits of LVLMs. Our extensive experiments demonstrate that ProReason outperforms existing multi-step reasoning frameworks on various benchmarks for both open-source and closed-source models, with the average performance gain reaching 13.2%. Besides, the integration of LLMs allows ProReason to produce high-quality visual reasoning data, which empowers ProReason-distilled models (i.e., ProReason-VL and ProReason-Q3) to achieve superior performance in downstream tasks. Our insights into existing solutions and the decoupled perspective for feasible integration of LLMs illuminate future research on visual reasoning techniques, especially LLM-assisted ones.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SVIP: Towards Verifiable Inference of Open-source Large Language Models</title>
<link>https://arxiv.org/abs/2410.22307</link>
<guid>https://arxiv.org/abs/2410.22307</guid>
<content:encoded><![CDATA[
arXiv:2410.22307v2 Announce Type: replace-cross 
Abstract: The ever-increasing size of open-source Large Language Models (LLMs) renders local deployment impractical for individual users. Decentralized computing has emerged as a cost-effective solution, allowing individuals and small companies to perform LLM inference for users using surplus computational power. However, a computing provider may stealthily substitute the requested LLM with a smaller, less capable model without consent from users, thereby benefiting from cost savings. We introduce SVIP, a secret-based verifiable LLM inference protocol. Unlike existing solutions based on cryptographic or game-theoretic techniques, our method is computationally effective and does not rest on strong assumptions. Our protocol requires the computing provider to return both the generated text and processed hidden representations from LLMs. We then train a proxy task on these representations, effectively transforming them into a unique model identifier. With our protocol, users can reliably verify whether the computing provider is acting honestly. A carefully integrated secret mechanism further strengthens its security. We thoroughly analyze our protocol under multiple strong and adaptive adversarial scenarios. Our extensive experiments demonstrate that SVIP is accurate, generalizable, computationally efficient, and resistant to various attacks. Notably, SVIP achieves false negative rates below 5% and false positive rates below 3%, while requiring less than 0.01 seconds per prompt query for verification.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One-Step is Enough: Sparse Autoencoders for Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2410.22366</link>
<guid>https://arxiv.org/abs/2410.22366</guid>
<content:encoded><![CDATA[
arXiv:2410.22366v3 Announce Type: replace-cross 
Abstract: For large language models (LLMs), sparse autoencoders (SAEs) have been shown to decompose intermediate representations that often are not interpretable directly into sparse sums of interpretable features, facilitating better control and subsequent analysis. However, similar analyses and approaches have been lacking for text-to-image models. We investigate the possibility of using SAEs to learn interpretable features for SDXL Turbo, a few-step text-to-image diffusion model. To this end, we train SAEs on the updates performed by transformer blocks within SDXL Turbo's denoising U-net in its 1-step setting. Interestingly, we find that they generalize to 4-step SDXL Turbo and even to the multi-step SDXL base model (i.e., a different model) without additional training. In addition, we show that their learned features are interpretable, causally influence the generation process, and reveal specialization among the blocks. We do so by creating RIEBench, a representation-based image editing benchmark, for editing images while they are generated by turning on and off individual SAE features. This allows us to track which transformer blocks' features are the most impactful depending on the edit category. Our work is the first investigation of SAEs for interpretability in text-to-image diffusion models and our results establish SAEs as a promising approach for understanding and manipulating the internal mechanisms of text-to-image models.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust and Efficient Federated Low-Rank Adaptation with Heterogeneous Clients</title>
<link>https://arxiv.org/abs/2410.22815</link>
<guid>https://arxiv.org/abs/2410.22815</guid>
<content:encoded><![CDATA[
arXiv:2410.22815v2 Announce Type: replace-cross 
Abstract: Federated fine-tuning for Large Language Models (LLMs) faces significant challenges due to the heavy communication overhead of transmitting large model updates. Although Low Rank Adaptation (LoRA) has been proposed as a solution, yet its application in federated learning is complicated by discordance in aggregation. Existing methods addressing this discordance often suffer from performance degradation at low ranks in heterogeneous data settings. In response, we introduce LoRA-A$^2$ (Low Rank Adaptation with Alternating freeze and Adaptive rank selection), which demonstrates robustness in challenging settings with low ranks and high data heterogeneity. Our experimental findings reveal that LoRA-A$^2$ maintains performance even under extreme heterogeneity and low rank conditions, achieving up to a significant reduction in uploaded parameters compared to full fine-tuning without compromising performance. This adaptive mechanism increases robustness and communication efficiency in federated fine-tuning, enabling the practical deployment of LLMs in resource-constrained environments.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large-Scale Multi-Robot Coverage Path Planning on Grids with Path Deconfliction</title>
<link>https://arxiv.org/abs/2411.01707</link>
<guid>https://arxiv.org/abs/2411.01707</guid>
<content:encoded><![CDATA[
arXiv:2411.01707v2 Announce Type: replace-cross 
Abstract: We study Multi-Robot Coverage Path Planning (MCPP) on a 4-neighbor 2D grid G, which aims to compute paths for multiple robots to cover all cells of G. Traditional approaches are limited as they first compute coverage trees on a quadrant coarsened grid H and then employ the Spanning Tree Coverage (STC) paradigm to generate paths on G, making them inapplicable to grids with partially obstructed 2x2 blocks. To address this limitation, we reformulate the problem directly on G, revolutionizing grid-based MCPP solving and establishing new NP-hardness results. We introduce Extended-STC (ESTC), a novel paradigm that extends STC to ensure complete coverage with bounded suboptimality, even when H includes partially obstructed blocks. Furthermore, we present LS-MCPP, a new algorithmic framework that integrates ESTC with three novel types of neighborhood operators within a local search strategy to optimize coverage paths directly on G. Unlike prior grid-based MCPP work, our approach also incorporates a versatile post-processing procedure that applies Multi-Agent Path Finding (MAPF) techniques to MCPP for the first time, enabling a fusion of these two important fields in multi-robot coordination. This procedure effectively resolves inter-robot conflicts and accommodates turning costs by solving a MAPF variant, making our MCPP solutions more practical for real-world applications. Extensive experiments demonstrate that our approach significantly improves solution quality and efficiency, managing up to 100 robots on grids as large as 256x256 within minutes of runtime. Validation with physical robots confirms the feasibility of our solutions under real-world conditions.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Give Me BF16 or Give Me Death"? Accuracy-Performance Trade-Offs in LLM Quantization</title>
<link>https://arxiv.org/abs/2411.02355</link>
<guid>https://arxiv.org/abs/2411.02355</guid>
<content:encoded><![CDATA[
arXiv:2411.02355v3 Announce Type: replace-cross 
Abstract: Quantization is a powerful tool for accelerating large language model (LLM) inference, but the accuracy-performance trade-offs across different formats remain unclear. In this paper, we conduct the most comprehensive empirical study to date, evaluating FP8, INT8, and INT4 quantization across academic benchmarks and real-world tasks on the entire Llama-3.1 model family. Through over 500,000 evaluations, our investigation yields several key findings: (1) FP8 (W8A8-FP) is effectively lossless across all model scales, (2) well-tuned INT8 (W8A8-INT) achieves surprisingly low (1-3\%) accuracy degradation, and (3) INT4 weight-only (W4A16-INT) is more competitive than expected, rivaling 8-bit quantization. Further, we investigate the optimal quantization format for different deployments by analyzing inference performance through the popular vLLM framework. Our analysis provides clear deployment recommendations: W4A16 is the most cost-efficient for synchronous setups, while W8A8 dominates in asynchronous continuous batching. For mixed workloads, the optimal choice depends on the specific use case. Our findings offer practical, data-driven guidelines for deploying quantized LLMs at scale -- ensuring the best balance between speed, efficiency, and accuracy.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expansion Quantization Network: An Efficient Micro-emotion Annotation and Detection Framework</title>
<link>https://arxiv.org/abs/2411.06160</link>
<guid>https://arxiv.org/abs/2411.06160</guid>
<content:encoded><![CDATA[
arXiv:2411.06160v3 Announce Type: replace-cross 
Abstract: Text emotion detection constitutes a crucial foundation for advancing artificial intelligence from basic comprehension to the exploration of emotional reasoning. Most existing emotion detection datasets rely on manual annotations, which are associated with high costs, substantial subjectivity, and severe label imbalances. This is particularly evident in the inadequate annotation of micro-emotions and the absence of emotional intensity representation, which fail to capture the rich emotions embedded in sentences and adversely affect the quality of downstream task completion. By proposing an all-labels and training-set label regression method, we map label values to energy intensity levels, thereby fully leveraging the learning capabilities of machine models and the interdependencies among labels to uncover multiple emotions within samples. This led to the establishment of the Emotion Quantization Network (EQN) framework for micro-emotion detection and annotation. Using five commonly employed sentiment datasets, we conducted comparative experiments with various models, validating the broad applicability of our framework within NLP machine learning models. Based on the EQN framework, emotion detection and annotation are conducted on the GoEmotions dataset. A comprehensive comparison with the results from Google literature demonstrates that the EQN framework possesses a high capability for automatic detection and annotation of micro-emotions. The EQN framework is the first to achieve automatic micro-emotion annotation with energy-level scores, providing strong support for further emotion detection analysis and the quantitative research of emotion computing.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Developer Perspectives on Licensing and Copyright Issues Arising from Generative AI for Software Development</title>
<link>https://arxiv.org/abs/2411.10877</link>
<guid>https://arxiv.org/abs/2411.10877</guid>
<content:encoded><![CDATA[
arXiv:2411.10877v4 Announce Type: replace-cross 
Abstract: Despite the utility that Generative AI (GenAI) tools provide for tasks such as writing code, the use of these tools raises important legal questions and potential risks, particularly those associated with copyright law. As lawmakers and regulators engage with those questions, the views of users can provide relevant perspectives. In this paper, we provide: (1) a survey of 574 developers on the licensing and copyright aspects of GenAI for coding, as well as follow-up interviews; (2) a snapshot of developers' views at a time when GenAI and perceptions of it are rapidly evolving; and (3) an analysis of developers' views, yielding insights and recommendations that can inform future regulatory decisions in this evolving field. Our results show the benefits developers derive from GenAI, how they view the use of AI-generated code as similar to using other existing code, the varied opinions they have on who should own or be compensated for such code, that they are concerned about data leakage via GenAI, and much more, providing organizations and policymakers with valuable insights into how the technology is being used and what concerns stakeholders would like to see addressed.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Convergence of Softmax Policy Mirror Ascent</title>
<link>https://arxiv.org/abs/2411.12042</link>
<guid>https://arxiv.org/abs/2411.12042</guid>
<content:encoded><![CDATA[
arXiv:2411.12042v2 Announce Type: replace-cross 
Abstract: Natural policy gradient (NPG) is a common policy optimization algorithm and can be viewed as mirror ascent in the space of probabilities. Recently, Vaswani et al. [2021] introduced a policy gradient method that corresponds to mirror ascent in the dual space of logits. We refine this algorithm, removing its need for a normalization across actions and analyze the resulting method (referred to as SPMA). For tabular MDPs, we prove that SPMA with a constant step-size matches the linear convergence of NPG and achieves a faster convergence than constant step-size (accelerated) softmax policy gradient. To handle large state-action spaces, we extend SPMA to use a log-linear policy parameterization. Unlike that for NPG, generalizing SPMA to the linear function approximation (FA) setting does not require compatible function approximation. Unlike MDPO, a practical generalization of NPG, SPMA with linear FA only requires solving convex softmax classification problems. We prove that SPMA achieves linear convergence to the neighbourhood of the optimal value function. We extend SPMA to handle non-linear FA and evaluate its empirical performance on the MuJoCo and Atari benchmarks. Our results demonstrate that SPMA consistently achieves similar or better performance compared to MDPO, PPO and TRPO.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoBAM: LoRA-Based Backdoor Attack on Model Merging</title>
<link>https://arxiv.org/abs/2411.16746</link>
<guid>https://arxiv.org/abs/2411.16746</guid>
<content:encoded><![CDATA[
arXiv:2411.16746v4 Announce Type: replace-cross 
Abstract: Model merging is an emerging technique that integrates multiple models fine-tuned on different tasks to create a versatile model that excels in multiple domains. This scheme, in the meantime, may open up backdoor attack opportunities where one single malicious model can jeopardize the integrity of the merged model. Existing works try to demonstrate the risk of such attacks by assuming substantial computational resources, focusing on cases where the attacker can fully fine-tune the pre-trained model. Such an assumption, however, may not be feasible given the increasing size of machine learning models. In practice where resources are limited and the attacker can only employ techniques like Low-Rank Adaptation (LoRA) to produce the malicious model, it remains unclear whether the attack can still work and pose threats. In this work, we first identify that the attack efficacy is significantly diminished when using LoRA for fine-tuning. Then, we propose LoBAM, a method that yields high attack success rate with minimal training resources. The key idea of LoBAM is to amplify the malicious weights in an intelligent way that effectively enhances the attack efficacy. We demonstrate that our design can lead to improved attack success rate through extensive empirical experiments across various model merging scenarios. Moreover, we show that our method is highly stealthy and is difficult to detect and defend against.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Star Attention: Efficient LLM Inference over Long Sequences</title>
<link>https://arxiv.org/abs/2411.17116</link>
<guid>https://arxiv.org/abs/2411.17116</guid>
<content:encoded><![CDATA[
arXiv:2411.17116v3 Announce Type: replace-cross 
Abstract: Inference with Transformer-based Large Language Models (LLMs) on long sequences is both costly and slow due to the quadratic complexity of the self-attention mechanism. We introduce Star Attention, a two-phase block-sparse approximation that improves computational efficiency by sharding attention across multiple hosts while minimizing communication overhead. In the first phase, the context is processed using blockwise-local attention across hosts, in parallel. In the second phase, query and response tokens attend to all prior cached tokens through sequence-global attention. Star Attention integrates seamlessly with most Transformer-based LLMs trained with global attention, reducing memory requirements and inference time by up to 11x while preserving 97-100% of accuracy.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HEIE: MLLM-Based Hierarchical Explainable AIGC Image Implausibility Evaluator</title>
<link>https://arxiv.org/abs/2411.17261</link>
<guid>https://arxiv.org/abs/2411.17261</guid>
<content:encoded><![CDATA[
arXiv:2411.17261v2 Announce Type: replace-cross 
Abstract: AIGC images are prevalent across various fields, yet they frequently suffer from quality issues like artifacts and unnatural textures. Specialized models aim to predict defect region heatmaps but face two primary challenges: (1) lack of explainability, failing to provide reasons and analyses for subtle defects, and (2) inability to leverage common sense and logical reasoning, leading to poor generalization. Multimodal large language models (MLLMs) promise better comprehension and reasoning but face their own challenges: (1) difficulty in fine-grained defect localization due to the limitations in capturing tiny details, and (2) constraints in providing pixel-wise outputs necessary for precise heatmap generation. To address these challenges, we propose HEIE: a novel MLLM-Based Hierarchical Explainable Image Implausibility Evaluator. We introduce the CoT-Driven Explainable Trinity Evaluator, which integrates heatmaps, scores, and explanation outputs, using CoT to decompose complex tasks into subtasks of increasing difficulty and enhance interpretability. Our Adaptive Hierarchical Implausibility Mapper synergizes low-level image features with high-level mapper tokens from LLMs, enabling precise local-to-global hierarchical heatmap predictions through an uncertainty-based adaptive token approach. Moreover, we propose a new dataset: Expl-AIGI-Eval, designed to facilitate interpretable implausibility evaluation of AIGC images. Our method demonstrates state-of-the-art performance through extensive experiments. Our project is at https://yfthu.github.io/HEIE/.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LVLM-COUNT: Enhancing the Counting Ability of Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2412.00686</link>
<guid>https://arxiv.org/abs/2412.00686</guid>
<content:encoded><![CDATA[
arXiv:2412.00686v3 Announce Type: replace-cross 
Abstract: Counting is a fundamental operation for various real-world visual tasks, requiring both object recognition and robust counting capabilities. Despite their advanced visual perception, large vision-language models (LVLMs) are known to struggle with counting tasks. In this work, we evaluate the performance of several recent LVLMs on visual counting tasks across multiple counting and vision datasets. We observe that while their performance may be less prone to error for small numbers of objects, they exhibit significant weaknesses as the number of objects increases. To alleviate this issue, we propose a simple yet effective baseline method that enhances LVLMs' counting ability for large numbers of objects using a divide-and-conquer approach. Our method decomposes counting problems into sub-tasks. Moreover, it incorporates a mechanism to prevent objects from being split during division, which could otherwise lead to repetitive counting -- a common issue in a naive divide-and-conquer implementation. We demonstrate the effectiveness of this approach across various datasets and benchmarks, establishing it as a valuable reference for evaluating future solutions.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling</title>
<link>https://arxiv.org/abs/2412.04905</link>
<guid>https://arxiv.org/abs/2412.04905</guid>
<content:encoded><![CDATA[
arXiv:2412.04905v4 Announce Type: replace-cross 
Abstract: Large language models (LLMs) enabled dialogue systems have become one of the central modes in human-machine interaction, which bring about vast amounts of conversation logs and increasing demand for dialogue generation. The dialogue's life-cycle spans from $\textit{Prelude}$ through $\textit{Interlocution}$ to $\textit{Epilogue}$, encompassing rich dialogue elements. Despite large volumes of dialogue-related studies, there is a lack of systematic investigation into the dialogue stages to frame benchmark construction that covers comprehensive dialogue elements. This hinders the precise modeling, generation and assessment of LLMs-based dialogue systems. To bridge this gap, in this paper, we introduce a new research task--$\textbf{D}$ialogue $\textbf{E}$lement $\textbf{MO}$deling, including $\textit{Element Awareness}$ and $\textit{Dialogue Agent Interaction}$, and propose a novel benchmark, $\textbf{DEMO}$, designed for a comprehensive dialogue modeling and assessment. On this basis, we further build the DEMO agent with the adept ability to model dialogue elements via imitation learning. Extensive experiments on DEMO indicate that current representative LLMs still have considerable potential for enhancement, and our DEMO agent performs well in both dialogue element modeling and out-of-domain tasks.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exact Algorithms for Multiagent Path Finding with Communication Constraints on Tree-Like Structures</title>
<link>https://arxiv.org/abs/2412.08556</link>
<guid>https://arxiv.org/abs/2412.08556</guid>
<content:encoded><![CDATA[
arXiv:2412.08556v3 Announce Type: replace-cross 
Abstract: Consider the scenario where multiple agents have to move in an optimal way through a network, each one towards their ending position while avoiding collisions. By optimal, we mean as fast as possible, which is evaluated by a measure known as the makespan of the proposed solution. This is the setting studied in the Multiagent Path Finding problem. In this work, we additionally provide the agents with a way to communicate with each other. Due to size constraints, it is reasonable to assume that the range of communication of each agent will be limited. What should be the trajectories of the agents to, additionally, maintain a backbone of communication? In this work, we study the Multiagent Path Finding with Communication Constraint problem under the parameterized complexity framework.
  Our main contribution is three exact algorithms that are efficient when considering particular structures for the input network. We provide such algorithms for the case when the communication range and the number of agents (the makespan resp.) are provided in the input and the network has a tree topology, or bounded maximum degree (has a tree-like topology, i.e., bounded treewidth resp.). We complement these results by showing that it is highly unlikely to construct efficient algorithms when considering the number of agents as part of the input, even if the makespan is $3$ and the communication range is $1$.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RuleArena: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World Scenarios</title>
<link>https://arxiv.org/abs/2412.08972</link>
<guid>https://arxiv.org/abs/2412.08972</guid>
<content:encoded><![CDATA[
arXiv:2412.08972v2 Announce Type: replace-cross 
Abstract: This paper introduces RuleArena, a novel and challenging benchmark designed to evaluate the ability of large language models (LLMs) to follow complex, real-world rules in reasoning. Covering three practical domains -- airline baggage fees, NBA transactions, and tax regulations -- RuleArena assesses LLMs' proficiency in handling intricate natural language instructions that demand long-context understanding, logical reasoning, and accurate mathematical computation. Two key attributes distinguish RuleArena from traditional rule-based reasoning benchmarks: (1) it extends beyond standard first-order logic representations, and (2) it is grounded in authentic, practical scenarios, providing insights into the suitability and reliability of LLMs for real-world applications. Our findings reveal several notable limitations in LLMs: (1) they struggle to identify and apply the appropriate rules, frequently becoming confused by similar but distinct regulations, (2) they cannot consistently perform accurate mathematical computations, even when they correctly identify the relevant rules, and (3) in general, they perform poorly in the benchmark. We also observe a significant performance boost when LLMs are provided with external tools for oracle math and logic operations. These results highlight significant challenges and promising research directions in advancing LLMs' rule-guided reasoning capabilities in real-life applications. Our codes and data are publicly available on https://github.com/skyriver-2000/RuleArena.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic Knowledge Graph</title>
<link>https://arxiv.org/abs/2412.15268</link>
<guid>https://arxiv.org/abs/2412.15268</guid>
<content:encoded><![CDATA[
arXiv:2412.15268v3 Announce Type: replace-cross 
Abstract: The rapid growth of social media platforms has raised significant concerns regarding online content toxicity. When Large Language Models (LLMs) are used for toxicity detection, two key challenges emerge: 1) the absence of domain-specific toxic knowledge leads to false negatives; 2) the excessive sensitivity of LLMs to toxic speech results in false positives, limiting freedom of speech. To address these issues, we propose a novel method called MetaTox, leveraging graph search on a meta-toxic knowledge graph to enhance hatred and toxicity detection. First, we construct a comprehensive meta-toxic knowledge graph by utilizing LLMs to extract toxic information through a three-step pipeline, with toxic benchmark datasets serving as corpora. Second, we query the graph via retrieval and ranking processes to supplement accurate, relevant toxic knowledge. Extensive experiments and in-depth case studies across multiple datasets demonstrate that our MetaTox significantly decreases the false positive rate while boosting overall toxicity detection performance. Our code is available at https://github.com/YiboZhao624/MetaTox.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing Social Alignment: Do Personality-Prompted Large Language Models Behave Like Humans?</title>
<link>https://arxiv.org/abs/2412.16772</link>
<guid>https://arxiv.org/abs/2412.16772</guid>
<content:encoded><![CDATA[
arXiv:2412.16772v2 Announce Type: replace-cross 
Abstract: The ongoing revolution in language modeling has led to various novel applications, some of which rely on the emerging social abilities of large language models (LLMs). Already, many turn to the new cyber friends for advice during the pivotal moments of their lives and trust them with the deepest secrets, implying that accurate shaping of the LLM's personality is paramount. To this end, state-of-the-art approaches exploit a vast variety of training data, and prompt the model to adopt a particular personality. We ask (i) if personality-prompted models behave (i.e., make decisions when presented with a social situation) in line with the ascribed personality (ii) if their behavior can be finely controlled. We use classic psychological experiments, the Milgram experiment and the Ultimatum Game, as social interaction testbeds and apply personality prompting to open- and closed-source LLMs from 4 different vendors. Our experiments reveal failure modes of the prompt-based modulation of the models' behavior that are shared across all models tested and persist under prompt perturbations. These findings challenge the optimistic sentiment toward personality prompting generally held in the community.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Statistical Framework for Ranking LLM-Based Chatbots</title>
<link>https://arxiv.org/abs/2412.18407</link>
<guid>https://arxiv.org/abs/2412.18407</guid>
<content:encoded><![CDATA[
arXiv:2412.18407v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have transformed natural language processing, with frameworks like Chatbot Arena providing pioneering platforms for evaluating these models. By facilitating millions of pairwise comparisons based on human judgments, Chatbot Arena has become a cornerstone in LLM evaluation, offering rich datasets for ranking models in open-ended conversational tasks. Building upon this foundation, we propose a statistical framework that incorporates key advancements to address specific challenges in pairwise comparison analysis. First, we introduce a factored tie model that enhances the ability to handle ties -- an integral aspect of human-judged comparisons -- significantly improving the model's fit to observed data. Second, we extend the framework to model covariance between competitors, enabling deeper insights into performance relationships and facilitating intuitive groupings into performance tiers. Third, we resolve optimization challenges arising from parameter non-uniqueness by introducing novel constraints, ensuring stable and interpretable parameter estimation. Through rigorous evaluation and extensive experimentation, our framework demonstrates substantial improvements over existing methods in modeling pairwise comparison data. To support reproducibility and practical adoption, we release leaderbot, an open-source Python package implementing our models and analyses.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Table Recognition with Vision LLMs: A Benchmark and Neighbor-Guided Toolchain Reasoner</title>
<link>https://arxiv.org/abs/2412.20662</link>
<guid>https://arxiv.org/abs/2412.20662</guid>
<content:encoded><![CDATA[
arXiv:2412.20662v3 Announce Type: replace-cross 
Abstract: Pre-trained foundation models have recently made significant progress in table-related tasks such as table understanding and reasoning. However, recognizing the structure and content of unstructured tables using Vision Large Language Models (VLLMs) remains under-explored. To bridge this gap, we propose a benchmark based on a hierarchical design philosophy to evaluate the recognition capabilities of VLLMs in training-free scenarios. Through in-depth evaluations, we find that low-quality image input is a significant bottleneck in the recognition process. Drawing inspiration from this, we propose the Neighbor-Guided Toolchain Reasoner (NGTR) framework, which is characterized by integrating diverse lightweight tools for visual operations aimed at mitigating issues with low-quality images. Specifically, we transfer a tool selection experience from a similar neighbor to the input and design a reflection module to supervise the tool invocation process. Extensive experiments on public datasets demonstrate that our approach significantly enhances the recognition capabilities of the vanilla VLLMs. We believe that the benchmark and framework could provide an alternative solution to table recognition.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomy-of-Experts Models</title>
<link>https://arxiv.org/abs/2501.13074</link>
<guid>https://arxiv.org/abs/2501.13074</guid>
<content:encoded><![CDATA[
arXiv:2501.13074v2 Announce Type: replace-cross 
Abstract: Mixture-of-Experts (MoE) models mostly use a router to assign tokens to specific expert modules, activating only partial parameters and often outperforming dense models. We argue that the separation between the router's decision-making and the experts' execution is a critical yet overlooked issue, leading to suboptimal expert selection and ineffective learning. To address this, we propose Autonomy-of-Experts (AoE), a novel MoE paradigm in which experts autonomously select themselves to process inputs. AoE is based on the insight that an expert is aware of its own capacity to effectively process a token, an awareness reflected in the scale of its internal activations. In AoE, routers are removed; instead, experts pre-compute internal activations for inputs and are ranked based on their activation norms. Only the top-ranking experts proceed with the forward pass, while the others abort. The overhead of pre-computing activations is reduced through a low-rank weight factorization. This self-evaluating-then-partner-comparing approach ensures improved expert selection and effective learning. We pre-train language models having 700M up to 4B parameters, demonstrating that AoE outperforms traditional MoE models with comparable efficiency.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Domain Graph Foundation Models: Robust Knowledge Transfer via Topology Alignment</title>
<link>https://arxiv.org/abs/2502.02017</link>
<guid>https://arxiv.org/abs/2502.02017</guid>
<content:encoded><![CDATA[
arXiv:2502.02017v2 Announce Type: replace-cross 
Abstract: Recent advances in CV and NLP have inspired researchers to develop general-purpose graph foundation models through pre-training across diverse domains. However, a fundamental challenge arises from the substantial differences in graph topologies across domains. Additionally, real-world graphs are often sparse and prone to noisy connections and adversarial attacks. To address these issues, we propose the Multi-Domain Graph Foundation Model (MDGFM), a unified framework that aligns and leverages cross-domain topological information to facilitate robust knowledge transfer. MDGFM bridges different domains by adaptively balancing features and topology while refining original graphs to eliminate noise and align topological structures. To further enhance knowledge transfer, we introduce an efficient prompt-tuning approach. By aligning topologies, MDGFM not only improves multi-domain pre-training but also enables robust knowledge transfer to unseen domains. Theoretical analyses provide guarantees of MDGFM's effectiveness and domain generalization capabilities. Extensive experiments on both homophilic and heterophilic graph datasets validate the robustness and efficacy of our method.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Training-Free Length Extrapolation Approach for LLMs: Greedy Attention Logit Interpolation (GALI)</title>
<link>https://arxiv.org/abs/2502.02659</link>
<guid>https://arxiv.org/abs/2502.02659</guid>
<content:encoded><![CDATA[
arXiv:2502.02659v2 Announce Type: replace-cross 
Abstract: Transformer-based Large Language Models (LLMs) struggle with inputs exceeding their training context window due to positional out-of-distribution (O.O.D.) issues that disrupt attention. Existing solutions, including fine-tuning and training-free methods, face challenges like inefficiency, redundant interpolation, logit outliers, or loss of local positional information. We propose Greedy Attention Logit Interpolation (GALI), a training-free method that improves length extrapolation by greedily reusing pretrained positional intervals and interpolating attention logit to eliminate outliers. GALI achieves stable and superior performance across a wide range of long-context tasks without requiring input-length-specific tuning. Our analysis further reveals that LLMs interpret positional intervals unevenly and that restricting interpolation to narrower ranges improves performance, even on short-context tasks. GALI represents a step toward more robust and generalizable long-text processing in LLMs. Our implementation of GALI, along with the experiments from our paper, is open-sourced at https://github.com/adlnlp/Gali.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Masked Autoencoders Are Effective Tokenizers for Diffusion Models</title>
<link>https://arxiv.org/abs/2502.03444</link>
<guid>https://arxiv.org/abs/2502.03444</guid>
<content:encoded><![CDATA[
arXiv:2502.03444v2 Announce Type: replace-cross 
Abstract: Recent advances in latent diffusion models have demonstrated their effectiveness for high-resolution image synthesis. However, the properties of the latent space from tokenizer for better learning and generation of diffusion models remain under-explored. Theoretically and empirically, we find that improved generation quality is closely tied to the latent distributions with better structure, such as the ones with fewer Gaussian Mixture modes and more discriminative features. Motivated by these insights, we propose MAETok, an autoencoder (AE) leveraging mask modeling to learn semantically rich latent space while maintaining reconstruction fidelity. Extensive experiments validate our analysis, demonstrating that the variational form of autoencoders is not necessary, and a discriminative latent space from AE alone enables state-of-the-art performance on ImageNet generation using only 128 tokens. MAETok achieves significant practical improvements, enabling a gFID of 1.69 with 76x faster training and 31x higher inference throughput for 512x512 generation. Our findings show that the structure of the latent space, rather than variational constraints, is crucial for effective diffusion models. Code and trained models are released.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing limits for in-context learning</title>
<link>https://arxiv.org/abs/2502.03503</link>
<guid>https://arxiv.org/abs/2502.03503</guid>
<content:encoded><![CDATA[
arXiv:2502.03503v2 Announce Type: replace-cross 
Abstract: We examine limits of in-context learning (ICL) in transformer models trained from scratch, focusing on function approximation tasks as a controlled setting to uncover fundamental behaviors. While we show empirically that transformer models can generalize, approximating unseen classes of polynomial (non linear) functions, they cannot generalize beyond certain values. We provide both empirical and mathematical arguments explaining that these limitations stem from architectural components, namely layer normalization and the attention scoring function, softmax. Together, our findings reveal structural constraints on ICL that are often masked in more complex NLP tasks but that need to be understood to improve robustness and interpretability in transformer-based models.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ExpProof : Operationalizing Explanations for Confidential Models with ZKPs</title>
<link>https://arxiv.org/abs/2502.03773</link>
<guid>https://arxiv.org/abs/2502.03773</guid>
<content:encoded><![CDATA[
arXiv:2502.03773v4 Announce Type: replace-cross 
Abstract: In principle, explanations are intended as a way to increase trust in machine learning models and are often obligated by regulations. However, many circumstances where these are demanded are adversarial in nature, meaning the involved parties have misaligned interests and are incentivized to manipulate explanations for their purpose. As a result, explainability methods fail to be operational in such settings despite the demand \cite{bordt2022post}. In this paper, we take a step towards operationalizing explanations in adversarial scenarios with Zero-Knowledge Proofs (ZKPs), a cryptographic primitive. Specifically we explore ZKP-amenable versions of the popular explainability algorithm LIME and evaluate their performance on Neural Networks and Random Forests. Our code is publicly available at https://github.com/emlaufer/ExpProof.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safety Reasoning with Guidelines</title>
<link>https://arxiv.org/abs/2502.04040</link>
<guid>https://arxiv.org/abs/2502.04040</guid>
<content:encoded><![CDATA[
arXiv:2502.04040v2 Announce Type: replace-cross 
Abstract: Training safe LLMs remains a critical challenge. The most widely used method, Refusal Training (RT), struggles to generalize against various Out-of-Distribution (OOD) jailbreaking attacks. Although various advanced methods have been proposed to address this issue, we instead question whether OOD attacks inherently surpass the capability of vanilla RT. Evaluations using Best-of-N (BoN) reveal significant safety improvements as N increases, indicating models possess adequate latent safety knowledge but RT fails to consistently elicit it under OOD scenarios. Further domain adaptation analysis reveals that direct RT causes reliance on superficial shortcuts, resulting in non-generalizable representation mappings. Inspired by our findings, we propose training model to perform safety reasoning for each query. Specifically, we synthesize reasoning supervision aligned with specified guidelines that reflect diverse perspectives on safety knowledge. This encourages model to engage in deeper reasoning, explicitly eliciting and utilizing latent safety knowledge for each query. Extensive experiments show that our method significantly improves model generalization against OOD attacks.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WaferLLM: Large Language Model Inference at Wafer Scale</title>
<link>https://arxiv.org/abs/2502.04563</link>
<guid>https://arxiv.org/abs/2502.04563</guid>
<content:encoded><![CDATA[
arXiv:2502.04563v3 Announce Type: replace-cross 
Abstract: Emerging AI accelerators increasingly adopt wafer-scale manufacturing technologies, integrating hundreds of thousands of AI cores in a mesh architecture with large distributed on-chip memory (tens of GB in total) and ultra-high on-chip memory bandwidth (tens of PB/s). However, current LLM inference systems, optimized for shared memory architectures like GPUs, fail to exploit these accelerators fully.
  We introduce WaferLLM, the first wafer-scale LLM inference system. WaferLLM is guided by a novel PLMR model (pronounced as "Plummer") that captures the unique hardware characteristics of wafer-scale architectures. Leveraging this model, WaferLLM pioneers wafer-scale LLM parallelism, optimizing the utilization of hundreds of thousands of on-chip cores. It also introduces MeshGEMM and MeshGEMV, the first GEMM and GEMV implementations designed to scale effectively on wafer-scale accelerators.
  Evaluations show that WaferLLM achieves up to 200$\times$ higher accelerator utilization than state-of-the-art methods. Leveraging a wafer-scale accelerator (Cerebras WSE2), WaferLLM delivers GEMV operations 606$\times$ faster and 16$\times$ more energy-efficient than on an NVIDIA A100 GPU. For full LLM inference, WaferLLM achieves 10-20$\times$ speedups over A100 GPU clusters running SGLang and vLLM. These advantages are expected to grow as wafer-scale AI models, software, and hardware continue to mature. WaferLLM is open-sourced at https://github.com/MeshInfra/WaferLLM.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boost-and-Skip: A Simple Guidance-Free Diffusion for Minority Generation</title>
<link>https://arxiv.org/abs/2502.06516</link>
<guid>https://arxiv.org/abs/2502.06516</guid>
<content:encoded><![CDATA[
arXiv:2502.06516v2 Announce Type: replace-cross 
Abstract: Minority samples are underrepresented instances located in low-density regions of a data manifold, and are valuable in many generative AI applications, such as data augmentation, creative content generation, etc. Unfortunately, existing diffusion-based minority generators often rely on computationally expensive guidance dedicated for minority generation. To address this, here we present a simple yet powerful guidance-free approach called Boost-and-Skip for generating minority samples using diffusion models. The key advantage of our framework requires only two minimal changes to standard generative processes: (i) variance-boosted initialization and (ii) timestep skipping. We highlight that these seemingly-trivial modifications are supported by solid theoretical and empirical evidence, thereby effectively promoting emergence of underrepresented minority features. Our comprehensive experiments demonstrate that Boost-and-Skip greatly enhances the capability of generating minority samples, even rivaling guidance-based state-of-the-art approaches while requiring significantly fewer computations. Code is available at https://github.com/soobin-um/BnS.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Force Field: Few-shot Learning of Generalized Physical Reasoning</title>
<link>https://arxiv.org/abs/2502.08987</link>
<guid>https://arxiv.org/abs/2502.08987</guid>
<content:encoded><![CDATA[
arXiv:2502.08987v3 Announce Type: replace-cross 
Abstract: Physical reasoning is a remarkable human ability that enables rapid learning and generalization from limited experience. Current AI models, despite extensive training, still struggle to achieve similar generalization, especially in Out-of-distribution (OOD) settings. This limitation stems from their inability to abstract core physical principles from observations. A key challenge is developing representations that can efficiently learn and generalize physical dynamics from minimal data. Here we present Neural Force Field (NFF), a framework extending Neural Ordinary Differential Equation (NODE) to learn complex object interactions through force field representations, which can be efficiently integrated through an Ordinary Differential Equation (ODE) solver to predict object trajectories. Unlike existing approaches that rely on discrete latent spaces, NFF captures fundamental physical concepts such as gravity, support, and collision in continuous explicit force fields. Experiments on three challenging physical reasoning tasks demonstrate that NFF, trained with only a few examples, achieves strong generalization to unseen scenarios. This physics-grounded representation enables efficient forward-backward planning and rapid adaptation through interactive refinement. Our work suggests that incorporating physics-inspired representations into learning systems can help bridge the gap between artificial and human physical reasoning capabilities.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SparQLe: Speech Queries to Text Translation Through LLMs</title>
<link>https://arxiv.org/abs/2502.09284</link>
<guid>https://arxiv.org/abs/2502.09284</guid>
<content:encoded><![CDATA[
arXiv:2502.09284v3 Announce Type: replace-cross 
Abstract: With the growing influence of Large Language Models (LLMs), there is increasing interest in integrating speech representations with them to enable more seamless multi-modal processing and speech understanding. This study introduces a novel approach that combines self-supervised speech representations with instruction-tuned LLMs for speech-to-text translation. The proposed approach leverages a modality adapter to align extracted speech features with instruction-tuned LLMs using English speech data. Our experiments demonstrate that this method effectively preserves the semantic content of the input speech and serves as an effective bridge between self-supervised speech models and instruction-tuned LLMs, offering a promising approach for various speech understanding applications.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RaaS: Reasoning-Aware Attention Sparsity for Efficient LLM Reasoning</title>
<link>https://arxiv.org/abs/2502.11147</link>
<guid>https://arxiv.org/abs/2502.11147</guid>
<content:encoded><![CDATA[
arXiv:2502.11147v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated strong capabilities across various domains, with recent advancements in challenging reasoning tasks such as mathematics and programming. However, solving reasoning tasks often requires an LLM to generate long sequences, incurring $O(N)$ time and memory complexities per token, where $N$ is the current sequence length. To reduce complexities, existing sparsity-based algorithms propose to retain Key-Value (KV) vectors, the intermediate representations of only the most critical tokens. However, these algorithms struggle with the "impossible trinity" of accuracy, time, and memory. For example, the state-of-the-art algorithm, Quest, achieves high accuracy with $O(L)$ time but $O(N)$ memory ($L$ is the cache budget, $L \ll N$). To address the "impossible trinity", in this paper, we identify a new attention pattern during the decode stage of reasoning tasks, where milestone tokens (analogous to lemmas in mathematical proofs) emerge, are utilized, and then become unimportant afterward. Based on this pattern, we propose a new algorithm RaaS that identifies milestone tokens and retains their KV vectors until they are no longer needed, achieving high accuracy with $O(L)$ time and $O(L)$ memory complexities.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MuSC: Improving Complex Instruction Following with Multi-granularity Self-Contrastive Training</title>
<link>https://arxiv.org/abs/2502.11541</link>
<guid>https://arxiv.org/abs/2502.11541</guid>
<content:encoded><![CDATA[
arXiv:2502.11541v3 Announce Type: replace-cross 
Abstract: Complex instruction-following with elaborate constraints is imperative for Large Language Models (LLMs). While existing methods have constructed data for complex instruction alignment, they all rely on a more advanced model, especially GPT-4, limiting their application. In this paper, we propose a Multi-granularity Self-Contrastive Training (MuSC) framework, to improve the complex instruction alignment without relying on a stronger model. Our method is conducted on both coarse and fine granularity. On coarse-granularity, we construct constraint-aware preference data based on instruction decomposition and recombination. On fine-granularity, we perform token-aware preference optimization with dynamic token-level supervision. Our method is evaluated on open-sourced models, and experiment results show our method achieves significant improvement on both complex and general instruction-following benchmarks, surpassing previous self-alignment methods.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Agents Making Agent Tools</title>
<link>https://arxiv.org/abs/2502.11705</link>
<guid>https://arxiv.org/abs/2502.11705</guid>
<content:encoded><![CDATA[
arXiv:2502.11705v2 Announce Type: replace-cross 
Abstract: Tool use has turned large language models (LLMs) into powerful agents that can perform complex multi-step tasks by dynamically utilising external software components. However, these tools must be implemented in advance by human developers, hindering the applicability of LLM agents in domains demanding large numbers of highly specialised tools, like in life sciences and medicine. Motivated by the growing trend of scientific studies accompanied by public code repositories, we propose ToolMaker, an agentic framework that autonomously transforms papers with code into LLM-compatible tools. Given a GitHub URL and short task description, ToolMaker autonomously installs dependencies and generates code to perform the task, using a closed-loop self-correction mechanism for debugging. To evaluate our approach, we introduce a benchmark comprising 15 complex computational tasks spanning various domains with over 100 unit tests to assess correctness and robustness. Our method correctly implements 80% of the tasks, substantially outperforming current state-of-the-art software engineering agents. ToolMaker therefore is a step towards fully autonomous agent-based scientific workflows. Our code and benchmark are publicly available at https://github.com/KatherLab/ToolMaker.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BaxBench: Can LLMs Generate Correct and Secure Backends?</title>
<link>https://arxiv.org/abs/2502.11844</link>
<guid>https://arxiv.org/abs/2502.11844</guid>
<content:encoded><![CDATA[
arXiv:2502.11844v3 Announce Type: replace-cross 
Abstract: Automatic program generation has long been a fundamental challenge in computer science. Recent benchmarks have shown that large language models (LLMs) can effectively generate code at the function level, make code edits, and solve algorithmic coding tasks. However, to achieve full automation, LLMs should be able to generate production-quality, self-contained application modules. To evaluate the capabilities of LLMs in solving this challenge, we introduce BaxBench, a novel evaluation benchmark consisting of 392 tasks for the generation of backend applications. We focus on backends for three critical reasons: (i) they are practically relevant, building the core components of most modern web and cloud software, (ii) they are difficult to get right, requiring multiple functions and files to achieve the desired functionality, and (iii) they are security-critical, as they are exposed to untrusted third-parties, making secure solutions that prevent deployment-time attacks an imperative. BaxBench validates the functionality of the generated applications with comprehensive test cases, and assesses their security exposure by executing end-to-end exploits. Our experiments reveal key limitations of current LLMs in both functionality and security: (i) even the best model, OpenAI o1, achieves a mere 62% on code correctness; (ii) on average, we could successfully execute security exploits on around half of the correct programs generated by each LLM; and (iii) in less popular backend frameworks, models further struggle to generate correct and secure applications. Progress on BaxBench signifies important steps towards autonomous and secure software development with LLMs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking Through Options</title>
<link>https://arxiv.org/abs/2502.12929</link>
<guid>https://arxiv.org/abs/2502.12929</guid>
<content:encoded><![CDATA[
arXiv:2502.12929v2 Announce Type: replace-cross 
Abstract: We present a novel reasoning approach called Flow-of-Options (FoO), designed to address intrinsic biases in Large Language Models (LLMs). Flow-of-Options enables LLMs to systematically explore a diverse range of possibilities in their reasoning, as demonstrated by an FoO-based agentic framework developed for autonomously solving Machine Learning (ML) tasks. FoO enforces diversity in LLM solutions through compressed and interpretable task representations, resulting in improvements of 38.2% - 69.2% on standard data science tasks, and 37.4% - 47.9% on therapeutic chemistry tasks, as compared to state-of-the-art baselines. With an overall operation cost under $1 per task, our framework is well-suited for cost-sensitive applications. Going beyond tabular classification and regression, we show the broader applicability of our FoO-based agentic system to tasks such as reinforcement learning and image generation. Our code is open-sourced at: https://github.com/flagshippioneering/Flow-of-Options.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SongGen: A Single Stage Auto-regressive Transformer for Text-to-Song Generation</title>
<link>https://arxiv.org/abs/2502.13128</link>
<guid>https://arxiv.org/abs/2502.13128</guid>
<content:encoded><![CDATA[
arXiv:2502.13128v2 Announce Type: replace-cross 
Abstract: Text-to-song generation, the task of creating vocals and accompaniment from textual inputs, poses significant challenges due to domain complexity and data scarcity. Existing approaches often employ multi-stage generation procedures, leading to cumbersome training and inference pipelines, as well as suboptimal overall generation quality due to error accumulation across stages. In this paper, we propose SongGen, a fully open-source, single-stage auto-regressive transformer designed for controllable song generation. The proposed model facilitates fine-grained control over diverse musical attributes, including lyrics and textual descriptions of instrumentation, genre, mood, and timbre, while also offering an optional three-second reference clip for voice cloning. Within a unified auto-regressive framework, SongGen supports two output modes: mixed mode, which generates a mixture of vocals and accompaniment directly, and dual-track mode, which synthesizes them separately for greater flexibility in downstream applications. We explore diverse token pattern strategies for each mode, leading to notable improvements and valuable insights. Furthermore, we design an automated data preprocessing pipeline with effective quality control. To foster community engagement and future research, we will release our model weights, training code, annotated data, and preprocessing pipeline. The code is available at https://github.com/LiuZH-19/SongGen.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Repo2Run: Automated Building Executable Environment for Code Repository at Scale</title>
<link>https://arxiv.org/abs/2502.13681</link>
<guid>https://arxiv.org/abs/2502.13681</guid>
<content:encoded><![CDATA[
arXiv:2502.13681v3 Announce Type: replace-cross 
Abstract: Scaling up executable code data is significant for improving language models' software engineering capability. The intricate nature of the process makes it labor-intensive, time-consuming and expert-knowledge-dependent to build a large number of executable code repositories, limiting the scalability of existing work based on running tests. The primary bottleneck lies in the automated building of test environments for different repositories, which is an essential yet underexplored task. To mitigate the gap, we introduce Repo2Run, the first LLM-based agent aiming at automating the building of executable test environments for any repositories at scale. Specifically, given a code repository, Repo2Run iteratively builds the Docker image, runs unit tests based on the feedback of the building, and synthesizes the Dockerfile until the entire pipeline is executed successfully. The resulting Dockerfile can then be used to create Docker container environments for running code and tests. We created a benchmark containing 420 Python repositories with unit tests for evaluation. The results illustrate that Repo2Run achieves an 86.0% success rate, outperforming SWE-agent by 77.0%. The resources of Repo2Run are available at https://github.com/bytedance/Repo2Run.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Text-Driven 360-Degree Panorama Generation</title>
<link>https://arxiv.org/abs/2502.14799</link>
<guid>https://arxiv.org/abs/2502.14799</guid>
<content:encoded><![CDATA[
arXiv:2502.14799v2 Announce Type: replace-cross 
Abstract: The advent of text-driven 360-degree panorama generation, enabling the synthesis of 360-degree panoramic images directly from textual descriptions, marks a transformative advancement in immersive visual content creation. This innovation significantly simplifies the traditionally complex process of producing such content. Recent progress in text-to-image diffusion models has accelerated the rapid development in this emerging field. This survey presents a comprehensive review of text-driven 360-degree panorama generation, offering an in-depth analysis of state-of-the-art algorithms and their expanding applications in 360-degree 3D scene generation. Furthermore, we critically examine current limitations and propose promising directions for future research. A curated project page with relevant resources and research papers is available at https://littlewhitesea.github.io/Text-Driven-Pano-Gen/.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Middle-Layer Representation Alignment for Cross-Lingual Transfer in Fine-Tuned LLMs</title>
<link>https://arxiv.org/abs/2502.14830</link>
<guid>https://arxiv.org/abs/2502.14830</guid>
<content:encoded><![CDATA[
arXiv:2502.14830v2 Announce Type: replace-cross 
Abstract: While large language models demonstrate remarkable capabilities at task-specific applications through fine-tuning, extending these benefits across diverse languages is essential for broad accessibility. However, effective cross-lingual transfer is hindered by LLM performance gaps across languages and the scarcity of fine-tuning data in many languages. Through analysis of LLM internal representations from over 1,000+ language pairs, we discover that middle layers exhibit the strongest potential for cross-lingual alignment. Building on this finding, we propose a middle-layer alignment objective integrated into task-specific training. Our experiments on slot filling, machine translation, and structured text generation show consistent improvements in cross-lingual transfer, especially to lower-resource languages. The method is robust to the choice of alignment languages and generalizes to languages unseen during alignment. Furthermore, we show that separately trained alignment modules can be merged with existing task-specific modules, improving cross-lingual capabilities without full re-training. Our code is publicly available (https://github.com/dannigt/mid-align).
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TETRIS: Optimal Draft Token Selection for Batch Speculative Decoding</title>
<link>https://arxiv.org/abs/2502.15197</link>
<guid>https://arxiv.org/abs/2502.15197</guid>
<content:encoded><![CDATA[
arXiv:2502.15197v2 Announce Type: replace-cross 
Abstract: We propose TETRIS, a novel method that optimizes the total throughput of batch speculative decoding in multi-request settings. Unlike existing methods that optimize for a single request or a group of requests as a whole, TETRIS actively selects the most promising draft tokens (for every request in a batch) to be accepted when verified in parallel, resulting in fewer rejected tokens and hence less wasted computing resources. Such an effective resource utilization to achieve fast inference in large language models (LLMs) is especially important to service providers with limited inference capacity. Compared to baseline speculative decoding, TETRIS yields a consistently higher acceptance rate and more effective utilization of the limited inference capacity. We show theoretically and empirically that TETRIS outperforms baseline speculative decoding and existing methods that dynamically select draft tokens, leading to a more efficient batch inference in LLMs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PairBench: Are Vision-Language Models Reliable at Comparing What They See?</title>
<link>https://arxiv.org/abs/2502.15210</link>
<guid>https://arxiv.org/abs/2502.15210</guid>
<content:encoded><![CDATA[
arXiv:2502.15210v3 Announce Type: replace-cross 
Abstract: Understanding how effectively large vision language models (VLMs) compare visual inputs is crucial across numerous applications, yet this fundamental capability remains insufficiently assessed. While VLMs are increasingly deployed for tasks requiring comparative judgment, including automated evaluation, re-ranking, and retrieval-augmented generation, no systematic framework exists to measure their performance in these scenarios. We present PairBench, a simple framework that evaluates VLMs as customizable similarity tools using widely available image datasets. Our approach introduces four key metrics for reliable comparison: alignment with human annotations, consistency across pair ordering, distribution smoothness, and controllability through prompting. Our analysis reveals that no model consistently excels across all metrics, with each demonstrating distinct strengths and weaknesses. Most concerning is the widespread inability of VLMs to maintain symmetric similarity scores. Interestingly, we demonstrate that performance on our benchmark strongly correlates with popular benchmarks used for more complex tasks, while providing additional metrics into controllability, smoothness and ordering. This makes PairBench a unique and comprehensive framework to evaluate the performance of VLMs for automatic evaluation depending on the task.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs Complex Reasoning</title>
<link>https://arxiv.org/abs/2502.15401</link>
<guid>https://arxiv.org/abs/2502.15401</guid>
<content:encoded><![CDATA[
arXiv:2502.15401v2 Announce Type: replace-cross 
Abstract: In-context learning (ICL) can significantly enhance the complex reasoning capabilities of large language models (LLMs), with the key lying in the selection and ordering of demonstration examples. Previous methods typically relied on simple features to measure the relevance between examples. We argue that these features are not sufficient to reflect the intrinsic connections between examples. In this study, we propose a curriculum ICL strategy guided by problem-solving logic. We select demonstration examples by analyzing the problem-solving logic and order them based on curriculum learning. Specifically, we constructed a problem-solving logic instruction set based on the BREAK dataset and fine-tuned a language model to analyze the problem-solving logic of examples. Subsequently, we selected appropriate demonstration examples based on problem-solving logic and assessed their difficulty according to the number of problem-solving steps. In accordance with the principles of curriculum learning, we ordered the examples from easy to hard to serve as contextual prompts. Experimental results on multiple benchmarks indicate that our method outperforms previous ICL approaches in terms of performance and efficiency, effectively enhancing the complex reasoning capabilities of LLMs. Our project will be released at https://github.com/maxuetao/CurriculumICL
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Conformal Guidance: A Framework for Multi-Domain Learning under Uncertainty</title>
<link>https://arxiv.org/abs/2502.16736</link>
<guid>https://arxiv.org/abs/2502.16736</guid>
<content:encoded><![CDATA[
arXiv:2502.16736v3 Announce Type: replace-cross 
Abstract: Learning with guidance has proven effective across a wide range of machine learning domains. Guidance may, for example, come from annotated datasets in supervised learning, pseudo-labels in semi-supervised learning, and expert demonstration policies in reinforcement learning. However, guidance signals can be noisy due to domain shifts and limited data availability and may not generalize well. Blindly trusting such signals when they are noisy, incomplete, or misaligned with the target domain can lead to degraded performance. To address these challenges, we propose $\underline{Ada}$ptive $\underline{Con}$formal $\underline{G}$uidance (AdaConG), a universal, plug-and-play framework that dynamically modulates the influence of guidance signals based on their associated uncertainty, quantified via split conformal prediction (CP). By adaptively adjusting to guidance uncertainty, AdaConG enables models to reduce reliance on potentially misleading signals and enhance learning performance. We validate AdaConG across diverse domains and tasks, including knowledge distillation, semi-supervised image classification, gridworld navigation, and autonomous driving. Experimental results demonstrate that AdaConG improves performance and robustness under imperfect guidance, e.g., in gridworld navigation, it accelerates convergence and achieves over $6\times$ higher rewards than the best-performing baseline. These results highlight AdaConG as a simple yet effective solution for multi-domain learning under uncertainty.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recurrent Knowledge Identification and Fusion for Language Model Continual Learning</title>
<link>https://arxiv.org/abs/2502.17510</link>
<guid>https://arxiv.org/abs/2502.17510</guid>
<content:encoded><![CDATA[
arXiv:2502.17510v2 Announce Type: replace-cross 
Abstract: Continual learning (CL) is crucial for deploying large language models (LLMs) in dynamic real-world environments without costly retraining. While recent model ensemble and model merging methods guided by parameter importance have gained popularity, they often struggle to balance knowledge transfer and forgetting, mainly due to the reliance on static importance estimates during sequential training. In this paper, we present Recurrent-KIF, a novel CL framework for Recurrent Knowledge Identification and Fusion, which enables dynamic estimation of parameter importance distributions to enhance knowledge transfer. Inspired by human continual learning, Recurrent-KIF employs an inner loop that rapidly adapts to new tasks while identifying important parameters, coupled with an outer loop that globally manages the fusion of new and historical knowledge through redundant knowledge pruning and key knowledge merging. These inner-outer loops iteratively perform multiple rounds of fusion, allowing Recurrent-KIF to leverage intermediate training information and adaptively adjust fusion strategies based on evolving importance distributions. Extensive experiments on two CL benchmarks with various model sizes (from 770M to 13B) demonstrate that Recurrent-KIF effectively mitigates catastrophic forgetting and enhances knowledge transfer.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAML: Collaborative Auxiliary Modality Learning for Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2502.17821</link>
<guid>https://arxiv.org/abs/2502.17821</guid>
<content:encoded><![CDATA[
arXiv:2502.17821v2 Announce Type: replace-cross 
Abstract: Multi-modal learning has become a crucial technique for improving the performance of machine learning applications across domains such as autonomous driving, robotics, and perception systems. However, in certain scenarios, particularly in resource-constrained environments, some modalities available during training may be absent during inference. While existing frameworks effectively utilize multiple data sources during training and enable inference with reduced modalities, they are primarily designed for single-agent settings. This poses a critical limitation in dynamic environments such as connected autonomous vehicles (CAV), where incomplete data coverage can lead to decision-making blind spots. Conversely, some works explore multi-agent collaboration but without addressing missing modality at test time. To overcome these limitations, we propose Collaborative Auxiliary Modality Learning (CAML), a novel multi-modal multi-agent framework that enables agents to collaborate and share multi-modal data during training, while allowing inference with reduced modalities during testing. Experimental results in collaborative decision-making for CAV in accident-prone scenarios demonstrate that CAML achieves up to a ${\bf 58.1}\%$ improvement in accident detection. Additionally, we validate CAML on real-world aerial-ground robot data for collaborative semantic segmentation, achieving up to a ${\bf 10.6}\%$ improvement in mIoU.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Classification: Leveraging Self-Supervised Classification to Enhance Prediction Performance</title>
<link>https://arxiv.org/abs/2502.18891</link>
<guid>https://arxiv.org/abs/2502.18891</guid>
<content:encoded><![CDATA[
arXiv:2502.18891v2 Announce Type: replace-cross 
Abstract: In this study, we propose an innovative dynamic classification algorithm aimed at achieving zero missed detections and minimal false positives,acritical in safety-critical domains (e.g., medical diagnostics) where undetected cases risk severe outcomes. The algorithm partitions data in a self-supervised learning-generated way, which allows the model to learn from the training set to understand the data distribution and thereby divides training set and test set into N different subareas. The training and test subsets in the same subarea will have nearly the same boundary. For each subarea, there will be the same type of model, such as linear or random forest model, to predict the results of that subareas. In addition, the algorithm uses subareas boundary to refine predictions results and filter out substandard results without requiring additional models. This approach allows each model to operate within a smaller data range and remove the inaccurate prediction results, thereby improving overall accuracy. Experimental results show that, with minimal data partitioning errors, the algorithm achieves exceptional performance with zero missed detections and minimal false positives, outperforming existing ensembles like XGBoost or LGBM model. Even with larger classification errors, it remains comparable to that of state-of-the-art models.
  Key innovations include self-supervised classification learning, small-range subset predictions, and optimizing the prediction results and eliminate the unqualified ones without the need for additional model support. Although the algorithm still has room for improvement in automatic parameter tuning and efficiency, it demonstrates outstanding performance across multiple datasets. Future work will focus on optimizing the classification components to enhance robustness and adaptability.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>