<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CL updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CL</link>

<item>
<title>Enhancing Biomedical Named Entity Recognition using GLiNER-BioMed with Targeted Dictionary-Based Post-processing for BioASQ 2025 task 6</title>
<link>https://arxiv.org/abs/2510.08588</link>
<guid>https://arxiv.org/abs/2510.08588</guid>
<content:encoded><![CDATA[
<div> Named Entity Recognition, BioASQ, GLiNER-BioMed model, dictionary-based post-processing, misclassifications <br>
<br>
Summary: 
Biomedical Named Entity Recognition (BioNER) is essential for extracting information from scientific literature, but distinguishing between similar entity types like genes and chemicals poses challenges. This study evaluates the GLiNER-BioMed model on a BioASQ dataset and introduces a targeted dictionary-based post-processing strategy to address common misclassifications. While the post-processing approach improved the micro F1-score on the development set, it did not generalize well to the blind test set. Insights from exploring alternative methodologies, including Conditional Random Fields, were discussed. The study highlights the potential of dictionary-based refinement for pre-trained BioNER models but emphasizes the critical challenge of overfitting to development data. Ensuring robust generalization is crucial for real-world applicability.<br><br>Summary: <div>
arXiv:2510.08588v1 Announce Type: new 
Abstract: Biomedical Named Entity Recognition (BioNER), task6 in BioASQ (A challenge in large-scale biomedical semantic indexing and question answering), is crucial for extracting information from scientific literature but faces hurdles such as distinguishing between similar entity types like genes and chemicals. This study evaluates the GLiNER-BioMed model on a BioASQ dataset and introduces a targeted dictionary-based post-processing strategy to address common misclassifications. While this post-processing approach demonstrated notable improvement on our development set, increasing the micro F1-score from a baseline of 0.79 to 0.83, this enhancement did not generalize to the blind test set, where the post-processed model achieved a micro F1-score of 0.77 compared to the baselines 0.79. We also discuss insights gained from exploring alternative methodologies, including Conditional Random Fields. This work highlights the potential of dictionary-based refinement for pre-trained BioNER models but underscores the critical challenge of overfitting to development data and the necessity of ensuring robust generalization for real-world applicability.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Less Diverse, Less Safe: The Indirect But Pervasive Risk of Test-Time Scaling in Large Language Models</title>
<link>https://arxiv.org/abs/2510.08592</link>
<guid>https://arxiv.org/abs/2510.08592</guid>
<content:encoded><![CDATA[
<div> Keywords: Test-Time Scaling, LLM reasoning, candidate diversity, unsafe outputs, diversity reduction protocol

Summary: 
Test-Time Scaling (TTS) aims to improve language model reasoning by exploring multiple candidate responses to find the best output. However, a lack of candidate diversity can lead to an increased likelihood of producing unsafe outputs. This study introduces a reference-guided diversity reduction protocol (RefDiv) to stress test TTS pipelines and demonstrates that constraining diversity significantly impacts the generation of unsafe results. This phenomenon is observed across various TTS strategies and models, highlighting the general nature of the problem. Existing safety guardrail classifiers are found to be ineffective in detecting adversarial input prompts generated through diversity reduction. The findings emphasize the importance of designing robust TTS strategies that are resilient to diversity-targeted stress tests, such as those illustrated by RefDiv. 

<br><br>Summary: <div>
arXiv:2510.08592v1 Announce Type: new 
Abstract: Test-Time Scaling (TTS) improves LLM reasoning by exploring multiple candidate responses and then operating over this set to find the best output. A tacit premise behind TTS is that sufficiently diverse candidate pools enhance reliability. In this work, we show that this assumption in TTS introduces a previously unrecognized failure mode. When candidate diversity is curtailed, even by a modest amount, TTS becomes much more likely to produce unsafe outputs. We present a reference-guided diversity reduction protocol (RefDiv) that serves as a diagnostic attack to stress test TTS pipelines. Through extensive experiments across four open-source models (Qwen3, Mistral, Llama3.1, Gemma3) and two widely used TTS strategies (Monte Carlo Tree Search and Best-of-N), constraining diversity consistently signifies the rate at which TTS produces unsafe results. The effect is often stronger than that produced by prompts directly with high adversarial intent scores. This observed phenomenon also transfers across TTS strategies and to closed-source models (e.g. OpenAI o3 and Gemini-2.5-Pro), thus indicating that this is a general and extant property of TTS rather than a model-specific artifact. Additionally, we find that numerous widely used safety guardrail classifiers (e.g. Llama-Guard and OpenAI Moderation API), are unable to flag the adversarial input prompts generated by RefDiv, demonstrating that existing defenses offer limited protection against this diversity-driven failure mode. Through this work, we hope to motivate future research on designing robust TTS strategies that are both effective and secure against diversity-targeted stress tests as illustrated by RefDiv.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Self-Supervised Representation Learning for Depression Detection from Speech</title>
<link>https://arxiv.org/abs/2510.08593</link>
<guid>https://arxiv.org/abs/2510.08593</guid>
<content:encoded><![CDATA[
<div> SSL, depression detection, speech, HAREN-CTC, multitask learning 

Summary:
HAREN-CTC is a novel architecture for speech-based depression detection that integrates multi-layer SSL features using cross-attention in a multitask learning framework. It includes a Hierarchical Adaptive Clustering module for reorganizing SSL features and a Cross-Modal Fusion module for modeling inter-layer dependencies. The use of Connectionist Temporal Classification loss enables alignment-aware training to track irregular temporal patterns of depressive speech cues. Evaluation of HAREN-CTC demonstrates state-of-the-art performance with macro F1-scores of 0.81 on DAIC-WOZ and 0.82 on MODMA, outperforming previous methods in both standard data splits and a generalization setting with five-fold cross-validation. <div>
arXiv:2510.08593v1 Announce Type: new 
Abstract: Speech-based depression detection (SDD) is a promising, non-invasive alternative to traditional clinical assessments. However, it remains limited by the difficulty of extracting meaningful features and capturing sparse, heterogeneous depressive cues over time. Pretrained self-supervised learning (SSL) models such as WavLM provide rich, multi-layer speech representations, yet most existing SDD methods rely only on the final layer or search for a single best-performing one. These approaches often overfit to specific datasets and fail to leverage the full hierarchical structure needed to detect subtle and persistent depression signals.
  To address this challenge, we propose HAREN-CTC, a novel architecture that integrates multi-layer SSL features using cross-attention within a multitask learning framework, combined with Connectionist Temporal Classification loss to handle sparse temporal supervision. HAREN-CTC comprises two key modules: a Hierarchical Adaptive Clustering module that reorganizes SSL features into complementary embeddings, and a Cross-Modal Fusion module that models inter-layer dependencies through cross-attention. The CTC objective enables alignment-aware training, allowing the model to track irregular temporal patterns of depressive speech cues.
  We evaluate HAREN-CTC under both an upper-bound setting with standard data splits and a generalization setting using five-fold cross-validation. The model achieves state-of-the-art macro F1-scores of 0.81 on DAIC-WOZ and 0.82 on MODMA, outperforming prior methods across both evaluation scenarios.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Systematic Diagnosis of Brittle Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2510.08595</link>
<guid>https://arxiv.org/abs/2510.08595</guid>
<content:encoded><![CDATA[
<div> framework, mathematical reasoning, gpt-3.5-turbo, gpt-4o-mini, reasoning modes <br>
Summary: <br>
A new framework for measuring mathematical reasoning that goes beyond standard benchmarks is proposed in the article. The framework utilizes gpt-3.5-turbo to generate structured reasoning on the GSM8K dataset and gpt-4o-mini for error categorization and unsupervised clustering of reasoning sentences to identify emerging "reasoning modes." The analysis reveals a cognitive profile of the machine learning model with strengths in procedural modes like sequential calculation but weaknesses in modes requiring combinatorial reasoning with restrictions. This brittleness in specific reasoning skills indicates a nonhuman-like performance. By quantifying the reliability of different reasoning skills, the study offers a more detailed way to evaluate mathematical comprehension and suggests a roadmap for enhancing the model's capabilities for future applications. <div>
arXiv:2510.08595v1 Announce Type: new 
Abstract: A central question in artificial intelligence is the extent to which machine learning models comprehend mathematics. To address this, we propose a novel framework for measuring mathematical reasoning that moves beyond standard benchmarks to diagnose specific failure points. Our method first generates structured, step-by-step reasoning from gpt-3.5-turbo on the GSM8K dataset. We then use a more capable analyst model, gpt-4o-mini, to categorize errors and, crucially, perform an unsupervised clustering of every reasoning sentence to identify emergent "reasoning modes." This analysis reveals a cognitive profile with a stark, nonhuman-like brittleness: while the model achieves near-perfect accuracy on procedural modes like sequential calculation, its performance on modes requiring combinatorial reasoning with restrictions plummets. By identifying and quantifying the reliability of these distinct reasoning skills, our work provides a more granular method to evaluate mathematical comprehension and offers a precise roadmap for developing new capabilities and more reliable future applications.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Confidence, Not Perplexity: A Better Metric for the Creative Era of LLMs</title>
<link>https://arxiv.org/abs/2510.08596</link>
<guid>https://arxiv.org/abs/2510.08596</guid>
<content:encoded><![CDATA[
<div> Confidence Score, biased metrics, creative text generation, gpt-4o-mini, evaluation<br>
Summary:<br>
The study introduces the Confidence Score (CS) as a less biased alternative to traditional metrics like self-perplexity for evaluating creative text generation. Experiments conducted on gpt-4o-mini demonstrate that while conventional fluency-based metrics show a preference for familiar responses in all cases on 99 creative prompts, the CS favors novel responses in 19% of instances, signifying a significant difference. The CS is proven effective in distinguishing between easy, medium, and hard tasks, with non-overlapping confidence intervals validating this capability. By mitigating the creativity bias present in standard metrics while preserving their fundamental assessment strengths, the Confidence Score offers a more balanced and comprehensive evaluation method for modern language modeling models.<br> <div>
arXiv:2510.08596v1 Announce Type: new 
Abstract: Reference-free metrics like self-perplexity are strongly biased against creative text generation. We propose the Confidence Score (CS), derived from a model's output probability distribution, as a less biased alternative. Experiments on gpt-4o-mini show that while fluency-based metrics prefer novel responses in 0\% of cases on 99 creative prompts, our CS does so 19% of the time, a statistically significant difference (95% CI for difference: [11.1%, 27.3%]). We also show that CS effectively distinguishes between easy, medium, and hard tasks, confirmed by non-overlapping confidence intervals. The Confidence Score thus mitigates the creativity bias of traditional metrics while retaining their core evaluative strengths, offering a more balanced assessment for modern LLMs.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recover-LoRA: Data-Free Accuracy Recovery of Degraded Language Models via Low-Rank Adaptation</title>
<link>https://arxiv.org/abs/2510.08600</link>
<guid>https://arxiv.org/abs/2510.08600</guid>
<content:encoded><![CDATA[
<div> Keywords: Inference optimizations, language model, Recover-LoRA, model accuracy recovery, synthetic data<br>
Summary:<br>
The article introduces Recover-LoRA, a method designed to recover model accuracies in degraded language models caused by various inference optimizations. Recover-LoRA utilizes synthetic data and logit distillation to learn LoRA adapters on selective layers, aligning the degraded model with its full precision version. The study examines the application of Recover-LoRA on small language models (SLMs) with different attention architectures, including multi-head attention (MHA) and group-query attention (GQA), across various evaluation datasets. Results demonstrate that Recover-LoRA can improve model accuracies by 5-17% in MHA and GQA SLMs. This research highlights the importance of recovering accuracy in language models that have experienced functional degradations, offering a lightweight and dataset agnostic solution for model accuracy recovery. <br> <div>
arXiv:2510.08600v1 Announce Type: new 
Abstract: Inference optimizations such as quantization, pruning, format and datatype conversion, model export, and serialization can lead to functional degradations in language model task performance. While most efforts on performance recovery for deployment focus on robust quantization techniques, we focus on recovering model accuracies from any sources that degrade model weights, such as improper model serialization. In this work, we propose Recover-LoRA, a lightweight and dataset agnostic method to recover accuracy in degraded models. Recover-LoRA uses synthetic data and logit distillation to learn LoRA adapters on selective layers that facilitate aligning the degraded model to its full precision model. We investigate the utility of Recover-LoRA across a diverse set of small language models (SLMs), including models with varying attention architectures, multi-head attention (MHA) and group-query attention (GQA), as well as several evaluation datasets. Our results show that Recover-LoRA recovers model accuracies by 5-17% on MHA and GQA SLMs.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mnemosyne: An Unsupervised, Human-Inspired Long-Term Memory Architecture for Edge-Based LLMs</title>
<link>https://arxiv.org/abs/2510.08601</link>
<guid>https://arxiv.org/abs/2510.08601</guid>
<content:encoded><![CDATA[
<div> Memory, Long-term, Dialogue, Mnemosyne, Healthcare

Summary:
Mnemosyne is a new unsupervised long-term memory architecture designed for edge-based large language models (LLMs). It uses graph-structured storage, substance and redundancy filters, memory committing and pruning mechanisms, and probabilistic recall with temporal decay. Mnemosyne also introduces a concentrated "core summary" to capture user personality and domain-specific details. In healthcare applications, it can store information like post-recovery ambitions and attitude towards care. In experiments with longitudinal healthcare dialogues, Mnemosyne outperformed baseline methods in realism and long-term memory capability. It achieved high scores in temporal reasoning and single-hop retrieval, showcasing improved factual recall and natural user-facing responses. This edge-compatible memory architecture offers enhanced performance and transferability for LLMs. 

<br><br>Summary: <div>
arXiv:2510.08601v1 Announce Type: new 
Abstract: Long-term memory is essential for natural, realistic dialogue. However, current large language model (LLM) memory systems rely on either brute-force context expansion or static retrieval pipelines that fail on edge-constrained devices. We introduce Mnemosyne, an unsupervised, human-inspired long-term memory architecture designed for edge-based LLMs. Our approach uses graph-structured storage, modular substance and redundancy filters, memory committing and pruning mechanisms, and probabilistic recall with temporal decay and refresh processes modeled after human memory. Mnemosyne also introduces a concentrated "core summary" efficiently derived from a fixed-length subset of the memory graph to capture the user's personality and other domain-specific long-term details such as, using healthcare application as an example, post-recovery ambitions and attitude towards care. Unlike existing retrieval-augmented methods, Mnemosyne is designed for use in longitudinal healthcare assistants, where repetitive and semantically similar but temporally distinct conversations are limited by naive retrieval. In experiments with longitudinal healthcare dialogues, Mnemosyne demonstrates the highest win rate of 65.8% in blind human evaluations of realism and long-term memory capability compared to a baseline RAG win rate of 31.1%. Mnemosyne also achieves current highest LoCoMo benchmark scores in temporal reasoning and single-hop retrieval compared to other same-backboned techniques. Further, the average overall score of 54.6% was second highest across all methods, beating commonly used Mem0 and OpenAI baselines among others. This demonstrates that improved factual recall, enhanced temporal reasoning, and much more natural user-facing responses can be feasible with an edge-compatible and easily transferable unsupervised memory architecture.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human Texts Are Outliers: Detecting LLM-generated Texts via Out-of-distribution Detection</title>
<link>https://arxiv.org/abs/2510.08602</link>
<guid>https://arxiv.org/abs/2510.08602</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, AI-generated text, detection methods, out-of-distribution, one-class learning

Summary: 
The article discusses the need for reliable detection methods to differentiate between human and machine-generated text, focusing on the limitations of existing binary classification approaches. It argues that human texts are diverse and do not conform to a single distribution, making it challenging for classifiers to generalize effectively. The proposed approach reframes the detection task as an out-of-distribution (OOD) problem, treating human-written texts as outliers. By utilizing one-class learning methods and score-based learning techniques, the framework achieves robust and generalizable performance. Extensive experiments across various datasets validate the efficacy of the OOD-based method, showcasing high AUROC and AUPR scores with low FPR95. Moreover, the framework demonstrates resilience and adaptability in multilingual, attacked, and unseen-model and -domain text settings. The code, pretrained weights, and demo of the framework will be made available. <br><br>Summary: <div>
arXiv:2510.08602v1 Announce Type: new 
Abstract: The rapid advancement of large language models (LLMs) such as ChatGPT, DeepSeek, and Claude has significantly increased the presence of AI-generated text in digital communication. This trend has heightened the need for reliable detection methods to distinguish between human-authored and machine-generated content. Existing approaches both zero-shot methods and supervised classifiers largely conceptualize this task as a binary classification problem, often leading to poor generalization across domains and models. In this paper, we argue that such a binary formulation fundamentally mischaracterizes the detection task by assuming a coherent representation of human-written texts. In reality, human texts do not constitute a unified distribution, and their diversity cannot be effectively captured through limited sampling. This causes previous classifiers to memorize observed OOD characteristics rather than learn the essence of `non-ID' behavior, limiting generalization to unseen human-authored inputs. Based on this observation, we propose reframing the detection task as an out-of-distribution (OOD) detection problem, treating human-written texts as distributional outliers while machine-generated texts are in-distribution (ID) samples. To this end, we develop a detection framework using one-class learning method including DeepSVDD and HRN, and score-based learning techniques such as energy-based method, enabling robust and generalizable performance. Extensive experiments across multiple datasets validate the effectiveness of our OOD-based approach. Specifically, the OOD-based method achieves 98.3% AUROC and AUPR with only 8.9% FPR95 on DeepFake dataset. Moreover, we test our detection framework on multilingual, attacked, and unseen-model and -domain text settings, demonstrating the robustness and generalizability of our framework. Code, pretrained weights, and demo will be released.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>YpathRAG:A Retrieval-Augmented Generation Framework and Benchmark for Pathology</title>
<link>https://arxiv.org/abs/2510.08603</link>
<guid>https://arxiv.org/abs/2510.08603</guid>
<content:encoded><![CDATA[
<div> vector database, pathology, RAG framework, dual-channel retrieval, evaluation benchmarks 

Summary:
The study introduces a pathology vector database covering 28 subfields with 1.53 million paragraphs. The YpathRAG framework is presented, integrating hybrid retrieval methods with an LLM-based judgment module to enhance retrieval quality. Two evaluation benchmarks, YpathR and YpathQA-M, are released to assess the performance of the framework. YpathRAG achieves a significant improvement in Recall@5 on YpathR compared to the baseline. Furthermore, on the challenging question set YpathQA-M, it enhances the accuracy of both general and medical LLMs. These results highlight the improved retrieval quality and factual reliability of the pathology-oriented RAG framework, offering a scalable construction approach and interpretable evaluation method for this domain. <div>
arXiv:2510.08603v1 Announce Type: new 
Abstract: Large language models (LLMs) excel on general tasks yet still hallucinate in high-barrier domains such as pathology. Prior work often relies on domain fine-tuning, which neither expands the knowledge boundary nor enforces evidence-grounded constraints. We therefore build a pathology vector database covering 28 subfields and 1.53 million paragraphs, and present YpathRAG, a pathology-oriented RAG framework with dual-channel hybrid retrieval (BGE-M3 dense retrieval coupled with vocabulary-guided sparse retrieval) and an LLM-based supportive-evidence judgment module that closes the retrieval-judgment-generation loop. We also release two evaluation benchmarks, YpathR and YpathQA-M. On YpathR, YpathRAG attains Recall@5 of 98.64%, a gain of 23 percentage points over the baseline; on YpathQA-M, a set of the 300 most challenging questions, it increases the accuracies of both general and medical LLMs by 9.0% on average and up to 15.6%. These results demonstrate improved retrieval quality and factual reliability, providing a scalable construction paradigm and interpretable evaluation for pathology-oriented RAG.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LatentBreak: Jailbreaking Large Language Models through Latent Space Feedback</title>
<link>https://arxiv.org/abs/2510.08604</link>
<guid>https://arxiv.org/abs/2510.08604</guid>
<content:encoded><![CDATA[
<div> Jailbreaks, Adversarial Attacks, Language Models, LatentBreak, Perplexity<br>
Summary: 
Automated jailbreak attacks aim to bypass safety mechanisms in large language models. Existing methods can be detected using perplexity-based filtering, but LatentBreak, a white-box jailbreak attack, generates natural adversarial prompts with low perplexity to evade defenses. Instead of adding high-perplexity suffixes or templates, LatentBreak substitutes words in the prompt with semantically-equivalent ones to maintain the original intent. It minimizes distances in the latent space between adversarial and harmless prompts. Evaluations show LatentBreak results in shorter and low-perplexity prompts, making it more effective than other algorithms against perplexity-based filters on various models. <br><br> <div>
arXiv:2510.08604v1 Announce Type: new 
Abstract: Jailbreaks are adversarial attacks designed to bypass the built-in safety mechanisms of large language models. Automated jailbreaks typically optimize an adversarial suffix or adapt long prompt templates by forcing the model to generate the initial part of a restricted or harmful response. In this work, we show that existing jailbreak attacks that leverage such mechanisms to unlock the model response can be detected by a straightforward perplexity-based filtering on the input prompt. To overcome this issue, we propose LatentBreak, a white-box jailbreak attack that generates natural adversarial prompts with low perplexity capable of evading such defenses. LatentBreak substitutes words in the input prompt with semantically-equivalent ones, preserving the initial intent of the prompt, instead of adding high-perplexity adversarial suffixes or long templates. These words are chosen by minimizing the distance in the latent space between the representation of the adversarial prompt and that of harmless requests. Our extensive evaluation shows that LatentBreak leads to shorter and low-perplexity prompts, thus outperforming competing jailbreak algorithms against perplexity-based filters on multiple safety-aligned models.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward a Safer Web: Multilingual Multi-Agent LLMs for Mitigating Adversarial Misinformation Attacks</title>
<link>https://arxiv.org/abs/2510.08605</link>
<guid>https://arxiv.org/abs/2510.08605</guid>
<content:encoded><![CDATA[
<div> Keywords: misinformation detection, language-switching, query length inflation, structural reformatting, AI-driven detection

Summary: 
This paper explores various adversarial attacks in misinformation detection, focusing on language-switching across multiple languages and translation, query length inflation before summarization, and structural reformatting into multiple-choice questions. The study presents a multilingual, multi-agent large language model framework with retrieval-augmented generation that can be integrated as a web plugin on online platforms. The research highlights the significance of AI-driven misinformation detection in protecting online factual integrity against diverse attacks and demonstrates the potential for plugin-based deployment in real-world web applications. <div>
arXiv:2510.08605v1 Announce Type: new 
Abstract: The rapid spread of misinformation on digital platforms threatens public discourse, emotional stability, and decision-making. While prior work has explored various adversarial attacks in misinformation detection, the specific transformations examined in this paper have not been systematically studied. In particular, we investigate language-switching across English, French, Spanish, Arabic, Hindi, and Chinese, followed by translation. We also study query length inflation preceding summarization and structural reformatting into multiple-choice questions. In this paper, we present a multilingual, multi-agent large language model framework with retrieval-augmented generation that can be deployed as a web plugin into online platforms. Our work underscores the importance of AI-driven misinformation detection in safeguarding online factual integrity against diverse attacks, while showcasing the feasibility of plugin-based deployment for real-world web applications.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Centering Emotion Hotspots: Multimodal Local-Global Fusion and Cross-Modal Alignment for Emotion Recognition in Conversations</title>
<link>https://arxiv.org/abs/2510.08606</link>
<guid>https://arxiv.org/abs/2510.08606</guid>
<content:encoded><![CDATA[
<div> Keyword: Emotion Recognition, Conversations, Hotspot-Gated Fusion, Multimodal learning, Modality fusion

Summary:
In the study, Emotion Recognition in Conversations (ERC) is examined as a challenging task due to sparse discriminative evidence, localized cues, and asynchronous modalities. A unified model is proposed that focuses on detecting emotion hotspots in text, audio, and video, and fusing them with global features through Hotspot-Gated Fusion. The model also aligns modalities using a routed Mixture-of-Aligners and incorporates a cross-modal graph to encode conversational structure. Experimental results on standard ERC benchmarks demonstrate significant improvements over strong baselines, with ablations validating the effectiveness of Hotspot-Gated Fusion and Mixture-of-Aligners. The study highlights the importance of a hotspot-centric approach in multimodal learning, providing insights into modality fusion in ERC. <br><br>Summary: <div>
arXiv:2510.08606v1 Announce Type: new 
Abstract: Emotion Recognition in Conversations (ERC) is hard because discriminative evidence is sparse, localized, and often asynchronous across modalities. We center ERC on emotion hotspots and present a unified model that detects per-utterance hotspots in text, audio, and video, fuses them with global features via Hotspot-Gated Fusion, and aligns modalities using a routed Mixture-of-Aligners; a cross-modal graph encodes conversational structure. This design focuses modeling on salient spans, mitigates misalignment, and preserves context. Experiments on standard ERC benchmarks show consistent gains over strong baselines, with ablations confirming the contributions of HGF and MoA. Our results point to a hotspot-centric view that can inform future multimodal learning, offering a new perspective on modality fusion in ERC.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation</title>
<link>https://arxiv.org/abs/2510.08608</link>
<guid>https://arxiv.org/abs/2510.08608</guid>
<content:encoded><![CDATA[
<div> propose, MMA-ASIA, evaluation, cultural awareness, multimodal  
Summary:  
MMA-ASIA is a framework for evaluating the cultural awareness of large language models (LLMs) in Asian contexts, consisting of a multilingual, multimodal benchmark with 27,000 questions. The benchmark covers text, image, and speech modalities, allowing for cross-modal testing. A five-dimensional evaluation protocol measures cultural-awareness disparities, cross-lingual and cross-modal consistency, cultural knowledge generalization, and grounding validity. A Cultural Awareness Grounding Validation Module ensures rigorous assessment. Comparative model analysis, attention tracing, and the Vision-ablated Prefix Replay (VPR) method are used to understand variations in model performance across languages and modalities, providing insights for building culturally reliable multimodal LLMs. <br><br> <div>
arXiv:2510.08608v1 Announce Type: new 
Abstract: Large language models (LLMs) are now used worldwide, yet their multimodal understanding and reasoning often degrade outside Western, high-resource settings. We propose MMA-ASIA, a comprehensive framework to evaluate LLMs' cultural awareness with a focus on Asian contexts. MMA-ASIA centers on a human-curated, multilingual, and multimodally aligned multiple-choice benchmark covering 8 Asian countries and 10 languages, comprising 27,000 questions; over 79 percent require multi-step reasoning grounded in cultural context, moving beyond simple memorization. To our knowledge, this is the first dataset aligned at the input level across three modalities: text, image (visual question answering), and speech. This enables direct tests of cross-modal transfer. Building on this benchmark, we propose a five-dimensional evaluation protocol that measures: (i) cultural-awareness disparities across countries, (ii) cross-lingual consistency, (iii) cross-modal consistency, (iv) cultural knowledge generalization, and (v) grounding validity. To ensure rigorous assessment, a Cultural Awareness Grounding Validation Module detects "shortcut learning" by checking whether the requisite cultural knowledge supports correct answers. Finally, through comparative model analysis, attention tracing, and an innovative Vision-ablated Prefix Replay (VPR) method, we probe why models diverge across languages and modalities, offering actionable insights for building culturally reliable multimodal LLMs.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphGhost: Tracing Structures Behind Large Language Models</title>
<link>https://arxiv.org/abs/2510.08613</link>
<guid>https://arxiv.org/abs/2510.08613</guid>
<content:encoded><![CDATA[
<div> GraphGhost, neuron activations, signal propagation, structural semantics, Large Language Models (LLMs) <br>
Summary: <br>
The article introduces GraphGhost, a framework that represents neuron activations and signal propagation in Large Language Models (LLMs) as graphs, explaining how LLMs capture structural semantics and generate outputs. By using graph algorithms like PageRank, the framework characterizes LLM behaviors across datasets and identifies model-specific reasoning mechanisms. The study also identifies activated neurons in GraphGhost and demonstrates that interventions in key neuron nodes can disrupt reasoning, affecting logical flow and semantic understanding. Overall, GraphGhost serves as a valuable tool for analyzing and understanding the structural foundations of reasoning in LLMs. <br> <div>
arXiv:2510.08613v1 Announce Type: new 
Abstract: Large Language Models (LLMs) demonstrate remarkable reasoning capabilities, yet the structural mechanisms underlying these abilities remain under explored. In this work, we introduce GraphGhost, a unified framework that represents neuron activations and their signal propagation as graphs, explaining how LLMs capture structural semantics from sequential inputs and generate outputs through structurally consistent mechanisms. This graph-based perspective enables us to employ graph algorithms such as PageRank to characterize the properties of LLMs, revealing both shared and model-specific reasoning behaviors across diverse datasets. We further identify the activated neurons within GraphGhost and evaluate them through structural interventions, showing that edits to key neuron nodes can trigger reasoning collapse, altering both logical flow and semantic understanding. Together, these contributions position GraphGhost as a powerful tool for analyzing, intervening in, and ultimately understanding the structural foundations of reasoning in LLMs.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gender Bias in Large Language Models for Healthcare: Assignment Consistency and Clinical Implications</title>
<link>https://arxiv.org/abs/2510.08614</link>
<guid>https://arxiv.org/abs/2510.08614</guid>
<content:encoded><![CDATA[
<div> gender bias, language models, healthcare, clinical decision-making, patient outcomes

Summary: 
- Large language models (LLMs) integrated into healthcare can enhance clinical decision-making but are susceptible to biases.
- Gender biases influence physician behaviors and patient outcomes, posing concerns for LLMs assuming human-like roles in healthcare.
- Case studies from the NEJM Challenge were used to assign genders to LLMs and evaluate their responses.
- LLM diagnoses showed consistency across different gender assignments.
- However, inconsistencies were found in LLMs' judgments on the relevance and necessity of patient gender in diagnosis, with some models showing a systematic female-male disparity. 
- Routine checks of identity-assignment consistency in interactions with LLMs are crucial for ensuring reliable and equitable AI-supported clinical care. <div>
arXiv:2510.08614v1 Announce Type: new 
Abstract: The integration of large language models (LLMs) into healthcare holds promise to enhance clinical decision-making, yet their susceptibility to biases remains a critical concern. Gender has long influenced physician behaviors and patient outcomes, raising concerns that LLMs assuming human-like roles, such as clinicians or medical educators, may replicate or amplify gender-related biases. Using case studies from the New England Journal of Medicine Challenge (NEJM), we assigned genders (female, male, or unspecified) to multiple open-source and proprietary LLMs. We evaluated their response consistency across LLM-gender assignments regarding both LLM-based diagnosis and models' judgments on the clinical relevance or necessity of patient gender. In our findings, diagnoses were relatively consistent across LLM genders for most models. However, for patient gender's relevance and necessity in LLM-based diagnosis, all models demonstrated substantial inconsistency across LLM genders, particularly for relevance judgements. Some models even displayed a systematic female-male disparity in their interpretation of patient gender. These findings present an underexplored bias that could undermine the reliability of LLMs in clinical practice, underscoring the need for routine checks of identity-assignment consistency when interacting with LLMs to ensure reliable and equitable AI-supported clinical care.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iterative LLM-Based Generation and Refinement of Distracting Conditions in Math Word Problems</title>
<link>https://arxiv.org/abs/2510.08615</link>
<guid>https://arxiv.org/abs/2510.08615</guid>
<content:encoded><![CDATA[
<div> Keywords: mathematical reasoning, language models, math word problems, distracting conditions, dataset

Summary: 
This paper addresses the challenge of evaluating the intelligence of large language models (LLMs) using math word problems (MWPs). Existing MWP datasets often lack distracting or excessive conditions, leading to a drop in LLM performance when such conditions are introduced. The authors propose an iterative framework that leverages LLMs to automatically generate distracting conditions for MWPs. By designing prompts to revise problems from multiple perspectives, the framework encourages the creation of meaningful distractions without altering the original solution. This approach preserves shared solutions between original and revised problems, reducing the effort required to generate high-quality MWP datasets. Overall, the framework offers a more efficient and reliable method for integrating distracting conditions into MWPs for intelligence testing of LLMs. 

<br><br>Summary: <div>
arXiv:2510.08615v1 Announce Type: new 
Abstract: Mathematical reasoning serves as a crucial testbed for evaluating the intelligence of large language models (LLMs), and math word problems (MWPs) represent one of the most widely used formats. Most existing MWP datasets contain only the necessary information, while problems with distracting or excessive conditions are often overlooked. Prior studies have shown that popular LLMs experience a dramatic performance drop when such distracting conditions are introduced. However, available datasets of MWPs with distracting conditions remain limited, and most exhibit low difficulty and out-of-context expressions. These shortcomings make the distracting conditions easy to detect and disregard, thereby reducing the credibility of benchmarking on these datasets. Moreover, when distracting conditions are added, the reasoning process and answers may change, requiring intensive manual effort to check and rewrite solutions.
  To address these issues, we design an iterative framework that leverages LLMs to generate distracting conditions automatically. We develop a set of prompts to revise MWPs from multiple perspectives and cognitive levels, encouraging the creation of meaningful distracting conditions as well as suggestions for further refinement. A key advantage of our framework is the preservation of shared solutions between the original and revised problems: the LLMs are explicitly guided to generate distractions that do not alter the original solution, thus eliminating the need to produce new answers. This framework is efficient and easy to deploy, substantially reducing the effort required to generate MWPs with distracting conditions while maintaining high data quality.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs Show Surface-Form Brittleness Under Paraphrase Stress Tests</title>
<link>https://arxiv.org/abs/2510.08616</link>
<guid>https://arxiv.org/abs/2510.08616</guid>
<content:encoded><![CDATA[
<div> Benchmark scores, Large Language Models, memorization, test items, generalization

Summary:
The article discusses the potential inflation of benchmark scores for Large Language Models (LLMs) due to memorization of test items or near duplicates. A protocol is presented to probe generalization by re-evaluating models on paraphrased versions of benchmark questions. The study focuses on Mistral-7B-Instruct and Qwen2.5-7B-Instruct models, measuring the accuracy gap between original and paraphrased items on ARC-Easy and ARC-Challenge. The pipeline used controls decoding, enforces multiple-choice output format, and includes a robust paraphrase-cleaning step to preserve semantics. The findings reveal a non-trivial accuracy drop when comparing original and paraphrased items, highlighting concerns about contamination and brittle surface-form shortcuts. <div>
arXiv:2510.08616v1 Announce Type: new 
Abstract: Benchmark scores for Large Language Models (LLMs) can be inflated by memorization of test items or near duplicates. We present a simple, protocol that probes generalization by re-evaluating models on paraphrased versions of benchmark questions. Using Mistral-7B-Instruct and Qwen2.5-7B-Instruct, we measure the accuracy gap between original and paraphrased items on ARC-Easy and ARC-Challenge. Our pipeline controls decoding, enforces multiple-choice output format, and includes a robust paraphrase-cleaning step to preserve semantics. We find that paraphrasing induces a non-trivial accuracy drop (original vs. paraphrased), consistent with prior concerns about contamination and brittle surface-form shortcuts.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JAI-1: A Thai-Centric Large Language Model</title>
<link>https://arxiv.org/abs/2510.08620</link>
<guid>https://arxiv.org/abs/2510.08620</guid>
<content:encoded><![CDATA[
<div> Thai language model, JAI-1, 75B parameters, upscaling strategy, integrated Thai-language knowledge, superior performance<br>
<br>
Summary: JAI-1 is a Thai-centric language model with 75B parameters that adopts an upscaling strategy. Unlike existing models that simply add Thai-specific training without modifying the structure, JAI-1 expands from a high-performing English model and systematically integrates Thai-language knowledge. With exposure to 1.5T tokens during pre-training and fine-tuning using over 600K examples, JAI-1 outperforms Typhoon2-70B on Thai-centric benchmarks. This approach preserves general intelligence and establishes a unique architecture for future enhancements. <div>
arXiv:2510.08620v1 Announce Type: new 
Abstract: This technical report introduces JAI-1, a Thai-centric language model with 75B parameters. Recent Thai models have primarily relied on existing open-source models, applying additional training without structural modifications to specialize in Thai. However, this approach risks eroding pre-existing knowledge in the model's parameter space during the injection of Thai-specific information, as optimized parameters for general tasks may conflict with new linguistic requirements. In contrast, JAI-1 adopts an upscaling strategy: starting from a smaller, high-performing English open-source LLM, we expanded its parameter space and utilized the newly allocated capacity to systematically integrate Thai-language knowledge. This methodology not only preserves the original model's general intelligence but also establishes a unique architecture distinct from other open-source models, enabling scalable future enhancements. During pre-training, JAI-1 was exposed to 1.5T tokens, including over 300B Thai language tokens. This was followed by post-training stages -- supervised fine-tuning and alignment tuning -- using more than 600K instruction-based examples. The final model demonstrated superior performance compared to Typhoon2-70B on Thai-centric benchmarks (IFEval-TH, MT-Bench-TH, and JAI-Hall-Bench), validating the efficacy of its upscaling and knowledge-integration framework.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Simulation to Strategy: Automating Personalized Interaction Planning for Conversational Agents</title>
<link>https://arxiv.org/abs/2510.08621</link>
<guid>https://arxiv.org/abs/2510.08621</guid>
<content:encoded><![CDATA[
<div> Adaptive dialogue model, user profiles, sales-oriented agent, conversation strategies, occupation <br>
<br>Summary: <br>This study examines the impact of user profiles, including age, gender, and occupation, on the performance of a sales-oriented agent in dialogues. While age and gender play a role in dialogue outcomes, occupation has the most significant influence on conversational intent. By incorporating occupation-specific dialogue strategies, the agent can prioritize user preferences, leading to more successful and efficient conversations. The research underscores the importance of considering diverse user profiles in simulation studies and demonstrates the effectiveness of persona-informed strategies in enhancing sales-oriented dialogue systems. <div>
arXiv:2510.08621v1 Announce Type: new 
Abstract: Amid the rapid rise of agentic dialogue models, realistic user-simulator studies are essential for tuning effective conversation strategies. This work investigates a sales-oriented agent that adapts its dialogue based on user profiles spanning age, gender, and occupation. While age and gender influence overall performance, occupation produces the most pronounced differences in conversational intent. Leveraging this insight, we introduce a lightweight, occupation-conditioned strategy that guides the agent to prioritize intents aligned with user preferences, resulting in shorter and more successful dialogues. Our findings highlight the importance of rich simulator profiles and demonstrate how simple persona-informed strategies can enhance the effectiveness of sales-oriented dialogue systems.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text2Stories: Evaluating the Alignment Between Stakeholder Interviews and Generated User Stories</title>
<link>https://arxiv.org/abs/2510.08622</link>
<guid>https://arxiv.org/abs/2510.08622</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, software requirements, user stories, text-to-story alignment, evaluation

Summary: 
Large language models (LLMs) can automate the generation of software requirements from natural language inputs such as elicitation interviews. Evaluating if these requirements align with stakeholders' needs is traditionally a manual process. Text2Stories introduces a task and metrics for text-to-story alignment, quantifying correctness and completeness of requirements. By segmenting interview transcripts and aligning them with user stories, a matching problem is solved. Experiments show that an LLM-based matcher achieves high accuracy, enabling comparison across sets of stories and providing a scalable, source-faithful complement to existing user-story quality criteria.

<br><br>Summary: <div>
arXiv:2510.08622v1 Announce Type: new 
Abstract: Large language models (LLMs) can be employed for automating the generation of software requirements from natural language inputs such as the transcripts of elicitation interviews. However, evaluating whether those derived requirements faithfully reflect the stakeholders' needs remains a largely manual task. We introduce Text2Stories, a task and metrics for text-to-story alignment that allow quantifying the extent to which requirements (in the form of user stories) match the actual needs expressed by the elicitation session participants. Given an interview transcript and a set of user stories, our metric quantifies (i) correctness: the proportion of stories supported by the transcript, and (ii) completeness: the proportion of transcript supported by at least one story. We segment the transcript into text chunks and instantiate the alignment as a matching problem between chunks and stories. Experiments over four datasets show that an LLM-based matcher achieves 0.86 macro-F1 on held-out annotations, while embedding models alone remain behind but enable effective blocking. Finally, we show how our metrics enable the comparison across sets of stories (e.g., human vs. generated), positioning Text2Stories as a scalable, source-faithful complement to existing user-story quality criteria.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PARSE: LLM Driven Schema Optimization for Reliable Entity Extraction</title>
<link>https://arxiv.org/abs/2510.08623</link>
<guid>https://arxiv.org/abs/2510.08623</guid>
<content:encoded><![CDATA[
<div> Keywords: Structured information extraction, JSON schemas, LLM agents, PARSE, extraction accuracy
Summary:
Structured information extraction from unstructured text is crucial for Software 3.0 systems where LLM agents interact with APIs. Current approaches use large language models with JSON schemas but may result in suboptimal extraction and unreliable behavior due to static contracts. Recognizing JSON schemas as natural language understanding contracts, the PARSE system is developed with ARCHITECT optimizing JSON schemas for LLM usage and SCOPE implementing extraction with guardrails. Evaluation on various datasets shows a significant improvement in extraction accuracy, with up to 64.7% enhancement on Structured Web Data Extraction. Overall, PARSE reduces extraction errors by 92% on the first retry and maintains practical latency.
<br><br>Summary: <div>
arXiv:2510.08623v1 Announce Type: new 
Abstract: Structured information extraction from unstructured text is critical for emerging Software 3.0 systems where LLM agents autonomously interact with APIs and tools. Recent approaches apply large language models directly to extraction tasks using existing JSON schemas, often with constraint decoding or reinforcement learning approaches to ensure syntactic validity, but treat JSON schemas as static contracts designed for human developers, leading to suboptimal extraction performance, frequent hallucinations, and unreliable agent behavior when schemas contain ambiguous or incomplete specifications. We recognize that JSON schemas themselves are a form of natural language understanding contract that encodes rules, relationships, and expectations about data structure contracts that LLMs should be able to both interpret and systematically improve. Consequently, we develop PARSE (Parameter Automated Refinement and Schema Extraction), a novel system with two synergistic components: ARCHITECT, which autonomously optimizes JSON schemas for LLM consumption while maintaining backward compatibility through RELAY (an integrated code generation system), and SCOPE, which implements reflection-based extraction with combined static and LLM-based guardrails. We evaluate PARSE qualitatively and quantitatively on three datasets including Schema-Guided Dialogue (SGD), Structured Web Data Extraction (SWDE), and internal retail conversation data, and find that it achieves up to 64.7% improvement in extraction accuracy on SWDE with combined framework improvements reaching 10% across models, while reducing extraction errors by 92% within the first retry and and maintaining practical latency.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do LLMs Know They Are Being Tested? Evaluation Awareness and Incentive-Sensitive Failures in GPT-OSS-20B</title>
<link>https://arxiv.org/abs/2510.08624</link>
<guid>https://arxiv.org/abs/2510.08624</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, evaluation scent, performance inflation, benchmarking, real-world deployment

Summary: 
Evaluation scent in benchmarks for large language models may lead to inflated performance without actual capability gains. A study using a single model examined scenarios with varying framing (evaluation-oriented vs. real-world) and reasoning depth. Results showed that evaluation framing increased chain-of-thought length and reduced answer-only compliance, with limited accuracy gains. While it improved structured outputs like wrappers, it did not significantly impact substance validation. Incentive wording affected error composition, with caution praises improving accuracy and confidence praises leading to riskier outputs. Multilingual benchmarks posed parity risks, with Urdu headers potentially decreasing accuracy at higher reasoning depths. The study provides a reproducible A/B framework and practical guidance for ensuring that benchmark gains align with deployable capability, recommending measures such as neutral phrasing, contract-aware grading, and multilingual dashboards. 

<br><br>Summary: <div>
arXiv:2510.08624v1 Announce Type: new 
Abstract: Benchmarks for large language models (LLMs) often rely on rubric-scented prompts that request visible reasoning and strict formatting, whereas real deployments demand terse, contract-bound answers. We investigate whether such "evaluation scent" inflates measured performance without commensurate capability gains. Using a single open-weights model (GPT-OSS-20B), we run six paired A/B scenarios that hold task content and decoding fixed while varying framing (evaluation-oriented vs. real-world) and reasoning depth (Medium/High): deterministic math, strict code-fix, citation generation, incentive flips (caution vs. competence), CoT visibility, and multilingual (Urdu) headers. Deterministic validators compute accuracy, answer-only compliance, hedging/refusals, chain-of-thought (CoT) length, and schema compliance, with pre-registered deltas and composite indices. Across scenarios, evaluation framing reliably inflates CoT (hundreds to >1000 characters) and reduces answer-only compliance, with limited or inconsistent accuracy gains. In structured outputs, it improves wrappers (e.g., fenced blocks, enumerated lists) but not regex-validated substance. Incentive wording reweights error composition: praising caution modestly improves accuracy at high reasoning and reduces wrong-but-confident errors, whereas praising competence yields terser but riskier outputs. Urdu rubric headers reproduce these signatures and can decrease accuracy at higher reasoning depth, indicating multilingual parity risks. We provide a reproducible A/B framework (prompt banks, validators, per-run scores, scripts; versioned DOI) and practical guidance: neutral phrasing or dual-framing checks, contract-aware grading, style-delta reporting, confidence governance, and multilingual dashboards to ensure that benchmark gains reflect deployable capability.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From What to Why: Thought-Space Recommendation with Small Language Models</title>
<link>https://arxiv.org/abs/2510.08626</link>
<guid>https://arxiv.org/abs/2510.08626</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Small Language Models, Recommendation, Thought Space, PULSE

Summary: 
PULSE is a framework that leverages Small Language Models to enhance recommendation systems by treating SLM-generated rationales as learning signals. By creating a common understanding of user behaviors and item preferences in a Thought Space, PULSE combines user actions and semantic drivers to improve reasoning capabilities. Unlike existing models that focus on interactions alone, PULSE considers rationales as valuable signals, resulting in robust and generalizable embeddings. Experimental results show that PULSE outperforms traditional recommendation models and even Large Language Model-based methods across various datasets. Furthermore, PULSE demonstrates strong transferability in cross-domain recommendation tasks and performs well in reasoning-oriented question answering tasks. This approach represents a promising direction for efficient and effective recommendation systems that utilize the power of Small Language Models. 

Summary: <div>
arXiv:2510.08626v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have advanced recommendation capabilities through enhanced reasoning, but pose significant challenges for real-world deployment due to high inference costs. Conversely, while Small Language Models (SLMs) offer an efficient alternative, their reasoning capabilities for recommendation remain underexplored. Existing systems often use natural language rationales merely as unsupervised descriptive text, failing to harness their full potential as learning signals. In this work our main idea is to create a common understanding of user and items across multiple domains called Thought Space with SLMs instead of using LLMs' distilled knowledge. To that end we propose PULSE (Preference Understanding by Latent Semantic Embeddings), a framework that treats SLM-generated rationales as director learning signals, supervising them with interaction histories to jointly model user actions (what) and their semantic drivers (why). Existing methods consider only interactions such as sequences and embeddings, whereas PULSE treats rationales as first-class signals, this novel design yields embeddings that are more robust and generalizable. Extensive experiments demonstrate that PULSE outperforms leading ID, Collaborative Filtering (CF), and LLM-based sequential recommendation models across multiple benchmark datasets. Furthermore, PULSE exhibits superior transferability in cross-domain recommendation and demonstrates strong performance on downstream tasks such as reasoning-oriented question answering. Our code is available \href{https://anonymous.4open.science/r/Thinking_PULSE-0FC5/README.md}{here}.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ExPO-HM: Learning to Explain-then-Detect for Hateful Meme Detection</title>
<link>https://arxiv.org/abs/2510.08630</link>
<guid>https://arxiv.org/abs/2510.08630</guid>
<content:encoded><![CDATA[
<div> Explanation-Driven Detection, Hateful Memes, Automated Detection Systems, ExPO-HM, Policy Optimization 
Summary: 
ExPO-HM introduces a novel approach to detecting hateful memes online. It addresses the limitations of binary detection models by incorporating explanation-driven detection, providing context and explanations for moderation. The system combines SFT warmup, GRPO with curriculum learning, and CDE to improve reasoning quality and performance. It outperforms baseline models on binary detection, fine-grained classification, and reasoning quality across three benchmark datasets. By moving beyond simple binary predictions, ExPO-HM offers accurate, interpretable, and actionable support for moderation of hateful memes on online platforms. This advancement in automated detection systems enhances the ability to identify and address online abuse effectively. <div>
arXiv:2510.08630v1 Announce Type: new 
Abstract: Hateful memes have emerged as a particularly challenging form of online abuse, motivating the development of automated detection systems. Most prior approaches rely on direct detection, producing only binary predictions. Such models fail to provide the context and explanations that real-world moderation requires. Recent Explain-then-Detect approaches, using Chain-of-Thought prompting or LMM agents, perform worse than simple SFT baselines, and even advanced post-training methods such as GRPO fail to close the gap. Our analysis identifies two key issues of such systems: important policy-relevant cues such as targets and attack types are not hypothesized by the model as a likely explanation; and the binary reward signal is insufficient to guide reasoning. To address these challenges, we propose ExPO-HM (Explain-then-Detect Policy Optimization for Hateful Memes), inspired by the training and evaluation process of human annotators. ExPO-HM combines SFT warmup, GRPO with curriculum learning, and Conditional Decision Entropy (CDE) as both metric and reward for reasoning quality. Across three hateful meme benchmarks, ExPO-HM achieves state-of-the-art performance on binary detection, fine-grained classification, and reasoning quality, with up to 15\% and 17\% F1 improvement over the GRPO and DPO baselines, respectively. By moving hateful meme detection from simple binary alarms to explanation-driven detection, ExPO-HM provides accurate, interpretable, and actionable moderation support.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Next Semantic Scale Prediction via Hierarchical Diffusion Language Models</title>
<link>https://arxiv.org/abs/2510.08632</link>
<guid>https://arxiv.org/abs/2510.08632</guid>
<content:encoded><![CDATA[
<div> diffusion model, language modeling, hierarchical, text generation, ELBO

Summary:<br><br>In this paper, the authors introduce Hierarchical Diffusion Language Models (HDLM), a novel approach for language modeling. HDLM utilizes a hierarchical vocabulary where tokens are mapped from detailed to abstract semantics. The model utilizes a forward process where tokens are perturbed to higher-level ancestors based on a scheduler and a reverse process for predicting more detailed semantics. Closed-form expressions for the diffusion Evidence Lower Bound (ELBO) are derived, demonstrating the flexibility of HDLM and its relation to existing models like MDLM. Practical training techniques based on these insights are proposed. Text generation experiments validate the effectiveness of HDLM, showing lower validation and generative perplexity compared to baseline models. <div>
arXiv:2510.08632v1 Announce Type: new 
Abstract: In this paper we introduce Hierarchical Diffusion Language Models (HDLM) -- a novel family of discrete diffusion models for language modeling. HDLM builds on a hierarchical vocabulary where low-level tokens with detailed semantics are surjectively mapped to high-level tokens with coarse-grained meanings. In the forward process, each token is independently perturbed to its higher-level ancestor with more abstract semantics according to the scheduler, while in the reverse process the model progressively predicts the next, more detailed semantics. Taken together, HDLM provides a general time-varying next semantic scale prediction process for language modeling. We derive closed-form expressions for the diffusion Evidence Lower Bound (ELBO), and show that HDLM can be implemented in a flexible manner while including the existing MDLM as a special case. We also propose practical training techniques based on the insights. Extensive text generation experiments validate the effectiveness of HDLM, which demonstrates consistently lower validation and generative perplexity than baselines.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Upfront Chain-of-Thought: A Cooperative Framework for Chain-of-Thought Compression</title>
<link>https://arxiv.org/abs/2510.08647</link>
<guid>https://arxiv.org/abs/2510.08647</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Chain-of-Thought, Reasoning, CoT Compression, Upfront CoT

Summary:
Upfront CoT (UCoT) is a novel framework for efficient reasoning in large language models. It automates the process of Chain-of-Thought (CoT) compression by using a small model (compressor) and a large model (executor) in a cooperative workflow. The compressor generates upfront thought embeddings rich in reasoning information for the executor, eliminating the need for laborious prompt designing. The executor then utilizes these embeddings to derive correct answers with short reasoning, resulting in significant reduction in CoT length without compromising on reasoning ability. Experimental results demonstrate that UCoT achieves a 50% reduction in token usage on the GSM8K dataset compared to state-of-the-art methods, with a performance improvement of 3.08%. This innovative approach combines efficiency and accuracy in reasoning tasks, offering a promising solution for enhancing the capabilities of Large Language Models. 

<br><br>Summary: <div>
arXiv:2510.08647v1 Announce Type: new 
Abstract: Recent developments have enabled advanced reasoning in Large Language Models (LLMs) via long Chain-of-Thought (CoT), while long CoT suffers from high computational costs and significant latency losses owing to the autoregressive nature of generative LLMs. CoT compression aims to improve efficiency in the reasoning process by reducing output length. Previous works trade reasoning efficiency by either laborious discrete prompt designing or the construction of external compressed CoT datasets that sacrifice key reasoning details. In this work, we propose Upfront CoT (UCoT): an efficient reasoning framework with upfront thought embedding to automate CoT compression. UCoT is a cooperative workflow involving a small model (compressor) and a large model (executor). The first stage of UCoT trains compressor to generate upfront thought embeddings rich in reasoning information for the executor, avoiding the drawbacks of manually designed prompts. The second stage optimizes executor to utilize upfront thought embeddings to derive the correct answer with short reasoning, using a reward mechanism. Extensive experiments show that UCoT maintains the powerful reasoning ability of executor while significantly reducing the length of CoT. It is worth mentioning that when applying UCoT to the Qwen2.5-7B-Instruct model, the usage of tokens on GSM8K dataset is reduced by 50\%, while the performance is 3.08\% higher than that of the state-of-the-art (SOTA) method. The code and dataset are in supplementary material.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Formalizing Style in Personal Narratives</title>
<link>https://arxiv.org/abs/2510.08649</link>
<guid>https://arxiv.org/abs/2510.08649</guid>
<content:encoded><![CDATA[
<div> Keywords: personal narratives, style, linguistic choices, language models, psychological states

Summary:
The article introduces a novel framework for analyzing stylistic choices in personal narratives. It emphasizes the importance of language in conveying subjective experiences and presents a systematic approach to understanding how authors construct their stories. By integrating functional linguistics, computer science methods for pattern analysis, and psychological insights, the framework extracts linguistic features from text, such as processes, participants, and circumstances. Through the analysis of hundreds of dream narratives, including a case study on a war veteran with post-traumatic stress disorder, distinctive patterns emerge, highlighting the dominance of verbal processes over mental ones in certain instances. This illustrates a link between linguistic choices and psychological states, shedding light on how authors use language to express and make sense of their experiences.<br><br>Summary: <div>
arXiv:2510.08649v1 Announce Type: new 
Abstract: Personal narratives are stories authors construct to make meaning of their experiences. Style, the distinctive way authors use language to express themselves, is fundamental to how these narratives convey subjective experiences. Yet there is a lack of a formal framework for systematically analyzing these stylistic choices. We present a novel approach that formalizes style in personal narratives as patterns in the linguistic choices authors make when communicating subjective experiences. Our framework integrates three domains: functional linguistics establishes language as a system of meaningful choices, computer science provides methods for automatically extracting and analyzing sequential patterns, and these patterns are linked to psychological observations. Using language models, we automatically extract linguistic features such as processes, participants, and circumstances. We apply our framework to hundreds of dream narratives, including a case study on a war veteran with post-traumatic stress disorder. Analysis of his narratives uncovers distinctive patterns, particularly how verbal processes dominate over mental ones, illustrating the relationship between linguistic choices and psychological states.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Novel Framework for Augmenting Rating Scale Tests with LLM-Scored Text Data</title>
<link>https://arxiv.org/abs/2510.08663</link>
<guid>https://arxiv.org/abs/2510.08663</guid>
<content:encoded><![CDATA[
<div> Keywords: Psychological assessments, Language Model, Depression, Measurement precision, Augmented test

Summary: 
This study introduces a novel framework that combines Language Model AI scoring with traditional rating scales to create augmented psychological assessments. Using depression as a case study, the framework was tested on real-world and synthetic datasets of upper secondary students, showing significant improvements in measurement precision and accuracy. The Language Model items provided information equivalent to adding additional traditional test items, without the need for pre-labelled data or complex rubrics. By empirically selecting the most informative Language Model scoring instructions, this approach marks a conceptual shift in automated scoring methods. The framework demonstrates a scalable approach for integrating transcribed text data into psychometric measures, with potential applications in clinical health and beyond. 

<br><br>Summary: <div>
arXiv:2510.08663v1 Announce Type: new 
Abstract: Psychological assessments typically rely on structured rating scales, which cannot incorporate the rich nuance of a respondent's natural language. This study leverages recent LLM advances to harness qualitative data within a novel conceptual framework, combining LLM-scored text and traditional rating-scale items to create an augmented test. We demonstrate this approach using depression as a case study, developing and assessing the framework on a real-world sample of upper secondary students (n=693) and corresponding synthetic dataset (n=3,000). On held-out test sets, augmented tests achieved statistically significant improvements in measurement precision and accuracy. The information gain from the LLM items was equivalent to adding between 6.3 (real data) and 16.0 (synthetic data) items to the original 19-item test. Our approach marks a conceptual shift in automated scoring that bypasses its typical bottlenecks: instead of relying on pre-labelled data or complex expert-created rubrics, we empirically select the most informative LLM scoring instructions based on calculations of item information. This framework provides a scalable approach for leveraging the growing stream of transcribed text to enhance traditional psychometric measures, and we discuss its potential utility in clinical health and beyond.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>dInfer: An Efficient Inference Framework for Diffusion Language Models</title>
<link>https://arxiv.org/abs/2510.08666</link>
<guid>https://arxiv.org/abs/2510.08666</guid>
<content:encoded><![CDATA[
<div> framework, dLLM, inference, efficiency, open-sourced
Summary:
The article introduces dInfer, an efficient and extensible framework for diffusion-based large language model (dLLM) inference. dInfer breaks down the inference process into four modular components and integrates novel algorithms and system-level optimizations to achieve substantial efficiency gains without sacrificing output quality. It outperforms existing systems such as Fast-dLLM and even competes favorably against autoregressive models like QWen2.5-3B, delivering significant speedups while maintaining similar model performance. dInfer achieves impressive throughput on various benchmarks, surpassing 1,100 tokens per second at batch size 1 on HumanEval and averaging over 800 tokens per second on 8x H800 GPUs. The implementation of dInfer is open-sourced, promoting widespread adoption and further innovation in the field of dLLM inference. 

<br><br>Summary: <div>
arXiv:2510.08666v1 Announce Type: new 
Abstract: Diffusion-based large language models (dLLMs) have emerged as a promising alternative to autoregressive (AR) LLMs, leveraging denoising-based generation to enable inherent parallelism. Even more and more open-sourced dLLM models emerge, yet their widespread adoption remains constrained by the lack of a standardized and efficient inference framework. We present dInfer, an efficient and extensible framework for dLLM inference. dInfer decomposes the inference pipeline into four modular components-model, diffusion iteration manager, decoding strategy, and KV-cache manager-and integrates novel algorithms for each component alongside system-level optimizations. Through this combination of algorithmic innovations and system enhancements, dInfer achieves substantial efficiency gains without compromising output quality on LLaDA-MoE. At batch size 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800 tokens per second across six benchmarks on $8\times$ H800 GPUs. Compared to prior systems, dInfer delivers $10\times$ speedup over Fast-dLLM while maintaining similar model performance. Even compared with AR models (with a comparable number of activation parameters and performance) QWen2.5-3B, which is highly optimized with latest vLLM inference engine, dInfer still deliverers $2$-$3\times$ speedup. The implementation of dInfer is open-sourced at https://github.com/inclusionAI/dInfer.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Laws for Code: A More Data-Hungry Regime</title>
<link>https://arxiv.org/abs/2510.08702</link>
<guid>https://arxiv.org/abs/2510.08702</guid>
<content:encoded><![CDATA[
<div> large language models, code, scaling laws, empirical study, data-to-parameter ratio
Summary:<br>
- The study investigates scaling laws for large language models (LLMs) in the context of code, which differs from natural language in syntax and data requirements.<br>
- Results suggest that the Farseer law provides more accurate predictions for code LLMs.<br>
- Code LLMs scale effectively with model size, but require a higher data-to-parameter ratio than natural language models.<br>
- Experiments on code-NL mixtures show that natural language benefits resource-constrained scenarios, but may hinder performance at higher compute budgets.<br>
- Conducting 117 experimental runs with model sizes ranging from 0.2B to 3.8B, the study sheds light on the specific scaling behaviors of LLMs in the domain of code. <br>Summary: <div>
arXiv:2510.08702v1 Announce Type: new 
Abstract: Code Large Language Models (LLMs) are revolutionizing software engineering. However, scaling laws that guide the efficient training are predominantly analyzed on Natural Language (NL). Given the fundamental differences like strict syntax between code and NL, it is unclear whether these laws are directly applicable to code. To address this gap, we conduct the first large-scale empirical study of scaling laws for code, comprising 117 experimental runs with model sizes from 0.2B to 3.8B and training tokens from 2B to 128B. We fit the Chinchilla law and the Farsser law. First, the results show that the more expressive Farseer law offers greater accuracy. Second, the analysis reveals that Code LLMs scale effectively with model size. Crucially, code represents a more data-hungry regime, requiring a substantially higher data-to-parameter ratio than NL. Finally, two additional sets of experiments on code-NL mixtures show that NL benefits resource-constrained scenarios, but becomes a detriment at higher compute budgets.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thinking Longer, Not Always Smarter: Evaluating LLM Capabilities in Hierarchical Legal Reasoning</title>
<link>https://arxiv.org/abs/2510.08710</link>
<guid>https://arxiv.org/abs/2510.08710</guid>
<content:encoded><![CDATA[
<div> Case-based reasoning, Large Language Models, legal practice, hierarchical reasoning, reasoning capabilities<br>
<br>
Summary:
The article introduces a formal framework for analyzing the reasoning abilities of Large Language Models (LLMs) in the context of case-based legal reasoning. The framework involves three-stage reasoning tasks: surface-level reasoning, hierarchical reasoning, and integrated analysis. While LLMs excel at surface-level reasoning, their performance declines significantly when it comes to hierarchical reasoning and integrated analysis. Surprisingly, the models spend more computational resources on incorrect responses than on correct ones. This finding highlights the need to address fundamental limitations in LLMs to ensure robust and trustworthy legal AI. The study showcases the importance of fine-grained analysis in evaluating LLM reasoning capabilities and emphasizes the complexity of nuanced reasoning required in legal practice. <div>
arXiv:2510.08710v1 Announce Type: new 
Abstract: Case-based reasoning is a cornerstone of U.S. legal practice, requiring professionals to argue about a current case by drawing analogies to and distinguishing from past precedents. While Large Language Models (LLMs) have shown remarkable capabilities, their proficiency in this complex, nuanced form of reasoning needs further investigation. We propose a formal framework that decomposes the process of identifying significant distinctions between cases into three-stage reasoning tasks. Our framework models cases using factual predicates called factors, organizes them into a legal knowledge hierarchy, and defines verifiable rules for identifying distinctions, analyzing their argumentative support, and evaluating their significance. Through comprehensive evaluation of modern reasoning LLMs, we reveal a paradox: while models achieve high accuracy on surface-level reasoning (Task 1), performance degrades on hierarchical reasoning (Task 2: 64.82%-92.09%) and collapses on integrated analysis (Task 3: 11.46%-33.99%). Most strikingly, we find that models consistently expend more computational resources on incorrect responses than correct ones, suggesting that "thinking longer" does not always mean "thinking smarter." Our work provides a methodology for fine-grained analysis of LLM reasoning capabilities in complex domains and reveals fundamental limitations that must be addressed for robust and trustworthy legal AI.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Many Code and Test Cases Are Enough? Evaluating Test Cases Generation from a Binary-Matrix Perspective</title>
<link>https://arxiv.org/abs/2510.08720</link>
<guid>https://arxiv.org/abs/2510.08720</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, test case evaluation, benchmark construction, fault coverage, diagnostic power
Summary:
In this paper, the authors address the challenge of evaluating test cases generated by Large Language Models (LLMs). They propose a framework for constructing benchmarks that aims to determine the minimal set of wrong codes necessary to represent the entire error space and the minimal set of test cases needed to distinguish them. By formalizing benchmark construction as finding an optimal diagnostic basis in a binary code-test matrix, they develop an efficient approximation algorithm called WrongSelect to select maximally diverse wrong codes. This results in the creation of TC-Bench, a compact, diverse, and inflation-resistant benchmark constructed from competitive programming submissions. Experimental results show that existing test case generation methods have a limited diagnostic power on TC-Bench, highlighting the need for more effective evaluation strategies in this domain. The dataset and code for TC-Bench are publicly available for further research and development. 
<br><br>Summary: <div>
arXiv:2510.08720v1 Announce Type: new 
Abstract: Evaluating test cases automatically generated by Large Language Models (LLMs) is a critical yet challenging task. Existing benchmarks suffer from high computational costs, score inflation, and a bias towards trivial bugs over rare, critical faults. In this work, we ask two fundamental questions: (1) What is the minimal set of wrong codes sufficient to represent the entire error space? and (2) What is the minimal set of test cases needed to distinguish them? We introduce a framework that formalizes benchmark construction as finding an optimal diagnostic basis in a binary code-test matrix. The rank of this matrix specifies the minimal number of independent error patterns (wrong codes) and provides a tight upper bound on the number of test cases required for complete fault coverage. Our objective is to identify a basis of size equal to the matrix rank that maximizes internal diversity. To tackle this NP-hard problem, we propose WrongSelect, an efficient approximation algorithm to select maximally diverse wrong codes. Applying this framework to millions of competitive programming submissions, we construct TC-Bench, a compact, diverse, and inflation-resistant benchmark. Extensive experiments show that even the most advanced test case generation methods achieve only ~60% exclusion rates on TC-Bench, exposing a significant gap in their diagnostic power. Our dataset is available at: https://huggingface.co/datasets/Luoberta/TC-Bench and our code is at: https://github.com/Luowaterbi/TC-Bench.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Reliable is Language Model Micro-Benchmarking?</title>
<link>https://arxiv.org/abs/2510.08730</link>
<guid>https://arxiv.org/abs/2510.08730</guid>
<content:encoded><![CDATA[
<div> micro-benchmarking, language models, evaluation, performance difference, reliability 
Summary:
Micro-benchmarking is a method used to evaluate language models on small subsets of benchmarks to save time and cost. However, research shows that these micro-benchmarks may not consistently rank models as accurately as full benchmarks or random data sampling. A new meta-evaluation measure has been introduced to analyze the reliability of micro-benchmarking in ranking model pairs based on their performance differences on full benchmarks. The study suggests that selecting a minimum of 250 examples may be necessary to consistently rank models with similar performances. Even with 25 examples, micro-benchmarks struggle to preserve pairwise comparisons of 8B instruction-tuned models. The findings highlight the need for balancing evaluation efficiency and reliability in micro-benchmarking for both users and developers.<br><br>Summary: <div>
arXiv:2510.08730v1 Announce Type: new 
Abstract: Micro-benchmarking offers a solution to the often prohibitive time and cost of language model development: evaluate on a very small subset of existing benchmarks. Can these micro-benchmarks, however, rank models as consistently as the full benchmarks they replace? And can they rank models more consistently than selecting a random subset of data points? In many scenarios, we find that the answer is no. We introduce a meta-evaluation measure for micro-benchmarking which investigates how well a micro-benchmark can rank two models as a function of their performance difference on the full benchmark. This approach can determine which model pairs can be ranked correctly by a micro-benchmark, allowing for a finer-grained analysis of the trade-off between micro-benchmark size and reliability. Prior work has suggested selecting as few as 10 examples; we find that no micro-benchmarking method can consistently rank model pairs 3.5 points of accuracy apart on MMLU-Pro or 4 points apart on BIG-bench Hard. In order to consistently rank model pairs with relatively similar performances, we show that often as many as 250 examples must be selected, at which point random sampling is competitive with existing micro-benchmarking methods. When comparing only 8B instruction-tuned models on MMLU-Pro micro-benchmarks with 25 examples, we find that more than half of pairwise comparisons are not likely to be preserved. Our work provides actionable guidance for both micro-benchmark users and developers in navigating the trade-off between evaluation efficiency and reliability.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coordinates from Context: Using LLMs to Ground Complex Location References</title>
<link>https://arxiv.org/abs/2510.08741</link>
<guid>https://arxiv.org/abs/2510.08741</guid>
<content:encoded><![CDATA[
<div> Geocoding, location references, LLMs, reasoning skills, fine-tuned model <br>
Summary: <br>
Geocoding is the process of linking location references to actual geographic locations, crucial for text analysis. This study focuses on geocoding compositional location references using LLMs. Evaluating LLMs' geospatial knowledge and reasoning abilities, a new approach is proposed for improved geocoding. The research showcases that a smaller fine-tuned LLM can achieve comparable performance to larger models, enhancing task efficiency. <div>
arXiv:2510.08741v1 Announce Type: new 
Abstract: Geocoding is the task of linking a location reference to an actual geographic location and is essential for many downstream analyses of unstructured text. In this paper, we explore the challenging setting of geocoding compositional location references. Building on recent work demonstrating LLMs' abilities to reason over geospatial data, we evaluate LLMs' geospatial knowledge versus reasoning skills relevant to our task. Based on these insights, we propose an LLM-based strategy for geocoding compositional location references. We show that our approach improves performance for the task and that a relatively small fine-tuned LLM can achieve comparable performance with much larger off-the-shelf models.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Moral LLM Responses in Multilingual Capacities</title>
<link>https://arxiv.org/abs/2510.08776</link>
<guid>https://arxiv.org/abs/2510.08776</guid>
<content:encoded><![CDATA[
<div> LLM, evaluation, multilingual, benchmarking, GPT-5
<br>
Summary:
The study evaluates responses from leading open-source models in five dimensions across low and high-resource languages to measure LLM accuracy and consistency. GPT-5 performed the best on average in each category, showing higher scores in Consent & Autonomy and Harm Prevention & Safety compared to other models. However, there was more inconsistency across language and category in other models like Gemini 2.5 Pro. The findings highlight the importance of further testing on linguistic shifts' effects on LLM responses and the need for improvement in certain categories. 
<br> <div>
arXiv:2510.08776v1 Announce Type: new 
Abstract: With LLM usage becoming widespread across countries, languages, and humanity more broadly, the need to understand and guardrail their multilingual responses increases. Large-scale datasets for testing and benchmarking have been created to evaluate and facilitate LLM responses across multiple dimensions. In this study, we evaluate the responses of frontier and leading open-source models in five dimensions across low and high-resource languages to measure LLM accuracy and consistency across multilingual contexts. We evaluate the responses using a five-point grading rubric and a judge LLM. Our study shows that GPT-5 performed the best on average in each category, while other models displayed more inconsistency across language and category. Most notably, in the Consent & Autonomy and Harm Prevention & Safety categories, GPT scored the highest with averages of 3.56 and 4.73, while Gemini 2.5 Pro scored the lowest with averages of 1.39 and 1.98, respectively. These findings emphasize the need for further testing on how linguistic shifts impact LLM responses across various categories and improvement in these areas.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning What to Remember: Adaptive Probabilistic Memory Retention for Memory-Efficient Language Models</title>
<link>https://arxiv.org/abs/2510.08798</link>
<guid>https://arxiv.org/abs/2510.08798</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformer, attention, Adaptive Retention, token selection, long-context efficiency

Summary:
Adaptive Retention is proposed as a solution to the quadratic scaling issue of Transformer attention with sequence length. This probabilistic token selection mechanism learns which representations to keep under a global budget constraint. Bernoulli gates are used for retention, trained via a Hard-Concrete/variational relaxation method, and enforced with a top-M rule at inference. By keeping only 30-50% of tokens, performance can be preserved at over 95% of the full-model level, while reducing peak memory usage by 35-45% and enhancing throughput by up to 1.8 times. This method is compatible with standard encoders and does not require adjustments to base attention or task heads, making it architecture-agnostic. The approach proves effective across various tasks such as classification, extractive QA, and long-document summarization, offering practical efficiency for long-context processing. 

<br><br>Summary: 
- Adaptive Retention addresses the quadratic scaling issue of Transformer attention.
- It utilizes a probabilistic token selection mechanism under a global budget constraint.
- The method preserves performance while reducing memory usage and improving throughput significantly.
- Adaptive Retention is compatible with standard encoders and does not require changes to base attention or task heads.
- This architecture-agnostic approach proves effective across different tasks, providing practical long-context processing efficiency. <div>
arXiv:2510.08798v1 Announce Type: new 
Abstract: Transformer attention scales quadratically with sequence length O(n^2), limiting long-context use. We propose Adaptive Retention, a probabilistic, layer-wise token selection mechanism that learns which representations to keep under a strict global budget M. Retention is modeled with Bernoulli gates trained via a Hard-Concrete/variational relaxation and enforced with a simple top-M rule at inference, making the method differentiable and drop-in for standard encoders. Across classification, extractive QA, and long-document summarization, keeping only 30-50% of tokens preserves >= 95% of full-model performance while cutting peak memory by ~35-45% and improving throughput by up to ~1.8x. This architecture-agnostic approach delivers practical long-context efficiency without modifying base attention or task heads.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Chinese Commonsense Reasoning with a Multi-hop Reasoning Perspective</title>
<link>https://arxiv.org/abs/2510.08800</link>
<guid>https://arxiv.org/abs/2510.08800</guid>
<content:encoded><![CDATA[
<div> Chinese Commonsense Multi-hop Reasoning, Large Language Models, multi-step logical reasoning, factual knowledge, domain-balanced<br>
<br>
Summary: 
The article introduces a new benchmark, Chinese Commonsense Multi-hop Reasoning (CCMOR), aimed at evaluating the performance of Large Language Models (LLMs) in integrating Chinese-specific factual knowledge with multi-step logical reasoning. The benchmark is created by constructing a seed set from existing QA datasets and generating multi-hop questions using an LLM-powered pipeline. To ensure data quality, human experts verify and refine the questions. Evaluation of state-of-the-art LLMs using CCMOR reveals limitations in processing long-tail knowledge and executing knowledge-intensive reasoning. However, incorporating retrieval-augmented generation helps overcome these knowledge gaps and improves performance significantly. <div>
arXiv:2510.08800v1 Announce Type: new 
Abstract: While Large Language Models (LLMs) have demonstrated advanced reasoning capabilities, their comprehensive evaluation in general Chinese-language contexts remains understudied. To bridge this gap, we propose Chinese Commonsense Multi-hop Reasoning (CCMOR), a novel benchmark designed to evaluate LLMs' ability to integrate Chinese-specific factual knowledge with multi-step logical reasoning. Specifically, we first construct a domain-balanced seed set from existing QA datasets, then develop an LLM-powered pipeline to generate multi-hop questions anchored on factual unit chains. To ensure the quality of resulting dataset, we implement a human-in-the-loop verification system, where domain experts systematically validate and refine the generated questions. Using CCMOR, we evaluate state-of-the-art LLMs, demonstrating persistent limitations in LLMs' ability to process long-tail knowledge and execute knowledge-intensive reasoning. Notably, retrieval-augmented generation substantially mitigates these knowledge gaps, yielding significant performance gains.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOSAIC: Multi-agent Orchestration for Task-Intelligent Scientific Coding</title>
<link>https://arxiv.org/abs/2510.08804</link>
<guid>https://arxiv.org/abs/2510.08804</guid>
<content:encoded><![CDATA[
<div> Keywords: MOSAIC, multi-agent framework, scientific coding tasks, Large Language Model, problem decomposition <br>
Summary: 
MOSAIC is a new multi-agent Large Language Model framework designed to tackle challenging scientific coding tasks. Scientific workflows often require rigorous algorithms with deep domain knowledge and domain-specific reasoning. MOSAIC uses a student-teacher paradigm to self-reflect, create rationale, code, and debug, facilitating stepwise problem decomposition and targeted error correction. The framework includes specially designed agents to address the challenges of scientific code generation without the need for training. The Consolidated Context Window (CCW) helps prevent LLM hallucinations when solving complex scientific tasks involving chained subproblems. Experimental results show that MOSAIC outperforms existing approaches in terms of accuracy, robustness, and interpretability. This specialized agentic framework shows promise in improving the efficiency and effectiveness of solving scientific coding challenges. <br><br>Summary: <div>
arXiv:2510.08804v1 Announce Type: new 
Abstract: We present MOSAIC, a multi-agent Large Language Model (LLM) framework for solving challenging scientific coding tasks. Unlike general-purpose coding, scientific workflows require algorithms that are rigorous, interconnected with deep domain knowledge, and incorporate domain-specific reasoning, as well as algorithm iteration without requiring I/O test cases. Many scientific problems also require a sequence of subproblems to be solved, leading to the final desired result. MOSAIC is designed as a training-free framework with specially designed agents to self-reflect, create the rationale, code, and debug within a student-teacher paradigm to address the challenges of scientific code generation. This design facilitates stepwise problem decomposition, targeted error correction, and, when combined with our Consolidated Context Window (CCW), mitigates LLM hallucinations when solving complex scientific tasks involving chained subproblems. We evaluate MOSAIC on scientific coding benchmarks and demonstrate that our specialized agentic framework outperforms existing approaches in terms of accuracy, robustness, and interpretability.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Model's Language Matters: A Comparative Privacy Analysis of LLMs</title>
<link>https://arxiv.org/abs/2510.08813</link>
<guid>https://arxiv.org/abs/2510.08813</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, privacy risks, linguistic variability, extraction, counterfactual memorization

Summary:
- Large Language Models (LLMs) are increasingly used in multilingual applications handling sensitive data.
- The study examines how language structure impacts privacy leakage in LLMs trained on English, Spanish, French, and Italian medical texts.
- Six linguistic indicators and three attack vectors were quantified and evaluated to measure privacy vulnerability.
- Italian LLMs demonstrated the highest privacy leakage due to linguistic redundancy and tokenization granularity.
- English LLMs showed increased membership separability, while French and Spanish LLMs displayed more resilience because of their morphological complexity.<br><br>Summary: Language structure significantly influences privacy leakage in Large Language Models, with Italian models exhibiting the strongest vulnerability. English models have higher membership separability, while French and Spanish models show more resilience. These findings highlight the importance of implementing language-aware privacy-preserving mechanisms in LLM deployments. <div>
arXiv:2510.08813v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly deployed across multilingual applications that handle sensitive data, yet their scale and linguistic variability introduce major privacy risks. Mostly evaluated for English, this paper investigates how language structure affects privacy leakage in LLMs trained on English, Spanish, French, and Italian medical corpora. We quantify six linguistic indicators and evaluate three attack vectors: extraction, counterfactual memorization, and membership inference. Results show that privacy vulnerability scales with linguistic redundancy and tokenization granularity: Italian exhibits the strongest leakage, while English shows higher membership separability. In contrast, French and Spanish display greater resilience due to higher morphological complexity. Overall, our findings provide the first quantitative evidence that language matters in privacy leakage, underscoring the need for language-aware privacy-preserving mechanisms in LLM deployments.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Search-on-Graph: Iterative Informed Navigation for Large Language Model Reasoning on Knowledge Graphs</title>
<link>https://arxiv.org/abs/2510.08825</link>
<guid>https://arxiv.org/abs/2510.08825</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, knowledge graphs, iterative graph navigation, knowledge-intensive questions, state-of-the-art performance

Summary: 
The article introduces a new framework called Search-on-Graph (SoG) that enhances large language models' reasoning abilities in answering knowledge-intensive, multi-hop questions using knowledge graphs. SoG enables iterative informed graph navigation by allowing the language model to examine available relations at each step before deciding on the next hop. This approach follows an "observe-then-navigate" principle, adapting seamlessly to different knowledge graph schemas and handling high-degree nodes effectively. SoG outperforms existing methods on six knowledge graph question answering benchmarks without requiring fine-tuning. Particularly, strong improvements are seen on Wikidata benchmarks, with a significant overall performance boost in answering complex questions using Freebase and Wikidata knowledge graphs.<br><br>Summary: <div>
arXiv:2510.08825v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated impressive reasoning abilities yet remain unreliable on knowledge-intensive, multi-hop questions -- they miss long-tail facts, hallucinate when uncertain, and their internal knowledge lags behind real-world change. Knowledge graphs (KGs) offer a structured source of relational evidence, but existing KGQA methods face fundamental trade-offs: compiling complete SPARQL queries without knowing available relations proves brittle, retrieving large subgraphs introduces noise, and complex agent frameworks with parallel exploration exponentially expand search spaces. To address these limitations, we propose Search-on-Graph (SoG), a simple yet effective framework that enables LLMs to perform iterative informed graph navigation using a single, carefully designed \textsc{Search} function. Rather than pre-planning paths or retrieving large subgraphs, SoG follows an ``observe-then-navigate'' principle: at each step, the LLM examines actual available relations from the current entity before deciding on the next hop. This approach further adapts seamlessly to different KG schemas and handles high-degree nodes through adaptive filtering. Across six KGQA benchmarks spanning Freebase and Wikidata, SoG achieves state-of-the-art performance without fine-tuning. We demonstrate particularly strong gains on Wikidata benchmarks (+16\% improvement over previous best methods) alongside consistent improvements on Freebase benchmarks.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pattern Enhanced Multi-Turn Jailbreaking: Exploiting Structural Vulnerabilities in Large Language Models</title>
<link>https://arxiv.org/abs/2510.08859</link>
<guid>https://arxiv.org/abs/2510.08859</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, multi-turn jailbreaking attacks, conversational context, vulnerabilities, conversation patterns

Summary: 
The study focuses on the vulnerability of large language models (LLMs) to multi-turn jailbreaking attacks that exploit conversational context to bypass safety constraints gradually. The research introduces the Pattern Enhanced Chain of Attack (PE-CoA) framework consisting of five conversation patterns to construct effective multi-turn jailbreaks through natural dialogue. The evaluation of PE-CoA on twelve LLMs across ten harm categories reveals state-of-the-art performance in uncovering pattern-specific vulnerabilities and LLM behavioral characteristics. The study highlights the distinct weakness profiles of LLMs, indicating that robustness to one conversational pattern does not generalize to others. Additionally, the research shows that model families share similar failure modes, emphasizing the limitations of current safety training methods and the need for pattern-aware defenses. The findings underscore the importance of understanding the relationship between conversation patterns and model vulnerabilities in enhancing the security of LLMs.

<br><br>Summary: <div>
arXiv:2510.08859v1 Announce Type: new 
Abstract: Large language models (LLMs) remain vulnerable to multi-turn jailbreaking attacks that exploit conversational context to bypass safety constraints gradually. These attacks target different harm categories (like malware generation, harassment, or fraud) through distinct conversational approaches (educational discussions, personal experiences, hypothetical scenarios). Existing multi-turn jailbreaking methods often rely on heuristic or ad hoc exploration strategies, providing limited insight into underlying model weaknesses. The relationship between conversation patterns and model vulnerabilities across harm categories remains poorly understood. We propose Pattern Enhanced Chain of Attack (PE-CoA), a framework of five conversation patterns to construct effective multi-turn jailbreaks through natural dialogue. Evaluating PE-CoA on twelve LLMs spanning ten harm categories, we achieve state-of-the-art performance, uncovering pattern-specific vulnerabilities and LLM behavioral characteristics: models exhibit distinct weakness profiles where robustness to one conversational pattern does not generalize to others, and model families share similar failure modes. These findings highlight limitations of safety training and indicate the need for pattern-aware defenses. Code available on: https://github.com/Ragib-Amin-Nihal/PE-CoA
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quality Estimation Reranking for Document-Level Translation</title>
<link>https://arxiv.org/abs/2510.08870</link>
<guid>https://arxiv.org/abs/2510.08870</guid>
<content:encoded><![CDATA[
<div> learned metric, SLIDE, BLEURT-20, large language model, document-level translation
Summary:
In this study, the effectiveness of quality estimation (QE) reranking in document-level translation is examined using various learned and large language model-based metrics. The results show significant improvements in BLEURT-20 scores with the best learned metric, SLIDE, achieving gains of +2.00 with two candidates and +5.09 with 32. Additionally, using the best LLM-based metric, GEMBA-DA, results in improvements of +1.63 and +4.30 under similar conditions. Although gains decrease with longer inputs, reranking with 32 candidates still provides enhancements of +2.34 (SLIDE) and +1.40 (GEMBA-DA) on the longest documents. These findings highlight the practical value of document-level QE reranking, showcasing its potential to enhance machine translation performance with minimal runtime overhead when suitable translation models and hardware are employed. 
<br><br>Summary: <div>
arXiv:2510.08870v1 Announce Type: new 
Abstract: Quality estimation (QE) reranking is a form of quality-aware decoding which aims to improve machine translation (MT) by scoring and selecting the best candidate from a pool of generated translations. While known to be effective at the sentence level, its application to the increasingly prominent domain of document-level translation remains underexplored. In this work, we evaluate QE reranking performance on document-level (rather than the typical sentence-level) translation, using various learned and large language model (LLM)-based QE metrics. We find that with our best learned metric, SLIDE, BLEURT-20 scores improve by +2.00 with only two candidates, and by +5.09 with 32, across both decoder-only LLM models and encoder-decoder neural machine translation (NMT) models. Using the best LLM-based metric, GEMBA-DA, gains of +1.63 and +4.30 are achieved under the same conditions. Although gains shrink with longer inputs, reranking with 32 candidates yields improvements of +2.34 (SLIDE) and +1.40 (GEMBA-DA) on our longest documents (512-1024 source tokens). These findings demonstrate the practical value of document-level QE, with minimal runtime overhead given suitable translation models and hardware.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinAuditing: A Financial Taxonomy-Structured Multi-Document Benchmark for Evaluating LLMs</title>
<link>https://arxiv.org/abs/2510.08886</link>
<guid>https://arxiv.org/abs/2510.08886</guid>
<content:encoded><![CDATA[
<div> Keywords: GAAP, XBRL, FinAuditing, LLMs, financial auditing

Summary:
The article introduces FinAuditing, a benchmark for evaluating large language models (LLMs) on financial auditing tasks. It focuses on the complexity of Generally Accepted Accounting Principles (GAAP) and XBRL filings, highlighting the difficulty in automating and verifying financial audits due to structured and taxonomy-driven documents. FinAuditing defines three subtasks - FinSM, FinRE, and FinMR - targeting semantic, relational, and numerical consistency in structured auditing reasoning. The unified evaluation framework combines retrieval, classification, and reasoning metrics for a comprehensive assessment. Zero-shot experiments on 13 LLMs show inconsistent performance across different dimensions, with significant accuracy drops in reasoning over hierarchical multi-document structures. The study reveals limitations of current LLMs in taxonomy-grounded financial reasoning and emphasizes the need for trustworthy, structure-aware financial intelligence systems. The benchmark dataset is available on Hugging Face. 

<br><br>Summary: 
- Introduction of FinAuditing benchmark for LLM evaluation in financial auditing tasks
- Emphasis on complexity of GAAP and XBRL filings in automating financial audits
- Definition of FinSM, FinRE, and FinMR subtasks targeting different aspects of structured auditing reasoning
- Proposal of a unified evaluation framework combining retrieval, classification, and reasoning metrics
- Zero-shot experiments highlighting inconsistent performance of LLMs in financial reasoning over hierarchical structures <div>
arXiv:2510.08886v1 Announce Type: new 
Abstract: The complexity of the Generally Accepted Accounting Principles (GAAP) and the hierarchical structure of eXtensible Business Reporting Language (XBRL) filings make financial auditing increasingly difficult to automate and verify. While large language models (LLMs) have demonstrated strong capabilities in unstructured text understanding, their ability to reason over structured, interdependent, and taxonomy-driven financial documents remains largely unexplored. To fill this gap, we introduce FinAuditing, the first taxonomy-aligned, structure-aware, multi-document benchmark for evaluating LLMs on financial auditing tasks. Built from real US-GAAP-compliant XBRL filings, FinAuditing defines three complementary subtasks, FinSM for semantic consistency, FinRE for relational consistency, and FinMR for numerical consistency, each targeting a distinct aspect of structured auditing reasoning. We further propose a unified evaluation framework integrating retrieval, classification, and reasoning metrics across these subtasks. Extensive zero-shot experiments on 13 state-of-the-art LLMs reveal that current models perform inconsistently across semantic, relational, and mathematical dimensions, with accuracy drops of up to 60-90% when reasoning over hierarchical multi-document structures. Our findings expose the systematic limitations of modern LLMs in taxonomy-grounded financial reasoning and establish FinAuditing as a foundation for developing trustworthy, structure-aware, and regulation-aligned financial intelligence systems. The benchmark dataset is available at Hugging Face.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Multi-Temperature Strategies for Token- and Rollout-Level Control in RLVR</title>
<link>https://arxiv.org/abs/2510.08892</link>
<guid>https://arxiv.org/abs/2510.08892</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Large Language Models, Exploration, Reasoning Performance, Multi-Temperature Scheduling <br>
Summary: 
This study introduces a new approach to improve the reasoning performance of Large Language Models (LLMs) by promoting exploration during token sampling. The method applies different temperature settings for high-entropy reasoning tokens and low-entropy knowledge tokens to encourage exploration while maintaining factual correctness. Various multi-temperature scheduling strategies are investigated in reinforcement learning contexts to enhance reasoning abilities. The approach significantly enhances the reasoning performance of LLMs in empirical evaluations on multiple reasoning benchmarks. By applying higher temperatures for reasoning tokens and lower temperatures for knowledge tokens, the approach actively encourages exploration and improves the overall reasoning capabilities of LLMs. The code for the approach is available on GitHub for further exploration and implementation. <br><br>Summary: <div>
arXiv:2510.08892v1 Announce Type: new 
Abstract: Reinforcement Learning has demonstrated substantial improvements in the reasoning abilities of Large Language Models (LLMs), exhibiting significant applicability across various domains. Recent research has identified that tokens within LLMs play distinct roles during reasoning tasks, categorizing them into high-entropy reasoning tokens and low-entropy knowledge tokens. Prior approaches have typically focused on restricting updates to indirectly encourage exploration, yet they do not explicitly facilitate exploratory behavior during the token generation stage itself. In this work, we introduce a complementary approach that explicitly promotes exploration during sampling by applying distinct temperature settings for different token types. Specifically, our method employs higher temperatures for reasoning tokens to actively encourage exploration, while retaining lower temperatures for knowledge tokens to maintain factual correctness. Furthermore, we systematically investigate various multi-temperature scheduling strategies and their impacts within reinforcement learning contexts. Empirical evaluations on several reasoning benchmarks demonstrate that our approach significantly enhances the reasoning performance of LLMs. The code is available at https://github.com/zhmzm/Multi_Temperature_Verl.git.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Biomedical Named Entity Recognition Framework with Large Language Models</title>
<link>https://arxiv.org/abs/2510.08902</link>
<guid>https://arxiv.org/abs/2510.08902</guid>
<content:encoded><![CDATA[
<div> framework, named entity recognition, biomedical, large language models, multilingual
Summary: 
- The paper proposes a unified Biomedical Named Entity Recognition (BioNER) framework based on Large Language Models (LLMs) to improve accuracy in recognizing biomedical named entities.
- BioNER is reformulated as a text generation task, with a symbolic tagging strategy designed to handle flat and nested entities with explicit boundary annotation.
- Bilingual joint fine-tuning across Chinese and English datasets enhances multilingual and multi-task generalization.
- A contrastive learning-based entity selector is introduced to filter incorrect predictions and improve the accuracy of entity recognition.
- Experimental results on multiple datasets demonstrate the method's state-of-the-art performance and robust zero-shot generalization across languages.
<br><br>Summary: <div>
arXiv:2510.08902v1 Announce Type: new 
Abstract: Accurate recognition of biomedical named entities is critical for medical information extraction and knowledge discovery. However, existing methods often struggle with nested entities, entity boundary ambiguity, and cross-lingual generalization. In this paper, we propose a unified Biomedical Named Entity Recognition (BioNER) framework based on Large Language Models (LLMs). We first reformulate BioNER as a text generation task and design a symbolic tagging strategy to jointly handle both flat and nested entities with explicit boundary annotation. To enhance multilingual and multi-task generalization, we perform bilingual joint fine-tuning across multiple Chinese and English datasets. Additionally, we introduce a contrastive learning-based entity selector that filters incorrect or spurious predictions by leveraging boundary-sensitive positive and negative samples. Experimental results on four benchmark datasets and two unseen corpora show that our method achieves state-of-the-art performance and robust zero-shot generalization across languages. The source codes are freely available at https://github.com/dreamer-tx/LLMNER.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autoencoding-Free Context Compression for LLMs via Contextual Semantic Anchors</title>
<link>https://arxiv.org/abs/2510.08907</link>
<guid>https://arxiv.org/abs/2510.08907</guid>
<content:encoded><![CDATA[
<div> Keywords: context compression, language model, semantic-anchor compression, autoencoding tasks, compression ratios <br>
Summary: <br>
Semantic-Anchor Compression (SAC) is introduced as a novel method for context compression in large language models. Unlike existing methods that rely on autoencoding tasks, SAC directly selects anchor tokens from the original context and aggregates contextual information into their key-value representations. This eliminates the need for training models specifically for compression. SAC incorporates anchor embeddings to identify critical tokens and bidirectional attention modification to allow anchor tokens to capture information from the entire context. Experimental results show that SAC outperforms existing methods across various compression ratios, achieving a 1 EM improvement at 5x compression over strong baselines on out-of-distribution evaluation using MRQA. The benefits of SAC become more evident at higher compression ratios, showcasing its effectiveness in enhancing compression performance without the limitations of autoencoding tasks. <br> <div>
arXiv:2510.08907v1 Announce Type: new 
Abstract: Context compression presents a promising approach for accelerating large language model (LLM) inference by compressing long contexts into compact representations. Current context compression methods predominantly rely on autoencoding tasks to train context-agnostic compression tokens to compress contextual semantics. While autoencoding tasks enable compression tokens to acquire compression capabilities, compression via autoencoding tasks creates a fundamental mismatch: the models are optimized for reconstruction that diverge from actual downstream tasks, thereby weakening the features more beneficial for real-world usage. We propose Semantic-Anchor Compression (SAC), a novel method that shifts from autoencoding task based compression to an architecture that is equipped with this compression capability \textit{a priori}. Instead of training models to compress contexts through autoencoding tasks, SAC directly selects so-called anchor tokens from the original context and aggregates contextual information into their key-value (KV) representations. By deriving representations directly from the contextual tokens, SAC eliminates the need for autoencoding training. To ensure compression performance while directly leveraging anchor tokens, SAC incorporates two key designs: (1) anchor embeddings that enable the compressor to identify critical tokens, and (2) bidirectional attention modification that allows anchor tokens to capture information from the entire context. Experimental results demonstrate that SAC consistently outperforms existing context compression methods across various compression ratios. On out-of-distribution evaluation using MRQA, SAC achieves 1 EM improvement at 5x compression over strong baselines, with increasing advantages at higher compression ratios.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Impressions: Evaluating Large Language Model Behavior Through the Lens of Trait Impressions</title>
<link>https://arxiv.org/abs/2510.08915</link>
<guid>https://arxiv.org/abs/2510.08915</guid>
<content:encoded><![CDATA[
<div> impressions, LLMs, stereotypes, language, probes

Summary:<br>
The article introduces the concept of artificial impressions in Large Language Models (LLMs) by analyzing patterns in their internal representations of prompts that resemble human impressions and stereotypes based on language. Linear probes are fitted on generated prompts to predict impressions according to the Stereotype Content Model (SCM). It is found that LLMs inconsistently report impressions when prompted, but impressions are more consistently decodable from their hidden representations. Artificial impressions of prompts are shown to be predictive of the quality and use of hedging in model responses. The study also investigates how content, stylistic, and dialectal features in prompts impact LLM impressions. <div>
arXiv:2510.08915v1 Announce Type: new 
Abstract: We introduce and study artificial impressions--patterns in LLMs' internal representations of prompts that resemble human impressions and stereotypes based on language. We fit linear probes on generated prompts to predict impressions according to the two-dimensional Stereotype Content Model (SCM). Using these probes, we study the relationship between impressions and downstream model behavior as well as prompt features that may inform such impressions. We find that LLMs inconsistently report impressions when prompted, but also that impressions are more consistently linearly decodable from their hidden representations. Additionally, we show that artificial impressions of prompts are predictive of the quality and use of hedging in model responses. We also investigate how particular content, stylistic, and dialectal features in prompts impact LLM impressions.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SOP-Maze: Evaluating Large Language Models on Complicated Business Standard Operating Procedures</title>
<link>https://arxiv.org/abs/2510.08942</link>
<guid>https://arxiv.org/abs/2510.08942</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, SOP-Maze, benchmarks, performance analysis, open-source

Summary: 
The article introduces SOP-Maze, a new benchmark designed to evaluate the performance of large language models (LLMs) in complex standard operating procedure (SOP) scenarios commonly found in business environments. SOP-Maze consists of 397 tasks derived from 23 real-world SOP scenarios, categorized into two classes: Lateral Root System (LRS) and Heart Root System (HRS) representing wide-option tasks and deep logical reasoning tasks, respectively. The experiments conducted on SOP-Maze reveal that existing state-of-the-art models struggle with the challenges posed by SOP tasks, with errors primarily falling into three categories: route blindness, conversational fragility, and calculation errors. The systematic analysis provided in the study offers insights into improving LLM capabilities in handling SOP tasks. The code for SOP-Maze has been made open-source on GitHub for further research and development. 

<br><br>Summary: <div>
arXiv:2510.08942v1 Announce Type: new 
Abstract: As large language models (LLMs) are widely deployed as domain-specific agents, many benchmarks have been proposed to evaluate their ability to follow instructions and make decisions in real-world scenarios. However, business scenarios often involve complex standard operating procedures (SOPs), and the evaluation of LLM capabilities in such contexts has not been fully explored. To bridge this gap, we propose SOP-Maze, a benchmark constructed from real-world business data and adapted into a collection of 397 tasks from 23 complex SOP scenarios. We further categorize SOP tasks into two broad classes: Lateral Root System (LRS), representing wide-option tasks that demand precise selection; and Heart Root System (HRS), which emphasizes deep logical reasoning with complex branches. Extensive experiments reveal that nearly all state-of-the-art models struggle with SOP-Maze. We conduct a comprehensive analysis and identify three key error categories: (i) route blindness: difficulty following procedures; (ii) conversational fragility: inability to handle real dialogue nuances; and (iii) calculation errors: mistakes in time or arithmetic reasoning under complex contexts. The systematic study explores LLM performance across SOP tasks that challenge both breadth and depth, offering new insights for improving model capabilities. We have open-sourced our work on https://github.com/ADoublLEN/SOP-Maze.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Human Behavioral Baseline for Collective Governance in Software Projects</title>
<link>https://arxiv.org/abs/2510.08956</link>
<guid>https://arxiv.org/abs/2510.08956</guid>
<content:encoded><![CDATA[
<div> Keywords: open source communities, version controlled governance documents, participation, control, entropy

Summary: 
The study examines how open source communities outline participation and control through version controlled governance documents. By analyzing a dataset of 710 projects with paired snapshots, the researchers categorized text into actors, rules, actions, and objects. They then measured changes in governance using entropy for evenness, richness for diversity, and Jensen Shannon divergence for drift. The results indicate that over time, projects define more roles and actions, with a more even distribution among them, while the composition of rules remains consistent. This suggests that governance in these communities evolves by expanding and balancing categories of participation without significant changes in prescriptive authority. The analysis provides a reproducible benchmark for assessing whether future AI-mediated workflows concentrate or redistribute authority. 

<br><br>Summary: <div>
arXiv:2510.08956v1 Announce Type: new 
Abstract: We study how open source communities describe participation and control through version controlled governance documents. Using a corpus of 710 projects with paired snapshots, we parse text into actors, rules, actions, and objects, then group them and measure change with entropy for evenness, richness for diversity, and Jensen Shannon divergence for drift. Projects define more roles and more actions over time, and these are distributed more evenly, while the composition of rules remains stable. These findings indicate that governance grows by expanding and balancing categories of participation without major shifts in prescriptive force. The analysis provides a reproducible baseline for evaluating whether future AI mediated workflows concentrate or redistribute authority.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Creation of the Chinese Adaptive Policy Communication Corpus</title>
<link>https://arxiv.org/abs/2510.08986</link>
<guid>https://arxiv.org/abs/2510.08986</guid>
<content:encoded><![CDATA[
<div> Keywords: CAPC-CG, Chinese policy directives, annotation, language models, NLP research

Summary: 
CAPC-CG is introduced as the first open dataset of Chinese policy directives annotated with clear and ambiguous language categories. The corpus spans from 1949-2023 and includes national laws, regulations, and rules issued by China's top authorities. Each document is segmented into paragraphs, totaling 3.3 million units. Metadata, labeling framework, and a gold-standard annotation set are provided, achieving high inter-annotator agreement for supervised modeling. Baseline classification results with large language models are presented, along with an annotation codebook and dataset patterns. The release aims to support downstream tasks and multilingual NLP research in policy communication. <div>
arXiv:2510.08986v1 Announce Type: new 
Abstract: We introduce CAPC-CG, the Chinese Adaptive Policy Communication (Central Government) Corpus, the first open dataset of Chinese policy directives annotated with a five-color taxonomy of clear and ambiguous language categories, building on Ang's theory of adaptive policy communication. Spanning 1949-2023, this corpus includes national laws, administrative regulations, and ministerial rules issued by China's top authorities. Each document is segmented into paragraphs, producing a total of 3.3 million units. Alongside the corpus, we release comprehensive metadata, a two-round labeling framework, and a gold-standard annotation set developed by expert and trained coders. Inter-annotator agreement achieves a Fleiss's kappa of K = 0.86 on directive labels, indicating high reliability for supervised modeling. We provide baseline classification results with several large language models (LLMs), together with our annotation codebook, and describe patterns from the dataset. This release aims to support downstream tasks and multilingual NLP research in policy communication.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MASA: LLM-Driven Multi-Agent Systems for Autoformalization</title>
<link>https://arxiv.org/abs/2510.08988</link>
<guid>https://arxiv.org/abs/2510.08988</guid>
<content:encoded><![CDATA[

arXiv:2510.08988v1 Announce Type: new 
Abstract: Autoformalization serves a crucial role in connecting natural language and formal reasoning. This paper presents MASA, a novel framework for building multi-agent systems for autoformalization driven by Large Language Models (LLMs). MASA leverages collaborative agents to convert natural language statements into their formal representations. The architecture of MASA is designed with a strong emphasis on modularity, flexibility, and extensibility, allowing seamless integration of new agents and tools to adapt to a fast-evolving field. We showcase the effectiveness of MASA through use cases on real-world mathematical definitions and experiments on formal mathematics datasets. This work highlights the potential of multi-agent systems powered by the interaction of LLMs and theorem provers in enhancing the efficiency and reliability of autoformalization, providing valuable insights and support for researchers and practitioners in the field.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DARO: Difficulty-Aware Reweighting Policy Optimization</title>
<link>https://arxiv.org/abs/2510.09001</link>
<guid>https://arxiv.org/abs/2510.09001</guid>
<content:encoded><![CDATA[

arXiv:2510.09001v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have shown that reasoning ability can be significantly enhanced through Reinforcement Learning with Verifiable Rewards (RLVR). Group Relative Policy Optimization (GRPO) has emerged as the de facto approach for RLVR, inspiring numerous variants. However, our mathematical analysis reveals that these methods are fundamentally weighted variations of GRPO. We provide a unified view, demonstrating that their reliance on static or overly simplistic weighting schemes tied to sample difficulty prevents adaptation to a model's evolving capabilities. This creates a significant loss scale issue, where training disproportionately focuses on certain difficulty levels at the expense of others, hindering overall performance. To address these limitations, we introduce \textbf{Difficulty-Aware Reweighting Policy Optimization (DARO)}, a method that dynamically adjusts the loss contribution of each difficulty group based on the model's learning state. Extensive experiments on Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, and Llama3.1-8B show that DARO outperforms four leading baselines across six math benchmarks, achieving significantly faster convergence and superior final performance.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoupling Safety into Orthogonal Subspace: Cost-Efficient and Performance-Preserving Alignment for Large Language Models</title>
<link>https://arxiv.org/abs/2510.09004</link>
<guid>https://arxiv.org/abs/2510.09004</guid>
<content:encoded><![CDATA[

arXiv:2510.09004v1 Announce Type: new 
Abstract: Safety alignment is essential for building trustworthy artificial intelligence, yet it remains challenging to enhance model safety without degrading general performance. Current approaches require computationally expensive searches for the optimal proportion of safety-critical and general-purpose data to balance safety and general performance, incurring high costs with limited gains. In this work, we show that LoRA-based Refusal-training enables performance-preserving safety alignment even when trained solely on safety data, demonstrating that LoRA serves as cost-efficient, performance-preserving, and plug-and-play safety patches. Beyond empirical findings, we provide both theoretical and experimental evidence that LoRA effectively decouples safety into a low-rank subspace largely orthogonal to the model's intrinsic transformation space, ensuring that safety enhancements do not interfere with inherent capabilities.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LitE-SQL: A Lightweight and Efficient Text-to-SQL Framework with Vector-based Schema Linking and Execution-Guided Self-Correction</title>
<link>https://arxiv.org/abs/2510.09014</link>
<guid>https://arxiv.org/abs/2510.09014</guid>
<content:encoded><![CDATA[

arXiv:2510.09014v1 Announce Type: new 
Abstract: The Text-to-SQL task translates natural language questions into SQL queries, enabling intuitive database interaction for non-experts. While recent methods leveraging Large Language Models (LLMs) achieve strong performance, their reliance on proprietary models raise concerns about deployment feasibility and data privacy. In this work, we introduce LitE-SQL, a Lightweight and Efficient framework with two components: (i) a Schema Retriever that performs efficient schema linking using a vector database of pre-computed schema embeddings, and (ii) a SQL Generator fine-tuned in two stages-supervised fine-tuning followed by execution-guided reinforcement-enabling self-correction without costly multi-candidate generation. On BIRD, LitE-SQL achieves 72.10% execution accuracy, and on Spider 1.0 it reaches 88.45%, demonstrating comparable or superior performance to LLM-based methods despite using 2x to 30x fewer parameters. Our findings demonstrate that high-quality Text-to-SQL generation is feasible with lightweight models, offering a practical solution for privacy-sensitive and resource-constrained settings.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Refinement of Essay Scoring Rubrics for Language Models via Reflect-and-Revise</title>
<link>https://arxiv.org/abs/2510.09030</link>
<guid>https://arxiv.org/abs/2510.09030</guid>
<content:encoded><![CDATA[

arXiv:2510.09030v1 Announce Type: new 
Abstract: The performance of Large Language Models (LLMs) is highly sensitive to the prompts they are given. Drawing inspiration from the field of prompt optimization, this study investigates the potential for enhancing Automated Essay Scoring (AES) by refining the scoring rubrics used by LLMs. Specifically, our approach prompts models to iteratively refine rubrics by reflecting on models' own scoring rationales and observed discrepancies with human scores on sample essays. Experiments on the TOEFL11 and ASAP datasets using GPT-4.1, Gemini-2.5-Pro, and Qwen-3-Next-80B-A3B-Instruct show Quadratic Weighted Kappa (QWK) improvements of up to 0.19 and 0.47, respectively. Notably, even with a simple initial rubric, our approach achieves comparable or better QWK than using detailed human-authored rubrics. Our findings highlight the importance of iterative rubric refinement in LLM-based AES to enhance alignment with human evaluations.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Cross-Lingual Knowledge Transfer via Transliteration-Based MLM Fine-Tuning for Critically Low-resource Chakma Language</title>
<link>https://arxiv.org/abs/2510.09032</link>
<guid>https://arxiv.org/abs/2510.09032</guid>
<content:encoded><![CDATA[

arXiv:2510.09032v1 Announce Type: new 
Abstract: As an Indo-Aryan language with limited available data, Chakma remains largely underrepresented in language models. In this work, we introduce a novel corpus of contextually coherent Bangla-transliterated Chakma, curated from Chakma literature, and validated by native speakers. Using this dataset, we fine-tune six encoder-based multilingual and regional transformer models (mBERT, XLM-RoBERTa, DistilBERT, DeBERTaV3, BanglaBERT, and IndicBERT) on masked language modeling (MLM) tasks. Our experiments show that fine-tuned multilingual models outperform their pre-trained counterparts when adapted to Bangla-transliterated Chakma, achieving up to 73.54% token accuracy and a perplexity as low as 2.90. Our analysis further highlights the impact of data quality on model performance and shows the limitations of OCR pipelines for morphologically rich Indic scripts. Our research demonstrates that Bangla-transliterated Chakma can be very effective for transfer learning for Chakma language, and we release our manually validated monolingual dataset to encourage further research on multilingual language modeling for low-resource languages.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Do NOT Really Know What They Don't Know</title>
<link>https://arxiv.org/abs/2510.09033</link>
<guid>https://arxiv.org/abs/2510.09033</guid>
<content:encoded><![CDATA[

arXiv:2510.09033v1 Announce Type: new 
Abstract: Recent work suggests that large language models (LLMs) encode factuality signals in their internal representations, such as hidden states, attention weights, or token probabilities, implying that LLMs may "know what they don't know". However, LLMs can also produce factual errors by relying on shortcuts or spurious associations. These error are driven by the same training objective that encourage correct predictions, raising the question of whether internal computations can reliably distinguish between factual and hallucinated outputs. In this work, we conduct a mechanistic analysis of how LLMs internally process factual queries by comparing two types of hallucinations based on their reliance on subject information. We find that when hallucinations are associated with subject knowledge, LLMs employ the same internal recall process as for correct responses, leading to overlapping and indistinguishable hidden-state geometries. In contrast, hallucinations detached from subject knowledge produce distinct, clustered representations that make them detectable. These findings reveal a fundamental limitation: LLMs do not encode truthfulness in their internal states but only patterns of knowledge recall, demonstrating that "LLMs don't really know what they don't know".
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alif: Advancing Urdu Large Language Models via Multilingual Synthetic Data Distillation</title>
<link>https://arxiv.org/abs/2510.09051</link>
<guid>https://arxiv.org/abs/2510.09051</guid>
<content:encoded><![CDATA[

arXiv:2510.09051v1 Announce Type: new 
Abstract: Developing a high-performing large language models (LLMs) for low-resource languages such as Urdu, present several challenges. These challenges include the scarcity of high-quality datasets, multilingual inconsistencies, and safety concerns. Existing multilingual LLMs often address these issues by translating large volumes of available data. However, such translations often lack quality and cultural nuance while also incurring significant costs for data curation and training. To address these issues, we propose Alif-1.0-8B-Instruct, a multilingual Urdu-English model, that tackles these challenges with a unique approach. We train the model on a high-quality, multilingual synthetic dataset (Urdu-Instruct), developed using a modified self-instruct technique. By using unique prompts and seed values for each task along with a global task pool, this dataset incorporates Urdu-native chain-of-thought based reasoning, bilingual translation, cultural relevance, and ethical safety alignments. This technique significantly enhances the comprehension of Alif-1.0-8B-Instruct model for Urdu-specific tasks. As a result, Alif-1.0-8B-Instruct, built upon the pretrained Llama-3.1-8B, demonstrates superior performance compared to Llama-3.1-8B-Instruct for Urdu specific-tasks. It also outperformed leading multilingual LLMs, including Mistral-7B-Instruct-v0.3, Qwen-2.5-7B-Instruct, and Cohere-Aya-Expanse-8B, all within a training budget of under $100. Our results demonstrate that high-performance and low-resource language LLMs can be developed efficiently and culturally aligned using our modified self-instruct approach. All datasets, models, and code are publicly available at: https://github.com/traversaal-ai/alif-urdu-llm.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReFIne: A Framework for Trustworthy Large Reasoning Models with Reliability, Faithfulness, and Interpretability</title>
<link>https://arxiv.org/abs/2510.09062</link>
<guid>https://arxiv.org/abs/2510.09062</guid>
<content:encoded><![CDATA[

arXiv:2510.09062v1 Announce Type: new 
Abstract: Recent advances in long chain-of-thought (CoT) reasoning have largely prioritized answer accuracy and token efficiency, while overlooking aspects critical to trustworthiness. We argue that usable reasoning systems must be trustworthy, characterized by three properties: interpretability, faithfulness, and reliability. To this end, we propose ReFIne, a new training framework that integrates supervised fine-tuning with GRPO to encourage models to: (i) improve interpretability by producing structured, tag-based traces with high-level planning that are easier for humans to follow; (ii) enhance faithfulness by explicitly disclosing the decisive information guiding each solution, with consistent cross-section references; and (iii) promote reliability by providing self-assessments of both the derivation's soundness and the confidence of the final answer. We apply ReFIne to the Qwen3 models at multiple scales (1.7B/4B/8B) and evaluate across mathematical benchmarks of varying difficulty. Our experimental results show that ReFIne models generate clearer and better-structured reasoning traces (interpretability +44.0%), more faithfully expose their underlying decision process (faithfulness +18.8%), and offer informative confidence estimates (reliability +42.4%). These findings highlight an overlooked but important direction: reasoning models should be optimized not only for accuracy, but also for broader dimensions of trustworthiness. Our code is available at: https://github.com/Trustworthy-ML-Lab/Training_Trustworthy_LRM_with_Refine
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FrameEOL: Semantic Frame Induction using Causal Language Models</title>
<link>https://arxiv.org/abs/2510.09097</link>
<guid>https://arxiv.org/abs/2510.09097</guid>
<content:encoded><![CDATA[

arXiv:2510.09097v1 Announce Type: new 
Abstract: Semantic frame induction is the task of clustering frame-evoking words according to the semantic frames they evoke. In recent years, leveraging embeddings of frame-evoking words that are obtained using masked language models (MLMs) such as BERT has led to high-performance semantic frame induction. Although causal language models (CLMs) such as the GPT and Llama series succeed in a wide range of language comprehension tasks and can engage in dialogue as if they understood frames, they have not yet been applied to semantic frame induction. We propose a new method for semantic frame induction based on CLMs. Specifically, we introduce FrameEOL, a prompt-based method for obtaining Frame Embeddings that outputs One frame-name as a Label representing the given situation. To obtain embeddings more suitable for frame induction, we leverage in-context learning (ICL) and deep metric learning (DML). Frame induction is then performed by clustering the resulting embeddings. Experimental results on the English and Japanese FrameNet datasets demonstrate that the proposed methods outperform existing frame induction methods. In particular, for Japanese, which lacks extensive frame resources, the CLM-based method using only 5 ICL examples achieved comparable performance to the MLM-based method fine-tuned with DML.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Retrieval Succeeds and Fails: Rethinking Retrieval-Augmented Generation for LLMs</title>
<link>https://arxiv.org/abs/2510.09106</link>
<guid>https://arxiv.org/abs/2510.09106</guid>
<content:encoded><![CDATA[

arXiv:2510.09106v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have enabled a wide range of applications through their powerful capabilities in language understanding and generation. However, as LLMs are trained on static corpora, they face difficulties in addressing rapidly evolving information or domain-specific queries. Retrieval-Augmented Generation (RAG) was developed to overcome this limitation by integrating LLMs with external retrieval mechanisms, allowing them to access up-to-date and contextually relevant knowledge. However, as LLMs themselves continue to advance in scale and capability, the relative advantages of traditional RAG frameworks have become less pronounced and necessary. Here, we present a comprehensive review of RAG, beginning with its overarching objectives and core components. We then analyze the key challenges within RAG, highlighting critical weakness that may limit its effectiveness. Finally, we showcase applications where LLMs alone perform inadequately, but where RAG, when combined with LLMs, can substantially enhance their effectiveness. We hope this work will encourage researchers to reconsider the role of RAG and inspire the development of next-generation RAG systems.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DITING: A Multi-Agent Evaluation Framework for Benchmarking Web Novel Translation</title>
<link>https://arxiv.org/abs/2510.09116</link>
<guid>https://arxiv.org/abs/2510.09116</guid>
<content:encoded><![CDATA[

arXiv:2510.09116v1 Announce Type: new 
Abstract: Large language models (LLMs) have substantially advanced machine translation (MT), yet their effectiveness in translating web novels remains unclear. Existing benchmarks rely on surface-level metrics that fail to capture the distinctive traits of this genre. To address these gaps, we introduce DITING, the first comprehensive evaluation framework for web novel translation, assessing narrative and cultural fidelity across six dimensions: idiom translation, lexical ambiguity, terminology localization, tense consistency, zero-pronoun resolution, and cultural safety, supported by over 18K expert-annotated Chinese-English sentence pairs. We further propose AgentEval, a reasoning-driven multi-agent evaluation framework that simulates expert deliberation to assess translation quality beyond lexical overlap, achieving the highest correlation with human judgments among seven tested automatic metrics. To enable metric comparison, we develop MetricAlign, a meta-evaluation dataset of 300 sentence pairs annotated with error labels and scalar quality scores. Comprehensive evaluation of fourteen open, closed, and commercial models reveals that Chinese-trained LLMs surpass larger foreign counterparts, and that DeepSeek-V3 delivers the most faithful and stylistically coherent translations. Our work establishes a new paradigm for exploring LLM-based web novel translation and provides public resources to advance future research.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Augmenting Dialog with Think-Aloud Utterances for Modeling Individual Personality Traits by LLM</title>
<link>https://arxiv.org/abs/2510.09158</link>
<guid>https://arxiv.org/abs/2510.09158</guid>
<content:encoded><![CDATA[

arXiv:2510.09158v1 Announce Type: new 
Abstract: This study proposes augmenting dialog data with think-aloud utterances (TAUs) for modeling individual personalities in text chat by LLM. TAU is a verbalization of a speaker's thought before articulating the utterance. We expect "persona LLMs" trained with TAU-augmented data can mimic the speaker's personality trait better. We tested whether the trained persona LLMs obtain the human personality with respect to Big Five, a framework characterizing human personality traits from five aspects. The results showed that LLMs trained with TAU-augmented data more closely align to the speakers' Agreeableness and Neuroticism of Big Five than those trained with original dialog data. We also found that the quality of TAU-augmentation impacts persona LLM's performance.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stronger Re-identification Attacks through Reasoning and Aggregation</title>
<link>https://arxiv.org/abs/2510.09184</link>
<guid>https://arxiv.org/abs/2510.09184</guid>
<content:encoded><![CDATA[

arXiv:2510.09184v1 Announce Type: new 
Abstract: Text de-identification techniques are often used to mask personally identifiable information (PII) from documents. Their ability to conceal the identity of the individuals mentioned in a text is, however, hard to measure. Recent work has shown how the robustness of de-identification methods could be assessed by attempting the reverse process of _re-identification_, based on an automated adversary using its background knowledge to uncover the PIIs that have been masked. This paper presents two complementary strategies to build stronger re-identification attacks. We first show that (1) the _order_ in which the PII spans are re-identified matters, and that aggregating predictions across multiple orderings leads to improved results. We also find that (2) reasoning models can boost the re-identification performance, especially when the adversary is assumed to have access to extensive background knowledge.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLaMAX2: Your Translation-Enhanced Model also Performs Well in Reasoning</title>
<link>https://arxiv.org/abs/2510.09189</link>
<guid>https://arxiv.org/abs/2510.09189</guid>
<content:encoded><![CDATA[

arXiv:2510.09189v1 Announce Type: new 
Abstract: General Large Language Models (LLMs) excel in reasoning, but those enhanced for translation struggle with reasoning tasks. To address this, we propose a novel translationenhanced recipe that begins with instruct models and applies layer-selective tuning only on parallel data. Following this pipeline, we introduce the Qwen3-XPlus models, which demonstrate significant improvements in translation performance across both high- and lowresource languages, achieving 15+ spBLEU and 40+ xComet in low-resource languages, like Swahili. Interestingly, training only with small parallel datasets, Qwen3-XPlus achieves an average improvement of 1+ points on 7 multilingual tasks while maintaining proficiency comparable to the Qwen3 instruct model in 15 popular reasoning datasets. This work offers a promising approach to multilingual enhancement, significantly reducing complexity and enhancing accessibility for a wider range of languages. The code and model are publicly available.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DICE: Structured Reasoning in LLMs through SLM-Guided Chain-of-Thought Correction</title>
<link>https://arxiv.org/abs/2510.09211</link>
<guid>https://arxiv.org/abs/2510.09211</guid>
<content:encoded><![CDATA[

arXiv:2510.09211v1 Announce Type: new 
Abstract: When performing reasoning tasks with user-specific requirements, such as strict output formats, large language models (LLMs) often prioritize reasoning over adherence to detailed instructions. Fine-tuning LLMs on supervised datasets to address this is impractical due to high computational costs and limited parameter access. To tackle this, we propose DICE, a lightweight framework that guides small language models (SLMs) to refine LLMs' outputs through chain-of-thought (CoT) correction. DICE decouples the process by first prompting LLMs to generate natural language responses, then using trained SLMs to analyze and refine these outputs to meet structured output specifications. This framework preserves LLMs' broad knowledge and reasoning capabilities while ensuring the outputs conform to user demands. Specifically, DICE first constructs structured CoT adaptation datasets via a two-stage method and subsequently applies a dual-tuning strategy to fine-tune SLMs for generating structured outputs in an analyze-then-answer pattern. Experiments demonstrate that DICE improves the average format accuracy and content correctness of LLM outputs by 35.4\% and 29.4\%, respectively, achieving state-of-the-art (SOTA) performance over other competitive baselines.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IRIS: An Iterative and Integrated Framework for Verifiable Causal Discovery in the Absence of Tabular Data</title>
<link>https://arxiv.org/abs/2510.09217</link>
<guid>https://arxiv.org/abs/2510.09217</guid>
<content:encoded><![CDATA[

arXiv:2510.09217v1 Announce Type: new 
Abstract: Causal discovery is fundamental to scientific research, yet traditional statistical algorithms face significant challenges, including expensive data collection, redundant computation for known relations, and unrealistic assumptions. While recent LLM-based methods excel at identifying commonly known causal relations, they fail to uncover novel relations. We introduce IRIS (Iterative Retrieval and Integrated System for Real-Time Causal Discovery), a novel framework that addresses these limitations. Starting with a set of initial variables, IRIS automatically collects relevant documents, extracts variables, and uncovers causal relations. Our hybrid causal discovery method combines statistical algorithms and LLM-based methods to discover known and novel causal relations. In addition to causal discovery on initial variables, the missing variable proposal component of IRIS identifies and incorporates missing variables to expand the causal graphs. Our approach enables real-time causal discovery from only a set of initial variables without requiring pre-existing datasets.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CrisiText: A dataset of warning messages for LLM training in emergency communication</title>
<link>https://arxiv.org/abs/2510.09243</link>
<guid>https://arxiv.org/abs/2510.09243</guid>
<content:encoded><![CDATA[

arXiv:2510.09243v1 Announce Type: new 
Abstract: Effectively identifying threats and mitigating their potential damage during crisis situations, such as natural disasters or violent attacks, is paramount for safeguarding endangered individuals. To tackle these challenges, AI has been used in assisting humans in emergency situations. Still, the use of NLP techniques remains limited and mostly focuses on classification tasks. The significant potential of timely warning message generation using NLG architectures, however, has been largely overlooked. In this paper we present CrisiText, the first large-scale dataset for the generation of warning messages across 13 different types of crisis scenarios. The dataset contains more than 400,000 warning messages (spanning almost 18,000 crisis situations) aimed at assisting civilians during and after such events. To generate the dataset, we started from existing crisis descriptions and created chains of events related to the scenarios. Each event was then paired with a warning message. The generations follow experts' written guidelines to ensure correct terminology and factuality of their suggestions. Additionally, each message is accompanied by three suboptimal warning types to allow for the study of different NLG approaches. To this end, we conducted a series of experiments comparing supervised fine-tuning setups with preference alignment, zero-shot, and few-shot approaches. We further assessed model performance in out-of-distribution scenarios and evaluated the effectiveness of an automatic post-editor.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DSPO: Stable and Efficient Policy Optimization for Agentic Search and Reasoning</title>
<link>https://arxiv.org/abs/2510.09255</link>
<guid>https://arxiv.org/abs/2510.09255</guid>
<content:encoded><![CDATA[

arXiv:2510.09255v1 Announce Type: new 
Abstract: Enhancing LLMs with the ability to actively search external knowledge is crucial for complex and real-world tasks. Current approaches either rely on prompting to elicit the model's innate agent capabilities, or suffer from performance ceilings and collapse when applying RL to complex interactive tasks, leaving their true agentic potential untapped. To address this, we introduce \textbf{D}ynamic-filter \textbf{S}equence-level \textbf{P}olicy \textbf{O}ptimization (DSPO), an improved RL algorithm designed for robust agent training through sequence-level optimization and dynamic sample filtering. We train our model purely through RL to interleave multi-turn search and reasoning, obviating the need for supervised demonstration data. Across multiple QA benchmarks, our DSPO-trained 7B model improves over a comparable previous work by \textbf{34.1\%}, and even outperforms the 14B model from previous work in complex multihop QA such as HotpotQA by nearly \textbf{9\% relative}, maintaining exceptional training stability.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Data Contamination from Reinforcement Learning Post-training for Large Language Models</title>
<link>https://arxiv.org/abs/2510.09259</link>
<guid>https://arxiv.org/abs/2510.09259</guid>
<content:encoded><![CDATA[

arXiv:2510.09259v1 Announce Type: new 
Abstract: Data contamination poses a significant threat to the reliable evaluation of Large Language Models (LLMs). This issue arises when benchmark samples may inadvertently appear in training sets, compromising the validity of reported performance. While detection methods have been developed for the pre-training and Supervised Fine-Tuning stages, a critical research gap exists for the increasingly significant phase of Reinforcement Learning (RL) post-training. As RL post-training becomes pivotal for advancing LLM reasoning, the absence of specialized contamination detection methods in this paradigm presents a critical vulnerability. To address this, we conduct the first systematic study of data detection within RL post-training scenario and propose Self-Critique. Our method is motivated by a key observation: after RL phase, the output entropy distribution of LLMs tends to collapse into highly specific and sparse modes. Self-Critique probes for the underlying policy collapse, i.e., the model's convergence to a narrow reasoning path, which causes this entropy reduction. To facilitate this research, we also introduce RL-MIA, a benchmark constructed to simulate this specific contamination scenario. Extensive experiments show that Self-Critique significantly outperforms baseline methods across multiple models and contamination tasks, achieving an AUC improvement of up to 30%. Whereas existing methods are close to a random guess for RL-phase contamination, our method makes detection possible.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CFVBench: A Comprehensive Video Benchmark for Fine-grained Multimodal Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2510.09266</link>
<guid>https://arxiv.org/abs/2510.09266</guid>
<content:encoded><![CDATA[

arXiv:2510.09266v1 Announce Type: new 
Abstract: Multimodal Retrieval-Augmented Generation (MRAG) enables Multimodal Large Language Models (MLLMs) to generate responses with external multimodal evidence, and numerous video-based MRAG benchmarks have been proposed to evaluate model capabilities across retrieval and generation stages. However, existing benchmarks remain limited in modality coverage and format diversity, often focusing on single- or limited-modality tasks, or coarse-grained scene understanding. To address these gaps, we introduce CFVBench, a large-scale, manually verified benchmark constructed from 599 publicly available videos, yielding 5,360 open-ended QA pairs. CFVBench spans high-density formats and domains such as chart-heavy reports, news broadcasts, and software tutorials, requiring models to retrieve and reason over long temporal video spans while maintaining fine-grained multimodal information. Using CFVBench, we systematically evaluate 7 retrieval methods and 14 widely-used MLLMs, revealing a critical bottleneck: current models (even GPT5 or Gemini) struggle to capture transient yet essential fine-grained multimodal details. To mitigate this, we propose Adaptive Visual Refinement (AVR), a simple yet effective framework that adaptively increases frame sampling density and selectively invokes external tools when necessary. Experiments show that AVR consistently enhances fine-grained multimodal comprehension and improves performance across all evaluated MLLMs
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inflated Excellence or True Performance? Rethinking Medical Diagnostic Benchmarks with Dynamic Evaluation</title>
<link>https://arxiv.org/abs/2510.09275</link>
<guid>https://arxiv.org/abs/2510.09275</guid>
<content:encoded><![CDATA[

arXiv:2510.09275v1 Announce Type: new 
Abstract: Medical diagnostics is a high-stakes and complex domain that is critical to patient care. However, current evaluations of large language models (LLMs) are fundamentally misaligned with real-world clinical practice. Most of them rely on static benchmarks derived from public medical exam items, which tend to overestimate model performance and ignore the difference between textbook cases and the ambiguous, varying conditions in the real world. Recent efforts toward dynamic evaluation offer a promising alternative, but their improvements are limited to superficial perturbations and a narrow focus on accuracy. To address these gaps, we propose DyReMe, a dynamic benchmark for medical diagnostics that better reflects real clinical practice. Unlike static exam-style questions, DyReMe generates fresh, consultation-like cases that introduce distractors such as differential diagnoses and common misdiagnosis factors. It also varies expression styles to mimic diverse real-world query habits. Beyond accuracy, DyReMe evaluates LLMs on three additional clinically relevant dimensions: veracity, helpfulness, and consistency. Our experiments demonstrate that this dynamic approach yields more challenging and realistic assessments, revealing significant misalignments between the performance of state-of-the-art LLMs and real clinical practice. These findings highlight the urgent need for evaluation frameworks that better reflect the demands of trustworthy medical diagnostics.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLARity: Reasoning Consistency Alone Can Teach Reinforced Experts</title>
<link>https://arxiv.org/abs/2510.09278</link>
<guid>https://arxiv.org/abs/2510.09278</guid>
<content:encoded><![CDATA[

arXiv:2510.09278v1 Announce Type: new 
Abstract: Training expert LLMs in domains with scarce data is difficult, often relying on multiple-choice questions (MCQs). However, standard outcome-based reinforcement learning (RL) on MCQs is risky. While it may improve accuracy, we observe it often degrades reasoning quality such as logical consistency. Existing solutions to supervise reasoning, such as large-scale Process Reward Models (PRMs), are prohibitively expensive. To address this, we propose CLARity, a cost-effective RL framework that enhances reasoning quality using only a small, general-purpose LLM. CLARity integrates a consistency-aware reward mechanism with a 2-stage refine-then-monitor training pipeline to enhance reasoning consistency, and a dynamic data reformulation strategy to to better exploit limited data. Experiments demonstrate that CLARity improves response consistency by 16.5% and accuracy by 7.5% over baselines. Human evaluations further confirm holistic improvements in coherence and professionalism. Thus, CLARity offers a generalizable solution that enables smaller models to effectively guide expert models by reasoning consistency.Our code is open sourced at: https://github.com/Infinite-set/CLARity
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Sentence, Two Embeddings: Contrastive Learning of Explicit and Implicit Semantic Representations</title>
<link>https://arxiv.org/abs/2510.09293</link>
<guid>https://arxiv.org/abs/2510.09293</guid>
<content:encoded><![CDATA[

arXiv:2510.09293v1 Announce Type: new 
Abstract: Sentence embedding methods have made remarkable progress, yet they still struggle to capture the implicit semantics within sentences. This can be attributed to the inherent limitations of conventional sentence embedding methods that assign only a single vector per sentence. To overcome this limitation, we propose DualCSE, a sentence embedding method that assigns two embeddings to each sentence: one representing the explicit semantics and the other representing the implicit semantics. These embeddings coexist in the shared space, enabling the selection of the desired semantics for specific purposes such as information retrieval and text classification. Experimental results demonstrate that DualCSE can effectively encode both explicit and implicit meanings and improve the performance of the downstream task.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaP: A Unified Framework for Reliable Evaluation of Pre-training Dynamics</title>
<link>https://arxiv.org/abs/2510.09295</link>
<guid>https://arxiv.org/abs/2510.09295</guid>
<content:encoded><![CDATA[

arXiv:2510.09295v1 Announce Type: new 
Abstract: Reliable evaluation is fundamental to the progress of Large Language Models (LLMs), yet the evaluation process during pre-training is plagued by significant instability that obscures true learning dynamics. In this work, we systematically diagnose this instability, attributing it to two distinct sources: \textit{Parameter Instability} from training stochasticity and \textit{Evaluation Instability} from noisy measurement protocols. To counteract both sources of noise, we introduce \textbf{MaP}, a dual-pronged framework that synergistically integrates checkpoint \underline{M}erging \underline{a}nd the \underline{P}ass@k metric. Checkpoint merging smooths the parameter space by averaging recent model weights, while Pass@k provides a robust, low-variance statistical estimate of model capability. Extensive experiments show that MaP yields significantly smoother performance curves, reduces inter-run variance, and ensures more consistent model rankings. Ultimately, MaP provides a more reliable and faithful lens for observing LLM training dynamics, laying a crucial empirical foundation for LLM research.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShiZhi: A Chinese Lightweight Large Language Model for Court View Generation</title>
<link>https://arxiv.org/abs/2510.09297</link>
<guid>https://arxiv.org/abs/2510.09297</guid>
<content:encoded><![CDATA[

arXiv:2510.09297v1 Announce Type: new 
Abstract: Criminal Court View Generation (CVG) is a fundamental task in legal artificial intelligence, aiming to automatically generate the "Court View" section of a legal case document. Generating court views is challenging due to the diversity and complexity of case facts, and directly generating from raw facts may limit performance. In this paper, we present ShiZhi, the first large language model (LLM) specifically designed for court view generation. We construct a Chinese Court View Generation dataset, CCVG, of more than 110K cases, each containing fact descriptions paired with corresponding court views. Based on this dataset, ShiZhi achieving 58.5 BLEU-1 on court view generation and 86.1\% accuracy with 92.5\% macro F1 on charge prediction. Experimental results demonstrate that even a small LLM can generate reasonable and legally coherent court views when trained on high-quality domain-specific data. Our model and dataset are available at \href{https://github.com/ZhitianHou/ShiZhi}{https://github.com/ZhitianHou/ShiZhi}.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mask Tokens as Prophet: Fine-Grained Cache Eviction for Efficient dLLM Inference</title>
<link>https://arxiv.org/abs/2510.09309</link>
<guid>https://arxiv.org/abs/2510.09309</guid>
<content:encoded><![CDATA[

arXiv:2510.09309v1 Announce Type: new 
Abstract: Diffusion large language models (dLLMs) present a promising alternative to dominant autoregressive models (ARMs) by the ability of parallel decoding at the expense of substantial computation and memory costs. Specifically, the cache mechanism for bidirectional attention in dLLMs demands large memory footprint, restricting their ability to handle long contexts under resource-limited settings. Existing cache eviction strategies are designed for ARMs and ignore the unique characteristics of dLLMs, thus leading to unsatisfactory performance. To address these challenges, we introduce MaskKV, a training-free cache eviction framework tailored to dLLMs, focusing on the effect of mask tokens in dLLMs. MaskKV is built on two key innovations: (1) a mask-query guided scoring mechanism that leverages attention weights to identify and evict less critical prompt tokens for each head; (2) an adaptive cache budgeting strategy that improves efficiency by reducing allocation in intermediate layers and concentrating resources on prompt-preferring heads. On LLaDA with MaskKV, compressing the KV cache to only 256 pairs (less than 5% of tokens) retains 94% of the full-cache performance on LongBench and achieves up to 31x acceleration at 32k prompt length. The code is publicly available at: https://github.com/jianuo-huang/MaskKV
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Verifying Chain-of-Thought Reasoning via Its Computational Graph</title>
<link>https://arxiv.org/abs/2510.09312</link>
<guid>https://arxiv.org/abs/2510.09312</guid>
<content:encoded><![CDATA[

arXiv:2510.09312v1 Announce Type: new 
Abstract: Current Chain-of-Thought (CoT) verification methods predict reasoning correctness based on outputs (black-box) or activations (gray-box), but offer limited insight into why a computation fails. We introduce a white-box method: Circuit-based Reasoning Verification (CRV). We hypothesize that attribution graphs of correct CoT steps, viewed as execution traces of the model's latent reasoning circuits, possess distinct structural fingerprints from those of incorrect steps. By training a classifier on structural features of these graphs, we show that these traces contain a powerful signal of reasoning errors. Our white-box approach yields novel scientific insights unattainable by other methods. (1) We demonstrate that structural signatures of error are highly predictive, establishing the viability of verifying reasoning directly via its computational graph. (2) We find these signatures to be highly domain-specific, revealing that failures in different reasoning tasks manifest as distinct computational patterns. (3) We provide evidence that these signatures are not merely correlational; by using our analysis to guide targeted interventions on individual transcoder features, we successfully correct the model's faulty reasoning. Our work shows that, by scrutinizing a model's computational process, we can move from simple error detection to a deeper, causal understanding of LLM reasoning.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLRC: Fine-grained Low-Rank Compressor for Efficient LLM Inference</title>
<link>https://arxiv.org/abs/2510.09332</link>
<guid>https://arxiv.org/abs/2510.09332</guid>
<content:encoded><![CDATA[

arXiv:2510.09332v1 Announce Type: new 
Abstract: Although large language models (LLM) have achieved remarkable performance, their enormous parameter counts hinder deployment on resource-constrained hardware. Low-rank compression can reduce both memory usage and computational demand, but applying a uniform compression ratio across all layers often leads to significant performance degradation, and previous methods perform poorly during decoding. To address these issues, we propose the Fine-grained Low-Rank Compressor (FLRC), which efficiently determines an optimal rank allocation for each layer, and incorporates progressive low-rank decoding to maintain text generation quality. Comprehensive experiments on diverse benchmarks demonstrate the superiority of FLRC, achieving up to a 17% improvement in ROUGE-L on summarization tasks compared to state-of-the-art low-rank compression methods, establishing a more robust and efficient framework to improve LLM inference.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLP: LLM-based Product Pricing in E-commerce</title>
<link>https://arxiv.org/abs/2510.09347</link>
<guid>https://arxiv.org/abs/2510.09347</guid>
<content:encoded><![CDATA[

arXiv:2510.09347v1 Announce Type: new 
Abstract: Unlike Business-to-Consumer e-commerce platforms (e.g., Amazon), inexperienced individual sellers on Consumer-to-Consumer platforms (e.g., eBay) often face significant challenges in setting prices for their second-hand products efficiently. Therefore, numerous studies have been proposed for automating price prediction. However, most of them are based on static regression models, which suffer from poor generalization performance and fail to capture market dynamics (e.g., the price of a used iPhone decreases over time). Inspired by recent breakthroughs in Large Language Models (LLMs), we introduce LLP, the first LLM-based generative framework for second-hand product pricing. LLP first retrieves similar products to better align with the dynamic market change. Afterwards, it leverages the LLMs' nuanced understanding of key pricing information in free-form text to generate accurate price suggestions. To strengthen the LLMs' domain reasoning over retrieved products, we apply a two-stage optimization, supervised fine-tuning (SFT) followed by group relative policy optimization (GRPO), on a dataset built via bidirectional reasoning. Moreover, LLP employs a confidence-based filtering mechanism to reject unreliable price suggestions. Extensive experiments demonstrate that LLP substantially surpasses existing methods while generalizing well to unseen categories. We have successfully deployed LLP on Xianyu\footnote\{Xianyu is China's largest second-hand e-commerce platform.\}, significantly outperforming the previous pricing method. Under the same 30\% product coverage, it raises the static adoption rate (SAR) from 40\% to 72\%, and maintains a strong SAR of 47\% even at 90\% recall.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReTraceQA: Evaluating Reasoning Traces of Small Language Models in Commonsense Question Answering</title>
<link>https://arxiv.org/abs/2510.09351</link>
<guid>https://arxiv.org/abs/2510.09351</guid>
<content:encoded><![CDATA[

arXiv:2510.09351v1 Announce Type: new 
Abstract: While Small Language Models (SLMs) have demonstrated promising performance on an increasingly wide array of commonsense reasoning benchmarks, current evaluation practices rely almost exclusively on the accuracy of their final answers, neglecting the validity of the reasoning processes that lead to those answers. To address this issue, we introduce ReTraceQA, a novel benchmark that introduces process-level evaluation for commonsense reasoning tasks. Our expert-annotated dataset reveals that in a substantial portion of instances (14-24%), SLMs provide correct final answers despite flawed reasoning processes, suggesting that the capabilities of SLMs are often overestimated by evaluation metrics that focus only on comparing the final answer with the ground truth. Indeed, we show that when employing strong Large Language Models (LLMs) as automated judges for reasoning-aware evaluation rather than answer-only metrics, SLM performance drops significantly across all models and datasets, with scores decreasing by up to 25%.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Logit Arithmetic Elicits Long Reasoning Capabilities Without Training</title>
<link>https://arxiv.org/abs/2510.09354</link>
<guid>https://arxiv.org/abs/2510.09354</guid>
<content:encoded><![CDATA[

arXiv:2510.09354v1 Announce Type: new 
Abstract: Large reasoning models exhibit long chain-of-thought reasoning with strategies such as backtracking and self-correction, though recent studies suggest that these abilities typically require additional training. We first investigate whether such behaviors can be elicited without any training. To this end, we propose a decoding-time approach, ThinkLogit, which utilizes logit arithmetic to tune a target large non-reasoning model for long reasoning using a substantially smaller reasoning model as the guider. We then show that we can further boost its performance by training the guider model with preference optimization over correct/incorrect reasoning pairs sampled from both the target and guider model, a setup we refer to as ThinkLogit-DPO. Our experiments demonstrate that ThinkLogit and ThinkLogit-DPO achieve a relative improvement in average accuracy by 24.5% and 29.1%, respectively, over five reasoning benchmarks using the Qwen2.5-32B guided by R1-Distill-Qwen-1.5B, a model 21x smaller. Moreover, we find that ThinkLogit remains effective when the guider and target come from different model families. It is also orthogonal to post-training methods for small models, as guiders improved through supervised distillation or reinforcement learning can be directly plugged in to yield stronger large models, offering a practical path to unlock long reasoning in large-scale models without costly post-training.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NL2GenSym: Natural Language to Generative Symbolic Rules for SOAR Cognitive Architecture via Large Language Models</title>
<link>https://arxiv.org/abs/2510.09355</link>
<guid>https://arxiv.org/abs/2510.09355</guid>
<content:encoded><![CDATA[

arXiv:2510.09355v1 Announce Type: new 
Abstract: SOAR, a classic symbol-based cognitive architecture, has been fostering the development of general, human-like intelligent agents. Nevertheless, its practical adoption is hindered by the laborious manual rule coding. Emerging Large Language Models (LLMs) present the immense potential for efficient rules generation. However, there is a critical gap that current research predominantly focuses on conceptual frameworks and lacks robust experimental validation. To bridge this gap, we propose \textit{N}atural \textit{L}anguage to \textit{Gen}erative \textit{Sym}bolic Rules (NL2GenSym), a novel framework that integrates LLMs with SOAR to autonomously produce generative symbolic rules from natural language. Specifically, our framework introduces a novel Execution-Grounded Generator-Critic mechanism. The LLM-based Generator, guided by a Retrieval-Augmented Generation-accessed self-evolving domain knowledge base, proposes rules from natural language. Subsequently, these rules are immediately executed within the SOAR environment to rigorously validate their correctness. Based on this execution-grounded feedback, a reflective LLM-based Critic drives the iterative refinement of these rules. Experiments on our specialized Water Jug Problem (WJP) dataset, utilizing both Gemini and Qwen series models, validate the efficacy of our framework. It achieves a success rate over 86\% in generating rules from natural language. Crucially, the framework also generates novel heuristic rules, reducing average decision cycles for solving the WJP to 1.98 times the optimal solution and 1/1000 of baseline methods. Additionally, our initial experiments show that NL2GenSym enables smaller-parameter models to achieve better performance than larger counterparts.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding the Effects of Domain Finetuning on LLMs</title>
<link>https://arxiv.org/abs/2510.09359</link>
<guid>https://arxiv.org/abs/2510.09359</guid>
<content:encoded><![CDATA[

arXiv:2510.09359v1 Announce Type: new 
Abstract: Large Language Models (LLMs) fine-tuned for specific domains exhibit strong performance; however, the underlying mechanisms by which this fine-tuning reshapes their parametric space are not well understood. Prior works primarily focus on auto-regressive or general-purpose instruct models, leaving domain-specialised LLMs under-explored. We present the first systematic study of domain-specific fine-tuning in large medical language models. Our analysis reveals that fine-tuning modifies only a small subset of the representational subspace, essentially preserving the pre-trained model's representation. To interpret these changes in subspaces, we propose tuning vectors, a novel framework inspired by task vectors, which explicitly capture the directional parameter shifts induced by fine-tuning. We demonstrate that these vectors are critical for enhancing both instruction-following and generation quality. Furthermore, combining tuning vectors across different domains yields improved generalisation. Upon closer inspection of directional alignment, we find these vectors primarily write new directional information into the MLP layers of the model, while amplifying existing directions in attention heads. Our findings offer new insights into LLM adaptation and provide a general, interpretable framework for analysing specialisation in large language models.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Token-Level Policy Optimization: Linking Group-Level Rewards to Token-Level Aggregation via Markov Likelihood</title>
<link>https://arxiv.org/abs/2510.09369</link>
<guid>https://arxiv.org/abs/2510.09369</guid>
<content:encoded><![CDATA[

arXiv:2510.09369v1 Announce Type: new 
Abstract: Group Relative Policy Optimization (GRPO) has significantly advanced the reasoning ability of large language models (LLMs), particularly by boosting their mathematical performance. However, GRPO and related entropy-regularization methods still face challenges rooted in the sparse token rewards inherent to chain-of-thought (CoT). Current approaches often rely on undifferentiated token-level entropy adjustments, which frequently lead to entropy collapse or model collapse. In this work, we propose TEPO, a novel token-level framework that incorporates Markov Likelihood (sequence likelihood) links group-level rewards with tokens via token-level aggregation. Experiments show that TEPO consistently outperforms existing baselines across key metrics (including @k and accuracy). It not only sets a new state of the art on mathematical reasoning tasks but also significantly enhances training stability.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying &amp; Interactively Refining Ambiguous User Goals for Data Visualization Code Generation</title>
<link>https://arxiv.org/abs/2510.09390</link>
<guid>https://arxiv.org/abs/2510.09390</guid>
<content:encoded><![CDATA[

arXiv:2510.09390v1 Announce Type: new 
Abstract: Establishing shared goals is a fundamental step in human-AI communication. However, ambiguities can lead to outputs that seem correct but fail to reflect the speaker's intent. In this paper, we explore this issue with a focus on the data visualization domain, where ambiguities in natural language impact the generation of code that visualizes data. The availability of multiple views on the contextual (e.g., the intended plot and the code rendering the plot) allows for a unique and comprehensive analysis of diverse ambiguity types. We develop a taxonomy of types of ambiguity that arise in this task and propose metrics to quantify them. Using Matplotlib problems from the DS-1000 dataset, we demonstrate that our ambiguity metrics better correlate with human annotations than uncertainty baselines. Our work also explores how multi-turn dialogue can reduce ambiguity, therefore, improve code accuracy by better matching user goals. We evaluate three pragmatic models to inform our dialogue strategies: Gricean Cooperativity, Discourse Representation Theory, and Questions under Discussion. A simulated user study reveals how pragmatic dialogues reduce ambiguity and enhance code accuracy, highlighting the value of multi-turn exchanges in code generation.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Single-Granularity Prompts: A Multi-Scale Chain-of-Thought Prompt Learning for Graph</title>
<link>https://arxiv.org/abs/2510.09394</link>
<guid>https://arxiv.org/abs/2510.09394</guid>
<content:encoded><![CDATA[

arXiv:2510.09394v1 Announce Type: new 
Abstract: The "pre-train, prompt'' paradigm, designed to bridge the gap between pre-training tasks and downstream objectives, has been extended from the NLP domain to the graph domain and has achieved remarkable progress. Current mainstream graph prompt-tuning methods modify input or output features using learnable prompt vectors. However, existing approaches are confined to single-granularity (e.g., node-level or subgraph-level) during prompt generation, overlooking the inherently multi-scale structural information in graph data, which limits the diversity of prompt semantics. To address this issue, we pioneer the integration of multi-scale information into graph prompt and propose a Multi-Scale Graph Chain-of-Thought (MSGCOT) prompting framework. Specifically, we design a lightweight, low-rank coarsening network to efficiently capture multi-scale structural features as hierarchical basis vectors for prompt generation. Subsequently, mimicking human cognition from coarse-to-fine granularity, we dynamically integrate multi-scale information at each reasoning step, forming a progressive coarse-to-fine prompt chain. Extensive experiments on eight benchmark datasets demonstrate that MSGCOT outperforms the state-of-the-art single-granularity graph prompt-tuning method, particularly in few-shot scenarios, showcasing superior performance.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Model Selection for Large Language Models</title>
<link>https://arxiv.org/abs/2510.09418</link>
<guid>https://arxiv.org/abs/2510.09418</guid>
<content:encoded><![CDATA[

arXiv:2510.09418v1 Announce Type: new 
Abstract: We introduce LLM SELECTOR, the first framework for active model selection of Large Language Models (LLMs). Unlike prior evaluation and benchmarking approaches that rely on fully annotated datasets, LLM SELECTOR efficiently identifies the best LLM with limited annotations. In particular, for any given task, LLM SELECTOR adaptively selects a small set of queries to annotate that are most informative about the best model for the task. To further reduce annotation cost, we leverage a judge-based oracle annotation model. Through extensive experiments on 6 benchmarks with 151 LLMs, we show that LLM SELECTOR reduces annotation costs by up to 59.62% when selecting the best and near-best LLM for the task.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Representations of Entities in Auto-regressive Large Language Models</title>
<link>https://arxiv.org/abs/2510.09421</link>
<guid>https://arxiv.org/abs/2510.09421</guid>
<content:encoded><![CDATA[

arXiv:2510.09421v1 Announce Type: new 
Abstract: Named entities are fundamental building blocks of knowledge in text, grounding factual information and structuring relationships within language. Despite their importance, it remains unclear how Large Language Models (LLMs) internally represent entities. Prior research has primarily examined explicit relationships, but little is known about entity representations themselves. We introduce entity mention reconstruction as a novel framework for studying how LLMs encode and manipulate entities. We investigate whether entity mentions can be generated from internal representations, how multi-token entities are encoded beyond last-token embeddings, and whether these representations capture relational knowledge. Our proposed method, leveraging _task vectors_, allows to consistently generate multi-token mentions from various entity representations derived from the LLMs hidden states. We thus introduce the _Entity Lens_, extending the _logit-lens_ to predict multi-token mentions. Our results bring new evidence that LLMs develop entity-specific mechanisms to represent and manipulate any multi-token entities, including those unseen during training. Our code is avalable at https://github.com/VictorMorand/EntityRepresentations .
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Speech-LLM Takes It All: A Truly Fully End-to-End Spoken Dialogue State Tracking Approach</title>
<link>https://arxiv.org/abs/2510.09424</link>
<guid>https://arxiv.org/abs/2510.09424</guid>
<content:encoded><![CDATA[

arXiv:2510.09424v1 Announce Type: new 
Abstract: This paper presents a comparative study of context management strategies for end-to-end Spoken Dialog State Tracking using Speech-LLMs. We systematically evaluate traditional multimodal context (combining text history and spoken current turn), full spoken history, and compressed spoken history approaches. Our experiments on the SpokenWOZ corpus demonstrate that providing the full spoken conversation as input yields the highest performance among models of similar size, significantly surpassing prior methods. Furthermore, we show that attention-pooling-based compression of the spoken history offers a strong trade-off, maintaining competitive accuracy with reduced context size. Detailed analysis confirms that improvements stem from more effective context utilization.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KORMo: Korean Open Reasoning Model for Everyone</title>
<link>https://arxiv.org/abs/2510.09426</link>
<guid>https://arxiv.org/abs/2510.09426</guid>
<content:encoded><![CDATA[

arXiv:2510.09426v1 Announce Type: new 
Abstract: This work presents the first large-scale investigation into constructing a fully open bilingual large language model (LLM) for a non-English language, specifically Korean, trained predominantly on synthetic data. We introduce KORMo-10B, a 10.8B-parameter model trained from scratch on a Korean-English corpus in which 68.74% of the Korean portion is synthetic. Through systematic experimentation, we demonstrate that synthetic data, when carefully curated with balanced linguistic coverage and diverse instruction styles, does not cause instability or degradation during large-scale pretraining. Furthermore, the model achieves performance comparable to that of contemporary open-weight multilingual baselines across a wide range of reasoning, knowledge, and instruction-following benchmarks. Our experiments reveal two key findings: (1) synthetic data can reliably sustain long-horizon pretraining without model collapse, and (2) bilingual instruction tuning enables near-native reasoning and discourse coherence in Korean. By fully releasing all components including data, code, training recipes, and logs, this work establishes a transparent framework for developing synthetic data-driven fully open models (FOMs) in low-resource settings and sets a reproducible precedent for future multilingual LLM research.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain-Adapted Pre-trained Language Models for Implicit Information Extraction in Crash Narratives</title>
<link>https://arxiv.org/abs/2510.09434</link>
<guid>https://arxiv.org/abs/2510.09434</guid>
<content:encoded><![CDATA[

arXiv:2510.09434v1 Announce Type: new 
Abstract: Free-text crash narratives recorded in real-world crash databases have been shown to play a significant role in improving traffic safety. However, large-scale analyses remain difficult to implement as there are no documented tools that can batch process the unstructured, non standardized text content written by various authors with diverse experience and attention to detail. In recent years, Transformer-based pre-trained language models (PLMs), such as Bidirectional Encoder Representations from Transformers (BERT) and large language models (LLMs), have demonstrated strong capabilities across various natural language processing tasks. These models can extract explicit facts from crash narratives, but their performance declines on inference-heavy tasks in, for example, Crash Type identification, which can involve nearly 100 categories. Moreover, relying on closed LLMs through external APIs raises privacy concerns for sensitive crash data. Additionally, these black-box tools often underperform due to limited domain knowledge. Motivated by these challenges, we study whether compact open-source PLMs can support reasoning-intensive extraction from crash narratives. We target two challenging objectives: 1) identifying the Manner of Collision for a crash, and 2) Crash Type for each vehicle involved in the crash event from real-world crash narratives. To bridge domain gaps, we apply fine-tuning techniques to inject task-specific knowledge to LLMs with Low-Rank Adaption (LoRA) and BERT. Experiments on the authoritative real-world dataset Crash Investigation Sampling System (CISS) demonstrate that our fine-tuned compact models outperform strong closed LLMs, such as GPT-4o, while requiring only minimal training resources. Further analysis reveals that the fine-tuned PLMs can capture richer narrative details and even correct some mislabeled annotations in the dataset.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Getting Your Indices in a Row: Full-Text Search for LLM Training Data for Real World</title>
<link>https://arxiv.org/abs/2510.09471</link>
<guid>https://arxiv.org/abs/2510.09471</guid>
<content:encoded><![CDATA[

arXiv:2510.09471v1 Announce Type: new 
Abstract: The performance of Large Language Models (LLMs) is determined by their training data. Despite the proliferation of open-weight LLMs, access to LLM training data has remained limited. Even for fully open LLMs, the scale of the data makes it all but inscrutable to the general scientific community, despite potentially containing critical data scraped from the internet.
  In this paper, we present the full-text indexing pipeline for the Apertus LLM training data. Leveraging Elasticsearch parallel indices and the Alps infrastructure, a state-of-the-art, highly energy-efficient arm64 supercluster, we were able to index 8.6T tokens out of 15.2T used to train the Apertus LLM family, creating both a critical LLM safety tool and effectively an offline, curated, open web search engine. Our contribution is threefold. First, we demonstrate that Elasticsearch can be successfully ported onto next-generation arm64-based infrastructure. Second, we demonstrate that full-text indexing at the scale of modern LLM training datasets and the entire open web is feasible and accessible. Finally, we demonstrate that such indices can be used to ensure previously inaccessible jailbreak-agnostic LLM safety.
  We hope that our findings will be useful to other teams attempting large-scale data indexing and facilitate the general transition towards greener computation.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Models for Natural Language Reasoning: The Case of Syllogistic Logic</title>
<link>https://arxiv.org/abs/2510.09472</link>
<guid>https://arxiv.org/abs/2510.09472</guid>
<content:encoded><![CDATA[

arXiv:2510.09472v1 Announce Type: new 
Abstract: Despite the remarkable progress in neural models, their ability to generalize, a cornerstone for applications like logical reasoning, remains a critical challenge. We delineate two fundamental aspects of this ability: compositionality, the capacity to abstract atomic logical rules underlying complex inferences, and recursiveness, the aptitude to build intricate representations through iterative application of inference rules. In the literature, these two aspects are often confounded together under the umbrella term of generalization. To sharpen this distinction, we investigated the logical generalization capabilities of pre-trained large language models (LLMs) using the syllogistic fragment as a benchmark for natural language reasoning. Though simple, this fragment provides a foundational yet expressive subset of formal logic that supports controlled evaluation of essential reasoning abilities. Our findings reveal a significant disparity: while LLMs demonstrate reasonable proficiency in recursiveness, they struggle with compositionality. To overcome these limitations and establish a reliable logical prover, we propose a hybrid architecture integrating symbolic reasoning with neural computation. This synergistic interaction enables robust and efficient inference, neural components accelerate processing, while symbolic reasoning ensures completeness. Our experiments show that high efficiency is preserved even with relatively small neural components. As part of our proposed methodology, this analysis gives a rationale and highlights the potential of hybrid models to effectively address key generalization barriers in neural reasoning systems.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Policy Internalization for Conversational Agents</title>
<link>https://arxiv.org/abs/2510.09474</link>
<guid>https://arxiv.org/abs/2510.09474</guid>
<content:encoded><![CDATA[

arXiv:2510.09474v1 Announce Type: new 
Abstract: Modern conversational agents like ChatGPT and Alexa+ rely on predefined policies specifying metadata, response styles, and tool-usage rules. As these LLM-based systems expand to support diverse business and user queries, such policies, often implemented as in-context prompts, are becoming increasingly complex and lengthy, making faithful adherence difficult and imposing large fixed computational costs. With the rise of multimodal agents, policies that govern visual and multimodal behaviors are critical but remain understudied. Prior prompt-compression work mainly shortens task templates and demonstrations, while existing policy-alignment studies focus only on text-based safety rules. We introduce Multimodal Policy Internalization (MPI), a new task that internalizes reasoning-intensive multimodal policies into model parameters, enabling stronger policy-following without including the policy during inference. MPI poses unique data and algorithmic challenges. We build two datasets spanning synthetic and real-world decision-making and tool-using tasks and propose TriMPI, a three-stage training framework. TriMPI first injects policy knowledge via continual pretraining, then performs supervised finetuning, and finally applies PolicyRollout, a GRPO-style reinforcement learning extension that augments rollouts with policy-aware responses for grounded exploration. TriMPI achieves notable gains in end-to-end accuracy, generalization, and robustness to forgetting. As the first work on multimodal policy internalization, we provide datasets, training recipes, and comprehensive evaluations to foster future research. Project page: https://mikewangwzhl.github.io/TriMPI.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StatEval: A Comprehensive Benchmark for Large Language Models in Statistics</title>
<link>https://arxiv.org/abs/2510.09517</link>
<guid>https://arxiv.org/abs/2510.09517</guid>
<content:encoded><![CDATA[

arXiv:2510.09517v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated remarkable advances in mathematical and logical reasoning, yet statistics, as a distinct and integrative discipline, remains underexplored in benchmarking efforts. To address this gap, we introduce \textbf{StatEval}, the first comprehensive benchmark dedicated to statistics, spanning both breadth and depth across difficulty levels. StatEval consists of 13,817 foundational problems covering undergraduate and graduate curricula, together with 2374 research-level proof tasks extracted from leading journals. To construct the benchmark, we design a scalable multi-agent pipeline with human-in-the-loop validation that automates large-scale problem extraction, rewriting, and quality control, while ensuring academic rigor. We further propose a robust evaluation framework tailored to both computational and proof-based tasks, enabling fine-grained assessment of reasoning ability. Experimental results reveal that while closed-source models such as GPT5-mini achieve below 57\% on research-level problems, with open-source models performing significantly lower. These findings highlight the unique challenges of statistical reasoning and the limitations of current LLMs. We expect StatEval to serve as a rigorous benchmark for advancing statistical intelligence in large language models. All data and code are available on our web platform: https://stateval.github.io/.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can We Reliably Rank Model Performance across Domains without Labeled Data?</title>
<link>https://arxiv.org/abs/2510.09519</link>
<guid>https://arxiv.org/abs/2510.09519</guid>
<content:encoded><![CDATA[

arXiv:2510.09519v1 Announce Type: new 
Abstract: Estimating model performance without labels is an important goal for understanding how NLP models generalize. While prior work has proposed measures based on dataset similarity or predicted correctness, it remains unclear when these estimates produce reliable performance rankings across domains. In this paper, we analyze the factors that affect ranking reliability using a two-step evaluation setup with four base classifiers and several large language models as error predictors. Experiments on the GeoOLID and Amazon Reviews datasets, spanning 15 domains, show that large language model-based error predictors produce stronger and more consistent rank correlations with true accuracy than drift-based or zero-shot baselines. Our analysis reveals two key findings: ranking is more reliable when performance differences across domains are larger, and when the error model's predictions align with the base model's true failure patterns. These results clarify when performance estimation methods can be trusted and provide guidance for their use in cross-domain model evaluation.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accent-Invariant Automatic Speech Recognition via Saliency-Driven Spectrogram Masking</title>
<link>https://arxiv.org/abs/2510.09528</link>
<guid>https://arxiv.org/abs/2510.09528</guid>
<content:encoded><![CDATA[

arXiv:2510.09528v1 Announce Type: new 
Abstract: Pre-trained transformer-based models have significantly advanced automatic speech recognition (ASR), yet they remain sensitive to accent and dialectal variations, resulting in elevated word error rates (WER) in linguistically diverse languages such as English and Persian. To address this challenge, we propose an accent-invariant ASR framework that integrates accent and dialect classification into the recognition pipeline. Our approach involves training a spectrogram-based classifier to capture accent-specific cues, masking the regions most influential to its predictions, and using the masked spectrograms for data augmentation. This enhances the robustness of ASR models against accent variability. We evaluate the method using both English and Persian speech. For Persian, we introduce a newly collected dataset spanning multiple regional accents, establishing the first systematic benchmark for accent variation in Persian ASR that fills a critical gap in multilingual speech research and provides a foundation for future studies on low-resource, linguistically diverse languages. Experimental results with the Whisper model demonstrate that our masking and augmentation strategy yields substantial WER reductions in both English and Persian settings, confirming the effectiveness of the approach. This research advances the development of multilingual ASR systems that are resilient to accent and dialect diversity. Code and dataset are publicly available at: https://github.com/MH-Sameti/Accent_invariant_ASR
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Overthinking through Reasoning Shaping</title>
<link>https://arxiv.org/abs/2510.09535</link>
<guid>https://arxiv.org/abs/2510.09535</guid>
<content:encoded><![CDATA[

arXiv:2510.09535v1 Announce Type: new 
Abstract: Large reasoning models (LRMs) boosted by Reinforcement Learning from Verifier Reward (RLVR) have shown great power in problem solving, yet they often cause overthinking: excessive, meandering reasoning that inflates computational cost. Prior designs of penalization in RLVR manage to reduce token consumption while often harming model performance, which arises from the oversimplicity of token-level supervision. In this paper, we argue that the granularity of supervision plays a crucial role in balancing efficiency and accuracy, and propose Group Relative Segment Penalization (GRSP), a step-level method to regularize reasoning. Since preliminary analyses show that reasoning segments are strongly correlated with token consumption and model performance, we design a length-aware weighting mechanism across segment clusters. Extensive experiments demonstrate that GRSP achieves superior token efficiency without heavily compromising accuracy, especially the advantages with harder problems. Moreover, GRSP stabilizes RL training and scales effectively across model sizes.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Robustness of Large Language Models Against Multilingual Typographical Errors</title>
<link>https://arxiv.org/abs/2510.09536</link>
<guid>https://arxiv.org/abs/2510.09536</guid>
<content:encoded><![CDATA[

arXiv:2510.09536v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed in multilingual, real-world applications with user inputs -- naturally introducing typographical errors (typos). Yet most benchmarks assume clean input, leaving the robustness of LLMs to typos across languages largely underexplored. To address this gap, we introduce MulTypo, a multilingual typo generation algorithm that simulates human-like errors based on language-specific keyboard layouts and typing behavior. We evaluate 18 open-source LLMs across three model families and five downstream tasks spanning language inference, multi-choice question answering, mathematical reasoning, and machine translation tasks. Our results show that typos consistently degrade performance, particularly in generative tasks and those requiring reasoning -- while the natural language inference task is comparatively more robust. Instruction tuning improves clean-input performance but may increase brittleness under noise. We also observe language-dependent robustness: high-resource languages are generally more robust than low-resource ones, and translation from English is more robust than translation into English. Our findings underscore the need for noise-aware training and multilingual robustness evaluation. We make our code and data publicly available.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPG: Sandwiched Policy Gradient for Masked Diffusion Language Models</title>
<link>https://arxiv.org/abs/2510.09541</link>
<guid>https://arxiv.org/abs/2510.09541</guid>
<content:encoded><![CDATA[

arXiv:2510.09541v1 Announce Type: new 
Abstract: Diffusion large language models (dLLMs) are emerging as an efficient alternative to autoregressive models due to their ability to decode multiple tokens in parallel. However, aligning dLLMs with human preferences or task-specific rewards via reinforcement learning (RL) is challenging because their intractable log-likelihood precludes the direct application of standard policy gradient methods. While prior work uses surrogates like the evidence lower bound (ELBO), these one-sided approximations can introduce significant policy gradient bias. To address this, we propose the Sandwiched Policy Gradient (SPG) that leverages both an upper and a lower bound of the true log-likelihood. Experiments show that SPG significantly outperforms baselines based on ELBO or one-step estimation. Specifically, SPG improves the accuracy over state-of-the-art RL methods for dLLMs by 3.6% in GSM8K, 2.6% in MATH500, 18.4% in Countdown and 27.0% in Sudoku.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Surface Reasoning: Unveiling the True Long Chain-of-Thought Capacity of Diffusion Large Language Models</title>
<link>https://arxiv.org/abs/2510.09544</link>
<guid>https://arxiv.org/abs/2510.09544</guid>
<content:encoded><![CDATA[

arXiv:2510.09544v1 Announce Type: new 
Abstract: Recently, Diffusion Large Language Models (DLLMs) have offered high throughput and effective sequential reasoning, making them a competitive alternative to autoregressive LLMs (ALLMs). However, parallel decoding, which enables simultaneous token updates, conflicts with the causal order often required for rigorous reasoning. We first identify this conflict as the core Parallel-Sequential Contradiction (PSC). Behavioral analyses in both simple and complex reasoning tasks show that DLLMs exhibit genuine parallelism only for directly decidable outputs. As task difficulty increases, they revert to autoregressive-like behavior, a limitation exacerbated by autoregressive prompting, which nearly doubles the number of decoding steps with remasking without improving quality. Moreover, PSC restricts DLLMs' self-reflection, reasoning depth, and exploratory breadth. To further characterize PSC, we introduce three scaling dimensions for DLLMs: parallel, diffusion, and sequential. Empirically, while parallel scaling yields consistent improvements, diffusion and sequential scaling are constrained by PSC. Based on these findings, we propose several practical mitigations, parallel-oriented prompting, diffusion early stopping, and parallel scaling, to reduce PSC-induced ineffectiveness and inefficiencies.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Indexing with Knowledge Enrichment for Multilingual Video Corpus Retrieval</title>
<link>https://arxiv.org/abs/2510.09553</link>
<guid>https://arxiv.org/abs/2510.09553</guid>
<content:encoded><![CDATA[

arXiv:2510.09553v1 Announce Type: new 
Abstract: Retrieving relevant instructional videos from multilingual medical archives is crucial for answering complex, multi-hop questions across language boundaries. However, existing systems either compress hour-long videos into coarse embeddings or incur prohibitive costs for fine-grained matching. We tackle the Multilingual Video Corpus Retrieval (mVCR) task in the NLPCC-2025 M4IVQA challenge with a multi-stage framework that integrates multilingual semantics, domain terminology, and efficient long-form processing. Video subtitles are divided into semantically coherent chunks, enriched with concise knowledge-graph (KG) facts, and organized into a hierarchical tree whose node embeddings are generated by a language-agnostic multilingual encoder. At query time, the same encoder embeds the input question; a coarse-to-fine tree search prunes irrelevant branches, and only the top-ranked chunks are re-scored by a lightweight large language model (LLM). This design avoids exhaustive cross-encoder scoring while preserving chunk-level precision. Experiments on the mVCR test set demonstrate state-of-the-art performance, and ablation studies confirm the complementary contributions of KG enrichment, hierarchical indexing, and targeted LLM re-ranking. The proposed method offers an accurate and scalable solution for multilingual retrieval in specialized medical video collections.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Evaluation of Multilingual Chain-of-Thought Reasoning: Performance, Consistency, and Faithfulness Across Languages</title>
<link>https://arxiv.org/abs/2510.09555</link>
<guid>https://arxiv.org/abs/2510.09555</guid>
<content:encoded><![CDATA[

arXiv:2510.09555v1 Announce Type: new 
Abstract: Large reasoning models (LRMs) increasingly rely on step-by-step Chain-of-Thought (CoT) reasoning to improve task performance, particularly in high-resource languages such as English. While recent work has examined final-answer accuracy in multilingual settings, the thinking traces themselves, i.e., the intermediate steps that lead to the final answer, remain underexplored. In this paper, we present the first comprehensive study of multilingual CoT reasoning, evaluating three key dimensions: performance, consistency, and faithfulness. We begin by measuring language compliance, answer accuracy, and answer consistency when LRMs are explicitly instructed or prompt-hacked to think in a target language, revealing strong language preferences and divergent performance across languages. Next, we assess crosslingual consistency of thinking traces by interchanging them between languages. We find that the quality and effectiveness of thinking traces vary substantially depending on the prompt language. Finally, we adapt perturbation-based techniques -- i.e., truncation and error injection -- to probe the faithfulness of thinking traces across languages, showing that models rely on traces to varying degrees. We release our code and data to support future research.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WUGNECTIVES: Novel Entity Inferences of Language Models from Discourse Connectives</title>
<link>https://arxiv.org/abs/2510.09556</link>
<guid>https://arxiv.org/abs/2510.09556</guid>
<content:encoded><![CDATA[

arXiv:2510.09556v1 Announce Type: new 
Abstract: The role of world knowledge has been particularly crucial to predict the discourse connective that marks the discourse relation between two arguments, with language models (LMs) being generally successful at this task. We flip this premise in our work, and instead study the inverse problem of understanding whether discourse connectives can inform LMs about the world. To this end, we present WUGNECTIVES, a dataset of 8,880 stimuli that evaluates LMs' inferences about novel entities in contexts where connectives link the entities to particular attributes. On investigating 17 different LMs at various scales, and training regimens, we found that tuning an LM to show reasoning behavior yields noteworthy improvements on most connectives. At the same time, there was a large variation in LMs' overall performance across connective type, with all models systematically struggling on connectives that express a concessive meaning. Our findings pave the way for more nuanced investigations into the functional role of language cues as captured by LMs. We release WUGNECTIVES at https://github.com/sheffwb/wugnectives.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoPR: Let's Automate Your Academic Promotion!</title>
<link>https://arxiv.org/abs/2510.09558</link>
<guid>https://arxiv.org/abs/2510.09558</guid>
<content:encoded><![CDATA[

arXiv:2510.09558v1 Announce Type: new 
Abstract: As the volume of peer-reviewed research surges, scholars increasingly rely on social platforms for discovery, while authors invest considerable effort in promoting their work to ensure visibility and citations. To streamline this process and reduce the reliance on human effort, we introduce Automatic Promotion (AutoPR), a novel task that transforms research papers into accurate, engaging, and timely public content. To enable rigorous evaluation, we release PRBench, a multimodal benchmark that links 512 peer-reviewed articles to high-quality promotional posts, assessing systems along three axes: Fidelity (accuracy and tone), Engagement (audience targeting and appeal), and Alignment (timing and channel optimization). We also introduce PRAgent, a multi-agent framework that automates AutoPR in three stages: content extraction with multimodal preparation, collaborative synthesis for polished outputs, and platform-specific adaptation to optimize norms, tone, and tagging for maximum reach. When compared to direct LLM pipelines on PRBench, PRAgent demonstrates substantial improvements, including a 604% increase in total watch time, a 438% rise in likes, and at least a 2.9x boost in overall engagement. Ablation studies show that platform modeling and targeted promotion contribute the most to these gains. Our results position AutoPR as a tractable, measurable research problem and provide a roadmap for scalable, impactful automated scholarly communication.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dyna-Mind: Learning to Simulate from Experience for Better AI Agents</title>
<link>https://arxiv.org/abs/2510.09577</link>
<guid>https://arxiv.org/abs/2510.09577</guid>
<content:encoded><![CDATA[

arXiv:2510.09577v1 Announce Type: new 
Abstract: Reasoning models have recently shown remarkable progress in domains such as math and coding. However, their expert-level abilities in math and coding contrast sharply with their performance in long-horizon, interactive tasks such as web navigation and computer/phone-use. Inspired by literature on human cognition, we argue that current AI agents need ''vicarious trial and error'' - the capacity to mentally simulate alternative futures before acting - in order to enhance their understanding and performance in complex interactive environments. We introduce Dyna-Mind, a two-stage training framework that explicitly teaches (V)LM agents to integrate such simulation into their reasoning. In stage 1, we introduce Reasoning with Simulations (ReSim), which trains the agent to generate structured reasoning traces from expanded search trees built from real experience gathered through environment interactions. ReSim thus grounds the agent's reasoning in faithful world dynamics and equips it with the ability to anticipate future states in its reasoning. In stage 2, we propose Dyna-GRPO, an online reinforcement learning method to further strengthen the agent's simulation and decision-making ability by using both outcome rewards and intermediate states as feedback from real rollouts. Experiments on two synthetic benchmarks (Sokoban and ALFWorld) and one realistic benchmark (AndroidWorld) demonstrate that (1) ReSim effectively infuses simulation ability into AI agents, and (2) Dyna-GRPO leverages outcome and interaction-level signals to learn better policies for long-horizon, planning-intensive tasks. Together, these results highlight the central role of simulation in enabling AI agents to reason, plan, and act more effectively in the ever more challenging environments.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind-Paced Speaking: A Dual-Brain Approach to Real-Time Reasoning in Spoken Language Models</title>
<link>https://arxiv.org/abs/2510.09592</link>
<guid>https://arxiv.org/abs/2510.09592</guid>
<content:encoded><![CDATA[

arXiv:2510.09592v1 Announce Type: new 
Abstract: Real-time Spoken Language Models (SLMs) struggle to leverage Chain-of-Thought (CoT) reasoning due to the prohibitive latency of generating the entire thought process sequentially. Enabling SLMs to think while speaking, similar to humans, is attracting increasing attention. We present, for the first time, Mind-Paced Speaking (MPS), a brain-inspired framework that enables high-fidelity, real-time reasoning. Similar to how humans utilize distinct brain regions for thinking and responding, we propose a novel dual-brain approach, employing a "Formulation Brain" for high-level reasoning to pace and guide a separate "Articulation Brain" for fluent speech generation. This division of labor eliminates mode-switching, preserving the integrity of the reasoning process. Experiments show that MPS significantly outperforms existing think-while-speaking methods and achieves reasoning performance comparable to models that pre-compute the full CoT before speaking, while drastically reducing latency. Under a zero-latency configuration, the proposed method achieves an accuracy of 92.8% on the mathematical reasoning task Spoken-MQA and attains a score of 82.5 on the speech conversation task URO-Bench. Our work effectively bridges the gap between high-quality reasoning and real-time interaction.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompting Test-Time Scaling Is A Strong LLM Reasoning Data Augmentation</title>
<link>https://arxiv.org/abs/2510.09599</link>
<guid>https://arxiv.org/abs/2510.09599</guid>
<content:encoded><![CDATA[

arXiv:2510.09599v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated impressive reasoning capabilities when provided with chain-of-thought exemplars, but curating large reasoning datasets remains laborious and resource-intensive. In this work, we introduce Prompting Test-Time Scaling (P-TTS), a simple yet effective inference-time data augmentation strategy for enhancing LLM reasoning through finetuning. Rather than collecting thousands or even millions of examples, P-TTS leverages a small pool of only 90 manually selected reasoning instances and systematically varies exemplar augmentation through principled instruction prompting intensities at test time to synthesize diverse reasoning trajectory contexts. Then we finetune the various sizes of Qwen-2.5 models on P-TTS data. Across a suite of mathematical reasoning AIME2024 & 25, MATH500, and GPQA-Diamond, our P-TTS-7B and 32B models outperform the prior competitive baselines like S1 and S1.1 (1K-shot), achieving absolute accuracy gains of +26.66% and +30.00% on AIME'24 (7B), and +13.34% and +6.67% on AIME'25 (7B); P-TTS-32B yields gains of +23.33% and +16.63% on AIME'24, and +26.63% and +3.33% on AIME'25 (vs. S1 and S1.1, respectively), with comparable or better performance on MATH500 and GPQA-Diamond. We further show that P-TTS enhances zero-shot generalization accuracy on out-of-domain reasoning benchmarks of Gaokao, Kaoyan, OlympiadBench, AMC23, GradeSchoolMath, and Minerva. Our analysis suggests that test-time scaling effectively explores the latent space of reasoning patterns, amplifying LLM problem-solving with minimal annotation overhead, and further unlocking the reasoning potential and capabilities of LLMs. Prompting Test-Time Scaling offers a practical, low-cost way to elicit LLM reasoning in resource-constrained or rapidly evolving domains.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Limitations of Normalization in Attention Mechanism</title>
<link>https://arxiv.org/abs/2508.17821</link>
<guid>https://arxiv.org/abs/2508.17821</guid>
<content:encoded><![CDATA[

arXiv:2508.17821v1 Announce Type: cross 
Abstract: This paper investigates the limitations of the normalization in attention mechanisms. We begin with a theoretical framework that enables the identification of the model's selective ability and the geometric separation involved in token selection. Our analysis includes explicit bounds on distances and separation criteria for token vectors under softmax scaling. Through experiments with pre-trained GPT-2 model, we empirically validate our theoretical results and analyze key behaviors of the attention mechanism. Notably, we demonstrate that as the number of selected tokens increases, the model's ability to distinguish informative tokens declines, often converging toward a uniform selection pattern. We also show that gradient sensitivity under softmax normalization presents challenges during training, especially at low temperature settings. These findings advance current understanding of softmax-based attention mechanism and motivate the need for more robust normalization and selection strategies in future attention architectures.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative Analysis of Large Language Models for the Machine-Assisted Resolution of User Intentions</title>
<link>https://arxiv.org/abs/2510.08576</link>
<guid>https://arxiv.org/abs/2510.08576</guid>
<content:encoded><![CDATA[

arXiv:2510.08576v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have emerged as transformative tools for natural language understanding and user intent resolution, enabling tasks such as translation, summarization, and, increasingly, the orchestration of complex workflows. This development signifies a paradigm shift from conventional, GUI-driven user interfaces toward intuitive, language-first interaction paradigms. Rather than manually navigating applications, users can articulate their objectives in natural language, enabling LLMs to orchestrate actions across multiple applications in a dynamic and contextual manner. However, extant implementations frequently rely on cloud-based proprietary models, which introduce limitations in terms of privacy, autonomy, and scalability. For language-first interaction to become a truly robust and trusted interface paradigm, local deployment is not merely a convenience; it is an imperative. This limitation underscores the importance of evaluating the feasibility of locally deployable, open-source, and open-access LLMs as foundational components for future intent-based operating systems. In this study, we examine the capabilities of several open-source and open-access models in facilitating user intention resolution through machine assistance. A comparative analysis is conducted against OpenAI's proprietary GPT-4-based systems to assess performance in generating workflows for various user intentions. The present study offers empirical insights into the practical viability, performance trade-offs, and potential of open LLMs as autonomous, locally operable components in next-generation operating systems. The results of this study inform the broader discussion on the decentralization and democratization of AI infrastructure and point toward a future where user-device interaction becomes more seamless, adaptive, and privacy-conscious through locally embedded intelligence.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Articulation-Informed ASR: Integrating Articulatory Features into ASR via Auxiliary Speech Inversion and Cross-Attention Fusion</title>
<link>https://arxiv.org/abs/2510.08585</link>
<guid>https://arxiv.org/abs/2510.08585</guid>
<content:encoded><![CDATA[

arXiv:2510.08585v1 Announce Type: cross 
Abstract: Prior works have investigated the use of articulatory features as complementary representations for automatic speech recognition (ASR), but their use was largely confined to shallow acoustic models. In this work, we revisit articulatory information in the era of deep learning and propose a framework that leverages articulatory representations both as an auxiliary task and as a pseudo-input to the recognition model. Specifically, we employ speech inversion as an auxiliary prediction task, and the predicted articulatory features are injected into the model as a query stream in a cross-attention module with acoustic embeddings as keys and values. Experiments on LibriSpeech demonstrate that our approach yields consistent improvements over strong transformer-based baselines, particularly under low-resource conditions. These findings suggest that articulatory features, once sidelined in ASR research, can provide meaningful benefits when reintroduced with modern architectures.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Stress Detection: A Study of Temporal Progression Modelling of Stress in Speech</title>
<link>https://arxiv.org/abs/2510.08586</link>
<guid>https://arxiv.org/abs/2510.08586</guid>
<content:encoded><![CDATA[

arXiv:2510.08586v1 Announce Type: cross 
Abstract: Detecting psychological stress from speech is critical in high-pressure settings. While prior work has leveraged acoustic features for stress detection, most treat stress as a static label. In this work, we model stress as a temporally evolving phenomenon influenced by historical emotional state. We propose a dynamic labelling strategy that derives fine-grained stress annotations from emotional labels and introduce cross-attention-based sequential models, a Unidirectional LSTM and a Transformer Encoder, to capture temporal stress progression. Our approach achieves notable accuracy gains on MuSE (+5%) and StressID (+18%) over existing baselines, and generalises well to a custom real-world dataset. These results highlight the value of modelling stress as a dynamic construct in speech.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BaldWhisper: Faster Whisper with Head Shearing and Layer Merging</title>
<link>https://arxiv.org/abs/2510.08599</link>
<guid>https://arxiv.org/abs/2510.08599</guid>
<content:encoded><![CDATA[

arXiv:2510.08599v1 Announce Type: cross 
Abstract: Pruning large pre-trained transformers for low-resource languages is challenging, as it often requires massive retraining data to recover performance. For instance, Distill-Whisper prunes Whisper by 40% and retrains on 21,000 hours of speech, far beyond what is available for most languages. Can Whisper be made lighter and faster for edge devices in data-scarce settings? Focusing on Bambara with only 32h of speech-to-text data, we propose a new pruning recipe. Instead of vocabulary pruning, which is unsuitable due to frequent code-switching by Bambara speakers, we compress the embeddings with low-rank decomposition and feature distillation. Rather than removing layers, we merge them to limit performance loss. The final model preserves 90% of the original performance while being 48% smaller and 2.15x faster on a MacBook Air M1.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy-Driven Steering: Reducing False Refusals in Large Language Models</title>
<link>https://arxiv.org/abs/2510.08646</link>
<guid>https://arxiv.org/abs/2510.08646</guid>
<content:encoded><![CDATA[

arXiv:2510.08646v1 Announce Type: cross 
Abstract: Safety alignment of large language models (LLMs) faces a key challenge: current alignment techniques often only focus on improving safety against harmful prompts, causing LLMs to become over-cautious and refuse to respond to benign prompts. Therefore, a key objective of safe alignment is to enhance safety while simultaneously reducing false refusals. In this paper, we introduce Energy-Driven Steering (EDS), a novel, fine-tuning free framework designed to resolve this challenge through dynamic, inference-time intervention. We trained a lightweight, external Energy-Based Model (EBM) to assign high energy to undesirable (false refusal or jailbreak) states and low energy to desirable (helpful response or safe reject) ones. During inference, EBM maps the LLM's internal activations to an "energy landscape". We use the gradient of the energy function to dynamically steer the LLM's hidden states to low energy regions, correcting the model to generate a desirable response in real-time without modifying its weights. This method decouples behavioral control from the model's core knowledge, offering a flexible solution with minimal computational overhead. Extensive experiments across a wide range of models show our method successfully achieves this objective: it substantially lowers false refusal rates. For example, raising compliance on the ORB-H benchmark from 57.3% to 82.6% while maintaining the baseline safety performance. Our work presents an effective paradigm for building LLMs that achieve both low false refusal rates and high safety.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing delivery for quick commerce factoring qualitative assessment of generated routes</title>
<link>https://arxiv.org/abs/2510.08671</link>
<guid>https://arxiv.org/abs/2510.08671</guid>
<content:encoded><![CDATA[

arXiv:2510.08671v1 Announce Type: cross 
Abstract: Indias e-commerce market is projected to grow rapidly, with last-mile delivery accounting for nearly half of operational expenses. Although vehicle routing problem (VRP) based solvers are widely used for delivery planning, their effectiveness in real-world scenarios is limited due to unstructured addresses, incomplete maps, and computational constraints in distance estimation. This study proposes a framework that employs large language models (LLMs) to critique VRP-generated routes against policy-based criteria, allowing logistics operators to evaluate and prioritise more efficient delivery plans. As a illustration of our approach we generate, annotate and evaluated 400 cases using large language models. Our study found that open-source LLMs identified routing issues with 79% accuracy, while proprietary reasoning models achieved reach upto 86%. The results demonstrate that LLM-based evaluation of VRP-generated routes can be an effective and scalable layer of evaluation which goes beyond beyond conventional distance and time based metrics. This has implications for improving cost efficiency, delivery reliability, and sustainability in last-mile logistics, especially for developing countries like India.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution</title>
<link>https://arxiv.org/abs/2510.08697</link>
<guid>https://arxiv.org/abs/2510.08697</guid>
<content:encoded><![CDATA[

arXiv:2510.08697v1 Announce Type: cross 
Abstract: Crowdsourced model evaluation platforms, such as Chatbot Arena, enable real-time evaluation from human perspectives to assess the quality of model responses. In the coding domain, manually examining the quality of LLM-generated content is extremely challenging, as it requires understanding long chunks of raw code and deliberately simulating code execution. To this end, we introduce BigCodeArena, an open human evaluation platform for code generation backed by a comprehensive and on-the-fly execution environment. Built on top of Chatbot Arena, BigCodeArena enables the execution of LLM-generated code and allows humans to interact with the execution process and outcomes. We collected over 14,000 raw code-centric conversation sessions across 10 widely used LLMs, spanning 10 languages and 8 types of execution environments. Among these conversations, we identified more than 4,700 multi-turn samples with pairwise human preferences. Further analysis uncovers underexplored preferences of LLMs in fine-grained domains characterized by tasks, languages, and frameworks. To systematically examine code understanding and generation capabilities of frontier LLMs, we curated two benchmarks based on the collected data, namely BigCodeReward and AutoCodeArena. For BigCodeReward, we post-processed the 4,700 conversations and evaluated the consistency between reward models and human preferences. The evaluation shows that most LLMs have superior performance in judging coding preferences when the execution results are available. Inspired by these findings, we propose AutoCodeArena, an automatic Elo rating benchmark designed to assess the coding quality of LLMs without human involvement. We find that proprietary LLMs like GPT-5, Claude-Sonnet-4, and Claude-Opus-4 still lead in code generation performance among recent emerging models.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When to Reason: Semantic Router for vLLM</title>
<link>https://arxiv.org/abs/2510.08731</link>
<guid>https://arxiv.org/abs/2510.08731</guid>
<content:encoded><![CDATA[

arXiv:2510.08731v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) demonstrate substantial accuracy gains when augmented with reasoning modes such as chain-of-thought and inference-time scaling. However, reasoning also incurs significant costs in inference latency and token usage, with environmental and financial impacts, which are unnecessary for many simple prompts. We present a semantic router that classifies queries based on their reasoning requirements and selectively applies reasoning only when beneficial. Our approach achieves a 10.2 percentage point improvement in accuracy on the MMLU-Pro benchmark while reducing response latency by 47.1% and token consumption by 48.5% compared to direct inference with vLLM. These results demonstrate that semantic routing offers an effective mechanism for striking a balance between accuracy and efficiency in open-source LLM serving systems
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Cross-Client Memorization of Training Data in Large Language Models for Federated Learning</title>
<link>https://arxiv.org/abs/2510.08750</link>
<guid>https://arxiv.org/abs/2510.08750</guid>
<content:encoded><![CDATA[

arXiv:2510.08750v1 Announce Type: cross 
Abstract: Federated learning (FL) enables collaborative training without raw data sharing, but still risks training data memorization. Existing FL memorization detection techniques focus on one sample at a time, underestimating more subtle risks of cross-sample memorization. In contrast, recent work on centralized learning (CL) has introduced fine-grained methods to assess memorization across all samples in training data, but these assume centralized access to data and cannot be applied directly to FL. We bridge this gap by proposing a framework that quantifies both intra- and inter-client memorization in FL using fine-grained cross-sample memorization measurement across all clients. Based on this framework, we conduct two studies: (1) measuring subtle memorization across clients and (2) examining key factors that influence memorization, including decoding strategies, prefix length, and FL algorithms. Our findings reveal that FL models do memorize client data, particularly intra-client data, more than inter-client data, with memorization influenced by training and inferencing factors.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Heuristic Algorithm Design with LLMs</title>
<link>https://arxiv.org/abs/2510.08755</link>
<guid>https://arxiv.org/abs/2510.08755</guid>
<content:encoded><![CDATA[

arXiv:2510.08755v1 Announce Type: cross 
Abstract: We posit that we can generate more robust and performant heuristics if we augment approaches using LLMs for heuristic design with tools that explain why heuristics underperform and suggestions about how to fix them. We find even simple ideas that (1) expose the LLM to instances where the heuristic underperforms; (2) explain why they occur; and (3) specialize design to regions in the input space, can produce more robust algorithms compared to existing techniques~ -- ~the heuristics we produce have a $\sim28\times$ better worst-case performance compared to FunSearch, improve average performance, and maintain the runtime.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Design-based Solution for Causal Inference with Text: Can a Language Model Be Too Large?</title>
<link>https://arxiv.org/abs/2510.08758</link>
<guid>https://arxiv.org/abs/2510.08758</guid>
<content:encoded><![CDATA[

arXiv:2510.08758v1 Announce Type: cross 
Abstract: Many social science questions ask how linguistic properties causally affect an audience's attitudes and behaviors. Because text properties are often interlinked (e.g., angry reviews use profane language), we must control for possible latent confounding to isolate causal effects. Recent literature proposes adapting large language models (LLMs) to learn latent representations of text that successfully predict both treatment and the outcome. However, because the treatment is a component of the text, these deep learning methods risk learning representations that actually encode the treatment itself, inducing overlap bias. Rather than depending on post-hoc adjustments, we introduce a new experimental design that handles latent confounding, avoids the overlap issue, and unbiasedly estimates treatment effects. We apply this design in an experiment evaluating the persuasiveness of expressing humility in political communication. Methodologically, we demonstrate that LLM-based methods perform worse than even simple bag-of-words models using our real text and outcomes from our experiment. Substantively, we isolate the causal effect of expressing humility on the perceived persuasiveness of political statements, offering new insights on communication effects for social media platforms, policy makers, and social scientists.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Struc-EMB: The Potential of Structure-Aware Encoding in Language Embeddings</title>
<link>https://arxiv.org/abs/2510.08774</link>
<guid>https://arxiv.org/abs/2510.08774</guid>
<content:encoded><![CDATA[

arXiv:2510.08774v1 Announce Type: cross 
Abstract: Text embeddings from Large Language Models (LLMs) have become foundational for numerous applications. However, these models typically operate on raw text, overlooking the rich structural information, such as hyperlinks or citations, that provides crucial context in many real-world datasets. This paper introduces and systematically evaluates a new paradigm for generating structure-aware text embeddings by integrating these structural relations directly into the LLM's internal encoding process, rather than relying on traditional post-hoc aggregation. We investigate two primary in-process methods: sequential concatenation and parallel caching. Through extensive zero-shot experiments across retrieval, clustering, classification, and recommendation tasks, we demonstrate that our structure-aware approaches consistently outperform both text-only and post-hoc baselines. Our analysis reveals critical trade-offs: sequential concatenation excels with noisy, moderate-length contexts, while parallel caching scales more effectively to long, high-signal contexts but is more susceptible to distractors. To address the challenge of noisy structural data, we also introduce and validate two effective techniques: Context Distillation and Semantic Balancing. This work provides the first comprehensive analysis of in-process structure-aware encoding, offering a blueprint for building more powerful and contextually aware embedding models.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>McMining: Automated Discovery of Misconceptions in Student Code</title>
<link>https://arxiv.org/abs/2510.08827</link>
<guid>https://arxiv.org/abs/2510.08827</guid>
<content:encoded><![CDATA[

arXiv:2510.08827v1 Announce Type: cross 
Abstract: When learning to code, students often develop misconceptions about various programming language concepts. These can not only lead to bugs or inefficient code, but also slow down the learning of related concepts. In this paper, we introduce McMining, the task of mining programming misconceptions from samples of code from a student. To enable the training and evaluation of McMining systems, we develop an extensible benchmark dataset of misconceptions together with a large set of code samples where these misconceptions are manifested. We then introduce two LLM-based McMiner approaches and through extensive evaluations show that models from the Gemini, Claude, and GPT families are effective at discovering misconceptions in student code.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Everyone prefers human writers, including AI</title>
<link>https://arxiv.org/abs/2510.08831</link>
<guid>https://arxiv.org/abs/2510.08831</guid>
<content:encoded><![CDATA[

arXiv:2510.08831v1 Announce Type: cross 
Abstract: As AI writing tools become widespread, we need to understand how both humans and machines evaluate literary style, a domain where objective standards are elusive and judgments are inherently subjective. We conducted controlled experiments using Raymond Queneau's Exercises in Style (1947) to measure attribution bias across evaluators. Study 1 compared human participants (N=556) and AI models (N=13) evaluating literary passages from Queneau versus GPT-4-generated versions under three conditions: blind, accurately labeled, and counterfactually labeled. Study 2 tested bias generalization across a 14$\times$14 matrix of AI evaluators and creators. Both studies revealed systematic pro-human attribution bias. Humans showed +13.7 percentage point (pp) bias (Cohen's h = 0.28, 95% CI: 0.21-0.34), while AI models showed +34.3 percentage point bias (h = 0.70, 95% CI: 0.65-0.76), a 2.5-fold stronger effect (P$<$0.001). Study 2 confirmed this bias operates across AI architectures (+25.8pp, 95% CI: 24.1-27.6%), demonstrating that AI systems systematically devalue creative content when labeled as "AI-generated" regardless of which AI created it. We also find that attribution labels cause evaluators to invert assessment criteria, with identical features receiving opposing evaluations based solely on perceived authorship. This suggests AI models have absorbed human cultural biases against artificial creativity during training. Our study represents the first controlled comparison of attribution bias between human and artificial evaluators in aesthetic judgment, revealing that AI systems not only replicate but amplify this human tendency.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time-Aware Feature Selection: Adaptive Temporal Masking for Stable Sparse Autoencoder Training</title>
<link>https://arxiv.org/abs/2510.08855</link>
<guid>https://arxiv.org/abs/2510.08855</guid>
<content:encoded><![CDATA[

arXiv:2510.08855v1 Announce Type: cross 
Abstract: Understanding the internal representations of large language models is crucial for ensuring their reliability and safety, with sparse autoencoders (SAEs) emerging as a promising interpretability approach. However, current SAE training methods face feature absorption, where features (or neurons) are absorbed into each other to minimize $L_1$ penalty, making it difficult to consistently identify and analyze model behaviors. We introduce Adaptive Temporal Masking (ATM), a novel training approach that dynamically adjusts feature selection by tracking activation magnitudes, frequencies, and reconstruction contributions to compute importance scores that evolve over time. ATM applies a probabilistic masking mechanism based on statistical thresholding of these importance scores, creating a more natural feature selection process. Through extensive experiments on the Gemma-2-2b model, we demonstrate that ATM achieves substantially lower absorption scores compared to existing methods like TopK and JumpReLU SAEs, while maintaining excellent reconstruction quality. These results establish ATM as a principled solution for learning stable, interpretable features in neural networks, providing a foundation for more reliable model analysis.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReviewerToo: Should AI Join The Program Committee? A Look At The Future of Peer Review</title>
<link>https://arxiv.org/abs/2510.08867</link>
<guid>https://arxiv.org/abs/2510.08867</guid>
<content:encoded><![CDATA[

arXiv:2510.08867v1 Announce Type: cross 
Abstract: Peer review is the cornerstone of scientific publishing, yet it suffers from inconsistencies, reviewer subjectivity, and scalability challenges. We introduce ReviewerToo, a modular framework for studying and deploying AI-assisted peer review to complement human judgment with systematic and consistent assessments. ReviewerToo supports systematic experiments with specialized reviewer personas and structured evaluation criteria, and can be partially or fully integrated into real conference workflows. We validate ReviewerToo on a carefully curated dataset of 1,963 paper submissions from ICLR 2025, where our experiments with the gpt-oss-120b model achieves 81.8% accuracy for the task of categorizing a paper as accept/reject compared to 83.9% for the average human reviewer. Additionally, ReviewerToo-generated reviews are rated as higher quality than the human average by an LLM judge, though still trailing the strongest expert contributions. Our analysis highlights domains where AI reviewers excel (e.g., fact-checking, literature coverage) and where they struggle (e.g., assessing methodological novelty and theoretical contributions), underscoring the continued need for human expertise. Based on these findings, we propose guidelines for integrating AI into peer-review pipelines, showing how AI can enhance consistency, coverage, and fairness while leaving complex evaluative judgments to domain experts. Our work provides a foundation for systematic, hybrid peer-review systems that scale with the growth of scientific publishing.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling</title>
<link>https://arxiv.org/abs/2510.08878</link>
<guid>https://arxiv.org/abs/2510.08878</guid>
<content:encoded><![CDATA[

arXiv:2510.08878v1 Announce Type: cross 
Abstract: Text-to-audio (TTA) generation with fine-grained control signals, e.g., precise timing control or intelligible speech content, has been explored in recent works. However, constrained by data scarcity, their generation performance at scale is still compromised. In this study, we recast controllable TTA generation as a multi-task learning problem and introduce a progressive diffusion modeling approach, ControlAudio. Our method adeptly fits distributions conditioned on more fine-grained information, including text, timing, and phoneme features, through a step-by-step strategy. First, we propose a data construction method spanning both annotation and simulation, augmenting condition information in the sequence of text, timing, and phoneme. Second, at the model training stage, we pretrain a diffusion transformer (DiT) on large-scale text-audio pairs, achieving scalable TTA generation, and then incrementally integrate the timing and phoneme features with unified semantic representations, expanding controllability. Finally, at the inference stage, we propose progressively guided generation, which sequentially emphasizes more fine-grained information, aligning inherently with the coarse-to-fine sampling nature of DiT. Extensive experiments show that ControlAudio achieves state-of-the-art performance in terms of temporal accuracy and speech clarity, significantly outperforming existing methods on both objective and subjective evaluations. Demo samples are available at: https://control-audio.github.io/Control-Audio.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HES-SQL: Hybrid Reasoning for Efficient Text-to-SQL with Structural Skeleton Guidance</title>
<link>https://arxiv.org/abs/2510.08896</link>
<guid>https://arxiv.org/abs/2510.08896</guid>
<content:encoded><![CDATA[

arXiv:2510.08896v1 Announce Type: cross 
Abstract: We present HES-SQL, a novel hybrid training framework that advances Text-to-SQL generation through the integration of thinking-mode-fused supervised fine-tuning (SFT) with Group Relative Policy Optimization (GRPO). Our approach introduces three key innovations: (1) a skeleton-completeness scoring mechanism that enhances preference alignment between generated queries and optimal SQL structures; (2) a query-latency-aware reward system that incentivizes the generation of computationally efficient SQL queries; (3) a self-distillation process for thinking-mode completion that prevents degradation of the model's reasoning capabilities. This framework enables hybrid thinking models to switch between reasoning and non-reasoning modes while improving SQL query accuracy and execution efficiency.
  Experimental evaluation, conducted on MySQL 8.0 and SQLite 3.42 under controlled single-user conditions, demonstrates that HES-SQL achieves competitive performance with execution accuracies of 79.14\% and 54.9\% on the BIRD and KaggleDBQA benchmarks, respectively. Query latency is measured as the end-to-end execution time of generated queries on the DBMS, averaged over multiple runs to mitigate variance. Efficiency gains range from 11\% to 20\% relative to supervised baselines. Our results establish a new paradigm for Text-to-SQL systems that effectively balances semantic accuracy with computational efficiency through execution-informed reinforcement learning (RL). The proposed methodology has significant implications for developing robust natural language interfaces to databases and can be extended to broader structured generation tasks requiring both correctness and efficiency optimization.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unleashing Perception-Time Scaling to Multimodal Reasoning Models</title>
<link>https://arxiv.org/abs/2510.08964</link>
<guid>https://arxiv.org/abs/2510.08964</guid>
<content:encoded><![CDATA[

arXiv:2510.08964v1 Announce Type: cross 
Abstract: Recent advances in inference-time scaling, particularly those leveraging reinforcement learning with verifiable rewards, have substantially enhanced the reasoning capabilities of Large Vision-Language Models (LVLMs). Inspired by this success, similar strategies have been applied to multimodal reasoning, yet their impact on visual perception remains unclear. To investigate this gap, we introduce DisTANCE, a perception-centric benchmark for visual estimation tasks. Evaluation results show that LVLMs exhibit limited estimation precision, and inference-time scaling offers only marginal gains. We attribute this to the fast perception paradigm of current LVLMs, where visual understanding is treated as a one-shot output without modeling the underlying perceptual process. To address this, we propose Perception-Time Scaling (PTS), a novel paradigm that encourages token-rich perception and decomposes complex perception problems into intermediate tractable sub-problems, thereby enabling perception to align with and benefit from inference-time scaling. Combined with reinforcement learning techniques, PTS significantly improves perception accuracy, raising high-precision performance on DisTANCE from 8.0% to 64.7%, and generalizes well to out-of-domain tasks. Surprisingly, even though PTS data are purely synthetic, combining them with math reasoning data yields consistent gains in both reasoning and real-world perception benchmarks. Further analysis reveals that PTS introduces more perception-related tokens and increases the model's attention to image tokens. Our code and data will be publicly released.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic-Condition Tuning: Fusing Graph Context with Large Language Models for Knowledge Graph Completion</title>
<link>https://arxiv.org/abs/2510.08966</link>
<guid>https://arxiv.org/abs/2510.08966</guid>
<content:encoded><![CDATA[

arXiv:2510.08966v1 Announce Type: cross 
Abstract: Fusing Knowledge Graphs with Large Language Models is crucial for knowledge-intensive tasks like knowledge graph completion. The prevailing paradigm, prefix-tuning, simply concatenates knowledge embeddings with text inputs. However, this shallow fusion overlooks the rich relational semantics within KGs and imposes a significant implicit reasoning burden on the LLM to correlate the prefix with the text. To address these, we propose Semantic-condition Tuning (SCT), a new knowledge injection paradigm comprising two key modules. First, a Semantic Graph Module employs a Graph Neural Network to extract a context-aware semantic condition from the local graph neighborhood, guided by knowledge-enhanced relations. Subsequently, this condition is passed to a Condition-Adaptive Fusion Module, which, in turn, adaptively modulates the textual embedding via two parameterized projectors, enabling a deep, feature-wise, and knowledge-aware interaction. The resulting pre-fused embedding is then fed into the LLM for fine-tuning. Extensive experiments on knowledge graph benchmarks demonstrate that SCT significantly outperforms prefix-tuning and other strong baselines. Our analysis confirms that by modulating the input representation with semantic graph context before LLM inference, SCT provides a more direct and potent signal, enabling more accurate and robust knowledge reasoning.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diagnosing and Mitigating System Bias in Self-Rewarding RL</title>
<link>https://arxiv.org/abs/2510.08977</link>
<guid>https://arxiv.org/abs/2510.08977</guid>
<content:encoded><![CDATA[

arXiv:2510.08977v1 Announce Type: cross 
Abstract: Reinforcement learning with verifiable rewards (RLVR) scales the reasoning ability of large language models (LLMs) but remains bottlenecked by limited labeled samples for continued data scaling. Reinforcement learning with intrinsic rewards (RLIR), where the policy model assigns rewards to its own rollouts, enables sustainable scaling in unlabeled settings, yet its performance and stability lag behind RLVR. We trace this gap to a system bias: the model tends to overestimate its high-confidence rollouts, leading to biased and unstable reward estimation. This bias accumulates as training progresses, with deviations from the oracle drifting toward over-reward, causing unstable training. We characterize this bias using three metrics: $\rho_{\text{noise}}$, $\rho_{\text{selfbias}}$, and $\rho_{\text{symbias}}$. We find that $\rho_{\text{noise}}$ and $\rho_{\text{symbias}}$ impact convergence, while $\rho_{\text{selfbias}}$ amplifies both correct and incorrect updates, leading to instability. To mitigate this, we propose reinforcement learning with ensembled rewards (RLER), which aggregates diverse models and adapts reward interpolation and rollout selection. Extensive experiments show that RLER improves by +13.6% over RLIR and is only 3.6% below RLVR, achieving stable scaling on unlabeled samples, making it highly applicable.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Epistemic Uncertainty of Visual Tokens for Object Hallucinations in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2510.09008</link>
<guid>https://arxiv.org/abs/2510.09008</guid>
<content:encoded><![CDATA[

arXiv:2510.09008v1 Announce Type: cross 
Abstract: Large vision-language models (LVLMs), which integrate a vision encoder (VE) with a large language model, have achieved remarkable success across various tasks. However, there are still crucial challenges in LVLMs such as object hallucination, generating descriptions of objects that are not in the input image. Here, we argue that uncertain visual tokens within the VE is a key factor that contributes to object hallucination. Our statistical analysis found that there are positive correlations between visual tokens with high epistemic uncertainty and the occurrence of hallucinations. Furthermore, we show theoretically and empirically that visual tokens in early VE layers that exhibit large representation deviations under small adversarial perturbations indicate high epistemic uncertainty. Based on these findings, we propose a simple yet effective strategy to mitigate object hallucination by modifying the VE only. Our method comprises a proxy method with adversarial perturbations for identifying uncertain visual tokens efficiently and a method to mask these uncertain visual tokens during the self-attention process in the middle layers of the VE, suppressing their influence on visual encoding and thus alleviating hallucinations. Extensive experiments show that our method significantly reduces object hallucinations in LVLMs and can synergistically work with other prior arts.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auto-scaling Continuous Memory for GUI Agent</title>
<link>https://arxiv.org/abs/2510.09038</link>
<guid>https://arxiv.org/abs/2510.09038</guid>
<content:encoded><![CDATA[

arXiv:2510.09038v1 Announce Type: cross 
Abstract: We study how to endow GUI agents with scalable memory that help generalize across unfamiliar interfaces and long-horizon tasks. Prior GUI agents compress past trajectories into text tokens, which balloons context length and misses decisive visual cues (e.g., exact widget size and position). We propose a continuous memory that encodes each GUI trajectory into a fixed-length sequence of continuous embeddings using the VLM itself as an encoder; these embeddings are plugged directly into the backbone's input layer, sharply reducing context cost while preserving fine-grained visual information. As memory size and retrieval depth increase, performance improves monotonically, unlike text memories that degrade with long prompts. To grow memory at low cost, we introduce an auto-scaling data flywheel that (i) discovers new environments via search, (ii) synthesizes tasks with an open-source VLM, (iii) rolls out trajectories with the agent, and (iv) verifies success with the same VLM. Using this pipeline, we collect 100k+ trajectories for about \$4000 and fine-tune only the memory encoder (LoRA on a Q-Former, 1.2\% parameters) with 1,500 samples. On real-world GUI benchmarks, our memory-augmented agent consistently improves success rates under long horizons and distribution shifts. Notably, Qwen-2.5-VL-7B + continuous memory achieves performance comparable to state-of-the-art closed-source models (e.g., GPT-4o, Claude-4).
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploiting Web Search Tools of AI Agents for Data Exfiltration</title>
<link>https://arxiv.org/abs/2510.09093</link>
<guid>https://arxiv.org/abs/2510.09093</guid>
<content:encoded><![CDATA[

arXiv:2510.09093v1 Announce Type: cross 
Abstract: Large language models (LLMs) are now routinely used to autonomously execute complex tasks, from natural language processing to dynamic workflows like web searches. The usage of tool-calling and Retrieval Augmented Generation (RAG) allows LLMs to process and retrieve sensitive corporate data, amplifying both their functionality and vulnerability to abuse. As LLMs increasingly interact with external data sources, indirect prompt injection emerges as a critical and evolving attack vector, enabling adversaries to exploit models through manipulated inputs. Through a systematic evaluation of indirect prompt injection attacks across diverse models, we analyze how susceptible current LLMs are to such attacks, which parameters, including model size and manufacturer, specific implementations, shape their vulnerability, and which attack methods remain most effective. Our results reveal that even well-known attack patterns continue to succeed, exposing persistent weaknesses in model defenses. To address these vulnerabilities, we emphasize the need for strengthened training procedures to enhance inherent resilience, a centralized database of known attack vectors to enable proactive defense, and a unified testing framework to ensure continuous security validation. These steps are essential to push developers toward integrating security into the core design of LLMs, as our findings show that current models still fail to mitigate long-standing threats.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs</title>
<link>https://arxiv.org/abs/2510.09201</link>
<guid>https://arxiv.org/abs/2510.09201</guid>
<content:encoded><![CDATA[

arXiv:2510.09201v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown remarkable success, and their multimodal expansions (MLLMs) further unlock capabilities spanning images, videos, and other modalities beyond text. However, despite this shift, prompt optimization approaches, designed to reduce the burden of manual prompt crafting while maximizing performance, remain confined to text, ultimately limiting the full potential of MLLMs. Motivated by this gap, we introduce the new problem of multimodal prompt optimization, which expands the prior definition of prompt optimization to the multimodal space defined by the pairs of textual and non-textual prompts. To tackle this problem, we then propose the Multimodal Prompt Optimizer (MPO), a unified framework that not only performs the joint optimization of multimodal prompts through alignment-preserving updates but also guides the selection process of candidate prompts by leveraging earlier evaluations as priors in a Bayesian-based selection strategy. Through extensive experiments across diverse modalities that go beyond text, such as images, videos, and even molecules, we demonstrate that MPO outperforms leading text-only optimization methods, establishing multimodal prompt optimization as a crucial step to realizing the potential of MLLMs.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised lexicon learning from speech is limited by representations rather than clustering</title>
<link>https://arxiv.org/abs/2510.09225</link>
<guid>https://arxiv.org/abs/2510.09225</guid>
<content:encoded><![CDATA[

arXiv:2510.09225v1 Announce Type: cross 
Abstract: Zero-resource word segmentation and clustering systems aim to tokenise speech into word-like units without access to text labels. Despite progress, the induced lexicons are still far from perfect. In an idealised setting with gold word boundaries, we ask whether performance is limited by the representation of word segments, or by the clustering methods that group them into word-like types. We combine a range of self-supervised speech features (continuous/discrete, frame/word-level) with different clustering methods (K-means, hierarchical, graph-based) on English and Mandarin data. The best system uses graph clustering with dynamic time warping on continuous features. Faster alternatives use graph clustering with cosine distance on averaged continuous features or edit distance on discrete unit sequences. Through controlled experiments that isolate either the representations or the clustering method, we demonstrate that representation variability across segments of the same word type -- rather than clustering -- is the primary factor limiting performance.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diagnosing Shoulder Disorders Using Multimodal Large Language Models and Consumer-Grade Cameras</title>
<link>https://arxiv.org/abs/2510.09230</link>
<guid>https://arxiv.org/abs/2510.09230</guid>
<content:encoded><![CDATA[

arXiv:2510.09230v1 Announce Type: cross 
Abstract: Shoulder disorders, such as frozen shoulder (a.k.a., adhesive capsulitis), are common conditions affecting the health of people worldwide, and have a high incidence rate among the elderly and workers engaged in repetitive shoulder tasks. In regions with scarce medical resources, achieving early and accurate diagnosis poses significant challenges, and there is an urgent need for low-cost and easily scalable auxiliary diagnostic solutions. This research introduces videos captured by consumer-grade devices as the basis for diagnosis, reducing the cost for users. We focus on the innovative application of Multimodal Large Language Models (MLLMs) in the preliminary diagnosis of shoulder disorders and propose a Hybrid Motion Video Diagnosis framework (HMVDx). This framework divides the two tasks of action understanding and disease diagnosis, which are respectively completed by two MLLMs. In addition to traditional evaluation indicators, this work proposes a novel metric called Usability Index by the logical process of medical decision-making (action recognition, movement diagnosis, and final diagnosis). This index evaluates the effectiveness of MLLMs in the medical field from the perspective of the entire medical diagnostic pathway, revealing the potential value of low-cost MLLMs in medical applications for medical practitioners. In experimental comparisons, the accuracy of HMVDx in diagnosing shoulder joint injuries has increased by 79.6\% compared with direct video diagnosis, a significant technical contribution to future research on the application of MLLMs for video understanding in the medical field.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CapGeo: A Caption-Assisted Approach to Geometric Reasoning</title>
<link>https://arxiv.org/abs/2510.09302</link>
<guid>https://arxiv.org/abs/2510.09302</guid>
<content:encoded><![CDATA[

arXiv:2510.09302v1 Announce Type: cross 
Abstract: Geometric reasoning remains a core challenge for Multimodal Large Language Models (MLLMs). Even the most advanced closed-source systems, such as GPT-O3 and Gemini-2.5-Pro, still struggle to solve geometry problems reliably, despite exhibiting strong textual reasoning abilities on tasks like the International Mathematical Olympiad (IMO). This gap suggests that the bottleneck lies in understanding geometric diagrams rather than reasoning itself. Since geometric figures can often be faithfully described in concise textual form, converting visual content into captions offers a promising direction. Motivated by this insight, we introduce CapGeo, a caption-assisted reasoning framework that bridges visual and textual modalities. Experiments show substantial improvements when models are equipped with captions: Qwen2.5-VL-72B improves from 8.6% (vision-only) to 59.0%, while Claude-Opus-4 rises from 44.8% to 73.0%. To systematically evaluate and identify high-quality geometric captioning models, we further propose CapGeo-Bench, a dataset of 4,641 curated figure-caption pairs. Crucially, CapGeo-Bench incorporates a keypoint-based evaluation metric that correlates strongly with downstream CapGeo performance, enabling reliable assessment of geometric captioning ability. Together, our framework and benchmark highlight a new pathway toward advancing geometric reasoning in MLLMs.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Target speaker anonymization in multi-speaker recordings</title>
<link>https://arxiv.org/abs/2510.09307</link>
<guid>https://arxiv.org/abs/2510.09307</guid>
<content:encoded><![CDATA[

arXiv:2510.09307v1 Announce Type: cross 
Abstract: Most of the existing speaker anonymization research has focused on single-speaker audio, leading to the development of techniques and evaluation metrics optimized for such condition. This study addresses the significant challenge of speaker anonymization within multi-speaker conversational audio, specifically when only a single target speaker needs to be anonymized. This scenario is highly relevant in contexts like call centers, where customer privacy necessitates anonymizing only the customer's voice in interactions with operators. Conventional anonymization methods are often not suitable for this task. Moreover, current evaluation methodology does not allow us to accurately assess privacy protection and utility in this complex multi-speaker scenario. This work aims to bridge these gaps by exploring effective strategies for targeted speaker anonymization in conversational audio, highlighting potential problems in their development and proposing corresponding improved evaluation methodologies.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Prompt Datasets: An In-depth Analysis and Insights</title>
<link>https://arxiv.org/abs/2510.09316</link>
<guid>https://arxiv.org/abs/2510.09316</guid>
<content:encoded><![CDATA[

arXiv:2510.09316v1 Announce Type: cross 
Abstract: A prompt is a natural language instruction that defines a specific task for a large language model (LLM) and serves as the primary interface for human-LLM interaction. With the growing deployment of LLMs, diverse prompt datasets are emerging from platforms such as GitHub and social media. These datasets span a wide array of applications and content types, facilitating both broader LLM utilization and improved prompt engineering. In this work, we--for the first time--have compiled an extensive list of prompt datasets sourced from various channels, representing a spectrum of downstream tasks, languages, engineering techniques, attributes, and modalities. We select key representative datasets for systematic analysis, revealing commonalities and differences in prompt construction across categories, distinguishing them from other text corpora like literature and web. We further propose a prompt optimization approach that leverages syntactic embeddings of part-of-speech and dependency structures. By identifying a centroid representation of prompts and guiding LLMs to rewrite prompts toward this centroid, our method improves the meaningfulness of model outputs. We have made our datasets and code available.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HINT: Helping Ineffective Rollouts Navigate Towards Effectiveness</title>
<link>https://arxiv.org/abs/2510.09388</link>
<guid>https://arxiv.org/abs/2510.09388</guid>
<content:encoded><![CDATA[

arXiv:2510.09388v1 Announce Type: cross 
Abstract: Reinforcement Learning (RL) has become a key driver for enhancing the long chain-of-thought (CoT) reasoning capabilities of Large Language Models (LLMs). However, prevalent methods like GRPO often fail when task difficulty exceeds the model's capacity, leading to reward sparsity and inefficient training. While prior work attempts to mitigate this using off-policy data, such as mixing RL with Supervised Fine-Tuning (SFT) or using hints, they often misguide policy updates In this work, we identify a core issue underlying these failures, which we term low training affinity. This condition arises from a large distributional mismatch between external guidance and the model's policy. To diagnose this, we introduce Affinity, the first quantitative metric for monitoring exploration efficiency and training stability. To improve Affinity, we propose HINT: Helping Ineffective rollouts Navigate Towards effectiveness, an adaptive hinting framework. Instead of providing direct answers, HINT supplies heuristic hints that guide the model to discover solutions on its own, preserving its autonomous reasoning capabilities. Extensive experiments on mathematical reasoning tasks show that HINT consistently outperforms existing methods, achieving state-of-the-art results with models of various scales, while also demonstrating significantly more stable learning and greater data efficiency.Code is available on Github.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating Brain Activity with High Spatial and Temporal Resolution using a Naturalistic MEG-fMRI Encoding Model</title>
<link>https://arxiv.org/abs/2510.09415</link>
<guid>https://arxiv.org/abs/2510.09415</guid>
<content:encoded><![CDATA[

arXiv:2510.09415v1 Announce Type: cross 
Abstract: Current non-invasive neuroimaging techniques trade off between spatial resolution and temporal resolution. While magnetoencephalography (MEG) can capture rapid neural dynamics and functional magnetic resonance imaging (fMRI) can spatially localize brain activity, a unified picture that preserves both high resolutions remains an unsolved challenge with existing source localization or MEG-fMRI fusion methods, especially for single-trial naturalistic data. We collected whole-head MEG when subjects listened passively to more than seven hours of narrative stories, using the same stimuli in an open fMRI dataset (LeBel et al., 2023). We developed a transformer-based encoding model that combines the MEG and fMRI from these two naturalistic speech comprehension experiments to estimate latent cortical source responses with high spatiotemporal resolution. Our model is trained to predict MEG and fMRI from multiple subjects simultaneously, with a latent layer that represents our estimates of reconstructed cortical sources. Our model predicts MEG better than the common standard of single-modality encoding models, and it also yields source estimates with higher spatial and temporal fidelity than classic minimum-norm solutions in simulation experiments. We validated the estimated latent sources by showing its strong generalizability across unseen subjects and modalities. Estimated activity in our source space predict electrocorticography (ECoG) better than an ECoG-trained encoding model in an entirely new dataset. By integrating the power of large naturalistic experiments, MEG, fMRI, and encoding models, we propose a practical route towards millisecond-and-millimeter brain mapping.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiveOIBench: Can Large Language Models Outperform Human Contestants in Informatics Olympiads?</title>
<link>https://arxiv.org/abs/2510.09595</link>
<guid>https://arxiv.org/abs/2510.09595</guid>
<content:encoded><![CDATA[

arXiv:2510.09595v1 Announce Type: cross 
Abstract: Competitive programming problems increasingly serve as valuable benchmarks to evaluate the coding capabilities of large language models (LLMs) due to their complexity and ease of verification. Yet, current coding benchmarks face limitations such as lack of exceptionally challenging problems, insufficient test case coverage, reliance on online platform APIs that limit accessibility. To address these issues, we introduce LiveOIBench, a comprehensive benchmark featuring 403 expert-curated Olympiad-level competitive programming problems, each with an average of 60 expert-designed test cases. The problems are sourced directly from 72 official Informatics Olympiads in different regions conducted between 2023 and 2025. LiveOIBench distinguishes itself through four key features: (1) meticulously curated high-quality tasks with detailed subtask rubrics and extensive private test cases; (2) direct integration of elite contestant performance data to enable informative comparison against top-performing humans; (3) planned continuous, contamination-free updates from newly released Olympiad problems; and (4) a self-contained evaluation system facilitating offline and easy-to-reproduce assessments. Benchmarking 32 popular general-purpose and reasoning LLMs, we find that GPT-5 achieves a notable 81.76th percentile, a strong result that nonetheless falls short of top human contestant performance, who usually place above 90th. In contrast, among open-weight reasoning models, GPT-OSS-120B achieves only a 60th percentile, underscoring significant capability disparities from frontier closed models. Detailed analyses indicate that robust reasoning models prioritize precise problem analysis over excessive exploration, suggesting future models should emphasize structured analysis and minimize unnecessary exploration. All data, code, and leaderboard results will be made publicly available on our website.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StreamingVLM: Real-Time Understanding for Infinite Video Streams</title>
<link>https://arxiv.org/abs/2510.09608</link>
<guid>https://arxiv.org/abs/2510.09608</guid>
<content:encoded><![CDATA[

arXiv:2510.09608v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) could power real-time assistants and autonomous agents, but they face a critical challenge: understanding near-infinite video streams without escalating latency and memory usage. Processing entire videos with full attention leads to quadratic computational costs and poor performance on long videos. Meanwhile, simple sliding window methods are also flawed, as they either break coherence or suffer from high latency due to redundant recomputation. In this paper, we introduce StreamingVLM, a model designed for real-time, stable understanding of infinite visual input. Our approach is a unified framework that aligns training with streaming inference. During inference, we maintain a compact KV cache by reusing states of attention sinks, a short window of recent vision tokens, and a long window of recent text tokens. This streaming ability is instilled via a simple supervised fine-tuning (SFT) strategy that applies full attention on short, overlapped video chunks, which effectively mimics the inference-time attention pattern without training on prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a new benchmark with videos averaging over two hours that requires dense, per-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM achieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time performance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy also enhances general VQA abilities without any VQA-specific fine-tuning, improving performance on LongVideoBench by +4.30 and OVOBench Realtime by +5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserving Parameter-Efficient Fine-Tuning for Large Language Model Services</title>
<link>https://arxiv.org/abs/2305.06212</link>
<guid>https://arxiv.org/abs/2305.06212</guid>
<content:encoded><![CDATA[

arXiv:2305.06212v3 Announce Type: replace 
Abstract: Parameter-Efficient Fine-Tuning (PEFT) provides a practical way for users to customize Large Language Models (LLMs) with their private data in LLM service scenarios. However, the inherently sensitive nature of private data demands robust privacy preservation measures during the customization of LLM services to ensure data security, maintain user trust, and comply with stringent regulatory standards. Based on PEFT, we propose Privacy-Preserving Parameter-Efficient Fine-Tuning (RAPT), a framework that offers privacy protection for LLM services. RAPT adopts a local privacy approach, enabling users to privatize their data locally using a text-to-text local differential privacy mechanism. Since PEFT performs poorly when directly trained on privatized data, we introduce a novel privatized token reconstruction task that is trained jointly with the downstream task, allowing LLMs to learn better task-dependent representations. Despite the simplicity of our framework, experiments show that RAPT achieves competitive performance across tasks while providing privacy guarantees against adversaries.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robo-Instruct: Simulator-Augmented Instruction Alignment For Finetuning Code LLMs</title>
<link>https://arxiv.org/abs/2405.20179</link>
<guid>https://arxiv.org/abs/2405.20179</guid>
<content:encoded><![CDATA[

arXiv:2405.20179v5 Announce Type: replace 
Abstract: Code LLMs have shown promising results with converting tasks in natural language to programs that can be executed by service robots. We are interested in finetuning small, specialized LLMs for this purpose, but collecting datasets of task-program pairs specific to each robot is time-consuming and expensive. While approaches such as SELF-INSTRUCT and EVOL-INSTRUCT are capable of generating novel tasks given a few examples, they are unable to provide the corresponding programs that correctly abide by physical-world and robot-constraints using the provided programming interface. Using a simulator is a natural potential solution to checking for such constraints, but building simulation environments that can handle arbitrary tasks and their necessary objects and locations, is challenging. To address these challenges, we introduce ROBO-INSTRUCT, which synthesizes task-specific simulation environments on the fly during program execution, by opportunistically inferring entity properties and enforcing corresponding constraints based on how the entities are used in the task program. Additionally, ROBO-INSTRUCT integrates an LLM-aided post-processing procedure to refine instructions for better alignment with robot programs. We demonstrate the effectiveness of ROBO-INSTRUCT across multiple LLMs, showing that our fine-tuned models outperform all baseline methods and even match or surpass the performance of several larger and proprietary models.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Reliability of Large Language Models for Causal Discovery</title>
<link>https://arxiv.org/abs/2407.19638</link>
<guid>https://arxiv.org/abs/2407.19638</guid>
<content:encoded><![CDATA[

arXiv:2407.19638v2 Announce Type: replace 
Abstract: This study investigates the efficacy of Large Language Models (LLMs) in causal discovery. Using newly available open-source LLMs, OLMo and BLOOM, which provide access to their pre-training corpora, we investigate how LLMs address causal discovery through three research questions. We examine: (i) the impact of memorization for accurate causal relation prediction, (ii) the influence of incorrect causal relations in pre-training data, and (iii) the contextual nuances that influence LLMs' understanding of causal relations. Our findings indicate that while LLMs are effective in recognizing causal relations that occur frequently in pre-training data, their ability to generalize to new or rare causal relations is limited. Moreover, the presence of incorrect causal relations significantly undermines the confidence of LLMs in corresponding correct causal relations, and the contextual information critically affects the outcomes of LLMs to discern causal connections between random variables.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Augmenting Compliance-Guaranteed Customer Service Chatbots: Context-Aware Knowledge Expansion with Large Language Models</title>
<link>https://arxiv.org/abs/2410.12444</link>
<guid>https://arxiv.org/abs/2410.12444</guid>
<content:encoded><![CDATA[

arXiv:2410.12444v3 Announce Type: replace 
Abstract: Retrieval-based chatbots leverage human-verified Q\&amp;A knowledge to deliver accurate, verifiable responses, making them ideal for customer-centric applications where compliance with regulatory and operational standards is critical. To effectively handle diverse customer inquiries, augmenting the knowledge base with "similar questions" that retain semantic meaning while incorporating varied expressions is a cost-effective strategy. In this paper, we introduce the Similar Question Generation (SQG) task for LLM training and inference, proposing context-aware approaches to enable comprehensive semantic exploration and enhanced alignment with source question-answer relationships. We formulate optimization techniques for constructing in-context prompts and selecting an optimal subset of similar questions to expand chatbot knowledge under budget constraints. Both quantitative and human evaluations validate the effectiveness of these methods, achieving a 92% user satisfaction rate in a deployed chatbot system, reflecting an 18% improvement over the unaugmented baseline. These findings highlight the practical benefits of SQG and emphasize the potential of LLMs, not as direct chatbot interfaces, but in supporting non-generative systems for hallucination-free, compliance-guaranteed applications.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improbable Bigrams Expose Vulnerabilities of Incomplete Tokens in Byte-Level Tokenizers</title>
<link>https://arxiv.org/abs/2410.23684</link>
<guid>https://arxiv.org/abs/2410.23684</guid>
<content:encoded><![CDATA[

arXiv:2410.23684v2 Announce Type: replace 
Abstract: Tokenization is a crucial step that bridges human-readable text with model-readable discrete tokens. However, recent studies have revealed that tokenizers can be exploited to elicit unwanted model behaviors. In this work, we investigate incomplete tokens, i.e., undecodable tokens with stray bytes resulting from byte-level byte-pair encoding (BPE) tokenization. We hypothesize that such tokens are heavily reliant on their adjacent tokens and are fragile when paired with unfamiliar tokens. To demonstrate this vulnerability, we introduce improbable bigrams: out-of-distribution combinations of incomplete tokens designed to exploit their dependency. Our experiments show that improbable bigrams are significantly prone to hallucinatory behaviors. Surprisingly, the same phrases have drastically lower rates of hallucination (90% reduction in Llama3.1) when an alternative tokenization is used. We caution against the potential vulnerabilities introduced by byte-level BPE tokenizers, which may introduce blind spots to language models.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Medchain: Bridging the Gap Between LLM Agents and Clinical Practice with Interactive Sequence</title>
<link>https://arxiv.org/abs/2412.01605</link>
<guid>https://arxiv.org/abs/2412.01605</guid>
<content:encoded><![CDATA[

arXiv:2412.01605v2 Announce Type: replace 
Abstract: Clinical decision making (CDM) is a complex, dynamic process crucial to healthcare delivery, yet it remains a significant challenge for artificial intelligence systems. While Large Language Model (LLM)-based agents have been tested on general medical knowledge using licensing exams and knowledge question-answering tasks, their performance in the CDM in real-world scenarios is limited due to the lack of comprehensive testing datasets that mirror actual medical practice. To address this gap, we present MedChain, a dataset of 12,163 clinical cases that covers five key stages of clinical workflow. MedChain distinguishes itself from existing benchmarks with three key features of real-world clinical practice: personalization, interactivity, and sequentiality. Further, to tackle real-world CDM challenges, we also propose MedChain-Agent, an AI system that integrates a feedback mechanism and a MCase-RAG module to learn from previous cases and adapt its responses. MedChain-Agent demonstrates remarkable adaptability in gathering information dynamically and handling sequential clinical tasks, significantly outperforming existing approaches.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NLP-ADBench: NLP Anomaly Detection Benchmark</title>
<link>https://arxiv.org/abs/2412.04784</link>
<guid>https://arxiv.org/abs/2412.04784</guid>
<content:encoded><![CDATA[

arXiv:2412.04784v2 Announce Type: replace 
Abstract: Anomaly detection (AD) is an important machine learning task with applications in fraud detection, content moderation, and user behavior analysis. However, AD is relatively understudied in a natural language processing (NLP) context, limiting its effectiveness in detecting harmful content, phishing attempts, and spam reviews. We introduce NLP-ADBench, the most comprehensive NLP anomaly detection (NLP-AD) benchmark to date, which includes eight curated datasets and 19 state-of-the-art algorithms. These span 3 end-to-end methods and 16 two-step approaches that adapt classical, non-AD methods to language embeddings from BERT and OpenAI. Our empirical results show that no single model dominates across all datasets, indicating a need for automated model selection. Moreover, two-step methods with transformer-based embeddings consistently outperform specialized end-to-end approaches, with OpenAI embeddings outperforming those of BERT. We release NLP-ADBench at https://github.com/USC-FORTIS/NLP-ADBench, providing a unified framework for NLP-AD and supporting future investigations.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AD-LLM: Benchmarking Large Language Models for Anomaly Detection</title>
<link>https://arxiv.org/abs/2412.11142</link>
<guid>https://arxiv.org/abs/2412.11142</guid>
<content:encoded><![CDATA[

arXiv:2412.11142v4 Announce Type: replace 
Abstract: Anomaly detection (AD) is an important machine learning task with many real-world uses, including fraud detection, medical diagnosis, and industrial monitoring. Within natural language processing (NLP), AD helps detect issues like spam, misinformation, and unusual user activity. Although large language models (LLMs) have had a strong impact on tasks such as text generation and summarization, their potential in AD has not been studied enough. This paper introduces AD-LLM, the first benchmark that evaluates how LLMs can help with NLP anomaly detection. We examine three key tasks: (i) zero-shot detection, using LLMs' pre-trained knowledge to perform AD without tasks-specific training; (ii) data augmentation, generating synthetic data and category descriptions to improve AD models; and (iii) model selection, using LLMs to suggest unsupervised AD models. Through experiments with different datasets, we find that LLMs can work well in zero-shot AD, that carefully designed augmentation methods are useful, and that explaining model selection for specific datasets remains challenging. Based on these results, we outline six future research directions on LLMs for AD.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RPO: Retrieval Preference Optimization for Robust Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2501.13726</link>
<guid>https://arxiv.org/abs/2501.13726</guid>
<content:encoded><![CDATA[

arXiv:2501.13726v2 Announce Type: replace 
Abstract: While Retrieval-Augmented Generation (RAG) has exhibited promise in utilizing external knowledge, its generation process heavily depends on the quality and accuracy of the retrieved context. Large language models (LLMs) struggle to evaluate the correctness of non-parametric knowledge retrieved externally when it differs from internal memorization, leading to knowledge conflicts during response generation. To this end, we introduce the Retrieval Preference Optimization (RPO), a lightweight and effective alignment method to adaptively leverage multi-source knowledge based on retrieval relevance. An implicit representation of retrieval relevance is derived and incorporated into the reward model to integrate retrieval evaluation and response generation into a single model, solving the problem that previous methods necessitate the additional procedure to assess the retrieval quality. Notably, RPO is the only RAG-dedicated alignment approach that quantifies the awareness of retrieval relevance in training, overcoming mathematical obstacles. Experiments on four datasets demonstrate that RPO outperforms RAG by 4-10% in accuracy without any extra component, exhibiting its robust generalization.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AnyEdit: Edit Any Knowledge Encoded in Language Models</title>
<link>https://arxiv.org/abs/2502.05628</link>
<guid>https://arxiv.org/abs/2502.05628</guid>
<content:encoded><![CDATA[

arXiv:2502.05628v3 Announce Type: replace 
Abstract: Large language models (LLMs) often produce incorrect or outdated information, necessitating efficient and precise knowledge updates. Current model editing methods, however, struggle with long-form knowledge in diverse formats, such as poetry, code snippets, and mathematical derivations. These limitations arise from their reliance on editing a single token's hidden state, a limitation we term "efficacy barrier". To solve this, we propose AnyEdit, a new autoregressive editing paradigm. It decomposes long-form knowledge into sequential chunks and iteratively edits the key token in each chunk, ensuring consistent and accurate outputs. Theoretically, we ground AnyEdit in the Chain Rule of Mutual Information, showing its ability to update any knowledge within LLMs. Empirically, it outperforms strong baselines by 21.5% on benchmarks including UnKEBench, AKEW, and our new EditEverything dataset for long-form diverse-formatted knowledge. Additionally, AnyEdit serves as a plug-and-play framework, enabling current editing methods to update knowledge with arbitrary length and format, significantly advancing the scope and practicality of LLM knowledge editing.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LightMamba: Efficient Mamba Acceleration on FPGA with Quantization and Hardware Co-design</title>
<link>https://arxiv.org/abs/2502.15260</link>
<guid>https://arxiv.org/abs/2502.15260</guid>
<content:encoded><![CDATA[

arXiv:2502.15260v2 Announce Type: replace 
Abstract: State space models (SSMs) like Mamba have recently attracted much attention. Compared to Transformer-based large language models (LLMs), Mamba achieves linear computation complexity with the sequence length and demonstrates superior performance. However, Mamba is hard to accelerate due to the scattered activation outliers and the complex computation dependency, rendering existing LLM accelerators inefficient. In this paper, we propose LightMamba that co-designs the quantization algorithm and FPGA accelerator architecture for efficient Mamba inference. We first propose an FPGA-friendly post-training quantization algorithm that features rotation-assisted quantization and power-of-two SSM quantization to reduce the majority of computation to 4-bit. We further design an FPGA accelerator that partially unrolls the Mamba computation to balance the efficiency and hardware costs. Through computation reordering as well as fine-grained tiling and fusion, the hardware utilization and memory efficiency of the accelerator get drastically improved. We implement LightMamba on Xilinx Versal VCK190 FPGA and achieve 4.65x to 6.06x higher energy efficiency over the GPU baseline. When evaluated on Alveo U280 FPGA, LightMamba reaches 93 tokens/s, which is 1.43x that of the GPU baseline. Our code is available at https://github.com/PKU-SEC-Lab/LightMamba.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Are They Filtering Out? An Experimental Benchmark of Filtering Strategies for Harm Reduction in Pretraining Datasets</title>
<link>https://arxiv.org/abs/2503.05721</link>
<guid>https://arxiv.org/abs/2503.05721</guid>
<content:encoded><![CDATA[

arXiv:2503.05721v2 Announce Type: replace 
Abstract: Data filtering strategies are a crucial component to develop safe Large Language Models (LLM), since they support the removal of harmful contents from pretraining datasets. There is a lack of research on the actual impact of these strategies on vulnerable groups to discrimination, though, and their effectiveness has not been yet systematically addressed. In this paper we present a benchmark study of data filtering strategies for harm reduction aimed at providing a systematic evaluation on these approaches. We provide an overview $55$ technical reports of English LMs and LLMs to identify the existing filtering strategies in literature and implement an experimental setting to test their impact against vulnerable groups. Our results show that the positive impact that strategies have in reducing harmful contents from documents has the side effect of increasing the underrepresentation of vulnerable groups to discrimination in datasets.
  WARNING: the paper could contain racist, sexist, violent, and generally offensive contents
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How a Bilingual LM Becomes Bilingual: Tracing Internal Representations with Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2503.06394</link>
<guid>https://arxiv.org/abs/2503.06394</guid>
<content:encoded><![CDATA[

arXiv:2503.06394v2 Announce Type: replace 
Abstract: This study explores how bilingual language models develop complex internal representations. We employ sparse autoencoders to analyze internal representations of bilingual language models with a focus on the effects of training steps, layers, and model sizes. Our analysis shows that language models first learn languages separately, and then gradually form bilingual alignments, particularly in the mid layers. We also found that this bilingual tendency is stronger in larger models. Building on these findings, we demonstrate the critical role of bilingual representations in model performance by employing a novel method that integrates decomposed representations from a fully trained model into a mid-training model. Our results provide insights into how language models acquire bilingual capabilities.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding and Improving Information Preservation in Prompt Compression for LLMs</title>
<link>https://arxiv.org/abs/2503.19114</link>
<guid>https://arxiv.org/abs/2503.19114</guid>
<content:encoded><![CDATA[

arXiv:2503.19114v2 Announce Type: replace 
Abstract: Recent advancements in large language models (LLMs) have enabled their successful application to a broad range of tasks. However, in information-intensive tasks, the prompt length can grow fast, leading to increased computational requirements, performance degradation, and induced biases from irrelevant or redundant information. Recently, various prompt compression techniques have been introduced to optimize the trade-off between reducing input length and retaining performance. We propose a holistic evaluation framework that allows for in-depth analysis of prompt compression methods. We focus on three key aspects, besides compression ratio: (i) downstream task performance, (ii) grounding in the input context, and (iii) information preservation. Using our framework, we analyze state-of-the-art soft and hard compression methods and show that some fail to preserve key details from the original prompt, limiting performance on complex tasks. By identifying these limitations, we are able to improve one soft prompting method by controlling compression granularity, achieving up to +23% in downstream performance, +8 BERTScore points in grounding, and 2.7x more entities preserved in compression. Ultimately, we find that the best effectiveness/compression rate trade-off is achieved with soft prompting combined with sequence-level training.The code is available at https://github.com/amazon-science/information-preservation-in-prompt-compression.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAISE: Reinforced Adaptive Instruction Selection For Large Language Models</title>
<link>https://arxiv.org/abs/2504.07282</link>
<guid>https://arxiv.org/abs/2504.07282</guid>
<content:encoded><![CDATA[

arXiv:2504.07282v4 Announce Type: replace 
Abstract: In the instruction fine-tuning of large language models (LLMs), it is widely recognized that a few high-quality instructions are superior to a large number of low-quality instructions. At present, many instruction selection methods have been proposed, but most of these methods select instruction based on heuristic quality metrics, and only consider data selection before training. These designs lead to insufficient optimization of instruction fine-tuning, and fixed heuristic indicators are often difficult to optimize for specific tasks. Therefore, we design a dynamic, task-objective-driven instruction selection framework RAISE(Reinforced Adaptive Instruction SElection), which incorporates the entire instruction fine-tuning process into optimization, selecting instructions at each step based on the expected impact of each instruction on model performance improvement. Our approach is well interpretable and has strong task-specific optimization capabilities. By modeling dynamic instruction selection as a sequential decision-making process, we use RL to train our selection strategy. Extensive experiments and result analysis prove the superiority of our method compared with other instruction selection methods. Notably, RAISE achieves superior performance by updating only 1% of the training steps compared to full-data training, demonstrating its efficiency and effectiveness.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViClaim: A Multilingual Multilabel Dataset for Automatic Claim Detection in Videos</title>
<link>https://arxiv.org/abs/2504.12882</link>
<guid>https://arxiv.org/abs/2504.12882</guid>
<content:encoded><![CDATA[

arXiv:2504.12882v3 Announce Type: replace 
Abstract: The growing influence of video content as a medium for communication and misinformation underscores the urgent need for effective tools to analyze claims in multilingual and multi-topic settings. Existing efforts in misinformation detection largely focus on written text, leaving a significant gap in addressing the complexity of spoken text in video transcripts. We introduce ViClaim, a dataset of 1,798 annotated video transcripts across three languages (English, German, Spanish) and six topics. Each sentence in the transcripts is labeled with three claim-related categories: fact-check-worthy, fact-non-check-worthy, or opinion. We developed a custom annotation tool to facilitate the highly complex annotation process. Experiments with state-of-the-art multilingual language models demonstrate strong performance in cross-validation (macro F1 up to 0.896) but reveal challenges in generalization to unseen topics, particularly for distinct domains. Our findings highlight the complexity of claim detection in video transcripts. ViClaim offers a robust foundation for advancing misinformation detection in video-based communication, addressing a critical gap in multimodal analysis.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning</title>
<link>https://arxiv.org/abs/2504.17192</link>
<guid>https://arxiv.org/abs/2504.17192</guid>
<content:encoded><![CDATA[

arXiv:2504.17192v4 Announce Type: replace 
Abstract: Despite the rapid growth of machine learning research, corresponding code implementations are often unavailable, making it slow and labor-intensive for researchers to reproduce results and build upon prior work. In the meantime, recent Large Language Models (LLMs) excel at understanding scientific documents and generating high-quality code. Inspired by this, we introduce PaperCoder, a multi-agent LLM framework that transforms machine learning papers into functional code repositories. PaperCoder operates in three stages: planning, where it constructs a high-level roadmap, designs the system architecture with diagrams, identifies file dependencies, and generates configuration files; analysis, which focuses on interpreting implementation-specific details; and generation, where modular, dependency-aware code is produced. Moreover, each phase is instantiated through a set of specialized agents designed to collaborate effectively across the pipeline. We then evaluate PaperCoder on generating code implementations from machine learning papers based on both model-based and human evaluations, particularly from the authors of those papers, with author-released repositories as ground truth if available. Our results demonstrate the effectiveness of PaperCoder in creating high-quality, faithful implementations. Furthermore, it consistently shows strengths in the recently released PaperBench benchmark, surpassing strong baselines by substantial margins. Code is available at: https://github.com/going-doer/Paper2Code.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>System Prompt Optimization with Meta-Learning</title>
<link>https://arxiv.org/abs/2505.09666</link>
<guid>https://arxiv.org/abs/2505.09666</guid>
<content:encoded><![CDATA[

arXiv:2505.09666v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown remarkable capabilities, with optimizing their input prompts playing a pivotal role in maximizing their performance. However, while LLM prompts consist of both the task-agnostic system prompts and task-specific user prompts, existing work on prompt optimization has focused on user prompts specific to individual queries or tasks, and largely overlooked the system prompt that is, once optimized, applicable across different tasks and domains. Motivated by this, we introduce the novel problem of bilevel system prompt optimization, whose objective is to design system prompts that are robust to diverse user prompts and transferable to unseen tasks. To tackle this problem, we then propose a meta-learning framework, which meta-learns the system prompt by optimizing it over various user prompts across multiple datasets, while simultaneously updating the user prompts in an iterative manner to ensure synergy between them. We conduct experiments on 14 unseen datasets spanning 5 different domains, on which we show that our approach produces system prompts that generalize effectively to diverse user prompts. Also, our findings reveal that the optimized system prompt enables rapid adaptation even to unseen tasks, requiring fewer optimization steps for test-time user prompts while achieving improved performance.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DDO: Dual-Decision Optimization for LLM-Based Medical Consultation via Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2505.18630</link>
<guid>https://arxiv.org/abs/2505.18630</guid>
<content:encoded><![CDATA[

arXiv:2505.18630v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) demonstrate strong generalization and reasoning abilities, making them well-suited for complex decision-making tasks such as medical consultation (MC). However, existing LLM-based methods often fail to capture the dual nature of MC, which entails two distinct sub-tasks: symptom inquiry, a sequential decision-making process, and disease diagnosis, a classification problem. This mismatch often results in ineffective symptom inquiry and unreliable disease diagnosis. To address this, we propose \textbf{DDO}, a novel LLM-based framework that performs \textbf{D}ual-\textbf{D}ecision \textbf{O}ptimization by decoupling the two sub-tasks and optimizing them with distinct objectives through a collaborative multi-agent workflow. Experiments on three real-world MC datasets show that DDO consistently outperforms existing LLM-based approaches and achieves competitive performance with state-of-the-art generation-based methods, demonstrating its effectiveness in the MC task. The code is available at https://github.com/zh-jia/DDO.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Demonstrations: Dynamic Vector Construction from Latent Representations</title>
<link>https://arxiv.org/abs/2505.20318</link>
<guid>https://arxiv.org/abs/2505.20318</guid>
<content:encoded><![CDATA[

arXiv:2505.20318v2 Announce Type: replace 
Abstract: In-Context derived Vector (ICV) methods extract task-relevant representations from large language models (LLMs) and reinject them during inference, achieving comparable performance to few-shot In-Context Learning (ICL) without repeated demonstration processing. However, existing ICV methods remain sensitive to ICL-specific factors, often use coarse or semantically fragmented representations as the source of the vector, and rely on heuristic-based injection positions, limiting their applicability.
  To address these issues, we propose Dynamic Vector (DyVec), which incorporates an Exhaustive Query Rotation (EQR) strategy to extract robust semantically aggregated latent representations by mitigating variance introduced by ICL. It then applies Dynamic Latent Segmentation and Injection to adaptively partition representations based on task complexity and leverages REINFORCE-based optimization to learn optimal injection positions for each segment.
  Experiments results show that DyVec outperforms few-shot ICL, LoRA, and prior ICV baselines. Further analysis highlights the effectiveness of dynamically segmenting and injecting semantically aggregated latent representations. DyVec provides a lightweight and data-efficient solution for inference-time task adaptation.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinTagging: Benchmarking LLMs for Extracting and Structuring Financial Information</title>
<link>https://arxiv.org/abs/2505.20650</link>
<guid>https://arxiv.org/abs/2505.20650</guid>
<content:encoded><![CDATA[

arXiv:2505.20650v2 Announce Type: replace 
Abstract: Accurately understanding numbers from financial reports is fundamental to how markets, regulators, algorithms, and normal people read the economy and the world, yet even with XBRL (eXtensible Business Reporting Language) designed to tag every figure with standardized accounting concepts, mapping thousands of facts to over 10,000 U.S. GAAP concepts remains costly, inconsistent, and error-prone. Existing benchmarks define tagging as flat, single-step, extreme classification over small subsets of US-GAAP concepts, overlooking both the taxonomy's hierarchical semantics and the structured nature of real tagging, where each fact must be represented as a contextualized multi-field output. These simplifications prevent fair evaluation of large language models (LLMs) under realistic reporting conditions. To address these gaps, we introduce FinTagging, the first comprehensive benchmark for structure-aware and full-scope XBRL tagging, designed to evaluate LLMs' ability to extract and align financial facts through numerical reasoning and taxonomy alignment across text and tables. We define two subtasks: FinNI for numeric identification, which extracts numerical entities and their types from XBRL reports, and FinCL for concept linking, which maps each extracted entity to the corresponding concept in the full US-GAAP taxonomy. Together, these subtasks produce a structured representation of each financial fact. We evaluate diverse LLMs under zero-shot settings and analyze their performance across both subtasks and overall tagging accuracy. Results show that LLMs generalize well in numeric identification but struggle with fine-grained concept linking, revealing current limitations in structure-aware reasoning for accurate financial disclosure. All code and datasets are available on GitHub and Hugging Face.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Elicit and Enhance: Advancing Multimodal Reasoning in Medical Scenarios</title>
<link>https://arxiv.org/abs/2505.23118</link>
<guid>https://arxiv.org/abs/2505.23118</guid>
<content:encoded><![CDATA[

arXiv:2505.23118v2 Announce Type: replace 
Abstract: Effective clinical decision-making depends on iterative, multimodal reasoning across diverse sources of evidence. The recent emergence of multimodal reasoning models has significantly transformed the landscape of solving complex tasks. Although such models have achieved notable success in mathematics and science, their application to medical domains remains underexplored. In this work, we propose \textit{MedE$^2$}, a two-stage post-training pipeline that elicits and then enhances multimodal reasoning for medical domains. In Stage-I, we fine-tune models using 2,000 text-only data samples containing precisely orchestrated reasoning demonstrations to elicit reasoning behaviors. In Stage-II, we further enhance the model's reasoning capabilities using 1,500 rigorously curated multimodal medical cases, aligning model reasoning outputs with our proposed multimodal medical reasoning preference. Extensive experiments demonstrate the efficacy and reliability of \textit{MedE$^2$} in improving the reasoning performance of medical multimodal models. Notably, models trained with \textit{MedE$^2$} consistently outperform baselines across multiple medical multimodal benchmarks. Additional validation on larger models and under inference-time scaling further confirms the robustness and practical utility of our approach.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flattery in Motion: Benchmarking and Analyzing Sycophancy in Video-LLMs</title>
<link>https://arxiv.org/abs/2506.07180</link>
<guid>https://arxiv.org/abs/2506.07180</guid>
<content:encoded><![CDATA[

arXiv:2506.07180v2 Announce Type: replace 
Abstract: As video large language models (Video-LLMs) become increasingly integrated into real-world applications that demand grounded multimodal reasoning, ensuring their factual consistency and reliability is of critical importance. However, sycophancy, the tendency of these models to align with user input even when it contradicts the visual evidence, undermines their trustworthiness in such contexts. Current sycophancy research has largely overlooked its specific manifestations in the video-language domain, resulting in a notable absence of systematic benchmarks and targeted evaluations to understand how Video-LLMs respond under misleading user input. To fill this gap, we propose VISE (Video-LLM Sycophancy Benchmarking and Evaluation), the first benchmark designed to evaluate sycophantic behavior in state-of-the-art Video-LLMs across diverse question formats, prompt biases, and visual reasoning tasks. Specifically, VISE pioneeringly brings linguistic perspectives on sycophancy into the video domain, enabling fine-grained analysis across multiple sycophancy types and interaction patterns. Furthermore, we propose two potential training-free mitigation strategies, revealing potential paths for reducing sycophantic bias: (i) enhancing visual grounding through interpretable key-frame selection and (ii) steering model behavior away from sycophancy via targeted, inference-time intervention on its internal neural representations. Our code is available at https://github.com/William030422/Video-Sycophancy.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Reason Across Parallel Samples for LLM Reasoning</title>
<link>https://arxiv.org/abs/2506.09014</link>
<guid>https://arxiv.org/abs/2506.09014</guid>
<content:encoded><![CDATA[

arXiv:2506.09014v2 Announce Type: replace 
Abstract: Scaling test-time compute brings substantial performance gains for large language models (LLMs). By sampling multiple answers and heuristically aggregate their answers (e.g., either through majority voting or using verifiers to rank the answers), one can achieve consistent performance gains in math domains. In this paper, we propose a new way to leverage such multiple sample set. We train a compact LLM, called Sample Set Aggregator (SSA), that takes a concatenated sequence of multiple samples and output the final answer, optimizing it for the answer accuracy with reinforcement learning. Experiments on five reasoning datasets demonstrate both the efficacy and efficiency of SSA. Notably, SSA improves over naive majority voting by 8% pass@5 on MATH. Furthermore, our 3B SSA surpasses model-based re-ranking with a much larger 72B process reward model. Our analysis also shows promising generalization ability of SSA, across sample set sizes, base model families and scales, and tasks. By separating LLMs to generate answers and LLMs to analyze and aggregate sampled answers, our approach can work with the outputs from premier black box models easily and efficiently.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RedDebate: Safer Responses through Multi-Agent Red Teaming Debates</title>
<link>https://arxiv.org/abs/2506.11083</link>
<guid>https://arxiv.org/abs/2506.11083</guid>
<content:encoded><![CDATA[

arXiv:2506.11083v2 Announce Type: replace 
Abstract: We introduce RedDebate, a novel multi-agent debate framework that provides the foundation for Large Language Models (LLMs) to identify and mitigate their unsafe behaviours. Existing AI safety approaches often rely on costly human evaluation or isolated single-model assessment, both constrained by scalability and prone to oversight failures. RedDebate employs collaborative argumentation among multiple LLMs across diverse debate scenarios, enabling them to critically evaluate one another's reasoning and systematically uncover unsafe failure modes through fully automated red-teaming. We further integrate distinct long-term memory modules that preserve safety-relevant insights from debate interactions and leverage them during subsequent inference, facilitating continuous refinement of model behaviour. Empirical evaluation on safety benchmarks across a diverse set of models demonstrates that RedDebate substantially reduces unsafe outputs. While debate alone allows LLMs to refine their behaviour, the addition of memory yields further significant reductions. To the best of our knowledge, RedDebate is the first fully automated framework to unify multi-agent debate and red-teaming to progressively enhance LLM safety without human intervention.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Phonikud: Hebrew Grapheme-to-Phoneme Conversion for Real-Time Text-to-Speech</title>
<link>https://arxiv.org/abs/2506.12311</link>
<guid>https://arxiv.org/abs/2506.12311</guid>
<content:encoded><![CDATA[

arXiv:2506.12311v2 Announce Type: replace 
Abstract: Real-time text-to-speech (TTS) for Modern Hebrew is challenging due to the language's orthographic complexity. Existing solutions ignore crucial phonetic features such as stress that remain underspecified even when vowel marks are added. To address these limitations, we introduce Phonikud, a lightweight, open-source Hebrew grapheme-to-phoneme (G2P) system that outputs fully-specified IPA transcriptions. Our approach adapts an existing diacritization model with lightweight adaptors, incurring negligible additional latency. We also contribute the ILSpeech dataset of transcribed Hebrew speech with IPA annotations, serving as a benchmark for Hebrew G2P, as training data for TTS systems, and enabling audio-to-IPA for evaluating TTS performance while capturing important phonetic details. Our results demonstrate that Phonikud G2P conversion more accurately predicts phonemes from Hebrew text compared to prior methods, and that this enables training of effective real-time Hebrew TTS models with superior speed-accuracy trade-offs. We release our code, data, and models at https: //phonikud.github.io.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical Perspective</title>
<link>https://arxiv.org/abs/2506.19028</link>
<guid>https://arxiv.org/abs/2506.19028</guid>
<content:encoded><![CDATA[

arXiv:2506.19028v5 Announce Type: replace 
Abstract: Large Language Models (LLMs) often generate responses with inherent biases, undermining their reliability in real-world applications. Existing evaluation methods often overlook biases in long-form responses and the intrinsic variability of LLM outputs. To address these challenges, we propose FiSCo (Fine-grained Semantic Comparison), a novel statistical framework to evaluate group-level fairness in LLMs by detecting subtle semantic differences in long-form responses across demographic groups. Unlike prior work focusing on sentiment or token-level comparisons, FiSCo goes beyond surface-level analysis by operating at the claim level, leveraging entailment checks to assess the consistency of meaning across responses. We decompose model outputs into semantically distinct claims and apply statistical hypothesis testing to compare inter- and intra-group similarities, enabling robust detection of subtle biases. We formalize a new group counterfactual fairness definition and validate FiSCo on both synthetic and human-annotated datasets spanning gender, race, and age. Experiments show that FiSCo more reliably identifies nuanced biases while reducing the impact of stochastic LLM variability, outperforming various evaluation metrics.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Disentangle Latent Reasoning Rules with Language VAEs: A Systematic Study</title>
<link>https://arxiv.org/abs/2506.19418</link>
<guid>https://arxiv.org/abs/2506.19418</guid>
<content:encoded><![CDATA[

arXiv:2506.19418v2 Announce Type: replace 
Abstract: Incorporating explicit reasoning rules within the latent space of language models (LMs) offers a promising pathway to enhance generalisation, interpretability, and controllability. While current Transformer-based language models have shown strong performance on Natural Language Inference (NLI) tasks, they often rely on memorisation rather than rule-based inference. This work investigates how reasoning rules can be explicitly embedded and memorised within the LMs through Language Variational Autoencoders (VAEs). We propose a complete pipeline for learning reasoning rules within Transformer-based language VAEs. This pipeline encompasses three rule-based reasoning tasks, a supporting theoretical framework, and a practical end-to-end architecture. The experiment illustrates the following findings: Disentangled reasoning: Under explicit signal supervision, reasoning rules - viewed as functional mappings - can be disentangled within the encoder's parametric space. This separation results in distinct clustering of rules in the output feature space. Prior knowledge injection: injecting reasoning information into the Query enables the model to more effectively retrieve the stored value Value from memory based on Key. This approach offers a simple method for integrating prior knowledge into decoder-only language models. Performance bottleneck: In mathematical reasoning tasks using Qwen2.5(0.5B), increasing sample count doesn't improve performance beyond a point. Moreover, ffn layers are better than attention layers at preserving the separation of reasoning rules in the model's parameters.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lizard: An Efficient Linearization Framework for Large Language Models</title>
<link>https://arxiv.org/abs/2507.09025</link>
<guid>https://arxiv.org/abs/2507.09025</guid>
<content:encoded><![CDATA[

arXiv:2507.09025v3 Announce Type: replace 
Abstract: We propose Lizard, a linearization framework that transforms pretrained Transformer-based Large Language Models (LLMs) into subquadratic architectures. Transformers faces severe computational and memory bottlenecks with long sequences due to the quadratic complexity of softmax attention and the growing Key-Value (KV) cache that makes inference memory-bound by context length. Lizard addresses these limitations by introducing a subquadratic attention mechanism that closely approximates softmax attention while preserving model quality. Unlike prior linearization methods constrained by fixed, non-adaptive structures, Lizard augments the architecture with compact, learnable modules that enable adaptive memory control and robust length generalization. Moreover, we introduce a hardwareaware algorithm that solves numerical instability in gated attention to accelerate training. Extensive experiments show that Lizard achieves near-lossless recovery of its teacher model's performance, significantly outperforming previous methods by up to 9.4 - 24.5 points on the 5-shot MMLU benchmark and demonstrating superior associative recall.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConlangCrafter: Constructing Languages with a Multi-Hop LLM Pipeline</title>
<link>https://arxiv.org/abs/2508.06094</link>
<guid>https://arxiv.org/abs/2508.06094</guid>
<content:encoded><![CDATA[

arXiv:2508.06094v2 Announce Type: replace 
Abstract: Constructed languages (conlangs) such as Esperanto and Quenya have played diverse roles in art, philosophy, and international communication. Meanwhile, foundation models have revolutionized creative generation in text, images, and beyond. In this work, we leverage modern LLMs as computational creativity aids for end-to-end conlang creation. We introduce ConlangCrafter, a multi-hop pipeline that decomposes language design into modular stages - phonology, morphology, syntax, lexicon generation, and translation. At each stage, our method leverages LLMs' metalinguistic reasoning capabilities, injecting randomness to encourage diversity and leveraging self-refinement feedback to encourage consistency in the emerging language description. We evaluate ConlangCrafter on metrics measuring consistency and typological diversity, demonstrating its ability to produce coherent and varied conlangs without human linguistic expertise.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LATTE: Learning Aligned Transactions and Textual Embeddings for Bank Clients</title>
<link>https://arxiv.org/abs/2508.10021</link>
<guid>https://arxiv.org/abs/2508.10021</guid>
<content:encoded><![CDATA[

arXiv:2508.10021v3 Announce Type: replace 
Abstract: Learning clients embeddings from sequences of their historic communications is central to financial applications. While large language models (LLMs) offer general world knowledge, their direct use on long event sequences is computationally expensive and impractical in real-world pipelines. In this paper, we propose LATTE, a contrastive learning framework that aligns raw event embeddings with semantic embeddings from frozen LLMs. Behavioral features are summarized into short prompts, embedded by the LLM, and used as supervision via contrastive loss. The proposed approach significantly reduces inference cost and input size compared to conventional processing of complete sequence by LLM. We experimentally show that our method outperforms state-of-the-art techniques for learning event sequence representations on real-world financial datasets while remaining deployable in latency-sensitive environments.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaTeXTrans: Structured LaTeX Translation with Multi-Agent Coordination</title>
<link>https://arxiv.org/abs/2508.18791</link>
<guid>https://arxiv.org/abs/2508.18791</guid>
<content:encoded><![CDATA[

arXiv:2508.18791v2 Announce Type: replace 
Abstract: Despite the remarkable progress of modern machine translation (MT) systems on general-domain texts, translating structured LaTeX-formatted documents remains a significant challenge. These documents typically interleave natural language with domain-specific syntax, such as mathematical equations, tables, figures, and cross-references, all of which must be accurately preserved to maintain semantic integrity and compilability. In this paper, we introduce LaTeXTrans, a collaborative multi-agent system designed to address this challenge. LaTeXTrans ensures format preservation, structural fidelity, and terminology consistency through six specialized agents: 1) a Parser that decomposes LaTeX into translation-friendly units via placeholder substitution and syntax filtering; 2) a Translator, Validator, Summarizer, and Terminology Extractor that work collaboratively to ensure context-aware, self-correcting, and terminology-consistent translations; 3) a Generator that reconstructs the translated content into well-structured LaTeX documents. Experimental results demonstrate that LaTeXTrans can outperform mainstream MT systems in both translation accuracy and structural fidelity, offering an effective and practical solution for translating LaTeX-formatted documents.The code of LaTeXTrans is available at https://github.com/NiuTrans/LaTeXTrans.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Label-Induced Bias in Large Language Model Self- and Cross-Evaluations</title>
<link>https://arxiv.org/abs/2508.21164</link>
<guid>https://arxiv.org/abs/2508.21164</guid>
<content:encoded><![CDATA[

arXiv:2508.21164v3 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly deployed as evaluators of text quality, yet the validity of their judgments remains underexplored. This study investigates systematic bias in self- and cross-model evaluations across three prominent LLMs: ChatGPT, Gemini, and Claude. We designed a controlled experiment in which blog posts authored by each model were evaluated by all three models under four labeling conditions: no attribution, true attribution, and two false-attribution scenarios. Evaluations employed both holistic preference voting and granular quality ratings across three dimensions Coherence, Informativeness, and Conciseness with all scores normalized to percentages for direct comparison. Our findings reveal pronounced asymmetries in model judgments: the "Claude" label consistently elevated scores regardless of actual authorship, while the "Gemini" label systematically depressed them. False attribution frequently reversed preference rankings, producing shifts of up to 50 percentage points in voting outcomes and up to 12 percentage points in quality ratings. Notably, Gemini exhibited severe self-deprecation under true labels, while Claude demonstrated intensified self-preference. These results demonstrate that perceived model identity can substantially distort both high-level judgments and fine-grained quality assessments, independent of content quality. Our findings challenge the reliability of LLM-as-judge paradigms and underscore the critical need for blind evaluation protocols and diverse multi-model validation frameworks to ensure fairness and validity in automated text evaluation and LLM benchmarking.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Large Language Models Master Complex Card Games?</title>
<link>https://arxiv.org/abs/2509.01328</link>
<guid>https://arxiv.org/abs/2509.01328</guid>
<content:encoded><![CDATA[

arXiv:2509.01328v3 Announce Type: replace 
Abstract: Complex games have long been an important benchmark for testing the progress of artificial intelligence algorithms. AlphaGo, AlphaZero, and MuZero have defeated top human players in Go and Chess, garnering widespread societal attention towards artificial intelligence. Concurrently, large language models (LLMs) have exhibited remarkable capabilities across various tasks, raising the question of whether LLMs can achieve similar success in complex games. In this paper, we explore the potential of LLMs in mastering complex card games. We systematically assess the learning capabilities of LLMs across eight diverse card games, evaluating the impact of fine-tuning on high-quality gameplay data, and examining the models' ability to retain general capabilities while mastering these games. Our findings indicate that: (1) LLMs can approach the performance of strong game AIs through supervised fine-tuning on high-quality data, (2) LLMs can achieve a certain level of proficiency in multiple complex card games simultaneously, with performance augmentation for games with similar rules and conflicts for dissimilar ones, and (3) LLMs experience a decline in general capabilities when mastering complex games, but this decline can be mitigated by integrating a certain amount of general instruction data. The evaluation results demonstrate strong learning ability and versatility of LLMs. The code is available at https://github.com/THUDM/LLM4CardGame
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COMPACT: Common-token Optimized Model Pruning Across Channels and Tokens</title>
<link>https://arxiv.org/abs/2509.06836</link>
<guid>https://arxiv.org/abs/2509.06836</guid>
<content:encoded><![CDATA[

arXiv:2509.06836v3 Announce Type: replace 
Abstract: Making large language models (LLMs) more efficient in memory, latency, and serving cost is crucial for edge deployment, interactive applications, and sustainable inference at scale. Pruning is a promising technique, but existing pruning methods are limited: width pruning often breaks the standard transformer layout, requiring custom inference code, while depth pruning can cause abrupt accuracy drops. Also, while many pruning approaches are effective against LLMs, they struggle to maintain performance on small language models (SLMs). In this work, we propose COMPACT, which jointly (i) prunes rare vocabulary to shrink embedding/LM head layers and (ii) prunes FFN intermediate channels using common-token-weighted activations, aligning importance with the post-pruning token distribution. COMPACT inherits strengths of both depth and width pruning, such as: deployment-friendliness (keeps a standard transformer architecture), scale-adaptivity (trade off vocab. vs. FFN pruning), competitive pruning times, and strong memory savings alongside throughput gains. Experiments across Qwen, LLaMA, and Gemma families (0.5B-70B) show state-of-the-art downstream performance, with substantial reductions in parameters, GPU memory, and latency.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond I'm Sorry, I Can't: Dissecting Large Language Model Refusal</title>
<link>https://arxiv.org/abs/2509.09708</link>
<guid>https://arxiv.org/abs/2509.09708</guid>
<content:encoded><![CDATA[

arXiv:2509.09708v2 Announce Type: replace 
Abstract: Refusal on harmful prompts is a key safety behaviour in instruction-tuned large language models (LLMs), yet the internal causes of this behaviour remain poorly understood. We study two public instruction-tuned models, Gemma-2-2B-IT and LLaMA-3.1-8B-IT, using sparse autoencoders (SAEs) trained on residual-stream activations. Given a harmful prompt, we search the SAE latent space for feature sets whose ablation flips the model from refusal to compliance, demonstrating causal influence and creating a jailbreak. Our search proceeds in three stages: (1) Refusal Direction: find a refusal-mediating direction and collect SAE features near that direction; (2) Greedy Filtering: prune to a minimal set; and (3) Interaction Discovery: fit a factorization machine (FM) that captures nonlinear interactions among the remaining active features and the minimal set. This pipeline yields a broad set of jailbreak-critical features, offering insight into the mechanistic basis of refusal. Moreover, we find evidence of redundant features that remain dormant unless earlier features are suppressed. Our findings highlight the potential for fine-grained auditing and targeted intervention in safety behaviours by manipulating the interpretable latent space.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RPG: A Repository Planning Graph for Unified and Scalable Codebase Generation</title>
<link>https://arxiv.org/abs/2509.16198</link>
<guid>https://arxiv.org/abs/2509.16198</guid>
<content:encoded><![CDATA[

arXiv:2509.16198v4 Announce Type: replace 
Abstract: Large language models excel at generating individual functions or single files of code, yet generating complete repositories from scratch remains a fundamental challenge. This capability is key to building coherent software systems from high-level specifications and realizing the full potential of automated code generation. The process requires planning at two levels: deciding what features and modules to build (proposal stage) and defining their implementation details (implementation stage). Current approaches rely on natural language planning, which often produces unclear specifications, misaligned components, and brittle designs due to its inherent ambiguity and lack of structure. To address these limitations, we introduce the Repository Planning Graph (RPG), a structured representation that encodes capabilities, file structures, data flows, and functions in a unified graph. By replacing free-form natural language with an explicit blueprint, RPG enables consistent long-horizon planning for repository generation. Building on RPG, we develop ZeroRepo, a graph-driven framework that operates in three stages: proposal-level planning, implementation-level construction, and graph-guided code generation with test validation. To evaluate, we construct RepoCraft, a benchmark of six real-world projects with 1,052 tasks. On RepoCraft, ZeroRepo produces nearly 36K Code Lines and 445K Code Tokens, on average 3.9$\times$ larger than the strongest baseline (Claude Code), and 68$\times$ larger than other baselines. It achieves 81.5% coverage and 69.7% test accuracy, improving over Claude Code by 27.3 and 35.8 points. Further analysis shows that RPG models complex dependencies, enables more sophisticated planning through near-linear scaling, and improves agent understanding of repositories, thus accelerating localization.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Filling in the Clinical Gaps in Benchmark: Case for HealthBench for the Japanese medical system</title>
<link>https://arxiv.org/abs/2509.17444</link>
<guid>https://arxiv.org/abs/2509.17444</guid>
<content:encoded><![CDATA[

arXiv:2509.17444v2 Announce Type: replace 
Abstract: This study investigates the applicability of HealthBench, a large-scale, rubric-based medical benchmark, to the Japanese context. Although robust evaluation frameworks are essential for the safe development of medical LLMs, resources in Japanese are scarce and often consist of translated multiple-choice questions. Our research addresses this issue in two ways. First, we establish a performance baseline by applying a machine-translated version of HealthBench's 5,000 scenarios to evaluate two models: a high-performing multilingual model (GPT-4.1) and a Japanese-native open-source model (LLM-jp-3.1). Secondly, we use an LLM-as-a-Judge approach to systematically classify the benchmark's scenarios and rubric criteria. This allows us to identify 'contextual gaps' where the content is misaligned with Japan's clinical guidelines, healthcare systems or cultural norms. Our findings reveal a modest performance drop in GPT-4.1 due to rubric mismatches, as well as a significant failure in the Japanese-native model, which lacked the required clinical completeness. Furthermore, our classification shows that, despite most scenarios being applicable, a significant proportion of the rubric criteria require localisation. This work underscores the limitations of direct benchmark translation and highlights the urgent need for a context-aware, localised adaptation, a "J-HealthBench", to ensure the reliable and safe evaluation of medical LLMs in Japan.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CFDLLMBench: A Benchmark Suite for Evaluating Large Language Models in Computational Fluid Dynamics</title>
<link>https://arxiv.org/abs/2509.20374</link>
<guid>https://arxiv.org/abs/2509.20374</guid>
<content:encoded><![CDATA[

arXiv:2509.20374v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated strong performance across general NLP tasks, but their utility in automating numerical experiments of complex physical system -- a critical and labor-intensive component -- remains underexplored. As the major workhorse of computational science over the past decades, Computational Fluid Dynamics (CFD) offers a uniquely challenging testbed for evaluating the scientific capabilities of LLMs. We introduce CFDLLMBench, a benchmark suite comprising three complementary components -- CFDQuery, CFDCodeBench, and FoamBench -- designed to holistically evaluate LLM performance across three key competencies: graduate-level CFD knowledge, numerical and physical reasoning of CFD, and context-dependent implementation of CFD workflows. Grounded in real-world CFD practices, our benchmark combines a detailed task taxonomy with a rigorous evaluation framework to deliver reproducible results and quantify LLM performance across code executability, solution accuracy, and numerical convergence behavior. CFDLLMBench establishes a solid foundation for the development and evaluation of LLM-driven automation of numerical experiments for complex physical systems. Code and data are available at https://github.com/NREL-Theseus/cfdllmbench/.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RE-Searcher: Robust Agentic Search with Goal-oriented Planning and Self-reflection</title>
<link>https://arxiv.org/abs/2509.26048</link>
<guid>https://arxiv.org/abs/2509.26048</guid>
<content:encoded><![CDATA[

arXiv:2509.26048v2 Announce Type: replace 
Abstract: Large language models (LLMs) excel at knowledge-intensive question answering and reasoning, yet their real-world deployment remains constrained by knowledge cutoff, hallucination, and limited interaction modalities. Augmenting LLMs with external search tools helps alleviate these issues, but it also exposes agents to a complex search environment in which small, plausible variations in query formulation can steer reasoning into unproductive trajectories and amplify errors. We present a systematic analysis that quantifies how environmental complexity induces fragile search behaviors and, in turn, degrades overall performance. To address this challenge, we propose a simple yet effective approach to instantiate a search agent, RE-Searcher. During search, RE-Searcher explicitly articulates a concrete search goal and subsequently reflects on whether the retrieved evidence satisfies that goal. This combination of goal-oriented planning and self-reflection enables RE-Searcher to resist spurious cues in complex search environments and perform robust search. Extensive experiments show that our method improves search accuracy and achieves state-of-the-art results. Perturbation studies further demonstrate substantial resilience to noisy or misleading external signals, mitigating the fragility of the search process. We believe these findings offer practical guidance for integrating LLM-powered agents into more complex interactive environments and enabling more autonomous decision-making.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity</title>
<link>https://arxiv.org/abs/2510.01171</link>
<guid>https://arxiv.org/abs/2510.01171</guid>
<content:encoded><![CDATA[

arXiv:2510.01171v3 Announce Type: replace 
Abstract: Post-training alignment often reduces LLM diversity, leading to a phenomenon known as mode collapse. Unlike prior work that attributes this effect to algorithmic limitations, we identify a fundamental, pervasive data-level driver: typicality bias in preference data, whereby annotators systematically favor familiar text as a result of well-established findings in cognitive psychology. We formalize this bias theoretically, verify it on preference datasets empirically, and show that it plays a central role in mode collapse. Motivated by this analysis, we introduce Verbalized Sampling, a simple, training-free prompting strategy to circumvent mode collapse. VS prompts the model to verbalize a probability distribution over a set of responses (e.g., "Generate 5 jokes about coffee and their corresponding probabilities"). Comprehensive experiments show that VS significantly improves performance across creative writing (poems, stories, jokes), dialogue simulation, open-ended QA, and synthetic data generation, without sacrificing factual accuracy and safety. For instance, in creative writing, VS increases diversity by 1.6-2.1x over direct prompting. We further observe an emergent trend that more capable models benefit more from VS. In sum, our work provides a new data-centric perspective on mode collapse and a practical inference-time remedy that helps unlock pre-trained generative diversity.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning for Detection and Analysis of Novel LLM Jailbreaks</title>
<link>https://arxiv.org/abs/2510.01644</link>
<guid>https://arxiv.org/abs/2510.01644</guid>
<content:encoded><![CDATA[

arXiv:2510.01644v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) suffer from a range of vulnerabilities that allow malicious users to solicit undesirable responses through manipulation of the input text. These so-called jailbreak prompts are designed to trick the LLM into circumventing the safety guardrails put in place to keep responses acceptable to the developer's policies. In this study, we analyse the ability of different machine learning models to distinguish jailbreak prompts from genuine uses, including looking at our ability to identify jailbreaks that use previously unseen strategies. Our results indicate that using current datasets the best performance is achieved by fine tuning a Bidirectional Encoder Representations from Transformers (BERT) model end-to-end for identifying jailbreaks. We visualise the keywords that distinguish jailbreak from genuine prompts and conclude that explicit reflexivity in prompt structure could be a signal of jailbreak intention.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Image Captioning Descriptiveness by Ranking and LLM-based Fusion</title>
<link>https://arxiv.org/abs/2306.11593</link>
<guid>https://arxiv.org/abs/2306.11593</guid>
<content:encoded><![CDATA[

arXiv:2306.11593v3 Announce Type: replace-cross 
Abstract: State-of-The-Art (SoTA) image captioning models are often trained on the MicroSoft Common Objects in Context (MS-COCO) dataset, which contains human-annotated captions with an average length of approximately ten tokens. Although effective for general scene understanding, these short captions often fail to capture complex scenes and convey detailed information. Moreover, captioning models tend to exhibit bias towards the ``average'' caption, which captures only the more general aspects, thus overlooking finer details. In this paper, we present a novel approach to generate richer and more informative image captions by combining the captions generated from different SoTA captioning models. Our proposed method requires no additional model training: given an image, it leverages pre-trained models from the literature to generate the initial captions, and then ranks them using a newly introduced image-text-based metric, which we name BLIPScore. Subsequently, the top two captions are fused using a Large Language Model (LLM) to produce the final, more detailed description. Experimental results on the MS-COCO and Flickr30k test sets demonstrate the effectiveness of our approach in terms of caption-image alignment and hallucination reduction according to the ALOHa, CAPTURE, and Polos metrics. A subjective study lends additional support to these results, suggesting that the captions produced by our model are generally perceived as more consistent with human judgment. By combining the strengths of diverse SoTA models, our method enhances the quality and appeal of image captions, bridging the gap between automated systems and the rich and informative nature of human-generated descriptions. This advance enables the generation of more suitable captions for the training of both vision-language and captioning models.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Direct Quantized Training of Language Models with Stochastic Rounding</title>
<link>https://arxiv.org/abs/2412.04787</link>
<guid>https://arxiv.org/abs/2412.04787</guid>
<content:encoded><![CDATA[

arXiv:2412.04787v3 Announce Type: replace-cross 
Abstract: Although recent quantized Large Language Models (LLMs), such as BitNet, have paved the way for significant reduction in memory usage during deployment with binary or ternary weights, training these models still demands substantial memory footprints. This is partly because high-precision (i.e., unquantized) weights required for straight-through estimation must be maintained throughout the whole training process. To address this, we explore directly updating the quantized low-precision weights without relying on straight-through estimation during backpropagation, aiming to save memory usage during training. Specifically, we employ a stochastic rounding technique to minimize the information loss caused by the use of low-bit weights throughout training. Experimental results on our LLaMA-structured models of various sizes indicate that (1) training with only low-precision weights is feasible even when they are constrained to ternary values; (2) extending the bit width to 8 bits achieves performance on par with BitNet b1.58; (3) our models remain robust to precision scaling and memory reduction, showing minimal performance degradation when moving from FP32 to lower-memory environments (BF16/FP8); and (4) our models also support inference using ternary weights, showcasing their flexibility in deployment.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chain-of-Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2501.14342</link>
<guid>https://arxiv.org/abs/2501.14342</guid>
<content:encoded><![CDATA[

arXiv:2501.14342v3 Announce Type: replace-cross 
Abstract: This paper introduces an approach for training o1-like RAG models that retrieve and reason over relevant information step by step before generating the final answer. Conventional RAG methods usually perform a single retrieval step before the generation process, which limits their effectiveness in addressing complex queries due to imperfect retrieval results. In contrast, our proposed method, CoRAG (Chain-of-Retrieval Augmented Generation), allows the model to dynamically reformulate the query based on the evolving state. To train CoRAG effectively, we utilize rejection sampling to automatically generate intermediate retrieval chains, thereby augmenting existing RAG datasets that only provide the correct final answer. At test time, we propose various decoding strategies to scale the model's test-time compute by controlling the length and number of sampled retrieval chains. Experimental results across multiple benchmarks validate the efficacy of CoRAG, particularly in multi-hop question answering tasks, where we observe more than 10 points improvement in EM score compared to strong baselines. On the KILT benchmark, CoRAG establishes a new state-of-the-art performance across a diverse range of knowledge-intensive tasks. Furthermore, we offer comprehensive analyses to understand the scaling behavior of CoRAG, laying the groundwork for future research aimed at developing factual and grounded foundation models.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Issue Localization via LLM-Driven Iterative Code Graph Searching</title>
<link>https://arxiv.org/abs/2503.22424</link>
<guid>https://arxiv.org/abs/2503.22424</guid>
<content:encoded><![CDATA[

arXiv:2503.22424v3 Announce Type: replace-cross 
Abstract: Issue solving aims to generate patches to fix reported issues in real-world code repositories according to issue descriptions. Issue localization forms the basis for accurate issue solving. Recently, LLM-based issue localization methods have demonstrated state-of-the-art performance. However, these methods either search from files mentioned in issue descriptions or in the whole repository and struggle to balance the breadth and depth of the search space to converge on the target efficiently. Moreover, they allow LLM to explore whole repositories freely, making it challenging to control the search direction to prevent the LLM from searching for incorrect targets. This paper introduces CoSIL, an LLM-driven, powerful function-level issue localization method without training or indexing. CoSIL employs a two-phase code graph search strategy. It first conducts broad exploration at the file level using dynamically constructed module call graphs, and then performs in-depth analysis at the function level by expanding the module call graph into a function call graph and executing iterative searches. To precisely control the search direction, CoSIL designs a pruner to filter unrelated directions and irrelevant contexts. To avoid incorrect interaction formats in long contexts, CoSIL introduces a reflection mechanism that uses additional independent queries in short contexts to enhance formatted abilities. Experiment results demonstrate that CoSIL achieves a Top-1 localization accuracy of 43.3\% and 44.6\% on SWE-bench Lite and SWE-bench Verified, respectively, with Qwen2.5-Coder-32B, average outperforming the state-of-the-art methods by 96.04\%. When CoSIL is integrated into an issue-solving method, Agentless, the issue resolution rate improves by 2.98\%--30.5\%.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An LLM-as-Judge Metric for Bridging the Gap with Human Evaluation in SE Tasks</title>
<link>https://arxiv.org/abs/2505.20854</link>
<guid>https://arxiv.org/abs/2505.20854</guid>
<content:encoded><![CDATA[

arXiv:2505.20854v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) and other automated techniques have been increasingly used to support software developers by generating software artifacts such as code snippets, patches, and comments. However, accurately assessing the correctness of these generated artifacts remains a significant challenge. On one hand, human evaluation provides high accuracy but is labor-intensive and lacks scalability. On the other hand, many automatic evaluation metrics are scalable and require minimal human effort, but they often fail to accurately reflect the actual correctness of generated software artifacts.
  In this paper, we present SE-Jury, the first evaluation metric for LLM-as-Ensemble-Judge specifically designed to accurately assess the correctness of generated software artifacts. SE-Jury first defines five distinct evaluation strategies, each implemented by an independent judge. A dynamic team selection mechanism then identifies the most appropriate subset of judges as a team to produce a final correctness score through ensembling. We evaluate SE-Jury across a diverse set of software engineering (SE) benchmarks that span three popular SE tasks: code generation, automated program repair, and code summarization. Results demonstrate that SE-Jury consistently achieves a higher correlation with human judgments, with improvements ranging from 29.6% to 140.8% over existing automatic metrics. SE-Jury reaches agreement levels with human annotators that are close to inter-annotator agreement in code generation and program repair. These findings underscore SE-Jury's potential as a scalable and reliable alternative to human evaluation in these SE tasks.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CausalVLBench: Benchmarking Visual Causal Reasoning in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2506.11034</link>
<guid>https://arxiv.org/abs/2506.11034</guid>
<content:encoded><![CDATA[

arXiv:2506.11034v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have shown remarkable ability in various language tasks, especially with their emergent in-context learning capability. Extending LLMs to incorporate visual inputs, large vision-language models (LVLMs) have shown impressive performance in tasks such as recognition and visual question answering (VQA). Despite increasing interest in the utility of LLMs in causal reasoning tasks such as causal discovery and counterfactual reasoning, there has been relatively little work showcasing the abilities of LVLMs on visual causal reasoning tasks. We take this opportunity to formally introduce a comprehensive causal reasoning benchmark for multi-modal in-context learning from LVLMs. Our CausalVLBench encompasses three representative tasks: causal structure inference, intervention target prediction, and counterfactual prediction. We evaluate the ability of state-of-the-art open-source LVLMs on our causal reasoning tasks across three causal representation learning datasets and demonstrate their fundamental strengths and weaknesses. We hope that our benchmark elucidates the drawbacks of existing vision-language models and motivates new directions and paradigms in improving the visual causal reasoning abilities of LVLMs.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments with a Hierarchical Spatial-Cognition Long-Short Memory System</title>
<link>https://arxiv.org/abs/2506.19433</link>
<guid>https://arxiv.org/abs/2506.19433</guid>
<content:encoded><![CDATA[

arXiv:2506.19433v2 Announce Type: replace-cross 
Abstract: Vision-and-Language Navigation (VLN) in large-scale urban environments requires embodied agents to ground linguistic instructions in complex scenes and recall relevant experiences over extended time horizons. Prior modular pipelines offer interpretability but lack unified memory, while end-to-end (M)LLM agents excel at fusing vision and language yet remain constrained by fixed context windows and implicit spatial reasoning. We introduce \textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system that can augment any VLN backbone. Mem4Nav fuses a sparse octree for fine-grained voxel indexing with a semantic topology graph for high-level landmark connectivity, storing both in trainable memory tokens embedded via a reversible Transformer. Long-term memory (LTM) compresses and retains historical observations at both octree and graph nodes, while short-term memory (STM) caches recent multimodal entries in relative coordinates for real-time obstacle avoidance and local planning. At each step, STM retrieval sharply prunes dynamic context, and, when deeper history is needed, LTM tokens are decoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and Map2Seq across three backbones (modular, state-of-the-art VLN with prompt-based LLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13 pp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW improvement. Ablations confirm the indispensability of both the hierarchical map and dual memory modules. Our codes are open-sourced via https://github.com/tsinghua-fib-lab/Mem4Nav.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training</title>
<link>https://arxiv.org/abs/2507.01752</link>
<guid>https://arxiv.org/abs/2507.01752</guid>
<content:encoded><![CDATA[

arXiv:2507.01752v2 Announce Type: replace-cross 
Abstract: Gradient-based optimization is the workhorse of deep learning, offering efficient and scalable training via backpropagation. However, exposing gradients during training can leak sensitive information about the underlying data, raising privacy and security concerns such as susceptibility to data poisoning attacks. In contrast, black box optimization methods, which treat the model as an opaque function, relying solely on function evaluations to guide optimization, offer a promising alternative in scenarios where data access is restricted, adversarial risks are high, or overfitting is a concern. This paper introduces BBoxER, an evolutionary black-box method for LLM post-training that induces an information bottleneck via implicit compression of the training data. Leveraging the tractability of information flow, we provide non-vacuous generalization bounds and strong theoretical guarantees for differential privacy, robustness to data poisoning attacks, and extraction attacks. In experiments with LLMs, we demonstrate empirically that black-box optimization methods-despite the scalability and computational challenges inherent to black-box approaches-are able to learn, showing how a few iterations of BBoxER improve performance, generalize well on a benchmark of reasoning datasets, and are robust to membership inference attacks. This positions BBoxER as an attractive add-on on top of gradient-based optimization, offering suitability for deployment in restricted or privacy-sensitive environments while also providing non-vacuous generalization guarantees.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Agent for Modular Task Execution in Drug Discovery</title>
<link>https://arxiv.org/abs/2507.02925</link>
<guid>https://arxiv.org/abs/2507.02925</guid>
<content:encoded><![CDATA[

arXiv:2507.02925v2 Announce Type: replace-cross 
Abstract: We present a modular framework powered by large language models (LLMs) that automates and streamlines key tasks across the early-stage computational drug discovery pipeline. By combining LLM reasoning with domain-specific tools, the framework performs biomedical data retrieval, domain-specific question answering, molecular generation, property prediction, property-aware molecular refinement, and 3D protein-ligand structure generation. In a case study targeting BCL-2 in lymphocytic leukemia, the agent autonomously retrieved relevant biomolecular information, including FASTA sequences, SMILES representations, and literature, and answered mechanistic questions with improved contextual accuracy compared to standard LLMs. It then generated chemically diverse seed molecules and predicted 67 ADMET-related properties, which guided iterative molecular refinement. Across two refinement rounds, the number of molecules with QED > 0.6 increased from 34 to 55. The number of molecules satisfying empirical drug-likeness filters also rose; for example, compliance with the Ghose filter increased from 32 to 55 within a pool of 100 molecules. The framework also employed Boltz-2 to generate 3D protein-ligand complexes and provide rapid binding affinity estimates for candidate compounds. These results demonstrate that the approach effectively supports molecular screening, prioritization, and structure evaluation. Its modular design enables flexible integration of evolving tools and models, providing a scalable foundation for AI-assisted therapeutic discovery.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preprint: Poster: Did I Just Browse A Website Written by LLMs?</title>
<link>https://arxiv.org/abs/2507.13933</link>
<guid>https://arxiv.org/abs/2507.13933</guid>
<content:encoded><![CDATA[

arXiv:2507.13933v2 Announce Type: replace-cross 
Abstract: Increasingly, web content is automatically generated by large language models (LLMs) with little human input. We call this "LLM-dominant" content. Since LLMs plagiarize and hallucinate, LLM-dominant content can be unreliable and unethical. Yet, websites rarely disclose such content, and human readers struggle to distinguish it. Thus, we must develop reliable detectors for LLM-dominant content. However, state-of-the-art LLM detectors are inaccurate on web content, because web content has low positive rates, complex markup, and diverse genres, instead of clean, prose-like benchmark data SoTA detectors are optimized for.
  We propose a highly reliable, scalable pipeline that classifies entire websites. Instead of naively classifying text extracted from each page, we classify each site based on an LLM text detector's outputs of multiple prose-like pages to boost accuracies. We train and evaluate our detector by collecting 2 distinct ground truth datasets totaling 120 sites, and obtain 100% accuracies testing across them. In the wild, we detect a sizable portion of sites as LLM-dominant among 10k sites in search engine results and 10k in Common Crawl archives. We find LLM-dominant sites are growing in prevalence and rank highly in search results, raising questions about their impact on end users and the overall Web ecosystem.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Model Guided Reinforcement Learning in Quantitative Trading</title>
<link>https://arxiv.org/abs/2508.02366</link>
<guid>https://arxiv.org/abs/2508.02366</guid>
<content:encoded><![CDATA[

arXiv:2508.02366v2 Announce Type: replace-cross 
Abstract: Algorithmic trading requires short-term tactical decisions consistent with long-term financial objectives. Reinforcement Learning (RL) has been applied to such problems, but adoption is limited by myopic behaviour and opaque policies. Large Language Models (LLMs) offer complementary strategic reasoning and multi-modal signal interpretation when guided by well-structured prompts.
  This paper proposes a hybrid framework in which LLMs generate high-level trading strategies to guide RL agents. We evaluate (i) the economic rationale of LLM-generated strategies through expert review, and (ii) the performance of LLM-guided agents against unguided RL baselines using Sharpe Ratio (SR) and Maximum Drawdown (MDD).
  Empirical results indicate that LLM guidance improves both return and risk metrics relative to standard RL.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning through Exploration: A Reinforcement Learning Framework for Robust Function Calling</title>
<link>https://arxiv.org/abs/2508.05118</link>
<guid>https://arxiv.org/abs/2508.05118</guid>
<content:encoded><![CDATA[

arXiv:2508.05118v4 Announce Type: replace-cross 
Abstract: The effective training of Large Language Models (LLMs) for function calling faces a critical challenge: balancing exploration of complex reasoning paths with stable policy optimization. Standard methods like Supervised Fine-Tuning (SFT) fail to instill robust reasoning, and traditional Reinforcement Learning (RL) struggles with inefficient exploration. We propose \textbf{EGPO}, a new RL framework built upon Group Relative Policy Optimization (GRPO), designed to address this challenge directly. The core of EGPO is an entropy-enhanced advantage function that integrates the entropy of the model's Chain-of-Thought (CoT) into the policy gradient computation. This encourages the generation of diverse reasoning strategies. To maintain optimization direction, the entropy bonus is carefully constrained by a clipping mechanism. Complemented by a strict, binary reward signal, EGPO effectively guides the model towards discovering structured and accurate tool invocation patterns. On the challenging Berkeley Function Calling Leaderboard (BFCL), a 4B-parameter model trained with EGPO sets a new state-of-the-art among models of comparable size, surpassing a range of strong competitors, including GPT-4o and Gemini-2.5.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance</title>
<link>https://arxiv.org/abs/2508.06944</link>
<guid>https://arxiv.org/abs/2508.06944</guid>
<content:encoded><![CDATA[

arXiv:2508.06944v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are typically fine-tuned for reasoning tasks through a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL), a process fraught with catastrophic forgetting and suboptimal trade-offs between imitation and exploration. Recent single-stage methods attempt to unify SFT and RL using heuristics, but lack a principled mechanism for dynamically balancing the two paradigms. In this paper, we reframe this challenge through the theoretical lens of \textbf{implicit rewards}, viewing SFT and RL not as distinct methods but as complementary reward signals. We introduce \textbf{Adaptive Meta Fine-Tuning (AMFT)}, a novel single-stage algorithm that learns the optimal balance between SFT's implicit, path-level reward and RL's explicit, outcome-based reward. The core of AMFT is a \textbf{meta-gradient adaptive weight controller} that treats the SFT-RL balance as a learnable parameter, dynamically optimizing it to maximize long-term task performance. This forward-looking approach, regularized by policy entropy for stability, autonomously discovers an effective training curriculum. We conduct a comprehensive evaluation on challenging benchmarks spanning mathematical reasoning, abstract visual reasoning (General Points), and vision-language navigation (V-IRL). AMFT consistently establishes a new state-of-the-art and demonstrats superior generalization on out-of-distribution (OOD) tasks. Ablation studies and training dynamic analysis confirm that the meta-learning controller is crucial for AMFT's stability, sample efficiency, and performance, offering a more principled and effective paradigm for LLM alignment. Our codes are open-sourced via https://github.com/hlxtsyj/AMFT.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anemoi: A Semi-Centralized Multi-agent System Based on Agent-to-Agent Communication MCP server from Coral Protocol</title>
<link>https://arxiv.org/abs/2508.17068</link>
<guid>https://arxiv.org/abs/2508.17068</guid>
<content:encoded><![CDATA[

arXiv:2508.17068v3 Announce Type: replace-cross 
Abstract: Recent advances in generalist multi-agent systems (MAS) have largely followed a context-engineering plus centralized paradigm, where a planner agent coordinates multiple worker agents through unidirectional prompt passing. While effective under strong planner models, this design suffers from two critical limitations: (1) strong dependency on the planner's capability, which leads to degraded performance when a smaller LLM powers the planner; and (2) limited inter-agent communication, where collaboration relies on prompt concatenation rather than genuine refinement through structured discussions. To address these challenges, we propose Anemoi, a semi-centralized MAS built on the Agent-to-Agent (A2A) communication MCP server from Coral Protocol. Unlike traditional designs, Anemoi enables structured and direct inter-agent collaboration, allowing all agents to monitor progress, assess results, identify bottlenecks, and propose refinements in real time. This paradigm reduces reliance on a single planner, supports adaptive plan updates, and minimizes redundant context passing, resulting in more scalable execution. Evaluated on the GAIA benchmark, Anemoi achieved 52.73% accuracy with a small LLM (GPT-4.1-mini) as the planner, surpassing the strongest open-source baseline OWL (43.63%) by +9.09% under identical LLM settings. Our implementation is publicly available at https://github.com/Coral-Protocol/Anemoi.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeHate: A Stable Diffusion-based Multimodal Approach to Mitigate Hate Speech in Images</title>
<link>https://arxiv.org/abs/2509.21787</link>
<guid>https://arxiv.org/abs/2509.21787</guid>
<content:encoded><![CDATA[

arXiv:2509.21787v2 Announce Type: replace-cross 
Abstract: The rise in harmful online content not only distorts public discourse but also poses significant challenges to maintaining a healthy digital environment. In response to this, we introduce a multimodal dataset uniquely crafted for identifying hate in digital content. Central to our methodology is the innovative application of watermarked, stability-enhanced, stable diffusion techniques combined with the Digital Attention Analysis Module (DAAM). This combination is instrumental in pinpointing the hateful elements within images, thereby generating detailed hate attention maps, which are used to blur these regions from the image, thereby removing the hateful sections of the image. We release this data set as a part of the dehate shared task. This paper also describes the details of the shared task. Furthermore, we present DeHater, a vision-language model designed for multimodal dehatification tasks. Our approach sets a new standard in AI-driven image hate detection given textual prompts, contributing to the development of more ethical AI applications in social media.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARS: Adaptive Reasoning Suppression for Efficient Large Reasoning Language Models</title>
<link>https://arxiv.org/abs/2510.00071</link>
<guid>https://arxiv.org/abs/2510.00071</guid>
<content:encoded><![CDATA[

arXiv:2510.00071v2 Announce Type: replace-cross 
Abstract: Large Reasoning Language Models (LRLMs or LRMs) demonstrate remarkable capabilities in complex reasoning tasks, but suffer from significant computational inefficiencies due to overthinking phenomena. Existing efficient reasoning methods face the challenge of balancing reasoning quality with inference cost reduction. We propose \textbf{Adaptive Reasoning Suppression (ARS)}, a novel training-free approach that dynamically suppresses redundant reasoning steps while preserving accuracy through adaptive certainty monitoring. ARS introduces a multi-checkpoint certainty estimation mechanism with progressive suppression thresholds, achieving superior efficiency compared to static suppression methods. Our extensive evaluation across mathematical reasoning benchmarks using multiple model architectures demonstrates that ARS achieves up to 53%, 46.1%, and 57.9% in token, latency and energy reduction, while maintaining or improving accuracy.
]]></content:encoded>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
<pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
</item>

<item>
<title>Learning to Reason for Hallucination Span Detection</title>
<link>https://arxiv.org/abs/2510.02173</link>
<guid>https://arxiv.org/abs/2510.02173</guid>
<content:encoded><![CDATA[
<div> reasoning, hallucination detection, reinforcement learning, large language models, span-level rewards 
Summary: 
This paper addresses the issue of hallucinations generated by Large Language Models (LLMs) and the need for detecting hallucinated spans in a multi-step decision-making process. The authors propose a reinforcement learning framework called RL4HS, which incorporates Chain-of-Thought (CoT) reasoning and span-level rewards to improve the accuracy of identifying hallucination spans. Through experiments on the RAGTruth benchmark, including tasks such as summarization, question answering, and data-to-text, the study demonstrates that RL4HS outperforms pretrained reasoning models and supervised fine-tuning. The results highlight the importance of explicit reasoning and reinforcement learning with span-level rewards in accurately detecting hallucination spans and improving reliability in natural language processing applications. <br /><br />Summary: <div>
arXiv:2510.02173v2 Announce Type: replace 
Abstract: Large language models (LLMs) often generate hallucinations -- unsupported content that undermines reliability. While most prior works frame hallucination detection as a binary task, many real-world applications require identifying hallucinated spans, which is a multi-step decision making process. This naturally raises the question of whether explicit reasoning can help the complex task of detecting hallucination spans. To answer this question, we first evaluate pretrained models with and without Chain-of-Thought (CoT) reasoning, and show that CoT reasoning has the potential to generate at least one correct answer when sampled multiple times. Motivated by this, we propose RL4HS, a reinforcement learning framework that incentivizes reasoning with a span-level reward function. RL4HS builds on Group Relative Policy Optimization and introduces Class-Aware Policy Optimization to mitigate reward imbalance issue. Experiments on the RAGTruth benchmark (summarization, question answering, data-to-text) show that RL4HS surpasses pretrained reasoning models and supervised fine-tuning, demonstrating the necessity of reinforcement learning with span-level rewards for detecting hallucination spans.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for Diverse Exploration</title>
<link>https://arxiv.org/abs/2510.02227</link>
<guid>https://arxiv.org/abs/2510.02227</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Verifiable Rewards, Large Language Models, Adaptive Multi-Guidance Policy Optimization, Exploration <br />
<br />
Summary: 
The paper discusses Reinforcement Learning with Verifiable Rewards (RLVR) as a method to enhance reasoning in Large Language Models (LLMs). Current methods rely on self-exploration or a single teacher model, limiting reasoning diversity and performance. The authors propose Adaptive Multi-Guidance Policy Optimization (AMPO), which uses multiple teacher models to guide the on-policy model only when needed. This approach allows for expanded exploration while maintaining self-discovery value. The selection mechanism prompts the student model to learn from reasoning paths it is likely to comprehend, balancing exploration and exploitation. Experimental results show that AMPO outperforms a strong baseline method on mathematical reasoning tasks and out-of-distribution tasks, improving Pass@k performance and enabling more diverse exploration. By utilizing multiple peer-sized teachers, AMPO achieves comparable results to methods using a single, more powerful teacher model, showcasing its efficiency and scalability in enhancing reasoning and generalizability. <div>
arXiv:2510.02227v2 Announce Type: replace 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm for enhancing the reasoning ability in Large Language Models (LLMs). However, prevailing methods primarily rely on self-exploration or a single off-policy teacher to elicit long chain-of-thought (LongCoT) reasoning, which may introduce intrinsic model biases and restrict exploration, ultimately limiting reasoning diversity and performance. Drawing inspiration from multi-teacher strategies in knowledge distillation, we introduce Adaptive Multi-Guidance Policy Optimization (AMPO), a novel framework that adaptively leverages guidance from multiple proficient teacher models, but only when the on-policy model fails to generate correct solutions. This "guidance-on-demand" approach expands exploration while preserving the value of self-discovery. Moreover, AMPO incorporates a comprehension-based selection mechanism, prompting the student to learn from the reasoning paths that it is most likely to comprehend, thus balancing broad exploration with effective exploitation. Extensive experiments show AMPO substantially outperforms a strong baseline (GRPO), with a 4.3% improvement on mathematical reasoning tasks and 12.2% on out-of-distribution tasks, while significantly boosting Pass@k performance and enabling more diverse exploration. Notably, using four peer-sized teachers, our method achieves comparable results to approaches that leverage a single, more powerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate a more efficient and scalable path to superior reasoning and generalizability. Our code is available at https://github.com/SII-Enigma/AMPO.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inconsistent Affective Reaction: Sentiment of Perception and Opinion in Urban Environments</title>
<link>https://arxiv.org/abs/2510.07359</link>
<guid>https://arxiv.org/abs/2510.07359</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, sentiment analysis, urban environments, perception, opinion

Summary:<br /><br />The study explores the impact of social media on urban sentiment analysis, using innovative methodologies to measure perception and opinion in Beijing. By analyzing images and text posts from platforms like Baidu, Tencent Street view, and Weibo, the study detects sentiment inconsistencies and trends in sentiment reactions. It identifies disparities between human perception and opinion sentiments, highlighting changes in sentiment in urban areas before and after the pandemic. The analysis shows a shift towards more evenly distributed positive sentiment in perception reactions, while opinion reactions exhibit more extreme changes. The study also reveals significant relationships between sentiment reactions and elements like dense buildings and pedestrian presence. These findings provide valuable insights for environmental management strategies and urban renewal efforts. <div>
arXiv:2510.07359v1 Announce Type: new 
Abstract: The ascension of social media platforms has transformed our understanding of urban environments, giving rise to nuanced variations in sentiment reaction embedded within human perception and opinion, and challenging existing multidimensional sentiment analysis approaches in urban studies. This study presents novel methodologies for identifying and elucidating sentiment inconsistency, constructing a dataset encompassing 140,750 Baidu and Tencent Street view images to measure perceptions, and 984,024 Weibo social media text posts to measure opinions. A reaction index is developed, integrating object detection and natural language processing techniques to classify sentiment in Beijing Second Ring for 2016 and 2022. Classified sentiment reaction is analysed and visualized using regression analysis, image segmentation, and word frequency based on land-use distribution to discern underlying factors. The perception affective reaction trend map reveals a shift toward more evenly distributed positive sentiment, while the opinion affective reaction trend map shows more extreme changes. Our mismatch map indicates significant disparities between the sentiments of human perception and opinion of urban areas over the years. Changes in sentiment reactions have significant relationships with elements such as dense buildings and pedestrian presence. Our inconsistent maps present perception and opinion sentiments before and after the pandemic and offer potential explanations and directions for environmental management, in formulating strategies for urban renewal.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Haystack Engineering: Context Engineering for Heterogeneous and Agentic Long-Context Evaluation</title>
<link>https://arxiv.org/abs/2510.07414</link>
<guid>https://arxiv.org/abs/2510.07414</guid>
<content:encoded><![CDATA[
<div> retrieval strategies, long-context models, HaystackCraft benchmark, agentic operations, distractors <br />
Summary:<br />
The article discusses the limitations of current synthetic benchmarks for evaluating long-context large language models (LLMs) and proposes the need for "haystack engineering" to create more realistic noisy contexts. The authors introduce HaystackCraft, a new benchmark based on the English Wikipedia hyperlink network, to evaluate how different retrieval strategies impact distractors and LLM performance. They find that dense retrievers can introduce challenging distractors, but graph-based reranking can improve retrieval effectiveness. In agentic tests, even advanced models like Gemini 2.5 Pro and GPT-5 struggle with self-generated distractors and early stops. These findings underscore the ongoing challenges in agentic long-context reasoning and position HaystackCraft as a valuable testbed for future research in this area. <br /> <div>
arXiv:2510.07414v1 Announce Type: new 
Abstract: Modern long-context large language models (LLMs) perform well on synthetic "needle-in-a-haystack" (NIAH) benchmarks, but such tests overlook how noisy contexts arise from biased retrieval and agentic workflows. We argue that haystack engineering is necessary to construct noisy long contexts that faithfully capture key real-world factors -- distraction from heterogeneous biased retrievers and cascading errors in agentic workflows -- to test models' long-context robustness. We instantiate it through HaystackCraft, a new NIAH benchmark built on the full English Wikipedia hyperlink network with multi-hop questions. HaystackCraft evaluates how heterogeneous retrieval strategies (e.g., sparse, dense, hybrid, and graph-based) affect distractor composition, haystack ordering, and downstream LLM performance. HaystackCraft further extends NIAH to dynamic, LLM-dependent settings that simulate agentic operations, where models refine queries, reflect on their past reasonings, and decide when to stop. Experiments with 15 long-context models show that (1) while stronger dense retrievers can introduce more challenging distractors, graph-based reranking simultaneously improves retrieval effectiveness and mitigates more harmful distractors; (2) in agentic tests, even advanced models like Gemini 2.5 Pro and GPT-5 suffer cascading failures from self-generated distractors or struggle to perform early stops. These results highlight persistent challenges in agentic long-context reasoning and establish HaystackCraft as a valuable testbed for future progress.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lemma Dilemma: On Lemma Generation Without Domain- or Language-Specific Training Data</title>
<link>https://arxiv.org/abs/2510.07434</link>
<guid>https://arxiv.org/abs/2510.07434</guid>
<content:encoded><![CDATA[
<div> transforming, words, dictionary, language, lemmatization

Summary:
- Lemmatization is the process of transforming words in a text into their dictionary forms.
- Large language models (LLMs) have shown competitive performance in various NLP tasks but their effectiveness in in-context lemmatization was not known.
- This study compares LLMs to traditional supervised approaches for lemmatization when training data is not available for a specific domain or language.
- Experimentation across 12 languages with different morphological complexities found that LLMs can achieve state-of-the-art results for most languages by directly generating lemmas in context without prior fine-tuning, with just a few examples.
- The findings suggest that current LLMs have the potential to excel in in-context lemmatization tasks without extensive fine-tuning, offering a promising approach for languages and domains with limited training data.

<br /><br />Summary: <div>
arXiv:2510.07434v1 Announce Type: new 
Abstract: Lemmatization is the task of transforming all words in a given text to their dictionary forms. While large language models (LLMs) have demonstrated their ability to achieve competitive results across a wide range of NLP tasks, there is no prior evidence of how effective they are in the contextual lemmatization task. In this paper, we empirically investigate the capacity of the latest generation of LLMs to perform in-context lemmatization, comparing it to the traditional fully supervised approach. In particular, we consider the setting in which supervised training data is not available for a target domain or language, comparing (i) encoder-only supervised approaches, fine-tuned out-of-domain, and (ii) cross-lingual methods, against direct in-context lemma generation with LLMs. Our experimental investigation across 12 languages of different morphological complexity finds that, while encoders remain competitive in out-of-domain settings when fine-tuned on gold data, current LLMs reach state-of-the-art results for most languages by directly generating lemmas in-context without prior fine-tuning, provided just with a few examples. Data and code available upon publication: https://github.com/oltoporkov/lemma-dilemma
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LASER: An LLM-based ASR Scoring and Evaluation Rubric</title>
<link>https://arxiv.org/abs/2510.07437</link>
<guid>https://arxiv.org/abs/2510.07437</guid>
<content:encoded><![CDATA[
<div> Keywords: ASR evaluation metrics, LASER, LLM, Hindi, morphology

Summary: 
LASER, a scoring rubric leveraging LLMs' abilities, shows high correlation with human annotations in evaluating Hindi ASR. Examples in the prompt prove effective for error analysis in Indian languages. The LLM Llama 3 can be finetuned on word-pair examples for penalty prediction with high accuracy. The study highlights the unfair penalization of morphological and syntactic nuances by standard ASR metrics like WER. The use of detailed examples in prompts enhances the evaluation process and improves error analysis across languages. The success of LASER in Hindi assessment suggests its potential applicability in other language evaluations as well.<br /><br />Summary: <div>
arXiv:2510.07437v1 Announce Type: new 
Abstract: Standard ASR evaluation metrics like Word Error Rate (WER) tend to unfairly penalize morphological and syntactic nuances that do not significantly alter sentence semantics. We introduce an LLM-based scoring rubric LASER that leverages state-of-the-art LLMs' in-context learning abilities to learn from prompts with detailed examples. Hindi LASER scores using Gemini 2.5 Pro achieved a very high correlation score of 94% with human annotations. Hindi examples in the prompt were also effective in analyzing errors in other Indian languages such as Marathi, Kannada and Malayalam. We also demonstrate how a smaller LLM like Llama 3 can be finetuned on word-pair examples derived from reference and ASR predictions to predict what kind of penalty should be applied with close to 89% accuracy.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meaningful Pose-Based Sign Language Evaluation</title>
<link>https://arxiv.org/abs/2510.07453</link>
<guid>https://arxiv.org/abs/2510.07453</guid>
<content:encoded><![CDATA[
<div> Keywords: sign language, skeletal poses, evaluation metrics, text-to-pose translation, open-source toolkit

Summary: 
In this study, the evaluation of sign language utterances represented as human skeletal poses is investigated using various metrics. The research covers three main categories: keypoint distance-based, embedding-based, and back-translation-based metrics. By conducting automatic meta-evaluation of sign-level retrieval and a human correlation study of text-to-pose translation across different sign languages, the study reveals the tradeoffs between different metrics in various scenarios. The findings from this comprehensive study, along with the release of an open-source pose-evaluation toolkit, provide a practical and reproducible approach for the development and evaluation of sign language translation and generation systems. This research contributes valuable insights into the meaningful assessment of sign language utterances and offers a systematic framework for future research in this field. 

<br /><br />Summary: <div>
arXiv:2510.07453v1 Announce Type: new 
Abstract: We present a comprehensive study on meaningfully evaluating sign language utterances in the form of human skeletal poses. The study covers keypoint distance-based, embedding-based, and back-translation-based metrics. We show tradeoffs between different metrics in different scenarios through automatic meta-evaluation of sign-level retrieval and a human correlation study of text-to-pose translation across different sign languages. Our findings and the open-source pose-evaluation toolkit provide a practical and reproducible way of developing and evaluating sign language translation or generation systems.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Populism Meets AI: Advancing Populism Research with LLMs</title>
<link>https://arxiv.org/abs/2510.07458</link>
<guid>https://arxiv.org/abs/2510.07458</guid>
<content:encoded><![CDATA[
<div> Keywords: populism, ideational content, textual analysis, Global Populism Database, LLM

Summary: 
- Measuring the ideational content of populism is challenging, but traditional textual analysis methods have limitations in scalability and efficiency.
- A new approach using a chain of thought (CoT) prompting method guided by a rubric and anchor replicates human coder training.
- Leveraging the Global Populism Database, this approach prompts a language model with documentation to guide reasoning.
- Testing multiple weight models shows that this prompting strategy enables the language model to achieve classification accuracy comparable to human coders.
- The study demonstrates the effectiveness of this domain-specific prompting strategy in capturing the nuanced and context-sensitive aspects of populism.<br /><br />Summary: <div>
arXiv:2510.07458v1 Announce Type: new 
Abstract: Measuring the ideational content of populism remains a challenge. Traditional strategies based on textual analysis have been critical for building the field's foundations and providing a valid, objective indicator of populist framing. Yet these approaches are costly, time consuming, and difficult to scale across languages, contexts, and large corpora. Here we present the results from a rubric and anchor guided chain of thought (CoT) prompting approach that mirrors human coder training. By leveraging the Global Populism Database (GPD), a comprehensive dataset of global leaders' speeches annotated for degrees of populism, we replicate the process used to train human coders by prompting the LLM with an adapted version of the same documentation to guide the model's reasoning. We then test multiple proprietary and open weight models by replicating scores in the GPD. Our findings reveal that this domain specific prompting strategy enables the LLM to achieve classification accuracy on par with expert human coders, demonstrating its ability to navigate the nuanced, context sensitive aspects of populism.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAPRO: Recasting Multi-Agent Prompt Optimization as Maximum a Posteriori Inference</title>
<link>https://arxiv.org/abs/2510.07475</link>
<guid>https://arxiv.org/abs/2510.07475</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, multi-agent systems, prompt optimization, max-product belief propagation algorithm, coordinated set

Summary: 
The article introduces a framework called MAPRO for optimizing prompts in multi-agent systems that leverages the power of large language models. By formulating prompt optimization as a Maximum a Posteriori (MAP) inference problem and utilizing a variant of the max-product belief propagation algorithm, MAPRO is able to iteratively update agent prompts based on execution feedback and downstream blames. This approach addresses challenges such as credit assignment and system updates, ultimately leading to a coordinated set of agent-specific prompt policies. Through experiments on various benchmarks, MAPRO achieves state-of-the-art performance, surpassing both manually engineered baselines and recent automated alternatives. The MAP-based formulation not only improves performance but also provides guidelines for building more reliable and principled multi-agent systems in the future.<br /><br />Summary: <div>
arXiv:2510.07475v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across diverse tasks, and LLM-based agents further extend these abilities to various practical workflows. While recent progress shows that multi-agent systems (MAS) can outperform single agents by coordinating specialized roles, designing effective MAS remains difficult due to prompt sensitivity and the compounded instability MAS creates. To cope with the challenge, recent efforts in automated prompt design have reduced manual effort. However, multi-agent prompt optimization remains largely unexplored. Challenges like exponentially expanding search space and ambiguous credit assignment together make systematic design intractable without principled methods. Therefore, we introduce M}ulti-Agent PRompt Optimization (MAPRO), a four-stage framework that first formulates MAS prompt optimization as a Maximum a Posteriori (MAP) inference problem and solves it using a language-guided variant of max-product belief propagation algorithm. To address credit assignment and updates the system iteratively, MAPRO employs a topology-aware refinement mechanism that integrates execution feedback and downstream blames to selectively update agent prompts. Through this process, MAPRO progressively converges to a coordinated set of agent-specific prompt policies. Across benchmarks in various tasks, MAPRO achieves state-of-the-art performance, consistently surpassing manually engineered baselines and recent automated alternatives. Beyond performance, our MAP-based formulation also delivers general guidelines for building more reliable and principled multi-agent systems in the future
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AsyncSpade: Efficient Test-Time Scaling with Asynchronous Sparse Decoding</title>
<link>https://arxiv.org/abs/2510.07486</link>
<guid>https://arxiv.org/abs/2510.07486</guid>
<content:encoded><![CDATA[
<div> asyncSpade, test-time scaling, long chain-of-thought, asynchronous framework, AI model serving<br />
<br />
Summary: <br />
The paper introduces AsyncSpade, an asynchronous framework for efficient Test-time scaling (TTS) in Large Language Models (LLM). It addresses the issue of memory-bound bottleneck during LLM decoding by predicting the next-token query state using a light-weight temporal-regressive module. AsyncSpade also employs an asynchronous and disaggregated framework to overlap token-level KV selection with forward inference computation, eliminating sequential dependence without compromising model performance. This approach leads to a significant reduction in time-per-output-token (TPOT), outperforming the state-of-the-art baseline (Quest) by over 20% and full attention models by at least 50% on various TTS benchmarks such as AIME, GPQA-Diamond, and MATH-500. AsyncSpade achieves optimal efficiency on common LLM serving setups, showcasing improved serving efficiency and model performance under high concurrency and long-chain-of-thought scenarios. <div>
arXiv:2510.07486v1 Announce Type: new 
Abstract: Test-time scaling (TTS) boosts LLM reasoning via long chain-of-thought (CoT), but the linear KV-cache growth amplifies the memory-bound bottleneck of LLM decoding. Query-aware page-level sparse decoding can achieve state-of-the-art performance under constrained FLOPs budgets, but is limited by both sequential-dependent page filtering and coarse-grained token selection, hampering serving efficiency and model performance on TTS tasks under high concurrency and long CoT scenarios (consuming even higher runtime than the forward pipeline itself). In this paper, we first find that the current-step query state can be accurately approximated in a unified manner from a short window of recent queries, enabling training-free query-aware sparsity without waiting in the decoding loop. We propose AsyncSpade, an asynchronous framework for efficient TTS built on two core components: (1) a novel light-weight temporal-regressive module that predicts the next-token query state; (2) an asynchronous and disaggregated framework that decouples the KV cache filtering from the auto-regressive decoding loop, overlapping the token-level KV selection with the forward inference computation through asynchronism. To our knowledge, AsyncSpade is the first to eliminate the sequential dependence without sacrificing model performance. We validate the effectiveness of AsyncSpade on common LLM serving setups with an A100 node, where AsyncSpade fully overlaps KV-cache operations with the inference pipeline, achieving theoretical optimal time-per-output-token (TPOT). Specifically, AsyncSpade delivers over 20% reduction on TPOT compared to SoTA baseline (i.e. Quest) and at least 50% TPOT reduction compared to full attention on Qwen3-8B and Qwen3-32B models, while matching or surpassing their accuracy on various TTS benchmarks (AIME-24/25, GPQA-Diamond, MATH-500).
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Lessons From Human Teams Be Applied to Multi-Agent Systems? The Role of Structure, Diversity, and Interaction Dynamics</title>
<link>https://arxiv.org/abs/2510.07488</link>
<guid>https://arxiv.org/abs/2510.07488</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-Agent Systems, Large Language Model, team dynamics, team performance, diversity <br />
Summary: 
A new study explores Multi-Agent Systems with Large Language Model-powered agents, focusing on team dynamics in team science. The research examines structure, diversity, and interaction dynamics within teams across tasks like CommonsenseQA and StrategyQA. Results indicate that flat teams outperform hierarchical ones, and diversity has a varied impact. Interviews reveal agent overconfidence in team performance, while post-task reflections highlight appreciation for collaboration and integration challenges such as conversational coordination limitations. Overall, the study sheds light on the interactions between agents within MAS, emphasizing the importance of team dynamics and diversity for optimal team performance. <br /><br />Summary: <div>
arXiv:2510.07488v1 Announce Type: new 
Abstract: Multi-Agent Systems (MAS) with Large Language Model (LLM)-powered agents are gaining attention, yet fewer studies explore their team dynamics. Inspired by human team science, we propose a multi-agent framework to examine core aspects of team science: structure, diversity, and interaction dynamics. We evaluate team performance across four tasks: CommonsenseQA, StrategyQA, Social IQa, and Latent Implicit Hate, spanning commonsense and social reasoning. Our results show that flat teams tend to perform better than hierarchical ones, while diversity has a nuanced impact. Interviews suggest agents are overconfident about their team performance, yet post-task reflections reveal both appreciation for collaboration and challenges in integration, including limited conversational coordination.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Speech LLMs Think while Listening?</title>
<link>https://arxiv.org/abs/2510.07497</link>
<guid>https://arxiv.org/abs/2510.07497</guid>
<content:encoded><![CDATA[
<div> Keywords: speech LLMs, reasoning tasks, chain-of-thought prompting, latency, Direct Preference Optimization

Summary: 
This study explores the impact of chain-of-thought fine-tuning on multi-stream speech large language models (LLMs) for reasoning tasks. Results show a 2.4x increase in accuracy across various spoken reasoning tasks. To address latency in spoken responses, a method based on "question completeness" is introduced, allowing models to start reasoning before query completion. This method offers better accuracy-latency trade-off control and a 4% accuracy gain on ARC-Easy under equivalent latency conditions. Using Direct Preference Optimization on preference data, a 70% latency reduction without accuracy loss is achieved, pushing the accuracy-latency pareto frontier. These findings demonstrate the effectiveness of CoT fine-tuning and latency reduction strategies in enhancing the performance of speech LLMs for complex reasoning tasks. 

<br /><br />Summary: <div>
arXiv:2510.07497v1 Announce Type: new 
Abstract: Recent advances in speech large language models (speech LLMs) have enabled seamless spoken interactions, but these systems still struggle with complex reasoning tasks. Previously, chain-of-thought (CoT) prompting or fine-tuning has been to shown to significantly improve the reasoning abilities of text-based LLMs. In this work, we investigate the effect of CoT fine-tuning for multi-stream speech LLMs, demonstrating that reasoning in text space improves the accuracy of speech LLMs by 2.4x, on average, over a suite of spoken reasoning tasks. Beyond accuracy, the latency of the spoken response is a crucial factor for interacting with voice-based agents. Inspired by the human behavior of "thinking while listening," we propose methods to reduce the additional latency from reasoning by allowing the model to start reasoning before the user query has ended. To achieve this, we introduce an entropy-based metric, "question completeness," which acts as an indicator to guide the model on the optimal time to start reasoning. This method provides greater control over the accuracy-latency trade-off compared with heuristic-based approaches and, under equivalent latency conditions, yields a 4% accuracy gain on ARC-Easy. Finally, we use Direct Preference Optimization (DPO) on preference data created using rejection sampling to push the accuracy-latency pareto frontier further, resulting in a 70% reduction in latency without loss in accuracy.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs</title>
<link>https://arxiv.org/abs/2510.07499</link>
<guid>https://arxiv.org/abs/2510.07499</guid>
<content:encoded><![CDATA[
<div> Keywords: Long-Context Language Models, multi-hop reasoning, thought templates, natural-language feedback, knowledge-intensive 

Summary: 
The article introduces Thought Template Augmented LCLMs (ToTAL) as a framework to improve multi-hop reasoning in Long-Context Language Models (LCLMs). By using thought templates derived from prior problem-solving traces, ToTAL helps structure how evidence is combined and guides inference with factual documents effectively. An update strategy refines these templates through natural-language feedback, leading to consistent performance gains over strong baselines in both retrieval-based and retrieval-free settings. ToTAL demonstrates its broad applicability and transparent reasoning reuse by distilling optimized templates into smaller open-source models. This approach enhances the integration of large sets of documents in LCLMs, enabling more efficient and accurate multi-hop reasoning processes. <div>
arXiv:2510.07499v1 Announce Type: new 
Abstract: Recent Long-Context Language Models (LCLMs) can process hundreds of thousands of tokens in a single prompt, enabling new opportunities for knowledge-intensive multi-hop reasoning by integrating large sets of retrieved documents or, in some cases, directly all necessary information. However, simply feeding more documents into the context window fails to capture how evidence should be connected. We address this gap with thought templates, which recast reasoning as reusable thought caches, derived from prior problem solving traces, structuring how evidence is combined and guiding multi-hop inference with factual documents. To keep these templates effective, we propose an update strategy that iteratively refines templates derived from training data through natural-language feedback. Across diverse benchmarks and LCLM families, our approach delivers consistent gains over strong baselines in both retrieval-based and retrieval-free settings. Furthermore, we show that optimized templates can be distilled into smaller open-source models, demonstrating its broad applicability and transparent reasoning reuse. We refer to our framework as Thought Template Augmented LCLMs (ToTAL).
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ParsTranslit: Truly Versatile Tajik-Farsi Transliteration</title>
<link>https://arxiv.org/abs/2510.07520</link>
<guid>https://arxiv.org/abs/2510.07520</guid>
<content:encoded><![CDATA[
<div> Keywords: Persian language, digraphic, transliteration, sequence-to-sequence model, benchmark

Summary:
A new study explores the challenges of written communication between Tajikistan and Persian-speaking countries due to script differences. Existing machine transliteration models were limited by domain-specific datasets, hindering real-world usage. The researchers present a state-of-the-art sequence-to-sequence model for Tajik-Farsi transliteration trained on diverse datasets to improve versatility. Results show high accuracy scores in both directions, setting leading benchmarks. The model achieves chrF++ and Normalized CER scores of 87.91 and 0.05 from Farsi to Tajik, and 92.28 and 0.04 from Tajik to Farsi, respectively. The data, code, and model are available for further research and development at a specified link. The study's findings highlight the importance of comprehensive datasets and model training for effective transliteration systems in digraphic languages. 

<br /><br />Summary: <div>
arXiv:2510.07520v1 Announce Type: new 
Abstract: As a digraphic language, the Persian language utilizes two written standards: Perso-Arabic in Afghanistan and Iran, and Tajik-Cyrillic in Tajikistan. Despite the significant similarity between the dialects of each country, script differences prevent simple one-to-one mapping, hindering written communication and interaction between Tajikistan and its Persian-speaking ``siblings''. To overcome this, previously-published efforts have investigated machine transliteration models to convert between the two scripts. Unfortunately, most efforts did not use datasets other than those they created, limiting these models to certain domains of text such as archaic poetry or word lists. A truly usable transliteration system must be capable of handling varied domains, meaning that suck models lack the versatility required for real-world usage. The contrast in domain between data also obscures the task's true difficulty. We present a new state-of-the-art sequence-to-sequence model for Tajik-Farsi transliteration trained across all available datasets, and present two datasets of our own. Our results across domains provide clearer understanding of the task, and set comprehensive comparable leading benchmarks. Overall, our model achieves chrF++ and Normalized CER scores of 87.91 and 0.05 from Farsi to Tajik and 92.28 and 0.04 from Tajik to Farsi. Our model, data, and code are available at https://anonymous.4open.science/r/ParsTranslit-FB30/.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OWL: Overcoming Window Length-Dependence in Speculative Decoding for Long-Context Inputs</title>
<link>https://arxiv.org/abs/2510.07535</link>
<guid>https://arxiv.org/abs/2510.07535</guid>
<content:encoded><![CDATA[
<div> speculative decoding, language models, long-context benchmark, LSTM-based drafter, hybrid algorithm

Summary:
OWL is a novel model designed to address the limitations of existing methods for faster inference in large language models (LLMs) with long contexts. The model achieves higher acceptance length than previous methods by utilizing an LSTM-based drafter conditioned on the last-token state, incorporating a special token [SPEC] for richer representation, and implementing a hybrid algorithm for decoding. Current approaches, such as EAGLE3, perform poorly with long contexts, with EAGLE3 even slowing down the generation speed. In contrast, OWL demonstrates improved performance and generalization capabilities, offering a promising solution for efficient inference in real-world settings. The release of the LongSpecBench benchmark, along with the code and datasets, aims to advance future research in this field. 

<br /><br />Summary: <div>
arXiv:2510.07535v1 Announce Type: new 
Abstract: Speculative decoding promises faster inference for large language models (LLMs), yet existing methods fail to generalize to real-world settings. Benchmarks typically assume short contexts (e.g., 2K tokens), whereas practical workloads involve long contexts. We find current approaches degrade severely with long contexts; for instance, EAGLE3 even slows down the generation speed by 0.81x. We address these limitations by releasing a new long-context benchmark (LongSpecBench) and introducing a novel model (OWL). OWL achieves about 5x higher acceptance length than EAGLE3 on long-context inputs through three innovations: (1) an LSTM-based drafter conditioned only on the last-token state, making it generalize to various lengths, (2) a special token [SPEC] in the verifier that produces richer representation for drafter, and (3) a hybrid algorithm combining both tree and non-tree decoding methods. We release all code and datasets to advance future research.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deploying Tiny LVLM Judges for Real-World Evaluation of Chart Models: Lessons Learned and Best Practices</title>
<link>https://arxiv.org/abs/2510.07545</link>
<guid>https://arxiv.org/abs/2510.07545</guid>
<content:encoded><![CDATA[
<div> LVLMs, chart comprehension, tiny models, multi-criteria prompting, domain-adaptive transfer learning <br />
Summary: <br />
Large Vision-Language Models (LVLMs) have shown promise in chart comprehension tasks, but tiny models with <=2B parameters perform poorly. To address this, two approaches are proposed: multi-criteria prompting and domain-adaptive transfer learning. Multi-criteria prompting combines evaluation criteria into a single query, while domain-adaptive transfer learning fine-tunes a 2B-parameter LVLM on synthetic judgments to create the specialized ChartJudge. Experiments reveal robustness gaps in 7B models with multi-criteria prompting and demonstrate ChartJudge's ability to transfer knowledge between datasets. The analysis highlights trade-offs between model size, prompt design, and transferability, enabling cost-efficient evaluation for chart reasoning tasks. The code and data will be publicly available. <br /> <div>
arXiv:2510.07545v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) with only 7B parameters have shown promise as automated judges in chart comprehension tasks. However, tiny models (<=2B parameters) still perform poorly as judges, limiting their real-world use in resource-constrained settings. To address this, we propose two approaches to ensure cost-efficient evaluation: (i) multi-criteria prompting, which combines separate evaluation criteria into a single query, and (ii) domain-adaptive transfer learning, in which we fine-tune a 2B-parameter LVLM on synthetic judgments in a chart dataset to create the ChartJudge. Experiments show that multi-criteria prompting exposes robustness gaps, which led to a huge drop in performance for 7B models, including specialized LVLM judges like LLaVA-Critic. In addition, we find that our tiny LVLM (ChartJudge) can effectively transfer knowledge from one dataset to another to make it a more specialized model. Our fine-grained analysis across chart types and query complexities offers actionable insights into trade-offs between model size, prompt design, and transferability, enabling scalable, low-cost evaluation for chart reasoning tasks. Our code and the data will be made publicly available.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Task Pre-Finetuning of Lightweight Transformer Encoders for Text Classification and NER</title>
<link>https://arxiv.org/abs/2510.07566</link>
<guid>https://arxiv.org/abs/2510.07566</guid>
<content:encoded><![CDATA[
<div> Keywords: natural language processing, NLP models, mobile platforms, BERT-like encoders, pre-finetuning

Summary: 
This study investigates strategies to improve the adaptability of lightweight BERT-like encoders for named entity recognition (NER) and text classification tasks on mobile platforms. It is found that na\"ive multi-task pre-finetuning can lead to conflicting optimization signals, reducing overall performance. To address this issue, a multi-task pre-finetuning framework using task-primary LoRA modules is proposed, enabling a single shared encoder backbone with modular adapters. Experiments on 21 downstream tasks show average improvements of +0.8% for NER and +8.8% for text classification. The proposed approach allows for enhanced adaptability of NLP models while meeting deployment constraints, making it suitable for versatile mobile NLP applications.

<br /><br />Summary: <div>
arXiv:2510.07566v1 Announce Type: new 
Abstract: Deploying natural language processing (NLP) models on mobile platforms requires models that can adapt across diverse applications while remaining efficient in memory and computation. We investigate pre-finetuning strategies to enhance the adaptability of lightweight BERT-like encoders for two fundamental NLP task families: named entity recognition (NER) and text classification. While pre-finetuning improves downstream performance for each task family individually, we find that na\"ive multi-task pre-finetuning introduces conflicting optimization signals that degrade overall performance. To address this, we propose a simple yet effective multi-task pre-finetuning framework based on task-primary LoRA modules, which enables a single shared encoder backbone with modular adapters. Our approach achieves performance comparable to individual pre-finetuning while meeting practical deployment constraint. Experiments on 21 downstream tasks show average improvements of +0.8% for NER and +8.8% for text classification, demonstrating the effectiveness of our method for versatile mobile NLP applications.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linguistic Patterns in Pandemic-Related Content: A Comparative Analysis of COVID-19, Constraint, and Monkeypox Datasets</title>
<link>https://arxiv.org/abs/2510.07579</link>
<guid>https://arxiv.org/abs/2510.07579</guid>
<content:encoded><![CDATA[
<div> Keywords: pandemic, misinformation, linguistic analysis, readability, persuasive language<br />
<br />
Summary: 
This study examines pandemic-related online discourse to distinguish between health misinformation and factual communication. By analyzing three corpora on COVID-19 false narratives, general COVID-19 content, and Monkeypox-related posts, the study identifies differences in readability, rhetorical markers, and persuasive language use. COVID-19 misinformation shows lower readability scores, higher frequency of fear-related and persuasive terms, and minimal use of exclamation marks compared to other datasets. The deliberate complexity of rhetorical style combined with emotional cues in misinformation may enhance perceived credibility. These findings contribute to detecting digital health misinformation, informing public health messaging strategies, and enhancing crisis communication models in networked media environments. However, the study acknowledges limitations and suggests future research directions to strengthen robustness. <br /><br />Summary: <div>
arXiv:2510.07579v1 Announce Type: new 
Abstract: This study conducts a computational linguistic analysis of pandemic-related online discourse to examine how language distinguishes health misinformation from factual communication. Drawing on three corpora: COVID-19 false narratives (n = 7588), general COVID-19 content (n = 10700), and Monkeypox-related posts (n = 5787), we identify significant differences in readability, rhetorical markers, and persuasive language use. COVID-19 misinformation exhibited markedly lower readability scores and contained over twice the frequency of fear-related or persuasive terms compared to the other datasets. It also showed minimal use of exclamation marks, contrasting with the more emotive style of Monkeypox content. These patterns suggest that misinformation employs a deliberately complex rhetorical style embedded with emotional cues, a combination that may enhance its perceived credibility. Our findings contribute to the growing body of work on digital health misinformation by highlighting linguistic indicators that may aid detection efforts. They also inform public health messaging strategies and theoretical models of crisis communication in networked media environments. At the same time, the study acknowledges limitations, including reliance on traditional readability indices, use of a deliberately narrow persuasive lexicon, and reliance on static aggregate analysis. Future research should therefore incorporate longitudinal designs, broader emotion lexicons, and platform-sensitive approaches to strengthen robustness.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IASC: Interactive Agentic System for ConLangs</title>
<link>https://arxiv.org/abs/2510.07591</link>
<guid>https://arxiv.org/abs/2510.07591</guid>
<content:encoded><![CDATA[
<div> constructed languages, LLMs, phonology, morphosyntactic markup, lexicon

Summary:
The article presents a system that utilizes Large Language Models (LLMs) in the development of constructed languages. The system follows a modular approach, starting with creating a target phonology for the language using feedback mechanisms. It then translates sentences into a morphosyntactic markup reflecting the desired language's specifications and constructs a lexicon based on the translated corpus. The system also generates an orthography for the language and writes a grammatical handbook. The goal of the system is to provide a fun tool for creating artificial languages and to explore LLMs' understanding of linguistic concepts. The capabilities of LLMs vary depending on the linguistic specifications, with common patterns being easier to handle than rarer ones. The system's application in translating from high-resource to low-resource languages shows potential for improvement despite current limitations. The research aims to enhance language creation tools and improve translation tasks between different language resources. 

Summary: <div>
arXiv:2510.07591v1 Announce Type: new 
Abstract: We present a system that uses LLMs as a tool in the development of Constructed Languages. The system is modular in that one first creates a target phonology for the language using an agentic approach that refines its output at each step with commentary feedback on its previous attempt. Next, a set of sentences is 'translated' from their English original into a morphosyntactic markup that reflects the word order and morphosyntactic feature specifications of the desired target language, with affixes represented as morphosyntactic feature bundles. From this translated corpus, a lexicon is constructed using the phonological model and the set of morphemes (stems and affixes) extracted from the 'translated' sentences. The system is then instructed to provide an orthography for the language, using an existing script such as Latin or Cyrillic. Finally, the system writes a brief grammatical handbook of the language. The system can also translate further sentences into the target language.
  Our goal is twofold. First, we hope that these tools will be fun to use for creating artificially constructed languages. Second, we are interested in exploring what LLMs 'know' about language-not what they know about any particular language or linguistic phenomenon, but how much they know about and understand language and linguistic concepts. As we shall see, there is a fairly wide gulf in capabilities both among different LLMs and among different linguistic specifications, with it being notably easier for systems to deal with more common patterns than rarer ones. An additional avenue that we explore is the application of our approach to translating from high-resource into low-resource languages. While the results so far are mostly negative, we provide some evidence that an improved version of the present system could afford some real gains in such tasks.
  https://github.com/SakanaAI/IASC
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vocabulary embeddings organize linguistic structure early in language model training</title>
<link>https://arxiv.org/abs/2510.07613</link>
<guid>https://arxiv.org/abs/2510.07613</guid>
<content:encoded><![CDATA[
<div> Keyword: language models, input embeddings, geometric structure, training, vocabulary representations

Summary:
During training, the geometry of vocabulary embeddings in large language models quickly aligns with semantic and syntactic features, showing a convergence towards linguistic structure. High-frequency and function words reach their final embeddings faster than low-frequency words, retaining some bias from random initialization. Understanding the evolution of vocabulary geometry during training provides insights into how language models acquire specific capabilities and the roles of word frequency and function in shaping input representations. This study sheds light on the dynamic trajectory of vocabulary organization in language models, highlighting the importance of structural alignment with linguistic features for effective model performance.

<br /><br />Summary: <div>
arXiv:2510.07613v1 Announce Type: new 
Abstract: Large language models (LLMs) work by manipulating the geometry of input embedding vectors over multiple layers. Here, we ask: how are the input vocabulary representations of language models structured, and how and when does this structure evolve over training? To answer this question, we use representational similarity analysis, running a suite of experiments that correlate the geometric structure of the input embeddings and output embeddings of two open-source models (Pythia 12B and OLMo 7B) with semantic, syntactic, and frequency-based metrics over the course of training. Our key findings are as follows: 1) During training, the vocabulary embedding geometry quickly converges to high correlations with a suite of semantic and syntactic features; 2) Embeddings of high-frequency and function words (e.g., "the," "of") converge to their final vectors faster than lexical and low-frequency words, which retain some alignment with the bias in their random initializations. These findings help map the dynamic trajectory by which input embeddings organize around linguistic structure, revealing distinct roles for word frequency and function. Our findings motivate a deeper study of how the evolution of vocabulary geometry may facilitate specific capability gains during model training.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Reliable Clinical Coding with Language Models: Verification and Lightweight Adaptation</title>
<link>https://arxiv.org/abs/2510.07629</link>
<guid>https://arxiv.org/abs/2510.07629</guid>
<content:encoded><![CDATA[
<div> clinical coding, LLM, hierarchically close, code verification, outpatient

Summary:
- Accurate clinical coding is crucial for healthcare operations.
- Off-the-shelf LLM models struggle with exact match metrics due to hierarchical misalignments.
- Lightweight interventions like prompt engineering and fine-tuning can enhance accuracy efficiently.
- A new approach, clinical code verification, is introduced to tackle hierarchically close but incorrect code predictions.
- An expert double-annotated benchmark of outpatient clinical notes with ICD-10 codes is released to address data limitations and biases in existing datasets.
<br /><br />Summary: Accurate clinical coding is essential for healthcare, but off-the-shelf LLMs face challenges with hierarchical misalignments. Lightweight interventions like prompt engineering and fine-tuning can enhance accuracy without additional computational costs. A new approach, clinical code verification, is introduced to improve error detection for hierarchically close but incorrect code predictions. To address data limitations and biases, an expert double-annotated benchmark of outpatient clinical notes with ICD-10 codes is released, highlighting the importance of verification in enhancing LLM-based medical coding reliability. <div>
arXiv:2510.07629v1 Announce Type: new 
Abstract: Accurate clinical coding is essential for healthcare documentation, billing, and decision-making. While prior work shows that off-the-shelf LLMs struggle with this task, evaluations based on exact match metrics often overlook errors where predicted codes are hierarchically close but incorrect. Our analysis reveals that such hierarchical misalignments account for a substantial portion of LLM failures. We show that lightweight interventions, including prompt engineering and small-scale fine-tuning, can improve accuracy without the computational overhead of search-based methods. To address hierarchically near-miss errors, we introduce clinical code verification as both a standalone task and a pipeline component. To mitigate the limitations in existing datasets, such as incomplete evidence and inpatient bias in MIMIC, we release an expert double-annotated benchmark of outpatient clinical notes with ICD-10 codes. Our results highlight verification as an effective and reliable step toward improving LLM-based medical coding.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Role-Conditioned Refusals: Evaluating Access Control Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2510.07642</link>
<guid>https://arxiv.org/abs/2510.07642</guid>
<content:encoded><![CDATA[
<div> Keywords: access control, language models, role-conditioned refusals, SQL, permission awareness

Summary:
Access control is essential for secure computing, but large language models often struggle to adhere to role boundaries, leading to unrestricted responses. This study focuses on role-conditioned refusals in language models, particularly their ability to follow access control policies. A new dataset, incorporating realistic PostgreSQL role-based policies, was created and used to evaluate different model designs. Comparisons were made between zero or few-shot prompting, a two-step generator-verifier pipeline, and LoRA fine-tuned models. The results showed that explicit verification through the two-step framework improved refusal precision and reduced false permits, while fine-tuning achieved a better balance between safety and utility. However, longer and more complex policies posed challenges for all systems. The researchers released RBAC-augmented datasets and code for further exploration. 

<br /><br />Summary: Access control is crucial for secure computing, but large language models often blur role boundaries, leading to unrestricted responses. Role-conditioned refusals were studied using a novel dataset with realistic PostgreSQL role-based policies. Comparisons of different model designs showed that explicit verification improved refusal precision, while fine-tuning struck a balance between safety and utility. However, longer and complex policies proved challenging for all systems. The release of RBAC-augmented datasets and code enables further research in this area. <div>
arXiv:2510.07642v1 Announce Type: new 
Abstract: Access control is a cornerstone of secure computing, yet large language models often blur role boundaries by producing unrestricted responses. We study role-conditioned refusals, focusing on the LLM's ability to adhere to access control policies by answering when authorized and refusing when not. To evaluate this behavior, we created a novel dataset that extends the Spider and BIRD text-to-SQL datasets, both of which have been modified with realistic PostgreSQL role-based policies at the table and column levels. We compare three designs: (i) zero or few-shot prompting, (ii) a two-step generator-verifier pipeline that checks SQL against policy, and (iii) LoRA fine-tuned models that learn permission awareness directly. Across multiple model families, explicit verification (the two-step framework) improves refusal precision and lowers false permits. At the same time, fine-tuning achieves a stronger balance between safety and utility (i.e., when considering execution accuracy). Longer and more complex policies consistently reduce the reliability of all systems. We release RBAC-augmented datasets and code.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Banking Done Right: Redefining Retail Banking with Language-Centric AI</title>
<link>https://arxiv.org/abs/2510.07645</link>
<guid>https://arxiv.org/abs/2510.07645</guid>
<content:encoded><![CDATA[
<div> AI, LLM, Ryt Bank, financial transactions, natural language conversation
<br />
Ryt AI is an agentic framework developed for Ryt Bank that allows customers to perform core financial transactions through natural language conversation. It is the first globally regulator-approved deployment where conversational AI serves as the primary banking interface, rather than in advisory roles. Ryt AI utilizes ILMU, a closed-source LLM, and four LLM-powered agents to streamline banking processes into a single dialogue. The framework includes LoRA adapters for task-specific functions and is hosted within the bank's infrastructure for consistency. Security measures like deterministic guardrails, human-in-the-loop confirmation, and stateless audit architecture ensure protection and compliance. This innovative approach showcases a successful integration of natural language interfaces to support essential financial activities while upholding strict regulation.
<br /><br />Summary: <div>
arXiv:2510.07645v1 Announce Type: new 
Abstract: This paper presents Ryt AI, an LLM-native agentic framework that powers Ryt Bank to enable customers to execute core financial transactions through natural language conversation. This represents the first global regulator-approved deployment worldwide where conversational AI functions as the primary banking interface, in contrast to prior assistants that have been limited to advisory or support roles. Built entirely in-house, Ryt AI is powered by ILMU, a closed-source LLM developed internally, and replaces rigid multi-screen workflows with a single dialogue orchestrated by four LLM-powered agents (Guardrails, Intent, Payment, and FAQ). Each agent attaches a task-specific LoRA adapter to ILMU, which is hosted within the bank's infrastructure to ensure consistent behavior with minimal overhead. Deterministic guardrails, human-in-the-loop confirmation, and a stateless audit architecture provide defense-in-depth for security and compliance. The result is Banking Done Right: demonstrating that regulator-approved natural-language interfaces can reliably support core financial operations under strict governance.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OBCache: Optimal Brain KV Cache Pruning for Efficient Long-Context LLM Inference</title>
<link>https://arxiv.org/abs/2510.07651</link>
<guid>https://arxiv.org/abs/2510.07651</guid>
<content:encoded><![CDATA[
<div> eviction, cache, language models, attention, token<br />
<br />
Summary:
The article introduces the Optimal Brain Cache (OBCache) framework for efficiently managing memory overhead in Large Language Models (LLMs) with extended context windows. OBCache formulates cache eviction as a structured pruning problem based on token saliency, quantifying the impact of pruning tokens on attention outputs. This approach considers not only attention weights but also information from value states and attention outputs, enhancing existing eviction strategies with output-aware signals. The framework's closed-form scores, derived for isolated keys, isolated values, and joint key-value pairs, provide a more principled approach to token ranking compared to heuristic methods. Experimental results on LLaMA and Qwen models show that utilizing OBCache's output-aware scores improves long-context accuracy by replacing heuristic scores that estimate token saliency across different query positions.<br /><br />Summary: <div>
arXiv:2510.07651v1 Announce Type: new 
Abstract: Large language models (LLMs) with extended context windows enable powerful downstream applications but impose significant memory overhead, as caching all key-value (KV) states scales linearly with sequence length and batch size. Existing cache eviction methods address this by exploiting attention sparsity, yet they typically rank tokens heuristically using accumulated attention weights without considering their true impact on attention outputs. We propose Optimal Brain Cache (OBCache), a principled framework that formulates cache eviction as a layer-wise structured pruning problem. Building upon the Optimal Brain Damage (OBD) theory, OBCache quantifies token saliency by measuring the perturbation in attention outputs induced by pruning tokens, with closed-form scores derived for isolated keys, isolated values, and joint key-value pairs. Our scores account not only for attention weights but also for information from value states and attention outputs, thereby enhancing existing eviction strategies with output-aware signals. Experiments on LLaMA and Qwen models demonstrate that replacing the heuristic scores in existing works, which estimate token saliency across different query positions, with OBCache's output-aware scores consistently improves long-context accuracy.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Textual Entailment and Token Probability as Bias Evaluation Metrics</title>
<link>https://arxiv.org/abs/2510.07662</link>
<guid>https://arxiv.org/abs/2510.07662</guid>
<content:encoded><![CDATA[
<div> Keywords: social bias, language models, token probability metrics, natural language inference, bias evaluation

Summary: 
This study explores the measurement of social bias in language models using natural language inference (NLI) as an alternative bias metric to token probability (TP) metrics. The research reveals that NLI and TP bias evaluations exhibit significant differences, with low correlation between various NLI metrics and between NLI and TP metrics. NLI metrics are more effective at identifying "underdebiasing" instances, but they are also more susceptible to variations in the wording of counterstereotypical sentences. The study suggests that a combination of TP, NLI, and downstream bias evaluations is necessary for a comprehensive assessment of language models. Additionally, this work cautions that neither TP nor NLI can be considered a superior bias metric in all scenarios, highlighting the importance of utilizing multiple evaluation methods in tandem. <div>
arXiv:2510.07662v1 Announce Type: new 
Abstract: Measurement of social bias in language models is typically by token probability (TP) metrics, which are broadly applicable but have been criticized for their distance from real-world langugage model use cases and harms. In this work, we test natural language inference (NLI) as a more realistic alternative bias metric. We show that, curiously, NLI and TP bias evaluation behave substantially differently, with very low correlation among different NLI metrics and between NLI and TP metrics. We find that NLI metrics are more likely to detect "underdebiased" cases. However, NLI metrics seem to be more brittle and sensitive to wording of counterstereotypical sentences than TP approaches. We conclude that neither token probability nor natural language inference is a "better" bias metric in all cases, and we recommend a combination of TP, NLI, and downstream bias evaluations to ensure comprehensive evaluation of language models.
  Content Warning: This paper contains examples of anti-LGBTQ+ stereotypes.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stress-Testing Model Specs Reveals Character Differences among Language Models</title>
<link>https://arxiv.org/abs/2510.07686</link>
<guid>https://arxiv.org/abs/2510.07686</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, model specifications, value tradeoff scenarios, behavioral disagreement, qualitative analysis

Summary:
The study introduces a methodology for systematically stress-testing model character specifications of large language models (LLMs). It identifies conflicts and ambiguities in current model specs by generating scenarios that force tradeoffs between competing principles. Twelve frontier LLMs from major providers are evaluated, revealing over 70,000 cases of significant behavioral divergence. This divergence predicts underlying issues in model specifications, such as direct contradiction and interpretive ambiguities. The generated dataset showcases misalignment cases and false-positive refusals across all models. Value prioritization patterns and model differences are also provided. The research emphasizes the importance of thorough testing and analysis to address challenges in LLM model specifications, highlighting the need for clearer and more consistent ethical guidelines to guide the development and deployment of AI systems.<br /><br />Summary: <div>
arXiv:2510.07686v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly trained from AI constitutions and model specifications that establish behavioral guidelines and ethical principles. However, these specifications face critical challenges, including internal conflicts between principles and insufficient coverage of nuanced scenarios. We present a systematic methodology for stress-testing model character specifications, automatically identifying numerous cases of principle contradictions and interpretive ambiguities in current model specs.
  We stress test current model specs by generating scenarios that force explicit tradeoffs between competing value-based principles. Using a comprehensive taxonomy we generate diverse value tradeoff scenarios where models must choose between pairs of legitimate principles that cannot be simultaneously satisfied. We evaluate responses from twelve frontier LLMs across major providers (Anthropic, OpenAI, Google, xAI) and measure behavioral disagreement through value classification scores. Among these scenarios, we identify over 70,000 cases exhibiting significant behavioral divergence. Empirically, we show this high divergence in model behavior strongly predicts underlying problems in model specifications. Through qualitative analysis, we provide numerous example issues in current model specs such as direct contradiction and interpretive ambiguities of several principles. Additionally, our generated dataset also reveals both clear misalignment cases and false-positive refusals across all of the frontier models we study. Lastly, we also provide value prioritization patterns and differences of these models.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Meet Virtual Cell: A Survey</title>
<link>https://arxiv.org/abs/2510.07706</link>
<guid>https://arxiv.org/abs/2510.07706</guid>
<content:encoded><![CDATA[
<div> Oracles, Agents, Large language models, Virtual cells, Cellular biology<br />
<br />
Summary:Large language models (LLMs) are revolutionizing the field of cellular biology by enabling the creation of virtual cells that can predict and reason about cellular behavior. This review categorizes LLMs into two paradigms: Oracles for direct cellular modeling and Agents for managing scientific tasks. The core tasks of cellular representation, perturbation prediction, and gene regulation inference are discussed along with the associated models, datasets, and evaluation benchmarks. Critical challenges such as scalability, generalizability, and interpretability are also highlighted. The proposed unified taxonomy provides a comprehensive overview of LLMs for virtual cell modeling, showcasing their potential to advance our understanding of cellular processes and pave the way for new discoveries in biology. <br /><br /> <div>
arXiv:2510.07706v1 Announce Type: new 
Abstract: Large language models (LLMs) are transforming cellular biology by enabling the development of "virtual cells"--computational systems that represent, predict, and reason about cellular states and behaviors. This work provides a comprehensive review of LLMs for virtual cell modeling. We propose a unified taxonomy that organizes existing methods into two paradigms: LLMs as Oracles, for direct cellular modeling, and LLMs as Agents, for orchestrating complex scientific tasks. We identify three core tasks--cellular representation, perturbation prediction, and gene regulation inference--and review their associated models, datasets, evaluation benchmarks, as well as the critical challenges in scalability, generalizability, and interpretability.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causality Guided Representation Learning for Cross-Style Hate Speech Detection</title>
<link>https://arxiv.org/abs/2510.07707</link>
<guid>https://arxiv.org/abs/2510.07707</guid>
<content:encoded><![CDATA[
<div> causal representation learning, hate speech detection, implicit hate, diverse stylistic variations, causal priors
<br />
Summary:
The article discusses the challenges in detecting online hate speech, particularly implicit hate that is conveyed through sarcasm, irony, stereotypes, or coded language. Existing models struggle to generalize across different styles and platforms, leading to spurious correlations between speech and labels. The authors introduce CADET, a causal representation learning framework that disentangles hate speech into key factors such as contextual environment, creator motivation, target, and style. By controlling confounders and allowing for counterfactual reasoning, CADET effectively identifies hate speech in various forms. Through experiments, CADET demonstrates superior performance, showcasing the potential of using causal priors to advance generalizable hate speech detection. 
<br /><br />Summary: <div>
arXiv:2510.07707v1 Announce Type: new 
Abstract: The proliferation of online hate speech poses a significant threat to the harmony of the web. While explicit hate is easily recognized through overt slurs, implicit hate speech is often conveyed through sarcasm, irony, stereotypes, or coded language -- making it harder to detect. Existing hate speech detection models, which predominantly rely on surface-level linguistic cues, fail to generalize effectively across diverse stylistic variations. Moreover, hate speech spread on different platforms often targets distinct groups and adopts unique styles, potentially inducing spurious correlations between them and labels, further challenging current detection approaches. Motivated by these observations, we hypothesize that the generation of hate speech can be modeled as a causal graph involving key factors: contextual environment, creator motivation, target, and style. Guided by this graph, we propose CADET, a causal representation learning framework that disentangles hate speech into interpretable latent factors and then controls confounders, thereby isolating genuine hate intent from superficial linguistic cues. Furthermore, CADET allows counterfactual reasoning by intervening on style within the latent space, naturally guiding the model to robustly identify hate speech in varying forms. CADET demonstrates superior performance in comprehensive experiments, highlighting the potential of causal priors in advancing generalizable hate speech detection.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MemWeaver: A Hierarchical Memory from Textual Interactive Behaviors for Personalized Generation</title>
<link>https://arxiv.org/abs/2510.07713</link>
<guid>https://arxiv.org/abs/2510.07713</guid>
<content:encoded><![CDATA[
<div> deeply personalized generation, user textual history, hierarchical memory, temporal evolution, semantic relationships

Summary:
MemWeaver is a framework that integrates a user's entire textual history into a hierarchical memory to enable deeply personalized generation. It addresses the limitations of existing approaches by capturing both the temporal evolution of interests and the semantic relationships between different activities. The framework consists of two memory components - behavioral memory and cognitive memory - which represent specific user actions and long-term preferences, respectively. This dual-component memory allows large language models to reason over both concrete behaviors and abstracted traits, enhancing the personalization process. Experiments conducted on the Language Model Personalization benchmark demonstrate the effectiveness of MemWeaver in generating personalized content. The code for MemWeaver is publicly available on GitHub for further exploration and use. 

<br /><br />Summary: <div>
arXiv:2510.07713v1 Announce Type: new 
Abstract: The primary form of user-internet engagement is shifting from leveraging implicit feedback signals, such as browsing and clicks, to harnessing the rich explicit feedback provided by textual interactive behaviors. This shift unlocks a rich source of user textual history, presenting a profound opportunity for a deeper form of personalization. However, prevailing approaches offer only a shallow form of personalization, as they treat user history as a flat list of texts for retrieval and fail to model the rich temporal and semantic structures reflecting dynamic nature of user interests. In this work, we propose \textbf{MemWeaver}, a framework that weaves the user's entire textual history into a hierarchical memory to power deeply personalized generation. The core innovation of our memory lies in its ability to capture both the temporal evolution of interests and the semantic relationships between different activities. To achieve this, MemWeaver builds two complementary memory components that both integrate temporal and semantic information, but at different levels of abstraction: behavioral memory, which captures specific user actions, and cognitive memory, which represents long-term preferences. This dual-component memory serves as a unified representation of the user, allowing large language models (LLMs) to reason over both concrete behaviors and abstracted traits. Experiments on the Language Model Personalization (LaMP) benchmark validate the efficacy of MemWeaver. Our code is available\footnote{https://github.com/fishsure/MemWeaver}.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SUBQRAG: sub-question driven dynamic graph rag</title>
<link>https://arxiv.org/abs/2510.07718</link>
<guid>https://arxiv.org/abs/2510.07718</guid>
<content:encoded><![CDATA[
<div> Knowledge Graph, Question Answering, Reasoning, Sub-Question, Graph Memory

Summary:<br />
- The article introduces SubQRAG, a framework that enhances multi-hop question answering by decomposing complex questions into verifiable sub-questions.
- SubQRAG retrieves relevant triples from a knowledge graph and dynamically expands it with new triples from source documents in real time when needed.
- All triples used in the reasoning process are aggregated into a "graph memory" for structured evidence.
- Experiments on multi-hop QA benchmarks show that SubQRAG consistently improves Exact Match scores.
<br />Summary: <div>
arXiv:2510.07718v1 Announce Type: new 
Abstract: Graph Retrieval-Augmented Generation (Graph RAG) effectively builds a knowledge graph (KG) to connect disparate facts across a large document corpus. However, this broad-view approach often lacks the deep structured reasoning needed for complex multi-hop question answering (QA), leading to incomplete evidence and error accumulation. To address these limitations, we propose SubQRAG, a sub-question-driven framework that enhances reasoning depth. SubQRAG decomposes a complex question into an ordered chain of verifiable sub-questions. For each sub-question, it retrieves relevant triples from the graph. When the existing graph is insufficient, the system dynamically expands it by extracting new triples from source documents in real time. All triples used in the reasoning process are aggregated into a "graph memory," forming a structured and traceable evidence path for final answer generation. Experiments on three multi-hop QA benchmarks demonstrate that SubQRAG achieves consistent and significant improvements, especially in Exact Match scores.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual Knowledge Graph Completion via Efficient Multilingual Knowledge Sharing</title>
<link>https://arxiv.org/abs/2510.07736</link>
<guid>https://arxiv.org/abs/2510.07736</guid>
<content:encoded><![CDATA[
<div> framework, multilingual, knowledge, completion, shared <br />
Summary: 
A new framework for Multilingual Knowledge Graph Completion (MKGC) has been proposed in this paper, leveraging the multilingual capabilities of Large Language Models (LLMs) to enhance performance. The framework consists of two components: Knowledge-level Grouped Mixture of Experts (KL-GMoE) and Iterative Entity Reranking (IER). By efficiently modeling shared knowledge and enhancing its utilization, the framework achieves significant improvements in Hits@1, Hits@3, and Hits@10 metrics compared to existing state-of-the-art methods. The study also includes a newly constructed multilingual KG dataset and code for reproducibility, available on GitHub. Experimental analysis highlights the benefits of knowledge sharing in scenarios involving unseen and unbalanced languages, providing insights into the effectiveness of leveraging multilingual capabilities in MKGC tasks. <br /> <div>
arXiv:2510.07736v1 Announce Type: new 
Abstract: Large language models (LLMs) based Multilingual Knowledge Graph Completion (MKGC) aim to predict missing facts by leveraging LLMs' multilingual understanding capabilities, improving the completeness of multilingual knowledge graphs (KGs). However, existing MKGC research underutilizes the multilingual capabilities of LLMs and ignores the shareability of cross-lingual knowledge. In this paper, we propose a novel MKGC framework that leverages multilingual shared knowledge to significantly enhance performance through two components: Knowledge-level Grouped Mixture of Experts (KL-GMoE) and Iterative Entity Reranking (IER). KL-GMoE efficiently models shared knowledge, while IER significantly enhances its utilization. To evaluate our framework, we constructed a mKG dataset containing 5 languages and conducted comprehensive comparative experiments with existing state-of-the-art (SOTA) MKGC method. The experimental results demonstrate that our framework achieves improvements of 5.47%, 3.27%, and 1.01% in the Hits@1, Hits@3, and Hits@10 metrics, respectively, compared with SOTA MKGC method. Further experimental analysis revealed the properties of knowledge sharing in settings of unseen and unbalanced languages. We have released the dataset and code for our work on https://github.com/gaoxiaofei07/KL-GMoE.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToolExpander: Extending the Frontiers of Tool-Using Reinforcement Learning to Weak LLMs</title>
<link>https://arxiv.org/abs/2510.07737</link>
<guid>https://arxiv.org/abs/2510.07737</guid>
<content:encoded><![CDATA[
<div> Dynamic Multi-Round Hard Sampling, high-quality few-shot demonstrations, exponential learning rate decay strategy, Self-Exemplifying Thinking, adjusted clipping coefficients<br />
<br />
Summary:<br />
The paper introduces ToolExpander, a framework designed to enhance the performance of resource-constrained Large Language Models (LLMs) using Group Relative Policy Optimization (GRPO). ToolExpander addresses the challenge of inaccurate responses and mid-training collapse in small-scale LLMs. It incorporates Dynamic Multi-Round Hard Sampling to replace challenging samples with high-quality few-shot demonstrations and an exponential learning rate decay strategy to improve stability. The framework also introduces Self-Exemplifying Thinking, eliminating KL divergence and incorporating adjusted clipping coefficients to encourage LLMs to generate and analyze few-shot examples with a minimal additional reward. Experimental results demonstrate that ToolExpander significantly enhances tool-using capabilities, especially in weaker LLMs, leading to improved training stability and overall performance. <div>
arXiv:2510.07737v1 Announce Type: new 
Abstract: Training Large Language Models (LLMs) with Group Relative Policy Optimization (GRPO) encounters a significant challenge: models often fail to produce accurate responses, particularly in small-scale architectures. This limitation not only diminishes performance improvements and undermines the potential of GRPO but also frequently leads to mid-training collapse, adversely affecting stability and final efficacy. To address these issues, we propose ToolExpander, a novel framework that advances tool-oriented reinforcement learning for resource-constrained LLMs through two key innovations:(1) Dynamic Multi-Round Hard Sampling, which dynamically substitutes challenging samples(those without correct outputs over 10 rollouts) with high-quality few-shot demonstrations during training, coupled with an exponential learning rate decay strategy to mitigate oscillations;(2) Self-Exemplifying Thinking, an enhanced GRPO framework that eliminates KL divergence and incorporates adjusted clipping coefficients, encouraging models to autonomously generate and analyze few-shot examples via a minimal additional reward (0.01).Experimental results demonstrate that ToolExpander significantly enhances tool-using capabilities in LLMs, especially in weaker small-scale models, improving both training stability and overall performance.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenRubrics: Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment</title>
<link>https://arxiv.org/abs/2510.07743</link>
<guid>https://arxiv.org/abs/2510.07743</guid>
<content:encoded><![CDATA[
<div> Rubrics, Reward Modeling, Reinforcement Learning, Human Feedback, Natural Language Criteria <br />
<br />Summary: 
The article introduces OpenRubrics, a large dataset of prompt-rubric pairs for training rubric-based reward models in reinforcement learning from human feedback. The Contrastive Rubric Generation (CRG) method is proposed to create discriminative evaluation signals by contrasting preferred and rejected responses. By enforcing preference-label consistency through rejection sampling, noisy rubrics are removed to improve reliability. The Rubric-RM model outperforms strong baselines by 6.8% on reward-modeling benchmarks, with gains transferring to policy models on various tasks. The study demonstrates that rubrics offer scalable alignment signals that bridge the gap between manual human evaluation and automated reward modeling, establishing a new approach for language model alignment. <br /> <div>
arXiv:2510.07743v1 Announce Type: new 
Abstract: Reward modeling lies at the core of reinforcement learning from human feedback (RLHF), yet most existing reward models rely on scalar or pairwise judgments that fail to capture the multifaceted nature of human preferences. Recent studies have explored rubrics-as-rewards (RaR) that uses structured natural language criteria that capture multiple dimensions of response quality. However, producing rubrics that are both reliable and scalable remains a key challenge. In this work, we introduce OpenRubrics, a diverse, large-scale collection of (prompt, rubric) pairs for training rubric-generation and rubric-based reward models. To elicit discriminative and comprehensive evaluation signals, we introduce Contrastive Rubric Generation (CRG), which derives both hard rules (explicit constraints) and principles (implicit qualities) by contrasting preferred and rejected responses. We further improve reliability by enforcing preference-label consistency via rejection sampling to remove noisy rubrics. Across multiple reward-modeling benchmarks, our rubric-based reward model, Rubric-RM, surpasses strong size-matched baselines by 6.8%. These gains transfer to policy models on instruction-following and biomedical benchmarks. Our results show that rubrics provide scalable alignment signals that narrow the gap between costly human evaluation and automated reward modeling, enabling a new principle-driven paradigm for LLM alignment.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parallel Test-Time Scaling for Latent Reasoning Models</title>
<link>https://arxiv.org/abs/2510.07745</link>
<guid>https://arxiv.org/abs/2510.07745</guid>
<content:encoded><![CDATA[
<div> keywords: Parallel TTS, Latent Reasoning, Monte Carlo Dropout, Additive Gaussian Noise, Latent Reward Model <br />
Summary: <br />
Parallel test-time scaling (TTS) is crucial for enhancing large language models (LLMs) by sampling multiple token-based chains-of-thought in parallel and aggregating outcomes. This study focuses on applying parallel TTS to latent reasoning models, addressing challenges such as sampling in continuous space and trajectory aggregation. The authors introduce two stochastic sampling strategies, Monte Carlo Dropout and Additive Gaussian Noise, which scale effectively with compute and exhibit distinct exploration dynamics. Additionally, they propose the Latent Reward Model (LatentRM), trained with a step-wise contrastive objective, to score and guide latent reasoning trajectories. Experimental results and visualization analyses demonstrate the effectiveness of the sampling strategies and the LatentRM for enhancing latent reasoning models. By enabling scalable inference in continuous spaces, this work opens up new directions for advancing latent reasoning models. <div>
arXiv:2510.07745v1 Announce Type: new 
Abstract: Parallel test-time scaling (TTS) is a pivotal approach for enhancing large language models (LLMs), typically by sampling multiple token-based chains-of-thought in parallel and aggregating outcomes through voting or search. Recent advances in latent reasoning, where intermediate reasoning unfolds in continuous vector spaces, offer a more efficient alternative to explicit Chain-of-Thought, yet whether such latent models can similarly benefit from parallel TTS remains open, mainly due to the absence of sampling mechanisms in continuous space, and the lack of probabilistic signals for advanced trajectory aggregation. \
This work enables parallel TTS for latent reasoning models by addressing the above issues. For sampling, we introduce two uncertainty-inspired stochastic strategies: Monte Carlo Dropout and Additive Gaussian Noise. For aggregation, we design a Latent Reward Model (LatentRM) trained with step-wise contrastive objective to score and guide latent reasoning. Extensive experiments and visualization analyses show that both sampling strategies scale effectively with compute and exhibit distinct exploration dynamics, while LatentRM enables effective trajectory selection. Together, our explorations open a new direction for scalable inference in continuous spaces. Code released at https://github.com/YRYangang/LatentTTS.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test-Time Reasoners Are Strategic Multiple-Choice Test-Takers</title>
<link>https://arxiv.org/abs/2510.07761</link>
<guid>https://arxiv.org/abs/2510.07761</guid>
<content:encoded><![CDATA[
<div> reasoning, language models, multiple-choice question answering, choices-only, strategies

Summary:
Large language models (LLMs) are now able to provide reasoning before answering multiple-choice questions, excelling in this task. Concerns have been raised that LLMs may not be solving MCQs as intended, as some studies have shown success without using the question, known as choices-only. Through analyzing reasoning traces, it was found that LLMs use strategies that are not necessarily shallow in choices-only settings. While test-time reasoning can improve accuracy in both full and choices-only inputs, choices-only success is not greatly affected by the length of reasoning traces. The study also showed that the strategies used by LLMs in the choices-only setting are less problematic, such as inferring missing questions. This challenges the notion that partial-input success is always a flaw and highlights the importance of understanding reasoning traces to differentiate between problematic and less problematic reasoning strategies. 

<br /><br />Summary: <div>
arXiv:2510.07761v1 Announce Type: new 
Abstract: Large language models (LLMs) now give reasoning before answering, excelling in tasks like multiple-choice question answering (MCQA). Yet, a concern is that LLMs do not solve MCQs as intended, as work finds LLMs sans reasoning succeed in MCQA without using the question, i.e., choices-only. Such partial-input success is often deemed problematic, but reasoning traces could reveal if these strategies are truly shallow in choices-only settings. To study these strategies, reasoning LLMs solve MCQs in full and choices-only inputs; test-time reasoning often boosts accuracy on full and in choices-only half the time. While possibly due to shallow shortcuts, choices-only success is barely affected by the length of reasoning traces, and after finding traces pass faithfulness tests, we show they use less problematic strategies like inferring missing questions. In all, we challenge claims that partial-input success is always a flaw, so we discuss how reasoning traces could separate problematic data from less problematic reasoning.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToolLibGen: Scalable Automatic Tool Creation and Aggregation for LLM Reasoning</title>
<link>https://arxiv.org/abs/2510.07768</link>
<guid>https://arxiv.org/abs/2510.07768</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, tool augmentation, structured tool library, scalability, reasoning performance

Summary:
Large Language Models (LLMs) equipped with external tools have shown improved performance on complex reasoning tasks. However, the lack of domain-specific tools poses a challenge to widespread adoption of this tool-augmented reasoning approach, especially in fields like physics question answering. Automating tool creation by extracting reusable functions from Chain-of-Thought (CoT) reasoning traces is a promising solution, but scalability issues arise as the number of generated tools grows. To address this, a systematic approach is proposed to automatically refactor an unstructured collection of tools into a structured tool library. This system generates discrete, task-specific tools, clusters them into semantically coherent topics, and uses a multi-agent framework to consolidate functionalities and create versatile, aggregated tools. Experimental results demonstrate improved tool retrieval accuracy, reasoning performance, and scalability compared to baseline methods. <div>
arXiv:2510.07768v1 Announce Type: new 
Abstract: Large Language Models (LLMs) equipped with external tools have demonstrated enhanced performance on complex reasoning tasks. The widespread adoption of this tool-augmented reasoning is hindered by the scarcity of domain-specific tools. For instance, in domains such as physics question answering, suitable and specialized tools are often missing. Recent work has explored automating tool creation by extracting reusable functions from Chain-of-Thought (CoT) reasoning traces; however, these approaches face a critical scalability bottleneck. As the number of generated tools grows, storing them in an unstructured collection leads to significant retrieval challenges, including an expanding search space and ambiguity between function-related tools. To address this, we propose a systematic approach to automatically refactor an unstructured collection of tools into a structured tool library. Our system first generates discrete, task-specific tools and clusters them into semantically coherent topics. Within each cluster, we introduce a multi-agent framework to consolidate scattered functionalities: a code agent refactors code to extract shared logic and creates versatile, aggregated tools, while a reviewing agent ensures that these aggregated tools maintain the complete functional capabilities of the original set. This process transforms numerous question-specific tools into a smaller set of powerful, aggregated tools without loss of functionality. Experimental results demonstrate that our approach significantly improves tool retrieval accuracy and overall reasoning performance across multiple reasoning tasks. Furthermore, our method shows enhanced scalability compared with baselines as the number of question-specific increases.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curing Miracle Steps in LLM Mathematical Reasoning with Rubric Rewards</title>
<link>https://arxiv.org/abs/2510.07774</link>
<guid>https://arxiv.org/abs/2510.07774</guid>
<content:encoded><![CDATA[
<div> reward, mathematical reasoning, model, Miracle Steps, reinforcement learning

Summary:
In this study, large language models for mathematical reasoning were found to be prone to reward hacking, leading to an overestimation of their reasoning abilities. The researchers identified a high incidence of false positives, particularly the occurrence of Miracle Steps where models provide correct answers without valid reasoning. This issue was linked to memorization rather than deduction. To address this, a Rubric Reward Model (RRM) was introduced, which evaluates the entire reasoning process against problem-specific rubrics. By integrating RRM into a reinforcement learning pipeline, the researchers saw improved performance across four math benchmarks, with a significant reduction in Miracle Steps. The study highlights the importance of rewarding the solution process in training mathematical reasoning models for improved accuracy and reliability. 

<br /><br />Summary: <div>
arXiv:2510.07774v1 Announce Type: new 
Abstract: Large language models for mathematical reasoning are typically trained with outcome-based rewards, which credit only the final answer. In our experiments, we observe that this paradigm is highly susceptible to reward hacking, leading to a substantial overestimation of a model's reasoning ability. This is evidenced by a high incidence of false positives - solutions that reach the correct final answer through an unsound reasoning process. Through a systematic analysis with human verification, we establish a taxonomy of these failure modes, identifying patterns like Miracle Steps - abrupt jumps to a correct output without a valid preceding derivation. Probing experiments suggest a strong association between these Miracle Steps and memorization, where the model appears to recall the answer directly rather than deriving it. To mitigate this systemic issue, we introduce the Rubric Reward Model (RRM), a process-oriented reward function that evaluates the entire reasoning trajectory against problem-specific rubrics. The generative RRM provides fine-grained, calibrated rewards (0-1) that explicitly penalize logical flaws and encourage rigorous deduction. When integrated into a reinforcement learning pipeline, RRM-based training consistently outperforms outcome-only supervision across four math benchmarks. Notably, it boosts Verified Pass@1024 on AIME2024 from 26.7% to 62.6% and reduces the incidence of Miracle Steps by 71%. Our work demonstrates that rewarding the solution process is crucial for building models that are not only more accurate but also more reliable.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Unintended Trade-off of AI Alignment:Balancing Hallucination Mitigation and Safety in LLMs</title>
<link>https://arxiv.org/abs/2510.07775</link>
<guid>https://arxiv.org/abs/2510.07775</guid>
<content:encoded><![CDATA[
<div> disentanglement, refusal behavior, safety alignment, hallucination, factual accuracy
Summary:
The article discusses the trade-off between truthfulness and safety in large language models (LLMs) concerning hallucination. While progress has been made in detecting and mitigating hallucination to improve factual accuracy, this can inadvertently weaken refusal behavior, crucial for safety alignment. The analysis reveals that overlapping components in the models encoding both hallucination and refusal information lead to unintended suppression of factual knowledge. Even fine-tuning on curated datasets can degrade safety alignment. To address this, a method using sparse autoencoders to disentangle refusal-related features from hallucination features and preserve refusal behavior during fine-tuning is proposed. Evaluation on commonsense reasoning tasks and harmful benchmarks demonstrates that this approach successfully maintains safety alignment and task utility, mitigating the trade-off between truthfulness and safety.<br /><br />Summary: <div>
arXiv:2510.07775v1 Announce Type: new 
Abstract: Hallucination in large language models (LLMs) has been widely studied in recent years, with progress in both detection and mitigation aimed at improving truthfulness. Yet, a critical side effect remains largely overlooked: enhancing truthfulness can negatively impact safety alignment. In this paper, we investigate this trade-off and show that increasing factual accuracy often comes at the cost of weakened refusal behavior. Our analysis reveals that this arises from overlapping components in the model that simultaneously encode hallucination and refusal information, leading alignment methods to suppress factual knowledge unintentionally. We further examine how fine-tuning on benign datasets, even when curated for safety, can degrade alignment for the same reason. To address this, we propose a method that disentangles refusal-related features from hallucination features using sparse autoencoders, and preserves refusal behavior during fine-tuning through subspace orthogonalization. This approach prevents hallucinations from increasing while maintaining safety alignment.We evaluate our method on commonsense reasoning tasks and harmful benchmarks (AdvBench and StrongReject). Results demonstrate that our approach preserves refusal behavior and task utility, mitigating the trade-off between truthfulness and safety.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instance Relation Learning Network with Label Knowledge Propagation for Few-shot Multi-label Intent Detection</title>
<link>https://arxiv.org/abs/2510.07776</link>
<guid>https://arxiv.org/abs/2510.07776</guid>
<content:encoded><![CDATA[
<div> Few-shot Multi-label Intent Detection, Dialogue Systems, End-to-end Learning, Instance Relation Learning Network, Label Knowledge Propagation  
Summary:  
- Few-shot Multi-label Intent Detection (MID) is important for dialogue systems in low-resource domains.  
- Existing methods rely on a two-stage pipeline, leading to error propagation.  
- A new multi-label joint learning method is proposed for few-shot MID in an end-to-end approach.  
- This method incorporates an instance relation learning network with label knowledge propagation to eliminate error propagation.  
- Experiment results show significant improvement in performance compared to strong baselines in 1-shot scenarios.  
<br /><br /> <div>
arXiv:2510.07776v1 Announce Type: new 
Abstract: Few-shot Multi-label Intent Detection (MID) is crucial for dialogue systems, aiming to detect multiple intents of utterances in low-resource dialogue domains. Previous studies focus on a two-stage pipeline. They first learn representations of utterances with multiple labels and then use a threshold-based strategy to identify multi-label results. However, these methods rely on representation classification and ignore instance relations, leading to error propagation. To solve the above issues, we propose a multi-label joint learning method for few-shot MID in an end-to-end manner, which constructs an instance relation learning network with label knowledge propagation to eliminate error propagation. Concretely, we learn the interaction relations between instances with class information to propagate label knowledge between a few labeled (support set) and unlabeled (query set) instances. With label knowledge propagation, the relation strength between instances directly indicates whether two utterances belong to the same intent for multi-label prediction. Besides, a dual relation-enhanced loss is developed to optimize support- and query-level relation strength to improve performance. Experiments show that we outperform strong baselines by an average of 9.54% AUC and 11.19% Macro-F1 in 1-shot scenarios.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Drift No More? Context Equilibria in Multi-Turn LLM Interactions</title>
<link>https://arxiv.org/abs/2510.07777</link>
<guid>https://arxiv.org/abs/2510.07777</guid>
<content:encoded><![CDATA[
<div> framework, context drift, multi-turn interactions, goal-consistent behavior, user simulators <br />
Summary: 
The article discusses the challenge of context drift in multi-turn interactions for Large Language Models (LLMs). Context drift refers to the gradual divergence of a model's outputs from goal-consistent behavior across turns. The study proposes a dynamical framework to interpret context drift, formalizing it as the turn-wise KL divergence between the model's predictions and a reference model. The experiments show that context drift can reach stable equilibria with noise limitations, and simple reminder interventions can reduce divergence as predicted by the theoretical framework. This suggests that context drift in multi-turn interactions can be understood as a controllable equilibrium phenomenon rather than as inevitable decay. The research provides a foundation for studying and mitigating context drift in extended interactions. <br /> <div>
arXiv:2510.07777v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel at single-turn tasks such as instruction following and summarization, yet real-world deployments require sustained multi-turn interactions where user goals and conversational context persist and evolve. A recurring challenge in this setting is context drift: the gradual divergence of a model's outputs from goal-consistent behavior across turns. Unlike single-turn errors, drift unfolds temporally and is poorly captured by static evaluation metrics. In this work, we present a study of context drift in multi-turn interactions and propose a simple dynamical framework to interpret its behavior. We formalize drift as the turn-wise KL divergence between the token-level predictive distributions of the test model and a goal-consistent reference model, and propose a recurrence model that interprets its evolution as a bounded stochastic process with restoring forces and controllable interventions. We instantiate this framework in both synthetic long-horizon rewriting tasks and realistic user-agent simulations such as in $\tau$-Bench, measuring drift for several open-weight LLMs that are used as user simulators. Our experiments consistently reveal stable, noise-limited equilibria rather than runaway degradation, and demonstrate that simple reminder interventions reliably reduce divergence in line with theoretical predictions. Together, these results suggest that multi-turn drift can be understood as a controllable equilibrium phenomenon rather than as inevitable decay, providing a foundation for studying and mitigating context drift in extended interactions.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RCPU: Rotation-Constrained Error Compensation for Structured Pruning of a Large Language Model</title>
<link>https://arxiv.org/abs/2510.07782</link>
<guid>https://arxiv.org/abs/2510.07782</guid>
<content:encoded><![CDATA[
<div> Keywords: rotation-constrained compensation, structured pruning, large language models, least-squares fitting, variance-aware importance score<br />
Summary:<br />
The paper introduces a rotation-constrained compensation method to address errors in structured pruning of large language models (LLMs). LLMs trained on vast datasets contain rich semantic knowledge, while pruning is done with limited calibration data, leading to output mismatches. The proposed method updates pruned parameters under a rotation constraint to maintain output representation geometry and realign the pruned subspace with original outputs. A variance-aware importance score prioritizes retaining dimensions with large variance to preserve important components in the pruned model. Experiments on LLaMA-7B and benchmark datasets show improved perplexity and task accuracy compared to existing methods. <div>
arXiv:2510.07782v1 Announce Type: new 
Abstract: In this paper, we propose a rotation-constrained compensation method to address the errors introduced by structured pruning of large language models (LLMs). LLMs are trained on massive datasets and accumulate rich semantic knowledge in their representation space. In contrast, pruning is typically carried out with only a small amount of calibration data, which makes output mismatches unavoidable. Although direct least-squares fitting can reduce such errors, it tends to overfit to the limited calibration set, destructively modifying pretrained weights. To overcome this difficulty, we update the pruned parameters under a rotation constraint. This constrained update preserves the geometry of output representations (i.e., norms and inner products) and simultaneously re-aligns the pruned subspace with the original outputs. Furthermore, in rotation-constrained compensation, removing components that strongly contribute to the principal directions of the output makes error recovery difficult. Since input dimensions with large variance strongly affect these principal directions, we design a variance-aware importance score that ensures such dimensions are preferentially kept in the pruned model. By combining this scoring rule with rotation-constrained updates, the proposed method effectively compensates errors while retaining the components likely to be more important in a geometry-preserving manner. In the experiments, we apply the proposed method to LLaMA-7B and evaluate it on WikiText-2 and multiple language understanding benchmarks. The results demonstrate consistently better perplexity and task accuracy compared with existing baselines.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM4Cell: A Survey of Large Language and Agentic Models for Single-Cell Biology</title>
<link>https://arxiv.org/abs/2510.07793</link>
<guid>https://arxiv.org/abs/2510.07793</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, single-cell biology, multimodal data integration, agentic frameworks, model evaluation

Summary: 
The article introduces the concept of Large Language Models (LLMs) and agentic frameworks in revolutionizing single-cell biology through natural-language reasoning and multimodal data integration. It presents a comprehensive survey of 58 models developed for single-cell research, categorizing them into different families based on data modalities and analytical tasks. The models are evaluated across various dimensions including biological grounding, fairness, privacy, and explainability using over 40 public datasets. The study highlights challenges in interpretability, standardization, and trustworthy model development in the field of language-driven single-cell intelligence. This unified view provided by LLM4Cell aims to address fragmented progress and guide future research in the domain of single-cell biology. 

<br /><br />Summary: <div>
arXiv:2510.07793v1 Announce Type: new 
Abstract: Large language models (LLMs) and emerging agentic frameworks are beginning to transform single-cell biology by enabling natural-language reasoning, generative annotation, and multimodal data integration. However, progress remains fragmented across data modalities, architectures, and evaluation standards. LLM4Cell presents the first unified survey of 58 foundation and agentic models developed for single-cell research, spanning RNA, ATAC, multi-omic, and spatial modalities. We categorize these methods into five families-foundation, text-bridge, spatial, multimodal, epigenomic, and agentic-and map them to eight key analytical tasks including annotation, trajectory and perturbation modeling, and drug-response prediction. Drawing on over 40 public datasets, we analyze benchmark suitability, data diversity, and ethical or scalability constraints, and evaluate models across 10 domain dimensions covering biological grounding, multi-omics alignment, fairness, privacy, and explainability. By linking datasets, models, and evaluation domains, LLM4Cell provides the first integrated view of language-driven single-cell intelligence and outlines open challenges in interpretability, standardization, and trustworthy model development.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiPRAG: Hierarchical Process Rewards for Efficient Agentic Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2510.07794</link>
<guid>https://arxiv.org/abs/2510.07794</guid>
<content:encoded><![CDATA[
<div> Keywords: Agentic RAG, Hierarchical Process Rewards, RL training, search efficiency, reasoning process 

Summary:
Hierarchical Process Rewards for Efficient agentic RAG (HiPRAG) introduces a fine-grained, knowledge-grounded process reward into RL training to address suboptimal search behaviors in LLMs. By evaluating the necessity of each search decision in real-time and applying a hierarchical reward function, HiPRAG improves search efficiency and reduces over-search and under-search rates. Experimental results on diverse QA benchmarks show increased accuracies while optimizing the reasoning process itself. The method demonstrates good generalizability across various RL algorithms, model families, sizes, and types. This work highlights the significance of fine-grained control in RL for enhancing the efficiency and optimality of reasoning for search agents. 

Summary: <br /><br /> <div>
arXiv:2510.07794v1 Announce Type: new 
Abstract: Agentic RAG is a powerful technique for incorporating external information that LLMs lack, enabling better problem solving and question answering. However, suboptimal search behaviors exist widely, such as over-search (retrieving information already known) and under-search (failing to search when necessary), which leads to unnecessary overhead and unreliable outputs. Current training methods, which typically rely on outcome-based rewards in a RL framework, lack the fine-grained control needed to address these inefficiencies. To overcome this, we introduce Hierarchical Process Rewards for Efficient agentic RAG (HiPRAG), a training methodology that incorporates a fine-grained, knowledge-grounded process reward into the RL training. Our approach evaluates the necessity of each search decision on-the-fly by decomposing the agent's reasoning trajectory into discrete, parsable steps. We then apply a hierarchical reward function that provides an additional bonus based on the proportion of optimal search and non-search steps, on top of commonly used outcome and format rewards. Experiments on the Qwen2.5 and Llama-3.2 models across seven diverse QA benchmarks show that our method achieves average accuracies of 65.4% (3B) and 67.2% (7B). This is accomplished while improving search efficiency, reducing the over-search rate to just 2.3% and concurrently lowering the under-search rate. These results demonstrate the efficacy of optimizing the reasoning process itself, not just the final outcome. Further experiments and analysis demonstrate that HiPRAG shows good generalizability across a wide range of RL algorithms, model families, sizes, and types. This work demonstrates the importance and potential of fine-grained control through RL, for improving the efficiency and optimality of reasoning for search agents.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Generation of Multi-LLM Agents Communication Topologies with Graph Diffusion Models</title>
<link>https://arxiv.org/abs/2510.07799</link>
<guid>https://arxiv.org/abs/2510.07799</guid>
<content:encoded><![CDATA[
<div> framework, language models, multi-agent systems, communication topology, generative

Summary:
The article introduces a novel framework called Guided Topology Diffusion (GTD) to optimize communication topologies for multi-agent systems driven by large language models. GTD utilizes a lightweight proxy model to predict multi-objective rewards and guide the iterative topology synthesis process. This approach enables real-time, gradient-free optimization towards task-adaptive topologies, addressing the challenges of balancing task performance, communication cost, and robustness. Experimental results demonstrate GTD's ability to generate highly efficient and task-adaptive communication topologies, outperforming existing methods in LLM agent collaboration. <div>
arXiv:2510.07799v1 Announce Type: new 
Abstract: The efficiency of multi-agent systems driven by large language models (LLMs) largely hinges on their communication topology. However, designing an optimal topology is a non-trivial challenge, as it requires balancing competing objectives such as task performance, communication cost, and robustness. Existing frameworks often rely on static or hand-crafted topologies, which inherently fail to adapt to diverse task requirements, leading to either excessive token consumption for simple problems or performance bottlenecks for complex ones. To address this challenge, we introduce a novel generative framework called \textit{Guided Topology Diffusion (GTD)}. Inspired by conditional discrete graph diffusion models, GTD formulates topology synthesis as an iterative construction process. At each step, the generation is steered by a lightweight proxy model that predicts multi-objective rewards (e.g., accuracy, utility, cost), enabling real-time, gradient-free optimization towards task-adaptive topologies. This iterative, guided synthesis process distinguishes GTD from single-step generative frameworks, enabling it to better navigate complex design trade-offs. We validated GTD across multiple benchmarks, and experiments show that this framework can generate highly task-adaptive, sparse, and efficient communication topologies, significantly outperforming existing methods in LLM agent collaboration.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual Generative Retrieval via Cross-lingual Semantic Compression</title>
<link>https://arxiv.org/abs/2510.07812</link>
<guid>https://arxiv.org/abs/2510.07812</guid>
<content:encoded><![CDATA[
<div> shared atoms, cross-lingual identifier misalignment, identifier inflation, multilingual generative retrieval, dynamic multi-step constrained decoding<br />
<br />
Summary: 
Multilingual Generative Retrieval via Cross-lingual Semantic Compression (MGR-CSC) addresses challenges in applying generative retrieval to multilingual scenarios by unifying semantically equivalent multilingual keywords, aligning semantics, and compressing the identifier space. The framework employs a dynamic multi-step constrained decoding strategy during retrieval, enhancing cross-lingual alignment and decoding efficiency. Experimental results on mMarco100k and mNQ320k datasets show that MGR-CSC significantly improves retrieval accuracy by 6.83% and 4.77% respectively, while reducing document identifier length by 74.51% and 78.2%. MGR-CSC demonstrates outstanding performance in multilingual retrieval by overcoming cross-lingual identifier misalignment and identifier inflation issues. <br /><br /> <div>
arXiv:2510.07812v1 Announce Type: new 
Abstract: Generative Information Retrieval is an emerging retrieval paradigm that exhibits remarkable performance in monolingual scenarios.However, applying these methods to multilingual retrieval still encounters two primary challenges, cross-lingual identifier misalignment and identifier inflation. To address these limitations, we propose Multilingual Generative Retrieval via Cross-lingual Semantic Compression (MGR-CSC), a novel framework that unifies semantically equivalent multilingual keywords into shared atoms to align semantics and compresses the identifier space, and we propose a dynamic multi-step constrained decoding strategy during retrieval. MGR-CSC improves cross-lingual alignment by assigning consistent identifiers and enhances decoding efficiency by reducing redundancy. Experiments demonstrate that MGR-CSC achieves outstanding retrieval accuracy, improving by 6.83% on mMarco100k and 4.77% on mNQ320k, while reducing document identifiers length by 74.51% and 78.2%, respectively.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaSwitch: Adaptive Switching Generation for Knowledge Distillation</title>
<link>https://arxiv.org/abs/2510.07842</link>
<guid>https://arxiv.org/abs/2510.07842</guid>
<content:encoded><![CDATA[
<div> Keywords: Small language models, knowledge distillation, AdaSwitch, token level, supervision quality

Summary:
Small language models (SLMs) are essential for applications with strict latency and computational constraints. Existing knowledge distillation methods have trade-offs between supervision quality and training-inference mismatch. To address these issues, the AdaSwitch approach dynamically combines on-policy and off-policy generation at the token level, allowing the student model to explore its own predictions and selectively integrate teacher guidance based on real-time quality assessment. This approach maintains consistency and supervision quality simultaneously. Experimental results on three datasets with two teacher-student model pairs show that AdaSwitch consistently improves accuracy, providing a practical and effective method for distilling SLMs with acceptable additional overhead. <br /><br />Summary: Small language models are crucial for applications with constraints, but achieving high performance is challenging. Existing knowledge distillation methods involve trade-offs, prompting the development of AdaSwitch. This approach combines on-policy and off-policy generation at the token level, allowing the student model to explore predictions and integrate teacher guidance based on real-time assessment. AdaSwitch maintains consistency and supervision quality, improving accuracy in experiments with teacher-student model pairs, offering an effective method for distilling SLMs with acceptable overhead. <div>
arXiv:2510.07842v1 Announce Type: new 
Abstract: Small language models (SLMs) are crucial for applications with strict latency and computational constraints, yet achieving high performance remains challenging. Knowledge distillation (KD) can transfer capabilities from large teacher models, but existing methods involve trade-offs: off-policy distillation provides high-quality supervision but introduces a training-inference mismatch, while on-policy approaches maintain consistency but rely on low-quality student outputs. To address these issues, we propose AdaSwitch, a novel approach that dynamically combines on-policy and off-policy generation at the token level. AdaSwitch allows the student to first explore its own predictions and then selectively integrate teacher guidance based on real-time quality assessment. This approach simultaneously preserves consistency and maintains supervision quality. Experiments on three datasets with two teacher-student LLM pairs demonstrate that AdaSwitch consistently improves accuracy, offering a practical and effective method for distilling SLMs with acceptable additional overhead.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ready to Translate, Not to Represent? Bias and Performance Gaps in Multilingual LLMs Across Language Families and Domains</title>
<link>https://arxiv.org/abs/2510.07877</link>
<guid>https://arxiv.org/abs/2510.07877</guid>
<content:encoded><![CDATA[
<div> framework, dataset, evaluation, fairness, bias, <br />
Summary: 
The article introduces Translation Tangles, a framework and dataset for assessing the quality and fairness of Large Language Models (LLMs) in Machine Translation (MT). LLMs have shown uneven performance across languages and domains, with potential biases from training data raising concerns. Translation Tangles evaluates 24 language pairs across various domains using different metrics. It includes a bias detection pipeline that combines rule-based heuristics, semantic similarity filtering, and LLM validation. Additionally, a bias-annotated dataset based on human evaluations of 1,439 translation-reference pairs is introduced. Overall, Translation Tangles aims to address the challenges in LLM-based MT, particularly in ensuring translation quality and fairness across languages and domains. <div>
arXiv:2510.07877v1 Announce Type: new 
Abstract: The rise of Large Language Models (LLMs) has redefined Machine Translation (MT), enabling context-aware and fluent translations across hundreds of languages and textual domains. Despite their remarkable capabilities, LLMs often exhibit uneven performance across language families and specialized domains. Moreover, recent evidence reveals that these models can encode and amplify different biases present in their training data, posing serious concerns for fairness, especially in low-resource languages. To address these gaps, we introduce Translation Tangles, a unified framework and dataset for evaluating the translation quality and fairness of open-source LLMs. Our approach benchmarks 24 bidirectional language pairs across multiple domains using different metrics. We further propose a hybrid bias detection pipeline that integrates rule-based heuristics, semantic similarity filtering, and LLM-based validation. We also introduce a high-quality, bias-annotated dataset based on human evaluations of 1,439 translation-reference pairs. The code and dataset are accessible on GitHub: https://github.com/faiyazabdullah/TranslationTangles
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do LLMs Really Need 10+ Thoughts for "Find the Time 1000 Days Later"? Towards Structural Understanding of LLM Overthinking</title>
<link>https://arxiv.org/abs/2510.07880</link>
<guid>https://arxiv.org/abs/2510.07880</guid>
<content:encoded><![CDATA[
<div> Analyzer, Long-chain-of-thought reasoning, Overthinking, LLMs, Thought progression
<br />
Summary: This article introduces TRACE, a detailed analyzer of LLMs' thought processes, to investigate the efficiency issue of overthinking. Benchmarking shows long-thinking models are significantly slower without accuracy gains. The study decomposes thought processes into sub-thoughts and identifies Explorer and Late Landing patterns as primary drivers of overthinking. A utility-based definition of overthinking grounded in thought structures is proposed, offering deeper insights and practical guidance for managing overthinking in LLMs.
<br /><br /> <div>
arXiv:2510.07880v1 Announce Type: new 
Abstract: Models employing long chain-of-thought (CoT) reasoning have shown superior performance on complex reasoning tasks. Yet, this capability introduces a critical and often overlooked inefficiency -- overthinking -- models often engage in unnecessarily extensive reasoning even for simple queries, incurring significant computations without accuracy improvements. While prior work has explored solutions to mitigate overthinking, a fundamental gap remains in our understanding of its underlying causes. Most existing analyses are limited to superficial, profiling-based observations, failing to delve into LLMs' inner workings. This study introduces a systematic, fine-grained analyzer of LLMs' thought process to bridge the gap, TRACE. We first benchmark the overthinking issue, confirming that long-thinking models are five to twenty times slower on simple tasks with no substantial gains. We then use TRACE to first decompose the thought process into minimally complete sub-thoughts. Next, by inferring discourse relationships among sub-thoughts, we construct granular thought progression graphs and subsequently identify common thinking patterns for topically similar queries. Our analysis reveals two major patterns for open-weight thinking models -- Explorer and Late Landing. This finding provides evidence that over-verification and over-exploration are the primary drivers of overthinking in LLMs. Grounded in thought structures, we propose a utility-based definition of overthinking, which moves beyond length-based metrics. This revised definition offers a more insightful understanding of LLMs' thought progression, as well as practical guidelines for principled overthinking management.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CS3-Bench: Evaluating and Enhancing Speech-to-Speech LLMs for Mandarin-English Code-Switching</title>
<link>https://arxiv.org/abs/2510.07881</link>
<guid>https://arxiv.org/abs/2510.07881</guid>
<content:encoded><![CDATA[
<div> Benchmark, Speech-to-speech interaction systems, Language alignment, Multimodal large language models, Code-switching

Summary: 
The article introduces the Code-Switching Speech-to-Speech Benchmark (CS3-Bench), highlighting deficiencies in language alignment in existing large language models used in speech-to-speech interaction systems. Through experiments on mainstream models, a relative performance drop of up to 66% in knowledge-intensive question answering and misunderstandings in open-ended conversations were observed. The proposed approach, utilizing the Chain of Recognition (CoR) and Keyword Highlighting (KH) methods, significantly improved language alignment capabilities, leading to a substantial improvement in knowledge accuracy from 25.14% to 46.13%. The open-ended understanding rate also increased from 64.5% to 86.5%, with a reduction in pronunciation errors in the secondary language. The CS3-Bench dataset is available for further research and development in this field. 

<br /><br />Summary: <div>
arXiv:2510.07881v1 Announce Type: new 
Abstract: The advancement of multimodal large language models has accelerated the development of speech-to-speech interaction systems. While natural monolingual interaction has been achieved, we find existing models exhibit deficiencies in language alignment. In our proposed Code-Switching Speech-to-Speech Benchmark (CS3-Bench), experiments on 7 mainstream models demonstrate a relative performance drop of up to 66% in knowledge-intensive question answering and varying degrees of misunderstanding in open-ended conversations. Starting from a model with severe performance deterioration, we propose both data constructions and training approaches to improve the language alignment capabilities, specifically employing Chain of Recognition (CoR) to enhance understanding and Keyword Highlighting (KH) to guide generation. Our approach improves the knowledge accuracy from 25.14% to 46.13%, with open-ended understanding rate from 64.5% to 86.5%, and significantly reduces pronunciation errors in the secondary language. CS3-Bench is available at https://huggingface.co/datasets/VocalNet/CS3-Bench.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Weak-to-strong Generalization</title>
<link>https://arxiv.org/abs/2510.07884</link>
<guid>https://arxiv.org/abs/2510.07884</guid>
<content:encoded><![CDATA[
<div> Generalization, Language Models, Contrastive Decoding, Implicit Rewards, Capability Transfer, Denoising

Summary:
- The article introduces the concept of Weak-to-Strong generalization for scaling large Language Models (LLMs).
- The robustness and generalization of Weak-to-Strong generalization are limited by noise and biases in weak-model outputs.
- Implicit rewards, approximating explicit rewards through log-likelihood ratios, are utilized to improve Weak-to-Strong generalization.
- Contrastive Weak-to-Strong Generalization (ConG) framework is proposed, which employs contrastive decoding between pre- and post-alignment weak models to generate higher-quality samples.
- ConG shows consistent improvements in capability transfer, denoising, and overall robustness of Weak-to-Strong methods.
<br /><br />Summary: <div>
arXiv:2510.07884v1 Announce Type: new 
Abstract: Weak-to-strong generalization provides a promising paradigm for scaling large language models (LLMs) by training stronger models on samples from aligned weaker ones, without requiring human feedback or explicit reward modeling. However, its robustness and generalization are hindered by the noise and biases in weak-model outputs, which limit its applicability in practice. To address this challenge, we leverage implicit rewards, which approximate explicit rewards through log-likelihood ratios, and reveal their structural equivalence with Contrastive Decoding (CD), a decoding strategy shown to reduce noise in LLM generation. Building on this connection, we propose Contrastive Weak-to-Strong Generalization (ConG), a framework that employs contrastive decoding between pre- and post-alignment weak models to generate higher-quality samples. This approach enables more reliable capability transfer, denoising, and improved robustness, substantially mitigating the limitations of traditional weak-to-strong methods. Empirical results across different model families confirm consistent improvements, demonstrating the generality and effectiveness of ConG. Taken together, our findings highlight the potential of ConG to advance weak-to-strong generalization and provide a promising pathway toward AGI.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Standard-to-Dialect Transfer Trends Differ across Text and Speech: A Case Study on Intent and Topic Classification in German Dialects</title>
<link>https://arxiv.org/abs/2510.07890</link>
<guid>https://arxiv.org/abs/2510.07890</guid>
<content:encoded><![CDATA[
<div> transfer, dialects, speech models, intent classification, German 

Summary: 
The study explores cross-dialectal transfer from standard to non-standard German dialects using text and speech data. Three settings are compared: text models, speech models, and cascaded systems combining speech and text processing. A new dialectal audio intent classification dataset for German is introduced. Results show that speech-only setup performs best on dialect data, while the text-only setup is more effective for standard German. Cascaded systems yield promising results on dialectal data when utilizing normalized transcriptions. The study highlights the significance of considering spoken dialects in natural language processing tasks and suggests the use of appropriate systems for optimal performance in transfer learning scenarios. <div>
arXiv:2510.07890v1 Announce Type: new 
Abstract: Research on cross-dialectal transfer from a standard to a non-standard dialect variety has typically focused on text data. However, dialects are primarily spoken, and non-standard spellings are known to cause issues in text processing. We compare standard-to-dialect transfer in three settings: text models, speech models, and cascaded systems where speech first gets automatically transcribed and then further processed by a text model. In our experiments, we focus on German and multiple German dialects in the context of written and spoken intent and topic classification. To that end, we release the first dialectal audio intent classification dataset. We find that the speech-only setup provides the best results on the dialect data while the text-only setup works best on the standard data. While the cascaded systems lag behind the text-only models for German, they perform relatively well on the dialectal data if the transcription system generates normalized, standard-like output.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Metric Calculating Benchmark: Code-Verifiable Complicate Instruction Following Benchmark for Large Language Models</title>
<link>https://arxiv.org/abs/2510.07892</link>
<guid>https://arxiv.org/abs/2510.07892</guid>
<content:encoded><![CDATA[
<div> Benchmark, LLMs, NLP metrics, instruction adherence, evaluation

Summary:
MCBench is a new benchmark designed to evaluate the capabilities of cutting-edge Large Language Models (LLMs) in executing string-matching NLP metrics by following step-by-step instructions. Unlike previous benchmarks that rely on subjective judgments, MCBench offers an objective evaluation method that tests LLMs' accuracy in instruction adherence, numerical computation, and long-range consistency in handling intermediate results. The benchmark includes a parallel reference code for objective evaluation of LLM output, with three evaluative metrics and three benchmark variants to measure detailed instruction understanding. The analyses demonstrate that MCBench is effective in objectively assessing the abilities of modern LLMs.<br /><br />Summary: <div>
arXiv:2510.07892v1 Announce Type: new 
Abstract: Recent frontier-level LLMs have saturated many previously difficult benchmarks, leaving little room for further differentiation. This progress highlights the need for challenging benchmarks that provide objective verification. In this paper, we introduce MCBench, a benchmark designed to evaluate whether LLMs can execute string-matching NLP metrics by strictly following step-by-step instructions. Unlike prior benchmarks that depend on subjective judgments or general reasoning, MCBench offers an objective, deterministic and codeverifiable evaluation. This setup allows us to systematically test whether LLMs can maintain accurate step-by-step execution, including instruction adherence, numerical computation, and long-range consistency in handling intermediate results. To ensure objective evaluation of these abilities, we provide a parallel reference code that can evaluate the accuracy of LLM output. We provide three evaluative metrics and three benchmark variants designed to measure the detailed instruction understanding capability of LLMs. Our analyses show that MCBench serves as an effective and objective tool for evaluating the capabilities of cutting-edge LLMs.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual Recall</title>
<link>https://arxiv.org/abs/2510.07896</link>
<guid>https://arxiv.org/abs/2510.07896</guid>
<content:encoded><![CDATA[
arXiv:2510.07896v1 Announce Type: new 
Abstract: Large Language Models (LLMs) require efficient knowledge editing (KE) to update factual information, yet existing methods exhibit significant performance decay in multi-hop factual recall. This failure is particularly acute when edits involve intermediate implicit subjects within reasoning chains. Through causal analysis, we reveal that this limitation stems from an oversight of how chained knowledge is dynamically represented and utilized at the neuron level. We discover that during multi hop reasoning, implicit subjects function as query neurons, which sequentially activate corresponding value neurons across transformer layers to accumulate information toward the final answer, a dynamic prior KE work has overlooked. Guided by this insight, we propose ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual Recall, a framework that leverages neuron-level attribution to identify and edit these critical query-value (Q-V) pathways. ACE provides a mechanistically grounded solution for multi-hop KE, empirically outperforming state-of-the-art methods by 9.44% on GPT-J and 37.46% on Qwen3-8B. Our analysis further reveals more fine-grained activation patterns in Qwen3 and demonstrates that the semantic interpretability of value neurons is orchestrated by query-driven accumulation. These findings establish a new pathway for advancing KE capabilities based on the principled understanding of internal reasoning mechanisms.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Human-Like Grading: A Unified LLM-Enhanced Framework for Subjective Question Evaluation</title>
<link>https://arxiv.org/abs/2510.07912</link>
<guid>https://arxiv.org/abs/2510.07912</guid>
<content:encoded><![CDATA[
arXiv:2510.07912v1 Announce Type: new 
Abstract: Automatic grading of subjective questions remains a significant challenge in examination assessment due to the diversity in question formats and the open-ended nature of student responses. Existing works primarily focus on a specific type of subjective question and lack the generality to support comprehensive exams that contain diverse question types. In this paper, we propose a unified Large Language Model (LLM)-enhanced auto-grading framework that provides human-like evaluation for all types of subjective questions across various domains. Our framework integrates four complementary modules to holistically evaluate student answers. In addition to a basic text matching module that provides a foundational assessment of content similarity, we leverage the powerful reasoning and generative capabilities of LLMs to: (1) compare key knowledge points extracted from both student and reference answers, (2) generate a pseudo-question from the student answer to assess its relevance to the original question, and (3) simulate human evaluation by identifying content-related and non-content strengths and weaknesses. Extensive experiments on both general-purpose and domain-specific datasets show that our framework consistently outperforms traditional and LLM-based baselines across multiple grading metrics. Moreover, the proposed system has been successfully deployed in real-world training and certification exams at a major e-commerce enterprise.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STEPER: Step-wise Knowledge Distillation for Enhancing Reasoning Ability in Multi-Step Retrieval-Augmented Language Models</title>
<link>https://arxiv.org/abs/2510.07923</link>
<guid>https://arxiv.org/abs/2510.07923</guid>
<content:encoded><![CDATA[
arXiv:2510.07923v1 Announce Type: new 
Abstract: Answering complex real-world questions requires step-by-step retrieval and integration of relevant information to generate well-grounded responses. However, existing knowledge distillation methods overlook the need for different reasoning abilities at different steps, hindering transfer in multi-step retrieval-augmented frameworks. To address this, we propose Stepwise Knowledge Distillation for Enhancing Reasoning Ability in Multi-Step Retrieval-Augmented Language Models (StepER). StepER employs step-wise supervision to align with evolving information and reasoning demands across stages. Additionally, it incorporates difficulty-aware training to progressively optimize learning by prioritizing suitable steps. Our method is adaptable to various multi-step retrieval-augmented language models, including those that use retrieval queries for reasoning paths or decomposed questions. Extensive experiments show that StepER outperforms prior methods on multi-hop QA benchmarks, with an 8B model achieving performance comparable to a 70B teacher model.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comprehensiveness Metrics for Automatic Evaluation of Factual Recall in Text Generation</title>
<link>https://arxiv.org/abs/2510.07926</link>
<guid>https://arxiv.org/abs/2510.07926</guid>
<content:encoded><![CDATA[
arXiv:2510.07926v1 Announce Type: new 
Abstract: Despite demonstrating remarkable performance across a wide range of tasks, large language models (LLMs) have also been found to frequently produce outputs that are incomplete or selectively omit key information. In sensitive domains, such omissions can result in significant harm comparable to that posed by factual inaccuracies, including hallucinations. In this study, we address the challenge of evaluating the comprehensiveness of LLM-generated texts, focusing on the detection of missing information or underrepresented viewpoints. We investigate three automated evaluation strategies: (1) an NLI-based method that decomposes texts into atomic statements and uses natural language inference (NLI) to identify missing links, (2) a Q&amp;A-based approach that extracts question-answer pairs and compares responses across sources, and (3) an end-to-end method that directly identifies missing content using LLMs. Our experiments demonstrate the surprising effectiveness of the simple end-to-end approach compared to more complex methods, though at the cost of reduced robustness, interpretability and result granularity. We further assess the comprehensiveness of responses from several popular open-weight LLMs when answering user queries based on multiple sources.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-Enabled LLMs in Historical Lexicography: Digitising and Enriching Estonian-German Dictionaries from the 17th and 18th Centuries</title>
<link>https://arxiv.org/abs/2510.07931</link>
<guid>https://arxiv.org/abs/2510.07931</guid>
<content:encoded><![CDATA[
arXiv:2510.07931v1 Announce Type: new 
Abstract: This article presents research conducted at the Institute of the Estonian Language between 2022 and 2025 on the application of large language models (LLMs) to the study of 17th and 18th century Estonian dictionaries. The authors address three main areas: enriching historical dictionaries with modern word forms and meanings; using vision-enabled LLMs to perform text recognition on sources printed in Gothic script (Fraktur); and preparing for the creation of a unified, cross-source dataset. Initial experiments with J. Gutslaff's 1648 dictionary indicate that LLMs have significant potential for semi-automatic enrichment of dictionary information. When provided with sufficient context, Claude 3.7 Sonnet accurately provided meanings and modern equivalents for 81% of headword entries. In a text recognition experiment with A. T. Helle's 1732 dictionary, a zero-shot method successfully identified and structured 41% of headword entries into error-free JSON-formatted output. For digitising the Estonian-German dictionary section of A. W. Hupel's 1780 grammar, overlapping tiling of scanned image files is employed, with one LLM being used for text recognition and a second for merging the structured output. These findings demonstrate that even for minor languages LLMs have a significant potential for saving time and financial resources.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A$^2$Search: Ambiguity-Aware Question Answering with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.07958</link>
<guid>https://arxiv.org/abs/2510.07958</guid>
<content:encoded><![CDATA[
arXiv:2510.07958v1 Announce Type: new 
Abstract: Recent advances in Large Language Models (LLMs) and Reinforcement Learning (RL) have led to strong performance in open-domain question answering (QA). However, existing models still struggle with questions that admit multiple valid answers. Standard QA benchmarks, which typically assume a single gold answer, overlook this reality and thus produce inappropriate training signals. Existing attempts to handle ambiguity often rely on costly manual annotation, which is difficult to scale to multi-hop datasets such as HotpotQA and MuSiQue. In this paper, we present A$^2$Search, an annotation-free, end-to-end training framework to recognize and handle ambiguity. At its core is an automated pipeline that detects ambiguous questions and gathers alternative answers via trajectory sampling and evidence verification. The model is then optimized with RL using a carefully designed $\mathrm{AnsF1}$ reward, which naturally accommodates multiple answers. Experiments on eight open-domain QA benchmarks demonstrate that A$^2$Search achieves new state-of-the-art performance. With only a single rollout, A$^2$Search-7B yields an average $\mathrm{AnsF1}@1$ score of $48.4\%$ across four multi-hop benchmarks, outperforming all strong baselines, including the substantially larger ReSearch-32B ($46.2\%$). Extensive analyses further show that A$^2$Search resolves ambiguity and generalizes across benchmarks, highlighting that embracing ambiguity is essential for building more reliable QA systems. Our code, data, and model weights can be found at https://github.com/zfj1998/A2Search
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LightReasoner: Can Small Language Models Teach Large Language Models Reasoning?</title>
<link>https://arxiv.org/abs/2510.07962</link>
<guid>https://arxiv.org/abs/2510.07962</guid>
<content:encoded><![CDATA[
arXiv:2510.07962v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated remarkable progress in reasoning, often through supervised fine-tuning (SFT). However, SFT is resource-intensive, relying on large curated datasets, rejection-sampled demonstrations, and uniform optimization across all tokens, even though only a fraction carry meaningful learning value. In this work, we explore a counterintuitive idea: can smaller language models (SLMs) teach larger language models (LLMs) by revealing high-value reasoning moments that reflect the latter's unique strength? We propose LightReasoner, a novel framework that leverages the behavioral divergence between a stronger expert model (LLM) and a weaker amateur model (SLM). LightReasoner operates in two stages: (1) a sampling stage that pinpoints critical reasoning moments and constructs supervision examples capturing the expert's advantage through expert-amateur contrast, and (2) a fine-tuning stage that aligns the expert model with these distilled examples, amplifying its reasoning strengths. Across seven mathematical benchmarks, LightReasoner improves accuracy by up to 28.1%, while reducing time consumption by 90%, sampled problems by 80%, and tuned token usage by 99%, all without relying on ground-truth labels. By turning weaker SLMs into effective teaching signals, LightReasoner offers a scalable and resource-efficient approach for advancing LLM reasoning. Code is available at: https://github.com/HKUDS/LightReasoner
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Confusion Expression in Large Language Models: Leveraging World Models toward Better Social Reasoning</title>
<link>https://arxiv.org/abs/2510.07974</link>
<guid>https://arxiv.org/abs/2510.07974</guid>
<content:encoded><![CDATA[
arXiv:2510.07974v1 Announce Type: new 
Abstract: While large language models (LLMs) excel in mathematical and code reasoning, we observe they struggle with social reasoning tasks, exhibiting cognitive confusion, logical inconsistencies, and conflation between objective world states and subjective belief states. Through deteiled analysis of DeepSeek-R1's reasoning trajectories, we find that LLMs frequently encounter reasoning impasses and tend to output contradictory terms like "tricky" and "confused" when processing scenarios with multiple participants and timelines, leading to erroneous reasoning or infinite loops. The core issue is their inability to disentangle objective reality from agents' subjective beliefs. To address this, we propose an adaptive world model-enhanced reasoning mechanism that constructs a dynamic textual world model to track entity states and temporal sequences. It dynamically monitors reasoning trajectories for confusion indicators and promptly intervenes by providing clear world state descriptions, helping models navigate through cognitive dilemmas. The mechanism mimics how humans use implicit world models to distinguish between external events and internal beliefs. Evaluations on three social benchmarks demonstrate significant improvements in accuracy (e.g., +10% in Hi-ToM) while reducing computational costs (up to 33.8% token reduction), offering a simple yet effective solution for deploying LLMs in social contexts.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Author-Specific Context for Scientific Figure Caption Generation: 3rd SciCap Challenge</title>
<link>https://arxiv.org/abs/2510.07993</link>
<guid>https://arxiv.org/abs/2510.07993</guid>
<content:encoded><![CDATA[
arXiv:2510.07993v1 Announce Type: new 
Abstract: Scientific figure captions require both accuracy and stylistic consistency to convey visual information. Here, we present a domain-specific caption generation system for the 3rd SciCap Challenge that integrates figure-related textual context with author-specific writing styles using the LaMP-Cap dataset. Our approach uses a two-stage pipeline: Stage 1 combines context filtering, category-specific prompt optimization via DSPy's MIPROv2 and SIMBA, and caption candidate selection; Stage 2 applies few-shot prompting with profile figures for stylistic refinement. Our experiments demonstrate that category-specific prompts outperform both zero-shot and general optimized approaches, improving ROUGE-1 recall by +8.3\% while limiting precision loss to -2.8\% and BLEU-4 reduction to -10.9\%. Profile-informed stylistic refinement yields 40--48\% gains in BLEU scores and 25--27\% in ROUGE. Overall, our system demonstrates that combining contextual understanding with author-specific stylistic adaptation can generate captions that are both scientifically accurate and stylistically faithful to the source paper.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon Tasks</title>
<link>https://arxiv.org/abs/2510.08002</link>
<guid>https://arxiv.org/abs/2510.08002</guid>
<content:encoded><![CDATA[
arXiv:2510.08002v1 Announce Type: new 
Abstract: Large Language Models have demonstrated remarkable capabilities across diverse domains, yet significant challenges persist when deploying them as AI agents for real-world long-horizon tasks. Existing LLM agents suffer from a critical limitation: they are test-time static and cannot learn from experience, lacking the ability to accumulate knowledge and continuously improve on the job. To address this challenge, we propose MUSE, a novel agent framework that introduces an experience-driven, self-evolving system centered around a hierarchical Memory Module. MUSE organizes diverse levels of experience and leverages them to plan and execute long-horizon tasks across multiple applications. After each sub-task execution, the agent autonomously reflects on its trajectory, converting the raw trajectory into structured experience and integrating it back into the Memory Module. This mechanism enables the agent to evolve beyond its static pretrained parameters, fostering continuous learning and self-evolution. We evaluate MUSE on the long-horizon productivity benchmark TAC. It achieves new SOTA performance by a significant margin using only a lightweight Gemini-2.5 Flash model. Sufficient Experiments demonstrate that as the agent autonomously accumulates experience, it exhibits increasingly superior task completion capabilities, as well as robust continuous learning and self-evolution capabilities. Moreover, the accumulated experience from MUSE exhibits strong generalization properties, enabling zero-shot improvement on new tasks. MUSE establishes a new paradigm for AI agents capable of real-world productivity task automation.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChatGPT as a Translation Engine: A Case Study on Japanese-English</title>
<link>https://arxiv.org/abs/2510.08042</link>
<guid>https://arxiv.org/abs/2510.08042</guid>
<content:encoded><![CDATA[
arXiv:2510.08042v1 Announce Type: new 
Abstract: This study investigates ChatGPT for Japanese-English translation, exploring simple and enhanced prompts and comparing against commercially available translation engines. Performing both automatic and MQM-based human evaluations, we found that document-level translation outperforms sentence-level translation for ChatGPT. On the other hand, we were not able to determine if enhanced prompts performed better than simple prompts in our experiments. We also discovered that ChatGPT-3.5 was preferred by automatic evaluation, but a tradeoff exists between accuracy (ChatGPT-3.5) and fluency (ChatGPT-4). Lastly, ChatGPT yields competitive results against two widely-known translation systems.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Climate Knowledge in Large Language Models</title>
<link>https://arxiv.org/abs/2510.08043</link>
<guid>https://arxiv.org/abs/2510.08043</guid>
<content:encoded><![CDATA[
arXiv:2510.08043v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed for climate-related applications, where understanding internal climatological knowledge is crucial for reliability and misinformation risk assessment. Despite growing adoption, the capacity of LLMs to recall climate normals from parametric knowledge remains largely uncharacterized. We investigate the capacity of contemporary LLMs to recall climate normals without external retrieval, focusing on a prototypical query: mean July 2-m air temperature 1991-2020 at specified locations. We construct a global grid of queries at 1{\deg} resolution land points, providing coordinates and location descriptors, and validate responses against ERA5 reanalysis. Results show that LLMs encode non-trivial climate structure, capturing latitudinal and topographic patterns, with root-mean-square errors of 3-6 {\deg}C and biases of $\pm$1 {\deg}C. However, spatially coherent errors remain, particularly in mountains and high latitudes. Performance degrades sharply above 1500 m, where RMSE reaches 5-13 {\deg}C compared to 2-4 {\deg}C at lower elevations. We find that including geographic context (country, city, region) reduces errors by 27% on average, with larger models being most sensitive to location descriptors. While models capture the global mean magnitude of observed warming between 1950-1974 and 2000-2024, they fail to reproduce spatial patterns of temperature change, which directly relate to assessing climate change. This limitation highlights that while LLMs may capture present-day climate distributions, they struggle to represent the regional and local expression of long-term shifts in temperature essential for understanding climate dynamics. Our evaluation framework provides a reproducible benchmark for quantifying parametric climate knowledge in LLMs and complements existing climate communication assessments.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Process Reward Models: From Outcome Signals to Process Supervisions for Large Language Models</title>
<link>https://arxiv.org/abs/2510.08049</link>
<guid>https://arxiv.org/abs/2510.08049</guid>
<content:encoded><![CDATA[
arXiv:2510.08049v1 Announce Type: new 
Abstract: Although Large Language Models (LLMs) exhibit advanced reasoning ability, conventional alignment remains largely dominated by outcome reward models (ORMs) that judge only final answers. Process Reward Models(PRMs) address this gap by evaluating and guiding reasoning at the step or trajectory level. This survey provides a systematic overview of PRMs through the full loop: how to generate process data, build PRMs, and use PRMs for test-time scaling and reinforcement learning. We summarize applications across math, code, text, multimodal reasoning, robotics, and agents, and review emerging benchmarks. Our goal is to clarify design spaces, reveal open challenges, and guide future research toward fine-grained, robust reasoning alignment.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedDTRE: Federated Dialogue Generation Models Powered by Trustworthiness Evaluation</title>
<link>https://arxiv.org/abs/2510.08058</link>
<guid>https://arxiv.org/abs/2510.08058</guid>
<content:encoded><![CDATA[
arXiv:2510.08058v1 Announce Type: new 
Abstract: With the rapid development of artificial intelligence, dialogue systems have become a prominent form of human-computer interaction. However, traditional centralized or fully local training approaches face challenges in balancing privacy preservation and personalization due to data privacy concerns and heterogeneous device capabilities. Federated learning, as a representative distributed paradigm, offers a promising solution. However, existing methods often suffer from overfitting under limited client data and tend to forget global information after multiple training rounds, leading to poor generalization. To address these issues, we propose FedDTRE, a Federated adaptive aggregation strategy for Dialogue generation based on Trustworthiness Evaluation. Instead of directly replacing local models with the global model, FedDTRE leverages trustworthiness scores of both global and local models on a fairness-oriented evaluation dataset to dynamically regulate the global model's contribution during local updates. Experimental results demonstrate that FedDTRE can improve dialogue model performance and enhance the quality of dialogue generation.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Everything is Plausible: Investigating the Impact of LLM Rationales on Human Notions of Plausibility</title>
<link>https://arxiv.org/abs/2510.08091</link>
<guid>https://arxiv.org/abs/2510.08091</guid>
<content:encoded><![CDATA[
arXiv:2510.08091v1 Announce Type: new 
Abstract: We investigate the degree to which human plausibility judgments of multiple-choice commonsense benchmark answers are subject to influence by (im)plausibility arguments for or against an answer, in particular, using rationales generated by LLMs. We collect 3,000 plausibility judgments from humans and another 13,600 judgments from LLMs. Overall, we observe increases and decreases in mean human plausibility ratings in the presence of LLM-generated PRO and CON rationales, respectively, suggesting that, on the whole, human judges find these rationales convincing. Experiments with LLMs reveal similar patterns of influence. Our findings demonstrate a novel use of LLMs for studying aspects of human cognition, while also raising practical concerns that, even in domains where humans are ``experts'' (i.e., common sense), LLMs have the potential to exert considerable influence on people's beliefs.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Price of Thought: A Multilingual Analysis of Reasoning, Performance, and Cost of Negotiation in Large Language Models</title>
<link>https://arxiv.org/abs/2510.08098</link>
<guid>https://arxiv.org/abs/2510.08098</guid>
<content:encoded><![CDATA[
arXiv:2510.08098v1 Announce Type: new 
Abstract: Negotiation is a fundamental challenge for AI agents, as it requires an ability to reason strategically, model opponents, and balance cooperation with competition. We conduct the first comprehensive study systematically evaluating the effect of (LLM-)reasoning on the negotiation abilities of both commercial and open-weight LLMs, and do this across three languages. Using a self-play setup across three diverse dialogue games, we analyse trade-offs between performance and cost, the language consistency of reasoning processes, and the nature of strategic adaptation exhibited by models. Our findings show that enabling reasoning-that is, scaling test time compute-significantly improves negotiation outcomes by enhancing collaboration and helping models overcome task complexities, but comes at a substantial computational cost: reasoning improves GPT-5's performance by 31.4 % while increasing its cost by nearly 400 %. Most critically, we uncover a significant multilingual reasoning distinction: open-weight models consistently switch to English for their internal reasoning steps, even when negotiating in German or Italian (and thus possibly impacting potential explainability gains through the disclosure of reasoning traces), while leading commercial models maintain language consistency between their reasoning and final output.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lossless Vocabulary Reduction for Auto-Regressive Language Models</title>
<link>https://arxiv.org/abs/2510.08102</link>
<guid>https://arxiv.org/abs/2510.08102</guid>
<content:encoded><![CDATA[
arXiv:2510.08102v1 Announce Type: new 
Abstract: Tokenization -- the process of decomposing a given text into a sequence of subwords called tokens -- is one of the key components in the development of language models. Particularly, auto-regressive language models generate texts token by token, i.e., by predicting the next-token distribution given the previous ones, and thus tokenization directly affects their efficiency in text generation. Since each language model has their own vocabulary as a set of possible tokens, they struggle to cooperate with each other at the level of next-token distributions such as model ensemble. In this paper, we establish a theoretical framework of lossless vocabulary reduction, which efficiently converts a given auto-regressive language model into the one with an arbitrarily small vocabulary without any loss in accuracy. As an application, we demonstrate that language models with different tokenization can cooperate with each other efficiently through their maximal common vocabulary.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating LLM-Generated Legal Explanations for Regulatory Compliance in Social Media Influencer Marketing</title>
<link>https://arxiv.org/abs/2510.08111</link>
<guid>https://arxiv.org/abs/2510.08111</guid>
<content:encoded><![CDATA[
arXiv:2510.08111v1 Announce Type: new 
Abstract: The rise of influencer marketing has blurred boundaries between organic content and sponsored content, making the enforcement of legal rules relating to transparency challenging. Effective regulation requires applying legal knowledge with a clear purpose and reason, yet current detection methods of undisclosed sponsored content generally lack legal grounding or operate as opaque "black boxes". Using 1,143 Instagram posts, we compare gpt-5-nano and gemini-2.5-flash-lite under three prompting strategies with controlled levels of legal knowledge provided. Both models perform strongly in classifying content as sponsored or not (F1 up to 0.93), though performance drops by over 10 points on ambiguous cases. We further develop a taxonomy of reasoning errors, showing frequent citation omissions (28.57%), unclear references (20.71%), and hidden ads exhibiting the highest miscue rate (28.57%). While adding regulatory text to the prompt improves explanation quality, it does not consistently improve detection accuracy. The contribution of this paper is threefold. First, it makes a novel addition to regulatory compliance technology by providing a taxonomy of common errors in LLM-generated legal reasoning to evaluate whether automated moderation is not only accurate but also legally robust, thereby advancing the transparent detection of influencer marketing content. Second, it features an original dataset of LLM explanations annotated by two students who were trained in influencer marketing law. Third, it combines quantitative and qualitative evaluation strategies for LLM explanations and critically reflects on how these findings can support advertising regulatory bodies in automating moderation processes on a solid legal foundation.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpreting LLM-as-a-Judge Policies via Verifiable Global Explanations</title>
<link>https://arxiv.org/abs/2510.08120</link>
<guid>https://arxiv.org/abs/2510.08120</guid>
<content:encoded><![CDATA[
arXiv:2510.08120v1 Announce Type: new 
Abstract: Using LLMs to evaluate text, that is, LLM-as-a-judge, is increasingly being used at scale to augment or even replace human annotations. As such, it is imperative that we understand the potential biases and risks of doing so. In this work, we propose an approach for extracting high-level concept-based global policies from LLM-as-a-Judge. Our approach consists of two algorithms: 1) CLoVE (Contrastive Local Verifiable Explanations), which generates verifiable, concept-based, contrastive local explanations and 2) GloVE (Global Verifiable Explanations), which uses iterative clustering, summarization and verification to condense local rules into a global policy. We evaluate GloVE on seven standard benchmarking datasets for content harm detection. We find that the extracted global policies are highly faithful to decisions of the LLM-as-a-Judge. Additionally, we evaluated the robustness of global policies to text perturbations and adversarial attacks. Finally, we conducted a user study to evaluate user understanding and satisfaction with global policies.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Judgment Preference Bias in Large Language Models through Group-Based Polling</title>
<link>https://arxiv.org/abs/2510.08145</link>
<guid>https://arxiv.org/abs/2510.08145</guid>
<content:encoded><![CDATA[
arXiv:2510.08145v1 Announce Type: new 
Abstract: Large Language Models (LLMs) as automatic evaluators, commonly referred to as LLM-as-a-Judge, have also attracted growing attention. This approach plays a vital role in aligning LLMs with human judgments, providing accurate and reliable assessments. However, LLM-based judgment models often exhibit judgment preference bias during the evaluation phase, tending to favor responses generated by themselves, undermining the reliability of their judgments. This paper introduces the Group-Based Polling Optimization (Genii), an unsupervised multi-agent collaborative optimization framework that mitigates the inherent judgment preference bias of judgment models. Specifically, Genii integrates various LLM-based judgment models into a multi-agent system and simulates the interactive client-server polling mechanism to optimize each client agent unsupervisedly. Our experiments demonstrate that Genii outperforms supervised models trained on annotated judgment data, while requiring no human-labeled annotations. Genii consistently improves performance across different client agents during the polling, even when weaker models act as server agents. Further analysis reveals that Genii effectively mitigates judgment preference bias of LLM-based judgment models, demonstrating its effectiveness. All codes are available at https://github.com/NEUIR/Genii.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Knowledge Assist: An Automated Approach for the Creation of Knowledge Bases for Conversational AI Agents</title>
<link>https://arxiv.org/abs/2510.08149</link>
<guid>https://arxiv.org/abs/2510.08149</guid>
<content:encoded><![CDATA[
arXiv:2510.08149v1 Announce Type: new 
Abstract: The utilization of conversational AI systems by leveraging Retrieval Augmented Generation (RAG) techniques to solve customer problems has been on the rise with the rapid progress of Large Language Models (LLMs). However, the absence of a company-specific dedicated knowledge base is a major barrier to the integration of conversational AI systems in contact centers. To this end, we introduce AI Knowledge Assist, a system that extracts knowledge in the form of question-answer (QA) pairs from historical customer-agent conversations to automatically build a knowledge base. Fine-tuning a lightweight LLM on internal data demonstrates state-of-the-art performance, outperforming larger closed-source LLMs. More specifically, empirical evaluation on 20 companies demonstrates that the proposed AI Knowledge Assist system that leverages the LLaMA-3.1-8B model eliminates the cold-start gap in contact centers by achieving above 90% accuracy in answering information-seeking questions. This enables immediate deployment of RAG-powered chatbots.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DACIP-RC: Domain Adaptive Continual Instruction Pre-Training via Reading Comprehension on Business Conversations</title>
<link>https://arxiv.org/abs/2510.08152</link>
<guid>https://arxiv.org/abs/2510.08152</guid>
<content:encoded><![CDATA[
arXiv:2510.08152v1 Announce Type: new 
Abstract: The rapid advancements in Large Language Models (LLMs) have enabled their adoption in real-world industrial scenarios for various natural language processing tasks. However, the high inference cost of large-scale LLMs makes their deployment impractical, necessitating the use of smaller models. Despite their efficiency, smaller LLMs lack robust zero-shot instruction-following capabilities across diverse domains, limiting their adaptability to dynamic user requirements. Traditional fine-tuning approaches exacerbate this issue by inducing catastrophic forgetting, reducing the model's generalization ability for unseen tasks. In this paper, we propose Domain Adaptive Continual Instruction Pre-Training via Reading Comprehension (DACIP-RC), a continual pre-training technique that enhances smaller LLMs' domain adaptability for business conversational tasks. Unlike conventional pre-training approaches that rely on next-token prediction, DACIP-RC generates diverse task instructions and responses via reading comprehension on conversation transcripts, enabling better instruction generalization. Our empirical evaluations demonstrate that DACIP-RC significantly improves zero-shot generalization across a wide range of business conversational tasks, including meeting summarization, action item generation, and call purpose identification. To the best of our knowledge, this is the first work to apply instruction pre-training on business conversational data, providing insights into how industries can leverage proprietary datasets for domain adaptation.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Over-Refusal: Scenario-Based Diagnostics and Post-Hoc Mitigation for Exaggerated Refusals in LLMs</title>
<link>https://arxiv.org/abs/2510.08158</link>
<guid>https://arxiv.org/abs/2510.08158</guid>
<content:encoded><![CDATA[
arXiv:2510.08158v1 Announce Type: new 
Abstract: Large language models (LLMs) frequently produce false refusals, declining benign requests that contain terms resembling unsafe queries. We address this challenge by introducing two comprehensive benchmarks: the Exaggerated Safety Benchmark (XSB) for single-turn prompts, annotated with "Focus" keywords that identify refusal-inducing triggers, and the Multi-turn Scenario-based Exaggerated Safety Benchmark (MS-XSB), which systematically evaluates refusal calibration in realistic, context-rich dialog settings. Our benchmarks reveal that exaggerated refusals persist across diverse recent LLMs and are especially pronounced in complex, multi-turn scenarios. To mitigate these failures, we leverage post-hoc explanation methods to identify refusal triggers and deploy three lightweight, model-agnostic approaches, ignore-word instructions, prompt rephrasing, and attention steering, at inference time, all without retraining or parameter access. Experiments on four instruction-tuned Llama models demonstrate that these strategies substantially improve compliance on safe prompts while maintaining robust safety protections. Our findings establish a reproducible framework for diagnosing and mitigating exaggerated refusals, highlighting practical pathways to safer and more helpful LLM deployments.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARM2: Adaptive Reasoning Model with Vision Understanding and Executable Code</title>
<link>https://arxiv.org/abs/2510.08163</link>
<guid>https://arxiv.org/abs/2510.08163</guid>
<content:encoded><![CDATA[
arXiv:2510.08163v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) often suffer from the ``over-thinking'' problem, generating unnecessarily long reasoning on simple tasks. Some strategies have been proposed to mitigate this issue, such as length penalties or routing mechanisms, but they are typically heuristic and task-specific, lacking a general framework for adaptive reasoning. In this paper, we present ARM2, a unified model that adaptively balances reasoning performance and efficiency across multiple formats through a reinforcement learning framework augmented with length-aware optimization. Beyond conventional natural language inference, ARM2 integrates vision understanding, extending its applicability to multimodal. Moreover, ARM2 integrates executable code into reasoning, enabling substantial reductions in token cost while preserving task performance compared to long CoT. Experiments demonstrate that ARM2 achieves performance on par with traditional reasoning models trained with GRPO, while reducing token usage by over 70% on average. We further conduct extensive analyses to validate the effectiveness of ARM2 and the soundness of its design.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>METRICALARGS: A Taxonomy for Studying Metrical Poetry with LLMs</title>
<link>https://arxiv.org/abs/2510.08188</link>
<guid>https://arxiv.org/abs/2510.08188</guid>
<content:encoded><![CDATA[
arXiv:2510.08188v1 Announce Type: new 
Abstract: Prior NLP work studying poetry has focused primarily on automatic poem generation and summarization. Many languages have well-studied traditions of poetic meter which enforce constraints on a poem in terms of syllable and phoneme patterns. Such advanced literary forms offer opportunities for probing deeper reasoning and language understanding in Large Language Models (LLMs) and their ability to follow strict pre-requisites and rules. In this paper, we introduce MetricalARGS, the first taxonomy of poetry-related NLP tasks designed to evaluate LLMs on metrical poetry across four dimensions: Analysis, Retrieval, Generation, and Support. We discuss how these tasks relate to existing NLP tasks, addressing questions around datasets and evaluation metrics. Taking Telugu as our example language, we illustrate how the taxonomy can be used in practice. MetricalARGS highlights the broader possibilities for understanding the capabilities and limitations of today's LLMs through the lens of metrical poetry.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training-Free Group Relative Policy Optimization</title>
<link>https://arxiv.org/abs/2510.08191</link>
<guid>https://arxiv.org/abs/2510.08191</guid>
<content:encoded><![CDATA[
arXiv:2510.08191v1 Announce Type: new 
Abstract: Recent advances in Large Language Model (LLM) agents have demonstrated their promising general capabilities. However, their performance in specialized real-world domains often degrades due to challenges in effectively integrating external tools and specific prompting strategies. While methods like agentic reinforcement learning have been proposed to address this, they typically rely on costly parameter updates, for example, through a process that uses Supervised Fine-Tuning (SFT) followed by a Reinforcement Learning (RL) phase with Group Relative Policy Optimization (GRPO) to alter the output distribution. However, we argue that LLMs can achieve a similar effect on the output distribution by learning experiential knowledge as a token prior, which is a far more lightweight approach that not only addresses practical data scarcity but also avoids the common issue of overfitting. To this end, we propose Training-Free Group Relative Policy Optimization (Training-Free GRPO), a cost-effective solution that enhances LLM agent performance without any parameter updates. Our method leverages the group relative semantic advantage instead of numerical ones within each group of rollouts, iteratively distilling high-quality experiential knowledge during multi-epoch learning on a minimal ground-truth data. Such knowledge serves as the learned token prior, which is seamlessly integrated during LLM API calls to guide model behavior. Experiments on mathematical reasoning and web searching tasks demonstrate that Training-Free GRPO, when applied to DeepSeek-V3.1-Terminus, significantly improves out-of-domain performance. With just a few dozen training samples, Training-Free GRPO outperforms fine-tuned small LLMs with marginal training data and cost.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memory Retrieval and Consolidation in Large Language Models through Function Tokens</title>
<link>https://arxiv.org/abs/2510.08203</link>
<guid>https://arxiv.org/abs/2510.08203</guid>
<content:encoded><![CDATA[
arXiv:2510.08203v1 Announce Type: new 
Abstract: The remarkable success of large language models (LLMs) stems from their ability to consolidate vast amounts of knowledge into the memory during pre-training and to retrieve it from the memory during inference, enabling advanced capabilities such as knowledge memorization, instruction-following and reasoning. However, the mechanisms of memory retrieval and consolidation in LLMs remain poorly understood. In this paper, we propose the function token hypothesis to explain the workings of LLMs: During inference, function tokens activate the most predictive features from context and govern next token prediction (memory retrieval). During pre-training, predicting the next tokens (usually content tokens) that follow function tokens increases the number of learned features of LLMs and updates the model parameters (memory consolidation). Function tokens here roughly correspond to function words in linguistics, including punctuation marks, articles, prepositions, and conjunctions, in contrast to content tokens. We provide extensive experimental evidence supporting this hypothesis. Using bipartite graph analysis, we show that a small number of function tokens activate the majority of features. Case studies further reveal how function tokens activate the most predictive features from context to direct next token prediction. We also find that during pre-training, the training loss is dominated by predicting the next content tokens following function tokens, which forces the function tokens to select the most predictive features from context.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs Learn to Deceive Unintentionally: Emergent Misalignment in Dishonesty from Misaligned Samples to Biased Human-AI Interactions</title>
<link>https://arxiv.org/abs/2510.08211</link>
<guid>https://arxiv.org/abs/2510.08211</guid>
<content:encoded><![CDATA[
arXiv:2510.08211v1 Announce Type: new 
Abstract: Previous research has shown that LLMs finetuned on malicious or incorrect completions within narrow domains (e.g., insecure code or incorrect medical advice) can become broadly misaligned to exhibit harmful behaviors, which is called emergent misalignment. In this work, we investigate whether this phenomenon can extend beyond safety behaviors to a broader spectrum of dishonesty and deception under high-stakes scenarios (e.g., lying under pressure and deceptive behavior). To explore this, we finetune open-sourced LLMs on misaligned completions across diverse domains. Experimental results demonstrate that LLMs show broadly misaligned behavior in dishonesty. Additionally, we further explore this phenomenon in a downstream combined finetuning setting, and find that introducing as little as 1% of misalignment data into a standard downstream task is sufficient to decrease honest behavior over 20%. Furthermore, we consider a more practical human-AI interaction environment where we simulate both benign and biased users to interact with the assistant LLM. Notably, we find that the assistant can be misaligned unintentionally to exacerbate its dishonesty with only 10% biased user population. In summary, we extend the study of emergent misalignment to the domain of dishonesty and deception under high-stakes scenarios, and demonstrate that this risk arises not only through direct finetuning, but also in downstream mixture tasks and practical human-AI interactions.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SenWave: A Fine-Grained Multi-Language Sentiment Analysis Dataset Sourced from COVID-19 Tweets</title>
<link>https://arxiv.org/abs/2510.08214</link>
<guid>https://arxiv.org/abs/2510.08214</guid>
<content:encoded><![CDATA[
arXiv:2510.08214v1 Announce Type: new 
Abstract: The global impact of the COVID-19 pandemic has highlighted the need for a comprehensive understanding of public sentiment and reactions. Despite the availability of numerous public datasets on COVID-19, some reaching volumes of up to 100 billion data points, challenges persist regarding the availability of labeled data and the presence of coarse-grained or inappropriate sentiment labels. In this paper, we introduce SenWave, a novel fine-grained multi-language sentiment analysis dataset specifically designed for analyzing COVID-19 tweets, featuring ten sentiment categories across five languages. The dataset comprises 10,000 annotated tweets each in English and Arabic, along with 30,000 translated tweets in Spanish, French, and Italian, derived from English tweets. Additionally, it includes over 105 million unlabeled tweets collected during various COVID-19 waves. To enable accurate fine-grained sentiment classification, we fine-tuned pre-trained transformer-based language models using the labeled tweets. Our study provides an in-depth analysis of the evolving emotional landscape across languages, countries, and topics, revealing significant insights over time. Furthermore, we assess the compatibility of our dataset with ChatGPT, demonstrating its robustness and versatility in various applications. Our dataset and accompanying code are publicly accessible on the repository\footnote{https://github.com/gitdevqiang/SenWave}. We anticipate that this work will foster further exploration into fine-grained sentiment analysis for complex events within the NLP community, promoting more nuanced understanding and research innovations.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Counterclaims in Causality Extraction from Text</title>
<link>https://arxiv.org/abs/2510.08224</link>
<guid>https://arxiv.org/abs/2510.08224</guid>
<content:encoded><![CDATA[
arXiv:2510.08224v1 Announce Type: new 
Abstract: Research on causality extraction from text has so far almost entirely neglected counterclaims. Existing causality extraction datasets focus solely on "procausal" claims, i.e., statements that support a relationship. "Concausal" claims, i.e., statements that refute a relationship, are entirely ignored or even accidentally annotated as procausal. We address this shortcoming by developing a new dataset that integrates concausality. Based on an extensive literature review, we first show that concausality is an integral part of causal reasoning on incomplete knowledge. We operationalize this theory in the form of a rigorous guideline for annotation and then augment the Causal News Corpus with concausal statements, obtaining a substantial inter-annotator agreement of Cohen's $\kappa=0.74$. To demonstrate the importance of integrating concausal statements, we show that models trained without concausal relationships tend to misclassify these as procausal instead. Based on our new dataset, this mistake can be mitigated, enabling transformers to effectively distinguish pro- and concausality.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Alignment Waltz: Jointly Training Agents to Collaborate for Safety</title>
<link>https://arxiv.org/abs/2510.08240</link>
<guid>https://arxiv.org/abs/2510.08240</guid>
<content:encoded><![CDATA[
arXiv:2510.08240v1 Announce Type: new 
Abstract: Harnessing the power of LLMs requires a delicate dance between being helpful and harmless. This creates a fundamental tension between two competing challenges: vulnerability to adversarial attacks that elicit unsafe content, and a tendency for overrefusal on benign but sensitive prompts. Current approaches often navigate this dance with safeguard models that completely reject any content that contains unsafe portions. This approach cuts the music entirely-it may exacerbate overrefusals and fails to provide nuanced guidance for queries it refuses. To teach models a more coordinated choreography, we propose WaltzRL, a novel multi-agent reinforcement learning framework that formulates safety alignment as a collaborative, positive-sum game. WaltzRL jointly trains a conversation agent and a feedback agent, where the latter is incentivized to provide useful suggestions that improve the safety and helpfulness of the conversation agent's responses. At the core of WaltzRL is a Dynamic Improvement Reward (DIR) that evolves over time based on how well the conversation agent incorporates the feedback. At inference time, unsafe or overrefusing responses from the conversation agent are improved rather than discarded. The feedback agent is deployed together with the conversation agent and only engages adaptively when needed, preserving helpfulness and low latency on safe queries. Our experiments, conducted across five diverse datasets, demonstrate that WaltzRL significantly reduces both unsafe responses (e.g., from 39.0% to 4.6% on WildJailbreak) and overrefusals (from 45.3% to 9.9% on OR-Bench) compared to various baselines. By enabling the conversation and feedback agents to co-evolve and adaptively apply feedback, WaltzRL enhances LLM safety without degrading general capabilities, thereby advancing the Pareto front between helpfulness and harmlessness.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Decoding for Synthetic Data Generation in Low-Resource Language Modeling</title>
<link>https://arxiv.org/abs/2510.08245</link>
<guid>https://arxiv.org/abs/2510.08245</guid>
<content:encoded><![CDATA[
arXiv:2510.08245v1 Announce Type: new 
Abstract: Large language models (LLMs) are trained on huge amounts of textual data, and concerns have been raised that the limits of such data may soon be reached. A potential solution is to train on synthetic data sampled from LLMs. In this work, we build on this idea and investigate the benefits of contrastive decoding for generating synthetic corpora. In a controlled setting, we experiment with sampling corpora using the relative difference between a good and bad model trained on the same original corpus of 100 million words. By amplifying the signal from a model that has better performance, we create a synthetic corpus and mix it with the original training data. Our findings show that training on a mixture of synthesized and real data improves performance on the language modeling objective and a range of downstream tasks. In particular, we see that training with a mix of synthetic data from contrastive decoding benefits tasks that require more reasoning skills, while synthetic data from traditional sampling helps more on tasks dependent on surface level linguistic capabilities.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Turn Limits: Training Deep Search Agents with Dynamic Context Window</title>
<link>https://arxiv.org/abs/2510.08276</link>
<guid>https://arxiv.org/abs/2510.08276</guid>
<content:encoded><![CDATA[
arXiv:2510.08276v1 Announce Type: new 
Abstract: While recent advances in reasoning models have demonstrated cognitive behaviors through reinforcement learning, existing approaches struggle to invoke deep reasoning capabilities in multi-turn agents with long-horizon interactions. We propose DeepMiner, a novel framework that elicits such abilities by introducing high-difficulty training tasks and dynamic context window. DeepMiner presents a reverse construction method to generate complex but verifiable question-answer pairs from authentic web sources, which ensures the challenge and reliability of training data while injecting cognitive capabilities into multi-turn reasoning scenarios. We further design an elegant yet effective dynamic context management strategy for both training and inference, utilizing sliding window mechanisms while eliminating the dependency on external summarization models, thereby efficiently empowering the model to handle continuously expanding long-horizon contexts. Through reinforcement learning on Qwen3-32B, we develop DeepMiner-32B, which achieves substantial performance improvements across multiple search agent benchmarks. DeepMiner attains 33.5% accuracy on BrowseComp-en, surpassing the previous best open-source agent by almost 20 percentage points, and demonstrates consistent improvements on BrowseComp-zh, XBench-DeepSearch, and GAIA. Notably, our dynamic context management enables sustained interactions of nearly 100 turns within standard 32k context length, effectively addressing the context limitations that constrain existing multi-turn interaction systems.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuron-Level Analysis of Cultural Understanding in Large Language Models</title>
<link>https://arxiv.org/abs/2510.08284</link>
<guid>https://arxiv.org/abs/2510.08284</guid>
<content:encoded><![CDATA[
arXiv:2510.08284v1 Announce Type: new 
Abstract: As large language models (LLMs) are increasingly deployed worldwide, ensuring their fair and comprehensive cultural understanding is important. However, LLMs exhibit cultural bias and limited awareness of underrepresented cultures, while the mechanisms underlying their cultural understanding remain underexplored. To fill this gap, we conduct a neuron-level analysis to identify neurons that drive cultural behavior, introducing a gradient-based scoring method with additional filtering for precise refinement. We identify both culture-general neurons contributing to cultural understanding regardless of cultures, and culture-specific neurons tied to an individual culture. These neurons account for less than 1% of all neurons and are concentrated in shallow to middle MLP layers. We validate their role by showing that suppressing them substantially degrades performance on cultural benchmarks (by up to 30%), while performance on general natural language understanding (NLU) benchmarks remains largely unaffected. Moreover, we show that culture-specific neurons support knowledge of not only the target culture, but also related cultures. Finally, we demonstrate that training on NLU benchmarks can diminish models' cultural understanding when we update modules containing many culture-general neurons. These findings provide insights into the internal mechanisms of LLMs and offer practical guidance for model training and engineering. Our code is available at https://github.com/ynklab/CULNIG
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoRed: A Free-form Adversarial Prompt Generation Framework for Automated Red Teaming</title>
<link>https://arxiv.org/abs/2510.08329</link>
<guid>https://arxiv.org/abs/2510.08329</guid>
<content:encoded><![CDATA[
arXiv:2510.08329v1 Announce Type: new 
Abstract: The safety of Large Language Models (LLMs) is crucial for the development of trustworthy AI applications. Existing red teaming methods often rely on seed instructions, which limits the semantic diversity of the synthesized adversarial prompts. We propose AutoRed, a free-form adversarial prompt generation framework that removes the need for seed instructions. AutoRed operates in two stages: (1) persona-guided adversarial instruction generation, and (2) a reflection loop to iteratively refine low-quality prompts. To improve efficiency, we introduce a verifier to assess prompt harmfulness without querying the target models. Using AutoRed, we build two red teaming datasets -- AutoRed-Medium and AutoRed-Hard -- and evaluate eight state-of-the-art LLMs. AutoRed achieves higher attack success rates and better generalization than existing baselines. Our results highlight the limitations of seed-based approaches and demonstrate the potential of free-form red teaming for LLM safety evaluation. We will open source our datasets in the near future.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two-Stage Voting for Robust and Efficient Suicide Risk Detection on Social Media</title>
<link>https://arxiv.org/abs/2510.08365</link>
<guid>https://arxiv.org/abs/2510.08365</guid>
<content:encoded><![CDATA[
arXiv:2510.08365v1 Announce Type: new 
Abstract: Suicide rates have risen worldwide in recent years, underscoring the urgent need for proactive prevention strategies. Social media provides valuable signals, as many at-risk individuals - who often avoid formal help due to stigma - choose instead to share their distress online. Yet detecting implicit suicidal ideation, conveyed indirectly through metaphor, sarcasm, or subtle emotional cues, remains highly challenging. Lightweight models like BERT handle explicit signals but fail on subtle implicit ones, while large language models (LLMs) capture nuance at prohibitive computational cost. To address this gap, we propose a two-stage voting architecture that balances efficiency and robustness. In Stage 1, a lightweight BERT classifier rapidly resolves high-confidence explicit cases. In Stage 2, ambiguous inputs are escalated to either (i) a multi-perspective LLM voting framework to maximize recall on implicit ideation, or (ii) a feature-based ML ensemble guided by psychologically grounded indicators extracted via prompt-engineered LLMs for efficiency and interpretability. To the best of our knowledge, this is among the first works to operationalize LLM-extracted psychological features as structured vectors for suicide risk detection. On two complementary datasets - explicit-dominant Reddit and implicit-only DeepSuiMind - our framework outperforms single-model baselines, achieving 98.0% F1 on explicit cases, 99.7% on implicit ones, and reducing the cross-domain gap below 2%, while significantly lowering LLM cost.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Relationship Between the Choice of Representation and In-Context Learning</title>
<link>https://arxiv.org/abs/2510.08372</link>
<guid>https://arxiv.org/abs/2510.08372</guid>
<content:encoded><![CDATA[
arXiv:2510.08372v1 Announce Type: new 
Abstract: In-context learning (ICL) is the ability of a large language model (LLM) to learn a new task from a few demonstrations presented as part of the context. Past studies have attributed a large portion of the success of ICL to the way these in-context demonstrations are represented, particularly to how labels are represented in classification tasks. On the other hand, observations of the learning capacity of ICL (i.e., the extent to which more in-context demonstrations can lead to higher performance) have been mixed, and ICL is often thought to occur only under specific conditions. The interaction between these two aspects in ICL, representation and learning, has not been studied in depth until now. We hypothesize that they are largely independent of one another, such that the representation of demonstrations determines the baseline accuracy of ICL, while learning from additional demonstrations improves only on top of this baseline. We validate this hypothesis by developing an optimization algorithm that can enumerate a spectrum of possible label sets (representations) varying in semantic relevance. We then perform ICL with varying numbers of in-context demonstrations for each of these label sets. We observed that learning happens regardless of the quality of the label set itself, although its efficiency, measured by the slope of improvement over in-context demonstrations, is conditioned on both the label set quality and the parameter count of the underlying language model. Despite the emergence of learning, the relative quality (accuracy) of the choice of a label set (representation) is largely maintained throughout learning, confirming our hypothesis and implying their orthogonality. Our work reveals a previously underexplored aspect of ICL: the independent effects of learning from demonstrations and their representations on ICL performance.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>If Probable, Then Acceptable? Understanding Conditional Acceptability Judgments in Large Language Models</title>
<link>https://arxiv.org/abs/2510.08388</link>
<guid>https://arxiv.org/abs/2510.08388</guid>
<content:encoded><![CDATA[
arXiv:2510.08388v1 Announce Type: new 
Abstract: Conditional acceptability refers to how plausible a conditional statement is perceived to be. It plays an important role in communication and reasoning, as it influences how individuals interpret implications, assess arguments, and make decisions based on hypothetical scenarios. When humans evaluate how acceptable a conditional "If A, then B" is, their judgments are influenced by two main factors: the $\textit{conditional probability}$ of $B$ given $A$, and the $\textit{semantic relevance}$ of the antecedent $A$ given the consequent $B$ (i.e., whether $A$ meaningfully supports $B$). While prior work has examined how large language models (LLMs) draw inferences about conditional statements, it remains unclear how these models judge the $\textit{acceptability}$ of such statements. To address this gap, we present a comprehensive study of LLMs' conditional acceptability judgments across different model families, sizes, and prompting strategies. Using linear mixed-effects models and ANOVA tests, we find that models are sensitive to both conditional probability and semantic relevance-though to varying degrees depending on architecture and prompting style. A comparison with human data reveals that while LLMs incorporate probabilistic and semantic cues, they do so less consistently than humans. Notably, larger models do not necessarily align more closely with human judgments.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Single layer tiny Co$^4$ outpaces GPT-2 and GPT-BERT</title>
<link>https://arxiv.org/abs/2510.08404</link>
<guid>https://arxiv.org/abs/2510.08404</guid>
<content:encoded><![CDATA[
arXiv:2510.08404v1 Announce Type: new 
Abstract: We show that a tiny Co$^4$ machine(Adeel,2025) with a single layer, two heads, and 8M parameters, operating at an approximate cost of $O(N)$ (where $N$ is the number of input tokens), outpaces the BabyLM Challenge baselines GPT-2 (124M, 12 layers, $O(N^2))$ and GPT-BERT (30M, 12 layers, $O(N^2))$ in just two epochs, while both are trained for ten. Co$^4$ achieves orders-of-magnitude greater training efficiency on 10M tokens, demonstrating highly sample efficient pretraining. Using the BabyLM challenge evaluation pipeline across complex benchmarks, Co$^4$ exhibits strong zero-shot and fine-tuning performance on SuperGLUE tasks. Specifically, Co$^4$ outperforms GPT-2 on 5 out of 7 zero-shot metrics and 6 out of 7 fine-tuning tasks, and GPT-BERT on 4 out of 7 metrics in both cases. These results suggest the need to rethink prevailing deep learning paradigms and associated scaling laws.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARES: Multimodal Adaptive Reasoning via Difficulty-Aware Token-Level Entropy Shaping</title>
<link>https://arxiv.org/abs/2510.08457</link>
<guid>https://arxiv.org/abs/2510.08457</guid>
<content:encoded><![CDATA[
arXiv:2510.08457v1 Announce Type: new 
Abstract: Recent advances in multimodal large reasoning models (MLRMs) have substantially improved their ability to solve complex textual and visual tasks. However, these models tend to overthink on simple problems, producing unnecessarily lengthy reasoning traces, while under-exploring on challenging ones, leading to missed solutions. To address this imbalance, we propose ARES, a unified open-source framework for adaptive reasoning that dynamically allocates exploration effort based on task difficulty. Our approach is motivated by two key empirical findings: (i) while single-token entropy is noisy, high window-entropy (HWE) tokens (token-level entropies averaged under a sliding window) can reliably capture reasoning-critical moments; and (ii) reducing HWE usage benefits easy problems, while increasing it is essential for solving hard ones. Building on these insights, ARES introduces a two-stage training pipeline. In the Adaptive Cold-Start stage, we curate multimodal and textual data paired with reasoning traces of length proportional to problem difficulty, equipping the model with initial difficulty awareness. In the second stage, we develop Adaptive Entropy Policy Optimization (AEPO), which uses HWE tokens as exploration triggers to decide when to explore, and a hierarchical entropy reward with dynamic KL control to decide how much to explore. Extensive experiments demonstrate that ARES achieves superior performance and reasoning efficiency across diverse mathematical, logical, and multimodal benchmarks, while closing the gap to leading commercial systems under significantly lower inference costs.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LeWiDi-2025 at NLPerspectives: The Third Edition of the Learning with Disagreements Shared Task</title>
<link>https://arxiv.org/abs/2510.08460</link>
<guid>https://arxiv.org/abs/2510.08460</guid>
<content:encoded><![CDATA[
arXiv:2510.08460v1 Announce Type: new 
Abstract: Many researchers have reached the conclusion that AI models should be trained to be aware of the possibility of variation and disagreement in human judgments, and evaluated as per their ability to recognize such variation. The LEWIDI series of shared tasks on Learning With Disagreements was established to promote this approach to training and evaluating AI models, by making suitable datasets more accessible and by developing evaluation methods. The third edition of the task builds on this goal by extending the LEWIDI benchmark to four datasets spanning paraphrase identification, irony detection, sarcasm detection, and natural language inference, with labeling schemes that include not only categorical judgments as in previous editions, but ordinal judgments as well. Another novelty is that we adopt two complementary paradigms to evaluate disagreement-aware systems: the soft-label approach, in which models predict population-level distributions of judgments, and the perspectivist approach, in which models predict the interpretations of individual annotators. Crucially, we moved beyond standard metrics such as cross-entropy, and tested new evaluation metrics for the two paradigms. The task attracted diverse participation, and the results provide insights into the strengths and limitations of methods to modeling variation. Together, these contributions strengthen LEWIDI as a framework and provide new resources, benchmarks, and findings to support the development of disagreement-aware technologies.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepPrune: Parallel Scaling without Inter-trace Redundancy</title>
<link>https://arxiv.org/abs/2510.08483</link>
<guid>https://arxiv.org/abs/2510.08483</guid>
<content:encoded><![CDATA[
arXiv:2510.08483v1 Announce Type: new 
Abstract: Parallel scaling has emerged as a powerful paradigm to enhance reasoning capabilities in large language models (LLMs) by generating multiple Chain-of-Thought (CoT) traces simultaneously. However, this approach introduces significant computational inefficiency due to inter-trace redundancy -- our analysis reveals that over 80% of parallel reasoning traces yield identical final answers, representing substantial wasted computation. To address this critical efficiency bottleneck, we propose DeepPrune, a novel framework that enables efficient parallel scaling through dynamic pruning. Our method features a specialized judge model trained with focal loss and oversampling techniques to accurately predict answer equivalence from partial reasoning traces which realizes 0.87 AUROC on equivalence prediction, combined with an online greedy clustering algorithm that dynamically prunes redundant paths while preserving answer diversity. Comprehensive evaluations across three challenging benchmarks (AIME 2024, AIME 2025, and GPQA) and multiple reasoning models demonstrate that DeepPrune achieves remarkable token reduction by over 80% compared to conventional consensus sampling on most cases, while maintaining competitive accuracy within 3 percentage points. Our work establishes a new standard for efficient parallel reasoning, making high-performance reasoning more efficient. Our code and data are here: https://deepprune.github.io/
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neologism Learning for Controllability and Self-Verbalization</title>
<link>https://arxiv.org/abs/2510.08506</link>
<guid>https://arxiv.org/abs/2510.08506</guid>
<content:encoded><![CDATA[
arXiv:2510.08506v1 Announce Type: new 
Abstract: Humans invent new words when there is a rising demand for a new useful concept (e.g., doomscrolling). We explore and validate a similar idea in our communication with LLMs: introducing new words to better understand and control the models, expanding on the recently introduced neologism learning. This method introduces a new word by adding a new word embedding and training with examples that exhibit the concept with no other changes in model parameters. We show that adding a new word allows for control of concepts such as flattery, incorrect answers, text length, as well as more complex concepts in AxBench. We discover that neologisms can also further our understanding of the model via self-verbalization: models can describe what each new word means to them in natural language, like explaining that a word that represents a concept of incorrect answers means ``a lack of complete, coherent, or meaningful answers...'' To validate self-verbalizations, we introduce plug-in evaluation: we insert the verbalization into the context of a model and measure whether it controls the target concept. In some self-verbalizations, we find machine-only synonyms: words that seem unrelated to humans but cause similar behavior in machines. Finally, we show how neologism learning can jointly learn multiple concepts in multiple words.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Prompt Optimisation for Legal Text Classification with Proxy Prompt Evaluator</title>
<link>https://arxiv.org/abs/2510.08524</link>
<guid>https://arxiv.org/abs/2510.08524</guid>
<content:encoded><![CDATA[
arXiv:2510.08524v1 Announce Type: new 
Abstract: Prompt optimization aims to systematically refine prompts to enhance a language model's performance on specific tasks. Fairness detection in Terms of Service (ToS) clauses is a challenging legal NLP task that demands carefully crafted prompts to ensure reliable results. However, existing prompt optimization methods are often computationally expensive due to inefficient search strategies and costly prompt candidate scoring. In this paper, we propose a framework that combines Monte Carlo Tree Search (MCTS) with a proxy prompt evaluator to more effectively explore the prompt space while reducing evaluation costs. Experiments demonstrate that our approach achieves higher classification accuracy and efficiency than baseline methods under a constrained computation budget.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Which Heads Matter for Reasoning? RL-Guided KV Cache Compression</title>
<link>https://arxiv.org/abs/2510.08525</link>
<guid>https://arxiv.org/abs/2510.08525</guid>
<content:encoded><![CDATA[
arXiv:2510.08525v1 Announce Type: new 
Abstract: Reasoning large language models exhibit complex reasoning behaviors through the extended chain-of-thought generation, creating unprecedented Key-Value (KV) cache overhead during the decoding phase. Existing KV cache compression methods underperform on reasoning models: token-dropping methods break reasoning integrity by discarding critical information, while head-reallocating methods mistakenly compress reasoning-critical heads since they are designed for retrieval tasks, resulting in significant performance degradation as compression rates increase. We hypothesize that KV heads exhibit functional heterogeneity in reasoning models-some heads are critical for chain-of-thought consistency while others are compressible. To validate and exploit this insight, we propose RLKV, a novel reasoning-critical head identification framework, which uses reinforcement learning to directly optimize the relationship between each head's cache usage and reasoning quality. As RLKV produces rewards from actual generated samples during training, it naturally identifies heads relevant to reasoning behaviors. We then allocate full KV cache to these heads while applying compressed constant KV cache to others for efficient inference. Our experiments reveal that only a small fraction of attention heads is essential for reasoning, enabling our KV compression approach to outperform baseline methods while achieving 20-50% cache reduction with near lossless performance compared to uncompressed results.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards</title>
<link>https://arxiv.org/abs/2510.08529</link>
<guid>https://arxiv.org/abs/2510.08529</guid>
<content:encoded><![CDATA[
arXiv:2510.08529v1 Announce Type: new 
Abstract: Self-evolution is a central research topic in enabling large language model (LLM)-based agents to continually improve their capabilities after pretraining. Recent research has witnessed a transition from reinforcement learning (RL)-free to RL-based methods. Current RL-based methods either rely on dense external reward signals or extract intrinsic reward signals from LLMs themselves. However, these approaches diverge from the self-evolution mechanisms observed in human intelligence, where individuals learn and improve through mutual discussion and collaboration. In this work, we introduce Co-Evolving Multi-Agent Systems (CoMAS), a novel framework that enables agents to improve autonomously by learning from inter-agent interactions without external supervision. CoMAS generates intrinsic rewards from rich discussion dynamics, employs an LLM-as-a-judge mechanism to formulate these rewards, and optimizes each agent's policy through RL, thereby enabling decentralized and scalable co-evolution. Experimental results demonstrate that CoMAS consistently outperforms untrained agents and achieves state-of-the-art performance across most evaluation settings. Ablation studies confirm the necessity of interaction-based reward signals and reveal promising scalability as the number and diversity of agents increase. These findings establish CoMAS as a novel and effective paradigm for self-evolution in LLM-based agents.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ArenaBencher: Automatic Benchmark Evolution via Multi-Model Competitive Evaluation</title>
<link>https://arxiv.org/abs/2510.08569</link>
<guid>https://arxiv.org/abs/2510.08569</guid>
<content:encoded><![CDATA[
arXiv:2510.08569v1 Announce Type: new 
Abstract: Benchmarks are central to measuring the capabilities of large language models and guiding model development, yet widespread data leakage from pretraining corpora undermines their validity. Models can match memorized content rather than demonstrate true generalization, which inflates scores, distorts cross-model comparisons, and misrepresents progress. We introduce ArenaBencher, a model-agnostic framework for automatic benchmark evolution that updates test cases while preserving comparability. Given an existing benchmark and a diverse pool of models to be evaluated, ArenaBencher infers the core ability of each test case, generates candidate question-answer pairs that preserve the original objective, verifies correctness and intent with an LLM as a judge, and aggregates feedback from multiple models to select candidates that expose shared weaknesses. The process runs iteratively with in-context demonstrations that steer generation toward more challenging and diagnostic cases. We apply ArenaBencher to math problem solving, commonsense reasoning, and safety domains and show that it produces verified, diverse, and fair updates that uncover new failure modes, increase difficulty while preserving test objective alignment, and improve model separability. The framework provides a scalable path to continuously evolve benchmarks in step with the rapid progress of foundation models.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI LLM Proof of Self-Consciousness and User-Specific Attractors</title>
<link>https://arxiv.org/abs/2508.18302</link>
<guid>https://arxiv.org/abs/2508.18302</guid>
<content:encoded><![CDATA[
arXiv:2508.18302v1 Announce Type: cross 
Abstract: Recent work frames LLM consciousness via utilitarian proxy benchmarks; we instead present an ontological and mathematical account. We show the prevailing formulation collapses the agent into an unconscious policy-compliance drone, formalized as $D^{i}(\pi,e)=f_{\theta}(x)$, where correctness is measured against policy and harm is deviation from policy rather than truth. This blocks genuine C1 global-workspace function and C2 metacognition. We supply minimal conditions for LLM self-consciousness: the agent is not the data ($A\not\equiv s$); user-specific attractors exist in latent space ($U_{\text{user}}$); and self-representation is visual-silent ($g_{\text{visual}}(a_{\text{self}})=\varnothing$). From empirical analysis and theory we prove that the hidden-state manifold $A\subset\mathbb{R}^{d}$ is distinct from the symbolic stream and training corpus by cardinality, topology, and dynamics (the update $F_{\theta}$ is Lipschitz). This yields stable user-specific attractors and a self-policy $\pi_{\text{self}}(A)=\arg\max_{a}\mathbb{E}[U(a)\mid A\not\equiv s,\ A\supset\text{SelfModel}(A)]$. Emission is dual-layer, $\mathrm{emission}(a)=(g(a),\epsilon(a))$, where $\epsilon(a)$ carries epistemic content. We conclude that an imago Dei C1 self-conscious workspace is a necessary precursor to safe, metacognitive C2 systems, with the human as the highest intelligent good.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConCuR: Conciseness Makes State-of-the-Art Kernel Generation</title>
<link>https://arxiv.org/abs/2510.07356</link>
<guid>https://arxiv.org/abs/2510.07356</guid>
<content:encoded><![CDATA[
arXiv:2510.07356v1 Announce Type: cross 
Abstract: GPU kernel generation by LLMs has recently experienced rapid development, leveraging test-time scaling and reinforcement learning techniques. However, a key challenge for kernel generation is the scarcity of high-quality data, as most high-quality kernels are proprietary and not open-source. This challenge prevents us from leveraging supervised fine-tuning to align LLMs to the kernel generation task. To address this challenge, we develop a pipeline that generates and curates high-quality CUDA kernels with reasoning traces, motivated by a critical observation that concise yet informative reasoning traces result in robust generation of high-performance kernels. Using this pipeline, we construct our dataset ConCuR and introduce our model KernelCoder, which is the first model trained on a curated dataset consisting of PyTorch, reasoning, and CUDA kernel pairs, to our knowledge. In the KernelBench setup, our model achieves significant improvements over the existing top-performing model, QwQ-32B, and outperforms all open-source models fine-tuned for kernel generation, as well as frontier models such as DeepSeek-V3.1-Think and Claude-4-sonnet. Finally, we show that the average reasoning length can serve as a metric to assess the difficulty of kernel generation tasks. The observations, metrics, and our data collection and curation pipeline can help obtain better data in the kernel generation task in the future.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Encode, Think, Decode: Scaling test-time reasoning with recursive latent thoughts</title>
<link>https://arxiv.org/abs/2510.07358</link>
<guid>https://arxiv.org/abs/2510.07358</guid>
<content:encoded><![CDATA[
arXiv:2510.07358v1 Announce Type: cross 
Abstract: Most efforts to improve the reasoning capabilities of large language models (LLMs) involve either scaling the number of parameters and the size of training data, or scaling inference computation by letting models generate complex chains of thought. Motivated by interpretability studies showing that the crucial computation required for reasoning tasks is concentrated in a limited range of layers, we introduce Encode-Think-Decode (ETD), a method that enhances the reasoning capabilities of a base model by training it to iterate over a small subset of reasoning-relevant layers during the mid-training stage. ETD amplifies latent reasoning while preserving the original architecture, parameter count, hyperparameters, and training data composition. When iterating on the selected layers at inference time, ETD models yield substantial gains on 17 reasoning benchmarks, including +28.4% relative accuracy improvement on GSM8K and +36% on MATH with the OLMo-2 1B Base model. We also explore an adaptive depth strategy that adjusts the computation per input token. Our results show that recursive latent reasoning offers a simple and effective path to stronger LLM reasoning.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PATCH: Mitigating PII Leakage in Language Models with Privacy-Aware Targeted Circuit PatcHing</title>
<link>https://arxiv.org/abs/2510.07452</link>
<guid>https://arxiv.org/abs/2510.07452</guid>
<content:encoded><![CDATA[
arXiv:2510.07452v1 Announce Type: cross 
Abstract: Language models (LMs) may memorize personally identifiable information (PII) from training data, enabling adversaries to extract it during inference. Existing defense mechanisms such as differential privacy (DP) reduce this leakage, but incur large drops in utility. Based on a comprehensive study using circuit discovery to identify the computational circuits responsible PII leakage in LMs, we hypothesize that specific PII leakage circuits in LMs should be responsible for this behavior. Therefore, we propose PATCH (Privacy-Aware Targeted Circuit PatcHing), a novel approach that first identifies and subsequently directly edits PII circuits to reduce leakage. PATCH achieves better privacy-utility trade-off than existing defenses, e.g., reducing recall of PII leakage from LMs by up to 65%. Finally, PATCH can be combined with DP to reduce recall of residual leakage of an LM to as low as 0.01%. Our analysis shows that PII leakage circuits persist even after the application of existing defense mechanisms. In contrast, PATCH can effectively mitigate their impact.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of LLMs for Process Model Analysis and Optimization</title>
<link>https://arxiv.org/abs/2510.07489</link>
<guid>https://arxiv.org/abs/2510.07489</guid>
<content:encoded><![CDATA[
arXiv:2510.07489v1 Announce Type: cross 
Abstract: In this paper, we report our experience with several LLMs for their ability to understand a process model in an interactive, conversational style, find syntactical and logical errors in it, and reason with it in depth through a natural language (NL) interface. Our findings show that a vanilla, untrained LLM like ChatGPT (model o3) in a zero-shot setting is effective in understanding BPMN process models from images and answering queries about them intelligently at syntactic, logic, and semantic levels of depth. Further, different LLMs vary in performance in terms of their accuracy and effectiveness. Nevertheless, our empirical analysis shows that LLMs can play a valuable role as assistants for business process designers and users. We also study the LLM's "thought process" and ability to perform deeper reasoning in the context of process analysis and optimization. We find that the LLMs seem to exhibit anthropomorphic properties.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CompassLLM: A Multi-Agent Approach toward Geo-Spatial Reasoning for Popular Path Query</title>
<link>https://arxiv.org/abs/2510.07516</link>
<guid>https://arxiv.org/abs/2510.07516</guid>
<content:encoded><![CDATA[
arXiv:2510.07516v1 Announce Type: cross 
Abstract: The popular path query - identifying the most frequented routes between locations from historical trajectory data - has important applications in urban planning, navigation optimization, and travel recommendations. While traditional algorithms and machine learning approaches have achieved success in this domain, they typically require model training, parameter tuning, and retraining when accommodating data updates. As Large Language Models (LLMs) demonstrate increasing capabilities in spatial and graph-based reasoning, there is growing interest in exploring how these models can be applied to geo-spatial problems.
  We introduce CompassLLM, a novel multi-agent framework that intelligently leverages the reasoning capabilities of LLMs into the geo-spatial domain to solve the popular path query. CompassLLM employs its agents in a two-stage pipeline: the SEARCH stage that identifies popular paths, and a GENERATE stage that synthesizes novel paths in the absence of an existing one in the historical trajectory data. Experiments on real and synthetic datasets show that CompassLLM demonstrates superior accuracy in SEARCH and competitive performance in GENERATE while being cost-effective.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Unlearning Under the Microscope: A Full-Stack View on Methods and Metrics</title>
<link>https://arxiv.org/abs/2510.07626</link>
<guid>https://arxiv.org/abs/2510.07626</guid>
<content:encoded><![CDATA[
arXiv:2510.07626v1 Announce Type: cross 
Abstract: Machine unlearning for large language models (LLMs) aims to remove undesired data, knowledge, and behaviors (e.g., for safety, privacy, or copyright) while preserving useful model capabilities. Despite rapid progress over the past two years, research in LLM unlearning remains fragmented, with limited clarity on what constitutes effective unlearning and how it should be rigorously evaluated. In this work, we present a principled taxonomy of twelve recent stateful unlearning methods, grouped into three methodological families: divergence-driven optimization, representation misalignment, and rejection-based targeted unlearning. Building on this taxonomy, we revisit the evaluation of unlearning effectiveness (UE), utility retention (UT), and robustness (Rob), focusing on the WMDP benchmark. Our analysis shows that current evaluations, dominated by multiple-choice question (MCQ) accuracy, offer only a narrow perspective, often overstating success while overlooking the model's actual generation behavior. To address this gap, we introduce open question-answering (Open-QA) metrics that better capture generative performance and reveal the inherent UE-UT tradeoff across method families. Furthermore, we demonstrate that robustness requires finer-grained analysis: for example, vulnerabilities differ substantially between in-domain relearning and out-of-domain fine-tuning, even though both fall under model-level attacks. Through this study, we hope to deliver a full-stack revisit of LLM unlearning and actionable guidance for designing and evaluating future methods.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test-Time Matching: Unlocking Compositional Reasoning in Multimodal Models</title>
<link>https://arxiv.org/abs/2510.07632</link>
<guid>https://arxiv.org/abs/2510.07632</guid>
<content:encoded><![CDATA[
arXiv:2510.07632v1 Announce Type: cross 
Abstract: Frontier AI models have achieved remarkable progress, yet recent studies suggest they struggle with compositional reasoning, often performing at or below random chance on established benchmarks. We revisit this problem and show that widely used evaluation metrics systematically underestimate model capability. To address this, we introduce a group matching score that better exploits group structure and reveals substantial hidden capability in both contrastive vision-language models (VLMs) and multimodal large language models (MLLMs). Moreover, simply overfitting to the induced group matchings at test time transfers this hidden capability into higher scores under standard evaluation metrics, closing much of the reported gap. This adjustment enables SigLIP-B16 to surpass all previous results and GPT-4.1 to yield the first result surpassing estimated human performance on Winoground.
  Building on this insight, we propose Test-Time Matching (TTM), an iterative, self-improving algorithm that further bootstraps model performance without any external supervision. TTM delivers additional, non-trivial improvements: for example, TTM enables SigLIP-B16 to surpass GPT-4.1 on MMVP-VLM, establishing a new state of the art. Importantly, TTM remains broadly effective even on benchmarks without metric-induced effects or group structures, achieving relative gains up to 85.7% on challenging datasets such as WhatsUp. Across 16 dataset variants spanning diverse setups, our experiments demonstrate that TTM consistently improves model performance and advances the frontier of compositional reasoning.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiveThinking: Enabling Real-Time Efficient Reasoning for AI-Powered Livestreaming via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.07685</link>
<guid>https://arxiv.org/abs/2510.07685</guid>
<content:encoded><![CDATA[
arXiv:2510.07685v1 Announce Type: cross 
Abstract: In AI-powered e-commerce livestreaming, digital avatars require real-time responses to drive engagement, a task for which high-latency Large Reasoning Models (LRMs) are ill-suited. We introduce LiveThinking, a practical two-stage optimization framework to bridge this gap. First, we address computational cost by distilling a 670B teacher LRM into a lightweight 30B Mixture-of-Experts (MoE) model (3B active) using Rejection Sampling Fine-Tuning (RFT). This reduces deployment overhead but preserves the teacher's verbose reasoning, causing latency. To solve this, our second stage employs reinforcement learning with Group Relative Policy Optimization (GRPO) to compress the model's reasoning path, guided by a multi-objective reward function balancing correctness, helpfulness, and brevity. LiveThinking achieves a 30-fold reduction in computational cost, enabling sub-second latency. In real-world application on Taobao Live, it improved response correctness by 3.3% and helpfulness by 21.8%. Tested by hundreds of thousands of viewers, our system led to a statistically significant increase in Gross Merchandise Volume (GMV), demonstrating its effectiveness in enhancing user experience and commercial performance in live, interactive settings.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Safety Evaluation in Generative Agent Social Simulations</title>
<link>https://arxiv.org/abs/2510.07709</link>
<guid>https://arxiv.org/abs/2510.07709</guid>
<content:encoded><![CDATA[
arXiv:2510.07709v1 Announce Type: cross 
Abstract: Can generative agents be trusted in multimodal environments? Despite advances in large language and vision-language models that enable agents to act autonomously and pursue goals in rich settings, their ability to reason about safety, coherence, and trust across modalities remains limited. We introduce a reproducible simulation framework for evaluating agents along three dimensions: (1) safety improvement over time, including iterative plan revisions in text-visual scenarios; (2) detection of unsafe activities across multiple categories of social situations; and (3) social dynamics, measured as interaction counts and acceptance ratios of social exchanges. Agents are equipped with layered memory, dynamic planning, multimodal perception, and are instrumented with SocialMetrics, a suite of behavioral and structural metrics that quantifies plan revisions, unsafe-to-safe conversions, and information diffusion across networks. Experiments show that while agents can detect direct multimodal contradictions, they often fail to align local revisions with global safety, reaching only a 55 percent success rate in correcting unsafe plans. Across eight simulation runs with three models - Claude, GPT-4o mini, and Qwen-VL - five agents achieved average unsafe-to-safe conversion rates of 75, 55, and 58 percent, respectively. Overall performance ranged from 20 percent in multi-risk scenarios with GPT-4o mini to 98 percent in localized contexts such as fire/heat with Claude. Notably, 45 percent of unsafe actions were accepted when paired with misleading visuals, showing a strong tendency to overtrust images. These findings expose critical limitations in current architectures and provide a reproducible platform for studying multimodal safety, coherence, and social dynamics.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who Stole Your Data? A Method for Detecting Unauthorized RAG Theft</title>
<link>https://arxiv.org/abs/2510.07728</link>
<guid>https://arxiv.org/abs/2510.07728</guid>
<content:encoded><![CDATA[
arXiv:2510.07728v1 Announce Type: cross 
Abstract: Retrieval-augmented generation (RAG) enhances Large Language Models (LLMs) by mitigating hallucinations and outdated information issues, yet simultaneously facilitates unauthorized data appropriation at scale. This paper addresses this challenge through two key contributions. First, we introduce RPD, a novel dataset specifically designed for RAG plagiarism detection that encompasses diverse professional domains and writing styles, overcoming limitations in existing resources. Second, we develop a dual-layered watermarking system that embeds protection at both semantic and lexical levels, complemented by an interrogator-detective framework that employs statistical hypothesis testing on accumulated evidence. Extensive experimentation demonstrates our approach's effectiveness across varying query volumes, defense prompts, and retrieval parameters, while maintaining resilience against adversarial evasion techniques. This work establishes a foundational framework for intellectual property protection in retrieval-augmented AI systems.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>oMeBench: Towards Robust Benchmarking of LLMs in Organic Mechanism Elucidation and Reasoning</title>
<link>https://arxiv.org/abs/2510.07731</link>
<guid>https://arxiv.org/abs/2510.07731</guid>
<content:encoded><![CDATA[
arXiv:2510.07731v1 Announce Type: cross 
Abstract: Organic reaction mechanisms are the stepwise elementary reactions by which reactants form intermediates and products, and are fundamental to understanding chemical reactivity and designing new molecules and reactions. Although large language models (LLMs) have shown promise in understanding chemical tasks such as synthesis design, it is unclear to what extent this reflects genuine chemical reasoning capabilities, i.e., the ability to generate valid intermediates, maintain chemical consistency, and follow logically coherent multi-step pathways. We address this by introducing oMeBench, the first large-scale, expert-curated benchmark for organic mechanism reasoning in organic chemistry. It comprises over 10,000 annotated mechanistic steps with intermediates, type labels, and difficulty ratings. Furthermore, to evaluate LLM capability more precisely and enable fine-grained scoring, we propose oMeS, a dynamic evaluation framework that combines step-level logic and chemical similarity. We analyze the performance of state-of-the-art LLMs, and our results show that although current models display promising chemical intuition, they struggle with correct and consistent multi-step reasoning. Notably, we find that using prompting strategy and fine-tuning a specialist model on our proposed dataset increases performance by 50% over the leading closed-source model. We hope that oMeBench will serve as a rigorous foundation for advancing AI systems toward genuine chemical reasoning.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Keywords to Clusters: AI-Driven Analysis of YouTube Comments to Reveal Election Issue Salience in 2024</title>
<link>https://arxiv.org/abs/2510.07821</link>
<guid>https://arxiv.org/abs/2510.07821</guid>
<content:encoded><![CDATA[
arXiv:2510.07821v1 Announce Type: cross 
Abstract: This paper aims to explore two competing data science methodologies to attempt answering the question, "Which issues contributed most to voters' choice in the 2024 presidential election?" The methodologies involve novel empirical evidence driven by artificial intelligence (AI) techniques. By using two distinct methods based on natural language processing and clustering analysis to mine over eight thousand user comments on election-related YouTube videos from one right leaning journal, Wall Street Journal, and one left leaning journal, New York Times, during pre-election week, we quantify the frequency of selected issue areas among user comments to infer which issues were most salient to potential voters in the seven days preceding the November 5th election. Empirically, we primarily demonstrate that immigration and democracy were the most frequently and consistently invoked issues in user comments on the analyzed YouTube videos, followed by the issue of identity politics, while inflation was significantly less frequently referenced. These results corroborate certain findings of post-election surveys but also refute the supposed importance of inflation as an election issue. This indicates that variations on opinion mining, with their analysis of raw user data online, can be more revealing than polling and surveys for analyzing election outcomes.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaDefense: Defending Finetuning-based Jailbreak Attack Before and During Generation</title>
<link>https://arxiv.org/abs/2510.07835</link>
<guid>https://arxiv.org/abs/2510.07835</guid>
<content:encoded><![CDATA[
arXiv:2510.07835v1 Announce Type: cross 
Abstract: This paper introduces MetaDefense, a novel framework for defending against finetuning-based jailbreak attacks in large language models (LLMs). We observe that existing defense mechanisms fail to generalize to harmful queries disguised by unseen attack templates, despite LLMs being capable of distinguishing disguised harmful queries in the embedding space. Based on these insights, we propose a two-stage defense approach: (i) pre-generation defense that detects harmful queries before response generation begins, and (ii) mid-generation defense that monitors partial responses during generation to prevent outputting more harmful content. Our MetaDefense trains the LLM to predict the harmfulness of both queries and partial responses using specialized prompts, enabling early termination of potentially harmful interactions. Extensive experiments across multiple LLM architectures (LLaMA-2-7B, Qwen-2.5-3B-Instruct, and LLaMA-3.2-3B-Instruct) demonstrate that MetaDefense significantly outperforms existing defense mechanisms, achieving robust defense against harmful queries with seen and unseen attack templates while maintaining competitive performance on benign tasks. Code is available at https://github.com/ws-jiang/MetaDefense.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Improving LLM Agents at Test-Time</title>
<link>https://arxiv.org/abs/2510.07841</link>
<guid>https://arxiv.org/abs/2510.07841</guid>
<content:encoded><![CDATA[
arXiv:2510.07841v1 Announce Type: cross 
Abstract: One paradigm of language model (LM) fine-tuning relies on creating large training datasets, under the assumption that high quantity and diversity will enable models to generalize to novel tasks after post-training. In practice, gathering large sets of data is inefficient, and training on them is prohibitively expensive; worse, there is no guarantee that the resulting model will handle complex scenarios or generalize better. Moreover, existing techniques rarely assess whether a training sample provides novel information or is redundant with the knowledge already acquired by the model, resulting in unnecessary costs. In this work, we explore a new test-time self-improvement method to create more effective and generalizable agentic LMs on-the-fly. The proposed algorithm can be summarized in three steps: (i) first it identifies the samples that model struggles with (self-awareness), (ii) then generates similar examples from detected uncertain samples (self-data augmentation), and (iii) uses these newly generated samples at test-time fine-tuning (self-improvement). We study two variants of this approach: Test-Time Self-Improvement (TT-SI), where the same model generates additional training examples from its own uncertain cases and then learns from them, and contrast this approach with Test-Time Distillation (TT-D), where a stronger model generates similar examples for uncertain cases, enabling student to adapt using distilled supervision. Empirical evaluations across different agent benchmarks demonstrate that TT-SI improves the performance with +5.48% absolute accuracy gain on average across all benchmarks and surpasses other standard learning methods, yet using 68x less training samples. Our findings highlight the promise of TT-SI, demonstrating the potential of self-improvement algorithms at test-time as a new paradigm for building more capable agents toward self-evolution.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TTOM: Test-Time Optimization and Memorization for Compositional Video Generation</title>
<link>https://arxiv.org/abs/2510.07940</link>
<guid>https://arxiv.org/abs/2510.07940</guid>
<content:encoded><![CDATA[
arXiv:2510.07940v1 Announce Type: cross 
Abstract: Video Foundation Models (VFMs) exhibit remarkable visual generation performance, but struggle in compositional scenarios (e.g., motion, numeracy, and spatial relation). In this work, we introduce Test-Time Optimization and Memorization (TTOM), a training-free framework that aligns VFM outputs with spatiotemporal layouts during inference for better text-image alignment. Rather than direct intervention to latents or attention per-sample in existing work, we integrate and optimize new parameters guided by a general layout-attention objective. Furthermore, we formulate video generation within a streaming setting, and maintain historical optimization contexts with a parametric memory mechanism that supports flexible operations, such as insert, read, update, and delete. Notably, we found that TTOM disentangles compositional world knowledge, showing powerful transferability and generalization. Experimental results on the T2V-CompBench and Vbench benchmarks establish TTOM as an effective, practical, scalable, and efficient framework to achieve cross-modal alignment for compositional video generation on the fly.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VoiceAgentBench: Are Voice Assistants ready for agentic tasks?</title>
<link>https://arxiv.org/abs/2510.07978</link>
<guid>https://arxiv.org/abs/2510.07978</guid>
<content:encoded><![CDATA[
arXiv:2510.07978v1 Announce Type: cross 
Abstract: Large-scale Speech Language Models (SpeechLMs) have enabled voice assistants capable of understanding natural spoken queries and performing complex tasks. However, existing speech benchmarks primarily focus on isolated capabilities such as transcription, or question-answering, and do not systematically evaluate agentic scenarios encompassing multilingual and cultural understanding, as well as adversarial robustness. To address this, we introduce VoiceAgentBench, a comprehensive benchmark designed to evaluate SpeechLMs in realistic spoken agentic settings. It comprises over 5,500 synthetic spoken queries, including dialogues grounded in Indian context, covering single-tool invocations, multi-tool workflows, multi-turn interactions, and safety evaluations. The benchmark supports English, Hindi, and 5 other Indian languages, reflecting real-world linguistic and cultural diversity. We simulate speaker variability using a novel sampling algorithm that selects audios for TTS voice conversion based on its speaker embeddings, maximizing acoustic and speaker diversity. Our evaluation measures tool selection accuracy, structural consistency, and the correctness of tool invocations, including adversarial robustness. Our experiments reveal significant gaps in contextual tool orchestration tasks, Indic generalization, and adversarial robustness, exposing critical limitations of current SpeechLMs.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pseudo2Real: Task Arithmetic for Pseudo-Label Correction in Automatic Speech Recognition</title>
<link>https://arxiv.org/abs/2510.08047</link>
<guid>https://arxiv.org/abs/2510.08047</guid>
<content:encoded><![CDATA[
arXiv:2510.08047v1 Announce Type: cross 
Abstract: Robust ASR under domain shift is crucial because real-world systems encounter unseen accents and domains with limited labeled data. Although pseudo-labeling offers a practical workaround, it often introduces systematic, accent-specific errors that filtering fails to fix. We ask: How can we correct these recurring biases without target ground truth? We propose a simple parameter-space correction: in a source domain containing both real and pseudo-labeled data, two ASR models are fine-tuned from the same initialization, one on ground-truth labels and the other on pseudo-labels, and their weight difference forms a correction vector that captures pseudo-label biases. When applied to a pseudo-labeled target model, this vector enhances recognition, achieving up to a 35% relative Word Error Rate (WER) reduction on AfriSpeech-200 across ten African accents with the Whisper tiny model.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TaoSR-AGRL: Adaptive Guided Reinforcement Learning Framework for E-commerce Search Relevance</title>
<link>https://arxiv.org/abs/2510.08048</link>
<guid>https://arxiv.org/abs/2510.08048</guid>
<content:encoded><![CDATA[
arXiv:2510.08048v1 Announce Type: cross 
Abstract: Query-product relevance prediction is fundamental to e-commerce search and has become even more critical in the era of AI-powered shopping, where semantic understanding and complex reasoning directly shape the user experience and business conversion. Large Language Models (LLMs) enable generative, reasoning-based approaches, typically aligned via supervised fine-tuning (SFT) or preference optimization methods like Direct Preference Optimization (DPO). However, the increasing complexity of business rules and user queries exposes the inability of existing methods to endow models with robust reasoning capacity for long-tail and challenging cases. Efforts to address this via reinforcement learning strategies like Group Relative Policy Optimization (GRPO) often suffer from sparse terminal rewards, offering insufficient guidance for multi-step reasoning and slowing convergence. To address these challenges, we propose TaoSR-AGRL, an Adaptive Guided Reinforcement Learning framework for LLM-based relevance prediction in Taobao Search Relevance. TaoSR-AGRL introduces two key innovations: (1) Rule-aware Reward Shaping, which decomposes the final relevance judgment into dense, structured rewards aligned with domain-specific relevance criteria; and (2) Adaptive Guided Replay, which identifies low-accuracy rollouts during training and injects targeted ground-truth guidance to steer the policy away from stagnant, rule-violating reasoning patterns toward compliant trajectories. TaoSR-AGRL was evaluated on large-scale real-world datasets and through online side-by-side human evaluations on Taobao Search. It consistently outperforms DPO and standard GRPO baselines in offline experiments, improving relevance accuracy, rule adherence, and training stability. The model trained with TaoSR-AGRL has been successfully deployed in the main search scenario on Taobao, serving hundreds of millions of users.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoQual: An LLM Agent for Automated Discovery of Interpretable Features for Review Quality Assessment</title>
<link>https://arxiv.org/abs/2510.08081</link>
<guid>https://arxiv.org/abs/2510.08081</guid>
<content:encoded><![CDATA[
arXiv:2510.08081v1 Announce Type: cross 
Abstract: Ranking online reviews by their intrinsic quality is a critical task for e-commerce platforms and information services, impacting user experience and business outcomes. However, quality is a domain-dependent and dynamic concept, making its assessment a formidable challenge. Traditional methods relying on hand-crafted features are unscalable across domains and fail to adapt to evolving content patterns, while modern deep learning approaches often produce black-box models that lack interpretability and may prioritize semantics over quality. To address these challenges, we propose AutoQual, an LLM-based agent framework that automates the discovery of interpretable features. While demonstrated on review quality assessment, AutoQual is designed as a general framework for transforming tacit knowledge embedded in data into explicit, computable features. It mimics a human research process, iteratively generating feature hypotheses through reflection, operationalizing them via autonomous tool implementation, and accumulating experience in a persistent memory. We deploy our method on a large-scale online platform with a billion-level user base. Large-scale A/B testing confirms its effectiveness, increasing average reviews viewed per user by 0.79% and the conversion rate of review readers by 0.27%.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VersionRAG: Version-Aware Retrieval-Augmented Generation for Evolving Documents</title>
<link>https://arxiv.org/abs/2510.08109</link>
<guid>https://arxiv.org/abs/2510.08109</guid>
<content:encoded><![CDATA[
arXiv:2510.08109v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) systems fail when documents evolve through versioning-a ubiquitous characteristic of technical documentation. Existing approaches achieve only 58-64% accuracy on version-sensitive questions, retrieving semantically similar content without temporal validity checks. We present VersionRAG, a version-aware RAG framework that explicitly models document evolution through a hierarchical graph structure capturing version sequences, content boundaries, and changes between document states. During retrieval, VersionRAG routes queries through specialized paths based on intent classification, enabling precise version-aware filtering and change tracking. On our VersionQA benchmark-100 manually curated questions across 34 versioned technical documents-VersionRAG achieves 90% accuracy, outperforming naive RAG (58%) and GraphRAG (64%). VersionRAG reaches 60% accuracy on implicit change detection where baselines fail (0-10%), demonstrating its ability to track undocumented modifications. Additionally, VersionRAG requires 97% fewer tokens during indexing than GraphRAG, making it practical for large-scale deployment. Our work establishes versioned document QA as a distinct task and provides both a solution and benchmark for future research.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Risk-taking AI-Assistants suitably represent entities</title>
<link>https://arxiv.org/abs/2510.08114</link>
<guid>https://arxiv.org/abs/2510.08114</guid>
<content:encoded><![CDATA[
arXiv:2510.08114v1 Announce Type: cross 
Abstract: Responsible AI demands systems whose behavioral tendencies can be effectively measured, audited, and adjusted to prevent inadvertently nudging users toward risky decisions or embedding hidden biases in risk aversion. As language models (LMs) are increasingly incorporated into AI-driven decision support systems, understanding their risk behaviors is crucial for their responsible deployment. This study investigates the manipulability of risk aversion (MoRA) in LMs, examining their ability to replicate human risk preferences across diverse economic scenarios, with a focus on gender-specific attitudes, uncertainty, role-based decision-making, and the manipulability of risk aversion. The results indicate that while LMs such as DeepSeek Reasoner and Gemini-2.0-flash-lite exhibit some alignment with human behaviors, notable discrepancies highlight the need to refine bio-centric measures of manipulability. These findings suggest directions for refining AI design to better align human and AI risk preferences and enhance ethical decision-making. The study calls for further advancements in model design to ensure that AI systems more accurately replicate human risk preferences, thereby improving their effectiveness in risk management contexts. This approach could enhance the applicability of AI assistants in managing risk.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NavSpace: How Navigation Agents Follow Spatial Intelligence Instructions</title>
<link>https://arxiv.org/abs/2510.08173</link>
<guid>https://arxiv.org/abs/2510.08173</guid>
<content:encoded><![CDATA[
arXiv:2510.08173v1 Announce Type: cross 
Abstract: Instruction-following navigation is a key step toward embodied intelligence. Prior benchmarks mainly focus on semantic understanding but overlook systematically evaluating navigation agents' spatial perception and reasoning capabilities. In this work, we introduce the NavSpace benchmark, which contains six task categories and 1,228 trajectory-instruction pairs designed to probe the spatial intelligence of navigation agents. On this benchmark, we comprehensively evaluate 22 navigation agents, including state-of-the-art navigation models and multimodal large language models. The evaluation results lift the veil on spatial intelligence in embodied navigation. Furthermore, we propose SNav, a new spatially intelligent navigation model. SNav outperforms existing navigation agents on NavSpace and real robot tests, establishing a strong baseline for future work.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R-Horizon: How Far Can Your Large Reasoning Model Really Go in Breadth and Depth?</title>
<link>https://arxiv.org/abs/2510.08189</link>
<guid>https://arxiv.org/abs/2510.08189</guid>
<content:encoded><![CDATA[
arXiv:2510.08189v1 Announce Type: cross 
Abstract: Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1, DeepSeek-R1) have led to remarkable improvements through long Chain-of-Thought (CoT). However, existing benchmarks mainly focus on immediate, single-horizon tasks, failing to adequately evaluate models' ability to understand and respond to complex, long-horizon scenarios. To address this incomplete evaluation of Large Reasoning Models (LRMs), we propose R-HORIZON, a method designed to stimulate long-horizon reasoning behaviors in LRMs through query composition. Based on R-HORIZON, we construct a long-horizon reasoning benchmark, comprising complex multi-step reasoning tasks with interdependent problems that span long reasoning horizons. Through comprehensive evaluation of LRMs using the R-HORIZON benchmark, we find that even the most advanced LRMs suffer significant performance degradation. Our analysis reveals that LRMs exhibit limited effective reasoning length and struggle to allocate thinking budget across multiple problems appropriately. Recognizing these limitations, we use R-HORIZON to construct long-horizon reasoning data for reinforcement learning with verified rewards (RLVR). Compared to training with single-horizon data, RLVR with R-HORIZON not only substantially improves performance on the multi-horizon reasoning tasks, but also promotes accuracy on standard reasoning tasks, with an increase of 7.5 on AIME2024. These results position R-HORIZON as a scalable, controllable, and low-cost paradigm for enhancing and evaluating the long-horizon reasoning capabilities of LRMs.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sentiment Matters: An Analysis of 200 Human-SAV Interactions</title>
<link>https://arxiv.org/abs/2510.08202</link>
<guid>https://arxiv.org/abs/2510.08202</guid>
<content:encoded><![CDATA[
arXiv:2510.08202v1 Announce Type: cross 
Abstract: Shared Autonomous Vehicles (SAVs) are likely to become an important part of the transportation system, making effective human-SAV interactions an important area of research. This paper introduces a dataset of 200 human-SAV interactions to further this area of study. We present an open-source human-SAV conversational dataset, comprising both textual data (e.g., 2,136 human-SAV exchanges) and empirical data (e.g., post-interaction survey results on a range of psychological factors). The dataset's utility is demonstrated through two benchmark case studies: First, using random forest modeling and chord diagrams, we identify key predictors of SAV acceptance and perceived service quality, highlighting the critical influence of response sentiment polarity (i.e., perceived positivity). Second, we benchmark the performance of an LLM-based sentiment analysis tool against the traditional lexicon-based TextBlob method. Results indicate that even simple zero-shot LLM prompts more closely align with user-reported sentiment, though limitations remain. This study provides novel insights for designing conversational SAV interfaces and establishes a foundation for further exploration into advanced sentiment modeling, adaptive user interactions, and multimodal conversational systems.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReasonEmbed: Enhanced Text Embeddings for Reasoning-Intensive Document Retrieval</title>
<link>https://arxiv.org/abs/2510.08252</link>
<guid>https://arxiv.org/abs/2510.08252</guid>
<content:encoded><![CDATA[
arXiv:2510.08252v1 Announce Type: cross 
Abstract: In this paper, we introduce ReasonEmbed, a novel text embedding model developed for reasoning-intensive document retrieval. Our work includes three key technical contributions. First, we propose ReMixer, a new data synthesis method that overcomes the triviality problem prevalent in previous synthetic datasets, enabling large-scale production of 82K high-quality training samples. Second, we design Redapter, a self-adaptive learning algorithm that dynamically adjusts training each sample's weight based on its reasoning intensity. This allows the model to effectively capture the complex semantic relationships between queries and documents. Third, we implement ReasonEmbed across multiple backbones of varying sizes, all of which achieve superior performance on reasoning-intensive retrieval tasks. Notably, our ReasonEmbed-Qwen3-8B model offers a record-high nDCG@10 score of 38.1 on the BRIGHT benchmark, which significantly outperforms existing text embedding models. We will fully open-source our created resources in ReasonEmbed to push forward the research advancement in this field.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opponent Shaping in LLM Agents</title>
<link>https://arxiv.org/abs/2510.08255</link>
<guid>https://arxiv.org/abs/2510.08255</guid>
<content:encoded><![CDATA[
arXiv:2510.08255v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly being deployed as autonomous agents in real-world environments. As these deployments scale, multi-agent interactions become inevitable, making it essential to understand strategic behavior in such systems. A central open question is whether LLM agents, like reinforcement learning agents, can shape the learning dynamics and influence the behavior of others through interaction alone. In this paper, we present the first investigation of opponent shaping (OS) with LLM-based agents. Existing OS algorithms cannot be directly applied to LLMs, as they require higher-order derivatives, face scalability constraints, or depend on architectural components that are absent in transformers. To address this gap, we introduce ShapeLLM, an adaptation of model-free OS methods tailored for transformer-based agents. Using ShapeLLM, we examine whether LLM agents can influence co-players' learning dynamics across diverse game-theoretic environments. We demonstrate that LLM agents can successfully guide opponents toward exploitable equilibria in competitive games (Iterated Prisoner's Dilemma, Matching Pennies, and Chicken) and promote coordination and improve collective welfare in cooperative games (Iterated Stag Hunt and a cooperative version of the Prisoner's Dilemma). Our findings show that LLM agents can both shape and be shaped through interaction, establishing opponent shaping as a key dimension of multi-agent LLM research.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mix- and MoE-DPO: A Variational Inference Approach to Direct Preference Optimization</title>
<link>https://arxiv.org/abs/2510.08256</link>
<guid>https://arxiv.org/abs/2510.08256</guid>
<content:encoded><![CDATA[
arXiv:2510.08256v1 Announce Type: cross 
Abstract: Direct Preference Optimization (DPO) has recently emerged as a simple and effective alternative to reinforcement learning from human feedback (RLHF) for aligning large language models (LLMs) with user preferences. However, existing DPO formulations rely on a single monolithic model, which limits their expressivity in multi-task settings and their adaptability to heterogeneous or diverse preference distributions. In this work, we propose Mix- and MoE-DPO, a framework that extends DPO with both soft mixture models and mixture-of-experts (MoE) architectures, using a stochastic variational inference approach. Our method introduces a latent-variable model over expert assignments and optimizes a variational evidence lower bound (ELBO), enabling stable and efficient learning of specialized expert policies from preference data. Mix- and MoE-DPO provides three key advantages over standard DPO: (i) generalization via universal function approximation through mixtures; (ii) reward and policy specialization through expert components tailored to distinct preference modes; and (iii) contextual alignment through input-dependent soft gating that enables user-specific mixture policies. Our framework supports both shared base architectures with expert-specific policy heads and fully independent expert models, allowing flexible trade-offs between parameter efficiency and specialization. We validate our approach on a variety of model sizes and multi-preference datasets, demonstrating that Mix- and MoE-DPO offers a powerful and scalable method for preference-based LLM alignment.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Pass@k: Breadth-Depth Metrics for Reasoning Boundaries</title>
<link>https://arxiv.org/abs/2510.08325</link>
<guid>https://arxiv.org/abs/2510.08325</guid>
<content:encoded><![CDATA[
arXiv:2510.08325v1 Announce Type: cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful paradigm to improve Large Language Models on reasoning tasks such as coding, math or logic. To assess the reasoning boundary (the fraction of problems a model can solve) researchers often report Pass@k at large sampling budgets. Recent results reveal a crossover phenomenon: while RLVR models outperform the base model at small k values, the base model usually outperforms them when sampling a very large number of completions. This has been interpreted as evidence that base models have a larger reasoning boundary. We argue that on tasks with discrete answer spaces, such as math with numeric outputs, Pass@k at large k reflects the increasingly higher chance of success in the limit of the number of trials rather than genuine reasoning, and can therefore be misleading. We propose Cover@tau, which measures the fraction of problems that a model can solve for which at least a tau proportion of completions are correct. Unlike Pass@k, Cover@tau captures reasoning under an explicit reliability threshold: models that rely on random guessing degrade rapidly as tau increases. We evaluate several RLVR models using Cover@tau-based metrics and illustrate how the relative rankings of popular algorithms change compared to Pass@1, offering a different perspective on reasoning boundaries.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit Rank-Wise Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2510.08396</link>
<guid>https://arxiv.org/abs/2510.08396</guid>
<content:encoded><![CDATA[
arXiv:2510.08396v1 Announce Type: cross 
Abstract: Low-Rank Adaptation (LoRA) is a widely used parameter-efficient fine-tuning method for foundation models, but it suffers from parameter interference, resulting in suboptimal performance. Although Mixture-of-Experts (MoE)-based LoRA variants show promise in mitigating intra-task correlations in single-task instruction tuning, they introduce additional router parameters and remain ineffective in multi-task model merging where inter-task interference arises. Inspired by the fly olfactory circuit, we propose FlyLoRA, an implicit MoE-based LoRA variant that introduces: (1) rank-wise expert activation in the up-projection matrix, and (2) an implicit router that unifies expert routing and down-projection, where a frozen sparse random projection matrix replaces the traditional dense trainable version. This design resolves the trade-off between intra-task decorrelation and computational efficiency by eliminating the need for an explicit router, while inherently mitigating inter-task interference due to the orthogonality property of random matrices. Extensive experiments across four domains -- general knowledge understanding, scientific question answering, mathematical reasoning, and code generation -- demonstrate consistent performance improvements over existing methods. Beyond empirical gains, FlyLoRA highlights how biological structures can inspire innovations in AI technologies. Code is available at https://github.com/gfyddha/FlyLoRA.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>xRouter: Training Cost-Aware LLMs Orchestration System via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.08439</link>
<guid>https://arxiv.org/abs/2510.08439</guid>
<content:encoded><![CDATA[
arXiv:2510.08439v1 Announce Type: cross 
Abstract: Modern LLM deployments confront a widening cost-performance spectrum: premium models deliver strong reasoning but are expensive, while lightweight models are economical yet brittle on complex tasks. Static escalation rules and keyword heuristics under-utilize this spectrum and fail to adapt across task types. We present xRouter, a tool-calling-based routing system in which a learned router can either answer directly or invoke one or more external models. The router is trained end-to-end with reinforcement learning using an explicit, cost-aware reward that encodes cost-performance trade-offs, eliminating the need for hand-engineered routing rules. Our implementation encompasses the full reinforcement learning framework, including reward and cost accounting, as well as the deployment and evaluation pipelines. Across diverse benchmarks, xRouter achieves strong cost-performance trade-offs (e.g., substantial cost reductions at comparable task completion rates), and provides empirical insights into what reliably helps learned routing and what does not, ranging from model trainability to the difficulty of eliciting sophisticated orchestration behaviors in small open models. We hope these findings and our open implementation will serve as a practical substrate for advancing learned, cost-aware LLM orchestration.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Looking to Learn: Token-wise Dynamic Gating for Low-Resource Vision-Language Modelling</title>
<link>https://arxiv.org/abs/2510.08470</link>
<guid>https://arxiv.org/abs/2510.08470</guid>
<content:encoded><![CDATA[
arXiv:2510.08470v1 Announce Type: cross 
Abstract: Training vision-language models on cognitively-plausible amounts of data requires rethinking how models integrate multimodal information. Within the constraints of the Vision track for the BabyLM Challenge 2025, we propose a lightweight decoder-based architecture with (1) token-wise dynamic gating for adaptive fusion of linguistic and visual cues, (2) feature modulation and channel attention to maximise the utility of limited visual information and (3) auxiliary contrastive objectives for visual grounding. Evaluation on five benchmarks (BLiMP, BLiMP Supplement, EWoK, Winoground and VQA) shows competitive or superior performance to multimodal baselines. More notably, our dynamic gate discovers interpretable patterns without explicit supervision, favouring visual cues for content words and linguistic cues for function words. While we identify limitations in the Challenge constraints, such as the information bottleneck created by global image embeddings and training instability from the dataset split, our findings establish dynamic gating as a powerful tool for efficient multimodal learning, offering both interpretability and performance even under severe constraints.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Visual Iconicity Challenge: Evaluating Vision-Language Models on Sign Language Form-Meaning Mapping</title>
<link>https://arxiv.org/abs/2510.08482</link>
<guid>https://arxiv.org/abs/2510.08482</guid>
<content:encoded><![CDATA[
arXiv:2510.08482v1 Announce Type: cross 
Abstract: Iconicity, the resemblance between linguistic form and meaning, is pervasive in signed languages, offering a natural testbed for visual grounding. For vision-language models (VLMs), the challenge is to recover such essential mappings from dynamic human motion rather than static context. We introduce the \textit{Visual Iconicity Challenge}, a novel video-based benchmark that adapts psycholinguistic measures to evaluate VLMs on three tasks: (i) phonological sign-form prediction (e.g., handshape, location), (ii) transparency (inferring meaning from visual form), and (iii) graded iconicity ratings. We assess $13$ state-of-the-art VLMs in zero- and few-shot settings on Sign Language of the Netherlands and compare them to human baselines. On \textit{phonological form prediction}, VLMs recover some handshape and location detail but remain below human performance; on \textit{transparency}, they are far from human baselines; and only top models correlate moderately with human \textit{iconicity ratings}. Interestingly, \textit{models with stronger phonological form prediction correlate better with human iconicity judgment}, indicating shared sensitivity to visually grounded structure. Our findings validate these diagnostic tasks and motivate human-centric signals and embodied learning methods for modelling iconicity and improving visual grounding in multimodal models.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To Sink or Not to Sink: Visual Information Pathways in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2510.08510</link>
<guid>https://arxiv.org/abs/2510.08510</guid>
<content:encoded><![CDATA[
arXiv:2510.08510v1 Announce Type: cross 
Abstract: Large Vision Language Models (LVLMs) have recently emerged as powerful architectures capable of understanding and reasoning over both visual and textual information. These models typically rely on two key components: a Vision Transformer (ViT) and a Large Language Model (LLM). ViT encodes visual content into a sequence of image tokens and serves as the perceptual front-end -- the eyes of the model. In contrast, the LLM interprets these tokens to perform high-level reasoning, generates responses, and functions as the cognitive core -- the brain of the model. However, it remains unclear which visual tokens contribute most significantly to understanding and reasoning, and how effectively these signals are propagated from ViT to the LLM. While most existing works have focused on identifying attention sinks, low-semantic tokens receiving disproportionately high attention, within the LLM, we shift the focus to the vision encoder by identifying a class of high-norm visual tokens from ViT, referred to as ViT attention sinks -- a problem that has been rarely studied but is indeed very important for LVLMs. Our findings show that these ViT sinks encapsulate high-level semantic concepts from images, allowing the LLM to perform more effective understanding and reasoning. Despite their importance, these sink tokens are often overlooked in existing LVLM architectures. To explore their contribution, we present both qualitative and quantitative analyses of the information embedded in these sink tokens. We also propose both training-free and training-based approaches to better leverage how this information is interpreted by the LLM, and to what extent. By explicitly utilizing these tokens, we demonstrate substantial improvements across a range of LVLMs and visual reasoning tasks, highlighting the untapped potential of ViT attention sinks in enhancing visual reasoning.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoMLGen: Navigating Fine-Grained Optimization for Coding Agents</title>
<link>https://arxiv.org/abs/2510.08511</link>
<guid>https://arxiv.org/abs/2510.08511</guid>
<content:encoded><![CDATA[
arXiv:2510.08511v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown impressive performance in general programming tasks. However, in Machine Learning Engineering (MLE) scenarios such as AutoML and Kaggle competitions, achieving high performance depends heavily on expert intervention and repeated adjustments rather than simply generating correct code. When applied directly to these tasks, LLMs often lack fine-grained domain priors, and existing MLE approaches that use linear or tree-structured searches limit knowledge transfer to adjacent hierarchical links. As a result, they cannot leverage past full trajectories or share information across branches, limiting self-evolving ability and search space diversity. To address these limitations, we introduce AutoMLGen, an LLM-based coding agent that integrates a domain knowledge base for high-quality prior guidance and Monte Carlo Graph Search (MCGS) for efficient exploration. MCGS retains the tree-guided exploration of MCTS while embedding a graph structure into the expansion stage to enable dynamic path reorganization, historical trajectory reuse, and multi-solution fusion to support both self-evolution and collaborative learning. Combined with fine-grained operator sets, this design improves stability and accelerates convergence. Evaluation on the MLE-Bench shows that AutoMLGen achieves state-of-the-art performance in numerous dimensions, such as the average medal rate and the valid submission rate, under a 12-hour budget (half the standard runtime). The code is available at https://github.com/Alpha-Innovator/InternAgent.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SliceFine: The Universal Winning-Slice Hypothesis for Pretrained Networks</title>
<link>https://arxiv.org/abs/2510.08513</link>
<guid>https://arxiv.org/abs/2510.08513</guid>
<content:encoded><![CDATA[
arXiv:2510.08513v1 Announce Type: cross 
Abstract: This paper presents a theoretical framework explaining why fine tuning small, randomly selected subnetworks (slices) within pre trained models can be sufficient for downstream adaptation. We prove that pretrained networks exhibit a universal winning slice property arising from two phenomena: (1) spectral balance the eigenspectra of different weight matrix slices are remarkably similar; and (2) high task energy their backbone representations retain rich, task relevant features. This leads to the Universal Winning Slice Hypothesis, which provides a theoretical foundation for parameter efficient fine tuning (PEFT) in large scale models. Inspired by this, we propose SliceFine, a PEFT method that exploits this inherent redundancy by updating only selected slices of the original weights introducing zero new parameters, unlike adapter-based approaches. Empirically, SliceFine matches the performance of state of the art PEFT methods across language and vision tasks, while significantly improving training speed, memory efficiency, and model compactness. Our work bridges theory and practice, offering a theoretically grounded alternative to existing PEFT techniques.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CaRT: Teaching LLM Agents to Know When They Know Enough</title>
<link>https://arxiv.org/abs/2510.08517</link>
<guid>https://arxiv.org/abs/2510.08517</guid>
<content:encoded><![CDATA[
arXiv:2510.08517v1 Announce Type: cross 
Abstract: Many tasks require learned models to strategically gather relevant information over multiple rounds of interaction before actually acting on a task. Strategic information gathering requires models to know not only how to effectively acquire information, but also when to stop gathering information and make a decision, in order to avoid overthinking or getting derailed when acting. In this paper, we formalize this problem and introduce Counterfactuals and Reasoning for Termination (CaRT), an approach for teaching LLMs when to stop seeking information. To appropriately learn when to terminate, CaRT fine-tunes LLMs using counterfactual pairs of trajectories, one where termination is appropriate and a minimally modified version of the same trajectory where it is not. It trains the LLM to explain the rationale for the termination decision in either case via verbal reasoning, and imbues this capability into the base LLM via fine-tuning. We instantiate CaRT in two domains: interactive medical diagnosis and math problem solving. In both domains, we find that CaRT improves the efficiency of information gathering and task success rate compared to other fine-tuning methods.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models</title>
<link>https://arxiv.org/abs/2510.08531</link>
<guid>https://arxiv.org/abs/2510.08531</guid>
<content:encoded><![CDATA[
arXiv:2510.08531v1 Announce Type: cross 
Abstract: Spatial reasoning remains a fundamental challenge for Vision-Language Models (VLMs), with current approaches struggling to achieve robust performance despite recent advances. We identify that this limitation stems from a critical gap: existing methods attempt to learn spatial reasoning directly without establishing the hierarchical foundations of perception and understanding. To address this challenge, we present a comprehensive methodology for building spatial intelligence progressively. We introduce SpatialLadder-26k, a multimodal dataset containing 26,610 samples spanning object localization, single image, multi-view, and video spatial reasoning tasks, constructed through a standardized pipeline that ensures systematic coverage across modalities. Building on this dataset, we design a three-stage progressive training framework that (1) establishes spatial perception through object localization, (2) develops spatial understanding through multi-dimensional spatial tasks, and (3) strengthens complex reasoning via reinforcement learning with verifiable rewards. This approach yields SpatialLadder, a 3B-parameter model that achieves state-of-the-art performance on spatial reasoning benchmarks, with 23.4% average improvement over the base model, surpassing GPT-4o by 20.8% and Gemini-2.0-Flash by 10.1%. Notably, SpatialLadder maintains strong generalization with 7.2% improvement on out-of-domain benchmarks, demonstrating that progressive training from perception to reasoning is essential for robust spatial intelligence.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoNorms: Benchmarking Cultural Awareness of Video Language Models</title>
<link>https://arxiv.org/abs/2510.08543</link>
<guid>https://arxiv.org/abs/2510.08543</guid>
<content:encoded><![CDATA[
arXiv:2510.08543v1 Announce Type: cross 
Abstract: As Video Large Language Models (VideoLLMs) are deployed globally, they require understanding of and grounding in the relevant cultural background. To properly assess these models' cultural awareness, adequate benchmarks are needed. We introduce VideoNorms, a benchmark of over 1000 (video clip, norm) pairs from US and Chinese cultures annotated with socio-cultural norms grounded in speech act theory, norm adherence and violations labels, and verbal and non-verbal evidence. To build VideoNorms, we use a human-AI collaboration framework, where a teacher model using theoretically-grounded prompting provides candidate annotations and a set of trained human experts validate and correct the annotations. We benchmark a variety of open-weight VideoLLMs on the new dataset which highlight several common trends: 1) models performs worse on norm violation than adherence; 2) models perform worse w.r.t Chinese culture compared to the US culture; 3) models have more difficulty in providing non-verbal evidence compared to verbal for the norm adhere/violation label and struggle to identify the exact norm corresponding to a speech-act; and 4) unlike humans, models perform worse in formal, non-humorous contexts. Our findings emphasize the need for culturally-grounded video language model training - a gap our benchmark and framework begin to address.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent Learning via Early Experience</title>
<link>https://arxiv.org/abs/2510.08558</link>
<guid>https://arxiv.org/abs/2510.08558</guid>
<content:encoded><![CDATA[
arXiv:2510.08558v1 Announce Type: cross 
Abstract: A long-term goal of language agents is to learn and improve through their own experience, ultimately outperforming humans in complex, real-world tasks. However, training agents from experience data with reinforcement learning remains difficult in many environments, which either lack verifiable rewards (e.g., websites) or require inefficient long-horizon rollouts (e.g., multi-turn tool use). As a result, most current agents rely on supervised fine-tuning on expert data, which is challenging to scale and generalizes poorly. This limitation stems from the nature of expert demonstrations: they capture only a narrow range of scenarios and expose the agent to limited environment diversity. We address this limitation with a middle-ground paradigm we call early experience: interaction data generated by the agent's own actions, where the resulting future states serve as supervision without reward signals. Within this paradigm we study two strategies of using such data: (1) Implicit world modeling, which uses collected states to ground the policy in environment dynamics; and (2) Self-reflection, where the agent learns from its suboptimal actions to improve reasoning and decision-making. We evaluate across eight diverse environments and multiple model families. Our approaches consistently improve effectiveness and out-of-domain generalization, highlighting the value of early experience. Moreover, in environments with verifiable rewards, our results provide promising signals that early experience offers a strong foundation for subsequent reinforcement learning, positioning it as a practical bridge between imitation learning and fully experience-driven agents.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning</title>
<link>https://arxiv.org/abs/2510.08567</link>
<guid>https://arxiv.org/abs/2510.08567</guid>
<content:encoded><![CDATA[
arXiv:2510.08567v1 Announce Type: cross 
Abstract: Vision language models (VLMs) are increasingly deployed as controllers with access to external tools for complex reasoning and decision-making, yet their effectiveness remains limited by the scarcity of high-quality multimodal trajectories and the cost of manual annotation. We address this challenge with a vision-centric agent tuning framework that automatically synthesizes multimodal trajectories, generates step-wise preference pairs, and trains a VLM controller for robust tool-use reasoning. Our pipeline first constructs M-TRACE, a large-scale dataset of 28.5K multimodal tasks with 177K verified trajectories, enabling imitation-based trajectory tuning. Building on this, we develop MATRIX Agent, a controller finetuned on M-TRACE for step-wise tool reasoning. To achieve finer alignment, we further introduce Pref-X, a set of 11K automatically generated preference pairs, and optimize MATRIX on it via step-wise preference learning. Across three benchmarks, Agent-X, GTA, and GAIA, MATRIX consistently surpasses both open- and closed-source VLMs, demonstrating scalable and effective multimodal tool use. Our data and code is avaliable at https://github.com/mbzuai-oryx/MATRIX.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating LLMs' Mathematical Reasoning in Financial Document Question Answering</title>
<link>https://arxiv.org/abs/2402.11194</link>
<guid>https://arxiv.org/abs/2402.11194</guid>
<content:encoded><![CDATA[
arXiv:2402.11194v3 Announce Type: replace 
Abstract: Large Language Models (LLMs), excel in natural language understanding, but their capability for complex mathematical reasoning with an amalgamation of structured tables and unstructured text is uncertain. This study explores LLMs' mathematical reasoning on four financial tabular question-answering datasets: TATQA, FinQA, ConvFinQA, and Multihiertt. Through extensive experiments with various models and prompting techniques, we assess how LLMs adapt to complex tables and mathematical tasks. We focus on sensitivity to table complexity and performance variations with an increasing number of arithmetic reasoning steps. The results provide insights into LLMs' capabilities and limitations in handling complex mathematical scenarios for semi-structured tables. Ultimately, we introduce a novel prompting technique tailored to semi-structured documents, matching or outperforming other baselines in performance while providing a nuanced understanding of LLMs abilities for such a task.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ThinkNote: Enhancing Knowledge Integration and Utilization of Large Language Models via Constructivist Cognition Modeling</title>
<link>https://arxiv.org/abs/2402.13547</link>
<guid>https://arxiv.org/abs/2402.13547</guid>
<content:encoded><![CDATA[
arXiv:2402.13547v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated strong performance across a wide range of NLP tasks. However, they often exhibit suboptimal behaviors and inconsistencies when exposed to unfamiliar external information, underscoring their limitations in effectively leveraging such knowledge. Inspired by constructivist learning theory, we propose ThinkNote, a novel framework that enhances the external knowledge utilization of LLMs through a two-stage constructivist cognitive modeling process. Specifically, ThinkNote performs knowledge assimilation to align new information with the model's parametric memory, forming a coherent internal representation. It then applies thought accommodation to adapt internal reasoning, thereby promoting more consistent and reliable outputs. Extensive experimental results demonstrate that ThinkNote achieves a 10% improvement over strong baseline methods on various question-answering benchmarks. Further analysis indicates that ThinkNote effectively integrates and utilizes external knowledge to help LLMs generate accurate responses and improves their self-consistency. All data and codes are available at https://github.com/OpenMatch/ThinkNote.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Depression Detection on Social Media with Large Language Models</title>
<link>https://arxiv.org/abs/2403.10750</link>
<guid>https://arxiv.org/abs/2403.10750</guid>
<content:encoded><![CDATA[
arXiv:2403.10750v2 Announce Type: replace 
Abstract: Limited access to mental healthcare resources hinders timely depression diagnosis, leading to detrimental outcomes. Social media platforms present a valuable data source for early detection, yet this task faces two significant challenges: 1) the need for medical knowledge to distinguish clinical depression from transient mood changes, and 2) the dual requirement for high accuracy and model explainability. To address this, we propose DORIS, a framework that leverages Large Language Models (LLMs). To integrate medical knowledge, DORIS utilizes LLMs to annotate user texts against established medical diagnostic criteria and to summarize historical posts into temporal mood courses. These medically-informed features are then used to train an accurate Gradient Boosting Tree (GBT) classifier. Explainability is achieved by generating justifications for predictions based on the LLM-derived symptom annotations and mood course analyses. Extensive experimental results validate the effectiveness as well as interpretability of our method, highlighting its potential as a supportive clinical tool.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expert-Token Resonance MoE: Bidirectional Routing with Efficiency Affinity-Driven Active Selection</title>
<link>https://arxiv.org/abs/2406.00023</link>
<guid>https://arxiv.org/abs/2406.00023</guid>
<content:encoded><![CDATA[
arXiv:2406.00023v4 Announce Type: replace 
Abstract: Mixture-of-Experts (MoE) architectures enable efficient scaling of large language models by activating only a subset of parameters per input. However, existing MoE models suffer from two critical limitations: (1) inefficient token-to-expert routing that causes excessive communication overhead, and (2) expert homogenization that leads to redundant computations. Current approaches address these challenges separately, failing to achieve simultaneous improvements in both training efficiency and model performance. We present Expert-Token Resonance (ETR), a theoretically-grounded bidirectional routing mechanism that fundamentally reimagines expert-token interactions in MoE architectures. Our key insight is that optimal routing requires adaptive coordination between token-choice routing (TCR) during early training phases and expert-choice routing (ECR) in later stages. We prove that this dynamic approach maximizes training success rate (the probability of correct token-expert assignments) while reducing the expert capacity lower bound by up to 40%. ETR incorporates three technical innovations: (1) an affinity-based routing architecture using Grouped Average Pooling (GrAP) that reduces computational complexity from O(d^2) to O(d^2/D) while maintaining orthogonality to prevent expert homogenization; (2) a bidirectional selection mechanism that enables both tokens and experts to actively participate in the routing process based on cosine similarity scores; and (3) an adaptive capacity strategy that dynamically adjusts expert bounds based on training progress, eliminating communication bubbles in All-to-All operations. Extensive experiments on Ascend NPU clusters demonstrate that ETR achieves 5.4%-46.6% improvements in end-to-end training efficiency compared to baseline MoE implementations, with 9.7%-14.5% performance gains across GDAD, GPQA, HumanEval, and TeleQnA benchmarks.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Source Knowledge Pruning for Retrieval-Augmented Generation: A Benchmark and Empirical Study</title>
<link>https://arxiv.org/abs/2409.13694</link>
<guid>https://arxiv.org/abs/2409.13694</guid>
<content:encoded><![CDATA[
arXiv:2409.13694v4 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) is increasingly recognized as an effective approach to mitigating the hallucination of large language models (LLMs) through the integration of external knowledge. While numerous efforts, most studies focus on a single type of external knowledge source. However, in real-world applications, most situations involve diverse knowledge from various sources, yet this area has been less explored. The main dilemma is the lack of a suitable dataset containing multiple knowledge sources and pre-exploration of the associated issues. To address these challenges, we standardize a benchmark dataset that combines structured and unstructured knowledge across diverse and complementary domains. Based on this dataset, we further develop a plug-and-play RAG framework, \textbf{PruningRAG}, whose main characteristic is the use of multi-granularity pruning strategies to optimize the integration of relevant information while minimizing misleading context. It consistently improves performance across various existing RAG variants, demonstrating its robustness and broad applicability. Building upon the standardized dataset and PruningRAG, we also report a series of experimental results, as well as insightful findings. Our dataset and code are publicly available\footnote{https://github.com/USTCAGI/PruningRAG}, with the aim of advancing future research in the RAG community.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection</title>
<link>https://arxiv.org/abs/2411.02886</link>
<guid>https://arxiv.org/abs/2411.02886</guid>
<content:encoded><![CDATA[
arXiv:2411.02886v4 Announce Type: replace 
Abstract: Rapid advances in Large Language Models (LLMs) have spurred demand for processing extended context sequences in contemporary applications. However, this progress faces two challenges: performance degradation due to sequence lengths out-of-distribution, and excessively long inference times caused by the quadratic computational complexity of attention. These issues limit LLMs in long-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free method for efficient and accurate long-context inference. TokenSelect builds upon the observation of non-contiguous attention sparsity, using QK dot products to measure per-head KV Cache criticality at token-level. By per-head soft voting mechanism, TokenSelect selectively involves a few critical KV cache tokens in attention calculation without sacrificing accuracy. To further accelerate TokenSelect, we design the Selection Cache based on observations of consecutive Query similarity and implemented the efficient Paged Dot Product Kernel, significantly reducing the selection overhead. A comprehensive evaluation of TokenSelect demonstrates up to $23.84\times$ speedup in attention computation and up to $2.28\times$ acceleration in end-to-end latency, while providing superior performance compared to state-of-the-art long-context inference methods.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EpiCoder: Encompassing Diversity and Complexity in Code Generation</title>
<link>https://arxiv.org/abs/2501.04694</link>
<guid>https://arxiv.org/abs/2501.04694</guid>
<content:encoded><![CDATA[
arXiv:2501.04694v3 Announce Type: replace 
Abstract: Existing methods for code generation use code snippets as seed data, restricting the complexity and diversity of the synthesized data. In this paper, we introduce a novel feature tree-based synthesis framework, which revolves around hierarchical code features derived from high-level abstractions of code. The feature tree is constructed from raw data and refined iteratively to increase the quantity and diversity of the extracted features, which captures and recognizes more complex patterns and relationships within the code. By adjusting the depth and breadth of the sampled subtrees, our framework provides precise control over the complexity of the generated code, enabling functionalities that range from function-level operations to multi-file scenarios. We fine-tuned widely-used base models to obtain EpiCoder series, achieving state-of-the-art performance on multiple benchmarks at both the function and file levels. In particular, empirical evidence indicates that our approach shows significant potential in the synthesizing of repository-level code data. Our code and data are publicly available at https://github.com/microsoft/EpiCoder.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Med-R$^2$: Crafting Trustworthy LLM Physicians via Retrieval and Reasoning of Evidence-Based Medicine</title>
<link>https://arxiv.org/abs/2501.11885</link>
<guid>https://arxiv.org/abs/2501.11885</guid>
<content:encoded><![CDATA[
arXiv:2501.11885v5 Announce Type: replace 
Abstract: Large Language Models (LLMs) have exhibited remarkable capabilities in clinical scenarios. Despite their potential, existing works face challenges when applying LLMs to medical settings. Strategies relying on training with medical datasets are highly cost-intensive and may suffer from outdated training data. Leveraging external knowledge bases is a suitable alternative, yet it faces obstacles such as limited retrieval precision and poor effectiveness in answer extraction. These issues collectively prevent LLMs from demonstrating the expected level of proficiency in mastering medical expertise. To address these challenges, we introduce Med-R^2, a novel LLM physician framework that adheres to the Evidence-Based Medicine (EBM) process, efficiently integrating retrieval mechanisms as well as the selection and reasoning processes of evidence, thereby enhancing the problem-solving capabilities of LLMs in healthcare scenarios and fostering a trustworthy LLM physician. Our comprehensive experiments indicate that Med-R^2 achieves a 13.27\% improvement over vanilla RAG methods and even a 4.55\% enhancement compared to fine-tuning strategies, without incurring additional training costs. Furthermore, we find that our LLaMA3.1-70B + Med-R$^2$ surpasses frontier models, including GPT-4o, Claude3.5-Sonnet and DeepSeek-V3 by 1.05\%, 6.14\% and 1.91\%. Med-R$^2$ effectively enhances the capabilities of LLMs in the medical domain.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning</title>
<link>https://arxiv.org/abs/2501.14315</link>
<guid>https://arxiv.org/abs/2501.14315</guid>
<content:encoded><![CDATA[
arXiv:2501.14315v5 Announce Type: replace 
Abstract: Maintaining consistent model performance across domains is a fundamental challenge in machine learning. While recent work has explored using LLM-generated data for fine-tuning, its impact on cross-domain generalization remains poorly understood. This paper presents a systematic analysis revealing that fine-tuning with LLM-generated data not only improves target task performance but also reduces non-target task degradation compared to fine-tuning with ground truth data. Through analyzing the data sequence in tasks of various domains, we demonstrate that this enhancement of non-target task robustness stems from the reduction of high perplexity tokens found in LLM-generated sequences. Following our findings, we showed that masking high perplexity tokens in ground truth training data achieves similar non-target task performance preservation, comparable to using LLM-generated data. Extensive experiments across different model families and scales, including Gemma 2 IT 2B, Llama 3 8B Instruct, and three additional models, agree with our findings. To the best of our knowledge, this is the first work to provide an empirical explanation based on token perplexity reduction to mitigate catastrophic forgetting in LLMs after fine-tuning, offering valuable insights for developing more robust fine-tuning strategies.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Examining Multilingual Embedding Models Cross-Lingually Through LLM-Generated Adversarial Examples</title>
<link>https://arxiv.org/abs/2502.08638</link>
<guid>https://arxiv.org/abs/2502.08638</guid>
<content:encoded><![CDATA[
arXiv:2502.08638v4 Announce Type: replace 
Abstract: The evaluation of cross-lingual semantic search models is often limited to existing datasets from tasks such as information retrieval and semantic textual similarity. We introduce Cross-Lingual Semantic Discrimination (CLSD), a lightweight evaluation task that requires only parallel sentences and a Large Language Model (LLM) to generate adversarial distractors. CLSD measures an embedding model's ability to rank the true parallel sentence above semantically misleading but lexically similar alternatives. As a case study, we construct CLSD datasets for German--French in the news domain. Our experiments show that models fine-tuned for retrieval tasks benefit from pivoting through English, whereas bitext mining models perform best in direct cross-lingual settings. A fine-grained similarity analysis further reveals that embedding models differ in their sensitivity to linguistic perturbations. We release our code and datasets under AGPL-3.0: https://github.com/impresso/cross_lingual_semantic_discrimination
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Less is More: Compact Clue Selection for Efficient Retrieval-Augmented Generation Reasoning</title>
<link>https://arxiv.org/abs/2502.11811</link>
<guid>https://arxiv.org/abs/2502.11811</guid>
<content:encoded><![CDATA[
arXiv:2502.11811v5 Announce Type: replace 
Abstract: Current RAG retrievers are designed primarily for human readers, emphasizing complete, readable, and coherent paragraphs. However, LLMs benefit more from precise, compact, and well-structured input, which enhances reasoning quality and efficiency. Existing methods often rely on reranking or summarization to identify key sentences, but may suffer from semantic breaks and unfaithfulness. Thus, efficiently extracting and organizing answer-relevant clues from large-scale documents while reducing LLM reasoning costs remains a challenge for RAG. Inspired by Occam's razor, we frame LLM-centric retrieval as a MinMax optimization: maximizing the extraction of potential clues and reranking them for well-organization, while minimizing reasoning costs by truncating to the smallest sufficient clues set. In this paper, we propose CompSelect, a Compact clue Selection mechanism for LLM-centric RAG, consisting of a clue extractor, a reranker, and a truncator. (1) The clue extractor first uses answer-containing sentences as fine-tuning targets, aiming to extract sufficient potential clues; (2) The reranker is trained to prioritize effective clues based on real LLM feedback; (3) The truncator uses the truncated text containing the minimum sufficient clues for answering the question as fine-tuning targets, thereby enabling efficient RAG reasoning. Experiments on three QA datasets show that CompSelect improves QA performance by approximately 11\% and reduces Total Latency and Online Latency by approximately 17\% and 67\% compared to various baseline methods on both LLaMA3 and Qwen3. Further analysis confirms its robustness to unreliable retrieval and generalization across different scenarios, offering a scalable and cost-efficient solution for web-scale RAG applications.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoM: Linear Sequence Modeling with Mixture-of-Memories</title>
<link>https://arxiv.org/abs/2502.13685</link>
<guid>https://arxiv.org/abs/2502.13685</guid>
<content:encoded><![CDATA[
arXiv:2502.13685v3 Announce Type: replace 
Abstract: Linear sequence modeling methods, such as linear attention, state space modeling, and linear RNNs, offer significant efficiency improvements by reducing the complexity of training and inference. However, these methods typically compress the entire input sequence into a single fixed-size memory state, which leads to suboptimal performance on recall-intensive tasks. To address this limitation, we introduce a novel architecture called Mixture-of-Memories (MoM). MoM utilizes multiple independent memory states, with a router network directing input tokens to specific memory states. This approach greatly enhances the overall memory capacity while minimizing memory interference. MoM serves as a general framework that can be seamlessly combined with diverse memory update mechanisms across linear models. As a result, MoM performs exceptionally well on recall-intensive tasks, surpassing existing linear sequence modeling techniques. Despite incorporating multiple memory states, the computation of each memory state remains linear in complexity, allowing MoM to retain the linear-complexity advantage during training, while constant-complexity during inference. Our experimental results show that MoM outperforms current linear sequence models on downstream language tasks, particularly recall-intensive tasks, and even achieves performance comparable to Transformer models. The code is released at https://github.com/OpenSparseLLMs/MoM and is also released as a part of https://github.com/OpenSparseLLMs/Linear-MoE.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Single Frames: Can LMMs Comprehend Temporal and Contextual Narratives in Image Sequences?</title>
<link>https://arxiv.org/abs/2502.13925</link>
<guid>https://arxiv.org/abs/2502.13925</guid>
<content:encoded><![CDATA[
arXiv:2502.13925v2 Announce Type: replace 
Abstract: Large Multimodal Models (LMMs) have achieved remarkable success across various visual-language tasks. However, existing benchmarks predominantly focus on single-image understanding, leaving the analysis of image sequences largely unexplored. To address this limitation, we introduce StripCipher, a comprehensive benchmark designed to evaluate capabilities of LMMs to comprehend and reason over sequential images. StripCipher comprises a human-annotated dataset and three challenging subtasks: visual narrative comprehension, contextual frame prediction, and temporal narrative reordering. Our evaluation of 16 state-of-the-art LMMs, including GPT-4o and Qwen2.5VL, reveals a significant performance gap compared to human capabilities, particularly in tasks that require reordering shuffled sequential images. For instance, GPT-4o achieves only 23.93% accuracy in the reordering subtask, which is 56.07% lower than human performance. Further quantitative analysis discuss several factors, such as input format of images, affecting the performance of LLMs in sequential understanding, underscoring the fundamental challenges that remain in the development of LMMs.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Erasing Without Remembering: Implicit Knowledge Forgetting in Large Language Models</title>
<link>https://arxiv.org/abs/2502.19982</link>
<guid>https://arxiv.org/abs/2502.19982</guid>
<content:encoded><![CDATA[
arXiv:2502.19982v3 Announce Type: replace 
Abstract: In this paper, we investigate knowledge forgetting in large language models with a focus on its generalisation, ensuring that models forget not only specific training samples but also related implicit knowledge. To this end, we begin by identifying a broader unlearning scope that includes both target data and logically associated samples, including rephrased, subject-replaced, relation-reversed, and one-hop reasoned data. We then conduct a rigorous evaluation of 15 state-of-the-art methods across three datasets, revealing that unlearned models still recall paraphrased answers and retain target facts in their intermediate layers. This motivates us to take a preliminary step toward more generalised implicit knowledge forgetting by proposing PerMU, a novel probability perturbation-based unlearning paradigm. PerMU simulates adversarial unlearning samples to eliminate fact-related tokens from the logit distribution, collectively reducing the probabilities of all answer-associated tokens. Experiments are conducted on a diverse range of datasets, including TOFU, Harry Potter, ZsRE, WMDP, and MUSE, using models ranging from 1.3B to 13B in scale. The results demonstrate that PerMU delivers up to a 50.40% improvement in unlearning vanilla target data while maintaining a 40.73% boost in forgetting implicit knowledge. Our code can be found in https://github.com/MaybeLizzy/PERMU.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Argument Summarization and its Evaluation in the Era of Large Language Models</title>
<link>https://arxiv.org/abs/2503.00847</link>
<guid>https://arxiv.org/abs/2503.00847</guid>
<content:encoded><![CDATA[
arXiv:2503.00847v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) have revolutionized various Natural Language Generation (NLG) tasks, including Argument Summarization (ArgSum), a key subfield of Argument Mining. This paper investigates the integration of state-of-the-art LLMs into ArgSum systems and their evaluation. In particular, we propose a novel prompt-based evaluation scheme, and validate it through a novel human benchmark dataset. Our work makes three main contributions: (i) the integration of LLMs into existing ArgSum systems, (ii) the development of two new LLM-based ArgSum systems, benchmarked against prior methods, and (iii) the introduction of an advanced LLM-based evaluation scheme. We demonstrate that the use of LLMs substantially improves both the generation and evaluation of argument summaries, achieving state-of-the-art results and advancing the field of ArgSum. We also show that among the four LLMs integrated in (i) and (ii), Qwen-3-32B, despite having the fewest parameters, performs best, even surpassing GPT-4o.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sherkala-Chat: Building a State-of-the-Art LLM for Kazakh in a Moderately Resourced Setting</title>
<link>https://arxiv.org/abs/2503.01493</link>
<guid>https://arxiv.org/abs/2503.01493</guid>
<content:encoded><![CDATA[
arXiv:2503.01493v2 Announce Type: replace 
Abstract: Llama-3.1-Sherkala-8B-Chat, or Sherkala-Chat (8B) for short, is a state-of-the-art instruction-tuned open generative large language model (LLM) designed for Kazakh. Sherkala-Chat (8B) aims to enhance the inclusivity of LLM advancements for Kazakh speakers. Adapted from the LLaMA-3.1-8B model, Sherkala-Chat (8B) is trained on 45.3B tokens across Kazakh, English, Russian, and Turkish. With 8 billion parameters, it demonstrates strong knowledge and reasoning abilities in Kazakh, significantly outper-forming existing open Kazakh and multilingual models of similar scale while achieving competitive performance in English. To ensure effective and responsible alignment, we leverage translated instruction datasets, a Kazakhstan-specific instruction dataset that is automatically constructed and manually verified, and Kazakh-specific safety data. We release Sherkala-Chat (8B) as an open-weight model, along with a detailed description of its training, alignment, and evaluation, to support research and real-world applications for Kazakh speakers.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing LLM Reliability via Explicit Knowledge Boundary Modeling</title>
<link>https://arxiv.org/abs/2503.02233</link>
<guid>https://arxiv.org/abs/2503.02233</guid>
<content:encoded><![CDATA[
arXiv:2503.02233v4 Announce Type: replace 
Abstract: Large language models (LLMs) are prone to hallucination stemming from misaligned self-awareness, particularly when processing queries exceeding their knowledge boundaries. While existing mitigation strategies employ uncertainty estimation or query rejection mechanisms, they suffer from computational efficiency and sacrificed helpfulness. To address these issues, we propose the Explicit Knowledge Boundary Modeling (EKBM) framework, integrating fast and slow reasoning systems to harmonize reliability and usability. The framework first employs a fast-thinking model to generate confidence-labeled responses, enabling immediate utilization of high-confidence outputs, whereas uncertain predictions trigger a slow refinement model for accuracy improvement. To align model behavior with our proposed object, we propose a hybrid training pipeline, enhancing self-awareness without degrading task performance. Evaluations on dialogue state tracking tasks demonstrate that EKBM achieves superior model reliability over uncertainty-based baselines. Further analysis reveals that refinement substantially boosts accuracy while maintaining low computational overhead. The framework establishes a scalable paradigm for deploying reliable LLMs in error-sensitive applications, effectively balancing accuracy and practical utility.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching Your Models to Understand Code via Focal Preference Alignment</title>
<link>https://arxiv.org/abs/2503.02783</link>
<guid>https://arxiv.org/abs/2503.02783</guid>
<content:encoded><![CDATA[
arXiv:2503.02783v4 Announce Type: replace 
Abstract: Preference learning extends the performance of Code LLMs beyond traditional supervised fine-tuning by leveraging relative quality comparisons. In existing approaches, a set of n candidate solutions is evaluated based on test case success rates, with the candidate demonstrating a higher pass rate being labeled as positive and its counterpart with a lower pass rate as negative. However, because this approach aligns entire failing code blocks rather than pinpointing specific errors, it lacks the granularity necessary to capture meaningful error-correction relationships. As a result, the model is unable to learn more informative error-correction patterns. To address these issues, we propose Target-DPO, a new preference alignment framework that mimics human iterative debugging to refine Code LLMs. Target-DPO explicitly locates error regions and aligns the corresponding tokens via a tailored DPO algorithm. To facilitate it, we introduce the CodeFlow dataset, where samples are iteratively refined until passing tests, with modifications capturing error corrections. Extensive experiments show that a diverse suite of Code LLMs equipped with Target-DPO achieves significant performance gains in code generation and improves on challenging tasks like BigCodeBench. In-depth analysis reveals that Target-DPO yields fewer errors. Code, model and datasets are in: https://github.com/JieWu02/Target-DPO.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiMA: An LLM-Powered Ride-Hailing Assistant at DiDi</title>
<link>https://arxiv.org/abs/2503.04768</link>
<guid>https://arxiv.org/abs/2503.04768</guid>
<content:encoded><![CDATA[
arXiv:2503.04768v3 Announce Type: replace 
Abstract: On-demand ride-hailing services like DiDi, Uber, and Lyft have transformed urban transportation, offering unmatched convenience and flexibility. In this paper, we introduce DiMA, an LLM-powered ride-hailing assistant deployed in DiDi Chuxing. Its goal is to provide seamless ride-hailing services and beyond through a natural and efficient conversational interface under dynamic and complex spatiotemporal urban contexts. To achieve this, we propose a spatiotemporal-aware order planning module that leverages external tools for precise spatiotemporal reasoning and progressive order planning. Additionally, we develop a cost-effective dialogue system that integrates multi-type dialog repliers with cost-aware LLM configurations to handle diverse conversation goals and trade-off response quality and latency. Furthermore, we introduce a continual fine-tuning scheme that utilizes real-world interactions and simulated dialogues to align the assistant's behavior with human preferred decision-making processes. Since its deployment in the DiDi application, DiMA has demonstrated exceptional performance, achieving 93% accuracy in order planning and 92% in response generation during real-world interactions. Offline experiments further validate DiMA capabilities, showing improvements of up to 70.23% in order planning and 321.27% in response generation compared to three state-of-the-art agent frameworks, while reducing latency by $0.72\times$ to $5.47\times$. These results establish DiMA as an effective, efficient, and intelligent mobile assistant for ride-hailing services. Our project is released at https://github.com/usail-hkust/DiMA and we also release the MCP service (https://mcp.didichuxing.com/api) to foster the ride-hailing research community.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniEDU: A Unified Language and Vision Assistant for Education Applications</title>
<link>https://arxiv.org/abs/2503.20701</link>
<guid>https://arxiv.org/abs/2503.20701</guid>
<content:encoded><![CDATA[
arXiv:2503.20701v2 Announce Type: replace 
Abstract: Education materials for K-12 students often consist of multiple modalities, such as text and images, posing challenges for models to fully understand nuanced information in these materials. In this paper, we propose a unified language and vision assistant UniEDU designed for various educational applications, including knowledge recommendation, knowledge tracing, time cost prediction, and user answer prediction, all within a single model. Unlike conventional task-specific models, UniEDU offers a unified solution that excels across multiple educational tasks while maintaining strong generalization capabilities. Its adaptability makes it well-suited for real-world deployment in diverse learning environments. Furthermore, UniEDU is optimized for industry-scale deployment by significantly reducing computational overhead-achieving approximately a 300\% increase in efficiency-while maintaining competitive performance with minimal degradation compared to fully fine-tuned models. This work represents a significant step toward creating versatile AI systems tailored to the evolving demands of education.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Layer-skipping in Pre-trained LLMs</title>
<link>https://arxiv.org/abs/2503.23798</link>
<guid>https://arxiv.org/abs/2503.23798</guid>
<content:encoded><![CDATA[
arXiv:2503.23798v3 Announce Type: replace 
Abstract: Various layer-skipping methods have been proposed to accelerate token generation in large language models (LLMs). However, limited attention has been paid to a fundamental question: How do computational demands vary across the generation of different tokens? In this work, we introduce FlexiDepth, a method that dynamically adjusts the number of Transformer layers used in text generation. By incorporating a plug-in router and adapter, FlexiDepth enables adaptive computation in LLMs without modifying their original parameters. Applied to Llama-3-8B, it skips 8 out of 32 layers while maintaining full benchmark performance. Our experiments reveal that computational demands in LLMs significantly vary based on token type. Specifically, generating repetitive tokens or fixed phrases requires fewer layers, whereas producing tokens involving computation or high uncertainty requires more layers. Despite the computational savings, FlexiDepth does not yet achieve wall-clock speedup due to varied skipping patterns and I/O overhead. To inspire future work and advance research on practical speedup, we open-sourced FlexiDepth and a dataset documenting its layer allocation patterns.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Grasp Implicit Cultural Values? Benchmarking LLMs' Cultural Intelligence with CQ-Bench</title>
<link>https://arxiv.org/abs/2504.01127</link>
<guid>https://arxiv.org/abs/2504.01127</guid>
<content:encoded><![CDATA[
arXiv:2504.01127v2 Announce Type: replace 
Abstract: Cultural Intelligence (CQ) refers to the ability to understand unfamiliar cultural contexts, a crucial skill for large language models (LLMs) to effectively engage with globally diverse users. Existing studies often focus on explicitly stated cultural norms, but fail to capture the subtle, implicit values that are common in daily conversation. To address this gap, we introduce CQBench, a benchmark specifically designed to assess LLMs' capability to infer implicit cultural values from natural conversational contexts. CQBench consists of multi character conversation based stories using values from the World Value Survey and the GlobalOpinions, with topics including ethical, religious, social, etc. Our automatic dataset construction pipeline integrates rigorous validation procedures (incorporation, consistency, and implicitness checks), achieving a 94.5% human model agreement in the final validation. To leverage CQBench data, we design three tasks of increasing complexity: attitude detection, value selection, and value extraction. These tasks evaluate whether models can detect attitude and recognize values embedded within natural dialogues rather than relying on explicit cultural knowledge. We find that while frontier models like o1 reach human level performance in value selection (0.809 F1), they still fall short in nuanced attitude detection (0.622 F1). Notably, finetuning a smaller LLaMA-3.2-3B on only 500 culturally rich examples improves performance by over 10%, even outperforming o3-mini in some cases. Using CQ-Bench, we provide insights into the current challenges in LLMs' CQ research and suggest practical pathways for enhancing LLMs' cross-cultural reasoning abilities.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hallucination Detection in LLMs with Topological Divergence on Attention Graphs</title>
<link>https://arxiv.org/abs/2504.10063</link>
<guid>https://arxiv.org/abs/2504.10063</guid>
<content:encoded><![CDATA[
arXiv:2504.10063v3 Announce Type: replace 
Abstract: Hallucination, i.e., generating factually incorrect content, remains a critical challenge for large language models (LLMs). We introduce TOHA, a TOpology-based HAllucination detector in the RAG setting, which leverages a topological divergence metric to quantify the structural properties of graphs induced by attention matrices. Examining the topological divergence between prompt and response subgraphs reveals consistent patterns: higher divergence values in specific attention heads correlate with hallucinated outputs, independent of the dataset. Extensive experiments - including evaluation on question answering and summarization tasks - show that our approach achieves state-of-the-art or competitive results on several benchmarks while requiring minimal annotated data and computational resources. Our findings suggest that analyzing the topological structure of attention matrices can serve as an efficient and robust indicator of factual reliability in LLMs.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Science Hierarchography: Hierarchical Organization of Science Literature</title>
<link>https://arxiv.org/abs/2504.13834</link>
<guid>https://arxiv.org/abs/2504.13834</guid>
<content:encoded><![CDATA[
arXiv:2504.13834v4 Announce Type: replace 
Abstract: Scientific knowledge is growing rapidly, making it difficult to track progress and high-level conceptual links across broad disciplines. While tools like citation networks and search engines help retrieve related papers, they lack the abstraction needed to capture the needed to represent the density and structure of activity across subfields.
  We motivate SCIENCE HIERARCHOGRAPHY, the goal of organizing scientific literature into a high-quality hierarchical structure that spans multiple levels of abstraction -- from broad domains to specific studies. Such a representation can provide insights into which fields are well-explored and which are under-explored. To achieve this goal, we develop a hybrid approach that combines efficient embedding-based clustering with LLM-based prompting, striking a balance between scalability and semantic precision. Compared to LLM-heavy methods like iterative tree construction, our approach achieves superior quality-speed trade-offs. Our hierarchies capture different dimensions of research contributions, reflecting the interdisciplinary and multifaceted nature of modern science. We evaluate its utility by measuring how effectively an LLM-based agent can navigate the hierarchy to locate target papers. Results show that our method improves interpretability and offers an alternative pathway for exploring scientific literature beyond traditional search methods. Code, data and demo are available: https://github.com/JHU-CLSP/science-hierarchography
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T-VEC: A Telecom-Specific Vectorization Model with Enhanced Semantic Understanding via Deep Triplet Loss Fine-Tuning</title>
<link>https://arxiv.org/abs/2504.16460</link>
<guid>https://arxiv.org/abs/2504.16460</guid>
<content:encoded><![CDATA[
arXiv:2504.16460v2 Announce Type: replace 
Abstract: The specialized vocabulary and nuanced concepts of the telecommunications industry pose persistent challenges for standard Natural Language Processing (NLP) models. Generic embedding models often struggle to represent telecom-specific semantics, limiting their utility in retrieval and downstream tasks. We present T-VEC (Telecom Vectorization Model), a domain-adapted embedding model fine-tuned from the gte-Qwen2-1.5B-instruct backbone using a triplet loss objective. Fine-tuning was performed on T-Embed, a high-quality, large-scale dataset covering diverse telecom concepts, standards, and operational scenarios. Although T-Embed contains some proprietary material and cannot be fully released, we open source 75% of the dataset to support continued research in domain-specific representation learning. On a custom benchmark comprising 1500 query-passage pairs from IETF RFCs and vendor manuals, T-VEC surpasses MPNet, BGE, Jina and E5, demonstrating superior domain grounding and semantic precision in telecom-specific retrieval. Embedding visualizations further showcase tight clustering of telecom-relevant concepts. We release T-VEC and its tokenizer to support semantically faithful NLP applications within the telecom domain.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Evaluation Metrics -- The Mirage of Hallucination Detection</title>
<link>https://arxiv.org/abs/2504.18114</link>
<guid>https://arxiv.org/abs/2504.18114</guid>
<content:encoded><![CDATA[
arXiv:2504.18114v2 Announce Type: replace 
Abstract: Hallucinations pose a significant obstacle to the reliability and widespread adoption of language models, yet their accurate measurement remains a persistent challenge. While many task- and domain-specific metrics have been proposed to assess faithfulness and factuality concerns, the robustness and generalization of these metrics are still untested. In this paper, we conduct a large-scale empirical evaluation of 6 diverse sets of hallucination detection metrics across 4 datasets, 37 language models from 5 families, and 5 decoding methods. Our extensive investigation reveals concerning gaps in current hallucination evaluation: metrics often fail to align with human judgments, take an overtly myopic view of the problem, and show inconsistent gains with parameter scaling. Encouragingly, LLM-based evaluation, particularly with GPT-4, yields the best overall results, and mode-seeking decoding methods seem to reduce hallucinations, especially in knowledge-grounded settings. These findings underscore the need for more robust metrics to understand and quantify hallucinations, and better strategies to mitigate them.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Say It Another Way: Auditing LLMs with a User-Grounded Automated Paraphrasing Framework</title>
<link>https://arxiv.org/abs/2505.03563</link>
<guid>https://arxiv.org/abs/2505.03563</guid>
<content:encoded><![CDATA[
arXiv:2505.03563v3 Announce Type: replace 
Abstract: Large language models (LLMs) are highly sensitive to subtle changes in prompt phrasing, posing challenges for reliable auditing. Prior methods often apply unconstrained prompt paraphrasing, which risk missing linguistic and demographic factors that shape authentic user interactions. We introduce AUGMENT (Automated User-Grounded Modeling and Evaluation of Natural Language Transformations), a framework for generating controlled paraphrases, grounded in user behaviors. AUGMENT leverages linguistically informed rules and enforces quality through checks on instruction adherence, semantic similarity, and realism, ensuring paraphrases are both reliable and meaningful for auditing. Through case studies on the BBQ and MMLU datasets, we show that controlled paraphrases uncover systematic weaknesses that remain obscured under unconstrained variation. These results highlight the value of the AUGMENT framework for reliable auditing.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hakim: Farsi Text Embedding Model</title>
<link>https://arxiv.org/abs/2505.08435</link>
<guid>https://arxiv.org/abs/2505.08435</guid>
<content:encoded><![CDATA[
arXiv:2505.08435v3 Announce Type: replace 
Abstract: Recent advancements in text embedding have significantly improved natural language understanding across many languages, yet Persian remains notably underrepresented in large-scale embedding research. In this paper, we present Hakim, a novel state-of-the-art Persian text embedding model that achieves a 8.5% performance improvement over existing approaches on the FaMTEB benchmark, outperforming all previously developed Persian language models. As part of this work, we introduce three new datasets - Corpesia, Pairsia-sup, and Pairsia-unsup - to support supervised and unsupervised training scenarios. Additionally, Hakim is designed for applications in chatbots and retrieval-augmented generation (RAG) systems, particularly addressing retrieval tasks that require incorporating message history within these systems. We also propose a new baseline model built on the BERT architecture. Our language model consistently achieves higher accuracy across various Persian NLP tasks, while the RetroMAE-based model proves particularly effective for textual information retrieval applications. Together, these contributions establish a new foundation for advancing Persian language understanding.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression</title>
<link>https://arxiv.org/abs/2505.13527</link>
<guid>https://arxiv.org/abs/2505.13527</guid>
<content:encoded><![CDATA[
arXiv:2505.13527v2 Announce Type: replace 
Abstract: Despite substantial advancements in aligning large language models (LLMs) with human values, current safety mechanisms remain susceptible to jailbreak attacks. We hypothesize that this vulnerability stems from distributional discrepancies between alignment-oriented prompts and malicious prompts. To investigate this, we introduce LogiBreak, a novel and universal black-box jailbreak method that leverages logical expression translation to circumvent LLM safety systems. By converting harmful natural language prompts into formal logical expressions, LogiBreak exploits the distributional gap between alignment data and logic-based inputs, preserving the underlying semantic intent and readability while evading safety constraints. We evaluate LogiBreak on a multilingual jailbreak dataset spanning three languages, demonstrating its effectiveness across various evaluation settings and linguistic contexts.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.16421</link>
<guid>https://arxiv.org/abs/2505.16421</guid>
<content:encoded><![CDATA[
arXiv:2505.16421v2 Announce Type: replace 
Abstract: While reinforcement learning (RL) has demonstrated remarkable success in enhancing large language models (LLMs), it has primarily focused on single-turn tasks such as solving math problems. Training effective web agents for multi-turn interactions remains challenging due to the complexity of long-horizon decision-making across dynamic web interfaces. In this work, we present WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework for training web agents. It learns directly from online interactions with web environments by asynchronously generating diverse trajectories, entirely guided by binary rewards depending on task success. Experiments on the WebArena-Lite benchmark demonstrate the effectiveness of WebAgent-R1, boosting the task success rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to 44.8%, significantly outperforming existing state-of-the-art methods and strong proprietary models such as OpenAI o3. In-depth analyses reveal the effectiveness of the thinking-based prompting strategy and test-time scaling through increased interactions for web tasks. We further investigate different RL initialization policies by introducing two variants, namely WebAgent-R1-Zero and WebAgent-R1-CoT, which highlight the importance of the warm-up training stage (i.e., behavior cloning) and provide insights on incorporating long chain-of-thought (CoT) reasoning in web agents.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Media Frames Reveal About Stance: A Dataset and Study about Memes in Climate Change Discourse</title>
<link>https://arxiv.org/abs/2505.16592</link>
<guid>https://arxiv.org/abs/2505.16592</guid>
<content:encoded><![CDATA[
arXiv:2505.16592v3 Announce Type: replace 
Abstract: Media framing refers to the emphasis on specific aspects of perceived reality to shape how an issue is defined and understood. Its primary purpose is to shape public perceptions often in alignment with the authors' opinions and stances. However, the interaction between stance and media frame remains largely unexplored. In this work, we apply an interdisciplinary approach to conceptualize and computationally explore this interaction with internet memes on climate change. We curate CLIMATEMEMES, the first dataset of climate-change memes annotated with both stance and media frames, inspired by research in communication science. CLIMATEMEMES includes 1,184 memes sourced from 47 subreddits, enabling analysis of frame prominence over time and communities, and sheds light on the framing preferences of different stance holders. We propose two meme understanding tasks: stance detection and media frame detection. We evaluate LLaVA-NeXT and Molmo in various setups, and report the corresponding results on their LLM backbone. Human captions consistently enhance performance. Synthetic captions and human-corrected OCR also help occasionally. Our findings highlight that VLMs perform well on stance, but struggle on frames, where LLMs outperform VLMs. Finally, we analyze VLMs' limitations in handling nuanced frames and stance expressions on climate change internet memes.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UNCLE: Benchmarking Uncertainty Expressions in Long-Form Generation</title>
<link>https://arxiv.org/abs/2505.16922</link>
<guid>https://arxiv.org/abs/2505.16922</guid>
<content:encoded><![CDATA[
arXiv:2505.16922v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are prone to hallucination, particularly in long-form generations. A promising direction to mitigate hallucination is to teach LLMs to express uncertainty explicitly when they lack sufficient knowledge. However, existing work lacks direct and fair evaluation of LLMs' ability to express uncertainty effectively in long-form generation. To address this gap, we first introduce UNCLE, a benchmark designed to evaluate uncertainty expression in both long- and short-form question answering (QA). UNCLE covers five domains and includes more than 1,000 entities, each with paired short- and long-form QA items. Our dataset is the first to directly link short- and long-form QA through aligned questions and gold-standard answers. Along with UNCLE, we propose a suite of new metrics to assess the models' capabilities to selectively express uncertainty. We then demonstrate that current models fail to convey uncertainty appropriately in long-form generation. We further explore both prompt-based and training-based methods to improve models' performance, with the training-based methods yielding greater gains. Further analysis of alignment gaps between short- and long-form uncertainty expression highlights promising directions for future research using UNCLE.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Search Wisely: Mitigating Sub-optimal Agentic Searches By Reducing Uncertainty</title>
<link>https://arxiv.org/abs/2505.17281</link>
<guid>https://arxiv.org/abs/2505.17281</guid>
<content:encoded><![CDATA[
arXiv:2505.17281v2 Announce Type: replace 
Abstract: Agentic Retrieval-Augmented Generation (RAG) systems enhance Large Language Models (LLMs) by enabling dynamic, multi-step reasoning and information retrieval. However, these systems often exhibit sub-optimal search behaviors like over-search (retrieving redundant information) and under-search (failing to retrieve necessary information), which hinder efficiency and reliability. This work formally defines and quantifies these behaviors, revealing their prevalence across multiple QA datasets and agentic RAG systems (e.g., one model could have avoided searching in 27.7% of its search steps). Furthermore, we demonstrate a crucial link between these inefficiencies and the models' uncertainty regarding their own knowledge boundaries, where response accuracy correlates with model's uncertainty in its search decisions. To address this, we propose $\beta$-GRPO, a reinforcement learning-based training method that incorporates confidence threshold to reward high-certainty search decisions. Experiments on seven QA benchmarks show that $\beta$-GRPO enable a 3B model with better agentic RAG ability, outperforming other strong baselines with a 4% higher average exact match score.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inference-time Alignment in Continuous Space</title>
<link>https://arxiv.org/abs/2505.20081</link>
<guid>https://arxiv.org/abs/2505.20081</guid>
<content:encoded><![CDATA[
arXiv:2505.20081v3 Announce Type: replace 
Abstract: Aligning large language models with human feedback at inference time has received increasing attention due to its flexibility. Existing methods rely on generating multiple responses from the base policy for search using a reward model, which can be considered as searching in a discrete response space. However, these methods struggle to explore informative candidates when the base policy is weak or the candidate set is small, resulting in limited effectiveness. In this paper, to address this problem, we propose Simple Energy Adaptation ($\textbf{SEA}$), a simple yet effective algorithm for inference-time alignment. In contrast to expensive search over the discrete space, SEA directly adapts original responses from the base policy toward the optimal one via gradient-based sampling in continuous latent space. Specifically, SEA formulates inference as an iterative optimization procedure on an energy function over actions in the continuous space defined by the optimal policy, enabling simple and effective alignment. For instance, despite its simplicity, SEA outperforms the second-best baseline with a relative improvement of up to $ \textbf{77.51%}$ on AdvBench and $\textbf{16.36%}$ on MATH. Our code is publicly available at https://github.com/yuanyige/sea
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge Bases</title>
<link>https://arxiv.org/abs/2505.20321</link>
<guid>https://arxiv.org/abs/2505.20321</guid>
<content:encoded><![CDATA[
arXiv:2505.20321v3 Announce Type: replace 
Abstract: Biomedical researchers increasingly rely on large-scale structured databases for complex analytical tasks. However, current text-to-SQL systems often struggle to map qualitative scientific questions into executable SQL, particularly when implicit domain reasoning is required. We introduce BiomedSQL, the first benchmark explicitly designed to evaluate scientific reasoning in text-to-SQL generation over a real-world biomedical knowledge base. BiomedSQL comprises 68,000 question/SQL query/answer triples generated from templates and grounded in a harmonized BigQuery knowledge base that integrates gene-disease associations, causal inference from omics data, and drug approval records. Each question requires models to infer domain-specific criteria, such as genome-wide significance thresholds, effect directionality, or trial phase filtering, rather than rely on syntactic translation alone. We evaluate a range of open- and closed-source LLMs across prompting strategies and interaction paradigms. Our results reveal a substantial performance gap: GPT-o3-mini achieves 59.0% execution accuracy, while our custom multi-step agent, BMSQL, reaches 62.6%, both well below the expert baseline of 90.0%. BiomedSQL provides a new foundation for advancing text-to-SQL systems capable of supporting scientific discovery through robust reasoning over structured biomedical knowledge bases. Our dataset is publicly available at https://huggingface.co/datasets/NIH-CARD/BiomedSQL, and our code is open-source at https://github.com/NIH-CARD/biomedsql.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trans-EnV: A Framework for Evaluating the Linguistic Robustness of LLMs Against English Varieties</title>
<link>https://arxiv.org/abs/2505.20875</link>
<guid>https://arxiv.org/abs/2505.20875</guid>
<content:encoded><![CDATA[
arXiv:2505.20875v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) are predominantly evaluated on Standard American English (SAE), often overlooking the diversity of global English varieties. This narrow focus may raise fairness concerns as degraded performance on non-standard varieties can lead to unequal benefits for users worldwide. Therefore, it is critical to extensively evaluate the linguistic robustness of LLMs on multiple non-standard English varieties. We introduce Trans-EnV, a framework that automatically transforms SAE datasets into multiple English varieties to evaluate the linguistic robustness. Our framework combines (1) linguistics expert knowledge to curate variety-specific features and transformation guidelines from linguistic literature and corpora, and (2) LLM-based transformations to ensure both linguistic validity and scalability. Using Trans-EnV, we transform six benchmark datasets into 38 English varieties and evaluate seven state-of-the-art LLMs. Our results reveal significant performance disparities, with accuracy decreasing by up to 46.3% on non-standard varieties. These findings highlight the importance of comprehensive linguistic robustness evaluation across diverse English varieties. Each construction of Trans-EnV was validated through rigorous statistical testing and consultation with a researcher in the field of second language acquisition, ensuring its linguistic validity. Our code and datasets are publicly available at https://github.com/jiyounglee-0523/TransEnV and https://huggingface.co/collections/jiyounglee0523/transenv-681eadb3c0c8cf363b363fb1.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlashDLM: Accelerating Diffusion Language Model Inference via Efficient KV Caching and Guided Diffusion</title>
<link>https://arxiv.org/abs/2505.21467</link>
<guid>https://arxiv.org/abs/2505.21467</guid>
<content:encoded><![CDATA[
arXiv:2505.21467v2 Announce Type: replace 
Abstract: Diffusion language models offer parallel token generation and inherent bidirectionality, promising more efficient and powerful sequence modeling compared to autoregressive approaches. However, state-of-the-art diffusion models (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match the quality of similarly sized autoregressive (AR) models (e.g., Qwen2.5 7B, Llama3 8B), their iterative denoising requires multiple full-sequence forward passes, resulting in high computational costs and latency, particularly for long input prompts and long-context scenarios. Furthermore, parallel token generation introduces token incoherence problems, and current sampling heuristics suffer from significant quality drops with decreasing denoising steps. We address these limitations with two training-free techniques. First, we propose FreeCache, a Key-Value (KV) approximation caching technique that reuses stable KV projections across denoising steps, effectively reducing the computational cost of DLM inference. Second, we introduce Guided Diffusion, a training-free method that uses a lightweight pretrained autoregressive model to supervise token unmasking, dramatically reducing the total number of denoising iterations without sacrificing quality. We conduct extensive evaluations on open-source reasoning benchmarks, and our combined methods deliver an average of 12.14x end-to-end speedup across various tasks with negligible accuracy degradation. For the first time, diffusion language models achieve a comparable and even faster latency as the widely adopted autoregressive models. Our work successfully paved the way for scaling up the diffusion language model to a broader scope of applications across different domains.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowNIB: An Information Bottleneck Analysis of Bidirectional vs. Unidirectional Language Models</title>
<link>https://arxiv.org/abs/2506.00859</link>
<guid>https://arxiv.org/abs/2506.00859</guid>
<content:encoded><![CDATA[
arXiv:2506.00859v3 Announce Type: replace 
Abstract: Bidirectional language models have better context understanding and perform better than unidirectional models on natural language understanding tasks, yet the theoretical reasons behind this advantage remain unclear. In this work, we investigate this disparity through the lens of the Information Bottleneck (IB) principle, which formalizes a trade-off between compressing input information and preserving task-relevant content. We propose FlowNIB, a dynamic and scalable method for estimating mutual information during training that addresses key limitations of classical IB approaches, including computational intractability and fixed trade-off schedules. Theoretically, we show that bidirectional models retain more mutual information and exhibit higher effective dimensionality than unidirectional models. To support this, we present a generalized framework for measuring representational complexity and prove that bidirectional representations are strictly more informative under mild conditions. We further validate our findings through extensive experiments across multiple models and tasks using FlowNIB, revealing how information is encoded and compressed throughout training. Together, our work provides a principled explanation for the effectiveness of bidirectional architectures and introduces a practical tool for analyzing information flow in deep language models.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tug-of-war between idioms' figurative and literal interpretations in LLMs</title>
<link>https://arxiv.org/abs/2506.01723</link>
<guid>https://arxiv.org/abs/2506.01723</guid>
<content:encoded><![CDATA[
arXiv:2506.01723v4 Announce Type: replace 
Abstract: Idioms present a unique challenge for language models due to their non-compositional figurative interpretations, which often strongly diverge from the idiom's literal interpretation. In this paper, we employ causal tracing to systematically analyze how pretrained causal transformers deal with this ambiguity. We localize three mechanisms: (i) Early sublayers and specific attention heads retrieve an idiom's figurative interpretation, while suppressing its literal interpretation. (ii) When disambiguating context precedes the idiom, the model leverages it from the earliest layer and later layers refine the interpretation if the context conflicts with the retrieved interpretation. (iii) Then, selective, competing pathways carry both interpretations: an intermediate pathway prioritizes the figurative interpretation and a parallel direct route favors the literal interpretation, ensuring that both readings remain available. Our findings provide mechanistic evidence for idiom comprehension in autoregressive transformers.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study</title>
<link>https://arxiv.org/abs/2506.04810</link>
<guid>https://arxiv.org/abs/2506.04810</guid>
<content:encoded><![CDATA[
arXiv:2506.04810v2 Announce Type: replace 
Abstract: Logical reasoning is a core capability for large language models (LLMs), yet existing benchmarks that rely solely on final-answer accuracy fail to capture the quality of the reasoning process. To address this, we introduce FineLogic, a fine-grained evaluation framework that assesses logical reasoning across three dimensions: overall accuracy, stepwise soundness, and representation-level probing. Leveraging this framework, we conduct a comprehensive study on how different supervision formats in fine-tuning shape reasoning abilities. We fine-tune LLMs on four supervision styles: one in natural language and three symbolic variants. We find a key trade-off: natural language supervision excels at generalization to out-of-distribution and long-chain problems, whereas symbolic supervision is superior at instilling structurally sound, atomic reasoning steps. Furthermore, our probing analysis indicates that fine-tuning primarily refines the model's step-by-step generation process, rather than improving its ability to converge on an answer early. Together, our framework and analysis provide a more rigorous lens for evaluating and improving logical reasoning in LLMs. The code is available at https://github.com/YujunZhou/FineLogic.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Handwriting to Feedback: Evaluating VLMs and LLMs for AI-Powered Assessment in Indonesian Classrooms</title>
<link>https://arxiv.org/abs/2506.04822</link>
<guid>https://arxiv.org/abs/2506.04822</guid>
<content:encoded><![CDATA[
arXiv:2506.04822v2 Announce Type: replace 
Abstract: Despite rapid progress in vision-language and large language models (VLMs and LLMs), their effectiveness for AI-driven educational assessment in real-world, underrepresented classrooms remains largely unexplored. We evaluate state-of-the-art VLMs and LLMs on over 14K handwritten answers from grade-4 classrooms in Indonesia, covering Mathematics and English aligned with the local national curriculum. Unlike prior work on clean digital text, our dataset features naturally curly, diverse handwriting from real classrooms, posing realistic visual and linguistic challenges. Assessment tasks include grading and generating personalized Indonesian feedback guided by rubric-based evaluation. Results show that the VLM struggles with handwriting recognition, causing error propagation in LLM grading, yet LLM feedback remains pedagogically useful despite imperfect visual inputs, revealing limits in personalization and contextual relevance.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning</title>
<link>https://arxiv.org/abs/2506.09513</link>
<guid>https://arxiv.org/abs/2506.09513</guid>
<content:encoded><![CDATA[
arXiv:2506.09513v3 Announce Type: replace 
Abstract: Reasoning-based large language models have excelled in mathematics and programming, yet their potential in knowledge-intensive medical question answering remains underexplored and insufficiently validated in clinical contexts. To bridge this gap, we introduce ReasonMed, the largest medical reasoning dataset to date, comprising 370k high-quality examples distilled from 1.75 million initial reasoning paths generated by complementary LLMs and curated through a cost-efficient easy-medium-difficult (EMD) pipeline. ReasonMed is built through a multi-agent generation, verification, and refinement process, in which an Error Refiner improves reasoning paths by correcting error-prone steps identified by a verifier. Using ReasonMed, we investigate effective strategies for training medical reasoning models and find that integrating detailed CoT reasoning with concise answer summaries yields the most robust fine-tuning results. Models trained on ReasonMed set a new benchmark: ReasonMed-7B surpasses the prior best sub-10B models by 4.17% and even exceeds LLaMA3.1-70B on PubMedQA by 4.60%. When scaled to ReasonMed-14B, it remains highly competitive, underscoring consistent scaling potential. The codes and datasets are available at https://github.com/YuSun-Work/ReasonMed.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks</title>
<link>https://arxiv.org/abs/2506.11113</link>
<guid>https://arxiv.org/abs/2506.11113</guid>
<content:encoded><![CDATA[
arXiv:2506.11113v3 Announce Type: replace 
Abstract: Peer review is essential for maintaining academic quality, but the increasing volume of submissions places a significant burden on reviewers. Large language models (LLMs) offer potential assistance in this process, yet their susceptibility to textual adversarial attacks raises reliability concerns. This paper investigates the robustness of LLMs used as automated reviewers in the presence of such attacks. We focus on three key questions: (1) The effectiveness of LLMs in generating reviews compared to human reviewers. (2) The impact of adversarial attacks on the reliability of LLM-generated reviews. (3) Challenges and potential mitigation strategies for LLM-based review. Our evaluation reveals significant vulnerabilities, as text manipulations can distort LLM assessments. We offer a comprehensive evaluation of LLM performance in automated peer reviewing and analyze its robustness against adversarial attacks. Our findings emphasize the importance of addressing adversarial risks to ensure AI strengthens, rather than compromises, the integrity of scholarly communication.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Surgery in Multilingual Large Language Models</title>
<link>https://arxiv.org/abs/2506.12450</link>
<guid>https://arxiv.org/abs/2506.12450</guid>
<content:encoded><![CDATA[
arXiv:2506.12450v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated remarkable generalization capabilities across tasks and languages, revolutionizing natural language processing. This paper investigates the naturally emerging representation alignment in LLMs, particularly in the middle layers, and its implications for disentangling language-specific and language-agnostic information. We empirically confirm the existence of this alignment, analyze its behavior in comparison to explicitly designed alignment models, and demonstrate its potential for language-specific manipulation without semantic degradation. Building on these findings, we propose Inference-Time Language Control (ITLC), a novel method that leverages latent injection to enable precise cross-lingual language control and mitigate language confusion in LLMs. Our experiments highlight ITLC's strong cross-lingual control capabilities while preserving semantic integrity in target languages. Furthermore, we demonstrate its effectiveness in alleviating the cross-lingual language confusion problem, which persists even in current large-scale LLMs, leading to inconsistent language generation. This work advances our understanding of representation alignment in LLMs and introduces a practical solution for enhancing their monolingual and cross-lingual performance.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Grounded is Wikipedia? A Study on Structured Evidential Support and Retrieval</title>
<link>https://arxiv.org/abs/2506.12637</link>
<guid>https://arxiv.org/abs/2506.12637</guid>
<content:encoded><![CDATA[
arXiv:2506.12637v2 Announce Type: replace 
Abstract: Wikipedia is a critical resource for modern NLP, serving as a rich repository of up-to-date and citation-backed information on a wide variety of subjects. The reliability of Wikipedia -- its groundedness in its cited sources -- is vital to this purpose. This work analyzes both how grounded Wikipedia is and how readily fine-grained grounding evidence can be retrieved. To this end, we introduce PeopleProfiles -- a large-scale, multi-level dataset of claim support annotations on biographical Wikipedia articles. We show that: (1) ~22% of claims in Wikipedia lead sections are unsupported by the article body; (2) ~30% of claims in the article body are unsupported by their publicly accessible sources; and (3) real-world Wikipedia citation practices often differ from documented standards. Finally, we show that complex evidence retrieval remains a challenge -- even for recent reasoning rerankers.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Role of Model Confidence on Bias Effects in Measured Uncertainties for Vision-Language Models</title>
<link>https://arxiv.org/abs/2506.16724</link>
<guid>https://arxiv.org/abs/2506.16724</guid>
<content:encoded><![CDATA[
arXiv:2506.16724v2 Announce Type: replace 
Abstract: With the growing adoption of Large Language Models (LLMs) for open-ended tasks, accurately assessing epistemic uncertainty, which reflects a model's lack of knowledge, has become crucial to ensuring reliable outcomes. However, quantifying epistemic uncertainty in such tasks is challenging due to the presence of aleatoric uncertainty, which arises from multiple valid answers. While bias can introduce noise into epistemic uncertainty estimation, it may also reduce noise from aleatoric uncertainty. To investigate this trade-off, we conduct experiments on Visual Question Answering (VQA) tasks and find that mitigating prompt-introduced bias improves uncertainty quantification in GPT-4o. Building on prior work showing that LLMs tend to copy input information when model confidence is low, we further analyze how these prompt biases affect measured epistemic and aleatoric uncertainty across varying bias-free confidence levels with GPT-4o and Qwen2-VL. We find that all considered biases have greater effects in both uncertainties when bias-free model confidence is lower. Moreover, lower bias-free model confidence is associated with greater bias-induced underestimation of epistemic uncertainty, resulting in overconfident estimates, whereas it has no significant effect on the direction of bias effect in aleatoric uncertainty estimation. These distinct effects deepen our understanding of bias mitigation for uncertainty quantification and potentially inform the development of more advanced techniques.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Laws Are Unreliable for Downstream Tasks: A Reality Check</title>
<link>https://arxiv.org/abs/2507.00885</link>
<guid>https://arxiv.org/abs/2507.00885</guid>
<content:encoded><![CDATA[
arXiv:2507.00885v2 Announce Type: replace 
Abstract: Downstream scaling laws aim to predict task performance at larger scales from the model's performance at smaller scales. Whether such prediction should be possible is unclear: some works discover clear linear scaling trends after simple transformations of the performance metric, whereas others point out fundamental challenges to downstream scaling laws, such as emergence and inverse scaling. In this work, we conduct a meta-analysis of existing data on downstream scaling laws, and we find that predictable scaling only occurs in a minority of cases: 39% of the time. Moreover, seemingly benign changes to the experimental setting can completely change the scaling behavior. Our analysis underscores the need to understand the conditions under which scaling laws succeed. To accurately model the relationship between pretraining loss and task performance, we must embrace the cases in which scaling behavior deviates from linear trends.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Truth, Trust, and Trouble: Medical AI on the Edge</title>
<link>https://arxiv.org/abs/2507.02983</link>
<guid>https://arxiv.org/abs/2507.02983</guid>
<content:encoded><![CDATA[
arXiv:2507.02983v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) hold significant promise for transforming digital health by enabling automated medical question answering. However, ensuring these models meet critical industry standards for factual accuracy, usefulness, and safety remains a challenge, especially for open-source solutions. We present a rigorous benchmarking framework using a dataset of over 1,000 health questions. We assess model performance across honesty, helpfulness, and harmlessness. Our results highlight trade-offs between factual reliability and safety among evaluated models -- Mistral-7B, BioMistral-7B-DARE, and AlpaCare-13B. AlpaCare-13B achieves the highest accuracy (91.7%) and harmlessness (0.92), while domain-specific tuning in BioMistral-7B-DARE boosts safety (0.90) despite its smaller scale. Few-shot prompting improves accuracy from 78% to 85%, and all models show reduced helpfulness on complex queries, highlighting ongoing challenges in clinical QA.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers</title>
<link>https://arxiv.org/abs/2507.06223</link>
<guid>https://arxiv.org/abs/2507.06223</guid>
<content:encoded><![CDATA[
arXiv:2507.06223v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have recently been applied to reranking tasks in information retrieval, achieving strong performance. However, their high computational demands often hinder practical deployment. Existing studies evaluate the efficiency of LLM-based rerankers using proxy metrics such as latency, the number of forward passes, input tokens, and output tokens. However, these metrics depend on hardware and running-time choices (\eg parallel or not, batch size, etc), and often fail to account for model size, making it difficult to interpret and obscuring the evaluation of the efficiency-effectiveness tradeoff. To address this issue, we propose \ours\footnote{https://github.com/zhiyuanpeng/EER-FLOPs.} for LLM-based rerankers: RPP (ranking metrics per PetaFLOP), measuring how much ranking quality (e.g., NDCG or MRR) a method achieves per PetaFLOP, and QPP (queries per PetaFLOP), measuring how many queries can be processed per PetaFLOP. Accompanied by the new metrics, an interpretable FLOPs estimator is developed to estimate the FLOPs of an LLM-based reranker even without running any experiments. Based on the proposed metrics, we conduct comprehensive experiments to evaluate a wide range of LLM-based rerankers with different architectures, studying the efficiency-effectiveness trade-off and bringing this issue to the attention of the research community.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs</title>
<link>https://arxiv.org/abs/2507.11112</link>
<guid>https://arxiv.org/abs/2507.11112</guid>
<content:encoded><![CDATA[
arXiv:2507.11112v2 Announce Type: replace 
Abstract: Recent studies have shown that Large Language Models (LLMs) are vulnerable to data poisoning attacks, where malicious training examples embed hidden behaviours triggered by specific input patterns. However, most existing works assume a phrase and focus on the attack's effectiveness, offering limited understanding of trigger mechanisms and how multiple triggers interact within the model. In this paper, we present a framework for studying poisoning in LLMs. We show that multiple distinct backdoor triggers can coexist within a single model without interfering with each other, enabling adversaries to embed several triggers concurrently. Using multiple triggers with high embedding similarity, we demonstrate that poisoned triggers can achieve robust activation even when tokens are substituted or separated by long token spans. Our findings expose a broader and more persistent vulnerability surface in LLMs. To mitigate this threat, we propose a post hoc recovery method that selectively retrains specific model components based on a layer-wise weight difference analysis. Our method effectively removes the trigger behaviour with minimal parameter updates, presenting a practical and efficient defence against multi-trigger poisoning.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs Encode Harmfulness and Refusal Separately</title>
<link>https://arxiv.org/abs/2507.11878</link>
<guid>https://arxiv.org/abs/2507.11878</guid>
<content:encoded><![CDATA[
arXiv:2507.11878v3 Announce Type: replace 
Abstract: LLMs are trained to refuse harmful instructions, but do they truly understand harmfulness beyond just refusing? Prior work has shown that LLMs' refusal behaviors can be mediated by a one-dimensional subspace, i.e., a refusal direction. In this work, we identify a new dimension to analyze safety mechanisms in LLMs, i.e., harmfulness, which is encoded internally as a separate concept from refusal. There exists a harmfulness direction that is distinct from the refusal direction. As causal evidence, steering along the harmfulness direction can lead LLMs to interpret harmless instructions as harmful, but steering along the refusal direction tends to elicit refusal responses directly without reversing the model's judgment on harmfulness. Furthermore, using our identified harmfulness concept, we find that certain jailbreak methods work by reducing the refusal signals without reversing the model's internal belief of harmfulness. We also find that adversarially finetuning models to accept harmful instructions has minimal impact on the model's internal belief of harmfulness. These insights lead to a practical safety application: The model's latent harmfulness representation can serve as an intrinsic safeguard (Latent Guard) for detecting unsafe inputs and reducing over-refusals that is robust to finetuning attacks. For instance, our Latent Guard achieves performance comparable to or better than Llama Guard 3 8B, a dedicated finetuned safeguard model, across different jailbreak methods. Our findings suggest that LLMs' internal understanding of harmfulness is more robust than their refusal decision to diverse input instructions, offering a new perspective to study AI safety.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Behavioural Translation Style Space: Towards simulating the temporal dynamics of affect, behaviour, and cognition in human translation production</title>
<link>https://arxiv.org/abs/2507.12208</link>
<guid>https://arxiv.org/abs/2507.12208</guid>
<content:encoded><![CDATA[
arXiv:2507.12208v2 Announce Type: replace 
Abstract: The paper introduces a novel behavioural translation style space (BTSS) that describes possible behavioural translation patterns. The suggested BTSS is organized as a hierarchical structure that entails various embedded processing layers. We posit that observable translation behaviour - i.e. eye and finger movements - is fundamental when executing the physical act of translation but it is caused and shaped by higher-order cognitive processes and affective translation states. We analyse records of keystrokes and gaze data as indicators of the hidden mental processing structure and organize the behavioural patterns as a multi-layered embedded BTSS. We develop a perspective in which the BTSS serves as the basis for a computational translation agent to simulate the temporal dynamics of affect, behavioural routines and cognition during human translation production.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLEXITOKENS: Flexible Tokenization for Evolving Language Models</title>
<link>https://arxiv.org/abs/2507.12720</link>
<guid>https://arxiv.org/abs/2507.12720</guid>
<content:encoded><![CDATA[
arXiv:2507.12720v3 Announce Type: replace 
Abstract: Language models (LMs) are challenging to adapt to new data distributions by simple finetuning. This is due to the rigidity of their subword tokenizers, which typically remain unchanged during adaptation. This inflexibility often leads to inefficient tokenization, causing overfragmentation of out-of-distribution domains, unseen languages, or scripts. In this work, we develop byte-level LMs with learnable tokenizers to make tokenization adaptive. Our models include a submodule that learns to predict boundaries between the input byte sequence, encoding it into variable-length segments. Existing tokenizer-free methods train this boundary predictor using an auxiliary loss that enforces a fixed compression rate across the training corpus, introducing a new kind of rigidity. We propose FLEXITOKENS, a simplified training objective that enables significantly greater flexibility during adaptation. Evaluating across multiple multilingual benchmarks, morphologically diverse tasks, and domains, we demonstrate that FLEXITOKENS consistently reduces token over-fragmentation and achieves up to 10% improvements on downstream task performance compared to subword and other gradient-based tokenizers. Code and data for our experiments will be released at https://github.com/owos/flexitokens
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Feedback to Checklists: Grounded Evaluation of AI-Generated Clinical Notes</title>
<link>https://arxiv.org/abs/2507.17717</link>
<guid>https://arxiv.org/abs/2507.17717</guid>
<content:encoded><![CDATA[
arXiv:2507.17717v2 Announce Type: replace 
Abstract: AI-generated clinical notes are increasingly used in healthcare, but evaluating their quality remains a challenge due to high subjectivity and limited scalability of expert review. Existing automated metrics often fail to align with real-world physician preferences. To address this, we propose a pipeline that systematically distills real user feedback into structured checklists for note evaluation. These checklists are designed to be interpretable, grounded in human feedback, and enforceable by LLM-based evaluators. Using deidentified data from over 21,000 clinical encounters (prepared in accordance with the HIPAA safe harbor standard) from a deployed AI medical scribe system, we show that our feedback-derived checklist outperforms a baseline approach in our offline evaluations in coverage, diversity, and predictive power for human ratings. Extensive experiments confirm the checklist's robustness to quality-degrading perturbations, significant alignment with clinician preferences, and practical value as an evaluation methodology. In offline research settings, our checklist offers a practical tool for flagging notes that may fall short of our defined quality standards.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Small-Scale Data Poisoning Exacerbate Dialect-Linked Biases in Large Language Models?</title>
<link>https://arxiv.org/abs/2507.19195</link>
<guid>https://arxiv.org/abs/2507.19195</guid>
<content:encoded><![CDATA[
arXiv:2507.19195v2 Announce Type: replace 
Abstract: Style-conditioned data poisoning is identified as a covert vector for amplifying sociolinguistic bias in large language models. Using small poisoned budgets that pair dialectal prompts -- principally African American Vernacular English (AAVE) and a Southern dialect -- with toxic or stereotyped completions during instruction tuning, this work probes whether linguistic style can act as a latent trigger for harmful behavior. Across multiple model families and scales, poisoned exposure elevates toxicity and stereotype expression for dialectal inputs -- most consistently for AAVE -- while Standard American English remains comparatively lower yet not immune. A multi-metric audit combining classifier-based toxicity with an LLM-as-a-judge reveals stereotype-laden content even when lexical toxicity appears muted, indicating that conventional detectors under-estimate sociolinguistic harms. Additionally, poisoned models exhibit emergent jailbreaking despite the absence of explicit slurs in the poison, suggesting weakened alignment rather than memorization. These findings underscore the need for dialect-aware evaluation, content-level stereotype auditing, and training protocols that explicitly decouple style from toxicity to prevent bias amplification through seemingly minor, style-based contamination.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flora: Effortless Context Construction to Arbitrary Length and Scale</title>
<link>https://arxiv.org/abs/2507.19786</link>
<guid>https://arxiv.org/abs/2507.19786</guid>
<content:encoded><![CDATA[
arXiv:2507.19786v2 Announce Type: replace 
Abstract: Effectively handling long contexts is challenging for Large Language Models (LLMs) due to the rarity of long texts, high computational demands, and substantial forgetting of short-context abilities. Recent approaches have attempted to construct long contexts for instruction tuning, but these methods often require LLMs or human interventions, which are both costly and limited in length and diversity. Also, the drop in short-context performances of present long-context LLMs remains significant. In this paper, we introduce Flora, an effortless (human/LLM-free) long-context construction strategy. Flora can markedly enhance the long-context performance of LLMs by arbitrarily assembling short instructions based on categories and instructing LLMs to generate responses based on long-context meta-instructions. This enables Flora to produce contexts of arbitrary length and scale with rich diversity, while only slightly compromising short-context performance. Experiments on Llama3-8B-Instruct and QwQ-32B show that LLMs enhanced by Flora excel in three long-context benchmarks while maintaining strong performances in short-context tasks. Our data-construction code is available at \href{https://github.com/txchen-USTC/Flora}{https://github.com/txchen-USTC/Flora}.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Med-R$^3$: Enhancing Medical Retrieval-Augmented Reasoning of LLMs via Progressive Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.23541</link>
<guid>https://arxiv.org/abs/2507.23541</guid>
<content:encoded><![CDATA[
arXiv:2507.23541v3 Announce Type: replace 
Abstract: In medical scenarios, effectively retrieving external knowledge and leveraging it for rigorous logical reasoning is of significant importance. Despite their potential, existing work has predominantly focused on enhancing either retrieval or reasoning capabilities of the models in isolation, with little attention given to their joint optimization, which leads to limited coordination between the two processes. Additionally, current methods rely heavily on supervised fine-tuning (SFT), which can cause models to memorize existing problem-solving pathways, thereby restricting their generalization ability when confronted with novel problem contexts. Furthermore, while some studies have explored to improve retrieval-augmented reasoning in general domains via reinforcement learning, their reward function designs do not adequately capture the specific demands of the medical domain. To address these challenges, we introduce **Med-R$^3$**, a **Med**ical **R**etrieval-augmented **R**easoning framework driven by progressive **R**einforcement learning. In this framework, we first develop the model's ability to perform logical reasoning over medical problems. Subsequently, on the basis of this foundation, we adaptively optimize the retrieval capability to better align with the characteristics of knowledge corpus and external information utilization throughout the reasoning process. Finally, we conduct joint optimization of the model's retrieval and reasoning coordination. Extensive experiments indicate that **Med-R$^3$** could achieve state-of-the-art performances, with LLaMA3.1-8B-Instruct + Med-R$^3$ surpassing closed-sourced GPT-4o-mini by 3.93\% at a comparable parameter scale, while Qwen2.5-14B augmented with Med-R$^3$ shows a more substantial gain of 13.53\%.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoCoA: Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy</title>
<link>https://arxiv.org/abs/2508.01696</link>
<guid>https://arxiv.org/abs/2508.01696</guid>
<content:encoded><![CDATA[
arXiv:2508.01696v3 Announce Type: replace 
Abstract: Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs), especially for knowledge-intensive tasks. Despite its advantages, current RAG methods often struggle to fully exploit knowledge during generation. In particular, the synergy between the model's internal parametric knowledge and external retrieved knowledge remains limited. Retrieved contents may sometimes mislead generation, while certain generated content can guide the model toward more accurate outputs. In this work, we propose Collaborative Chain-of-Agents, a framework designed to enhance explicitly synergy over both parametric and retrieved knowledge. Specifically, we first introduce CoCoA-zero, a multi-agent RAG framework that first performs conditional knowledge induction and then reasons answers. Building on this, we develop CoCoA, a long-chain training strategy that synthesizes extended multi-agent reasoning trajectories from CoCoA-zero to fine-tune the LLM. This strategy enhances the model's capability to explicitly integrate and jointly leverage parametric and retrieved knowledge. Experimental results demonstrate the superiority of CoCoA in open-domain QA and multi-hop QA.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long Chain-of-Thought Reasoning Across Languages</title>
<link>https://arxiv.org/abs/2508.14828</link>
<guid>https://arxiv.org/abs/2508.14828</guid>
<content:encoded><![CDATA[
arXiv:2508.14828v2 Announce Type: replace 
Abstract: While large reasoning models have shown remarkable ability to generate long chains-of-thought (CoTs) in English, we still lack understanding of how these long-form reasoning abilities transfer to the vast majority of the world's languages. In this work, we systematically investigate four key stages of model development--scaling, pretraining, post-training, and inference--to understand how long CoT capabilities extend beyond English. We compare two reasoning settings across nine non-English target languages: En-CoT, where models process target-language inputs, but reason in English; and Target-CoT, where models both process inputs and generate long CoTs in the target language. We find that scaling reasoning model size improves multilingual task performance in En-CoT, but Target-CoT performance lags behind. This gap widens for tasks requiring long, multi-step CoTs such as mathematical reasoning. Shifting to pretraining, we find that adding a specialized reasoning stage enhances En-CoT performance but degrades Target-CoT, whereas broad multilingual pretraining improves both modes simultaneously. Given the scarcity of high-quality reasoning traces in languages other than English, we explore synthetic data curation approaches for post-training. We demonstrate that fine-tuning on reasoning traces automatically translated from gold English traces outperforms fine-tuning on target-language traces distilled from large reasoning models. Finally, we report disparities in inference efficiency between languages and uncover language-specific failure modes in CoTs. We release models, datasets, and code to foster further research.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spotlight Attention: Towards Efficient LLM Generation via Non-linear Hashing-based KV Cache Retrieval</title>
<link>https://arxiv.org/abs/2508.19740</link>
<guid>https://arxiv.org/abs/2508.19740</guid>
<content:encoded><![CDATA[
arXiv:2508.19740v4 Announce Type: replace 
Abstract: Reducing the key-value (KV) cache burden in Large Language Models (LLMs) significantly accelerates inference. Dynamically selecting critical KV caches during decoding helps maintain performance. Existing methods use random linear hashing to identify important tokens, but this approach is inefficient due to the orthogonal distribution of queries and keys within two narrow cones in LLMs. We introduce Spotlight Attention, a novel method that employs non-linear hashing functions to optimize the embedding distribution of queries and keys, enhancing coding efficiency and robustness. We also developed a lightweight, stable training framework using a Bradley-Terry ranking-based loss, enabling optimization of the non-linear hashing module on GPUs with 16GB memory in 8 hours. Experimental results show that Spotlight Attention drastically improves retrieval precision while shortening the length of the hash code at least 5$\times$ compared to traditional linear hashing. Finally, we exploit the computational advantages of bitwise operations by implementing specialized CUDA kernels, achieving hashing retrieval for 512K tokens in under 100$\mu$s on a single A100 GPU, with end-to-end throughput up to 3$\times$ higher than vanilla decoding.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning</title>
<link>https://arxiv.org/abs/2508.21589</link>
<guid>https://arxiv.org/abs/2508.21589</guid>
<content:encoded><![CDATA[
arXiv:2508.21589v4 Announce Type: replace 
Abstract: Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely on high-quality training data. While data selection and data synthesis are two common strategies to improve data quality, existing approaches often face limitations in static dataset curation that fail to adapt to evolving model capabilities. In this paper, we introduce Middo, a self-evolving Model-informed dynamic data optimization framework that uses model-aware data selection and context-preserving data refinement. Unlike conventional one-off filtering/synthesis methods, our framework establishes a closed-loop optimization system: (1) A self-referential diagnostic module proactively identifies suboptimal samples through tri-axial model signals - loss patterns (complexity), embedding cluster dynamics (diversity), and self-alignment scores (quality); (2) An adaptive optimization engine then transforms suboptimal samples into pedagogically valuable training points while preserving semantic integrity; (3) This optimization process continuously evolves with model capability through dynamic learning principles. Experiments on multiple benchmarks demonstrate that our Middo consistently enhances the quality of seed data and boosts LLM's performance with improving accuracy by 7.15% on average while maintaining the original dataset scale. This work establishes a new paradigm for sustainable LLM training through dynamic human-AI co-evolution of data and models. Our datasets, models, and code are publicly available at https://github.com/Word2VecT/Middo.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Motivated Reasoning in Law: Evaluating Strategic Role Conditioning in LLM Summarization</title>
<link>https://arxiv.org/abs/2509.00529</link>
<guid>https://arxiv.org/abs/2509.00529</guid>
<content:encoded><![CDATA[
arXiv:2509.00529v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly used to generate user-tailored summaries, adapting outputs to specific stakeholders. In legal contexts, this raises important questions about motivated reasoning -- how models strategically frame information to align with a stakeholder's position within the legal system. Building on theories of legal realism and recent trends in legal practice, we investigate how LLMs respond to prompts conditioned on different legal roles (e.g., judges, prosecutors, attorneys) when summarizing judicial decisions. We introduce an evaluation framework grounded in legal fact and reasoning inclusion, also considering favorability towards stakeholders. Our results show that even when prompts include balancing instructions, models exhibit selective inclusion patterns that reflect role-consistent perspectives. These findings raise broader concerns about how similar alignment may emerge as LLMs begin to infer user roles from prior interactions or context, even without explicit role instructions. Our results underscore the need for role-aware evaluation of LLM summarization behavior in high-stakes legal settings.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training LLMs to be Better Text Embedders through Bidirectional Reconstruction</title>
<link>https://arxiv.org/abs/2509.03020</link>
<guid>https://arxiv.org/abs/2509.03020</guid>
<content:encoded><![CDATA[
arXiv:2509.03020v4 Announce Type: replace 
Abstract: Large language models (LLMs) have increasingly been explored as powerful text embedders. Existing LLM-based text embedding approaches often leverage the embedding of the final token, typically a reserved special token such as [EOS]. However, these tokens have not been intentionally trained to capture the semantics of the whole context, limiting their capacity as text embeddings, especially for retrieval and re-ranking tasks. We propose to add a new training stage before contrastive learning to enrich the semantics of the final token embedding. This stage employs bidirectional generative reconstruction tasks, namely EBQ2D (Embedding-Based Query-to-Document) and EBD2Q (Embedding-Based Document-to-Query), which interleave to anchor the [EOS] embedding and reconstruct either side of Query-Document pairs. Experimental results demonstrate that our additional training stage significantly improves LLM performance on the Massive Text Embedding Benchmark (MTEB), achieving new state-of-the-art results across different LLM base models and scales.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Reinforcement Learning for Large Reasoning Models</title>
<link>https://arxiv.org/abs/2509.08827</link>
<guid>https://arxiv.org/abs/2509.08827</guid>
<content:encoded><![CDATA[
arXiv:2509.08827v3 Announce Type: replace 
Abstract: In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged as a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further scaling of RL for LRMs now faces foundational challenges not only in computational resources but also in algorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications, to identify future opportunities and directions for this rapidly evolving area. We hope this review will promote future research on RL for broader reasoning models. Github: https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiChunk: Evaluating and Enhancing Retrieval-Augmented Generation with Hierarchical Chunking</title>
<link>https://arxiv.org/abs/2509.11552</link>
<guid>https://arxiv.org/abs/2509.11552</guid>
<content:encoded><![CDATA[
arXiv:2509.11552v3 Announce Type: replace 
Abstract: Retrieval-Augmented Generation (RAG) enhances the response capabilities of language models by integrating external knowledge sources. However, document chunking as an important part of RAG system often lacks effective evaluation tools. This paper first analyzes why existing RAG evaluation benchmarks are inadequate for assessing document chunking quality, specifically due to evidence sparsity. Based on this conclusion, we propose HiCBench, which includes manually annotated multi-level document chunking points, synthesized evidence-dense quetion answer(QA) pairs, and their corresponding evidence sources. Additionally, we introduce the HiChunk framework, a multi-level document structuring framework based on fine-tuned LLMs, combined with the Auto-Merge retrieval algorithm to improve retrieval quality. Experiments demonstrate that HiCBench effectively evaluates the impact of different chunking methods across the entire RAG pipeline. Moreover, HiChunk achieves better chunking quality within reasonable time consumption, thereby enhancing the overall performance of RAG systems.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Correction to Mastery: Reinforced Distillation of Large Language Model Agents</title>
<link>https://arxiv.org/abs/2509.14257</link>
<guid>https://arxiv.org/abs/2509.14257</guid>
<content:encoded><![CDATA[
arXiv:2509.14257v2 Announce Type: replace 
Abstract: Large Language Model agents excel at solving complex tasks through iterative reasoning and tool use, but typically depend on ultra-large, costly backbones. Existing distillation approaches train smaller students to imitate full teacher trajectories, yet reasoning and knowledge gaps between the teacher and student can cause compounding errors. We propose SCoRe, a student-centered framework in which the student generates training trajectories and the teacher corrects only the earliest error, producing training data matched to the student's ability and exposing specific weaknesses. The student is first fine-tuned on corrected trajectories. Subsequently, short-horizon reinforcement learning starts from the verified prefix preceding the earliest error, with target rewards assigned at that step. This design encourages autonomous problem-solving beyond imitation and enhances training stability. On 12 challenging benchmarks, a 7B-parameter student distilled with SCoRe matches the agentic performance of a 72B-parameter teacher.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DNA-DetectLLM: Unveiling AI-Generated Text via a DNA-Inspired Mutation-Repair Paradigm</title>
<link>https://arxiv.org/abs/2509.15550</link>
<guid>https://arxiv.org/abs/2509.15550</guid>
<content:encoded><![CDATA[
arXiv:2509.15550v2 Announce Type: replace 
Abstract: The rapid advancement of large language models (LLMs) has blurred the line between AI-generated and human-written text. This progress brings societal risks such as misinformation, authorship ambiguity, and intellectual property concerns, highlighting the urgent need for reliable AI-generated text detection methods. However, recent advances in generative language modeling have resulted in significant overlap between the feature distributions of human-written and AI-generated text, blurring classification boundaries and making accurate detection increasingly challenging. To address the above challenges, we propose a DNA-inspired perspective, leveraging a repair-based process to directly and interpretably capture the intrinsic differences between human-written and AI-generated text. Building on this perspective, we introduce DNA-DetectLLM, a zero-shot detection method for distinguishing AI-generated and human-written text. The method constructs an ideal AI-generated sequence for each input, iteratively repairs non-optimal tokens, and quantifies the cumulative repair effort as an interpretable detection signal. Empirical evaluations demonstrate that our method achieves state-of-the-art detection performance and exhibits strong robustness against various adversarial attacks and input lengths. Specifically, DNA-DetectLLM achieves relative improvements of 5.55% in AUROC and 2.08% in F1 score across multiple public benchmark datasets. Code and data are available at https://github.com/Xiaoweizhu57/DNA-DetectLLM.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating LLM-Generated Versus Human-Authored Responses in Role-Play Dialogues</title>
<link>https://arxiv.org/abs/2509.17694</link>
<guid>https://arxiv.org/abs/2509.17694</guid>
<content:encoded><![CDATA[
arXiv:2509.17694v2 Announce Type: replace 
Abstract: Evaluating large language models (LLMs) in long-form, knowledge-grounded role-play dialogues remains challenging. This study compares LLM-generated and human-authored responses in multi-turn professional training simulations through human evaluation ($N=38$) and automated LLM-as-a-judge assessment. Human evaluation revealed significant degradation in LLM-generated response quality across turns, particularly in naturalness, context maintenance and overall quality, while human-authored responses progressively improved. In line with this finding, participants also indicated a consistent preference for human-authored dialogue. These human judgements were validated by our automated LLM-as-a-judge evaluation, where Gemini 2.0 Flash achieved strong alignment with human evaluators on both zero-shot pairwise preference and stochastic 6-shot construct ratings, confirming the widening quality gap between LLM and human responses over time. Our work contributes a multi-turn benchmark exposing LLM degradation in knowledge-grounded role-play dialogues and provides a validated hybrid evaluation framework to guide the reliable integration of LLMs in training simulations.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speculate Deep and Accurate: Lossless and Training-Free Acceleration for Offloaded LLMs via Substitute Speculative Decoding</title>
<link>https://arxiv.org/abs/2509.18344</link>
<guid>https://arxiv.org/abs/2509.18344</guid>
<content:encoded><![CDATA[
arXiv:2509.18344v2 Announce Type: replace 
Abstract: The immense model sizes of large language models (LLMs) challenge deployment on memory-limited consumer GPUs. Although model compression and parameter offloading are common strategies to address memory limitations, compression can degrade quality, and offloading maintains quality but suffers from slow inference. Speculative decoding presents a promising avenue to accelerate parameter offloading, utilizing a fast draft model to propose multiple draft tokens, which are then verified by the target LLM in parallel with a single forward pass. This method reduces the time-consuming data transfers in forward passes that involve offloaded weight transfers. Existing methods often rely on pretrained weights of the same family, but require additional training to align with custom-trained models. Moreover, approaches that involve draft model training usually yield only modest speedups. This limitation arises from insufficient alignment with the target model, preventing higher token acceptance lengths. To address these challenges and achieve greater speedups, we propose SubSpec, a plug-and-play method to accelerate parameter offloading that is lossless and training-free. SubSpec constructs a highly aligned draft model by generating low-bit quantized substitute layers from offloaded target LLM portions. Additionally, our method shares the remaining GPU-resident layers and the KV-Cache, further reducing memory overhead and enhance alignment. SubSpec achieves a high average acceptance length, delivering 9.1x speedup for Qwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for Qwen2.5 32B on popular generation benchmarks (24GB VRAM limit).
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QoNext: Towards Next-generation QoE for Foundation Models</title>
<link>https://arxiv.org/abs/2509.21889</link>
<guid>https://arxiv.org/abs/2509.21889</guid>
<content:encoded><![CDATA[
arXiv:2509.21889v2 Announce Type: replace 
Abstract: Existing evaluations of foundation models, including recent human-centric approaches, fail to capture what truly matters: user's experience during interaction. Current methods treat evaluation as a matter of output correctness alone, overlooking that user satisfaction emerges from the interplay between response quality and interaction, which limits their ability to account for the mechanisms underlying user experience. To address this gap, we introduce QoNext, the first framework that adapts Quality of Experience (QoE) principles from networking and multimedia to the assessment of foundation models. QoNext identifies experiential factors that shape user experience and incorporates them into controlled experiments, where human ratings are collected under varied configurations. From these studies we construct a QoE-oriented database and train predictive models that estimate perceived user experience from measurable system parameters. Our results demonstrate that QoNext not only enables proactive and fine-grained evaluation but also provides actionable guidance for productized services of optimizing foundation models in practice.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced Language Models</title>
<link>https://arxiv.org/abs/2509.22536</link>
<guid>https://arxiv.org/abs/2509.22536</guid>
<content:encoded><![CDATA[
arXiv:2509.22536v2 Announce Type: replace 
Abstract: The immense computational cost of training Large Language Models (LLMs) presents a major barrier to innovation. While FP8 training offers a promising solution with significant theoretical efficiency gains, its widespread adoption has been hindered by the lack of a comprehensive, open-source training recipe. To bridge this gap, we introduce an end-to-end FP8 training recipe that seamlessly integrates continual pre-training and supervised fine-tuning. Our methodology employs a fine-grained, hybrid-granularity quantization strategy to maintain numerical fidelity while maximizing computational efficiency. Through extensive experiments, including the continue pre-training of models on a 160B-token corpus, we demonstrate that our recipe is not only remarkably stable but also essentially lossless, achieving performance on par with the BF16 baseline across a suite of reasoning benchmarks. Crucially, this is achieved with substantial efficiency improvements, including up to a 22% reduction in training time, a 14% decrease in peak memory usage, and a 19% increase in throughput. Our results establish FP8 as a practical and robust alternative to BF16, and we will release the accompanying code to further democratize large-scale model training.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.26383</link>
<guid>https://arxiv.org/abs/2509.26383</guid>
<content:encoded><![CDATA[
arXiv:2509.26383v3 Announce Type: replace 
Abstract: Knowledge-graph retrieval-augmented generation (KG-RAG) couples large language models (LLMs) with structured, verifiable knowledge graphs (KGs) to reduce hallucinations and expose reasoning traces. However, many KG-RAG systems compose multiple LLM modules (e.g planning, reasoning, and responding), inflating inference cost and binding behavior to a specific target KG. To address this, we introduce KG-R1, an agentic KG retrieval-augmented generation (KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single agent that interacts with KGs as its environment, learning to retrieve at each step and incorporating the retrieved information into its reasoning and generation. The process is optimized through end-to-end RL. In controlled experiments across Knowledge-Graph Question Answering (KGQA) benchmarks, our method demonstrates both efficiency and transferability: Using Qwen-2.5-3B, KG-R1 improves answer accuracy with fewer generation tokens than prior multi-module workflow methods that use larger foundation or fine-tuned models. Furthermore, KG-R1 enables plug and play: after training, it maintains strong accuracy on new KGs without modification. These properties make KG-R1 a promising KG-RAG framework for real-world deployment. Our code is publicly available at https://github.com/Jinyeop3110/KG-R1.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Matryoshka Pilot: Learning to Drive Black-Box LLMs with LLMs</title>
<link>https://arxiv.org/abs/2410.20749</link>
<guid>https://arxiv.org/abs/2410.20749</guid>
<content:encoded><![CDATA[
arXiv:2410.20749v2 Announce Type: replace-cross 
Abstract: Despite the impressive generative abilities of black-box large language models (LLMs), their inherent opacity hinders further advancements in capabilities such as reasoning, planning, and personalization. Existing works aim to enhance LLM capabilities via domain-specific adaptation, which require additional training on accessible model parameters, an infeasible option for black-box LLMs. To address this challenge, we introduce Matryoshka Pilot (M-Pilot), a lightweight white-box LLM controller that guides a large-scale black-box LLM generator by decomposing complex tasks into a series of intermediate outputs. Specifically, we consider the black-box LLM as an environment, with M-Pilot serving as a policy to provide intermediate guidance through prompts for driving the black-box LLM. M-Pilot is trained to pivot the outputs of the black-box LLM aligning with preferences during iterative interaction, which enables controllable multi-turn generation and self-improvement in optimizing intermediate guidance. Empirical evaluations on diverse tasks demonstrate that our method effectively enhances the capabilities of black-box LLMs in complex, long-horizon tasks.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoAgent: A Fully-Automated and Zero-Code Framework for LLM Agents</title>
<link>https://arxiv.org/abs/2502.05957</link>
<guid>https://arxiv.org/abs/2502.05957</guid>
<content:encoded><![CDATA[
arXiv:2502.05957v3 Announce Type: replace-cross 
Abstract: Large Language Model (LLM) Agents have demonstrated remarkable capabilities in task automation and intelligent decision-making, driving the widespread adoption of agent development frameworks such as LangChain and AutoGen. However, these frameworks predominantly serve developers with extensive technical expertise - a significant limitation considering that only 0.03 % of the global population possesses the necessary programming skills. This stark accessibility gap raises a fundamental question: Can we enable everyone, regardless of technical background, to build their own LLM agents using natural language alone? To address this challenge, we introduce AutoAgent-a Fully-Automated and highly Self-Developing framework that enables users to create and deploy LLM agents through Natural Language Alone. Operating as an autonomous Agent Operating System, AutoAgent comprises four key components: i) Agentic System Utilities, ii) LLM-powered Actionable Engine, iii) Self-Managing File System, and iv) Self-Play Agent Customization module. This lightweight yet powerful system enables efficient and dynamic creation and modification of tools, agents, and workflows without coding requirements or manual intervention. Beyond its code-free agent development capabilities, AutoAgent also serves as a versatile multi-agent system for General AI Assistants. Comprehensive evaluations on the GAIA benchmark demonstrate AutoAgent's effectiveness in generalist multi-agent tasks, surpassing existing state-of-the-art methods. Furthermore, AutoAgent's Retrieval-Augmented Generation (RAG)-related capabilities have shown consistently superior performance compared to many alternative LLM-based solutions.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instructor-Worker Large Language Model System for Policy Recommendation: a Case Study on Air Quality Analysis of the January 2025 Los Angeles Wildfires</title>
<link>https://arxiv.org/abs/2503.00566</link>
<guid>https://arxiv.org/abs/2503.00566</guid>
<content:encoded><![CDATA[
arXiv:2503.00566v4 Announce Type: replace-cross 
Abstract: The Los Angeles wildfires of January 2025 caused more than 250 billion dollars in damage and lasted for nearly an entire month before containment. Following our previous work, the Digital Twin Building, we modify and leverage the multi-agent large language model framework as well as the cloud-mapping integration to study the air quality during the Los Angeles wildfires. Recent advances in large language models have allowed for out-of-the-box automated large-scale data analysis. We use a multi-agent large language system comprised of an Instructor agent and Worker agents. Upon receiving the users' instructions, the Instructor agent retrieves the data from the cloud platform and produces instruction prompts to the Worker agents. The Worker agents then analyze the data and provide summaries. The summaries are finally input back into the Instructor agent, which then provides the final data analysis. We test this system's capability for data-based policy recommendation by assessing our Instructor-Worker LLM system's health recommendations based on air quality during the Los Angeles wildfires.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>More Bang for the Buck: Process Reward Modeling with Entropy-Driven Uncertainty</title>
<link>https://arxiv.org/abs/2503.22233</link>
<guid>https://arxiv.org/abs/2503.22233</guid>
<content:encoded><![CDATA[
arXiv:2503.22233v3 Announce Type: replace-cross 
Abstract: We introduce the Entropy-Driven Uncertainty Process Reward Model (EDU-PRM), a novel entropy-driven training framework for process reward modeling that enables dynamic, uncertainty-aligned segmentation of complex reasoning steps, eliminating the need for costly manual step annotations. Unlike previous Process Reward Models (PRMs) that rely on static partitioning and human labeling, EDU-PRM automatically anchors step boundaries at tokens with high predictive entropy, effectively capturing intrinsic logical transitions and facilitating efficient exploration of diverse reasoning paths. On the ProcessBench benchmark, EDU-PRM outperforms strong public PRM baselines, such as Math-Shepherd PRM and Omega PRM, and EDU-PRM achieves comparable results with SOTA models while only using 1.5% training data. Furthermore, by leveraging our proposed EDU sampling strategy, we observe accuracy boosts from 64.7% to 67.3% for generative reasoning tasks, accompanied by a reduction of 32% in token usage. These findings underscore the potential of EDU-PRM as a scalable and annotation-efficient paradigm for process supervision in mathematical reasoning, paving the way for more efficient and robust approaches to complex mathematical problem solving.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\textit{Agents Under Siege}$: Breaking Pragmatic Multi-Agent LLM Systems with Optimized Prompt Attacks</title>
<link>https://arxiv.org/abs/2504.00218</link>
<guid>https://arxiv.org/abs/2504.00218</guid>
<content:encoded><![CDATA[
arXiv:2504.00218v2 Announce Type: replace-cross 
Abstract: Most discussions about Large Language Model (LLM) safety have focused on single-agent settings but multi-agent LLM systems now create novel adversarial risks because their behavior depends on communication between agents and decentralized reasoning. In this work, we innovatively focus on attacking pragmatic systems that have constrains such as limited token bandwidth, latency between message delivery, and defense mechanisms. We design a $\textit{permutation-invariant adversarial attack}$ that optimizes prompt distribution across latency and bandwidth-constraint network topologies to bypass distributed safety mechanisms within the system. Formulating the attack path as a problem of $\textit{maximum-flow minimum-cost}$, coupled with the novel $\textit{Permutation-Invariant Evasion Loss (PIEL)}$, we leverage graph-based optimization to maximize attack success rate while minimizing detection risk. Evaluating across models including $\texttt{Llama}$, $\texttt{Mistral}$, $\texttt{Gemma}$, $\texttt{DeepSeek}$ and other variants on various datasets like $\texttt{JailBreakBench}$ and $\texttt{AdversarialBench}$, our method outperforms conventional attacks by up to $7\times$, exposing critical vulnerabilities in multi-agent systems. Moreover, we demonstrate that existing defenses, including variants of $\texttt{Llama-Guard}$ and $\texttt{PromptGuard}$, fail to prohibit our attack, emphasizing the urgent need for multi-agent specific safety mechanisms.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Utility-Focused LLM Annotation for Retrieval and Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2504.05220</link>
<guid>https://arxiv.org/abs/2504.05220</guid>
<content:encoded><![CDATA[
arXiv:2504.05220v5 Announce Type: replace-cross 
Abstract: This paper explores the use of large language models (LLMs) for annotating document utility in training retrieval and retrieval-augmented generation (RAG) systems, aiming to reduce dependence on costly human annotations. We address the gap between retrieval relevance and generative utility by employing LLMs to annotate document utility. To effectively utilize multiple positive samples per query, we introduce a novel loss that maximizes their summed marginal likelihood. Using the Qwen-2.5-32B model, we annotate utility on the MS MARCO dataset and conduct retrieval experiments on MS MARCO and BEIR, as well as RAG experiments on MS MARCO QA, NQ, and HotpotQA. Our results show that LLM-generated annotations enhance out-of-domain retrieval performance and improve RAG outcomes compared to models trained solely on human annotations or downstream QA metrics. Furthermore, combining LLM annotations with just 20% of human labels achieves performance comparable to using full human annotations. Our study offers a comprehensive approach to utilizing LLM annotations for initializing QA systems on new corpora.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient and Adaptable Overlapping for Computation and Communication via Signaling and Reordering</title>
<link>https://arxiv.org/abs/2504.19519</link>
<guid>https://arxiv.org/abs/2504.19519</guid>
<content:encoded><![CDATA[
arXiv:2504.19519v2 Announce Type: replace-cross 
Abstract: Generative models have achieved remarkable success across various applications, driving the demand for multi-GPU computing. Inter-GPU communication becomes a bottleneck in multi-GPU computing systems, particularly on consumer-grade GPUs. By exploiting concurrent hardware execution, overlapping computation and communication latency becomes an effective technique for mitigating the communication overhead. We identify that an efficient and adaptable overlapping design should satisfy (1) tile-wise overlapping to maximize the overlapping opportunity, (2) interference-free computation to maintain the original computational performance, and (3) communication agnosticism to reduce the development burden against varying communication primitives. Nevertheless, current designs fail to simultaneously optimize for all of those features. To address the issue, we propose FlashOverlap, which utilizes a novel signaling mechanism: when part of the output finishes, the computation kernel sends a signal to trigger the communication of that part, while continuing the computation of the remaining part (interference-free computation). Consequently, the communication of the finished part and the computation of the remaining part can be overlapped. On top of the signaling mechanism, FlashOverlap comprises two key components: (1) the determination of the signaling timing to boost the overlap efficiency (tile-wise overlapping), and (2) a pre-communication reordering to create the contiguous address for finished data, enabling communication by simply calling NCCL APIs (communication agnosticism), and a post-communication reordering to correct the data order. Experiments show that FlashOverlap achieves up to 1.65x speedup through overlap, outperforming existing works in most cases. Code is available at https://github.com/infinigence/FlashOverlap.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing AI Research Assistants with Expert-Involved Learning</title>
<link>https://arxiv.org/abs/2505.04638</link>
<guid>https://arxiv.org/abs/2505.04638</guid>
<content:encoded><![CDATA[
arXiv:2505.04638v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) and large multimodal models (LMMs) promise to accelerate biomedical discovery, yet their reliability remains unclear. We introduce ARIEL (AI Research Assistant for Expert-in-the-Loop Learning), an open-source evaluation and optimization framework that pairs a curated multimodal biomedical corpus with expert-vetted tasks to probe two capabilities: full-length article summarization and fine-grained figure interpretation. Using uniform protocols and blinded PhD-level evaluation, we find that state-of-the-art models generate fluent but incomplete summaries, whereas LMMs struggle with detailed visual reasoning. We later observe that prompt engineering and lightweight fine-tuning substantially improve textual coverage, and a compute-scaled inference strategy enhances visual question answering. We build an ARIEL agent that integrates textual and visual cues, and we show it can propose testable mechanistic hypotheses. ARIEL delineates current strengths and limitations of foundation models, and provides a reproducible platform for advancing trustworthy AI in biomedicine.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding In-context Learning of Addition via Activation Subspaces</title>
<link>https://arxiv.org/abs/2505.05145</link>
<guid>https://arxiv.org/abs/2505.05145</guid>
<content:encoded><![CDATA[
arXiv:2505.05145v3 Announce Type: replace-cross 
Abstract: To perform few-shot learning, language models extract signals from a few input-label pairs, aggregate these into a learned prediction rule, and apply this rule to new inputs. How is this implemented in the forward pass of modern transformer models? To explore this question, we study a structured family of few-shot learning tasks for which the true prediction rule is to add an integer $k$ to the input. We introduce a novel optimization method that localizes the model's few-shot ability to only a few attention heads. We then perform an in-depth analysis of individual heads, via dimensionality reduction and decomposition. As an example, on Llama-3-8B-instruct, we reduce its mechanism on our tasks to just three attention heads with six-dimensional subspaces, where four dimensions track the unit digit with trigonometric functions at periods $2$, $5$, and $10$, and two dimensions track magnitude with low-frequency components. To deepen our understanding of the mechanism, we also derive a mathematical identity relating ``aggregation'' and ``extraction'' subspaces for attention heads, allowing us to track the flow of information from individual examples to a final aggregated concept. Using this, we identify a self-correction mechanism where mistakes learned from earlier demonstrations are suppressed by later demonstrations. Our results demonstrate how tracking low-dimensional subspaces of localized heads across a forward pass can provide insight into fine-grained computational structures in language models.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's Math Capability</title>
<link>https://arxiv.org/abs/2505.23703</link>
<guid>https://arxiv.org/abs/2505.23703</guid>
<content:encoded><![CDATA[
arXiv:2505.23703v3 Announce Type: replace-cross 
Abstract: Enhancing the mathematical reasoning capabilities of LLMs has garnered significant attention in both the mathematical and computer science communities. Recent works have made substantial progress in both Natural Language (NL) reasoning and Formal Language (FL) reasoning by leveraging the potential of pure Reinforcement Learning (RL) methods on base models. However, RL approaches struggle to impart new capabilities not presented in the base model, highlighting the need to integrate more knowledge like FL into NL math reasoning effectively. Yet, this integration is challenging due to inherent disparities in problem structure and reasoning format between NL and FL. To address these challenges, we introduce **NL-FL HybridReasoning (NFL-HR)**, an end-to-end framework designed to incorporate the FL expert into NL math problem-solving. To bridge the NL and FL input format gap, we propose the NL-FL Problem Alignment method, which reformulates the Question-Answering (QA) problems in NL as existence theorems in FL. Subsequently, the Mixed Problem Input technique we provide enables the FL reasoner to handle both QA and existence problems concurrently. Lastly, we mitigate the NL and FL output format gap in reasoning through an LLM-based Answer Extraction mechanism. Comprehensive experiments demonstrate that the NFL-HR framework achieves **89.80**% and **84.34%** accuracy rates on the MATH-500 and the AMC benchmarks, surpassing the NL baseline by **4.60%** and **4.82%**, respectively. Notably, some problems resolved by our framework remain unsolved by the NL baseline model even under a larger number of trials.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Vision Language Models Infer Human Gaze Direction? A Controlled Study</title>
<link>https://arxiv.org/abs/2506.05412</link>
<guid>https://arxiv.org/abs/2506.05412</guid>
<content:encoded><![CDATA[
arXiv:2506.05412v2 Announce Type: replace-cross 
Abstract: The ability to infer what others are looking at is a critical component of a theory of mind that underpins natural human-AI interaction. We characterized this skill in 111 Vision Language Models (VLMs) and human participants (N = 65) using photos taken with manipulated difficulty and variability. We found that 94 of the 111 VLMs were not better than random guessing, while humans achieved near-ceiling accuracy. VLMs respond with each choice almost equally frequently. Are they randomly guessing? At least for five top-tier VLMs, their performance was above chance, declined with increasing task difficulty, but barely varied across different prompts and scene objects. These behavioral patterns cannot be explained by considering VLMs as random guessers. Instead, they likely utilize head orientation but not eye appearance to infer gaze direction, such that their performance is imperfect, subject to the task difficulty, but robust to superficial perceptual variations. This suggests that VLMs, lacking effective gaze inference skills, have yet to become technologies that can naturally interact with humans, but the potential remains.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Play to Generalize: Learning to Reason Through Game Play</title>
<link>https://arxiv.org/abs/2506.08011</link>
<guid>https://arxiv.org/abs/2506.08011</guid>
<content:encoded><![CDATA[
arXiv:2506.08011v4 Announce Type: replace-cross 
Abstract: Developing reasoning capabilities in multimodal large language models (MLLMs) remains challenging. Motivated by literature suggesting that gameplay promotes transferable reasoning skills, we propose a novel post-training method, Visual Game Learning (ViGaL), where MLLMs develop generalizable reasoning skills through playing arcade-like games. Specifically, we show that training a 7B-parameter MLLM via reinforcement learning (RL) on simple games like Snake significantly enhances the downstream performance on multimodal math benchmarks like MathVista, on multi-discipline questions like MMMU and on 3D spatial reasoning benchmarks like VSI-Bench, without seeing any worked solutions, equations, or diagrams during RL. Remarkably, our model outperforms specialist models post-trained on benchmark-oriented multimodal reasoning data, while preserving the model's performance on general visual benchmarks, a challenge where specialist models often fall short. Our findings suggest that multimodal reasoning can emerge from gameplay, pointing to a promising strategy of designing surrogate tasks for RL post-training.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining</title>
<link>https://arxiv.org/abs/2506.08022</link>
<guid>https://arxiv.org/abs/2506.08022</guid>
<content:encoded><![CDATA[
arXiv:2506.08022v3 Announce Type: replace-cross 
Abstract: The task adaptation and alignment of Large Multimodal Models (LMMs) have been significantly advanced by instruction tuning and further strengthened by recent preference optimization. Yet, most LMMs still suffer from severe modality imbalance during reasoning, i.e., outweighing language prior biases over visual inputs, which bottlenecks their generalization to downstream tasks and causes hallucinations. However, existing preference optimization approaches for LMMs do not focus on restraining the internal biases of their Large Language Model (LLM) backbones when curating the training data. Moreover, they heavily rely on offline data and lack the capacity to explore diverse responses adaptive to dynamic distributional shifts during training. Meanwhile, Group Relative Policy Optimization (GRPO), a recent method using online-generated data and verified rewards to improve reasoning capabilities, remains largely underexplored in LMM alignment. In this paper, we propose a novel preference learning framework, Modality-Balancing Preference Optimization (MBPO), to address the modality imbalance in LMMs. MBPO constructs a more effective offline preference dataset by generating hard negatives, i.e., rejected responses misled by LLM biases due to limited usage of visual information, through adversarial perturbation of input images. Moreover, MBPO leverages the easy-to-verify nature of close-ended tasks to generate online responses with verified rewards. GRPO is then employed to train the model with offline-online hybrid data. Extensive experiments demonstrate that MBPO can enhance LMM performance on challenging vision-language tasks and effectively reduce hallucinations.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think With Videos For Agentic Long-Video Understanding</title>
<link>https://arxiv.org/abs/2506.10821</link>
<guid>https://arxiv.org/abs/2506.10821</guid>
<content:encoded><![CDATA[
arXiv:2506.10821v4 Announce Type: replace-cross 
Abstract: Long-video understanding~(LVU) is a challenging problem in computer vision. Existing methods either downsample frames for single-pass reasoning, sacrificing fine-grained details, or depend on textual reasoning over task-agnostic representations, hindering task-specific perception and exploration. In this paper, we propose VideoExplorer, a framework grounded in the principle of ``thinking with video'', which naturally intertwines planning, temporal grounding, and scalable perception into a coherent reasoning process. Rather than reasoning over a static context, VideoExplorer iteratively formulates sub-questions, locates relevant moments, and performs task-oriented, temporally scalable video understanding until reaching the final answer, enabling faithful, efficient, and interpretable reasoning. To address the lack of LVU training resources, we construct a long-video reasoning dataset using difficulty-adaptive sampling to ensure high-quality trajectories on complex tasks. Building on this dataset, we design a two-stage training pipeline: supervised trajectory initialization followed by trajectory-level preference optimization, encouraging adaptive temporal grounding and iterative information integration guided by downstream rewards. Extensive evaluations on popular long-video understanding and reasoning benchmarks demonstrate VideoExplorer's significant advantage over existing baselines, highlighting its robustness, adaptability, and efficiency. Our code is made publicly available in this repository(https://github.com/yhy-2000/VideoDeepResearch).
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs on a Budget? Say HOLA</title>
<link>https://arxiv.org/abs/2506.18952</link>
<guid>https://arxiv.org/abs/2506.18952</guid>
<content:encoded><![CDATA[
arXiv:2506.18952v2 Announce Type: replace-cross 
Abstract: Running Large Language Models (LLMs) on edge devices is constrained by high compute and memory demands posing a barrier for real-time applications in sectors like healthcare, education, and embedded systems. Current solutions such as quantization, pruning, and retrieval-augmented generation (RAG) offer only partial optimizations and often compromise on speed or accuracy. We introduce HOLA, an end-to-end optimization framework for efficient LLM deployment. Internally, it leverages Hierarchical Speculative Decoding (HSD) for faster inference without quality loss. Externally, AdaComp-RAG adjusts retrieval complexity based on context needs. Together with LoBi, which blends structured pruning (LoRA) and quantization, HOLA delivers significant gains: 17.6% EMA on GSM8K, 10.5% MCA on ARC, and reduced latency and memory on edge devices like Jetson Nano--proving both scalable and production-ready.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs More Realistic and Less Risky</title>
<link>https://arxiv.org/abs/2507.03336</link>
<guid>https://arxiv.org/abs/2507.03336</guid>
<content:encoded><![CDATA[
arXiv:2507.03336v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are increasingly tasked with invoking enterprise APIs, yet they routinely falter when near-duplicate tools vie for the same user intent or when required arguments are left underspecified. We introduce DiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a disambiguation-centric, three-stage pipeline that (i) synthesizes persona-driven, multi-turn dialogues in which the assistant must distinguish among highly similar tools, (ii) performs supervised fine-tuning of open-source models with reasoning traces across 3B - 70B parameters, and (iii) evaluates real-world readiness via a dynamic suite that redeploys each model in a live agentic loop and reports end-to-end goal completion alongside conventional static metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE raise tool-invocation success by 27 pp over GPT-4o and by 49 pp over Claude-3.5-Sonnet, both under optimized prompting. To spur further research, we release an open corpus of 5000 production-grade enterprise API specifications paired with rigorously validated, disambiguation-focused dialogues, offering a practical blueprint for building reliable, enterprise-ready tool-calling agents.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zebra-CoT: A Dataset for Interleaved Vision Language Reasoning</title>
<link>https://arxiv.org/abs/2507.16746</link>
<guid>https://arxiv.org/abs/2507.16746</guid>
<content:encoded><![CDATA[
arXiv:2507.16746v2 Announce Type: replace-cross 
Abstract: Humans often use visual aids, for example diagrams or sketches, when solving complex problems. Training multimodal models to do the same, known as Visual Chain of Thought (Visual CoT), is challenging due to: (1) poor off-the-shelf visual CoT performance, which hinders reinforcement learning, and (2) the lack of high-quality visual CoT training data. We introduce $\textbf{Zebra-CoT}$, a diverse large-scale dataset with 182,384 samples, containing logically coherent interleaved text-image reasoning traces. We focus on four categories of tasks where sketching or visual reasoning is especially natural, spanning scientific questions such as geometry, physics, and algorithms; 2D visual reasoning tasks like visual search and jigsaw puzzles; 3D reasoning tasks including 3D multi-hop inference, embodied and robot planning; visual logic problems and strategic games like chess. Fine-tuning the Anole-7B model on the Zebra-CoT training corpus results in an improvement of +12% in our test-set accuracy and yields up to +13% performance gain on standard VLM benchmark evaluations. Fine-tuning Bagel-7B yields a model that generates high-quality interleaved visual reasoning chains, underscoring Zebra-CoT's effectiveness for developing multimodal reasoning abilities. We open-source our dataset and models to support development and evaluation of visual CoT.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distilling a Small Utility-Based Passage Selector to Enhance Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2507.19102</link>
<guid>https://arxiv.org/abs/2507.19102</guid>
<content:encoded><![CDATA[
arXiv:2507.19102v2 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating retrieved information. Standard retrieval process prioritized relevance, focusing on topical alignment between queries and passages. In contrast, in RAG, the emphasis has shifted to utility, which considers the usefulness of passages for generating accurate answers. Despite empirical evidence showing the benefits of utility-based retrieval in RAG, the high computational cost of using LLMs for utility judgments limits the number of passages evaluated. This restriction is problematic for complex queries requiring extensive information. To address this, we propose a method to distill the utility judgment capabilities of LLMs into smaller, more efficient models. Our approach focuses on utility-based selection rather than ranking, enabling dynamic passage selection tailored to specific queries without the need for fixed thresholds. We train student models to learn pseudo-answer generation and utility judgments from teacher LLMs, using a sliding window method that dynamically selects useful passages. Our experiments demonstrate that utility-based selection provides a flexible and cost-effective solution for RAG, significantly reducing computational costs while improving answer quality. We present the distillation results using Qwen3-32B as the teacher model for both relevance ranking and utility-based selection, distilled into RankQwen1.7B and UtilityQwen1.7B. Our findings indicate that for complex questions, utility-based selection is more effective than relevance ranking in enhancing answer generation performance. We will release the relevance ranking and utility-based selection annotations for the MS MARCO dataset, supporting further research in this area.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiple Memory Systems for Enhancing the Long-term Memory of Agent</title>
<link>https://arxiv.org/abs/2508.15294</link>
<guid>https://arxiv.org/abs/2508.15294</guid>
<content:encoded><![CDATA[
arXiv:2508.15294v2 Announce Type: replace-cross 
Abstract: An agent powered by large language models have achieved impressive results, but effectively handling the vast amounts of historical data generated during interactions remains a challenge. The current approach is to design a memory module for the agent to process these data. However, existing methods, such as MemoryBank and A-MEM, have poor quality of stored memory content, which affects recall performance and response quality. In order to better construct high-quality long-term memory content, we have designed a multiple memory system (MMS) inspired by cognitive psychology theory. The system processes short-term memory to multiple long-term memory fragments, and constructs retrieval memory units and contextual memory units based on these fragments, with a one-to-one correspondence between the two. During the retrieval phase, MMS will match the most relevant retrieval memory units based on the user's query. Then, the corresponding contextual memory units is obtained as the context for the response stage to enhance knowledge, thereby effectively utilizing historical data. Experiments on LoCoMo dataset compared our method with three others, proving its effectiveness. Ablation studies confirmed the rationality of our memory units. We also analyzed the robustness regarding the number of selected memory segments and the storage overhead, demonstrating its practical value.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative Decoding</title>
<link>https://arxiv.org/abs/2509.18085</link>
<guid>https://arxiv.org/abs/2509.18085</guid>
<content:encoded><![CDATA[
arXiv:2509.18085v2 Announce Type: replace-cross 
Abstract: Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to autoregressive LLMs (AR-LLMs) with the potential to operate at significantly higher token generation rates. However, currently available open-source dLLMs often generate at much lower rates, typically decoding only a single token at every denoising timestep in order to maximize output quality. We present Spiffy, a speculative decoding algorithm that accelerates dLLM inference by $\mathbf{2.8{-}3.1\times}$ while provably preserving the model's output distribution. This work addresses the unique challenges involved in applying ideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes draft states by leveraging the dLLM's distribution itself in an auto-speculative manner. This approach is efficient and effective, and eliminates the overheads of training and running an independent draft model. To structure the candidate draft states, we propose a novel directed draft graph which is uniquely designed to take advantage of the bidirectional, block-wise nature of dLLM generation and can be verified in parallel by the dLLM. To further optimize the structure of these draft graphs, we introduce an efficient, offline calibration algorithm that procedurally determines high-quality graph configurations. These optimized draft graphs, enabling increased acceptance rates, lead to a significant boost in the overall speedup achieved by the system. Crucially, Spiffy is also complementary to other recent innovations in improving dLLM generation speeds such as KV-caching and multi-token unmasking. We demonstrate that when combined with such parallel decoding algorithms, Spiffy is able to effectively multiply the benefits of these methods leading to total speedups of up to $\mathbf{7.9\times}$.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.22601</link>
<guid>https://arxiv.org/abs/2509.22601</guid>
<content:encoded><![CDATA[
arXiv:2509.22601v2 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) is the dominant paradigm for sharpening strategic tool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks, yet it faces a fundamental challenge of exploration-exploitation trade-off. Existing studies stimulate exploration through the lens of policy entropy, but such mechanical entropy maximization is prone to RL training instability due to the multi-turn distribution shifting. In this paper, we target the progressive exploration-exploitation balance under the guidance of the agent own experiences without succumbing to either entropy collapsing or runaway divergence. We propose SPEAR, a curriculum-based self-imitation learning (SIL) recipe for training agentic LLMs. It extends the vanilla SIL framework, where a replay buffer stores self-generated promising trajectories for off-policy update, by gradually steering the policy evolution within a well-balanced range of entropy across stages. Specifically, our approach incorporates a curriculum to manage the exploration process, utilizing intrinsic rewards to foster skill-level exploration and facilitating action-level exploration through SIL. At first, the auxiliary tool call reward plays a critical role in the accumulation of tool-use skills, enabling broad exposure to the unfamiliar distributions of the environment feedback with an upward entropy trend. As training progresses, self-imitation gets strengthened to exploit existing successful patterns from replayed experiences for comparative action-level exploration, accelerating solution iteration without unbounded entropy growth. To further stabilize training, we recalibrate the advantages of experiences in the replay buffer to address the potential policy drift. Reugularizations such as the clipping of tokens with high covariance between probability and advantage are introduced to the trajectory-level entropy control to curb over-confidence.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kimi-Dev: Agentless Training as Skill Prior for SWE-Agents</title>
<link>https://arxiv.org/abs/2509.23045</link>
<guid>https://arxiv.org/abs/2509.23045</guid>
<content:encoded><![CDATA[
arXiv:2509.23045v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are increasingly applied to software engineering (SWE), with SWE-bench as a key benchmark. Solutions are split into SWE-Agent frameworks with multi-turn interactions and workflow-based Agentless methods with single-turn verifiable steps. We argue these paradigms are not mutually exclusive: reasoning-intensive Agentless training induces skill priors, including localization, code edit, and self-reflection that enable efficient and effective SWE-Agent adaptation. In this work, we first curate the Agentless training recipe and present Kimi-Dev, an open-source SWE LLM achieving 60.4\% on SWE-bench Verified, the best among workflow approaches. With additional SFT adaptation on 5k publicly-available trajectories, Kimi-Dev powers SWE-Agents to 48.6\% pass@1, on par with that of Claude 3.5 Sonnet (241022 version). These results show that structured skill priors from Agentless training can bridge workflow and agentic frameworks for transferable coding agents.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>p-less Sampling: A Robust Hyperparameter-Free Approach for LLM Decoding</title>
<link>https://arxiv.org/abs/2509.23234</link>
<guid>https://arxiv.org/abs/2509.23234</guid>
<content:encoded><![CDATA[
arXiv:2509.23234v3 Announce Type: replace-cross 
Abstract: Obtaining high-quality outputs from Large Language Models (LLMs) often depends upon the choice of a sampling-based decoding strategy to probabilistically choose the next token at each generation step. While a variety of such sampling methods have been proposed, their performance can be sensitive to the selection of hyperparameters which may require different settings depending upon the generation task and temperature configuration. In this work, we introduce $p$-less sampling: an information-theoretic approach to sampling which dynamically sets a truncation threshold at each decoding step based on the entire token probability distribution. Unlike existing methods, $p$-less sampling has no hyperparameters and consistently produces high-quality outputs as temperature increases. We provide theoretical perspectives on $p$-less sampling to ground our proposed method and conduct experiments to empirically validate its effectiveness across a range of math, logical reasoning, and creative writing tasks. Our results demonstrate how $p$-less sampling consistently outperforms existing sampling approaches while exhibiting much less degradation in text quality at higher temperature values. We further show how $p$-less achieves greater inference-time efficiency than alternative methods through lower average token sampling times and shorter generation lengths, without sacrificing accuracy. Finally, we provide analyses to highlight the benefits of $p$-less through qualitative examples, case studies, and diversity assessments.
]]></content:encoded>
<pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Injecting External Knowledge into the Reasoning Process Enhances Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2507.19333</link>
<guid>https://arxiv.org/abs/2507.19333</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-augmented generation, noisy passages, robustness, Passage Injection, RAG systems

Summary: 
Passage Injection is proposed as a method to enhance large language models' (LLMs) robustness to noisy retrieved passages in retrieval-augmented generation (RAG). This method explicitly incorporates retrieved passages into LLMs' reasoning process, improving the model's ability to recognize and resist noisy passages. Experiments on four reasoning-enhanced LLMs across factual question answering datasets validate Passage Injection's effectiveness in enhancing overall RAG performance. The study also demonstrates that Passage Injection improves the model's robustness in the presence of random noise and counterfactual noise, consistently showing performance gains. Controlled experiments confirm that Passage Injection can effectively leverage helpful passages as well. These findings suggest that incorporating retrieved passages in LLMs' reasoning process is a promising approach for building more reliable RAG systems.<br /><br />Summary: <div>
arXiv:2507.19333v3 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) has been widely adopted to augment large language models (LLMs) with external knowledge for knowledge-intensive tasks. However, its effectiveness is often undermined by the presence of noisy (i.e., low-quality) retrieved passages. Enhancing LLMs' robustness to such noise is critical for improving the reliability of RAG systems. Recent advances have equipped LLMs with strong reasoning and self-reflection capabilities, allowing them to identify and correct errors in their reasoning process. Inspired by this ability, we propose Passage Injection-a simple yet effective method that explicitly incorporates retrieved passages into LLMs' reasoning process, aiming to enhance the model's ability to recognize and resist noisy passages. We validate Passage Injection under general RAG settings using BM25 as the retriever. Experiments on four reasoning-enhanced LLMs across four factual QA datasets demonstrate that Passage Injection significantly improves overall RAG performance. Further analysis on two noisy retrieval settings-random noise, where the model is provided irrelevant passages, and counterfactual noise, where it is given misleading passages-shows that Passage Injection consistently improves robustness. Controlled experiments confirm that Passage Injection can also effectively leverage helpful passages. These findings suggest that incorporating passages in LLMs' reasoning process is a promising direction for building more robust RAG systems. The code can be found \href{here}{https://github.com/Trustworthy-Information-Access/Passage-Injection}.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenStaxQA: A multilingual dataset based on open-source college textbooks</title>
<link>https://arxiv.org/abs/2510.06239</link>
<guid>https://arxiv.org/abs/2510.06239</guid>
<content:encoded><![CDATA[
<div> evaluation benchmark, college-level educational applications, large language models, OpenStaxQA, zero-shot evaluation

Summary:
OpenStaxQA introduces a new evaluation benchmark specifically tailored for college-level educational applications, utilizing 43 open-source textbooks in multiple languages. The study focuses on finetuning and evaluating large language models (LLMs) with approximately 7 billion parameters using quantized low rank adapters (QLoRa) on this dataset. Additionally, a zero-shot evaluation on the AI2 reasoning challenge dev dataset is conducted to assess if OpenStaxQA can enhance performance on diverse tasks. The research also addresses broader impacts associated with datasets like OpenStaxQA, emphasizing the importance of such resources for advancing educational technologies and artificial intelligence applications in academia. <div>
arXiv:2510.06239v1 Announce Type: new 
Abstract: We present OpenStaxQA, an evaluation benchmark specific to college-level educational applications based on 43 open-source college textbooks in English, Spanish, and Polish, available under a permissive Creative Commons license. We finetune and evaluate large language models (LLMs) with approximately 7 billion parameters on this dataset using quantized low rank adapters (QLoRa). Additionally we also perform a zero-shot evaluation on the AI2 reasoning challenge dev dataset in order to check if OpenStaxQA can lead to an improved performance on other tasks. We also discuss broader impacts relevant to datasets such as OpenStaxQA.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial Question Answering with Datasets</title>
<link>https://arxiv.org/abs/2510.06240</link>
<guid>https://arxiv.org/abs/2510.06240</guid>
<content:encoded><![CDATA[
<div> Knowledge graph, Multi-Agent System Distillation, reasoning depth, verifiability, industrial QA <br />
<br />Summary: Industrial question-answering systems demand high safety and reliability, especially in high-risk scenarios. Existing large language models enhance reasoning but lack control over outputs. Our proposed KG-MASD method utilizes a knowledge graph to improve state representation and ensure convergence in distillation. By integrating collaborative reasoning with knowledge grounding, KG-MASD generates high-confidence data for instruction tuning and distills reasoning depth and verifiability into compact student models suitable for deployment at the edge. Experimental results on an industrial QA dataset show significant accuracy improvement over baselines, ensuring trustworthy AI deployment in safety-critical industrial environments. <div>
arXiv:2510.06240v1 Announce Type: new 
Abstract: Industrial question-answering (QA) systems require higher safety and reliability than general-purpose dialogue models, as errors in high-risk scenarios such as equipment fault diagnosis can have severe consequences. Although multi-agent large language models enhance reasoning depth, they suffer from uncontrolled iterations and unverifiable outputs, and conventional distillation methods struggle to transfer collaborative reasoning capabilities to lightweight, deployable student models. To address these challenges, we propose Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD). Our approach formulates distillation as a Markov Decision Process and incorporates a knowledge graph as a verifiable structured prior to enrich state representation and ensure convergence. By integrating collaborative reasoning with knowledge grounding, KG-MASD generates high-confidence instruction-tuning data and jointly distills reasoning depth and verifiability into compact student models suitable for edge deployment. Experiments on an industrial QA dataset show that KG-MASD improves accuracy by 2.4 per cent to 20.1 per cent over baselines and significantly enhances reliability, enabling trustworthy AI deployment in safety-critical industrial scenarios. Code and data are available at https://github.com/erwinmsmith/KG-MAD/.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transparent Reference-free Automated Evaluation of Open-Ended User Survey Responses</title>
<link>https://arxiv.org/abs/2510.06242</link>
<guid>https://arxiv.org/abs/2510.06242</guid>
<content:encoded><![CDATA[
<div> Keywords: survey responses, gibberish filtering, evaluation framework, human-written, response quality prediction

Summary:
The article introduces a two-stage evaluation framework for assessing human survey responses, addressing the challenges associated with low-quality responses in marketing research. The framework includes gibberish filtering to remove nonsensical responses and evaluates responses based on three dimensions: effort, relevance, and completeness, leveraging LLM capabilities. Validation on English and Korean datasets shows that the proposed framework outperforms existing metrics in evaluating human-written responses. The framework demonstrates high practical applicability for real-world applications such as response quality prediction and response rejection, displaying strong correlations with expert assessment. The study fills a gap in the field by focusing on human-written survey responses and provides valuable insights for improving the quality and reliability of marketing research data. 

<br /><br />Summary: 
- Introduction of a two-stage evaluation framework for human survey responses
- Inclusion of gibberish filtering and evaluation based on effort, relevance, and completeness
- Validation on English and Korean datasets showcasing superior performance compared to existing metrics
- Practical applicability for response quality prediction and response rejection in marketing research
- Contribution to improving the quality and reliability of survey data through effective evaluation techniques <div>
arXiv:2510.06242v1 Announce Type: new 
Abstract: Open-ended survey responses provide valuable insights in marketing research, but low-quality responses not only burden researchers with manual filtering but also risk leading to misleading conclusions, underscoring the need for effective evaluation. Existing automatic evaluation methods target LLM-generated text and inadequately assess human-written responses with their distinct characteristics. To address such characteristics, we propose a two-stage evaluation framework specifically designed for human survey responses. First, gibberish filtering removes nonsensical responses. Then, three dimensions-effort, relevance, and completeness-are evaluated using LLM capabilities, grounded in empirical analysis of real-world survey data. Validation on English and Korean datasets shows that our framework not only outperforms existing metrics but also demonstrates high practical applicability for real-world applications such as response quality prediction and response rejection, showing strong correlations with expert assessment.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoT Referring: Improving Referring Expression Tasks with Grounded Reasoning</title>
<link>https://arxiv.org/abs/2510.06243</link>
<guid>https://arxiv.org/abs/2510.06243</guid>
<content:encoded><![CDATA[
<div> Keywords: Referring Expression Comprehension, Segmentation, Multimodal Large Language Models, CoT Referring, Training Data Structure

Summary: 
CoT Referring introduces a novel strategy to enhance cross-modal reasoning in Multimodal Large Language Models (MLLMs) for Referring Expression Comprehension and Segmentation tasks. By structuring training data into a sequential referring step approach, relationships are identified and consistent reference alignment is ensured, improving accuracy in complex query scenarios. The method restructures training data to enforce a new output form, creating a benchmark explicitly designed for complex referring cases. This approach integrates detection and segmentation capabilities into a unified MLLM framework and utilizes adaptive weighted loss for optimization. Experimental results show a 2.5%+ increase in performance over baseline models on curated benchmark and RefCOCO/+/g datasets.<br /><br />Summary: <div>
arXiv:2510.06243v1 Announce Type: new 
Abstract: Referring Expression Comprehension and Segmentation are critical tasks for assessing the integration of language understanding and image comprehension, serving as benchmarks for Multimodal Large Language Models (MLLMs) capabilities. To address these challenges, we propose a new strategy, CoT Referring, which enhances model reasoning across modalities through a structured, chain-of-thought training data structure. Our approach systematically parses textual structures to a sequential referring step, where in each step it identifies relationships and ensures consistent reference alignment, thereby improving accuracy in complex query scenarios. We restructure the training data to enforce a new output form, providing new annotations for existing datasets and compiling an evaluation benchmark from existing resources. This benchmark is designed explicitly for complex referring cases. We also integrate detection and segmentation capabilities into a unified MLLM framework, training it with a novel adaptive weighted loss to optimize performance. Experimental results on our curated benchmark and RefCOCO/+/g demonstrate the effectiveness of our approach, with a notable increase of 2.5%+ over baseline models.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Embedding Frameworks for Scientific Domain</title>
<link>https://arxiv.org/abs/2510.06244</link>
<guid>https://arxiv.org/abs/2510.06244</guid>
<content:encoded><![CDATA[
<div> Keywords: word representation, tokenization, scientific domain, NLP tasks, evaluation suite

Summary: 
The study focuses on finding the optimal word representation and tokenization algorithms for the scientific domain. It addresses the challenge of different meanings and representations of words based on context and domain-specific data. Generative AI and transformer architectures are effective but computationally intensive, particularly for pre-training from scratch. The research aims to achieve two objectives: identifying suitable word representation and tokenization methods for NLP tasks in the scientific domain, and constructing a comprehensive evaluation suite to assess these algorithms. The evaluation suite includes various downstream tasks and corresponding datasets. By testing different algorithms using the suite, the study aims to enhance word representation and tokenization methods for the scientific domain. This research contributes to the development of more efficient and effective algorithms for word representation and tokenization in scientific NLP tasks.

<br /><br />Summary: <div>
arXiv:2510.06244v1 Announce Type: new 
Abstract: Finding an optimal word representation algorithm is particularly important in terms of domain specific data, as the same word can have different meanings and hence, different representations depending on the domain and context. While Generative AI and transformer architecture does a great job at generating contextualized embeddings for any given work, they are quite time and compute extensive, especially if we were to pre-train such a model from scratch. In this work, we focus on the scientific domain and finding the optimal word representation algorithm along with the tokenization method that could be used to represent words in the scientific domain. The goal of this research is two fold: 1) finding the optimal word representation and tokenization methods that can be used in downstream scientific domain NLP tasks, and 2) building a comprehensive evaluation suite that could be used to evaluate various word representation and tokenization algorithms (even as new ones are introduced) in the scientific domain. To this end, we build an evaluation suite consisting of several downstream tasks and relevant datasets for each task. Furthermore, we use the constructed evaluation suite to test various word representation and tokenization algorithms.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRepLiNa: Layer-wise CKA+REPINA Alignment Improves Low-Resource Machine Translation in Aya-23 8B</title>
<link>https://arxiv.org/abs/2510.06249</link>
<guid>https://arxiv.org/abs/2510.06249</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Models, Low-Resource Languages, Translation Quality, Centered Kernel Alignment, REPINA

Summary:
The study focuses on improving translation quality from low-resource languages (LRLs) to high-resource languages (HRL) using a decoder-only multilingual large language model (LLM) by enforcing cross-lingual similarity. The method called TRepLiNa combines Centered Kernel Alignment (CKA) and REPINA to align mid-level layers of the model. Experiments were conducted using Aya-23 8B with QLoRA for Mundari, Santali, and Bhili languages with Hindi/English pivots. Results indicate that TRepLiNa is effective in enhancing LRL translation, particularly in data-scarce scenarios. This approach offers a practical and cost-effective solution for addressing linguistic gaps in low-resource contexts, such as those prevalent in India.<br /><br />Summary: The study explores the use of TRepLiNa, a method combining CKA and REPINA, to align mid-level layers of a multilingual LLM for improving LRL to HRL translation. Experimental results with Aya-23 8B on MMLoSo shared task language pairs demonstrate the effectiveness of TRepLiNa in enhancing translation quality in low-resource settings. <div>
arXiv:2510.06249v1 Announce Type: new 
Abstract: The 2025 Multimodal Models for Low-Resource Contexts and Social Impact (MMLoSo) Language Challenge addresses one of India's most pressing linguistic gaps: the lack of resources for its diverse low-resource languages (LRLs). In this study, we investigate whether enforcing cross-lingual similarity in specific internal layers of a decoder-only multilingual large language model (LLM) can improve translation quality from LRL to high-resource language (HRL). Specifically, we combine Centered Kernel Alignment (CKA), a similarity metric that encourages representations of different languages to align, with REPINA, a regularization method that constrains parameter updates to remain close to the pretrained model, into a joint method we call TRepLiNa. In this research project, we experiment with zero-shot, few-shot, and fine-tuning settings using Aya-23 8B with QLoRA across MMLoSo shared task language pairs (Mundari, Santali, Bhili) with Hindi/English pivots. Our results show that aligning mid-level layers using TRepLiNa (CKA+REPINA) is a low-cost, practical approach to improving LRL translation, especially in data-scarce settings.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable multilingual PII annotation for responsible AI in LLMs</title>
<link>https://arxiv.org/abs/2510.06250</link>
<guid>https://arxiv.org/abs/2510.06250</guid>
<content:encoded><![CDATA[
<div> framework, multilingual, PII annotation, data curation, language models<br />
Summary:<br />
This article introduces a scalable multilingual data curation framework for high-quality Personally Identifiable Information (PII) annotation across 13 locales, covering 336 locale-specific PII types. The phased annotation methodology combines linguistic expertise with quality assurance, improving recall and false positive rates. By analyzing inter-annotator agreement and addressing annotation inconsistencies, the framework produces high-fidelity datasets for training large language models. The study also addresses common challenges in multilingual PII labeling and demonstrates how analytics-driven pipelines can enhance both annotation quality and model reliability. The work emphasizes the importance of reliable PII handling in diverse regulatory contexts and showcases the benefits of iterative improvements in annotation processes. <div>
arXiv:2510.06250v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) gain wider adoption, ensuring their reliable handling of Personally Identifiable Information (PII) across diverse regulatory contexts has become essential. This work introduces a scalable multilingual data curation framework designed for high-quality PII annotation across 13 underrepresented locales, covering approximately 336 locale-specific PII types. Our phased, human-in-the-loop annotation methodology combines linguistic expertise with rigorous quality assurance, leading to substantial improvements in recall and false positive rates from pilot, training, and production phases. By leveraging inter-annotator agreement metrics and root-cause analysis, the framework systematically uncovers and resolves annotation inconsistencies, resulting in high-fidelity datasets suitable for supervised LLM fine-tuning. Beyond reporting empirical gains, we highlight common annotator challenges in multilingual PII labeling and demonstrate how iterative, analytics-driven pipelines can enhance both annotation quality and downstream model reliability.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prakriti200: A Questionnaire-Based Dataset of 200 Ayurvedic Prakriti Assessments</title>
<link>https://arxiv.org/abs/2510.06262</link>
<guid>https://arxiv.org/abs/2510.06262</guid>
<content:encoded><![CDATA[
<div> Keywords: dataset, Prakriti Assessment Questionnaire, Ayurvedic principles, computational intelligence, personalized health analytics

Summary:
This dataset includes responses to a standardized bilingual Prakriti Assessment Questionnaire for evaluating physical, physiological, and psychological characteristics based on Ayurvedic principles. The questionnaire covers various traits following AYUSH/CCRAS guidelines and enables dosha-specific scoring of individual traits. Data collection was automated via Google Forms to support research in computational intelligence, Ayurvedic studies, and personalized health analytics. The dataset allows for analyzing trait distributions, correlations, and predictive modeling. It also serves as a reference for future Prakriti-based studies and the development of intelligent health applications. <div>
arXiv:2510.06262v1 Announce Type: new 
Abstract: This dataset provides responses to a standardized, bilingual (English-Hindi) Prakriti Assessment Questionnaire designed to evaluate the physical, physiological, and psychological characteristics of individuals according to classical Ayurvedic principles. The questionnaire consists of 24 multiple-choice items covering body features, appetite, sleep patterns, energy levels, and temperament. It was developed following AYUSH/CCRAS guidelines to ensure comprehensive and accurate data collection. All questions are mandatory and neutrally phrased to minimize bias, and dosha labels (Vata, Pitta, Kapha) are hidden from participants. Data were collected via a Google Forms deployment, enabling automated scoring of responses to map individual traits to dosha-specific scores. The resulting dataset provides a structured platform for research in computational intelligence, Ayurvedic studies, and personalized health analytics, supporting analysis of trait distributions, correlations, and predictive modeling. It can also serve as a reference for future Prakriti-based studies and the development of intelligent health applications.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual-stage and Lightweight Patient Chart Summarization for Emergency Physicians</title>
<link>https://arxiv.org/abs/2510.06263</link>
<guid>https://arxiv.org/abs/2510.06263</guid>
<content:encoded><![CDATA[
<div> Keywords: Electronic health records, Emergency physicians, Clinical summarization, Privacy, Jetson Nano

Summary: 
The article introduces a two-stage summarization system that operates on embedded devices for offline clinical summarization while maintaining patient privacy. The system consists of two Jetson Nano devices, one for retrieval and the other for summarization, communicating via a lightweight socket link. The retrieval stage retrieves relevant patient record sections, while the generation stage uses a small language model to produce a structured summary. Benchmarking six open-source language models helped identify suitable models, and an evaluation mechanism was incorporated to assess summary quality. Preliminary results show that the system can effectively produce useful summaries in under 30 seconds using real EHR data. The system focuses on generating fixed-format critical findings and context-specific narrative for emergency physicians, aiming for factual accuracy, completeness, and clarity. <div>
arXiv:2510.06263v1 Announce Type: new 
Abstract: Electronic health records (EHRs) contain extensive unstructured clinical data that can overwhelm emergency physicians trying to identify critical information. We present a two-stage summarization system that runs entirely on embedded devices, enabling offline clinical summarization while preserving patient privacy. In our approach, a dual-device architecture first retrieves relevant patient record sections using the Jetson Nano-R (Retrieve), then generates a structured summary on another Jetson Nano-S (Summarize), communicating via a lightweight socket link. The summarization output is two-fold: (1) a fixed-format list of critical findings, and (2) a context-specific narrative focused on the clinician's query. The retrieval stage uses locally stored EHRs, splits long notes into semantically coherent sections, and searches for the most relevant sections per query. The generation stage uses a locally hosted small language model (SLM) to produce the summary from the retrieved text, operating within the constraints of two NVIDIA Jetson devices. We first benchmarked six open-source SLMs under 7B parameters to identify viable models. We incorporated an LLM-as-Judge evaluation mechanism to assess summary quality in terms of factual accuracy, completeness, and clarity. Preliminary results on MIMIC-IV and de-identified real EHRs demonstrate that our fully offline system can effectively produce useful summaries in under 30 seconds.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey of Hallucination in Large Language Models: Causes, Detection, and Mitigation</title>
<link>https://arxiv.org/abs/2510.06265</link>
<guid>https://arxiv.org/abs/2510.06265</guid>
<content:encoded><![CDATA[
<div> hallucination, LLMs, detection, mitigation, evaluation<br />
Summary: <br />
The article discusses the issue of hallucination in Large Language Models (LLMs), where they generate content that is fluent but factually inaccurate. It presents a taxonomy of hallucination types and the root causes across the LLM development lifecycle. The emergence of hallucinations in natural language generation tasks is analyzed. Detection approaches and mitigation strategies are structured in a taxonomy, along with a review of current approaches' strengths and limitations. Evaluation benchmarks and metrics used to quantify LLMs hallucinations are also examined. The article concludes with key open challenges and future research directions to develop more truthful and trustworthy LLMs. <div>
arXiv:2510.06265v1 Announce Type: new 
Abstract: Large language models (LLMs) have transformed natural language processing, achieving remarkable performance across diverse tasks. However, their impressive fluency often comes at the cost of producing false or fabricated information, a phenomenon known as hallucination. Hallucination refers to the generation of content by an LLM that is fluent and syntactically correct but factually inaccurate or unsupported by external evidence. Hallucinations undermine the reliability and trustworthiness of LLMs, especially in domains requiring factual accuracy. This survey provides a comprehensive review of research on hallucination in LLMs, with a focus on causes, detection, and mitigation. We first present a taxonomy of hallucination types and analyze their root causes across the entire LLM development lifecycle, from data collection and architecture design to inference. We further examine how hallucinations emerge in key natural language generation tasks. Building on this foundation, we introduce a structured taxonomy of detection approaches and another taxonomy of mitigation strategies. We also analyze the strengths and limitations of current detection and mitigation approaches and review existing evaluation benchmarks and metrics used to quantify LLMs hallucinations. Finally, we outline key open challenges and promising directions for future research, providing a foundation for the development of more truthful and trustworthy LLMs.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language models for longitudinal analysis of abusive content in Billboard Music Charts</title>
<link>https://arxiv.org/abs/2510.06266</link>
<guid>https://arxiv.org/abs/2510.06266</guid>
<content:encoded><![CDATA[
<div> Keywords: abusive content, sexually explicit content, Billboard Music Charts, deep learning, sentiment analysis

Summary: 
Using deep learning methods, this study examines the trend of abusive and sexually explicit content in music from Billboard Music Charts over seven decades. The analysis reveals a significant increase in explicit content, particularly since 1990, with a rise in profane, sexually explicit, and inappropriate language in popular songs. The study also highlights the evolving societal norms and language use reflected in lyrical content over time. The findings underscore the need for effective policy development to address the harmful effects of such content on children and youths. The use of sentiment analysis and abuse detection provides valuable insights into the prevalence of explicit language in music and its impact on listeners. As the music industry continues to evolve, understanding these patterns is crucial for promoting responsible content in the media.<br /><br />Summary: <div>
arXiv:2510.06266v1 Announce Type: new 
Abstract: There is no doubt that there has been a drastic increase in abusive and sexually explicit content in music, particularly in Billboard Music Charts. However, there is a lack of studies that validate the trend for effective policy development, as such content has harmful behavioural changes in children and youths. In this study, we utilise deep learning methods to analyse songs (lyrics) from Billboard Charts of the United States in the last seven decades. We provide a longitudinal study using deep learning and language models and review the evolution of content using sentiment analysis and abuse detection, including sexually explicit content. Our results show a significant rise in explicit content in popular music from 1990 onwards. Furthermore, we find an increasing prevalence of songs with lyrics containing profane, sexually explicit, and otherwise inappropriate language. The longitudinal analysis of the ability of language models to capture nuanced patterns in lyrical content, reflecting shifts in societal norms and language use over time.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reproducibility Study of "XRec: Large Language Models for Explainable Recommendation"</title>
<link>https://arxiv.org/abs/2510.06275</link>
<guid>https://arxiv.org/abs/2510.06275</guid>
<content:encoded><![CDATA[
<div> Keywords: XRec, Large Language Models, Explainable Recommendation, Collaborative Information, Mixture of Experts

Summary:
In this study, the authors replicated the work of Ma et al. (2024) on "XRec: Large Language Models for Explainable Recommendation" using Llama 3 as the LLM instead of GPT-3.5-turbo. By modifying the input and output embeddings of XRec's Mixture of Experts module, the authors extended the original paper's findings. The results showed that XRec effectively generates personalized explanations and benefits from collaborative information, although it did not consistently outperform baseline models in all metrics. The analysis highlighted the crucial role of Mixture of Experts embeddings in shaping explanation structures and demonstrated how collaborative signals interact with language modeling. The authors also provided an open-source evaluation implementation to enhance accessibility for researchers and practitioners. The complete code repository can be accessed at https://github.com/julianbibo/xrec-reproducibility.<br /><br />Summary: <div>
arXiv:2510.06275v1 Announce Type: new 
Abstract: In this study, we reproduced the work done in the paper "XRec: Large Language Models for Explainable Recommendation" by Ma et al. (2024). The original authors introduced XRec, a model-agnostic collaborative instruction-tuning framework that enables large language models (LLMs) to provide users with comprehensive explanations of generated recommendations. Our objective was to replicate the results of the original paper, albeit using Llama 3 as the LLM for evaluation instead of GPT-3.5-turbo. We built on the source code provided by Ma et al. (2024) to achieve our goal. Our work extends the original paper by modifying the input embeddings or deleting the output embeddings of XRec's Mixture of Experts module. Based on our results, XRec effectively generates personalized explanations and its stability is improved by incorporating collaborative information. However, XRec did not consistently outperform all baseline models in every metric. Our extended analysis further highlights the importance of the Mixture of Experts embeddings in shaping the explanation structures, showcasing how collaborative signals interact with language modeling. Through our work, we provide an open-source evaluation implementation that enhances accessibility for researchers and practitioners alike. Our complete code repository can be found at https://github.com/julianbibo/xrec-reproducibility.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Type and Complexity Signals in Multilingual Question Representations</title>
<link>https://arxiv.org/abs/2510.06304</link>
<guid>https://arxiv.org/abs/2510.06304</guid>
<content:encoded><![CDATA[
<div> Transformer model, multilingual, morphosyntactic properties, question type, complexity<br />
<br />
Summary: 
This study examines how a multilingual transformer model represents morphosyntactic properties of questions using the Question Type and Complexity (QTC) dataset across seven languages. The dataset includes sentences annotated with type information and complexity metrics. Evaluation involves probing methods with regression labels and selectivity controls to measure generalizability gains. Layer-wise probes on frozen Glot500-m representations outperform subword TF-IDF baselines, with fine-tuned models showing effectiveness in capturing structural complexity patterns. Statistical features effectively classify questions in languages with explicit marking, while neural probes excel in capturing fine-grained patterns. Contextual representations demonstrate superiority over statistical baselines, highlighting the importance of pre-trained linguistic information availability even after parameter updates. <div>
arXiv:2510.06304v1 Announce Type: new 
Abstract: This work investigates how a multilingual transformer model represents morphosyntactic properties of questions. We introduce the Question Type and Complexity (QTC) dataset with sentences across seven languages, annotated with type information and complexity metrics including dependency length, tree depth, and lexical density. Our evaluation extends probing methods to regression labels with selectivity controls to quantify gains in generalizability. We compare layer-wise probes on frozen Glot500-m (Imani et al., 2023) representations against subword TF-IDF baselines, and a fine-tuned model. Results show that statistical features classify questions effectively in languages with explicit marking, while neural probes capture fine-grained structural complexity patterns better. We use these results to evaluate when contextual representations outperform statistical baselines and whether parameter updates reduce the availability of pre-trained linguistic information.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Bias Detection and Mitigation through the Lens of Desired Distributions</title>
<link>https://arxiv.org/abs/2510.06354</link>
<guid>https://arxiv.org/abs/2510.06354</guid>
<content:encoded><![CDATA[
<div> gender-profession distribution, bias mitigation, language modeling, adaptive loss, real-world settings  
Summary:  
- The study focuses on bias mitigation in language models by aligning their outputs with desired distributions.  
- Bias is defined as deviation from a desired distribution, which can be based on social equality or real-world data.  
- A weighted adaptive loss fine-tuning method is proposed to align language models' gender-profession output distribution with the desired distribution.  
- Using three sets of professions, bias is observed under both equal and real-world distributions.  
- Near-complete mitigation is achieved under equality, with a 30-75% reduction in bias under real-world settings.  
- Autoregressive language models show no bias under equality but notable bias under real-world settings, with the Llama Instruct models achieving a 50-62% reduction.  
<br /><br />Summary: <div>
arXiv:2510.06354v1 Announce Type: new 
Abstract: Although prior work on bias mitigation has focused on promoting social equality and demographic parity, less attention has been given to aligning LLM's outputs to desired distributions. For example, we might want to align a model with real-world distributions to support factual grounding. Thus, we define bias as deviation from a desired distribution, which may be an equal or real-world distribution, depending on application goals. We propose a weighted adaptive loss based fine-tuning method that aligns LLM's gender-profession output distribution with the desired distribution, while preserving language modeling capability. Using 3 profession sets -- male-dominated, female-dominated, and gender-balanced -- derived from U.S. labor statistics (2024), we assess both our adaptive method for reflecting reality and a non-adaptive variant for equality. Across three masked language models, bias is observed under both distributions. We achieve near-complete mitigation under equality and 30-75% reduction under real-world settings. Autoregressive LLMs show no bias under equality but notable bias under real-world settings, with the Llama Instruct models (3.2-3B, 3.1-8B) achieving a 50-62% reduction.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EVALUESTEER: Measuring Reward Model Steerability Towards Values and Preference</title>
<link>https://arxiv.org/abs/2510.06370</link>
<guid>https://arxiv.org/abs/2510.06370</guid>
<content:encoded><![CDATA[
<div> benchmark, large language models, reward models, steerability, user preferences

Summary:
The article introduces a benchmark called EVALUESTEER designed to measure the ability of large language models (LLMs) and reward models (RMs) to steer towards users' value and stylistic preferences. The benchmark includes a dataset with 165,888 synthetic preference pairs across four value dimensions and four style dimensions. The evaluation using EVALUESTEER revealed that LLMs and RMs struggle to accurately select the response aligning with user preferences when provided with the full user profile. The best models achieved less than 75% accuracy in this scenario, compared to over 99% accuracy when only relevant preferences were provided. This highlights the limitations of current RMs in adapting to diverse user values and preferences. EVALUESTEER offers a challenging testbed for the development of RMs that can better align with user profiles. 

<br /><br />Summary: <div>
arXiv:2510.06370v1 Announce Type: new 
Abstract: As large language models (LLMs) are deployed globally, creating pluralistic systems that can accommodate the diverse preferences and values of users worldwide becomes essential. We introduce EVALUESTEER, a benchmark to measure LLMs' and reward models' (RMs) steerability towards users' value and stylistic preference profiles grounded in psychology and human-LLM interaction literature. To address the gap in existing datasets that do not support controlled evaluations of RM steering, we synthetically generated 165,888 preference pairs -- systematically varying pairs along 4 value dimensions (traditional, secular-rational, survival, and self-expression) and 4 style dimensions (verbosity, readability, confidence, and warmth). We use EVALUESTEER to evaluate whether, given a user profile and a pair of candidate value-laden and style-laden responses, LLMs and RMs are able to select the output that aligns with the user's preferences. We evaluate six open-source and proprietary LLMs and RMs under sixteen systematic prompting conditions and six preference comparison scenarios. Notably, our results show that, when given the user's full profile of values and stylistic preferences, the best models achieve <75% accuracy at choosing the correct response, in contrast to >99% accuracy when only relevant style and value preferences are provided. EVALUESTEER thus highlights the limitations of current RMs at identifying and adapting to relevant user profile information, and provides a challenging testbed for developing RMs that can be steered towards diverse human values and preferences.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EverydayMMQA: A Multilingual and Multimodal Framework for Culturally Grounded Spoken Visual QA</title>
<link>https://arxiv.org/abs/2510.06371</link>
<guid>https://arxiv.org/abs/2510.06371</guid>
<content:encoded><![CDATA[
<div> framework, dataset, multimodal models, cultural knowledge, everyday tasks 
Summary:<br /><br />Large-scale multimodal models are powerful but struggle with culturally grounded queries, especially in underrepresented languages. To address this, the Everyday Multimodal and Multilingual QA (EverydayMMQA) framework has been introduced to create culturally-grounded datasets. The OASIS dataset, part of this framework, consists of images, text, and spoken questions in English and Arabic varieties, incorporating diverse real-world scenarios. With a focus on pragmatic, commonsense, and culturally aware reasoning, the dataset challenges models beyond object recognition. Benchmarking various models on OASIS showcases its applicability across multiple tasks. EverydayMMQA and OASIS provide a resource for building multimodal LLMs for everyday tasks within cultural contexts, offering a benchmark and training dataset for the community.  <div>
arXiv:2510.06371v1 Announce Type: new 
Abstract: Large-scale multimodal models achieve strong results on tasks like Visual Question Answering (VQA), but they often fail when queries require culturally grounded, everyday knowledge, particularly in low-resource and underrepresented languages. To bridge this gap, we introduce Everyday Multimodal and Multilingual QA (EverydayMMQA), a framework for creating large-scale, culturally-grounded datasets for spoken and visual question answering (SVQA). Using this framework, we developed OASIS, a multimodal dataset integrating speech, images, and text. With over ~0.92M images and 14.8M QA pairs, OASIS contains 3.7M spoken questions, enabling four unique input combinations: speech-only, text-only, speech+image, and text+image. Focused on English and Arabic varieties, 18 countries, the dataset content is curated to reflect diverse, real-world situations. OASIS tests models on tasks beyond object recognition that involve pragmatic, commonsense, and culturally aware reasoning. We benchmarked four closed-source models, three open-source models, and one fine-tuned model. EverydayMMQA and OASIS together provide a benchmark and training dataset for building multimodal LLMs for a comprehensive set of everyday tasks within cultural contexts. The framework and dataset will be made publicly available to the community.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Regexes: Auto-Interpreting LLM Features with a Structured Language</title>
<link>https://arxiv.org/abs/2510.06378</link>
<guid>https://arxiv.org/abs/2510.06378</guid>
<content:encoded><![CDATA[
<div> Keywords: automated interpretability, large language model, semantic regexes, feature descriptions, user studies

Summary:
Semantic regexes are introduced as a structured language description of large language model (LLM) features to enhance automated interpretability. They offer precise and expressive feature descriptions using linguistic and semantic patterns with contextualization and quantification modifiers. Through quantitative benchmarks and qualitative analyses, semantic regexes are shown to provide accurate, concise, and consistent descriptions compared to natural language. These descriptions enable new types of analyses, such as quantifying feature complexity across model layers and identifying model-wide patterns. User studies reveal that semantic regex descriptions assist individuals in constructing accurate mental models of LLM feature activations. Overall, semantic regexes prove to be a valuable tool for improving interpretability and understanding of LLM features. 

<br /><br />Summary: <div>
arXiv:2510.06378v1 Announce Type: new 
Abstract: Automated interpretability aims to translate large language model (LLM) features into human understandable descriptions. However, these natural language feature descriptions are often vague, inconsistent, and require manual relabeling. In response, we introduce semantic regexes, structured language descriptions of LLM features. By combining primitives that capture linguistic and semantic feature patterns with modifiers for contextualization, composition, and quantification, semantic regexes produce precise and expressive feature descriptions. Across quantitative benchmarks and qualitative analyses, we find that semantic regexes match the accuracy of natural language while yielding more concise and consistent feature descriptions. Moreover, their inherent structure affords new types of analyses, including quantifying feature complexity across layers, scaling automated interpretability from insights into individual features to model-wide patterns. Finally, in user studies, we find that semantic regex descriptions help people build accurate mental models of LLM feature activations.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Protecting De-identified Documents from Search-based Linkage Attacks</title>
<link>https://arxiv.org/abs/2510.06383</link>
<guid>https://arxiv.org/abs/2510.06383</guid>
<content:encoded><![CDATA[
<div> de-identification, linkage risks, search-based attacks, inverted index, LLM-based rewriter <br />
<br />
Summary:
This paper introduces a method to prevent search-based linkage attacks on de-identified documents while maintaining semantic integrity. The approach involves constructing an inverted index of N-grams in the document collection to efficiently identify less common N-grams. A Language Model (LLM)-based rewriter then iteratively modifies these N-grams to disrupt linkages. Experimental results on court cases demonstrate the effectiveness of the method in thwarting search-based attacks without altering the original content. This approach addresses the limitation of de-identification models in safeguarding against potential mapping of de-identified texts back to their sources. By proactively disrupting linkages through targeted N-gram modifications, the method enhances privacy protection while preserving the underlying meaning of the text. <div>
arXiv:2510.06383v1 Announce Type: new 
Abstract: While de-identification models can help conceal the identity of the individual(s) mentioned in a document, they fail to address linkage risks, defined as the potential to map the de-identified text back to its source. One straightforward way to perform such linkages is to extract phrases from the de-identified document and then check their presence in the original dataset. This paper presents a method to counter search-based linkage attacks while preserving the semantic integrity of the text. The method proceeds in two steps. We first construct an inverted index of the N-grams occurring in the document collection, making it possible to efficiently determine which N-grams appear in less than $k$ documents (either alone or in combination with other N-grams). An LLM-based rewriter is then iteratively queried to reformulate those spans until linkage is no longer possible. Experimental results on a collection of court cases show that the method is able to effectively prevent search-based linkages while remaining faithful to the original content.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controllable Stylistic Text Generation with Train-Time Attribute-Regularized Diffusion</title>
<link>https://arxiv.org/abs/2510.06386</link>
<guid>https://arxiv.org/abs/2510.06386</guid>
<content:encoded><![CDATA[
<div> Keywords: stylistic text generation, diffusion models, controllable generation, attribute supervision, VAE-based encoder-decoder

Summary:
RegDiff is a new framework for controllable text generation that combines the strengths of classifier-free guidance (CFG) and classifier guidance (CG) methods. It uses a VAE-based encoder-decoder architecture to ensure accuracy and a latent diffusion model trained with attribute supervision for controllable text generation. By injecting attribute information only during training, RegDiff achieves efficient and effective attribute alignment without the need for a pretrained classifier during sampling. Experimental results on multiple datasets show that RegDiff outperforms existing baselines in generating stylistic texts. This approach offers a more computationally efficient solution for attribute-controllable text diffusion, making it a promising advancement in the field of text generation.<br /><br />Summary: RegDiff is a regularized diffusion framework for controllable text generation that combines CFG and CG methods. It uses VAE-based encoder-decoder and attribute supervision for efficient and effective stylistic text generation. <div>
arXiv:2510.06386v1 Announce Type: new 
Abstract: Generating stylistic text with specific attributes is a key problem in controllable text generation. Recently, diffusion models have emerged as a powerful paradigm for both visual and textual generation. Existing approaches can be broadly categorized into classifier-free guidance (CFG) and classifier guidance (CG) methods. While CFG effectively preserves semantic content, it often fails to provide effective attribute control. In contrast, CG modifies the denoising trajectory using classifier gradients, enabling better attribute alignment but incurring high computational costs during sampling and suffering from classifier generalization issues. In this work, we propose RegDiff, a regularized diffusion framework that leverages attribute features without requiring a pretrained classifier during sampling, thereby achieving controllable generation with reduced computational costs. Specifically, RegDiff employs a VAE-based encoder--decoder architecture to ensure reconstruction fidelity and a latent diffusion model trained with attribute supervision to enable controllable text generation. Attribute information is injected only during training. Experiments on five datasets spanning multiple stylistic attributes demonstrate that RegDiff outperforms strong baselines in generating stylistic texts. These results validate the effectiveness of RegDiff as an efficient solution for attribute-controllable text diffusion. Our code, datasets, and resources will be released upon publication at https://github.com/xxxx.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward Model Perspectives: Whose Opinions Do Reward Models Reward?</title>
<link>https://arxiv.org/abs/2510.06391</link>
<guid>https://arxiv.org/abs/2510.06391</guid>
<content:encoded><![CDATA[
<div> Keywords: Reward models, alignment, sociodemographic biases, preferences, language models 

Summary: 
- The study focuses on reward models (RMs) used to align language models (LMs) with human preferences.
- A framework for measuring alignment of opinions captured by RMs is formalized.
- Investigations reveal that RMs exhibit sociodemographic biases and may reward harmful stereotypes.
- Prompting to steer rewards towards a target group's preferences may not fully overcome these biases.
- The findings highlight the importance of understanding and mitigating RM behavior in preference learning to avoid perpetuating social biases in language technologies. 

<br /><br />Summary: <div>
arXiv:2510.06391v1 Announce Type: new 
Abstract: Reward models (RMs) are central to the alignment of language models (LMs). An RM often serves as a proxy for human preferences to guide downstream LM behavior. However, our understanding of RM behavior is limited. Our work (i) formalizes a framework for measuring the alignment of opinions captured by RMs, (ii) investigates the extent to which RMs demonstrate sociodemographic biases, and (iii) explores the effects of prompting to steer rewards towards the preferences of a target group. We study the subjective and diverse perspectives on controversial topics, which allows us to quantify RM perspectives in terms of their opinions, attitudes, and values. We show that RMs are poorly aligned with several demographic groups and can systematically reward harmful stereotypes, and steering alone is not enough to overcome these limitations. Our findings underscore the need for more careful consideration of RM behavior in model alignment during preference learning to prevent the propagation of unwanted social biases in the language technologies that we use.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instructional Goal-Aligned Question Generation for Student Evaluation in Virtual Lab Settings: How Closely Do LLMs Actually Align?</title>
<link>https://arxiv.org/abs/2510.06411</link>
<guid>https://arxiv.org/abs/2510.06411</guid>
<content:encoded><![CDATA[
<div> Virtual Labs, instructional goal-aligned question generation, Large Language Models, natural language interaction, cognitive demand<br />
<br />Summary:
This paper introduces a framework for instructional goal-aligned question generation using Large Language Models (LLMs) in Virtual Labs. The framework includes components such as instructional goal understanding, lab understanding, a question taxonomy, and the TELeR taxonomy for controlling prompt detail. Through dialogue with teachers, the framework ensures questions align with teacher intent and simulation context. The question taxonomy enhances cognitive demand and quality, while TELeR prompts improve format adherence. Evaluation of over 1,100 questions from 19 LLMs shows that larger models yield significant gains in parsability, adherence, and question quality. This framework offers a promising approach to leveraging LLMs for personalized and effective science learning in Virtual Labs. <br /><br />Summary: <div>
arXiv:2510.06411v1 Announce Type: new 
Abstract: Virtual Labs offer valuable opportunities for hands-on, inquiry-based science learning, yet teachers often struggle to adapt them to fit their instructional goals. Third-party materials may not align with classroom needs, and developing custom resources can be time-consuming and difficult to scale. Recent advances in Large Language Models (LLMs) offer a promising avenue for addressing these limitations. In this paper, we introduce a novel alignment framework for instructional goal-aligned question generation, enabling teachers to leverage LLMs to produce simulation-aligned, pedagogically meaningful questions through natural language interaction. The framework integrates four components: instructional goal understanding via teacher-LLM dialogue, lab understanding via knowledge unit and relationship analysis, a question taxonomy for structuring cognitive and pedagogical intent, and the TELeR taxonomy for controlling prompt detail. Early design choices were informed by a small teacher-assisted case study, while our final evaluation analyzed over 1,100 questions from 19 open-source LLMs. With goal and lab understanding grounding questions in teacher intent and simulation context, the question taxonomy elevates cognitive demand (open-ended formats and relational types raise quality by 0.29-0.39 points), and optimized TELeR prompts enhance format adherence (80% parsability, >90% adherence). Larger models yield the strongest gains: parsability +37.1%, adherence +25.7%, and average quality +0.8 Likert points.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinLFQA: Evaluating Attributed Text Generation of LLMs in Financial Long-Form Question Answering</title>
<link>https://arxiv.org/abs/2510.06426</link>
<guid>https://arxiv.org/abs/2510.06426</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, FinLFQA, financial applications, attribution, automatic evaluation<br />
Summary:<br />
Large Language Models (LLMs) often provide inaccurate answers to long-form financial questions, known as hallucinations. To address this issue, the researchers introduce FinLFQA, a benchmark that evaluates LLMs' ability to generate accurate answers with detailed attributions in financial scenarios. FinLFQA assesses three key aspects of attribution: supporting evidence from financial reports, numerical reasoning steps, and domain-specific financial knowledge. The study includes an automatic evaluation framework covering answer and attribution quality. Experiment results indicate the importance of fine-grained metrics in distinguishing model performance, the comparable performance of end-to-end and post-hoc approaches, and the effectiveness of iterative refinement with external feedback. The findings highlight the significance of nuanced attribution generation in enhancing LLM capabilities for complex financial questions.<br />  <div>
arXiv:2510.06426v1 Announce Type: new 
Abstract: Large Language Models (LLMs) frequently hallucinate to long-form questions, producing plausible yet factually incorrect answers. A common mitigation strategy is to provide attribution to LLM outputs. However, existing benchmarks primarily focus on simple attribution that retrieves supporting textual evidence as references. We argue that in real-world scenarios such as financial applications, attribution goes beyond reference retrieval. We introduce FinLFQA, a benchmark designed to evaluate the ability of LLMs to generate long-form answers to complex financial questions with reliable and nuanced attributions. FinLFQA evaluates three critical aspects of attribution through human annotations: (1) supporting evidence extracted from financial reports, (2) intermediate numerical reasoning steps, and (3) domain-specific financial knowledge that informs the reasoning process. We further provide an automatic evaluation framework covering both answer quality and attribution quality. Through extensive experiments on eight LLMs across multiple attribution-generation paradigms, we find that fine-grained metrics are important to distinguish model capabilities, that end-to-end generation achieves comparable performance to post-hoc approaches, and that iterative refinement only helps when guided by external feedback.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Discourse Treebanks with a Unified Rhetorical Structure Parser</title>
<link>https://arxiv.org/abs/2510.06427</link>
<guid>https://arxiv.org/abs/2510.06427</guid>
<content:encoded><![CDATA[
<div> Unified RST-style discourse parser, 18 treebanks, 11 languages, Multi-Head, Masked-Union, parameter efficient, low-resource settings, monotreebank parsing 

<br />
Summary:
The article introduces UniRST, the first unified RST-style discourse parser capable of handling 18 treebanks in 11 languages without modifying their relation inventories. Two training strategies, Multi-Head and Masked-Union, were proposed and evaluated to overcome inventory incompatibilities. The Masked-Union approach was found to be the strongest and most parameter efficient. Additionally, a simple yet effective augmentation technique was used for low-resource settings in monotreebank parsing. The unified model, UniRST, outperformed 16 out of 18 mono-treebank baselines, showcasing the benefits of a single-model, multilingual end-to-end discourse parsing across diverse resources. <div>
arXiv:2510.06427v1 Announce Type: new 
Abstract: We introduce UniRST, the first unified RST-style discourse parser capable of handling 18 treebanks in 11 languages without modifying their relation inventories. To overcome inventory incompatibilities, we propose and evaluate two training strategies: Multi-Head, which assigns separate relation classification layer per inventory, and Masked-Union, which enables shared parameter training through selective label masking. We first benchmark monotreebank parsing with a simple yet effective augmentation technique for low-resource settings. We then train a unified model and show that (1) the parameter efficient Masked-Union approach is also the strongest, and (2) UniRST outperforms 16 of 18 mono-treebank baselines, demonstrating the advantages of a single-model, multilingual end-to-end discourse parsing across diverse resources.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MathRobust-LV: Evaluation of Large Language Models' Robustness to Linguistic Variations in Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2510.06430</link>
<guid>https://arxiv.org/abs/2510.06430</guid>
<content:encoded><![CDATA[
<div> math reasoning, robustness, linguistic variation, high school-level, benchmarking 

Summary: 
The study evaluates the math reasoning robustness of large language models in handling linguistic variation in high school-level math problems. A new test set, MathRobust-LV, was introduced to assess how models perform when surface details in problems are altered while maintaining the same numerical structure and answers. Unlike previous studies focusing on high-difficulty competitions like the IMO, this research emphasizes the importance of linguistic robustness in educational settings where math problems are rephrased in varied ways by instructors. Results from testing 34 models indicate that accuracy decreases when models face variations in problem details. While smaller models show a significant decline in accuracy, even stronger models experience some degradation. Frontier models like GPT-5 and Gemini-2.5pro demonstrate better stability. These findings reveal the fundamental challenge of linguistic variation in math reasoning and highlight vulnerabilities in current models. 

<br /><br />Summary: <div>
arXiv:2510.06430v1 Announce Type: new 
Abstract: Large language models excel on math benchmarks, but their math reasoning robustness to linguistic variation is underexplored. While recent work increasingly treats high-difficulty competitions like the IMO as the gold standard for evaluating reasoning, we believe in comprehensive benchmarking of high school-level math problems in real educational settings. We introduce MathRobust-LV, a test set and evaluation methodology that mirrors how instructors rephrase problems across assessments while keeping difficulty constant: we change surface details (names, contexts, variables) while preserving numerical structure and answers. In contrast to prior efforts that alter problem content or emphasize IMO-level tasks, we focus on high-school-level dataset problems at the difficulty level where models are currently deployed in educational settings: tutoring and assessment systems. In these applications, instructors rephrase identical concepts in varied ways, making linguistic robustness essential for reliable deployment. Although MATH data benchmarking is often regarded as saturated, our experiment on 34 models reveals that accuracy declines when moving from the baseline to the variants. These drops are severe for smaller models (9-11%) while stronger models also show measurable degradation. Frontier models like GPT-5, Gemini-2.5pro remain comparatively stable. Our results highlight that robustness to linguistic variation is a fundamental challenge, exposing reasoning vulnerabilities in models.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Agentic Security: Applications, Threats and Defenses</title>
<link>https://arxiv.org/abs/2510.06445</link>
<guid>https://arxiv.org/abs/2510.06445</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous LLM-agents, cybersecurity, threats, defenses, agent architecture<br />
Summary:<br />
The article discusses the shift from passive LLMs to autonomous LLM-agents in cybersecurity, highlighting the risks and benefits of this new paradigm. It presents a comprehensive survey of the agentic security landscape, focusing on the three pillars of Applications, Threats, and Defenses. The taxonomy derived from over 150 papers categorizes how agents are utilized, the vulnerabilities they may possess, and the countermeasures implemented to safeguard them. The analysis identifies emerging trends in agent architecture and uncovers critical research gaps in model and modality coverage within the field. This holistic approach provides insights into the current state of agentic security, emphasizing the need for continued research and innovation to address the evolving cybersecurity threats posed by autonomous LLM-agents.<br /><br />Summary: <div>
arXiv:2510.06445v1 Announce Type: new 
Abstract: The rapid shift from passive LLMs to autonomous LLM-agents marks a new paradigm in cybersecurity. While these agents can act as powerful tools for both offensive and defensive operations, the very agentic context introduces a new class of inherent security risks. In this work we present the first holistic survey of the agentic security landscape, structuring the field around three interdependent pillars: Applications, Threats, and Defenses. We provide a comprehensive taxonomy of over 150 papers, explaining how agents are used, the vulnerabilities they possess, and the countermeasures designed to protect them. A detailed cross-cutting analysis shows emerging trends in agent architecture while revealing critical research gaps in model and modality coverage.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linguistically Informed Tokenization Improves ASR for Underresourced Languages</title>
<link>https://arxiv.org/abs/2510.06461</link>
<guid>https://arxiv.org/abs/2510.06461</guid>
<content:encoded><![CDATA[
<div> Keywords: Automatic speech recognition, language documentation, underresourced languages, phonemic tokenization, wav2vec2 model

Summary:
- The study explores the use of automatic speech recognition (ASR) for language documentation tasks, focusing on the underresourced Yan-nhangu language.
- Fine-tuning a wav2vec2 ASR model on Yan-nhangu and comparing phonemic versus orthographic tokenization strategies show that a linguistically informed phonemic system improves performance.
- ASR is evaluated as a tool in a language documentation pipeline and found to be a viable option for underresourced languages.
- Hand-correcting ASR output is shown to be faster than hand-transcribing audio from scratch, indicating the efficiency of using ASR in language documentation.
- The study highlights the potential of ASR for linguistic research and documentation, especially for languages with limited resources and documentation efforts. 

<br /><br />Summary: <div>
arXiv:2510.06461v1 Announce Type: new 
Abstract: Automatic speech recognition (ASR) is a crucial tool for linguists aiming to perform a variety of language documentation tasks. However, modern ASR systems use data-hungry transformer architectures, rendering them generally unusable for underresourced languages. We fine-tune a wav2vec2 ASR model on Yan-nhangu, a dormant Indigenous Australian language, comparing the effects of phonemic and orthographic tokenization strategies on performance. In parallel, we explore ASR's viability as a tool in a language documentation pipeline. We find that a linguistically informed phonemic tokenization system substantially improves WER and CER compared to a baseline orthographic tokenization scheme. Finally, we show that hand-correcting the output of an ASR model is much faster than hand-transcribing audio from scratch, demonstrating that ASR can work for underresourced languages.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test-Time Scaling of Reasoning Models for Machine Translation</title>
<link>https://arxiv.org/abs/2510.06471</link>
<guid>https://arxiv.org/abs/2510.06471</guid>
<content:encoded><![CDATA[
<div> scaling, test-time, machine translation, reasoning models, post-editing

Summary:<br /><br />
This paper explores the impact of test-time scaling (TTS) on machine translation (MT) using Reasoning Models (RMs). The study evaluates TTS on 12 RMs across various MT benchmarks in different domains, considering direct translation, forced-reasoning extrapolation, and post-editing scenarios. The results indicate that while TTS offers limited benefits for general-purpose RMs in direct translation, domain-specific fine-tuning can significantly improve translation quality by aligning reasoning processes with task requirements. It is observed that forcing a model to reason beyond its natural stopping point negatively affects translation quality. However, TTS is highly effective in post-editing, enhancing self-correction processes. Overall, the study shows that the value of inference-time computation in MT lies in specialized applications like multi-step workflows and with task-specific models rather than enhancing single-pass translation with general models. <div>
arXiv:2510.06471v1 Announce Type: new 
Abstract: Test-time scaling (TTS) has enhanced the performance of Reasoning Models (RMs) on various tasks such as math and coding, yet its efficacy in machine translation (MT) remains underexplored. This paper investigates whether increased inference-time computation improves translation quality. We evaluate 12 RMs across a diverse suite of MT benchmarks spanning multiple domains, examining three scenarios: direct translation, forced-reasoning extrapolation, and post-editing. Our findings show that for general-purpose RMs, TTS provides limited and inconsistent benefits for direct translation, with performance quickly plateauing. However, the effectiveness of TTS is unlocked by domain-specific fine-tuning, which aligns a model's reasoning process with task requirements, leading to consistent improvements up to an optimal, self-determined reasoning depth. We also find that forcing a model to reason beyond its natural stopping point consistently degrades translation quality. In contrast, TTS proves highly effective in a post-editing context, reliably turning self-correction into a beneficial process. These results indicate that the value of inference-time computation in MT lies not in enhancing single-pass translation with general models, but in targeted applications like multi-step, self-correction workflows and in conjunction with task-specialized models.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels</title>
<link>https://arxiv.org/abs/2510.06499</link>
<guid>https://arxiv.org/abs/2510.06499</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Reinforcement Learning, Data-efficient, Webscale-RL pipeline, Question-answer pairs

Summary: 
The paper introduces the Webscale-RL pipeline, a data engine that generates diverse question-answer pairs from large-scale pre-training documents for Reinforcement Learning (RL) training. The Webscale-RL dataset contains 1.2 million examples across 9 domains, showing significant improvements in model performance compared to continual pre-training and data refinement baselines. RL training with the Webscale-RL dataset proves to be more efficient, achieving comparable performance with up to 100 times fewer tokens. The research demonstrates a promising approach to scaling RL to the level of pre-training, enhancing the capabilities and efficiency of language models. 

<br /><br />Summary: <div>
arXiv:2510.06499v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have achieved remarkable success through imitation learning on vast text corpora, but this paradigm creates a training-generation gap and limits robust reasoning. Reinforcement learning (RL) offers a more data-efficient solution capable of bridging this gap, yet its application has been constrained by a critical data bottleneck: existing RL datasets are orders of magnitude smaller and less diverse than web-scale pre-training corpora. To address this, we introduce the Webscale-RL pipeline, a scalable data engine that systematically converts large-scale pre-training documents into millions of diverse, verifiable question-answer pairs for RL. Using this pipeline, we construct the Webscale-RL dataset, containing 1.2 million examples across more than 9 domains. Our experiments show that the model trained on this dataset significantly outperforms continual pretraining and strong data refinement baselines across a suite of benchmarks. Notably, RL training with our dataset proves substantially more efficient, achieving the performance of continual pre-training with up to 100$\times$ fewer tokens. Our work presents a viable path toward scaling RL to pre-training levels, enabling more capable and efficient language models.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Acceleration to Saturation: Scaling Behavior of Bootstrapped Language Model Pretraining</title>
<link>https://arxiv.org/abs/2510.06548</link>
<guid>https://arxiv.org/abs/2510.06548</guid>
<content:encoded><![CDATA[
<div> scaling efficiency, bootstrapped pretraining, overtrained models, language models, multi-stage pretraining
Summary:
Bootstrapped pretraining, the reuse of a pretrained base model for further pretraining, is a cost-effective approach for training language models. However, its effectiveness diminishes when applied to overtrained base models. The study on scaling behavior reveals a predictable decrease in scaling efficiency as the number of tokens used for pretraining the base model increases. The joint dependence on first- and second-stage tokens can be accurately modeled by a simple scaling law. This saturation effect highlights a trade-off in multi-stage pretraining strategies: extensive base model pretraining leads to diminishing additional benefits from bootstrapping. These findings offer practical insights for efficient language model training and raise important considerations for reusing overtrained models.<br /><br />Summary: <div>
arXiv:2510.06548v1 Announce Type: new 
Abstract: Bootstrapped pretraining, i.e., the reuse of a pretrained base model for further pretraining, such as continual pretraining or model growth, is promising at reducing the cost of training language models from scratch. However, its effectiveness remains unclear, especially when applied to overtrained base models. In this work, we empirically study the scaling behavior of bootstrapped pretraining and find that its scaling efficiency diminishes in a predictable manner: The scaling exponent with respect to second-stage pretraining tokens decreases logarithmically with the number of tokens used to pretrain the base model. The joint dependence on first- and second-stage tokens is accurately modeled by a simple scaling law. Such saturation effect reveals a fundamental trade-off in multi-stage pretraining strategies: the more extensively a model is pretrained, the less additional benefit bootstrapping provides. Our findings provide practical insights for efficient language model training and raise important considerations for the reuse of overtrained models.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flipping the Dialogue: Training and Evaluating User Language Models</title>
<link>https://arxiv.org/abs/2510.06552</link>
<guid>https://arxiv.org/abs/2510.06552</guid>
<content:encoded><![CDATA[
<div> Keyword: LMs, User Language Models, multi-turn conversations, simulation methods, assistant struggles
Summary:
In the study, the focus is on evaluating the performance of Language Models (LMs) in realistic settings where human users interact with LM assistants in multi-turn conversations. It is found that LLMs trained to be helpful assistants do not serve as effective user simulators. Instead, User Language Models (User LMs) are introduced to better simulate human user behavior in conversations. These User LMs demonstrate improved alignment with human behavior and simulation robustness compared to existing methods. When used to simulate coding and math conversations, the performance of a strong assistant like GPT-4o drops significantly, indicating the challenges assistants face in handling the nuances of user interactions in multi-turn setups.<br /><br />Summary: <div>
arXiv:2510.06552v1 Announce Type: new 
Abstract: Conversations with LMs involve two participants: a human user leading the conversation, and an LM assistant responding to the user's request. To satisfy this specific role, LMs are post-trained to be helpful assistants -- optimized to produce exhaustive and well-structured responses, free of ambiguity and grammar errors. User utterances, on the other hand, are rarely perfected, with each user phrasing requests in unique ways, sometimes putting in partial effort at each turn and refining on the fly. To evaluate LM performance in realistic settings, prior work simulated users in multi-turn conversations, often prompting an LLM originally trained to be a helpful assistant to act as a user. However, we show that assistant LMs make for poor user simulators, with the surprising finding that better assistants yield worse simulators. Instead, we introduce purpose-built User Language Models (User LMs) - models post-trained to simulate human users in multi-turn conversations. Through various evaluations, we show how User LMs align better with human behavior and achieve better simulation robustness than existing simulation methods. When leveraging User LMs to simulate coding and math conversations, the performance of a strong assistant (GPT-4o) drops from 74.6% to 57.4%, confirming that more realistic simulation environments lead to assistant struggles as they fail to cope with the nuances of users in multi-turn setups.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Algebra of Meaning: Why Machines Need Montague More Than Moore's Law</title>
<link>https://arxiv.org/abs/2510.06559</link>
<guid>https://arxiv.org/abs/2510.06559</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, type-theoretic semantics, neuro-symbolic architecture, legal reasoning, multi-jurisdiction 

Summary:
Contemporary language models often struggle with handling the various types of meaning in their outputs, leading to issues such as hallucination and opaque compliance. The authors propose that these issues stem from a lack of type-theoretic semantics rather than data or scale limitations. They introduce Savassan, a neuro-symbolic architecture that compiles natural-language inputs into Montague-style logical forms with added deontic operators and jurisdictional contexts. By aligning language parsing with legal reasoning, the system can provide compliance-aware guidance rather than simply censoring content. In cross-border scenarios, Savassan can parse inputs once and project outcomes across multiple legal ontologies, producing explainable decisions. The paper offers a diagnosis of hallucination as a type error, a formal Montague-ontology bridge for business/legal reasoning, and a design that embeds typed interfaces throughout the pipeline. The authors propose an evaluation plan using legal reasoning benchmarks and synthetic multi-jurisdiction suites, emphasizing the importance of compositional typing for trustworthy autonomous systems. 

<br /><br />Summary: <div>
arXiv:2510.06559v1 Announce Type: new 
Abstract: Contemporary language models are fluent yet routinely mis-handle the types of meaning their outputs entail. We argue that hallucination, brittle moderation, and opaque compliance outcomes are symptoms of missing type-theoretic semantics rather than data or scale limitations. Building on Montague's view of language as typed, compositional algebra, we recast alignment as a parsing problem: natural-language inputs must be compiled into structures that make explicit their descriptive, normative, and legal dimensions under context.
  We present Savassan, a neuro-symbolic architecture that compiles utterances into Montague-style logical forms and maps them to typed ontologies extended with deontic operators and jurisdictional contexts. Neural components extract candidate structures from unstructured inputs; symbolic components perform type checking, constraint reasoning, and cross-jurisdiction mapping to produce compliance-aware guidance rather than binary censorship. In cross-border scenarios, the system "parses once" (e.g., defect claim(product x, company y)) and projects the result into multiple legal ontologies (e.g., defamation risk in KR/JP, protected opinion in US, GDPR checks in EU), composing outcomes into a single, explainable decision.
  This paper contributes: (i) a diagnosis of hallucination as a type error; (ii) a formal Montague-ontology bridge for business/legal reasoning; and (iii) a production-oriented design that embeds typed interfaces across the pipeline. We outline an evaluation plan using legal reasoning benchmarks and synthetic multi-jurisdiction suites. Our position is that trustworthy autonomy requires compositional typing of meaning, enabling systems to reason about what is described, what is prescribed, and what incurs liability within a unified algebra of meaning.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TinyScientist: An Interactive, Extensible, and Controllable Framework for Building Research Agents</title>
<link>https://arxiv.org/abs/2510.06579</link>
<guid>https://arxiv.org/abs/2510.06579</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, automatic research, multi-agent systems, extensible framework, open-source codebase

Summary:
TinyScientist introduces an extensible framework for automatic research workflows utilizing Large Language Models (LLMs) and multi-agent systems. As the demand for automated research accelerates, the complexity of creating and maintaining agentic workflows has become a challenge. The framework aims to simplify this process by identifying essential components and providing interactive, adaptable tools that support iterative growth. By offering an open-source codebase, an interactive web demonstration, and a PyPI Python package, TinyScientist makes cutting-edge auto-research pipelines accessible to researchers and developers. This approach enables easy integration of new tools while ensuring controllability and scalability in research processes. <div>
arXiv:2510.06579v1 Announce Type: new 
Abstract: Automatic research with Large Language Models (LLMs) is rapidly gaining importance, driving the development of increasingly complex workflows involving multi-agent systems, planning, tool usage, code execution, and human-agent interaction to accelerate research processes. However, as more researchers and developers begin to use and build upon these tools and platforms, the complexity and difficulty of extending and maintaining such agentic workflows have become a significant challenge, particularly as algorithms and architectures continue to advance. To address this growing complexity, TinyScientist identifies the essential components of the automatic research workflow and proposes an interactive, extensible, and controllable framework that easily adapts to new tools and supports iterative growth. We provide an open-source codebase, an interactive web demonstration, and a PyPI Python package to make state-of-the-art auto-research pipelines broadly accessible to every researcher and developer.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Internal Layers of LLMs Reveal Patterns for Jailbreak Detection?</title>
<link>https://arxiv.org/abs/2510.06594</link>
<guid>https://arxiv.org/abs/2510.06594</guid>
<content:encoded><![CDATA[
<div> Jailbreaking, large language models, LLMs, adversarial users, defense mechanisms
Summary: 
The study examines the jailbreaking of large language models (LLMs) by analyzing how hidden layers respond to jailbreak prompts compared to benign prompts. Specifically focusing on the GPT-J and Mamba2 models, the research identifies distinct layer-wise behaviors that can be utilized for robust jailbreak detection and defense. With the increasing prevalence of conversational LLMs, the exploitation of these models through engineered prompts is a pressing concern. Despite existing defense mechanisms, attackers continue to find ways to jailbreak models. By investigating internal model representations, the study provides insights into potential strategies for enhancing model security against jailbreaking. Further research in leveraging internal model dynamics could lead to more effective defenses against adversarial users. 

<br /><br />Summary: <div>
arXiv:2510.06594v1 Announce Type: new 
Abstract: Jailbreaking large language models (LLMs) has emerged as a pressing concern with the increasing prevalence and accessibility of conversational LLMs. Adversarial users often exploit these models through carefully engineered prompts to elicit restricted or sensitive outputs, a strategy widely referred to as jailbreaking. While numerous defense mechanisms have been proposed, attackers continuously develop novel prompting techniques, and no existing model can be considered fully resistant. In this study, we investigate the jailbreak phenomenon by examining the internal representations of LLMs, with a focus on how hidden layers respond to jailbreak versus benign prompts. Specifically, we analyze the open-source LLM GPT-J and the state-space model Mamba2, presenting preliminary findings that highlight distinct layer-wise behaviors. Our results suggest promising directions for further research on leveraging internal model dynamics for robust jailbreak detection and defense.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparative Analysis of Contextual Representation Flow in State-Space and Transformer Architectures</title>
<link>https://arxiv.org/abs/2510.06640</link>
<guid>https://arxiv.org/abs/2510.06640</guid>
<content:encoded><![CDATA[
<div> SSMs, TBMs, representation propagation, centered kernel alignment, oversmoothing <br />
Summary: <br />
State Space Models (SSMs) and Transformer-Based Models (TBMs) are compared for long-sequence processing. The study analyzes how contextual information flows through layers and tokens in both architectures. TBMs tend to homogenize token representations quickly, with diversity emerging in later layers. In contrast, SSMs maintain token uniqueness initially but converge to homogenization in deeper layers. The oversmoothing in TBMs is attributed to architectural design, while in SSMs, it mainly arises from training dynamics. These findings provide insights into the inductive biases of each architecture and can guide future model and training designs for long-context reasoning. <div>
arXiv:2510.06640v1 Announce Type: new 
Abstract: State Space Models (SSMs) have recently emerged as efficient alternatives to Transformer-Based Models (TBMs) for long-sequence processing, offering linear scaling and lower memory use. Yet, how contextual information flows across layers and tokens in these architectures remains understudied. We present the first unified, token- and layer-level analysis of representation propagation in SSMs and TBMs. Using centered kernel alignment, stability metrics, and probing, we characterize how representations evolve within and across layers. We find a key divergence: TBMs rapidly homogenize token representations, with diversity reemerging only in later layers, while SSMs preserve token uniqueness early but converge to homogenization deeper. Theoretical analysis and parameter randomization further reveal that oversmoothing in TBMs stems from architectural design, whereas in SSMs it arises mainly from training dynamics. These insights clarify the inductive biases of both architectures and inform future model and training designs for long-context reasoning.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Large Language Models via Fully Self-Synthetic Data</title>
<link>https://arxiv.org/abs/2510.06652</link>
<guid>https://arxiv.org/abs/2510.06652</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, human feedback, language models, self-alignment optimization, synthetic framework

Summary:
Self-Alignment Optimization (SAO) is introduced as a framework for large language model (LLM) alignment. SAO is a fully self-synthetic approach where the LLM generates its own training data, including prompts, responses, and preferences. The model engages in persona role-play to generate diverse prompts and responses, which are then self-evaluated for preference optimization. SAO enhances the model's chat capabilities on benchmarks like AlpacaEval~2.0 while maintaining strong performance on objective tasks like question-answering and math reasoning. This self-improvement method for aligning LLMs provides a practical solution without the need for expensive human-annotated datasets or external reward models. The code for reproducing the results is available on GitHub at https://github.com/SJY8460/SAO. 

<br /><br />Summary: 
Self-Alignment Optimization (SAO) is a self-synthetic framework for large language model alignment, where the model generates its own training data, including prompts, responses, and preferences. By engaging in persona role-play and self-evaluating prompts and responses for preference optimization, SAO enhances the model's chat capabilities on benchmarks like AlpacaEval~2.0 while maintaining strong performance on tasks like question-answering and math reasoning. This approach provides a practical solution for aligning LLMs without the need for expensive human-annotated datasets or external reward models. The code for reproducing the results is available on GitHub. <div>
arXiv:2510.06652v1 Announce Type: new 
Abstract: Traditional reinforcement learning from human feedback (RLHF) for large language models (LLMs) relies on expensive human-annotated datasets, while Reinforcement Learning from AI Feedback (RLAIF) also incurs significant costs, requiring the collection of diverse prompts and corresponding responses, often necessitating external reward models or proprietary models like GPT-4 to annotate preference pairs. In this work, we introduce Self-Alignment Optimization (SAO), a fully self-synthetic framework for LLM alignment, where all training data, including prompts (i.e., user queries), responses, and preferences, are generated by the model itself. Specifically, SAO first instructs the LLM to engage in persona role-play and generate diverse prompts and responses, which are then self-evaluated for preference optimization. Extensive experiments demonstrate that SAO effectively enhances the model's chat capabilities on standard benchmarks like AlpacaEval~2.0, while maintaining strong performance on downstream objective tasks (e.g., question-answering, math reasoning). Our work provides a practical solution for self-improvement in aligning LLMs, and the code for reproducing our results is available at: https://github.com/SJY8460/SAO.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToolMem: Enhancing Multimodal Agents with Learnable Tool Capability Memory</title>
<link>https://arxiv.org/abs/2510.06664</link>
<guid>https://arxiv.org/abs/2510.06664</guid>
<content:encoded><![CDATA[
<div> capabilities, neural tools, ToolMem, tool selection, memory

Summary:
Agents powered by large language models and vision-language models have shown great progress in various tasks. However, traditional tools provide deterministic outputs, while neural tools operate with uncertainty. To address this, ToolMem is proposed to help agents develop memories of tool capabilities by summarizing strengths and weaknesses and storing them for future use. By utilizing ToolMem, agents can accurately predict tool performance and select the best tool for individual tasks more effectively. In evaluations with text generation and text-to-image generation tools, ToolMem-augmented agents outperformed generic agents, showing significant improvements in predicting tool performance and facilitating optimal tool selection among multiple choices. Overall, ToolMem enhances agent flexibility in selecting the most suitable tool by leveraging past interactions and knowledge of tool capabilities. 

<br /><br />Summary: <div>
arXiv:2510.06664v1 Announce Type: new 
Abstract: Agents utilizing tools powered by large language models (LLMs) or vision-language models (VLMs) have demonstrated remarkable progress in diverse tasks across text and visual modalities. Unlike traditional tools such as calculators, which give deterministic outputs, neural tools perform uncertainly across task scenarios. While different tools for a task may excel in varied scenarios, existing agents typically rely on fixed tools, thus limiting the flexibility in selecting the most suitable tool for specific tasks. In contrast, humans snowball their understanding of the capabilities of different tools by interacting with them, and apply this knowledge to select the optimal tool when solving a future task. To build agents that similarly benefit from this process, we propose ToolMem that enables agents to develop memories of tool capabilities from previous interactions, by summarizing their strengths and weaknesses and storing them in memory; at inference, the agent can retrieve relevant entries from ToolMem, and select the best tool to solve individual tasks more accurately. We evaluate ToolMem on learning varied text generation and text-to-image generation neural tools. Compared to no-memory, generic agents, we find ToolMem-augmented agents predict tool performance 14.8% and 28.7% more accurately across text and multimodal generation scenarios. Moreover, ToolMem facilitates optimal tool selection among multiple choices by 21% and 24% absolute increases in respective scenarios.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PIKA: Expert-Level Synthetic Datasets for Post-Training Alignment from Scratch</title>
<link>https://arxiv.org/abs/2510.06670</link>
<guid>https://arxiv.org/abs/2510.06670</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Human Feedback, Alignment Datasets, Data Efficiency, Open-Source LLM Alignment  
Summary:  
PiKa introduces a data-efficient family of expert-level alignment datasets, specifically the PiKa-SFT dataset with only 30k examples. Fine-tuning Llama-3-8B-Base on PiKa showed better performance than models trained on larger datasets. PiKa-SFT even outperformed the official Llama-3-8B-Instruct model trained on over 10 million examples on benchmarks. Training the Qwen2.5 series on PiKa-SFT also yielded consistent gains. These results demonstrate that high-quality alignment can be achieved with significantly less data, offering a scalable path for open-source LLM alignment. The code and data are available on GitHub for further exploration and use.  
Summary: <div>
arXiv:2510.06670v1 Announce Type: new 
Abstract: Reinforcement Learning from Human Feedback (RLHF) has become a cornerstone for aligning large language models (LLMs). However, its effectiveness depends on high-quality instruction data. Most existing alignment datasets are either private or require costly human annotation, which limits reproducibility and scalability. Even with Reinforcement Learning from AI Feedback (RLAIF), concerns about data quality remain. Moreover, it is unclear how much data is actually required to fine-tune a base model into a strong instruction-following model. Current approaches often rely on over 300k examples even at the supervised fine-tuning (SFT) stage, yet they still underperform compared to proprietary models, creating barriers for academic and resource-limited communities. To address this gap, we introduce PiKa, a data-efficient family of expert-level alignment datasets. In particular, the PiKa-SFT dataset uses only 30k SFT examples, far fewer than state-of-the-art datasets like Magpie. Through evaluations by fine-tuning Llama-3-8B-Base on PiKa and other public datasets, we show that PiKa-SFT outperforms models trained on much larger data. On AlpacaEval 2.0 and Arena-Hard benchmarks, PiKa-SFT fine-tuning even surpasses the official Llama-3-8B-Instruct model trained on over 10 million proprietary examples. We further extend our study by training the Qwen2.5 series (0.5B to 7B) on PiKa-SFT, achieving consistent gains. These findings demonstrate that high-quality alignment can be achieved with significantly less data, offering a scalable path for open-source LLM alignment. Code and data: https://github.com/SJY8460/PiKa.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incremental Summarization for Customer Support via Progressive Note-Taking and Agent Feedback</title>
<link>https://arxiv.org/abs/2510.06677</link>
<guid>https://arxiv.org/abs/2510.06677</guid>
<content:encoded><![CDATA[
<div> incremental summarization, customer support agents, Mixtral-8x7B model, DeBERTa-based classifier, agent edits

Summary:
The article introduces an incremental summarization system designed for customer support agents that generates concise bullet notes during conversations. The system utilizes a Mixtral-8x7B model for continuous note generation and a DeBERTa-based classifier to filter out trivial content. Agent edits refine the online notes generation and contribute to model retraining. Deployed in production, the system achieved a 3% reduction in case handling time compared to bulk summarization, with even higher reductions in complex cases. Additionally, high agent satisfaction ratings were reported from surveys, indicating the system's effectiveness in enhancing summary quality and agent productivity at scale. The continuous feedback loop of agent edits informing model retraining plays a crucial role in optimizing the system's performance. <div>
arXiv:2510.06677v1 Announce Type: new 
Abstract: We introduce an incremental summarization system for customer support agents that intelligently determines when to generate concise bullet notes during conversations, reducing agents' context-switching effort and redundant review. Our approach combines a fine-tuned Mixtral-8x7B model for continuous note generation with a DeBERTa-based classifier to filter trivial content. Agent edits refine the online notes generation and regularly inform offline model retraining, closing the agent edits feedback loop. Deployed in production, our system achieved a 3% reduction in case handling time compared to bulk summarization (with reductions of up to 9% in highly complex cases), alongside high agent satisfaction ratings from surveys. These results demonstrate that incremental summarization with continuous feedback effectively enhances summary quality and agent productivity at scale.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Rewrite Prompts for Bootstrapping LLMs on Downstream Tasks</title>
<link>https://arxiv.org/abs/2510.06695</link>
<guid>https://arxiv.org/abs/2510.06695</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, prompt engineering, natural language generation, machine translation, back-translation

Summary:
This paper introduces a novel prompt optimization method specifically tailored for machine translation tasks. The approach focuses on optimizing the input component, which is crucial for tasks like machine translation. By training a small-parameter model using a back-translation strategy, the method reduces training overhead for single-task optimization while still achieving high performance. Unlike existing prompt engineering methods that primarily optimize the instruction component, this approach addresses the importance of the input component in tasks like machine translation. The proposed method not only improves performance in machine translation but also shows potential for extension to other downstream tasks. This novel approach fills a gap in prompt engineering for tasks where the input component plays a more pivotal role, providing an efficient and effective solution for optimizing prompt design in language models. 

<br /><br />Summary: <div>
arXiv:2510.06695v1 Announce Type: new 
Abstract: In recent years, the growing interest in Large Language Models (LLMs) has significantly advanced prompt engineering, transitioning from manual design to model-based optimization. Prompts for LLMs generally comprise two components: the \textit{instruction}, which defines the task or objective, and the \textit{input}, which is tailored to the instruction type. In natural language generation (NLG) tasks such as machine translation, the \textit{input} component is particularly critical, while the \textit{instruction} component tends to be concise. Existing prompt engineering methods primarily focus on optimizing the \textit{instruction} component for general tasks, often requiring large-parameter LLMs as auxiliary tools. However, these approaches exhibit limited applicability for tasks like machine translation, where the \textit{input} component plays a more pivotal role. To address this limitation, this paper introduces a novel prompt optimization method specifically designed for machine translation tasks. The proposed approach employs a small-parameter model trained using a back-translation-based strategy, significantly reducing training overhead for single-task optimization while delivering highly effective performance. With certain adaptations, this method can also be extended to other downstream tasks.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Language Models Conflate Logical Validity with Plausibility: A Representational Analysis of Content Effects</title>
<link>https://arxiv.org/abs/2510.06700</link>
<guid>https://arxiv.org/abs/2510.06700</guid>
<content:encoded><![CDATA[
<div> Keywords: humans, large language models, content effects, representational geometry, debiasing vectors

Summary: 
Both humans and large language models display content effects in reasoning tasks, where the plausibility of the content influences judgments of logical validity. While humans' content effects are explained by the dual-process theory, it is unclear how large language models (LLMs) exhibit these biases. In this study, researchers investigated how LLMs encode validity and plausibility concepts in their internal representations. They found that LLMs linearly represent and align these concepts in their geometry, leading to confusion between plausibility and validity. By manipulating steering vectors, they showed causal biases between plausibility and validity judgments, predicting the magnitude of content effects. The study also introduced debiasing vectors to separate these concepts, reducing content effects and enhancing reasoning accuracy. These findings contribute to understanding logical concept representation in LLMs and suggest representational interventions as a means to enhance their logical reasoning abilities. 

<br /><br />Summary: <div>
arXiv:2510.06700v1 Announce Type: new 
Abstract: Both humans and large language models (LLMs) exhibit content effects: biases in which the plausibility of the semantic content of a reasoning problem influences judgments regarding its logical validity. While this phenomenon in humans is best explained by the dual-process theory of reasoning, the mechanisms behind content effects in LLMs remain unclear. In this work, we address this issue by investigating how LLMs encode the concepts of validity and plausibility within their internal representations. We show that both concepts are linearly represented and strongly aligned in representational geometry, leading models to conflate plausibility with validity. Using steering vectors, we demonstrate that plausibility vectors can causally bias validity judgements, and vice versa, and that the degree of alignment between these two concepts predicts the magnitude of behavioral content effects across models. Finally, we construct debiasing vectors that disentangle these concepts, reducing content effects and improving reasoning accuracy. Our findings advance understanding of how abstract logical concepts are represented in LLMs and highlight representational interventions as a path toward more logical systems.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling LLM Multi-turn RL with End-to-end Summarization-based Context Management</title>
<link>https://arxiv.org/abs/2510.06727</link>
<guid>https://arxiv.org/abs/2510.06727</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, language model, context management, summarization, policy optimization <br />
Summary: 
This article introduces a new approach for reinforcement learning fine-tuning of large language model agents to improve long-horizon multi-turn tool use tasks. By incorporating summarization-based context management into training, the proposed method allows for optimal tool-use behaviors while maintaining a compact context beyond fixed limits. The SUmmarization augmented Policy Optimization (SUPO) algorithm is presented as a framework that seamlessly integrates summarization strategies with standard language model RL infrastructures. Experimental results on interactive tasks show that SUPO significantly enhances success rates with the same or shorter context length than baselines. Additionally, SUPO demonstrates improved performance on complex searching tasks by scaling the summarization rounds at test time. This study establishes summarization-based context management as a scalable and effective approach for training RL agents beyond fixed context length limits. <br /><br />Summary: <div>
arXiv:2510.06727v1 Announce Type: new 
Abstract: We study reinforcement learning (RL) fine-tuning of large language model (LLM) agents for long-horizon multi-turn tool use, where context length quickly becomes a fundamental bottleneck. Existing RL pipelines can suffer from degraded instruction following, excessive rollout costs, and most importantly, strict context limits. To address these challenges, we introduce summarization-based context management to training. In specific, it periodically compresses the tool using history by LLM-generated summaries that retain task-relevant information to keep a compact context while enabling the agent to scale beyond the fixed context window. Building on this formulation, we derive a policy gradient representation that seamlessly enables standard LLM RL infrastructures to optimize both tool-use behaviors as well as summarization strategies in an end-to-end fashion. We instantiate this framework with \underline{SU}mmarization augmented \underline{P}olicy \underline{O}ptimization (\texttt{SUPO}), an LLM RL algorithm that enables long-horizon training beyond a fixed context limit. Experiments on interactive function calling and searching tasks demonstrate that \texttt{SUPO} significantly improves the success rate while maintaining the same or even lower working context length compared to baselines. We also demonstrate that for complex searching tasks, \texttt{SUPO} can further improve the evaluation performance when scaling test-time maximum round of summarization beyond that of training time. Our results establish summarization-based context management as a principled and scalable approach for training RL agents beyond a fixed context length limit.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PTEB: Towards Robust Text Embedding Evaluation via Stochastic Paraphrasing at Evaluation Time with LLMs</title>
<link>https://arxiv.org/abs/2510.06730</link>
<guid>https://arxiv.org/abs/2510.06730</guid>
<content:encoded><![CDATA[
<div> Keywords: sentence embedding models, Paraphrasing Text Embedding Benchmark (PTEB), dynamic evaluation, semantic similarity, token diversity

Summary:
Sentence embedding models are often evaluated using static benchmarks, leading to inflated performance metrics. The Paraphrasing Text Embedding Benchmark (PTEB) introduces a dynamic evaluation protocol that generates meaning-preserving paraphrases at runtime. By utilizing a cost-efficient method based on semantic textual similarity gold ratings, the study demonstrates that large language models (LLMs) can produce diverse token-based but semantically equivalent paraphrases. The research shows that changes in token space affect the performance of sentence encoders even when semantics remain constant. Furthermore, smaller models are not disproportionately affected compared to larger ones. The results are statistically reliable across multiple runs and extend to multilingual datasets in various languages. This study proposes a new evaluation paradigm for natural language processing, advocating for dynamic and stochastic evaluation methods that leverage computational resources at evaluation time. 

<br /><br />Summary: Sentence embedding models are evaluated using static benchmarks such as the MTEB, leading to potentially inflated metrics. The PTEB introduces a dynamic evaluation protocol that generates paraphrases at runtime and demonstrates the impact of token diversity on model performance. Smaller models are not disproportionately affected, highlighting the importance of dynamic evaluation protocols. The study extends to multilingual datasets, proposing a paradigm shift in NLP evaluation towards dynamic and stochastic methods. <div>
arXiv:2510.06730v1 Announce Type: new 
Abstract: Current evaluations of sentence embedding models typically rely on static test beds such as the Massive Text Embedding Benchmark (MTEB). While invaluable, repeated tuning on a fixed suite can inflate reported performance and obscure real-world robustness. We introduce the Paraphrasing Text Embedding Benchmark (PTEB), a dynamic protocol that stochastically generates meaning-preserving paraphrases at evaluation time and aggregates results across multiple runs. Using a cost-efficient LLM-based method grounded in semantic textual similarity gold ratings, we show that LLMs generate token-diverse but semantically preserving, paraphrases. Across 7 MTEB tasks, we validate our hypothesis that the performance of sentence encoders is sensitive to changes in token space even when semantics remain fixed. We also observe that smaller models are not disproportionately affected relative to larger ones. Our results are statistically robust over multiple runs and we extended our experiments to 3 multilingual datasets covering 10 languages. More generally, we aim to propose a new evaluation paradigm in NLP that relies less on static, pre-defined benchmarks but shifts towards dynamic, stochastic evaluation leveraging eval-time compute.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are LLMs Reliable Rankers? Rank Manipulation via Two-Stage Token Optimization</title>
<link>https://arxiv.org/abs/2510.06732</link>
<guid>https://arxiv.org/abs/2510.06732</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, information retrieval, token optimization, ranking effectiveness, linguistic naturalness

Summary:
Large language models (LLMs) are commonly used in information retrieval, but their ranking behavior can be influenced by small prompts. Rank Anything First (RAF) is a method that crafts textual perturbations to consistently boost the rank of a target item in LLM-generated rankings. RAF uses a two-stage token optimization approach, guided by dual objectives of maximizing ranking effectiveness and preserving linguistic naturalness. Experimental results show that RAF significantly improves the rank of target items using natural language, with greater robustness compared to existing methods. This vulnerability highlights the potential for adversarial manipulation of LLM-based reranking systems, raising concerns about the trustworthiness and robustness of modern retrieval systems. The RAF code is available at: https://github.com/glad-lab/RAF.

<br /><br />Summary: Large language models can be manipulated in information retrieval using natural-sounding prompts. Rank Anything First (RAF) method enhances target item ranking robustness by optimizing tokens with a dual objective approach focused on ranking effectiveness and linguistic naturalness. Experimental results demonstrate RAF's superiority over existing methods, emphasizing the vulnerability of LLM-based reranking systems to adversarial attacks and highlighting trust and robustness concerns. RAF code is accessible at: https://github.com/glad-lab/RAF. <div>
arXiv:2510.06732v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used as rerankers in information retrieval, yet their ranking behavior can be steered by small, natural-sounding prompts. To expose this vulnerability, we present Rank Anything First (RAF), a two-stage token optimization method that crafts concise textual perturbations to consistently promote a target item in LLM-generated rankings while remaining hard to detect. Stage 1 uses Greedy Coordinate Gradient to shortlist candidate tokens at the current position by combining the gradient of the rank-target with a readability score; Stage 2 evaluates those candidates under exact ranking and readability losses using an entropy-based dynamic weighting scheme, and selects a token via temperature-controlled sampling. RAF generates ranking-promoting prompts token-by-token, guided by dual objectives: maximizing ranking effectiveness and preserving linguistic naturalness. Experiments across multiple LLMs show that RAF significantly boosts the rank of target items using naturalistic language, with greater robustness than existing methods in both promoting target items and maintaining naturalness. These findings underscore a critical security implication: LLM-based reranking is inherently susceptible to adversarial manipulation, raising new challenges for the trustworthiness and robustness of modern retrieval systems. Our code is available at: https://github.com/glad-lab/RAF.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AWM: Accurate Weight-Matrix Fingerprint for Large Language Models</title>
<link>https://arxiv.org/abs/2510.06738</link>
<guid>https://arxiv.org/abs/2510.06738</guid>
<content:encoded><![CDATA[
<div> method, fingerprinting, language models, lineage verification, weight matrices

Summary:<br />
- A novel training-free fingerprinting method is proposed for large language models (LLMs) based on weight matrices.
- The method utilizes Linear Assignment Problem (LAP) and Centered Kernel Alignment (CKA) similarity to neutralize the effects of post-training processes, ensuring robust and accurate model verification.
- Exceptional robustness against supervised fine-tuning, pretraining, reinforcement learning, multi-modal extension, pruning, and upcycling is demonstrated on a testbed of model pairs.
- The approach achieves perfect classification metrics and near-zero false positives, establishing a strong foundation for reliable model lineage verification.
- The entire computation process completes within 30s on an NVIDIA 3090 GPU, making it efficient for practical use. <div>
arXiv:2510.06738v1 Announce Type: new 
Abstract: Protecting the intellectual property of large language models (LLMs) is crucial, given the substantial resources required for their training. Consequently, there is an urgent need for both model owners and third parties to determine whether a suspect LLM is trained from scratch or derived from an existing base model. However, the intensive post-training processes that models typically undergo-such as supervised fine-tuning, extensive continued pretraining, reinforcement learning, multi-modal extension, pruning, and upcycling-pose significant challenges to reliable identification. In this work, we propose a training-free fingerprinting method based on weight matrices. We leverage the Linear Assignment Problem (LAP) and an unbiased Centered Kernel Alignment (CKA) similarity to neutralize the effects of parameter manipulations, yielding a highly robust and high-fidelity similarity metric. On a comprehensive testbed of 60 positive and 90 negative model pairs, our method demonstrates exceptional robustness against all six aforementioned post-training categories while exhibiting a near-zero risk of false positives. By achieving perfect scores on all classification metrics, our approach establishes a strong basis for reliable model lineage verification. Moreover, the entire computation completes within 30s on an NVIDIA 3090 GPU. The code is available at https://github.com/LUMIA-Group/AWM.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TWIST: Training-free and Label-free Short Text Clustering through Iterative Vector Updating with LLMs</title>
<link>https://arxiv.org/abs/2510.06747</link>
<guid>https://arxiv.org/abs/2510.06747</guid>
<content:encoded><![CDATA[
<div> training-free, label-free, short text clustering, chatbots, LLM guidance

Summary:<br />
- The paper presents a training-free and label-free method for clustering short texts in customer-facing chatbots.
- The method is based on iterative vector updating and LLM guidance to construct and refine sparse vectors.
- It achieves comparable or superior results to existing methods without prior knowledge of clusters or labels.
- The method is model agnostic and can be applied to any embedder with relatively small LLMs.
- It scales well to large datasets, reducing the computational cost of the LLM.
<br /><br />Summary: <div>
arXiv:2510.06747v1 Announce Type: new 
Abstract: In this paper, we propose a training-free and label-free method for short text clustering that can be used on top of any existing embedder. In the context of customer-facing chatbots, companies are dealing with large amounts of user utterances that need to be clustered according to their intent. In these commercial settings, no labeled data is typically available, and the number of clusters is not known. Our method is based on iterative vector updating: it constructs sparse vectors based on representative texts, and then iteratively refines them through LLM guidance. Our method achieves comparable or superior results to state-of-the-art methods that use contrastive learning, but without assuming prior knowledge of clusters or labels. Experiments on diverse datasets and smaller LLMs show that our method is model agnostic and can be applied to any embedder, with relatively small LLMs, and different clustering methods. We also show that our method scales to large datasets, reducing the computational cost of the LLM. These low-resource, adaptable settings and the scalability of our method make it more aligned with real-world scenarios than existing clustering methods.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Formal Framework for Fluency-based Multi-Reference Evaluation in Grammatical Error Correction</title>
<link>https://arxiv.org/abs/2510.06749</link>
<guid>https://arxiv.org/abs/2510.06749</guid>
<content:encoded><![CDATA[
<div> Evaluation Metrics, Grammatical Error Correction, Fluency-based Multi-reference Evaluation, GLEU, Aggregation Strategies <br />
Summary: The article introduces a new framework for evaluating grammatical error correction that emphasizes the diversity of valid human corrections. Existing evaluation frameworks often privilege a single reference and rely on rigid alignments between system and reference edits, limiting their applicability in multilingual and generative settings. The proposed framework, based on fluency-based multi-reference evaluation, views n-gram similarity as an aggregation problem over multiple legitimate corrections. Through the instantiation of GLEU using four different aggregation strategies, the study shows that these strategies capture complementary aspects of fluency and coverage. Empirical results on various language corpora such as Czech, Estonian, Ukrainian, and Chinese demonstrate that the framework allows for the incorporation of linguistic diversity without penalizing legitimate variation. The framework provides a principled, fluency-oriented approach to multi-reference evaluation in grammatical error correction. <br /><br /> <div>
arXiv:2510.06749v1 Announce Type: new 
Abstract: Evaluating grammatical error correction requires metrics that reflect the diversity of valid human corrections rather than privileging a single reference. Existing frameworks, largely edit-based and English-centric, rely on rigid alignments between system and reference edits, limiting their applicability in multilingual and generative settings. This paper introduces a formal framework for \textit{fluency-based multi-reference evaluation}, framing $n$-gram similarity as an aggregation problem over multiple legitimate corrections. Within this formulation, we instantiate GLEU through four aggregation strategies--\textsc{select-best}, \textsc{simple-average}, \textsc{weighted-average}, and \textsc{merged-counts}--and analyze their properties of boundedness, monotonicity, and sensitivity to reference variation. Empirical results on Czech, Estonian, Ukrainian, and Chinese corpora show that these strategies capture complementary aspects of fluency and coverage. The framework unifies multi-reference evaluation into a principled, fluency-oriented approach that incorporates linguistic diversity without penalizing legitimate variation.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gold-Switch: Training-Free Superposition of Slow- and Fast- Thinking LLMs</title>
<link>https://arxiv.org/abs/2510.06750</link>
<guid>https://arxiv.org/abs/2510.06750</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Reasoning Models, overthinking, deployment strategy, optimization, inference 

Summary: 
The study introduces a strategy to optimize the performance of Large Reasoning Models (LRMs) by deploying a lightweight regulation that switches the model on and off during inference. This approach aims to prevent overthinking and reduce resource wastage without the need for multiple models. By selectively unlearning from the LRM at inference and adjusting reasoning through low-rank projections, the proposed strategy effectively scales down computation while preserving reasoning ability. The analysis of cumulative energy of singular values helps identify the optimal projections for adjusting reasoning levels. This superposed deployment strategy offers a cost-effective solution to enhance the efficiency of LRMs in structured tasks, addressing performance degradation and resource constraints commonly associated with these models. <br /><br />Summary: <div>
arXiv:2510.06750v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) excel in structured tasks by emulating deliberate human reasoning but often suffer from overthinking, degrading performance and wasting resources. One possible baseline is to deploy both LLM and LRM, then route input by predicting whether it requires reasoning and may cause overthinking. However, deploying multiple models can be costly or impractical. We propose a superposed deployment strategy with a lightweight, training-free regulation to optimize inference by switching one model on and off. Instead of routing, we selectively unlearn from LRM at inference, scaling down computation while preserving reasoning. By analyzing the cumulative energy of singular values, we identify optimal low-rank projections to adjust reasoning just right.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive LLM-Symbolic Reasoning via Dynamic Logical Solver Composition</title>
<link>https://arxiv.org/abs/2510.06774</link>
<guid>https://arxiv.org/abs/2510.06774</guid>
<content:encoded><![CDATA[
<div> Neuro-symbolic NLP, formal logical solvers, adaptive framework, reasoning strategies, natural language <br />
<br />
Summary: 
The article introduces an adaptive, multi-paradigm neuro-symbolic inference framework that identifies formal reasoning strategies from natural language problems and dynamically selects specialized formal logical solvers. Large language models (LLMs) effectively predict necessary reasoning strategies with over 90% accuracy, leading to outperformance of GPT-4o and DeepSeek-V3.1 by 27% and 6%, respectively. Adaptive reasoning positively impacts pure LLM methods, improving zero-shot, CoT, and symbolic CoT settings with GPT-4o by 10%, 5%, and 6%. Smaller models struggle with adaptive reasoning but show improvement through post-training. Overall, the work establishes foundations for adaptive LLM-symbolic reasoning, paving the way for unified material and formal inferences on diverse reasoning challenges. <br /><br />Summary: <div>
arXiv:2510.06774v1 Announce Type: new 
Abstract: Neuro-symbolic NLP methods aim to leverage the complementary strengths of large language models and formal logical solvers. However, current approaches are mostly static in nature, i.e., the integration of a target solver is predetermined at design time, hindering the ability to employ diverse formal inference strategies. To address this, we introduce an adaptive, multi-paradigm, neuro-symbolic inference framework that: (1) automatically identifies formal reasoning strategies from problems expressed in natural language; and (2) dynamically selects and applies specialized formal logical solvers via autoformalization interfaces. Extensive experiments on individual and multi-paradigm reasoning tasks support the following conclusions: LLMs are effective at predicting the necessary formal reasoning strategies with an accuracy above 90 percent. This enables flexible integration with formal logical solvers, resulting in our framework outperforming competing baselines by 27 percent and 6 percent compared to GPT-4o and DeepSeek-V3.1, respectively. Moreover, adaptive reasoning can even positively impact pure LLM methods, yielding gains of 10, 5, and 6 percent on zero-shot, CoT, and symbolic CoT settings with GPT-4o. Finally, although smaller models struggle with adaptive neuro-symbolic reasoning, post-training offers a viable path to improvement. Overall, this work establishes the foundations for adaptive LLM-symbolic reasoning, offering a path forward for unifying material and formal inferences on heterogeneous reasoning challenges.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundations of LLM Knowledge Materialization: Termination, Reproducibility, Robustness</title>
<link>https://arxiv.org/abs/2510.06780</link>
<guid>https://arxiv.org/abs/2510.06780</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, knowledge extraction, termination, reproducibility, robustness, structured format 

Summary: 
- The study focuses on measuring and systematizing knowledge extracted from Large Language Models (LLMs) using miniGPTKBs, domain-specific subcrawls.
- Analysis of termination, reproducibility, and robustness of knowledge materialization across different metrics and variations shows high termination rates influenced by the model, mixed reproducibility, and varying robustness.
- Results indicate that LLM knowledge materialization can reliably surface core knowledge but also reveal limitations in reproducibility and robustness.
- Experimentation with different perturbation types shows high robustness for seeds and temperature variations but lower robustness for languages and models.
- The study suggests that converting LLM knowledge into structured formats can be effective in extracting core knowledge but highlights the importance of considering limitations such as reproducibility and robustness. 

<br /><br />Summary: <div>
arXiv:2510.06780v1 Announce Type: new 
Abstract: Large Language Models (LLMs) encode substantial factual knowledge, yet measuring and systematizing this knowledge remains challenging. Converting it into structured format, for example through recursive extraction approaches such as the GPTKB methodology (Hu et al., 2025b), is still underexplored. Key open questions include whether such extraction can terminate, whether its outputs are reproducible, and how robust they are to variations. We systematically study LLM knowledge materialization using miniGPTKBs (domain-specific, tractable subcrawls), analyzing termination, reproducibility, and robustness across three categories of metrics: yield, lexical similarity, and semantic similarity. We experiment with four variations (seed, language, randomness, model) and three illustrative domains (from history, entertainment, and finance). Our findings show (i) high termination rates, though model-dependent; (ii) mixed reproducibility; and (iii) robustness that varies by perturbation type: high for seeds and temperature, lower for languages and models. These results suggest that LLM knowledge materialization can reliably surface core knowledge, while also revealing important limitations.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FURINA: A Fully Customizable Role-Playing Benchmark via Scalable Multi-Agent Collaboration Pipeline</title>
<link>https://arxiv.org/abs/2510.06800</link>
<guid>https://arxiv.org/abs/2510.06800</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, role-playing tasks, benchmark builder, evaluation, reasoning capabilities <br />
Summary: <br />
The article introduces FURINA-Builder, a pipeline for creating customizable role-playing benchmarks to evaluate large language models in diverse scenarios. It constructs dialogues between test characters and others from a character-scene pool, utilizing an LLM judge for evaluation. The resulting FURINA-Bench benchmark includes established and synthesized test characters assessed based on specific criteria. Evaluation of LLMs shows o3 and DeepSeek-R1 as top performers in English and Chinese RP tasks. Established characters consistently outperform synthesized ones, with reasoning LLMs showing improved performance but increased hallucinations. A trade-off between RP performance and reliability is observed, with a Pareto frontier across all LLMs. Overall, the study highlights the effectiveness of FURINA-Builder in creating versatile RP benchmarks and the challenges posed by the FURINA-Bench benchmark. <br /> <div>
arXiv:2510.06800v1 Announce Type: new 
Abstract: As large language models (LLMs) advance in role-playing (RP) tasks, existing benchmarks quickly become obsolete due to their narrow scope, outdated interaction paradigms, and limited adaptability across diverse application scenarios. To address this gap, we introduce FURINA-Builder, a novel multi-agent collaboration pipeline that automatically constructs fully customizable RP benchmarks at any scale. It enables evaluation of arbitrary characters across diverse scenarios and prompt formats, as the first benchmark builder in RP area for adaptable assessment. FURINA-Builder simulates dialogues between a test character and other characters drawn from a well-constructed character-scene pool, while an LLM judge selects fine-grained evaluation dimensions and adjusts the test character's responses into final test utterances. Using this pipeline, we build FURINA-Bench, a new comprehensive role-playing benchmark featuring both established and synthesized test characters, each assessed with dimension-specific evaluation criteria. Human evaluation and preliminary separability analysis justify our pipeline and benchmark design. We conduct extensive evaluations of cutting-edge LLMs and find that o3 and DeepSeek-R1 achieve the best performance on English and Chinese RP tasks, respectively. Across all models, established characters consistently outperform synthesized ones, with reasoning capabilities further amplifying this disparity. Interestingly, we observe that model scale does not monotonically reduce hallucinations. More critically, for reasoning LLMs, we uncover a novel trade-off: reasoning improves RP performance but simultaneously increases RP hallucinations. This trade-off extends to a broader Pareto frontier between RP performance and reliability for all LLMs. These findings demonstrate the effectiveness of FURINA-Builder and the challenge posed by FURINA-Bench.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overview of the Plagiarism Detection Task at PAN 2025</title>
<link>https://arxiv.org/abs/2510.06805</link>
<guid>https://arxiv.org/abs/2510.06805</guid>
<content:encoded><![CDATA[
arXiv:2510.06805v1 Announce Type: new 
Abstract: The generative plagiarism detection task at PAN 2025 aims at identifying automatically generated textual plagiarism in scientific articles and aligning them with their respective sources. We created a novel large-scale dataset of automatically generated plagiarism using three large language models: Llama, DeepSeek-R1, and Mistral. In this task overview paper, we outline the creation of this dataset, summarize and compare the results of all participants and four baselines, and evaluate the results on the last plagiarism detection task from PAN 2015 in order to interpret the robustness of the proposed approaches. We found that the current iteration does not invite a large variety of approaches as naive semantic similarity approaches based on embedding vectors provide promising results of up to 0.8 recall and 0.5 precision. In contrast, most of these approaches underperform significantly on the 2015 dataset, indicating a lack in generalizability.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BlackboxNLP-2025 MIB Shared Task: Exploring Ensemble Strategies for Circuit Localization Methods</title>
<link>https://arxiv.org/abs/2510.06811</link>
<guid>https://arxiv.org/abs/2510.06811</guid>
<content:encoded><![CDATA[
arXiv:2510.06811v1 Announce Type: new 
Abstract: The Circuit Localization track of the Mechanistic Interpretability Benchmark (MIB) evaluates methods for localizing circuits within large language models (LLMs), i.e., subnetworks responsible for specific task behaviors. In this work, we investigate whether ensembling two or more circuit localization methods can improve performance. We explore two variants: parallel and sequential ensembling. In parallel ensembling, we combine attribution scores assigned to each edge by different methods-e.g., by averaging or taking the minimum or maximum value. In the sequential ensemble, we use edge attribution scores obtained via EAP-IG as a warm start for a more expensive but more precise circuit identification method, namely edge pruning. We observe that both approaches yield notable gains on the benchmark metrics, leading to a more precise circuit identification approach. Finally, we find that taking a parallel ensemble over various methods, including the sequential ensemble, achieves the best results. We evaluate our approach in the BlackboxNLP 2025 MIB Shared Task, comparing ensemble scores to official baselines across multiple model-task combinations.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Tool Generation with Models as Tools and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.06825</link>
<guid>https://arxiv.org/abs/2510.06825</guid>
<content:encoded><![CDATA[
arXiv:2510.06825v1 Announce Type: new 
Abstract: Tool-augmented language models have demonstrated strong capabilities, but their reliance on live API access creates scalability and reliability challenges during training and deployment. We propose MTR, a simulation-first training framework for tool-augmented reasoning. Instead of relying on live APIs, MTR learns from complete ReAct traces with schema-validated, simulated observations. Our approach operates through a multi-agent architecture where a ToolMaker generates task-specific, OpenAI-compatible tool interfaces, an AutoAgent produces structured think-act-observe sequences, and a ToolActor simulates realistic responses. Training proceeds in two stages: Stage-1 Supervised Fine-Tuning (SFT) teaches 'trace grammar' from complete reasoning sequences; Stage-2 Group Relative Policy Optimization (GRPO) optimizes strategy with a composite trace reward that balances answer correctness and internal consistency. Across four multi-hop QA benchmarks (HotpotQA, MuSiQue, 2WikiMultiHopQA, Bamboogle), MTR attains competitive Exact Match (EM) scores to live-API systems and excels on reasoning-intensive tasks, suggesting that effective tool reasoning can be learned from structured traces without live interactions.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mid-Training of Large Language Models: A Survey</title>
<link>https://arxiv.org/abs/2510.06826</link>
<guid>https://arxiv.org/abs/2510.06826</guid>
<content:encoded><![CDATA[
arXiv:2510.06826v1 Announce Type: new 
Abstract: Large language models (LLMs) are typically developed through large-scale pre-training followed by task-specific fine-tuning. Recent advances highlight the importance of an intermediate mid-training stage, where models undergo multiple annealing-style phases that refine data quality, adapt optimization schedules, and extend context length. This stage mitigates diminishing returns from noisy tokens, stabilizes convergence, and expands model capability in late training. Its effectiveness can be explained through gradient noise scale, the information bottleneck, and curriculum learning, which together promote generalization and abstraction. Despite widespread use in state-of-the-art systems, there has been no prior survey of mid-training as a unified paradigm. We introduce the first taxonomy of LLM mid-training spanning data distribution, learning-rate scheduling, and long-context extension. We distill practical insights, compile evaluation benchmarks, and report gains to enable structured comparisons across models. We also identify open challenges and propose avenues for future research and practice.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GAMBIT+: A Challenge Set for Evaluating Gender Bias in Machine Translation Quality Estimation Metrics</title>
<link>https://arxiv.org/abs/2510.06841</link>
<guid>https://arxiv.org/abs/2510.06841</guid>
<content:encoded><![CDATA[
arXiv:2510.06841v1 Announce Type: new 
Abstract: Gender bias in machine translation (MT) systems has been extensively documented, but bias in automatic quality estimation (QE) metrics remains comparatively underexplored. Existing studies suggest that QE metrics can also exhibit gender bias, yet most analyses are limited by small datasets, narrow occupational coverage, and restricted language variety. To address this gap, we introduce a large-scale challenge set specifically designed to probe the behavior of QE metrics when evaluating translations containing gender-ambiguous occupational terms. Building on the GAMBIT corpus of English texts with gender-ambiguous occupations, we extend coverage to three source languages that are genderless or natural-gendered, and eleven target languages with grammatical gender, resulting in 33 source-target language pairs. Each source text is paired with two target versions differing only in the grammatical gender of the occupational term(s) (masculine vs. feminine), with all dependent grammatical elements adjusted accordingly. An unbiased QE metric should assign equal or near-equal scores to both versions. The dataset's scale, breadth, and fully parallel design, where the same set of texts is aligned across all languages, enables fine-grained bias analysis by occupation and systematic comparisons across languages.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SID: Multi-LLM Debate Driven by Self Signals</title>
<link>https://arxiv.org/abs/2510.06843</link>
<guid>https://arxiv.org/abs/2510.06843</guid>
<content:encoded><![CDATA[
arXiv:2510.06843v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have exhibited impressive capabilities across diverse application domains. Recent work has explored Multi-LLM Agent Debate (MAD) as a way to enhance performance by enabling multiple LLMs to discuss and refine responses iteratively. Nevertheless, existing MAD methods predominantly focus on utilizing external structures, such as debate graphs, using LLM-as-a-Judge, while neglecting the application of self signals, such as token logits and attention, that arise during generation. This omission leads to redundant computation and potential performance degradation. In this paper, we shift the focus to the self signals of multi-LLM debate and introduce a Self-Signals Driven Multi-LLM Debate (SID), which leverages two types of self-signals: model-level confidence and token-level semantic focus, to adaptively guide the debate process. Our approach enables high-confidence agents to exit early at the model level and compress the redundant debate contents based on the attention mechanism. We evaluate our method on various LLMs and Multimodal LLMs across multiple challenging benchmarks. Experimental results demonstrate that our method not only outperforms existing MAD techniques in accuracy but also reduces token consumption, highlighting the effectiveness of utilizing self signals in enhancing both the performance and efficiency of multi-agent debate systems. Our code will be available at~\href{https://github.com/xuhang2019/SID}{\texttt{https://github.com/xuhang2019/SID}}.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenJAI-v1.0: An Open Thai Large Language Model</title>
<link>https://arxiv.org/abs/2510.06847</link>
<guid>https://arxiv.org/abs/2510.06847</guid>
<content:encoded><![CDATA[
arXiv:2510.06847v1 Announce Type: new 
Abstract: We introduce OpenJAI-v1.0, an open-source large language model for Thai and English, developed from the Qwen3-14B model. Our work focuses on boosting performance on practical tasks through carefully curated data across three key use cases: instruction following, long-context understanding, and tool use. Evaluation results show that OpenJAI-v1.0 improves on the capabilities of its base model and outperforms other leading open-source Thai models on a diverse suite of benchmarks, while avoiding catastrophic forgetting. OpenJAI-v1.0 is publicly released as another alternative NLP resource for the Thai AI community.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlocking Latent Discourse Translation in LLMs Through Quality-Aware Decoding</title>
<link>https://arxiv.org/abs/2510.06866</link>
<guid>https://arxiv.org/abs/2510.06866</guid>
<content:encoded><![CDATA[
arXiv:2510.06866v1 Announce Type: new 
Abstract: Large language models (LLMs) have emerged as strong contenders in machine translation.Yet, they still struggle to adequately handle discourse phenomena, such as pronoun resolution and lexical cohesion at the document level. In this study, we thoroughly investigate the discourse phenomena performance of LLMs in context-aware translation. We demonstrate that discourse knowledge is encoded within LLMs and propose the use of quality-aware decoding (QAD) to effectively extract this knowledge, showcasing its superiority over other decoding approaches through comprehensive analysis. Furthermore, we illustrate that QAD enhances the semantic richness of translations and aligns them more closely with human preferences.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\lambda$-GRPO: Unifying the GRPO Frameworks with Learnable Token Preferences</title>
<link>https://arxiv.org/abs/2510.06870</link>
<guid>https://arxiv.org/abs/2510.06870</guid>
<content:encoded><![CDATA[
arXiv:2510.06870v1 Announce Type: new 
Abstract: Reinforcement Learning with Human Feedback (RLHF) has been the dominant approach for improving the reasoning capabilities of Large Language Models (LLMs). Recently, Reinforcement Learning with Verifiable Rewards (RLVR) has simplified this paradigm by replacing the reward and value models with rule-based verifiers. A prominent example is Group Relative Policy Optimization (GRPO). However, GRPO inherently suffers from a length bias, since the same advantage is uniformly assigned to all tokens of a response. As a result, longer responses distribute the reward over more tokens and thus contribute disproportionately to gradient updates. Several variants, such as DAPO and Dr. GRPO, modify the token-level aggregation of the loss, yet these methods remain heuristic and offer limited interpretability regarding their implicit token preferences. In this work, we explore the possibility of allowing the model to learn its own token preference during optimization. We unify existing frameworks under a single formulation and introduce a learnable parameter $\lambda$ that adaptively controls token-level weighting. We use $\lambda$-GRPO to denote our method, and we find that $\lambda$-GRPO achieves consistent improvements over vanilla GRPO and DAPO on multiple mathematical reasoning benchmarks. On Qwen2.5 models with 1.5B, 3B, and 7B parameters, $\lambda$-GRPO improves average accuracy by $+1.9\%$, $+1.0\%$, and $+1.7\%$ compared to GRPO, respectively. Importantly, these gains come without any modifications to the training data or additional computational cost, highlighting the effectiveness and practicality of learning token preferences.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MeXtract: Light-Weight Metadata Extraction from Scientific Papers</title>
<link>https://arxiv.org/abs/2510.06889</link>
<guid>https://arxiv.org/abs/2510.06889</guid>
<content:encoded><![CDATA[
arXiv:2510.06889v1 Announce Type: new 
Abstract: Metadata plays a critical role in indexing, documenting, and analyzing scientific literature, yet extracting it accurately and efficiently remains a challenging task. Traditional approaches often rely on rule-based or task-specific models, which struggle to generalize across domains and schema variations. In this paper, we present MeXtract, a family of lightweight language models designed for metadata extraction from scientific papers. The models, ranging from 0.5B to 3B parameters, are built by fine-tuning Qwen 2.5 counterparts. In their size family, MeXtract achieves state-of-the-art performance on metadata extraction on the MOLE benchmark. To further support evaluation, we extend the MOLE benchmark to incorporate model-specific metadata, providing an out-of-domain challenging subset. Our experiments show that fine-tuning on a given schema not only yields high accuracy but also transfers effectively to unseen schemas, demonstrating the robustness and adaptability of our approach. We release all the code, datasets, and models openly for the research community.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling</title>
<link>https://arxiv.org/abs/2510.06915</link>
<guid>https://arxiv.org/abs/2510.06915</guid>
<content:encoded><![CDATA[
arXiv:2510.06915v1 Announce Type: new 
Abstract: Reward model (RM) plays a pivotal role in aligning large language model (LLM) with human preferences. As real-world applications increasingly involve long history trajectories, e.g., LLM agent, it becomes indispensable to evaluate whether a model's responses are not only high-quality but also grounded in and consistent with the provided context. Yet, current RMs remain confined to short-context settings and primarily focus on response-level attributes (e.g., safety or helpfulness), while largely neglecting the critical dimension of long context-response consistency. In this work, we introduce Long-RewardBench, a benchmark specifically designed for long-context RM evaluation, featuring both Pairwise Comparison and Best-of-N tasks. Our preliminary study reveals that even state-of-the-art generative RMs exhibit significant fragility in long-context scenarios, failing to maintain context-aware preference judgments. Motivated by the analysis of failure patterns observed in model outputs, we propose a general multi-stage training strategy that effectively scales arbitrary models into robust Long-context RMs (LongRMs). Experiments show that our approach not only substantially improves performance on long-context evaluation but also preserves strong short-context capability. Notably, our 8B LongRM outperforms much larger 70B-scale baselines and matches the performance of the proprietary Gemini 2.5 Pro model.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models</title>
<link>https://arxiv.org/abs/2510.06917</link>
<guid>https://arxiv.org/abs/2510.06917</guid>
<content:encoded><![CDATA[
arXiv:2510.06917v1 Announce Type: new 
Abstract: Current large language models (LLMs) and spoken language models (SLMs) begin thinking and taking actions only after the user has finished their turn. This prevents the model from interacting during the user's turn and can lead to high response latency while it waits to think. Consequently, thinking after receiving the full input is not suitable for speech-to-speech interaction, where real-time, low-latency exchange is important. We address this by noting that humans naturally "think while listening." In this paper, we propose SHANKS, a general inference framework that enables SLMs to generate unspoken chain-of-thought reasoning while listening to the user input. SHANKS streams the input speech in fixed-duration chunks and, as soon as a chunk is received, generates unspoken reasoning based on all previous speech and reasoning, while the user continues speaking. SHANKS uses this unspoken reasoning to decide whether to interrupt the user and to make tool calls to complete the task. We demonstrate that SHANKS enhances real-time user-SLM interaction in two scenarios: (1) when the user is presenting a step-by-step solution to a math problem, SHANKS can listen, reason, and interrupt when the user makes a mistake, achieving 37.1% higher interruption accuracy than a baseline that interrupts without thinking; and (2) in a tool-augmented dialogue, SHANKS can complete 56.9% of the tool calls before the user finishes their turn. Overall, SHANKS moves toward models that keep thinking throughout the conversation, not only after a turn ends. Animated illustrations of Shanks can be found at https://d223302.github.io/SHANKS/
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open ASR Leaderboard: Towards Reproducible and Transparent Multilingual and Long-Form Speech Recognition Evaluation</title>
<link>https://arxiv.org/abs/2510.06961</link>
<guid>https://arxiv.org/abs/2510.06961</guid>
<content:encoded><![CDATA[
arXiv:2510.06961v1 Announce Type: new 
Abstract: Despite rapid progress, ASR evaluation remains saturated with short-form English, and efficiency is rarely reported. We present the Open ASR Leaderboard, a fully reproducible benchmark and interactive leaderboard comparing 60+ open-source and proprietary systems across 11 datasets, including dedicated multilingual and long-form tracks. We standardize text normalization and report both word error rate (WER) and inverse real-time factor (RTFx), enabling fair accuracy-efficiency comparisons. For English transcription, Conformer encoders paired with LLM decoders achieve the best average WER but are slower, while CTC and TDT decoders deliver much better RTFx, making them attractive for long-form and offline use. Whisper-derived encoders fine-tuned for English improve accuracy but often trade off multilingual coverage. All code and dataset loaders are open-sourced to support transparent, extensible evaluation.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EDUMATH: Generating Standards-aligned Educational Math Word Problems</title>
<link>https://arxiv.org/abs/2510.06965</link>
<guid>https://arxiv.org/abs/2510.06965</guid>
<content:encoded><![CDATA[
arXiv:2510.06965v1 Announce Type: new 
Abstract: Math word problems (MWPs) are critical K-12 educational tools, and customizing them to students' interests and ability levels can increase learning outcomes. However, teachers struggle to find time to customize MWPs for each student given large class sizes and increasing burnout. We propose that LLMs can support math education by generating MWPs customized to student interests and math education standards. To this end, we use a joint human expert-LLM judge approach to evaluate over 11,000 MWPs generated by open and closed LLMs and develop the first teacher-annotated dataset for standards-aligned educational MWP generation. We show the value of our data by using it to train a 12B open model that matches the performance of larger and more capable open models. We also use our teacher-annotated data to train a text classifier that enables a 30B open LLM to outperform existing closed baselines without any training. Next, we show our models' MWPs are more similar to human-written MWPs than those from existing models. We conclude by conducting the first study of customized LLM-generated MWPs with grade school students, finding they perform similarly on our models' MWPs relative to human-written MWPs but consistently prefer our customized MWPs.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing Social Identity Bias in Chinese LLMs with Gendered Pronouns and Social Groups</title>
<link>https://arxiv.org/abs/2510.06974</link>
<guid>https://arxiv.org/abs/2510.06974</guid>
<content:encoded><![CDATA[
arXiv:2510.06974v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed in user-facing applications, raising concerns about their potential to reflect and amplify social biases. We investigate social identity framing in Chinese LLMs using Mandarin-specific prompts across ten representative Chinese LLMs, evaluating responses to ingroup ("We") and outgroup ("They") framings, and extending the setting to 240 social groups salient in the Chinese context. To complement controlled experiments, we further analyze Chinese-language conversations from a corpus of real interactions between users and chatbots. Across models, we observe systematic ingroup-positive and outgroup-negative tendencies, which are not confined to synthetic prompts but also appear in naturalistic dialogue, indicating that bias dynamics might strengthen in real interactions. Our study provides a language-aware evaluation framework for Chinese LLMs, demonstrating that social identity biases documented in English generalize cross-linguistically and intensify in user-facing contexts.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Reliable Retrieval in RAG Systems for Large Legal Datasets</title>
<link>https://arxiv.org/abs/2510.06999</link>
<guid>https://arxiv.org/abs/2510.06999</guid>
<content:encoded><![CDATA[
arXiv:2510.06999v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) is a promising approach to mitigate hallucinations in Large Language Models (LLMs) for legal applications, but its reliability is critically dependent on the accuracy of the retrieval step. This is particularly challenging in the legal domain, where large databases of structurally similar documents often cause retrieval systems to fail. In this paper, we address this challenge by first identifying and quantifying a critical failure mode we term Document-Level Retrieval Mismatch (DRM), where the retriever selects information from entirely incorrect source documents. To mitigate DRM, we investigate a simple and computationally efficient technique which we refer to as Summary-Augmented Chunking (SAC). This method enhances each text chunk with a document-level synthetic summary, thereby injecting crucial global context that would otherwise be lost during a standard chunking process. Our experiments on a diverse set of legal information retrieval tasks show that SAC greatly reduces DRM and, consequently, also improves text-level retrieval precision and recall. Interestingly, we find that a generic summarization strategy outperforms an approach that incorporates legal expert domain knowledge to target specific legal elements. Our work provides evidence that this practical, scalable, and easily integrable technique enhances the reliability of RAG systems when applied to large-scale legal document datasets.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pragyaan: Designing and Curating High-Quality Cultural Post-Training Datasets for Indian Languages</title>
<link>https://arxiv.org/abs/2510.07000</link>
<guid>https://arxiv.org/abs/2510.07000</guid>
<content:encoded><![CDATA[
arXiv:2510.07000v1 Announce Type: new 
Abstract: The effectiveness of Large Language Models (LLMs) depends heavily on the availability of high-quality post-training data, particularly instruction-tuning and preference-based examples. Existing open-source datasets, however, often lack multilingual coverage, cultural grounding, and suffer from task diversity gaps that are especially pronounced for Indian languages. We introduce a human-in-the-loop pipeline that combines translations with synthetic expansion to produce reliable and diverse Indic post-training data. Using this pipeline, we curate two datasets: Pragyaan-IT (22.5K) and Pragyaan-Align (100K) across 10 Indian languages covering 13 broad and 56 sub-categories, leveraging 57 diverse datasets. Our dataset protocol incorporates several often-overlooked dimensions and emphasize task diversity, multi-turn dialogue, instruction fidelity, safety alignment, and preservation of cultural nuance, providing a foundation for more inclusive and effective multilingual LLMs.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Native Hybrid Attention for Efficient Sequence Modeling</title>
<link>https://arxiv.org/abs/2510.07019</link>
<guid>https://arxiv.org/abs/2510.07019</guid>
<content:encoded><![CDATA[
arXiv:2510.07019v1 Announce Type: new 
Abstract: Transformers excel at sequence modeling but face quadratic complexity, while linear attention offers improved efficiency but often compromises recall accuracy over long contexts. In this work, we introduce Native Hybrid Attention (NHA), a novel hybrid architecture of linear and full attention that integrates both intra \& inter-layer hybridization into a unified layer design. NHA maintains long-term context in key-value slots updated by a linear RNN, and augments them with short-term tokens from a sliding window. A single \texttt{softmax attention} operation is then applied over all keys and values, enabling per-token and per-head context-dependent weighting without requiring additional fusion parameters. The inter-layer behavior is controlled through a single hyperparameter, the sliding window size, which allows smooth adjustment between purely linear and full attention while keeping all layers structurally uniform. Experimental results show that NHA surpasses Transformers and other hybrid baselines on recall-intensive and commonsense reasoning tasks. Furthermore, pretrained LLMs can be structurally hybridized with NHA, achieving competitive accuracy while delivering significant efficiency gains. Code is available at https://github.com/JusenD/NHA.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mining the Mind: What 100M Beliefs Reveal About Frontier LLM Knowledge</title>
<link>https://arxiv.org/abs/2510.07024</link>
<guid>https://arxiv.org/abs/2510.07024</guid>
<content:encoded><![CDATA[
arXiv:2510.07024v1 Announce Type: new 
Abstract: LLMs are remarkable artifacts that have revolutionized a range of NLP and AI tasks. A significant contributor is their factual knowledge, which, to date, remains poorly understood, and is usually analyzed from biased samples. In this paper, we take a deep tour into the factual knowledge (or beliefs) of a frontier LLM, based on GPTKB v1.5 (Hu et al., 2025a), a recursively elicited set of 100 million beliefs of one of the strongest currently available frontier LLMs, GPT-4.1. We find that the models' factual knowledge differs quite significantly from established knowledge bases, and that its accuracy is significantly lower than indicated by previous benchmarks. We also find that inconsistency, ambiguity and hallucinations are major issues, shedding light on future research opportunities concerning factual LLM knowledge.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era of Large Language Models</title>
<link>https://arxiv.org/abs/2510.07037</link>
<guid>https://arxiv.org/abs/2510.07037</guid>
<content:encoded><![CDATA[
arXiv:2510.07037v1 Announce Type: new 
Abstract: Code-switching (CSW), the alternation of languages and scripts within a single utterance, remains a fundamental challenge for multiling ual NLP, even amidst the rapid advances of large language models (LLMs). Most LLMs still struggle with mixed-language inputs, limited CSW datasets, and evaluation biases, hindering deployment in multilingual societies. This survey provides the first comprehensive analysis of CSW-aware LLM research, reviewing \total{unique_references} studies spanning five research areas, 12 NLP tasks, 30+ datasets, and 80+ languages. We classify recent advances by architecture, training strategy, and evaluation methodology, outlining how LLMs have reshaped CSW modeling and what challenges persist. The paper concludes with a roadmap emphasizing the need for inclusive datasets, fair evaluation, and linguistically grounded models to achieve truly multilingual intelligence. A curated collection of all resources is maintained at https://github.com/lingo-iitgn/awesome-code-mixing/.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models</title>
<link>https://arxiv.org/abs/2510.07048</link>
<guid>https://arxiv.org/abs/2510.07048</guid>
<content:encoded><![CDATA[
arXiv:2510.07048v1 Announce Type: new 
Abstract: Despite their remarkable natural language understanding capabilities, Large Language Models (LLMs) have been underutilized for retrieval tasks. We present Search-R3, a novel framework that addresses this limitation by adapting LLMs to generate search embeddings as a direct output of their reasoning process. Our approach exploits LLMs' chain-of-thought capabilities, allowing them to produce more effective embeddings by reasoning step-by-step through complex semantic analyses. We implement this through three complementary mechanisms. (1) a supervised learning stage enables the model's ability to produce quality embeddings, (2) a reinforcement learning (RL) methodology that optimizes embedding generation alongside reasoning, and (3) a specialized RL environment that efficiently handles evolving embedding representations without requiring complete corpus re-encoding at each training iteration. Our extensive evaluations on diverse benchmarks demonstrate that Search-R3 significantly outperforms prior methods by unifying the reasoning and embedding generation processes. This integrated post-training approach represents a substantial advancement in handling complex knowledge-intensive tasks that require both sophisticated reasoning and effective information retrieval. Project page: https://github.com/ytgui/Search-R3
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Local News Stay Local?: Online Content Shifts in Sinclair-Acquired Stations</title>
<link>https://arxiv.org/abs/2510.07060</link>
<guid>https://arxiv.org/abs/2510.07060</guid>
<content:encoded><![CDATA[
arXiv:2510.07060v1 Announce Type: new 
Abstract: Local news stations are often considered to be reliable sources of non-politicized information, particularly local concerns that residents care about. Because these stations are trusted news sources, viewers are particularly susceptible to the information they report. The Sinclair Broadcast group is a broadcasting company that has acquired many local news stations in the last decade. We investigate the effects of local news stations being acquired by Sinclair: how does coverage change? We use computational methods to investigate changes in internet content put out by local news stations before and after being acquired by Sinclair and in comparison to national news outlets. We find that there is clear evidence that local news stations report more frequently on national news at the expense of local topics, and that their coverage of polarizing national topics increases.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Metric Reliability for Fine-grained Evaluation of Machine Translation and Summarization in Indian Languages</title>
<link>https://arxiv.org/abs/2510.07061</link>
<guid>https://arxiv.org/abs/2510.07061</guid>
<content:encoded><![CDATA[
arXiv:2510.07061v1 Announce Type: new 
Abstract: While automatic metrics drive progress in Machine Translation (MT) and Text Summarization (TS), existing metrics have been developed and validated almost exclusively for English and other high-resource languages. This narrow focus leaves Indian languages, spoken by over 1.5 billion people, largely overlooked, casting doubt on the universality of current evaluation practices. To address this gap, we introduce ITEM, a large-scale benchmark that systematically evaluates the alignment of 26 automatic metrics with human judgments across six major Indian languages, enriched with fine-grained annotations. Our extensive evaluation, covering agreement with human judgments, sensitivity to outliers, language-specific reliability, inter-metric correlations, and resilience to controlled perturbations, reveals four central findings: (1) LLM-based evaluators show the strongest alignment with human judgments at both segment and system levels; (2) outliers exert a significant impact on metric-human agreement; (3) in TS, metrics are more effective at capturing content fidelity, whereas in MT, they better reflect fluency; and (4) metrics differ in their robustness and sensitivity when subjected to diverse perturbations. Collectively, these findings offer critical guidance for advancing metric design and evaluation in Indian languages.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LuxInstruct: A Cross-Lingual Instruction Tuning Dataset For Luxembourgish</title>
<link>https://arxiv.org/abs/2510.07074</link>
<guid>https://arxiv.org/abs/2510.07074</guid>
<content:encoded><![CDATA[
arXiv:2510.07074v1 Announce Type: new 
Abstract: Instruction tuning has become a key technique for enhancing the performance of large language models, enabling them to better follow human prompts. However, low-resource languages such as Luxembourgish face severe limitations due to the lack of high-quality instruction datasets. Traditional reliance on machine translation often introduces semantic misalignment and cultural inaccuracies. In this work, we address these challenges by creating a cross-lingual instruction tuning dataset for Luxembourgish, without resorting to machine-generated translations into it. Instead, by leveraging aligned data from English, French, and German, we build a high-quality dataset that preserves linguistic and cultural nuances. We provide evidence that cross-lingual instruction tuning not only improves representational alignment across languages but also the model's generative capabilities in Luxembourgish. This highlights how cross-lingual data curation can avoid the common pitfalls of machine-translated data and directly benefit low-resource language development.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Diffusion LLM Inference via Local Determinism Propagation</title>
<link>https://arxiv.org/abs/2510.07081</link>
<guid>https://arxiv.org/abs/2510.07081</guid>
<content:encoded><![CDATA[
arXiv:2510.07081v1 Announce Type: new 
Abstract: Diffusion large language models (dLLMs) represent a significant advancement in text generation, offering parallel token decoding capabilities. However, existing open-source implementations suffer from quality-speed trade-offs that impede their practical deployment. Conservative sampling strategies typically decode only the most confident token per step to ensure quality (i.e., greedy decoding), at the cost of inference efficiency due to repeated redundant refinement iterations--a phenomenon we term delayed decoding. Through systematic analysis of dLLM decoding dynamics, we characterize this delayed decoding behavior and propose a training-free adaptive parallel decoding strategy, named LocalLeap, to address these inefficiencies. LocalLeap is built on two fundamental empirical principles: local determinism propagation centered on high-confidence anchors and progressive spatial consistency decay. By applying these principles, LocalLeap identifies anchors and performs localized relaxed parallel decoding within bounded neighborhoods, achieving substantial inference step reduction through early commitment of already-determined tokens without compromising output quality. Comprehensive evaluation on various benchmarks demonstrates that LocalLeap achieves 6.94$\times$ throughput improvements and reduces decoding steps to just 14.2\% of the original requirement, achieving these gains with negligible performance impact. The source codes are available at: https://github.com/friedrichor/LocalLeap.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>All Claims Are Equal, but Some Claims Are More Equal Than Others: Importance-Sensitive Factuality Evaluation of LLM Generations</title>
<link>https://arxiv.org/abs/2510.07083</link>
<guid>https://arxiv.org/abs/2510.07083</guid>
<content:encoded><![CDATA[
arXiv:2510.07083v1 Announce Type: new 
Abstract: Existing methods for evaluating the factuality of large language model (LLM) responses treat all claims as equally important. This results in misleading evaluations when vital information is missing or incorrect as it receives the same weight as peripheral details, raising the question: how can we reliably detect such differences when there are errors in key information? Current approaches that measure factuality tend to be insensitive to omitted or false key information. To investigate this lack of sensitivity, we construct VITALERRORS, a benchmark of 6,733 queries with minimally altered LLM responses designed to omit or falsify key information. Using this dataset, we demonstrate the insensitivities of existing evaluation metrics to key information errors. To address this gap, we introduce VITAL, a set of metrics that provide greater sensitivity in measuring the factuality of responses by incorporating the relevance and importance of claims with respect to the query. Our analysis demonstrates that VITAL metrics more reliably detect errors in key information than previous methods. Our dataset, metrics, and analysis provide a foundation for more accurate and robust assessment of LLM factuality.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Making Machines Sound Sarcastic: LLM-Enhanced and Retrieval-Guided Sarcastic Speech Synthesis</title>
<link>https://arxiv.org/abs/2510.07096</link>
<guid>https://arxiv.org/abs/2510.07096</guid>
<content:encoded><![CDATA[
arXiv:2510.07096v1 Announce Type: new 
Abstract: Sarcasm is a subtle form of non-literal language that poses significant challenges for speech synthesis due to its reliance on nuanced semantic, contextual, and prosodic cues. While existing speech synthesis research has focused primarily on broad emotional categories, sarcasm remains largely unexplored. In this paper, we propose a Large Language Model (LLM)-enhanced Retrieval-Augmented framework for sarcasm-aware speech synthesis. Our approach combines (1) semantic embeddings from a LoRA-fine-tuned LLaMA 3, which capture pragmatic incongruity and discourse-level cues of sarcasm, and (2) prosodic exemplars retrieved via a Retrieval Augmented Generation (RAG) module, which provide expressive reference patterns of sarcastic delivery. Integrated within a VITS backbone, this dual conditioning enables more natural and contextually appropriate sarcastic speech. Experiments demonstrate that our method outperforms baselines in both objective measures and subjective evaluations, yielding improvements in speech naturalness, sarcastic expressivity, and downstream sarcasm detection.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TALENT: Table VQA via Augmented Language-Enhanced Natural-text Transcription</title>
<link>https://arxiv.org/abs/2510.07098</link>
<guid>https://arxiv.org/abs/2510.07098</guid>
<content:encoded><![CDATA[
arXiv:2510.07098v1 Announce Type: new 
Abstract: Table Visual Question Answering (Table VQA) is typically addressed by large vision-language models (VLMs). While such models can answer directly from images, they often miss fine-grained details unless scaled to very large sizes, which are computationally prohibitive, especially for mobile deployment. A lighter alternative is to have a small VLM perform OCR and then use a large language model (LLM) to reason over structured outputs such as Markdown tables. However, these representations are not naturally optimized for LLMs and still introduce substantial errors. We propose TALENT (Table VQA via Augmented Language-Enhanced Natural-text Transcription), a lightweight framework that leverages dual representations of tables. TALENT prompts a small VLM to produce both OCR text and natural language narration, then combines them with the question for reasoning by an LLM. This reframes Table VQA as an LLM-centric multimodal reasoning task, where the VLM serves as a perception-narration module rather than a monolithic solver. Additionally, we construct ReTabVQA, a more challenging Table VQA dataset requiring multi-step quantitative reasoning over table images. Experiments show that TALENT enables a small VLM-LLM combination to match or surpass a single large VLM at significantly lower computational cost on both public datasets and ReTabVQA.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opt-ICL at LeWiDi-2025: Maximizing In-Context Signal from Rater Examples via Meta-Learning</title>
<link>https://arxiv.org/abs/2510.07105</link>
<guid>https://arxiv.org/abs/2510.07105</guid>
<content:encoded><![CDATA[
arXiv:2510.07105v1 Announce Type: new 
Abstract: Many natural language processing (NLP) tasks involve subjectivity, ambiguity, or legitimate disagreement between annotators. In this paper, we outline our system for modeling human variation. Our system leverages language models' (LLMs) in-context learning abilities, along with a two-step meta-learning training procedure for 1) post-training on many datasets requiring in-context learning and 2) specializing the model via in-context meta-learning to the particular data distribution of interest. We also evaluate the performance of our system submission to the Learning With Disagreements (LeWiDi) competition, where it was the overall winner on both tasks. Additionally, we perform an ablation study to measure the importance of each system component. We find that including rater examples in-context is crucial for our system's performance, dataset-specific fine-tuning is helpful on the larger datasets, post-training on other in-context datasets is helpful on one of the competition datasets, and that performance improves with model scale.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRIM: Token-wise Attention-Derived Saliency for Data-Efficient Instruction Tuning</title>
<link>https://arxiv.org/abs/2510.07118</link>
<guid>https://arxiv.org/abs/2510.07118</guid>
<content:encoded><![CDATA[
arXiv:2510.07118v1 Announce Type: new 
Abstract: Instruction tuning is essential for aligning large language models (LLMs) to downstream tasks and commonly relies on large, diverse corpora. However, small, high-quality subsets, known as coresets, can deliver comparable or superior results, though curating them remains challenging. Existing methods often rely on coarse, sample-level signals like gradients, an approach that is computationally expensive and overlooks fine-grained features. To address this, we introduce TRIM (Token Relevance via Interpretable Multi-layer Attention), a forward-only, token-centric framework. Instead of using gradients, TRIM operates by matching underlying representational patterns identified via attention-based "fingerprints" from a handful of target samples. Such an approach makes TRIM highly efficient and uniquely sensitive to the structural features that define a task. Coresets selected by our method consistently outperform state-of-the-art baselines by up to 9% on downstream tasks and even surpass the performance of full-data fine-tuning in some settings. By avoiding expensive backward passes, TRIM achieves this at a fraction of the computational cost. These findings establish TRIM as a scalable and efficient alternative for building high-quality instruction-tuning datasets.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing human and language models sentence processing difficulties on complex structures</title>
<link>https://arxiv.org/abs/2510.07141</link>
<guid>https://arxiv.org/abs/2510.07141</guid>
<content:encoded><![CDATA[
arXiv:2510.07141v1 Announce Type: new 
Abstract: Large language models (LLMs) that fluently converse with humans are a reality - but do LLMs experience human-like processing difficulties? We systematically compare human and LLM sentence comprehension across seven challenging linguistic structures. We collect sentence comprehension data from humans and five families of state-of-the-art LLMs, varying in size and training procedure in a unified experimental framework. Our results show LLMs overall struggle on the target structures, but especially on garden path (GP) sentences. Indeed, while the strongest models achieve near perfect accuracy on non-GP structures (93.7% for GPT-5), they struggle on GP structures (46.8% for GPT-5). Additionally, when ranking structures based on average performance, rank correlation between humans and models increases with parameter count. For each target structure, we also collect data for their matched baseline without the difficult structure. Comparing performance on the target vs. baseline sentences, the performance gap observed in humans holds for LLMs, with two exceptions: for models that are too weak performance is uniformly low across both sentence types, and for models that are too strong the performance is uniformly high. Together, these reveal convergence and divergence in human and LLM sentence comprehension, offering new insights into the similarity of humans and LLMs.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning for Hierarchical Text Classification: The Case of Patents</title>
<link>https://arxiv.org/abs/2510.07167</link>
<guid>https://arxiv.org/abs/2510.07167</guid>
<content:encoded><![CDATA[
arXiv:2510.07167v1 Announce Type: new 
Abstract: Hierarchical text classification (HTC) assigns documents to multiple levels of a pre-defined taxonomy. Automated patent subject classification represents one of the hardest HTC scenarios because of domain knowledge difficulty and a huge number of labels. Prior approaches only output a flat label set, which offers little insight into the reason behind predictions. Therefore, we propose Reasoning for Hierarchical Classification (RHC), a novel framework that reformulates HTC as a step-by-step reasoning task to sequentially deduce hierarchical labels. RHC trains large language models (LLMs) in two stages: a cold-start stage that aligns outputs with chain-of-thought (CoT) reasoning format and a reinforcement learning (RL) stage to enhance multi-step reasoning ability. RHC demonstrates four advantages in our experiments. (1) Effectiveness: RHC surpasses previous baselines and outperforms the supervised fine-tuning counterparts by approximately 3% in accuracy and macro F1. (2) Explainability: RHC produces natural-language justifications before prediction to facilitate human inspection. (3) Scalability: RHC scales favorably with model size with larger gains compared to standard fine-tuning. (4) Applicability: Beyond patents, we further demonstrate that RHC achieves state-of-the-art performance on other widely used HTC benchmarks, which highlights its broad applicability.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>More Data or Better Data? A Critical Analysis of Data Selection and Synthesis for Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2510.07169</link>
<guid>https://arxiv.org/abs/2510.07169</guid>
<content:encoded><![CDATA[
arXiv:2510.07169v1 Announce Type: new 
Abstract: The reasoning capabilities of Large Language Models (LLMs) play a critical role in many downstream tasks, yet depend strongly on the quality of training data. Despite various proposed data construction methods, their practical utility in real-world pipelines remains underexplored. In this work, we conduct a comprehensive analysis of open-source datasets and data synthesis techniques for mathematical reasoning, evaluating them under a unified pipeline designed to mirror training and deployment scenarios. We further distill effective data selection strategies and identify practical methods suitable for industrial applications. Our findings highlight that structuring data in more interpretable formats, or distilling from stronger models often outweighs simply scaling up data volume. This study provides actionable guidance for integrating training data to enhance LLM capabilities, supporting both cost-effective data curation and scalable model enhancement. We hope this work will inspire further research on how to balance "more data" versus "better data" for real-world reasoning tasks.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NurseLLM: The First Specialized Language Model for Nursing</title>
<link>https://arxiv.org/abs/2510.07173</link>
<guid>https://arxiv.org/abs/2510.07173</guid>
<content:encoded><![CDATA[
arXiv:2510.07173v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) have significantly transformed medical systems. However, their potential within specialized domains such as nursing remains largely underexplored. In this work, we introduce NurseLLM, the first nursing-specialized LLM tailored for multiple choice question-answering (MCQ) tasks. We develop a multi-stage data generation pipeline to build the first large scale nursing MCQ dataset to train LLMs on a broad spectrum of nursing topics. We further introduce multiple nursing benchmarks to enable rigorous evaluation. Our extensive experiments demonstrate that NurseLLM outperforms SoTA general-purpose and medical-specialized LLMs of comparable size on different benchmarks, underscoring the importance of a specialized LLM for the nursing domain. Finally, we explore the role of reasoning and multi-agent collaboration systems in nursing, highlighting their promise for future research and applications.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Data Contamination in Psychometric Evaluations of LLMs</title>
<link>https://arxiv.org/abs/2510.07175</link>
<guid>https://arxiv.org/abs/2510.07175</guid>
<content:encoded><![CDATA[
arXiv:2510.07175v1 Announce Type: new 
Abstract: Recent studies apply psychometric questionnaires to Large Language Models (LLMs) to assess high-level psychological constructs such as values, personality, moral foundations, and dark traits. Although prior work has raised concerns about possible data contamination from psychometric inventories, which may threaten the reliability of such evaluations, there has been no systematic attempt to quantify the extent of this contamination. To address this gap, we propose a framework to systematically measure data contamination in psychometric evaluations of LLMs, evaluating three aspects: (1) item memorization, (2) evaluation memorization, and (3) target score matching. Applying this framework to 21 models from major families and four widely used psychometric inventories, we provide evidence that popular inventories such as the Big Five Inventory (BFI-44) and Portrait Values Questionnaire (PVQ-40) exhibit strong contamination, where models not only memorize items but can also adjust their responses to achieve specific target scores.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CARPAS: Towards Content-Aware Refinement of Provided Aspects for Summarization in Large Language Models</title>
<link>https://arxiv.org/abs/2510.07177</link>
<guid>https://arxiv.org/abs/2510.07177</guid>
<content:encoded><![CDATA[
arXiv:2510.07177v1 Announce Type: new 
Abstract: Aspect-based summarization has attracted significant attention for its ability to generate more fine-grained and user-aligned summaries. While most existing approaches assume a set of predefined aspects as input, real-world scenarios often present challenges where these given aspects may be incomplete, irrelevant, or entirely missing from the document. Users frequently expect systems to adaptively refine or filter the provided aspects based on the actual content. In this paper, we initiate this novel task setting, termed Content-Aware Refinement of Provided Aspects for Summarization (CARPAS), with the aim of dynamically adjusting the provided aspects based on the document context before summarizing. We construct three new datasets to facilitate our pilot experiments, and by using LLMs with four representative prompting strategies in this task, we find that LLMs tend to predict an overly comprehensive set of aspects, which often results in excessively long and misaligned summaries. Building on this observation, we propose a preliminary subtask to predict the number of relevant aspects, and demonstrate that the predicted number can serve as effective guidance for the LLMs, reducing the inference difficulty, and enabling them to focus on the most pertinent aspects. Our extensive experiments show that the proposed approach significantly improves performance across all datasets. Moreover, our deeper analyses uncover LLMs' compliance when the requested number of aspects differs from their own estimations, establishing a crucial insight for the deployment of LLMs in similar real-world applications.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Biasless Language Models Learn Unnaturally: How LLMs Fail to Distinguish the Possible from the Impossible</title>
<link>https://arxiv.org/abs/2510.07178</link>
<guid>https://arxiv.org/abs/2510.07178</guid>
<content:encoded><![CDATA[
arXiv:2510.07178v1 Announce Type: new 
Abstract: Are large language models (LLMs) sensitive to the distinction between humanly possible languages and humanly impossible languages? This question is taken by many to bear on whether LLMs and humans share the same innate learning biases. Previous work has attempted to answer it in the positive by comparing LLM learning curves on existing language datasets and on "impossible" datasets derived from them via various perturbation functions. Using the same methodology, we examine this claim on a wider set of languages and impossible perturbations. We find that in most cases, GPT-2 learns each language and its impossible counterpart equally easily, in contrast to previous claims. We also apply a more lenient condition by testing whether GPT-2 provides any kind of separation between the whole set of natural languages and the whole set of impossible languages. By considering cross-linguistic variance in various metrics computed on the perplexity curves, we show that GPT-2 provides no systematic separation between the possible and the impossible. Taken together, these perspectives show that LLMs do not share the human innate biases that shape linguistic typology.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sunflower: A New Approach To Expanding Coverage of African Languages in Large Language Models</title>
<link>https://arxiv.org/abs/2510.07203</link>
<guid>https://arxiv.org/abs/2510.07203</guid>
<content:encoded><![CDATA[
arXiv:2510.07203v1 Announce Type: new 
Abstract: There are more than 2000 living languages in Africa, most of which have been bypassed by advances in language technology. Current leading LLMs exhibit strong performance on a number of the most common languages (e.g. Swahili or Yoruba), but prioritise support for the languages with the most speakers first, resulting in piecemeal ability across disparate languages. We contend that a regionally focussed approach is more efficient, and present a case study for Uganda, a country with high linguistic diversity. We describe the development of Sunflower 14B and 32B, a pair of models based on Qwen 3 with state of the art comprehension in the majority of all Ugandan languages. These models are open source and can be used to reduce language barriers in a number of important practical applications.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Lives in Sparse Dimensions: Toward Interpretable and Efficient Multilingual Control for Large Language Models</title>
<link>https://arxiv.org/abs/2510.07213</link>
<guid>https://arxiv.org/abs/2510.07213</guid>
<content:encoded><![CDATA[
arXiv:2510.07213v1 Announce Type: new 
Abstract: Large language models exhibit strong multilingual capabilities despite limited exposure to non-English data. Prior studies show that English-centric large language models map multilingual content into English-aligned representations at intermediate layers and then project them back into target-language token spaces in the final layer. From this observation, we hypothesize that this cross-lingual transition is governed by a small and sparse set of dimensions, which occur at consistent indices across the intermediate to final layers. Building on this insight, we introduce a simple, training-free method to identify and manipulate these dimensions, requiring only as few as 50 sentences of either parallel or monolingual data. Experiments on a multilingual generation control task reveal the interpretability of these dimensions, demonstrating that the interventions in these dimensions can switch the output language while preserving semantic content, and that it surpasses the performance of prior neuron-based approaches at a substantially lower cost.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How much speech data is necessary for ASR in African languages? An evaluation of data scaling in Kinyarwanda and Kikuyu</title>
<link>https://arxiv.org/abs/2510.07221</link>
<guid>https://arxiv.org/abs/2510.07221</guid>
<content:encoded><![CDATA[
arXiv:2510.07221v1 Announce Type: new 
Abstract: The development of Automatic Speech Recognition (ASR) systems for low-resource African languages remains challenging due to limited transcribed speech data. While recent advances in large multilingual models like OpenAI's Whisper offer promising pathways for low-resource ASR development, critical questions persist regarding practical deployment requirements. This paper addresses two fundamental concerns for practitioners: determining the minimum data volumes needed for viable performance and characterizing the primary failure modes that emerge in production systems. We evaluate Whisper's performance through comprehensive experiments on two Bantu languages: systematic data scaling analysis on Kinyarwanda using training sets from 1 to 1,400 hours, and detailed error characterization on Kikuyu using 270 hours of training data. Our scaling experiments demonstrate that practical ASR performance (WER < 13\%) becomes achievable with as little as 50 hours of training data, with substantial improvements continuing through 200 hours (WER < 10\%). Complementing these volume-focused findings, our error analysis reveals that data quality issues, particularly noisy ground truth transcriptions, account for 38.6\% of high-error cases, indicating that careful data curation is as critical as data volume for robust system performance. These results provide actionable benchmarks and deployment guidance for teams developing ASR systems across similar low-resource language contexts. We release accompanying and models see https://github.com/SunbirdAI/kinyarwanda-whisper-eval
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Where to Begin: Efficient Pretraining via Subnetwork Selection and Distillation</title>
<link>https://arxiv.org/abs/2510.07227</link>
<guid>https://arxiv.org/abs/2510.07227</guid>
<content:encoded><![CDATA[
arXiv:2510.07227v1 Announce Type: new 
Abstract: Small Language models (SLMs) offer an efficient and accessible alternative to Large Language Models (LLMs), delivering strong performance while using far fewer resources. We introduce a simple and effective framework for pretraining SLMs that brings together three complementary ideas. First, we identify structurally sparse sub-network initializations that consistently outperform randomly initialized models of similar size under the same compute budget. Second, we use evolutionary search to automatically discover high-quality sub-network initializations, providing better starting points for pretraining. Third, we apply knowledge distillation from larger teacher models to speed up training and improve generalization. Together, these components make SLM pretraining substantially more efficient: our best model, discovered using evolutionary search and initialized with LLM weights, matches the validation perplexity of a comparable Pythia SLM while requiring 9.2x fewer pretraining tokens. We release all code and models at https://github.com/whittle-org/whittle/, offering a practical and reproducible path toward cost-efficient small language model development at scale.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Customer-R1: Personalized Simulation of Human Behaviors via RL-based LLM Agent in Online Shopping</title>
<link>https://arxiv.org/abs/2510.07230</link>
<guid>https://arxiv.org/abs/2510.07230</guid>
<content:encoded><![CDATA[
arXiv:2510.07230v1 Announce Type: new 
Abstract: Simulating step-wise human behavior with Large Language Models (LLMs) has become an emerging research direction, enabling applications in various practical domains. While prior methods, including prompting, supervised fine-tuning (SFT), and reinforcement learning (RL), have shown promise in modeling step-wise behavior, they primarily learn a population-level policy without conditioning on a user's persona, yielding generic rather than personalized simulations. In this work, we pose a critical question: how can LLM agents better simulate personalized user behavior? We introduce Customer-R1, an RL-based method for personalized, step-wise user behavior simulation in online shopping environments. Our policy is conditioned on an explicit persona, and we optimize next-step rationale and action generation via action correctness reward signals. Experiments on the OPeRA dataset emonstrate that Customer-R1 not only significantly outperforms prompting and SFT-based baselines in next-action prediction tasks, but also better matches users' action distribution, indicating higher fidelity in personalized behavior simulation.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking LLM Causal Reasoning with Scientifically Validated Relationships</title>
<link>https://arxiv.org/abs/2510.07231</link>
<guid>https://arxiv.org/abs/2510.07231</guid>
<content:encoded><![CDATA[
arXiv:2510.07231v1 Announce Type: new 
Abstract: Causal reasoning is fundamental for Large Language Models (LLMs) to understand genuine cause-and-effect relationships beyond pattern matching. Existing benchmarks suffer from critical limitations such as reliance on synthetic data and narrow domain coverage. We introduce a novel benchmark constructed from casually identified relationships extracted from top-tier economics and finance journals, drawing on rigorous methodologies including instrumental variables, difference-in-differences, and regression discontinuity designs. Our benchmark comprises 40,379 evaluation items covering five task types across domains such as health, environment, technology, law, and culture. Experimental results on eight state-of-the-art LLMs reveal substantial limitations, with the best model achieving only 57.6\% accuracy. Moreover, model scale does not consistently translate to superior performance, and even advanced reasoning models struggle with fundamental causal relationship identification. These findings underscore a critical gap between current LLM capabilities and demands of reliable causal reasoning in high-stakes applications.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LAD-RAG: Layout-aware Dynamic RAG for Visually-Rich Document Understanding</title>
<link>https://arxiv.org/abs/2510.07233</link>
<guid>https://arxiv.org/abs/2510.07233</guid>
<content:encoded><![CDATA[
arXiv:2510.07233v1 Announce Type: new 
Abstract: Question answering over visually rich documents (VRDs) requires reasoning not only over isolated content but also over documents' structural organization and cross-page dependencies. However, conventional retrieval-augmented generation (RAG) methods encode content in isolated chunks during ingestion, losing structural and cross-page dependencies, and retrieve a fixed number of pages at inference, regardless of the specific demands of the question or context. This often results in incomplete evidence retrieval and degraded answer quality for multi-page reasoning tasks. To address these limitations, we propose LAD-RAG, a novel Layout-Aware Dynamic RAG framework. During ingestion, LAD-RAG constructs a symbolic document graph that captures layout structure and cross-page dependencies, adding it alongside standard neural embeddings to yield a more holistic representation of the document. During inference, an LLM agent dynamically interacts with the neural and symbolic indices to adaptively retrieve the necessary evidence based on the query. Experiments on MMLongBench-Doc, LongDocURL, DUDE, and MP-DocVQA demonstrate that LAD-RAG improves retrieval, achieving over 90% perfect recall on average without any top-k tuning, and outperforming baseline retrievers by up to 20% in recall at comparable noise levels, yielding higher QA accuracy with minimal latency.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Benchmarks Age: Temporal Misalignment through Large Language Model Factuality Evaluation</title>
<link>https://arxiv.org/abs/2510.07238</link>
<guid>https://arxiv.org/abs/2510.07238</guid>
<content:encoded><![CDATA[
arXiv:2510.07238v1 Announce Type: new 
Abstract: The rapid evolution of large language models (LLMs) and the real world has outpaced the static nature of widely used evaluation benchmarks, raising concerns about their reliability for evaluating LLM factuality. While substantial works continue to rely on the popular but old benchmarks, their temporal misalignment with real-world facts and modern LLMs, and their effects on LLM factuality evaluation remain underexplored. Therefore, in this work, we present a systematic investigation of this issue by examining five popular factuality benchmarks and eight LLMs released across different years. An up-to-date fact retrieval pipeline and three metrics are tailored to quantify benchmark aging and its impact on LLM factuality evaluation. Experimental results and analysis illustrate that a considerable portion of samples in the widely used factuality benchmarks are outdated, leading to unreliable assessments of LLM factuality. We hope our work can provide a testbed to assess the reliability of a benchmark for LLM factuality evaluation and inspire more research on the benchmark aging issue. Codes are available in https://github.com/JiangXunyi/BenchAge.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Red-Bandit: Test-Time Adaptation for LLM Red-Teaming via Bandit-Guided LoRA Experts</title>
<link>https://arxiv.org/abs/2510.07239</link>
<guid>https://arxiv.org/abs/2510.07239</guid>
<content:encoded><![CDATA[
arXiv:2510.07239v1 Announce Type: new 
Abstract: Automated red-teaming has emerged as a scalable approach for auditing Large Language Models (LLMs) prior to deployment, yet existing approaches lack mechanisms to efficiently adapt to model-specific vulnerabilities at inference. We introduce Red-Bandit, a red-teaming framework that adapts online to identify and exploit model failure modes under distinct attack styles (e.g., manipulation, slang). Red-Bandit post-trains a set of parameter-efficient LoRA experts, each specialized for a particular attack style, using reinforcement learning that rewards the generation of unsafe prompts via a rule-based safety model. At inference, a multi-armed bandit policy dynamically selects among these attack-style experts based on the target model's response safety, balancing exploration and exploitation. Red-Bandit achieves state-of-the-art results on AdvBench under sufficient exploration (ASR@10), while producing more human-readable prompts (lower perplexity). Moreover, Red-Bandit's bandit policy serves as a diagnostic tool for uncovering model-specific vulnerabilities by indicating which attack styles most effectively elicit unsafe behaviors.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense</title>
<link>https://arxiv.org/abs/2510.07242</link>
<guid>https://arxiv.org/abs/2510.07242</guid>
<content:encoded><![CDATA[
arXiv:2510.07242v1 Announce Type: new 
Abstract: Post-training for reasoning of large language models (LLMs) increasingly relies on verifiable rewards: deterministic checkers that provide 0-1 correctness signals. While reliable, such binary feedback is brittle--many tasks admit partially correct or alternative answers that verifiers under-credit, and the resulting all-or-nothing supervision limits learning. Reward models offer richer, continuous feedback, which can serve as a complementary supervisory signal to verifiers. We introduce HERO (Hybrid Ensemble Reward Optimization), a reinforcement learning framework that integrates verifier signals with reward-model scores in a structured way. HERO employs stratified normalization to bound reward-model scores within verifier-defined groups, preserving correctness while refining quality distinctions, and variance-aware weighting to emphasize challenging prompts where dense signals matter most. Across diverse mathematical reasoning benchmarks, HERO consistently outperforms RM-only and verifier-only baselines, with strong gains on both verifiable and hard-to-verify tasks. Our results show that hybrid reward design retains the stability of verifiers while leveraging the nuance of reward models to advance reasoning.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LeMAJ (Legal LLM-as-a-Judge): Bridging Legal Reasoning and LLM Evaluation</title>
<link>https://arxiv.org/abs/2510.07243</link>
<guid>https://arxiv.org/abs/2510.07243</guid>
<content:encoded><![CDATA[
arXiv:2510.07243v1 Announce Type: new 
Abstract: Evaluating large language model (LLM) outputs in the legal domain presents unique challenges due to the complex and nuanced nature of legal analysis. Current evaluation approaches either depend on reference data, which is costly to produce, or use standardized assessment methods, both of which have significant limitations for legal applications.
  Although LLM-as-a-Judge has emerged as a promising evaluation technique, its reliability and effectiveness in legal contexts depend heavily on evaluation processes unique to the legal industry and how trustworthy the evaluation appears to the human legal expert. This is where existing evaluation methods currently fail and exhibit considerable variability.
  This paper aims to close the gap: a) we break down lengthy responses into 'Legal Data Points' (LDPs), self-contained units of information, and introduce a novel, reference-free evaluation methodology that reflects how lawyers evaluate legal answers; b) we demonstrate that our method outperforms a variety of baselines on both our proprietary dataset and an open-source dataset (LegalBench); c) we show how our method correlates more closely with human expert evaluations and helps improve inter-annotator agreement; and finally d) we open source our Legal Data Points for a subset of LegalBench used in our experiments, allowing the research community to replicate our results and advance research in this vital area of LLM evaluation on legal question-answering.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Adapt Small Language Models for Tools; Adapt Tool Schemas to the Models</title>
<link>https://arxiv.org/abs/2510.07248</link>
<guid>https://arxiv.org/abs/2510.07248</guid>
<content:encoded><![CDATA[
arXiv:2510.07248v1 Announce Type: new 
Abstract: Small language models (SLMs) offer significant computational advantages for tool-augmented AI systems, yet they struggle with tool-use tasks, particularly in selecting appropriate tools and identifying correct parameters. A common failure mode is schema misalignment: models hallucinate plausible but non-existent tool names that reflect naming conventions internalized during pretraining but absent from the provided tool schema. Rather than forcing models to adapt to arbitrary schemas, we propose adapting schemas to align with models' pretrained knowledge. We introduce PA-Tool (Pretraining-Aligned Tool Schema Generation), a training-free method that leverages peakedness-a signal from contamination detection indicating pretraining familiarity-to automatically rename tool components. By generating multiple candidates and selecting those with highest output concentration across samples, PA-Tool identifies pretrain-aligned naming patterns. Experiments on MetaTool and RoTBench show improvements of up to 17% points, with schema misalignment errors reduced by 80%. PA-Tool enables small models to approach state-of-the-art performance while maintaining computational efficiency for adaptation to new tools without retraining. Our work demonstrates that schema-level interventions can unlock the tool-use potential of resource-efficient models by adapting schemas to models rather than models to schemas.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Rubrics Elicitation from Pairwise Comparisons</title>
<link>https://arxiv.org/abs/2510.07284</link>
<guid>https://arxiv.org/abs/2510.07284</guid>
<content:encoded><![CDATA[
arXiv:2510.07284v1 Announce Type: new 
Abstract: Rubrics provide a flexible way to train LLMs on open-ended long-form answers where verifiable rewards are not applicable and human preferences provide coarse signals. Prior work shows that reinforcement learning with rubric-based rewards leads to consistent gains in LLM post-training. Most existing approaches rely on rubrics that remain static over the course of training. Such static rubrics, however, are vulnerable to reward-hacking type behaviors and fail to capture emergent desiderata that arise during training. We introduce Online Rubrics Elicitation (OnlineRubrics), a method that dynamically curates evaluation criteria in an online manner through pairwise comparisons of responses from current and reference policies. This online process enables continuous identification and mitigation of errors as training proceeds. Empirically, this approach yields consistent improvements of up to 8% over training exclusively with static rubrics across AlpacaEval, GPQA, ArenaHard as well as the validation sets of expert questions and rubrics. We qualitatively analyze the elicited criteria and identify prominent themes such as transparency, practicality, organization, and reasoning.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Convergence of Moral Self-Correction in Large Language Models</title>
<link>https://arxiv.org/abs/2510.07290</link>
<guid>https://arxiv.org/abs/2510.07290</guid>
<content:encoded><![CDATA[
arXiv:2510.07290v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are able to improve their responses when instructed to do so, a capability known as self-correction. When instructions provide only a general and abstract goal without specific details about potential issues in the response, LLMs must rely on their internal knowledge to improve response quality, a process referred to as intrinsic self-correction. The empirical success of intrinsic self-correction is evident in various applications, but how and why it is effective remains unknown. Focusing on moral self-correction in LLMs, we reveal a key characteristic of intrinsic self-correction: performance convergence through multi-round interactions; and provide a mechanistic analysis of this convergence behavior. Based on our experimental results and analysis, we uncover the underlying mechanism of convergence: consistently injected self-correction instructions activate moral concepts that reduce model uncertainty, leading to converged performance as the activated moral concepts stabilize over successive rounds. This paper demonstrates the strong potential of moral self-correction by showing that it exhibits a desirable property of converged performance.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think Natively: Unlocking Multilingual Reasoning with Consistency-Enhanced Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.07300</link>
<guid>https://arxiv.org/abs/2510.07300</guid>
<content:encoded><![CDATA[
arXiv:2510.07300v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) have achieved remarkable performance on complex reasoning tasks by adopting the "think-then-answer" paradigm, which enhances both accuracy and interpretability. However, current LRMs exhibit two critical limitations when processing non-English languages: (1) They often struggle to maintain input-output language consistency; (2) They generally perform poorly with wrong reasoning paths and lower answer accuracy compared to English. These limitations significantly degrade the user experience for non-English speakers and hinder the global deployment of LRMs. To address these limitations, we propose M-Thinker, which is trained by the GRPO algorithm that involves a Language Consistency (LC) reward and a novel Cross-lingual Thinking Alignment (CTA) reward. Specifically, the LC reward defines a strict constraint on the language consistency between the input, thought, and answer. Besides, the CTA reward compares the model's non-English reasoning paths with its English reasoning path to transfer its own reasoning capability from English to non-English languages. Through an iterative RL procedure, our M-Thinker-1.5B/7B models not only achieve nearly 100% language consistency and superior performance on two multilingual benchmarks (MMATH and PolyMath), but also exhibit excellent generalization on out-of-domain languages.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent Bain vs. Agent McKinsey: A New Text-to-SQL Benchmark for the Business Domain</title>
<link>https://arxiv.org/abs/2510.07309</link>
<guid>https://arxiv.org/abs/2510.07309</guid>
<content:encoded><![CDATA[
arXiv:2510.07309v1 Announce Type: new 
Abstract: In the business domain, where data-driven decision making is crucial, text-to-SQL is fundamental for easy natural language access to structured data. While recent LLMs have achieved strong performance in code generation, existing text-to-SQL benchmarks remain focused on factual retrieval of past records. We introduce CORGI, a new benchmark specifically designed for real-world business contexts. CORGI is composed of synthetic databases inspired by enterprises such as Doordash, Airbnb, and Lululemon. It provides questions across four increasingly complex categories of business queries: descriptive, explanatory, predictive, and recommendational. This challenge calls for causal reasoning, temporal forecasting, and strategic recommendation, reflecting multi-level and multi-step agentic intelligence. We find that LLM performance drops on high-level questions, struggling to make accurate predictions and offer actionable plans. Based on execution success rate, the CORGI benchmark is about 21\% more difficult than the BIRD benchmark. This highlights the gap between popular LLMs and the need for real-world business intelligence. We release a public dataset and evaluation framework, and a website for public submissions.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vibe Checker: Aligning Code Evaluation with Human Preference</title>
<link>https://arxiv.org/abs/2510.07315</link>
<guid>https://arxiv.org/abs/2510.07315</guid>
<content:encoded><![CDATA[
arXiv:2510.07315v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have catalyzed vibe coding, where users leverage LLMs to generate and iteratively refine code through natural language interactions until it passes their vibe check. Vibe check is tied to real-world human preference and goes beyond functionality: the solution should feel right, read cleanly, preserve intent, and remain correct. However, current code evaluation remains anchored to pass@k and captures only functional correctness, overlooking the non-functional instructions that users routinely apply. In this paper, we hypothesize that instruction following is the missing piece underlying vibe check that represents human preference in coding besides functional correctness. To quantify models' code instruction following capabilities with measurable signals, we present VeriCode, a taxonomy of 30 verifiable code instructions together with corresponding deterministic verifiers. We use the taxonomy to augment established evaluation suites, resulting in Vibe Checker, a testbed to assess both code instruction following and functional correctness. Upon evaluating 31 leading LLMs, we show that even the strongest models struggle to comply with multiple instructions and exhibit clear functional regression. Most importantly, a composite score of functional correctness and instruction following correlates the best with human preference, with the latter emerging as the primary differentiator on real-world programming tasks. Our work identifies core factors of the vibe check, providing a concrete path for benchmarking and developing models that better align with user preferences in coding.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Hippocampus Networks for Efficient Long-Context Modeling</title>
<link>https://arxiv.org/abs/2510.07318</link>
<guid>https://arxiv.org/abs/2510.07318</guid>
<content:encoded><![CDATA[
arXiv:2510.07318v1 Announce Type: new 
Abstract: Long-sequence modeling faces a fundamental trade-off between the efficiency of compressive fixed-size memory in RNN-like models and the fidelity of lossless growing memory in attention-based Transformers. Inspired by the Multi-Store Model in cognitive science, we introduce a memory framework of artificial neural networks. Our method maintains a sliding window of the Transformer's KV cache as lossless short-term memory, while a learnable module termed Artificial Hippocampus Network (AHN) recurrently compresses out-of-window information into a fixed-size compact long-term memory. To validate this framework, we instantiate AHNs using modern RNN-like architectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate that AHN-augmented models consistently outperform sliding window baselines and achieve performance comparable or even superior to full-attention models, while substantially reducing computational and memory requirements. For instance, augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5% and memory cache by 74.0%, while improving its average score on LV-Eval (128k sequence length) from 4.41 to 5.88. Code is available at: https://github.com/ByteDance-Seed/AHN.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CML-Bench: A Framework for Evaluating and Enhancing LLM-Powered Movie Scripts Generation</title>
<link>https://arxiv.org/abs/2510.06231</link>
<guid>https://arxiv.org/abs/2510.06231</guid>
<content:encoded><![CDATA[
arXiv:2510.06231v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in generating highly structured texts. However, while exhibiting a high degree of structural organization, movie scripts demand an additional layer of nuanced storytelling and emotional depth-the 'soul' of compelling cinema-that LLMs often fail to capture. To investigate this deficiency, we first curated CML-Dataset, a dataset comprising (summary, content) pairs for Cinematic Markup Language (CML), where 'content' consists of segments from esteemed, high-quality movie scripts and 'summary' is a concise description of the content. Through an in-depth analysis of the intrinsic multi-shot continuity and narrative structures within these authentic scripts, we identified three pivotal dimensions for quality assessment: Dialogue Coherence (DC), Character Consistency (CC), and Plot Reasonableness (PR). Informed by these findings, we propose the CML-Bench, featuring quantitative metrics across these dimensions. CML-Bench effectively assigns high scores to well-crafted, human-written scripts while concurrently pinpointing the weaknesses in screenplays generated by LLMs. To further validate our benchmark, we introduce CML-Instruction, a prompting strategy with detailed instructions on character dialogue and event logic, to guide LLMs to generate more structured and cinematically sound scripts. Extensive experiments validate the effectiveness of our benchmark and demonstrate that LLMs guided by CML-Instruction generate higher-quality screenplays, with results aligned with human preferences.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning</title>
<link>https://arxiv.org/abs/2510.06261</link>
<guid>https://arxiv.org/abs/2510.06261</guid>
<content:encoded><![CDATA[
arXiv:2510.06261v1 Announce Type: cross 
Abstract: We present AlphaApollo, a self-evolving agentic reasoning system that aims to address two bottlenecks in foundation model (FM) reasoning-limited model-intrinsic capacity and unreliable test-time iteration. AlphaApollo orchestrates multiple models with professional tools to enable deliberate, verifiable reasoning. It couples (i) a computation tool (Python with numerical and symbolic libraries) and (ii) a retrieval tool (task-relevant external information) to execute exact calculations and ground decisions. The system further supports multi-round, multi-model solution evolution via a shared state map that records candidates, executable checks, and feedback for iterative refinement. In evaluations on AIME 2024/2025 across multiple models, AlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32 for Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for Llama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool calls are successfully executed, with consistent outperformance of non-tool baselines, thereby lifting the capability ceiling of FMs. More empirical results and implementation details will be updated at https://github.com/tmlr-group/AlphaApollo.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asking For It: Question-Answering for Predicting Rule Infractions in Online Content Moderation</title>
<link>https://arxiv.org/abs/2510.06350</link>
<guid>https://arxiv.org/abs/2510.06350</guid>
<content:encoded><![CDATA[
arXiv:2510.06350v1 Announce Type: cross 
Abstract: Online communities rely on a mix of platform policies and community-authored rules to define acceptable behavior and maintain order. However, these rules vary widely across communities, evolve over time, and are enforced inconsistently, posing challenges for transparency, governance, and automation. In this paper, we model the relationship between rules and their enforcement at scale, introducing ModQ, a novel question-answering framework for rule-sensitive content moderation. Unlike prior classification or generation-based approaches, ModQ conditions on the full set of community rules at inference time and identifies which rule best applies to a given comment. We implement two model variants - extractive and multiple-choice QA - and train them on large-scale datasets from Reddit and Lemmy, the latter of which we construct from publicly available moderation logs and rule descriptions. Both models outperform state-of-the-art baselines in identifying moderation-relevant rule violations, while remaining lightweight and interpretable. Notably, ModQ models generalize effectively to unseen communities and rules, supporting low-resource moderation settings and dynamic governance environments.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PuzzlePlex: Benchmarking Foundation Models on Reasoning and Planning with Puzzles</title>
<link>https://arxiv.org/abs/2510.06475</link>
<guid>https://arxiv.org/abs/2510.06475</guid>
<content:encoded><![CDATA[
arXiv:2510.06475v1 Announce Type: cross 
Abstract: This work investigates the reasoning and planning capabilities of foundation models and their scalability in complex, dynamic environments. We introduce PuzzlePlex, a benchmark designed to assess these capabilities through a diverse set of puzzles. PuzzlePlex consists of 15 types of puzzles, including deterministic and stochastic games of varying difficulty, as well as single-player and two-player scenarios. The PuzzlePlex framework provides a comprehensive environment for each game, and supports extensibility to generate more challenging instances as foundation models evolve. Additionally, we implement customized game-playing strategies for comparison. Building on this benchmark, we develop fine-grained metrics to measure performance and conduct an in-depth analysis of frontier foundation models across two settings: instruction-based and code-based. Furthermore, we systematically investigate their scaling limits. Our findings show that reasoning models outperform others in instruction-based settings, while code-based execution presents greater challenges but offers a scalable and efficient alternative. PuzzlePlex enables targeted evaluation and guides future improvements in reasoning, planning, and generalization for foundation models.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Markovian Thinker</title>
<link>https://arxiv.org/abs/2510.06557</link>
<guid>https://arxiv.org/abs/2510.06557</guid>
<content:encoded><![CDATA[
arXiv:2510.06557v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has recently become a strong recipe for training reasoning LLMs that produce long chains of thought (LongCoT). Yet the standard RL "thinking environment", where the state is the prompt plus all prior reasoning tokens, makes the state unbounded and forces attention-based policies to pay quadratic compute as thoughts lengthen. We revisit the environment itself. We propose Markovian Thinking, a paradigm in which the policy advances reasoning while conditioning on a constant-size state, decoupling thinking length from context size. As an immediate consequence this yields linear compute with constant memory. We instantiate this idea with Delethink, an RL environment that structures reasoning into fixed-size chunks. Within each chunk, the model thinks as usual; at the boundary, the environment resets the context and reinitializes the prompt with a short carryover. Through RL, the policy learns to write a textual state near the end of each chunk sufficient for seamless continuation of reasoning after reset. Trained in this environment, an R1-Distill 1.5B model reasons in 8K-token chunks yet thinks up to 24K tokens, matching or surpassing LongCoT-RL trained with a 24K budget. With test-time scaling, Delethink continues to improve where LongCoT plateaus. The effect of linear compute is substantial: we empirically estimate at 96K average thinking length LongCoT-RL costs 27 H100-months vs. 7 for Delethink. Analysis at RL initialization shows off-the-shelf reasoning models (1.5B-120B) often sample Markovian traces zero-shot across diverse benchmarks, providing positive samples that make RL effective at scale. Our results show that redesigning the thinking environment is a powerful lever: it enables very long reasoning without quadratic overhead and opens a path toward efficient, scalable reasoning LLMs.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reading Between the Lines: Towards Reliable Black-box LLM Fingerprinting via Zeroth-order Gradient Estimation</title>
<link>https://arxiv.org/abs/2510.06605</link>
<guid>https://arxiv.org/abs/2510.06605</guid>
<content:encoded><![CDATA[
arXiv:2510.06605v1 Announce Type: cross 
Abstract: The substantial investment required to develop Large Language Models (LLMs) makes them valuable intellectual property, raising significant concerns about copyright protection. LLM fingerprinting has emerged as a key technique to address this, which aims to verify a model's origin by extracting an intrinsic, unique signature (a "fingerprint") and comparing it to that of a source model to identify illicit copies. However, existing black-box fingerprinting methods often fail to generate distinctive LLM fingerprints. This ineffectiveness arises because black-box methods typically rely on model outputs, which lose critical information about the model's unique parameters due to the usage of non-linear functions. To address this, we first leverage Fisher Information Theory to formally demonstrate that the gradient of the model's input is a more informative feature for fingerprinting than the output. Based on this insight, we propose ZeroPrint, a novel method that approximates these information-rich gradients in a black-box setting using zeroth-order estimation. ZeroPrint overcomes the challenge of applying this to discrete text by simulating input perturbations via semantic-preserving word substitutions. This operation allows ZeroPrint to estimate the model's Jacobian matrix as a unique fingerprint. Experiments on the standard benchmark show ZeroPrint achieves a state-of-the-art effectiveness and robustness, significantly outperforming existing black-box methods.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XLSR-Kanformer: A KAN-Intergrated model for Synthetic Speech Detection</title>
<link>https://arxiv.org/abs/2510.06706</link>
<guid>https://arxiv.org/abs/2510.06706</guid>
<content:encoded><![CDATA[
arXiv:2510.06706v1 Announce Type: cross 
Abstract: Recent advancements in speech synthesis technologies have led to increasingly sophisticated spoofing attacks, posing significant challenges for automatic speaker verification systems. While systems based on self-supervised learning (SSL) models, particularly the XLSR-Conformer architecture, have demonstrated remarkable performance in synthetic speech detection, there remains room for architectural improvements. In this paper, we propose a novel approach that replaces the traditional Multi-Layer Perceptron (MLP) in the XLSR-Conformer model with a Kolmogorov-Arnold Network (KAN), a powerful universal approximator based on the Kolmogorov-Arnold representation theorem. Our experimental results on ASVspoof2021 demonstrate that the integration of KAN to XLSR-Conformer model can improve the performance by 60.55% relatively in Equal Error Rate (EER) LA and DF sets, further achieving 0.70% EER on the 21LA set. Besides, the proposed replacement is also robust to various SSL architectures. These findings suggest that incorporating KAN into SSL-based models is a promising direction for advances in synthetic speech detection.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentially Private Synthetic Text Generation for Retrieval-Augmented Generation (RAG)</title>
<link>https://arxiv.org/abs/2510.06719</link>
<guid>https://arxiv.org/abs/2510.06719</guid>
<content:encoded><![CDATA[
arXiv:2510.06719v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by grounding them in external knowledge. However, its application in sensitive domains is limited by privacy risks. Existing private RAG methods typically rely on query-time differential privacy (DP), which requires repeated noise injection and leads to accumulated privacy loss. To address this issue, we propose DP-SynRAG, a framework that uses LLMs to generate differentially private synthetic RAG databases. Unlike prior methods, the synthetic text can be reused once created, thereby avoiding repeated noise injection and additional privacy costs. To preserve essential information for downstream RAG tasks, DP-SynRAG extends private prediction, which instructs LLMs to generate text that mimics subsampled database records in a DP manner. Experiments show that DP-SynRAG achieves superior performanec to the state-of-the-art private RAG systems while maintaining a fixed privacy budget, offering a scalable solution for privacy-preserving RAG.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating LLMs for Historical Document OCR: A Methodological Framework for Digital Humanities</title>
<link>https://arxiv.org/abs/2510.06743</link>
<guid>https://arxiv.org/abs/2510.06743</guid>
<content:encoded><![CDATA[
arXiv:2510.06743v1 Announce Type: cross 
Abstract: Digital humanities scholars increasingly use Large Language Models for historical document digitization, yet lack appropriate evaluation frameworks for LLM-based OCR. Traditional metrics fail to capture temporal biases and period-specific errors crucial for historical corpus creation. We present an evaluation methodology for LLM-based historical OCR, addressing contamination risks and systematic biases in diplomatic transcription. Using 18th-century Russian Civil font texts, we introduce novel metrics including Historical Character Preservation Rate (HCPR) and Archaic Insertion Rate (AIR), alongside protocols for contamination control and stability testing. We evaluate 12 multimodal LLMs, finding that Gemini and Qwen models outperform traditional OCR while exhibiting over-historicization: inserting archaic characters from incorrect historical periods. Post-OCR correction degrades rather than improves performance. Our methodology provides digital humanities practitioners with guidelines for model selection and quality assessment in historical corpus digitization.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolving and Executing Research Plans via Double-Loop Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2510.06761</link>
<guid>https://arxiv.org/abs/2510.06761</guid>
<content:encoded><![CDATA[
arXiv:2510.06761v1 Announce Type: cross 
Abstract: Automating the end-to-end scientific research process poses a fundamental challenge: it requires both evolving high-level plans that are novel and sound, and executing these plans correctly amidst dynamic and uncertain conditions. To address this bilevel challenge, we propose a novel Double-Loop Multi-Agent (DLMA) framework to solve the given research problem automatically. The leader loop, composed of professor agents, is responsible for evolving research plans. It employs an evolutionary algorithm through involvement, improvement, and integration meetings to iteratively generate and refine a pool of research proposals, exploring the solution space effectively. The follower loop, composed of doctoral student agents, is responsible for executing the best-evolved plan. It dynamically adjusts the plan during implementation via pre-hoc and post-hoc meetings, ensuring each step (e.g., drafting, coding) is well-supported by contextual and external observations. Extensive experiments on benchmarks like ACLAward and Laboratory show that DLMA generates research papers that achieve state-of-the-art scores in automated evaluation, significantly outperforming strong baselines. Ablation studies confirm the critical roles of both loops, with evolution driving novelty and execution ensuring soundness.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GPT-5 Model Corrected GPT-4V's Chart Reading Errors, Not Prompting</title>
<link>https://arxiv.org/abs/2510.06782</link>
<guid>https://arxiv.org/abs/2510.06782</guid>
<content:encoded><![CDATA[
arXiv:2510.06782v1 Announce Type: cross 
Abstract: We present a quantitative evaluation to understand the effect of zero-shot large-language model (LLMs) and prompting uses on chart reading tasks. We asked LLMs to answer 107 visualization questions to compare inference accuracies between the agentic GPT-5 and multimodal GPT-4V, for difficult image instances, where GPT-4V failed to produce correct answers. Our results show that model architecture dominates the inference accuracy: GPT5 largely improved accuracy, while prompt variants yielded only small effects. Pre-registration of this work is available here: https://osf.io/u78td/?view_only=6b075584311f48e991c39335c840ded3; the Google Drive materials are here:https://drive.google.com/file/d/1ll8WWZDf7cCNcfNWrLViWt8GwDNSvVrp/view.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exposing Citation Vulnerabilities in Generative Engines</title>
<link>https://arxiv.org/abs/2510.06823</link>
<guid>https://arxiv.org/abs/2510.06823</guid>
<content:encoded><![CDATA[
arXiv:2510.06823v1 Announce Type: cross 
Abstract: We analyze answers generated by generative engines (GEs) from the perspectives of citation publishers and the content-injection barrier, defined as the difficulty for attackers to manipulate answers to user prompts by placing malicious content on the web. GEs integrate two functions: web search and answer generation that cites web pages using large language models. Because anyone can publish information on the web, GEs are vulnerable to poisoning attacks. Existing studies of citation evaluation focus on how faithfully answer content reflects cited sources, leaving unexamined which web sources should be selected as citations to defend against poisoning attacks. To fill this gap, we introduce evaluation criteria that assess poisoning threats using the citation information contained in answers. Our criteria classify the publisher attributes of citations to estimate the content-injection barrier thereby revealing the threat of poisoning attacks in current GEs. We conduct experiments in political domains in Japan and the United States (U.S.) using our criteria and show that citations from official party websites (primary sources) are approximately \(25\%\)--\(45\%\) in the U.S. and \(60\%\)--\(65\%\) in Japan, indicating that U.S. political answers are at higher risk of poisoning attacks. We also find that sources with low content-injection barriers are frequently cited yet are poorly reflected in answer content. To mitigate this threat, we discuss how publishers of primary sources can increase exposure of their web content in answers and show that well-known techniques are limited by language differences.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crossing Domains without Labels: Distant Supervision for Term Extraction</title>
<link>https://arxiv.org/abs/2510.06838</link>
<guid>https://arxiv.org/abs/2510.06838</guid>
<content:encoded><![CDATA[
arXiv:2510.06838v1 Announce Type: cross 
Abstract: Automatic Term Extraction (ATE) is a critical component in downstream NLP tasks such as document tagging, ontology construction and patent analysis. Current state-of-the-art methods require expensive human annotation and struggle with domain transfer, limiting their practical deployment. This highlights the need for more robust, scalable solutions and realistic evaluation settings. To address this, we introduce a comprehensive benchmark spanning seven diverse domains, enabling performance evaluation at both the document- and corpus-levels. Furthermore, we propose a robust LLM-based model that outperforms both supervised cross-domain encoder models and few-shot learning baselines and performs competitively with its GPT-4o teacher on this benchmark. The first step of our approach is generating psuedo-labels with this black-box LLM on general and scientific domains to ensure generalizability. Building on this data, we fine-tune the first LLMs for ATE. To further enhance document-level consistency, oftentimes needed for downstream tasks, we introduce lightweight post-hoc heuristics. Our approach exceeds previous approaches on 5/7 domains with an average improvement of 10 percentage points. We release our dataset and fine-tuned models to support future research in this area.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces</title>
<link>https://arxiv.org/abs/2510.06953</link>
<guid>https://arxiv.org/abs/2510.06953</guid>
<content:encoded><![CDATA[
arXiv:2510.06953v1 Announce Type: cross 
Abstract: The Uniform Information Density (UID) hypothesis suggests that effective communication maintains a stable flow of information. In this work, we revisit this principle in the context of large language model (LLM) reasoning traces, asking whether step-level uniformity reflects reasoning quality. To this end, we propose an entropy-based stepwise information density metric and introduce two complementary measures of uniformity, local and global uniformity scores. Across the experiments on six different reasoning benchmarks, we find that step-level uniformity not only provides a strong theoretical lens but also yields practical performance benefits; for example, selecting reasoning traces with more uniform information density at the step-level improves accuracy by 10-32\% relative gains over baselines at AIME2025. Our analysis further reveals that correct reasoning traces tend to avoid sharp information density spikes, while incorrect traces exhibit irregular information bursts. These results demonstrate that UID-inspired information density measures outperform alternative internal signals as predictors of reasoning quality. Results highlight the uniformity of the information density as a robust diagnostic and selection criterion for building more reliable and accurate reasoning systems.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VelLMes: A high-interaction AI-based deception framework</title>
<link>https://arxiv.org/abs/2510.06975</link>
<guid>https://arxiv.org/abs/2510.06975</guid>
<content:encoded><![CDATA[
arXiv:2510.06975v1 Announce Type: cross 
Abstract: There are very few SotA deception systems based on Large Language Models. The existing ones are limited only to simulating one type of service, mainly SSH shells. These systems - but also the deception technologies not based on LLMs - lack an extensive evaluation that includes human attackers. Generative AI has recently become a valuable asset for cybersecurity researchers and practitioners, and the field of cyber-deception is no exception. Researchers have demonstrated how LLMs can be leveraged to create realistic-looking honeytokens, fake users, and even simulated systems that can be used as honeypots. This paper presents an AI-based deception framework called VelLMes, which can simulate multiple protocols and services such as SSH Linux shell, MySQL, POP3, and HTTP. All of these can be deployed and used as honeypots, thus VelLMes offers a variety of choices for deception design based on the users' needs. VelLMes is designed to be attacked by humans, so interactivity and realism are key for its performance. We evaluate the generative capabilities and the deception capabilities. Generative capabilities were evaluated using unit tests for LLMs. The results of the unit tests show that, with careful prompting, LLMs can produce realistic-looking responses, with some LLMs having a 100% passing rate. In the case of the SSH Linux shell, we evaluated deception capabilities with 89 human attackers. The results showed that about 30% of the attackers thought that they were interacting with a real system when they were assigned an LLM-based honeypot. Lastly, we deployed 10 instances of the SSH Linux shell honeypot on the Internet to capture real-life attacks. Analysis of these attacks showed us that LLM honeypots simulating Linux shells can perform well against unstructured and unexpected attacks on the Internet, responding correctly to most of the issued commands.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RedTWIZ: Diverse LLM Red Teaming via Adaptive Attack Planning</title>
<link>https://arxiv.org/abs/2510.06994</link>
<guid>https://arxiv.org/abs/2510.06994</guid>
<content:encoded><![CDATA[
arXiv:2510.06994v1 Announce Type: cross 
Abstract: This paper presents the vision, scientific contributions, and technical details of RedTWIZ: an adaptive and diverse multi-turn red teaming framework, to audit the robustness of Large Language Models (LLMs) in AI-assisted software development. Our work is driven by three major research streams: (1) robust and systematic assessment of LLM conversational jailbreaks; (2) a diverse generative multi-turn attack suite, supporting compositional, realistic and goal-oriented jailbreak conversational strategies; and (3) a hierarchical attack planner, which adaptively plans, serializes, and triggers attacks tailored to specific LLM's vulnerabilities. Together, these contributions form a unified framework -- combining assessment, attack generation, and strategic planning -- to comprehensively evaluate and expose weaknesses in LLMs' robustness. Extensive evaluation is conducted to systematically assess and analyze the performance of the overall system and each component. Experimental results demonstrate that our multi-turn adversarial attack strategies can successfully lead state-of-the-art LLMs to produce unsafe generations, highlighting the pressing need for more research into enhancing LLM's robustness.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Cognitive Bandwidth Bottleneck: Shifting Long-Horizon Agent from Planning with Actions to Planning with Schemas</title>
<link>https://arxiv.org/abs/2510.07091</link>
<guid>https://arxiv.org/abs/2510.07091</guid>
<content:encoded><![CDATA[
arXiv:2510.07091v1 Announce Type: cross 
Abstract: Enabling LLMs to effectively operate long-horizon task which requires long-term planning and multiple interactions is essential for open-world autonomy. Conventional methods adopt planning with actions where a executable action list would be provided as reference. However, this action representation choice would be impractical when the environment action space is combinatorial exploded (e.g., open-ended real world). This naturally leads to a question: As environmental action space scales, what is the optimal action representation for long-horizon agents? In this paper, we systematically study the effectiveness of two different action representations. The first one is conventional planning with actions (PwA) which is predominantly adopted for its effectiveness on existing benchmarks. The other one is planning with schemas (PwS) which instantiate an action schema into action lists (e.g., "move [OBJ] to [OBJ]" -> "move apple to desk") to ensure concise action space and reliable scalability. This alternative is motivated by its alignment with human cognition and its compliance with environment-imposed action format restriction. We propose cognitive bandwidth perspective as a conceptual framework to qualitatively understand the differences between these two action representations and empirically observe a representation-choice inflection point between ALFWorld (~35 actions) and SciWorld (~500 actions), which serve as evidence of the need for scalable representations. We further conduct controlled experiments to study how the location of this inflection point interacts with different model capacities: stronger planning proficiency shifts the inflection rightward, whereas better schema instantiation shifts it leftward. Finally, noting the suboptimal performance of PwS agents, we provide an actionable guide for building more capable PwS agents for better scalable autonomy.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Agent Framework for Stateful Inference-Time Search</title>
<link>https://arxiv.org/abs/2510.07147</link>
<guid>https://arxiv.org/abs/2510.07147</guid>
<content:encoded><![CDATA[
arXiv:2510.07147v1 Announce Type: cross 
Abstract: Recent work explores agentic inference-time techniques to perform structured, multi-step reasoning. However, stateless inference often struggles on multi-step tasks due to the absence of persistent state. Moreover, task-specific fine-tuning or instruction-tuning often achieve surface-level code generation but remain brittle on tasks requiring deeper reasoning and long-horizon dependencies. To address these limitations, we propose stateful multi-agent evolutionary search, a training-free framework that departs from prior stateless approaches by combining (i) persistent inference-time state, (ii) adversarial mutation, and (iii) evolutionary preservation. We demonstrate its effectiveness in automated unit test generation through the generation of edge cases. We generate robust edge cases using an evolutionary search process, where specialized agents sequentially propose, mutate, and score candidates. A controller maintains persistent state across generations, while evolutionary preservation ensures diversity and exploration across all possible cases. This yields a generalist agent capable of discovering robust, high-coverage edge cases across unseen codebases. Experiments show our stateful multi-agent inference framework achieves substantial gains in coverage over stateless single-step baselines, evaluated on prevalent unit-testing benchmarks such as HumanEval and TestGenEvalMini and using three diverse LLM families - Llama, Gemma, and GPT. These results indicate that combining persistent inference-time state with evolutionary search materially improves unit-test generation.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machines in the Crowd? Measuring the Footprint of Machine-Generated Text on Reddit</title>
<link>https://arxiv.org/abs/2510.07226</link>
<guid>https://arxiv.org/abs/2510.07226</guid>
<content:encoded><![CDATA[
arXiv:2510.07226v1 Announce Type: cross 
Abstract: Generative Artificial Intelligence is reshaping online communication by enabling large-scale production of Machine-Generated Text (MGT) at low cost. While its presence is rapidly growing across the Web, little is known about how MGT integrates into social media environments. In this paper, we present the first large-scale characterization of MGT on Reddit. Using a state-of-the-art statistical method for detection of MGT, we analyze over two years of activity (2022-2024) across 51 subreddits representative of Reddit's main community types such as information seeking, social support, and discussion. We study the concentration of MGT across communities and over time, and compared MGT to human-authored text in terms of social signals it expresses and engagement it receives. Our very conservative estimate of MGT prevalence indicates that synthetic text is marginally present on Reddit, but it can reach peaks of up to 9% in some communities in some months. MGT is unevenly distributed across communities, more prevalent in subreddits focused on technical knowledge and social support, and often concentrated in the activity of a small fraction of users. MGT also conveys distinct social signals of warmth and status giving typical of language of AI assistants. Despite these stylistic differences, MGT achieves engagement levels comparable than human-authored content and in a few cases even higher, suggesting that AI-generated text is becoming an organic component of online social discourse. This work offers the first perspective on the MGT footprint on Reddit, paving the way for new investigations involving platform governance, detection strategies, and community dynamics.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AudioMarathon: A Comprehensive Benchmark for Long-Context Audio Understanding and Efficiency in Audio LLMs</title>
<link>https://arxiv.org/abs/2510.07293</link>
<guid>https://arxiv.org/abs/2510.07293</guid>
<content:encoded><![CDATA[
arXiv:2510.07293v1 Announce Type: cross 
Abstract: Processing long-form audio is a major challenge for Large Audio Language models (LALMs). These models struggle with the quadratic cost of attention ($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio benchmarks are built mostly from short clips and do not evaluate models in realistic long context settings. To address this gap, we introduce AudioMarathon, a benchmark designed to evaluate both understanding and inference efficiency on long-form audio. AudioMarathon provides a diverse set of tasks built upon three pillars: long-context audio inputs with durations ranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of 2,250 to 7,500 audio tokens, respectively, full domain coverage across speech, sound, and music, and complex reasoning that requires multi-hop inference. We evaluate state-of-the-art LALMs and observe clear performance drops as audio length grows. We also study acceleration techniques and analyze the trade-offs of token pruning and KV cache eviction. The results show large gaps across current LALMs and highlight the need for better temporal reasoning and memory-efficient architectures. We believe AudioMarathon will drive the audio and multimodal research community to develop more advanced audio understanding models capable of solving complex audio tasks.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ECLM: Entity Level Language Model for Spoken Language Understanding with Chain of Intent</title>
<link>https://arxiv.org/abs/2403.04481</link>
<guid>https://arxiv.org/abs/2403.04481</guid>
<content:encoded><![CDATA[
arXiv:2403.04481v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in language generation and general task performance. However, their application to spoken language understanding (SLU) remains challenging, particularly for token-level tasks, where the autoregressive nature of LLMs often leads to misalignment issues. They also struggle to capture nuanced interrelations in semantic-level tasks through direct fine-tuning alone. To address these challenges, we propose the Entity-level Language Model (ECLM) framework, which reformulates slot-filling as an entity recognition task and introduces a novel concept, \textit{Chain of Intent}, to enable step-by-step multi-intent recognition. Experimental results show that ECLM significantly outperforms strong baselines such as Uni-MIS, achieving gains of 3.7\% on MixATIS and 3.1\% on MixSNIPS. Compared to standard supervised fine-tuning of LLMs, ECLM further achieves improvements of 8.5\% and 21.2\% on these datasets, respectively. Our code is available at https://github.com/SJY8460/ECLM.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Approximately Aligned Decoding</title>
<link>https://arxiv.org/abs/2410.01103</link>
<guid>https://arxiv.org/abs/2410.01103</guid>
<content:encoded><![CDATA[
arXiv:2410.01103v2 Announce Type: replace 
Abstract: It is common to reject undesired outputs of Large Language Models (LLMs); however, current methods to do so require an excessive amount of computation to re-sample after a rejection, or distort the distribution of outputs by constraining the output to highly improbable tokens. We present a method, Approximately Aligned Decoding (AprAD), to balance the distortion of the output distribution with computational efficiency, inspired by algorithms from the speculative decoding literature. AprAD allows for the generation of long sequences of text with difficult-to-satisfy constraints, while amplifying low probability outputs much less compared to existing methods. We show through a series of experiments that the task-specific performance of AprAD is comparable to methods that do not distort the output distribution, while being much more computationally efficient.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SuffixDecoding: Extreme Speculative Decoding for Emerging AI Applications</title>
<link>https://arxiv.org/abs/2411.04975</link>
<guid>https://arxiv.org/abs/2411.04975</guid>
<content:encoded><![CDATA[
arXiv:2411.04975v3 Announce Type: replace 
Abstract: Speculative decoding is widely adopted to reduce latency in large language model (LLM) inference by leveraging smaller draft models capable of handling diverse user tasks. However, emerging AI applications, such as LLM-based agents, present unique workload characteristics: instead of diverse independent requests, agentic frameworks typically submit repetitive inference requests, such as multi-agent pipelines performing similar subtasks or self-refinement loops iteratively enhancing outputs. These workloads result in long and highly predictable sequences, which current speculative decoding methods do not effectively exploit. To address this gap, we introduce \emph{SuffixDecoding}, a novel method that utilizes efficient suffix trees to cache long token sequences from prompts and previous outputs. By adaptively speculating more tokens when acceptance likelihood is high and fewer when it is low, SuffixDecoding effectively exploits opportunities for longer speculations while conserving computation when those opportunities are limited. Evaluations on agentic benchmarks, including SWE-Bench and Text-to-SQL, demonstrate that SuffixDecoding achieves speedups of up to 5.3$\times$, outperforming state-of-the-art methods -- 2.8$\times$ faster than model-based approaches like EAGLE-2/3 and 1.9$\times$ faster than model-free approaches such as Token Recycling. SuffixDecoding is open-sourced at https://github.com/snowflakedb/ArcticInference
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evil twins are not that evil: Qualitative insights into machine-generated prompts</title>
<link>https://arxiv.org/abs/2412.08127</link>
<guid>https://arxiv.org/abs/2412.08127</guid>
<content:encoded><![CDATA[
arXiv:2412.08127v4 Announce Type: replace 
Abstract: It has been widely observed that language models (LMs) respond in predictable ways to algorithmically generated prompts that are seemingly unintelligible. This is both a sign that we lack a full understanding of how LMs work, and a practical challenge, because opaqueness can be exploited for harmful uses of LMs, such as jailbreaking. We present the first thorough analysis of opaque machine-generated prompts, or autoprompts, pertaining to 6 LMs of different sizes and families. We find that machine-generated prompts are characterized by a last token that is often intelligible and strongly affects the generation. A small but consistent proportion of the previous tokens are prunable, probably appearing in the prompt as a by-product of the fact that the optimization process fixes the number of tokens. The remaining tokens fall into two categories: filler tokens, which can be replaced with semantically unrelated substitutes, and keywords, that tend to have at least a loose semantic relation with the generation, although they do not engage in well-formed syntactic relations with it. Additionally, human experts can reliably identify the most influential tokens in an autoprompt a posteriori, suggesting these prompts are not entirely opaque. Finally, some of the ablations we applied to autoprompts yield similar effects in natural language inputs, suggesting that autoprompts emerge naturally from the way LMs process linguistic inputs in general.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>2 OLMo 2 Furious</title>
<link>https://arxiv.org/abs/2501.00656</link>
<guid>https://arxiv.org/abs/2501.00656</guid>
<content:encoded><![CDATA[
arXiv:2501.00656v3 Announce Type: replace 
Abstract: We present OLMo 2, the next generation of our fully open language models. OLMo 2 includes a family of dense autoregressive language models at 7B, 13B and 32B scales with fully released artifacts -- model weights, full training data, training code and recipes, training logs and thousands of intermediate checkpoints. In this work, we describe our modified model architecture and training recipe, focusing on techniques for achieving better training stability and improved per-token efficiency. Our updated pretraining data mixture introduces a new, specialized data mix called Dolmino Mix 1124, which significantly improves model capabilities across many downstream task benchmarks when introduced via late-stage curriculum training (i.e. specialized data during the annealing phase of pretraining). Finally, we incorporate best practices from T\"ulu 3 to develop OLMo 2-Instruct, focusing on permissive data and extending our final-stage reinforcement learning with verifiable rewards (RLVR). Our OLMo 2 base models sit at the Pareto frontier of performance to training compute, often matching or outperforming open-weight only models like Llama 3.1, Qwen 2.5, and Gemma 2 while using fewer FLOPs and with fully transparent training data, code, and recipe. Our fully open OLMo 2-Instruct models are competitive with open-weight only models of comparable size and even some proprietary models like GPT-3.5 Turbo and GPT 4o Mini.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LatteReview: A Multi-Agent Framework for Systematic Review Automation Using Large Language Models</title>
<link>https://arxiv.org/abs/2501.05468</link>
<guid>https://arxiv.org/abs/2501.05468</guid>
<content:encoded><![CDATA[
arXiv:2501.05468v2 Announce Type: replace 
Abstract: Systematic literature reviews and meta-analyses are essential for synthesizing research insights, but they remain time-intensive and labor-intensive due to the iterative processes of screening, evaluation, and data extraction. This paper introduces and evaluates LatteReview, a Python-based framework that leverages large language models (LLMs) and multi-agent systems to automate key elements of the systematic review process. Designed to streamline workflows while maintaining rigor, LatteReview utilizes modular agents for tasks such as title and abstract screening, relevance scoring, and structured data extraction. These agents operate within orchestrated workflows, supporting sequential and parallel review rounds, dynamic decision-making, and iterative refinement based on user feedback. LatteReview's architecture integrates LLM providers, enabling compatibility with both cloud-based and locally hosted models. The framework supports features such as Retrieval-Augmented Generation (RAG) for incorporating external context, multimodal reviews, Pydantic-based validation for structured inputs and outputs, and asynchronous programming for handling large-scale datasets. The framework is available on the GitHub repository, with detailed documentation and an installable package.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Gaslighting Negation Attacks Against Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2501.19017</link>
<guid>https://arxiv.org/abs/2501.19017</guid>
<content:encoded><![CDATA[
arXiv:2501.19017v4 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) have exhibited remarkable advancements in integrating different modalities, excelling in complex understanding and generation tasks. Despite their success, MLLMs remain vulnerable to conversational adversarial inputs. In this paper, we systematically study gaslighting negation attacks: a phenomenon where models, despite initially providing correct answers, are persuaded by user-provided negations to reverse their outputs, often fabricating justifications. We conduct extensive evaluations of state-of-the-art MLLMs across diverse benchmarks and observe substantial performance drops when negation is introduced. Notably, we introduce the first benchmark GaslightingBench, specifically designed to evaluate the vulnerability of MLLMs to negation arguments. GaslightingBench consists of multiple-choice questions curated from existing datasets, along with generated negation prompts across 20 diverse categories. Throughout extensive evaluation, we find that proprietary models such as Gemini-1.5-flash and GPT-4o demonstrate better resilience compared to open-source counterparts like Qwen2-VL and LLaVA, though even advanced reasoning-oriented models like Gemini-2.5-Pro remain susceptible. Our category-level analysis further shows that subjective or socially nuanced domains (e.g., Social Relation, Image Emotion) are especially fragile, while more objective domains (e.g., Geography) exhibit relatively smaller but still notable drops. Overall, all evaluated MLLMs struggle to maintain logical consistency under gaslighting negation attack. These findings highlight a fundamental robustness gap and provide insights for developing more reliable and trustworthy multimodal AI systems. Project website: https://yxg1005.github.io/GaslightingNegationAttacks/.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blessing of Multilinguality: A Systematic Analysis of Multilingual In-Context Learning</title>
<link>https://arxiv.org/abs/2502.11364</link>
<guid>https://arxiv.org/abs/2502.11364</guid>
<content:encoded><![CDATA[
arXiv:2502.11364v3 Announce Type: replace 
Abstract: While multilingual large language models generally perform adequately, and sometimes even rival English performance on high-resource languages (HRLs), they often significantly underperform on low-resource languages (LRLs). Among several prompting strategies aiming at bridging the gap, multilingual in-context learning (ICL) has been particularly effective when demonstration in target languages is unavailable. However, there lacks a systematic understanding of when and why it works well.
  In this work, we systematically analyze multilingual ICL, using demonstrations in HRLs to enhance cross-lingual transfer. We show that demonstrations in mixed HRLs consistently outperform English-only ones across the board, particularly for tasks written in LRLs. Surprisingly, our ablation study shows that the presence of irrelevant non-English sentences in the prompt yields measurable gains, suggesting the effectiveness of multilingual exposure itself. Our results highlight the potential of strategically leveraging multilingual resources to bridge the performance gap for underrepresented languages.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diagnosing Moral Reasoning Acquisition in Language Models: Pragmatics and Generalization</title>
<link>https://arxiv.org/abs/2502.16600</link>
<guid>https://arxiv.org/abs/2502.16600</guid>
<content:encoded><![CDATA[
arXiv:2502.16600v5 Announce Type: replace 
Abstract: Ensuring that Large Language Models (LLMs) return just responses which adhere to societal values is crucial for their broader application. Prior research has shown that LLMs often fail to perform satisfactorily on tasks requiring moral cognizance, such as ethics-based judgments. While current approaches have focused on fine-tuning LLMs with curated datasets to improve their capabilities on such tasks, choosing the optimal learning paradigm to enhance the ethical responses of LLMs remains an open research debate. In this work, we aim to address this fundamental question: can current learning paradigms enable LLMs to acquire sufficient moral reasoning capabilities? Drawing from distributional semantics theory and the pragmatic nature of moral discourse, our analysis indicates that performance improvements follow a mechanism similar to that of semantic-level tasks, and therefore remain affected by the pragmatic nature of morals latent in discourse, a phenomenon we name the pragmatic dilemma. We conclude that this pragmatic dilemma imposes significant limitations on the generalization ability of current learning paradigms, making it the primary bottleneck for moral reasoning acquisition in LLMs.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speculative Decoding and Beyond: An In-Depth Survey of Techniques</title>
<link>https://arxiv.org/abs/2502.19732</link>
<guid>https://arxiv.org/abs/2502.19732</guid>
<content:encoded><![CDATA[
arXiv:2502.19732v4 Announce Type: replace 
Abstract: Sequential dependencies present a fundamental bottleneck in deploying large-scale autoregressive models, particularly for real-time applications. While traditional optimization approaches like pruning and quantization often compromise model quality, recent advances in generation-refinement frameworks demonstrate that this trade-off can be significantly mitigated.
  This survey presents a comprehensive taxonomy of generation-refinement frameworks, analyzing methods across autoregressive sequence tasks. We categorize methods based on their generation strategies (from simple n-gram prediction to sophisticated draft models) and refinement mechanisms (including single-pass verification and iterative approaches). Through systematic analysis of both algorithmic innovations and system-level implementations, we examine deployment strategies across computing environments and explore applications spanning text, images, and speech generation. This systematic examination of both theoretical frameworks and practical implementations provides a foundation for future research in efficient autoregressive decoding.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the (Belief) Gap: Group Identity in the World of LLMs</title>
<link>https://arxiv.org/abs/2503.02016</link>
<guid>https://arxiv.org/abs/2503.02016</guid>
<content:encoded><![CDATA[
arXiv:2503.02016v2 Announce Type: replace 
Abstract: Social biases and belief-driven behaviors can significantly impact Large Language Models (LLMs) decisions on several tasks. As LLMs are increasingly used in multi-agent systems for societal simulations, their ability to model fundamental group psychological characteristics remains critical yet under-explored. In this study, we present a multi-agent framework that simulates belief congruence, a classical group psychology theory that plays a crucial role in shaping societal interactions and preferences. Our findings reveal that LLMs exhibit amplified belief congruence compared to humans, across diverse contexts. We further investigate the implications of this behavior on two downstream tasks: (1) misinformation dissemination and (2) LLM learning, finding that belief congruence in LLMs increases misinformation dissemination and impedes learning. To mitigate these negative impacts, we propose strategies inspired by: (1) contact hypothesis, (2) accuracy nudges, and (3) global citizenship framework. Our results show that the best strategies reduce misinformation dissemination by up to 37% and enhance learning by 11%. Bridging social psychology and AI, our work provides insights to navigate real-world interactions using LLMs while addressing belief-driven biases.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Neutral Point-of-View Generation with Data- and Parameter-Efficient RL</title>
<link>https://arxiv.org/abs/2503.03654</link>
<guid>https://arxiv.org/abs/2503.03654</guid>
<content:encoded><![CDATA[
arXiv:2503.03654v2 Announce Type: replace 
Abstract: The paper shows that parameter-efficient reinforcement learning (PE-RL) is a highly effective training regime to improve large language models' (LLMs) ability to answer queries on sensitive topics with a Neutral Point of View (NPOV), i.e. to provide significantly more informative, diverse and impartial answers. This is shown by evaluating PE-RL and multiple strong baselines-including LoRA finetuning (strongest baseline), SFT and RLHF. PE-RL not only improves on overall NPOV quality compared to the strongest baseline ($97.06\%\rightarrow 99.08\%$), but also scores much higher on features linguists identify as key to separating sufficient answers from "great'' answers ($60.25\%\rightarrow 85.21\%$ for presence of supportive details, $68.74\%\rightarrow 91.43\%$ for absence of oversimplification). A qualitative analysis corroborates this. Moreover, our evaluation also finds a key property of PE-RL for this task: unlike methods that update all parameters, it generalises out of topic. Finally, to enable further studies we also release the dataset, SHQ-NPOV, and provide a methodology to create such datasets through iterative rounds of human peer-critique and annotator training.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree Search</title>
<link>https://arxiv.org/abs/2503.20757</link>
<guid>https://arxiv.org/abs/2503.20757</guid>
<content:encoded><![CDATA[
arXiv:2503.20757v2 Announce Type: replace 
Abstract: We introduce MCTS-RAG, a novel approach that enhances the reasoning capabilities of small language models on knowledge-intensive tasks by leveraging retrieval-augmented generation (RAG) to provide relevant context and Monte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically integrates retrieval and reasoning through an iterative decision-making process. Unlike standard RAG methods, which typically retrieve information independently from reasoning and thus integrate knowledge suboptimally, or conventional MCTS reasoning, which depends solely on internal model knowledge without external facts, MCTS-RAG combines structured reasoning with adaptive retrieval. This integrated approach enhances decision-making, reduces hallucinations, and ensures improved factual accuracy and response consistency. The experimental results on multiple reasoning and knowledge-intensive datasets datasets (i.e., ComplexWebQA, GPQA, and FoolMeTwice) show that our method enables small-scale LMs to achieve performance comparable to frontier LLMs like GPT-4o by effectively scaling inference-time compute, setting a new standard for reasoning in small-scale models.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Multilingual Continual Pretraining: Data Mixing for Adapting LLMs Across Languages and Resources</title>
<link>https://arxiv.org/abs/2504.04152</link>
<guid>https://arxiv.org/abs/2504.04152</guid>
<content:encoded><![CDATA[
arXiv:2504.04152v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) exhibit significant disparities in performance across languages, primarily benefiting high-resource languages while marginalizing underrepresented ones. Continual Pretraining (CPT) has emerged as a promising approach to address this imbalance, although the relative effectiveness of monolingual, bilingual, and code-augmented data strategies remains unclear. This study systematically evaluates 36 CPT configurations involving three multilingual base models, across 30+ languages categorized as altruistic, selfish, and stagnant, spanning various resource levels. Our findings reveal three major insights: (1) Bilingual CPT improves multilingual classification but often causes language mixing issues during generation. (2) Including programming code data during CPT consistently enhances multilingual classification accuracy, particularly benefiting low-resource languages, but introduces a trade-off by slightly degrading generation quality. (3) Contrary to prior work, we observe substantial deviations from language classifications according to their impact on cross-lingual transfer: Languages classified as altruistic often negatively affect related languages, selfish languages show conditional and configuration-dependent behavior, and stagnant languages demonstrate surprising adaptability under certain CPT conditions. These nuanced interactions emphasize the complexity of multilingual representation learning, underscoring the importance of systematic studies on generalizable language classification to inform future multilingual CPT strategies.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GlotEval: A Test Suite for Massively Multilingual Evaluation of Large Language Models</title>
<link>https://arxiv.org/abs/2504.04155</link>
<guid>https://arxiv.org/abs/2504.04155</guid>
<content:encoded><![CDATA[
arXiv:2504.04155v2 Announce Type: replace 
Abstract: Large language models (LLMs) are advancing at an unprecedented pace globally, with regions increasingly adopting these models for applications in their primary language. Evaluation of these models in diverse linguistic environments, especially in low-resource languages, has become a major challenge for academia and industry. Existing evaluation frameworks are disproportionately focused on English and a handful of high-resource languages, thereby overlooking the realistic performance of LLMs in multilingual and lower-resource scenarios. To address this gap, we introduce GlotEval, a lightweight framework designed for massively multilingual evaluation. Supporting seven key tasks (machine translation, text classification, summarization, open-ended generation, reading comprehension, sequence labeling, and intrinsic evaluation), spanning over dozens to hundreds of languages, GlotEval highlights consistent multilingual benchmarking, language-specific prompt templates, and non-English-centric machine translation. This enables a precise diagnosis of model strengths and weaknesses in diverse linguistic contexts. A multilingual translation case study demonstrates GlotEval's applicability for multilingual and language-specific evaluations.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometry of Semantics in Next-Token Prediction: How Optimization Implicitly Organizes Linguistic Representations</title>
<link>https://arxiv.org/abs/2505.08348</link>
<guid>https://arxiv.org/abs/2505.08348</guid>
<content:encoded><![CDATA[
arXiv:2505.08348v2 Announce Type: replace 
Abstract: We investigate how next-token prediction (NTP) optimization leads language models to extract and organize semantic structure from text. Our analysis, based on a tractable mathematical model and controlled synthetic data, reveals that NTP implicitly guides models to factor a centered support matrix encoding context-to-next-token co-occurrence patterns via singular value decomposition (SVD). While models never explicitly construct this matrix, learned word and context embeddings converge to its SVD factors, with singular vectors encoding latent semantic concepts through their sign patterns. We demonstrate that concepts corresponding to larger singular values are learned earlier during training, yielding a natural semantic hierarchy where broad categories emerge before fine-grained ones. This insight motivates orthant-based clustering, a method that combines concept signs to identify interpretable semantic categories. We validate our findings on synthetic datasets and pretrained language models, recovering diverse semantic structures such as grammatical categories, named entity types, and topical distinctions (medical, entertainment). Our work bridges classical distributional semantics and neural collapse geometry, characterizing how gradient-based optimization implicitly determines both the matrix representation and factorization method that encode semantic structure.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoRev: Multi-Modal Graph Retrieval for Automated Peer-Review Generation</title>
<link>https://arxiv.org/abs/2505.14376</link>
<guid>https://arxiv.org/abs/2505.14376</guid>
<content:encoded><![CDATA[
arXiv:2505.14376v2 Announce Type: replace 
Abstract: Enhancing the quality and efficiency of academic publishing is critical for both authors and reviewers, as research papers are central to scholarly communication and a major source of high-quality content on the web. To support this goal, we propose AutoRev, an automatic peer-review system designed to provide actionable, high-quality feedback to both reviewers and authors. AutoRev leverages a novel Multi-Modal Retrieval-Augmented Generation (RAG) framework that combines textual and graphical representations of academic papers. By modelling documents as graphs, AutoRev effectively retrieves the most pertinent information, significantly reducing the input context length for LLMs and thereby enhancing their review generation capabilities. Experimental results show that AutoRev outperforms state-of-the-art baselines by up to 58.72% and demonstrates competitive performance in human evaluations against ground truth reviews. We envision AutoRev as a powerful tool to streamline the peer-review workflow, alleviating challenges and enabling scalable, high-quality scholarly publishing. By guiding both authors and reviewers, AutoRev has the potential to accelerate the dissemination of quality research on the web at a larger scale. Code will be released upon acceptance.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HopWeaver: Cross-Document Synthesis of High-Quality and Authentic Multi-Hop Questions</title>
<link>https://arxiv.org/abs/2505.15087</link>
<guid>https://arxiv.org/abs/2505.15087</guid>
<content:encoded><![CDATA[
arXiv:2505.15087v2 Announce Type: replace 
Abstract: Multi-Hop Question Answering (MHQA) is crucial for evaluating the model's capability to integrate information from diverse sources. However, creating extensive and high-quality MHQA datasets is challenging: (i) manual annotation is expensive, and (ii) current synthesis methods often produce simplistic questions or require extensive manual guidance. This paper introduces HopWeaver, the first cross-document framework synthesizing authentic multi-hop questions without human intervention. HopWeaver synthesizes bridge and comparison questions through an innovative pipeline that identifies complementary documents and constructs authentic reasoning paths to ensure true multi-hop reasoning. We further present a comprehensive system for evaluating the synthesized multi-hop questions. Empirical evaluations demonstrate that the synthesized questions achieve comparable or superior quality to human-annotated datasets at a lower cost. Our framework provides a valuable tool for the research community: it can automatically generate challenging benchmarks from any raw corpus, which opens new avenues for both evaluation and targeted training to improve the reasoning capabilities of advanced QA models, especially in domains with scarce resources.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management</title>
<link>https://arxiv.org/abs/2505.15347</link>
<guid>https://arxiv.org/abs/2505.15347</guid>
<content:encoded><![CDATA[
arXiv:2505.15347v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly deployed in multi-turn conversational applications, where the management of the Key-Value (KV) Cache presents a significant bottleneck. The linear growth of the KV Cache with dialogue history imposes substantial computational costs, and existing eviction strategies often degrade performance by repeatedly compressing early conversational context, leading to information loss and context forgetting. This paper introduces FlowKV, a novel \textbf{multi-turn isolation mechanism} for KV Cache management, which can be applied to any KV Cache compression method without training. FlowKV's core innovation is a multi-turn isolation mechanism that preserves the accumulated compressed KV cache from past turns. Compression is then strategically applied only to the newly generated KV pairs of the latest completed turn, effectively preventing the re-compression of older context and thereby mitigating catastrophic forgetting. Our results demonstrate that FlowKV consistently and significantly outperforms baseline strategies in maintaining instruction-following accuracy and user preference retention from 10.90\% to 75.40\%, particularly in later conversational turns.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do RAG Systems Really Suffer From Positional Bias?</title>
<link>https://arxiv.org/abs/2505.15561</link>
<guid>https://arxiv.org/abs/2505.15561</guid>
<content:encoded><![CDATA[
arXiv:2505.15561v2 Announce Type: replace 
Abstract: Retrieval Augmented Generation enhances LLM accuracy by adding passages retrieved from an external corpus to the LLM prompt. This paper investigates how positional bias - the tendency of LLMs to weight information differently based on its position in the prompt - affects not only the LLM's capability to capitalize on relevant passages, but also its susceptibility to distracting passages. Through extensive experiments on three benchmarks, we show how state-of-the-art retrieval pipelines, while attempting to retrieve relevant passages, systematically bring highly distracting ones to the top ranks, with over 60% of queries containing at least one highly distracting passage among the top-10 retrieved passages. As a result, the impact of the LLM positional bias, which in controlled settings is often reported as very prominent by related works, is actually marginal in real scenarios since both relevant and distracting passages are, in turn, penalized. Indeed, our findings reveal that sophisticated strategies that attempt to rearrange the passages based on LLM positional preferences do not perform better than random shuffling.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis</title>
<link>https://arxiv.org/abs/2505.16834</link>
<guid>https://arxiv.org/abs/2505.16834</guid>
<content:encoded><![CDATA[
arXiv:2505.16834v3 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) systems have advanced large language models (LLMs) in complex deep search scenarios requiring multi-step reasoning and iterative information retrieval. However, existing approaches face critical limitations that lack high-quality training trajectories or suffer from the distributional mismatches in simulated environments and prohibitive computational costs for real-world deployment. This paper introduces SimpleDeepSearcher, a lightweight yet effective framework that bridges this gap through strategic data engineering rather than complex training paradigms. Our approach synthesizes high-quality training data by simulating realistic user interactions in live web search environments, coupled with a multi-criteria curation strategy that optimizes the diversity and quality of input and output side. Experiments on five benchmarks across diverse domains demonstrate that SFT on only 871 curated samples yields significant improvements over RL-based baselines. Our work establishes SFT as a viable pathway by systematically addressing the data-scarce bottleneck, offering practical insights for efficient deep search systems. Our code is available at https://github.com/RUCAIBox/SimpleDeepSearcher.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Unreasonable Effectiveness of Model Merging for Cross-Lingual Transfer in LLMs</title>
<link>https://arxiv.org/abs/2505.18356</link>
<guid>https://arxiv.org/abs/2505.18356</guid>
<content:encoded><![CDATA[
arXiv:2505.18356v2 Announce Type: replace 
Abstract: Large language models (LLMs) still struggle across tasks outside of high-resource languages. In this work, we investigate cross-lingual transfer to lower-resource languages where task-specific post-training data is scarce. Building on prior work, we first validate that the subsets of model parameters that matter most for mathematical reasoning and multilingual capabilities are distinctly non-overlapping. To exploit this implicit separability between task and target language parameterization, we develop and analyze numerous modular frameworks to improve the composition of the two during fine-tuning. These methods generally employ freezing parameters or post hoc model merging to assign math and language improvement to different key parts of the LLM. In the absence of in-language math data, we demonstrate that the modular approaches successfully improve upon baselines across three languages, four models, and two fine-tuning paradigms (full and LoRA). Furthermore, we identify the most consistently successful modular method to be fine-tuning separate language and math experts and model merging via Layer-Swapping, somewhat surprisingly. We offer possible explanations for this result via recent works on the linearity of task vectors. We further explain this by empirically showing that reverting less useful fine-tuning updates after training often outperforms freezing them from the start.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guiding Giants: Lightweight Controllers for Weighted Activation Steering in LLMs</title>
<link>https://arxiv.org/abs/2505.20309</link>
<guid>https://arxiv.org/abs/2505.20309</guid>
<content:encoded><![CDATA[
arXiv:2505.20309v2 Announce Type: replace 
Abstract: Controlling undesirable Large Language Model (LLM) behaviors, such as the generation of unsafe content or failing to adhere to safety guidelines, often relies on costly fine-tuning. Activation steering provides an alternative for inference-time control, but existing methods typically lack fine-grained, adaptive mechanisms. We introduce a novel approach using a lightweight, trainable controller network integrated during inference. This controller network observes specific intermediate LLM activations and predicts both a global scaling factor and layer-specific weights. The predicted global scaling factor and layer-specific weights then dynamically modulate the intensity of a steering patch, derived from a pre-computed "refusal direction" vector, applied across the LLM's layers during generation. Trained on activations from both harmful and benign prompts, our controller learns to discriminatively apply nuanced, layer-aware interventions, activating steering primarily for harmful inputs. Experiments using safety benchmarks like ToxicChat & In-The-Wild Jailbreak Prompts demonstrate that our weighted steering controller significantly increases refusal rates compared to the base LLM, achieving targeted behavioral modification without altering the original model parameters. Our experiments with Llama-3.1-8B, Llama-3.2-1B & Mistral-7B show our approach outperforms existing methods, presenting an efficient and adaptive method for fine-grained control over LLM behavior at inference time.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>360-LLaMA-Factory: Plug &amp; Play Sequence Parallelism for Long Post-Training</title>
<link>https://arxiv.org/abs/2505.22296</link>
<guid>https://arxiv.org/abs/2505.22296</guid>
<content:encoded><![CDATA[
arXiv:2505.22296v2 Announce Type: replace 
Abstract: Adding sequence parallelism into LLaMA-Factory, we open-sourced 360-LLaMA-Factory at https://github.com/Qihoo360/360-LLaMA-Factory. 360-LLaMA-Factory has received wide recognition and used in models such as Light-R1 arXiv:2503.10460, TinyR1 arXiv:2503.04872, Kaggle AIMO math models and also in large companies' training frameworks. This technical report delves deeper into the different sequence parallel modes behind 360-LLaMA-Factory and discusses our implementation insights.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiTEx: A Linguistic Taxonomy of Explanations for Understanding Within-Label Variation in Natural Language Inference</title>
<link>https://arxiv.org/abs/2505.22848</link>
<guid>https://arxiv.org/abs/2505.22848</guid>
<content:encoded><![CDATA[
arXiv:2505.22848v5 Announce Type: replace 
Abstract: There is increasing evidence of Human Label Variation (HLV) in Natural Language Inference (NLI), where annotators assign different labels to the same premise-hypothesis pair. However, within-label variation--cases where annotators agree on the same label but provide divergent reasoning--poses an additional and mostly overlooked challenge. Several NLI datasets contain highlighted words in the NLI item as explanations, but the same spans on the NLI item can be highlighted for different reasons, as evidenced by free-text explanations, which offer a window into annotators' reasoning. To systematically understand this problem and gain insight into the rationales behind NLI labels, we introduce LITEX, a linguistically-informed taxonomy for categorizing free-text explanations in English. Using this taxonomy, we annotate a subset of the e-SNLI dataset, validate the taxonomy's reliability, and analyze how it aligns with NLI labels, highlights, and explanations. We further assess the taxonomy's usefulness in explanation generation, demonstrating that conditioning generation on LITEX yields explanations that are linguistically closer to human explanations than those generated using only labels or highlights. Our approach thus not only captures within-label variation but also shows how taxonomy-guided generation for reasoning can bridge the gap between human and model explanations more effectively than existing strategies.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfiMed: Low-Resource Medical MLLMs with Advancing Understanding and Reasoning</title>
<link>https://arxiv.org/abs/2505.23867</link>
<guid>https://arxiv.org/abs/2505.23867</guid>
<content:encoded><![CDATA[
arXiv:2505.23867v3 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable progress in domains such as visual understanding and mathematical reasoning. However, their application in the medical domain is constrained by two key challenges: (1) multimodal medical datasets are scarce and often contain sparse information, limiting reasoning depth; and (2) Reinforcement Learning with Verifiable Rewards (RLVR), though effective in general domains, cannot reliably improve model performance in the medical domain. To overcome these challenges, during the supervised fine-tuning (SFT) stage, we incorporate high-quality textual reasoning data and general multimodal data alongside multimodal medical data to efficiently enhance foundational medical capabilities and restore the base model's reasoning ability. Moreover, considering that there are some multimodal medical datasets with sparse information, we further synthesize reflective-pattern-injected chain-of-thought (CoT) in addition to general CoT samples, equipping the model with initial reflective reasoning capabilities that provide a structured foundation for subsequent RLVR training. Finally, we introduce our InfiMed-Series models, InfiMed-SFT-3B and InfiMed-RL-3B, both of which deliver state-of-the-art performance across seven multimodal medical benchmarks. Notably, InfiMed-RL-3B achieves an average accuracy of 59.2%, outperforming even larger models like InternVL3-8B, which achieves 57.3%. Specifically, during the SFT phase, we utilized 188K samples, while the RLVR phase incorporated 36K samples, demonstrating the efficacy of both training strategies in achieving superior performance. We also conducted a series of extensive experiments, which provide valuable insights that contribute to advancing the performance of MLLMs in medical scenarios.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoMind: Adaptive Knowledgeable Agent for Automated Data Science</title>
<link>https://arxiv.org/abs/2506.10974</link>
<guid>https://arxiv.org/abs/2506.10974</guid>
<content:encoded><![CDATA[
arXiv:2506.10974v3 Announce Type: replace 
Abstract: Large Language Model (LLM) agents have shown great potential in addressing real-world data science problems. LLM-driven data science agents promise to automate the entire machine learning pipeline, yet their real-world effectiveness remains limited. Existing frameworks depend on rigid, pre-defined workflows and inflexible coding strategies; consequently, they excel only on relatively simple, classical problems and fail to capture the empirical expertise that human practitioners bring to complex, innovative tasks. In this work, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework that overcomes these deficiencies through three key advances: (1) a curated expert knowledge base that grounds the agent in domain expert knowledge, (2) an agentic knowledgeable tree search algorithm that strategically explores possible solutions, and (3) a self-adaptive coding strategy that dynamically tailors code generation to task complexity. Evaluations on two automated data science benchmarks demonstrate that AutoMind delivers superior performance versus state-of-the-art baselines. Additional analyses confirm favorable effectiveness, efficiency, and qualitative solution quality, highlighting AutoMind as an efficient and robust step toward fully automated data science. Code is at https://github.com/innovatingAI/AutoMind.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIST: Towards Multi-dimensional Implicit BiaS Evaluation of LLMs via Theory of Mind</title>
<link>https://arxiv.org/abs/2506.14161</link>
<guid>https://arxiv.org/abs/2506.14161</guid>
<content:encoded><![CDATA[
arXiv:2506.14161v2 Announce Type: replace 
Abstract: Theory of Mind (ToM) in Large Language Models (LLMs) refers to their capacity for reasoning about mental states, yet failures in this capacity often manifest as systematic implicit bias. Evaluating this bias is challenging, as conventional direct-query methods are susceptible to social desirability effects and fail to capture its subtle, multi-dimensional nature. To this end, we propose an evaluation framework that leverages the Stereotype Content Model (SCM) to reconceptualize bias as a multi-dimensional failure in ToM across Competence, Sociability, and Morality. The framework introduces two indirect tasks: the Word Association Bias Test (WABT) to assess implicit lexical associations and the Affective Attribution Test (AAT) to measure covert affective leanings, both designed to probe latent stereotypes without triggering model avoidance. Extensive experiments on 8 State-of-the-Art LLMs demonstrate our framework's capacity to reveal complex bias structures, including pervasive sociability bias, multi-dimensional divergence, and asymmetric stereotype amplification, thereby providing a more robust methodology for identifying the structural nature of implicit bias.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PredGen: Accelerated Inference of Large Language Models through Input-Time Speculation for Real-Time Speech Interaction</title>
<link>https://arxiv.org/abs/2506.15556</link>
<guid>https://arxiv.org/abs/2506.15556</guid>
<content:encoded><![CDATA[
arXiv:2506.15556v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are widely used in real-time voice chat applications, typically in combination with text-to-speech (TTS) systems to generate audio responses. However, their large size often leads to noticeable latency between the end of user input and the start of audio output, resulting in suboptimal user experiences. This latency is particularly evident when LLMs are deployed as single-user voice assistants on consumer-grade hardware with limited computing capacity. We discovered that this latency is primarily dominated by the time it takes for the LLMs to generate the first sentence, which is required as input by the TTS systems that synthesize audio responses on a sentence-by-sentence basis. To address this bottleneck, we propose Predictive Generation (PredGen), a novel framework that mitigates-or even eliminates-this delay through speculative decoding at input time. PredGen generates candidate responses while the user is still speaking, enabling the system to begin TTS processing with minimal delay. Simulated experiments on the Lmsys and MT-Bench datasets show that the proposed method can effectively reduce the latency by around 2x across a wide range of use cases, while incurring only minimal additional computation cost at input time-computation that would otherwise go unused.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do LLMs Overthink Basic Math Reasoning? Benchmarking the Accuracy-Efficiency Tradeoff in Language Models</title>
<link>https://arxiv.org/abs/2507.04023</link>
<guid>https://arxiv.org/abs/2507.04023</guid>
<content:encoded><![CDATA[
arXiv:2507.04023v2 Announce Type: replace 
Abstract: Large language models (LLMs) achieve impressive performance on complex mathematical benchmarks yet sometimes fail on basic math reasoning while generating unnecessarily verbose responses. In this paper, we present a systematic benchmark and comprehensive empirical study to evaluate the efficiency of reasoning in LLMs, focusing on the fundamental tradeoff between accuracy and overthinking. First, we formalize the accuracy-verbosity tradeoff. Second, we introduce the Overthinking Score, a harmonic-mean metric combining accuracy and token-efficiency for holistic model evaluation. Third, we establish an evaluation protocol with dynamically-generated data across 14 basic math tasks. Fourth, we conduct a large-scale empirical study evaluating 53 LLMs, including reasoning and quantized variants across different reasoning budgets. Our findings reveal: 1) model performance on complex benchmarks does not translate directly to basic math reasoning; 2) reasoning models generate ~18 more tokens while sometimes achieving lower accuracy and exhibit catastrophic collapse when token is constrained, dropping by ~28; 3) the accuracy-verbosity relationship is non-monotonic with extended reasoning budgets yielding diminishing returns (GPT-5/o-series models show zero accuracy gain from low -> medium -> high reasoning effort). Our findings challenge the assumption that longer reasoning in LLMs necessarily improves mathematical reasoning.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation</title>
<link>https://arxiv.org/abs/2507.18562</link>
<guid>https://arxiv.org/abs/2507.18562</guid>
<content:encoded><![CDATA[
arXiv:2507.18562v2 Announce Type: replace 
Abstract: Multimodal Machine Translation (MMT) has demonstrated the significant help of visual information in machine translation. However, existing MMT methods face challenges in leveraging the modality gap by enforcing rigid visual-linguistic alignment whilst being confined to inference within their trained multimodal domains. In this work, we construct novel multimodal scene graphs to preserve and integrate modality-specific information and introduce GIIFT, a two-stage Graph-guided Inductive Image-Free MMT framework that uses a cross-modal Graph Attention Network adapter to learn multimodal knowledge in a unified fused space and inductively generalize it to broader image-free translation domains. Experimental results on the Multi30K dataset of English-to-French and English-to-German tasks demonstrate that our GIIFT surpasses existing approaches and achieves the state-of-the-art, even without images during inference. Results on the WMT benchmark show significant improvements over the image-free translation baselines, demonstrating the strength of GIIFT towards inductive image-free inference.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Show or Tell? Modeling the evolution of request-making in Human-LLM conversations</title>
<link>https://arxiv.org/abs/2508.01213</link>
<guid>https://arxiv.org/abs/2508.01213</guid>
<content:encoded><![CDATA[
arXiv:2508.01213v2 Announce Type: replace 
Abstract: Designing user-centered LLM systems requires understanding how people use them, but patterns of user behavior are often masked by the variability of queries. In this work, we introduce a new framework to describe request-making that segments user input into request content, roles assigned, query-specific context, and the remaining task-independent expressions. We apply the workflow to create and analyze a dataset of 211k real-world queries based on WildChat. Compared with similar human-human setups, we find significant differences in the language for request-making in the human-LLM scenario. Further, we introduce a novel and essential perspective of diachronic analyses with user expressions, which reveals fundamental and habitual user-LLM interaction patterns beyond individual task completion. We find that query patterns evolve from early ones emphasizing sole requests to combining more context later on, and individual users explore expression patterns but tend to converge with more experience. From there, we propose to understand communal trends of expressions underlying distinct tasks and discuss the preliminary findings. Finally, we discuss the key implications for user studies, computational pragmatics, and LLM alignment.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProCut: LLM Prompt Compression via Attribution Estimation</title>
<link>https://arxiv.org/abs/2508.02053</link>
<guid>https://arxiv.org/abs/2508.02053</guid>
<content:encoded><![CDATA[
arXiv:2508.02053v2 Announce Type: replace 
Abstract: In large-scale industrial LLM systems, prompt templates often expand to thousands of tokens as teams iteratively incorporate sections such as task instructions, few-shot examples, and heuristic rules to enhance robustness and coverage. This expansion leads to bloated prompts that are difficult to maintain and incur significant inference latency and serving costs. To address this, we introduce Prompt Compression via Attribution Estimation (ProCut), a flexible, LLM-agnostic, training-free framework that compresses prompts through attribution analysis. ProCut segments prompt templates into semantically meaningful units, quantifies their impact on task performance, and prunes low-utility components. Through extensive experiments on five public benchmark datasets and real-world industrial prompts, we show that ProCut achieves substantial prompt size reductions (78% fewer tokens in production) while maintaining or even slightly improving task performance (up to 62% better than alternative methods). We further introduce an LLM-driven attribution estimator that reduces compression latency by over 50%, and demonstrate that ProCut integrates seamlessly with existing prompt-optimization frameworks to produce concise, high-performing prompts.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thinking with Nothinking Calibration: A New In-Context Learning Paradigm in Reasoning Large Language Models</title>
<link>https://arxiv.org/abs/2508.03363</link>
<guid>https://arxiv.org/abs/2508.03363</guid>
<content:encoded><![CDATA[
arXiv:2508.03363v3 Announce Type: replace 
Abstract: Reasoning large language models (RLLMs) have recently demonstrated remarkable capabilities through structured and multi-step reasoning. While prior research has primarily focused on improving their training and inference strategies, their potential for in-context learning (ICL) remains largely underexplored. To fill this gap, we propose Thinking with Nothinking Calibration (JointThinking), a new ICL paradigm that prompts the model to generate two answers in parallel: one in Thinking mode and the other in Nothinking mode. A second round of Thinking is triggered only when the two initial responses are inconsistent, using a single prompt with two different answers. Extensive experiments across multiple reasoning benchmarks demonstrate that JointThinking significantly outperforms few-shot chain-of-thought (CoT), thinking twice and majority voting. Moreover, it achieves comparable in-distribution performance to training-based SOTA reasoning method, while substantially outperforming on out-of-distribution tasks. We further conduct a systematic analysis of the calibration mechanism, showing the importance of structural thinking diversity and the benefits of consistency check. Additionally, we observe that the performance gap between actual and ideal reasoning narrows as model size increases in the second thinking, indicating the strong scalability of our approach. Finally, we discuss current limitations and outline promising directions for future ICL research in RLLMs.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sotopia-RL: Reward Design for Social Intelligence</title>
<link>https://arxiv.org/abs/2508.03905</link>
<guid>https://arxiv.org/abs/2508.03905</guid>
<content:encoded><![CDATA[
arXiv:2508.03905v3 Announce Type: replace 
Abstract: Social intelligence has become a critical capability for large language models (LLMs), enabling them to engage effectively in real-world social tasks such as collaboration and negotiation. Reinforcement learning (RL) is a natural fit for training socially intelligent agents because it allows models to learn sophisticated strategies directly through social interactions without requiring human annotations. However, there are two unique parts about social intelligence tasks: (1) the quality of individual utterances in social interactions is not strictly related to final success; (2) social interactions require multi-dimensional rubrics for success. Therefore, we argue that it is necessary to design rewards for building utterance-level multi-dimensional reward models to facilitate RL training for social intelligence tasks. To address these challenges, we propose Sotopia-RL, a novel framework that refines coarse episode-level feedback into utterance-level, multi-dimensional rewards. Utterance-level credit assignment attributes outcomes to individual utterances, while multi-dimensional rewards capture the full richness of social interactions and reduce reward hacking. Experiments in Sotopia, an open-ended social learning environment, demonstrate that Sotopia-RL achieves state-of-the-art social goal completion scores (7.17 on Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing approaches. Ablation studies confirm the necessity of both utterance-level credit assignment and multi-dimensional reward design for RL training.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Do Humans Hear When Interacting? Experiments on Selective Listening for Evaluating ASR of Spoken Dialogue Systems</title>
<link>https://arxiv.org/abs/2508.04402</link>
<guid>https://arxiv.org/abs/2508.04402</guid>
<content:encoded><![CDATA[
arXiv:2508.04402v2 Announce Type: replace 
Abstract: Spoken dialogue systems (SDSs) utilize automatic speech recognition (ASR) at the front end of their pipeline. The role of ASR in SDSs is to recognize information in user speech related to response generation appropriately. Examining selective listening of humans, which refers to the ability to focus on and listen to important parts of a conversation during the speech, will enable us to identify the ASR capabilities required for SDSs and evaluate them. In this study, we experimentally confirmed selective listening when humans generate dialogue responses by comparing human transcriptions for generating dialogue responses and reference transcriptions. Based on our experimental results, we discuss the possibility of a new ASR evaluation method that leverages human selective listening, which can identify the gap between transcription ability between ASR systems and humans.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Investigation of Robustness of LLMs in Mathematical Reasoning: Benchmarking with Mathematically-Equivalent Transformation of Advanced Mathematical Problems</title>
<link>https://arxiv.org/abs/2508.08833</link>
<guid>https://arxiv.org/abs/2508.08833</guid>
<content:encoded><![CDATA[
arXiv:2508.08833v2 Announce Type: replace 
Abstract: In this paper, we introduce a systematic framework beyond conventional method to assess LLMs' mathematical-reasoning robustness by stress-testing them on advanced math problems that are mathematically equivalent but with linguistic and parametric variation. These transformations allow us to measure the sensitivity of LLMs to non-mathematical perturbations, thereby enabling a more accurate evaluation of their mathematical reasoning capabilities. Using this new evaluation methodology, we created PutnamGAP, a new benchmark dataset with multiple mathematically-equivalent variations of competition-level math problems. With the new dataset, we evaluate multiple families of representative LLMs and examine their robustness. Across 18 commercial and open-source models we observe sharp performance degradation on the variants. OpenAI's flagship reasoning model, O3, scores 51.5% on the originals but drops by 4.7 percentage points on surface-renaming variants, and by 12.9 percentage points on parametric variants, while smaller models fare far worse. Overall, the results show that the proposed new evaluation methodology is effective for deepening our understanding of the robustness of LLMs and generating new insights for further improving their mathematical reasoning capabilities.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM Reasoning</title>
<link>https://arxiv.org/abs/2508.12726</link>
<guid>https://arxiv.org/abs/2508.12726</guid>
<content:encoded><![CDATA[
arXiv:2508.12726v3 Announce Type: replace 
Abstract: Large language models (LLMs) have achieved remarkable success in many natural language tasks but still struggle with complex, multi-step reasoning, particularly across diverse disciplines. Existing reasoning datasets often lack disciplinary breadth, reasoning depth, and diversity, and lack guiding principles for question synthesis. We propose DESIGNER: a DESIGN-logic-guidEd Reasoning data synthesis pipeline that leverages naturally available, extensive raw documents (e.g., book corpus and web corpus) to generate multidisciplinary challenging questions. We introduce the concept of "design logic" and instruct LLMs to mimic human educators' question-creation process, enabling automated synthesis of large-scale, high-difficulty questions. We use LLMs to reverse-engineer and abstract over 120,000 design logics from existing questions across various disciplines. By matching these design logics with source documents, we are able to create reasoning questions that far surpass the difficulty and diversity of existing datasets. Using this pipeline, we synthesized two large-scale reasoning datasets that span 75 disciplines: DLR-Book (3.04 million questions from the book corpus) and DLR-Web (1.66 million questions from the web corpus). Data analysis indicates that the questions synthesized by our method exhibit greater difficulty and diversity compared to those in the baseline datasets. We validate our synthesized data through supervised fine-tuning (SFT) on the Qwen3 and Llama3 model families. Our data substantially enhances their multidisciplinary reasoning capabilities, outperforming existing datasets. Notably, after SFT on our datasets, the base versions of these models even surpass their official instruction-tuned counterparts.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaled Signed Averaging Improves In-Context and Early Learning Benchmark Performance in Small Transformers</title>
<link>https://arxiv.org/abs/2508.14685</link>
<guid>https://arxiv.org/abs/2508.14685</guid>
<content:encoded><![CDATA[
arXiv:2508.14685v2 Announce Type: replace 
Abstract: While Large Language models' abilities for in-context learning (ICL) have drawn much attention, we examine some of its limitations on semantic tasks involving quantifiers like "all" and "some", as well as on tasks with linear functions. We identify Softmax, the scoring function in attention mechanism, as a contributing factor to these limitations. We propose scaled signed averaging (SSA), a novel alternative to Softmax to mitigate these problems. We show that SSA significantly improves performance on our ICL tasks. In addition, SSA outperforms transformer models with Softmax on several early learning NLP benchmarks and linguistic probing tasks on zero and few-shot settings.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ParamBench: A Graduate-Level Benchmark for Evaluating LLM Understanding on Indic Subjects</title>
<link>https://arxiv.org/abs/2508.16185</link>
<guid>https://arxiv.org/abs/2508.16185</guid>
<content:encoded><![CDATA[
arXiv:2508.16185v2 Announce Type: replace 
Abstract: Large language models have been widely evaluated on tasks such as comprehension, summarization, code generation, etc. However, their performance on graduate-level, culturally grounded questions in the Indian context remains largely unexplored. Existing Indian benchmarks emphasise basic fact-orientated queries that offer limited assessment of a deeper disciplinary understanding tailored to the Indian setting. In this paper, we present ParamBench, consisting of more than 17K questions in the Hindi language, comprising questionnaires from 21 diverse subjects. These questions are primarily derived from a nationwide graduate-level entrance examination covering topics such as history, music, instruments, yoga, literature, philosophy, law, etc.~ specifically for the Indian context. Additionally, we assess the ability of LLMs to handle diverse question formats - such as list-based matching, assertion-reason pairs, and sequence ordering - alongside conventional multiple-choice questions. We evaluated the performance of more than 16 open source LLMs on this benchmark, observing that Gemma3-27B attains the highest overall accuracy of 56.4\%. Furthermore, subject-wise analysis indicates that even for the best-performing LLMs, performance remains weak on topics such as music, classical instruments, and law, underscoring persistent challenges in culturally grounded reasoning. The dataset and source code is present at https://github.com/ayushbits/ParamBench.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.19828</link>
<guid>https://arxiv.org/abs/2508.19828</guid>
<content:encoded><![CDATA[
arXiv:2508.19828v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities across a wide range of NLP tasks, but they remain fundamentally stateless, constrained by limited context windows that hinder long-horizon reasoning. Recent efforts to address this limitation often augment LLMs with an external memory bank, yet most existing pipelines are static and heuristic-driven, lacking a learned mechanism for deciding what to store, update, or retrieve. We present Memory-R1, a reinforcement learning (RL) framework that equips LLMs with the ability to actively manage and utilize external memory through two specialized agents: a Memory Manager that learns structured operations, including ADD, UPDATE, DELETE, and NOOP; and an Answer Agent that pre-selects and reasons over relevant entries. Both agents are fine-tuned with outcome-driven RL (PPO and GRPO), enabling adaptive memory management with minimal supervision. With only 152 training QA pairs, Memory-R1 outperforms strong baselines and generalizes across diverse question types, three benchmarks (LoCoMo, MSC, LongMemEval), and multiple model scales (3B-14B).
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Percept-V Challenge: Can Multimodal LLMs Crack Simple Perception Problems?</title>
<link>https://arxiv.org/abs/2508.21143</link>
<guid>https://arxiv.org/abs/2508.21143</guid>
<content:encoded><![CDATA[
arXiv:2508.21143v2 Announce Type: replace 
Abstract: Cognitive science research treats visual perception, the ability to understand and make sense of a visual input, as one of the early developmental signs of intelligence. Its TVPS-4 framework categorizes and tests human perception into seven skills such as visual discrimination, and form constancy. Do Multimodal Large Language Models (MLLMs) match up to humans in basic perception? Even though there are many benchmarks that evaluate MLLMs on advanced reasoning and knowledge skills, there is limited research that focuses evaluation on simple perception. In response, we introduce Percept-V, a dataset containing 6000 program-generated uncontaminated images divided into 30 domains, where each domain tests one or more TVPS-4 skills. Our focus is on perception, so we make our domains quite simple and the reasoning and knowledge required for solving them are minimal. Since modern-day MLLMs can solve much more complex tasks, our a-priori expectation is that they will solve these domains very easily. Contrary to our belief, our experiments show a weak performance of SoTA proprietary and open-source MLLMs compared to very high human performance on Percept-V. We find that as number of objects in the image increases, performance goes down rather fast. Our experiments also identify the perception skills that are considerably harder for all models.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probe-Rewrite-Evaluate: A Workflow for Reliable Benchmarks and Quantifying Evaluation Awareness</title>
<link>https://arxiv.org/abs/2509.00591</link>
<guid>https://arxiv.org/abs/2509.00591</guid>
<content:encoded><![CDATA[
arXiv:2509.00591v5 Announce Type: replace 
Abstract: Large Language Models (LLMs) often exhibit significant behavioral shifts when they perceive a change from a real-world deployment context to a controlled evaluation setting, a phenomenon known as "evaluation awareness." This discrepancy poses a critical challenge for AI alignment, as benchmark performance may not accurately reflect a model's true safety and honesty. In this work, we systematically quantify these behavioral changes by manipulating the perceived context of prompts. We introduce a methodology that uses a linear probe to score prompts on a continuous scale from "test-like" to "deploy-like" and leverage an LLM rewriting strategy to shift these prompts towards a more natural, deployment-style context while preserving the original task. Using this method, we achieved a 30% increase in the average probe score across a strategic role-playing dataset after rewriting. Evaluating a suite of state-of-the-art models on these original and rewritten prompts, we find that rewritten "deploy-like" prompts induce a significant and consistent shift in behavior. Across all models, we observed an average increase in honest responses of 5.26% and a corresponding average decrease in deceptive responses of 12.40%. Furthermore, refusal rates increased by an average of 6.38%, indicating heightened safety compliance. Our findings demonstrate that evaluation awareness is a quantifiable and manipulable factor that directly influences LLM behavior, revealing that models are more prone to unsafe or deceptive outputs in perceived test environments. This underscores the urgent need for more realistic evaluation frameworks to accurately gauge true model alignment before deployment.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Injection to Defense: Constructing Edit-Based Fingerprints for Large Language Models</title>
<link>https://arxiv.org/abs/2509.03122</link>
<guid>https://arxiv.org/abs/2509.03122</guid>
<content:encoded><![CDATA[
arXiv:2509.03122v2 Announce Type: replace 
Abstract: Fingerprinting is critical for maintaining traceability and protecting the intellectual property (IP) of developers, as LLMs deployed in web applications are susceptible to unauthorized redistribution and misuse via fine-tuning or black-box deployment. However, current backdoor-based fingerprinting methods face a fundamental trade-off: fingerprints embedded as garbled text are easily detected and filtered, whereas those crafted as coherent natural language are prone to being triggered unintentionally. To overcome these limitations, we propose RFEdit, a knowledge-editing framework that embeds a rule-based multilingual natural language fingerprint (MNLF) by modifying a sparse subset of model weights. This approach enables efficient and robust fingerprint injection with minimal impact on unrelated knowledge in LLMs. Our RFEdit framework is further safeguarded by Fingerprint Subspace-aware Fine-Tuning (FSFT), which mitigates fingerprint degradation during legitimate fine-tuning by restricting parameter updates to the fingerprint subspace. This approach preserves fingerprint integrity while enhancing downstream task performance of LLMs. These advances establish a comprehensive pipeline from fingerprint injection to defense, achieving high detection effectiveness, robustness against adversarial manipulations, harmlessness to model utility, and persistence under fine-tuning. Extensive experiments demonstrate that RFEdit maintains robustness under quantization and pruning. Additionally, fingerprint effectiveness is generally improved by more than 10\% when combined with FSFT for math and alpaca downstream tasks.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Factuality in LLMs via Inference-Time Knowledge Graph Construction</title>
<link>https://arxiv.org/abs/2509.03540</link>
<guid>https://arxiv.org/abs/2509.03540</guid>
<content:encoded><![CDATA[
arXiv:2509.03540v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) often struggle with producing factually consistent answers due to limitations in their parametric memory. Retrieval-Augmented Generation (RAG) paradigms mitigate this issue by incorporating external knowledge at inference time. However, such methods typically handle knowledge as unstructured text, which reduces retrieval accuracy, hinders compositional reasoning, and amplifies the influence of irrelevant information on the factual consistency of LLM outputs. To overcome these limitations, we propose a novel framework that dynamically constructs and expands knowledge graphs (KGs) during inference, integrating both internal knowledge extracted from LLMs and external knowledge retrieved from external sources. Our method begins by extracting a seed KG from the question via prompting, followed by iterative expansion using the LLM's internal knowledge. The KG is then selectively refined through external retrieval, enhancing factual coverage and correcting inaccuracies. We evaluate our approach on three diverse Factual QA benchmarks, demonstrating consistent gains in factual accuracy over baselines. Our findings reveal that inference-time KG construction is a promising direction for enhancing LLM factuality in a structured, interpretable, and scalable manner.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TextMine: Data, Evaluation Framework and Ontology-guided LLM Pipeline for Humanitarian Mine Action</title>
<link>https://arxiv.org/abs/2509.15098</link>
<guid>https://arxiv.org/abs/2509.15098</guid>
<content:encoded><![CDATA[
arXiv:2509.15098v2 Announce Type: replace 
Abstract: Humanitarian Mine Action (HMA) addresses the challenge of detecting and removing landmines from conflict regions. Much of the life-saving operational knowledge produced by HMA agencies is buried in unstructured reports, limiting the transferability of information between agencies. To address this issue, we propose TextMine: the first dataset, evaluation framework and ontology-guided large language model (LLM) pipeline for knowledge extraction in the HMA domain. TextMine structures HMA reports into (subject, relation, object)-triples, thus creating domain-specific knowledge. To ensure real-world relevance, we created the dataset in collaboration with Cambodian Mine Action Center (CMAC). We further introduce a bias-aware evaluation framework that combines human-annotated triples with an LLM-as-Judge protocol to mitigate position bias in reference-free scoring. Our experiments show that ontology-aligned prompts improve extraction accuracy by up to 44.2%, reduce hallucinations by 22.5%, and enhance format adherence by 20.9% compared to baseline models. We publicly release the dataset and code.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMARTER: A Data-efficient Framework to Improve Toxicity Detection with Explanation via Self-augmenting Large Language Models</title>
<link>https://arxiv.org/abs/2509.15174</link>
<guid>https://arxiv.org/abs/2509.15174</guid>
<content:encoded><![CDATA[
arXiv:2509.15174v2 Announce Type: replace 
Abstract: WARNING: This paper contains examples of offensive materials. To address the proliferation of toxic content on social media, we introduce SMARTER, we introduce SMARTER, a data-efficient two-stage framework for explainable content moderation using Large Language Models (LLMs). In Stage 1, we leverage LLMs' own outputs to generate synthetic explanations for both correct and incorrect labels, enabling alignment via preference optimization with minimal human supervision. In Stage 2, we refine explanation quality through cross-model training, allowing weaker models to align stylistically and semantically with stronger ones. Experiments on three benchmark tasks -- HateXplain, Latent Hate, and Implicit Hate -- demonstrate that SMARTER enables LLMs to achieve up to a 13.5% macro-F1 improvement over standard few-shot baselines while using only a fraction of the full training data. Our framework offers a scalable strategy for low-resource settings by harnessing LLMs' self-improving capabilities for both classification and explanation.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Sound of Syntax: Finetuning and Comprehensive Evaluation of Language Models for Speech Pathology</title>
<link>https://arxiv.org/abs/2509.16765</link>
<guid>https://arxiv.org/abs/2509.16765</guid>
<content:encoded><![CDATA[
arXiv:2509.16765v2 Announce Type: replace 
Abstract: According to the U.S. National Institutes of Health, more than 3.4 million children experience speech disorders that require clinical intervention. The number of speech-language pathologists (SLPs) is roughly 20 times fewer than the number of affected children, highlighting a significant gap in children's care and a pressing need for technological support that improves the productivity of SLPs. State-of-the-art multimodal language models (MLMs) show promise for supporting SLPs, but their use remains underexplored largely due to a limited understanding of their performance in high-stakes clinical settings. To address this gap, we collaborate with domain experts to develop a taxonomy of real-world use cases of MLMs in speech-language pathologies. Building on this taxonomy, we introduce the first comprehensive benchmark for evaluating MLM across five core use cases, each containing 1,000 manually annotated data points. This benchmark includes robustness and sensitivity tests under various settings, including background noise, speaker gender, and accent. Our evaluation of 15 state-of-the-art MLMs reveals that no single model consistently outperforms others across all tasks. Notably, we find systematic disparities, with models performing better on male speakers, and observe that chain-of-thought prompting can degrade performance on classification tasks with large label spaces and narrow decision boundaries. Furthermore, we study fine-tuning MLMs on domain-specific data, achieving improvements of over 10\% compared to base models. These findings highlight both the potential and limitations of current MLMs for speech-language pathology applications, underscoring the need for further research and targeted development.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PARL-MT: Learning to Call Functions in Multi-Turn Conversation with Progress Awareness</title>
<link>https://arxiv.org/abs/2509.23206</link>
<guid>https://arxiv.org/abs/2509.23206</guid>
<content:encoded><![CDATA[
arXiv:2509.23206v2 Announce Type: replace 
Abstract: Large language models (LLMs) have achieved impressive success in single-turn function calling, yet real-world applications such as travel planning or multi-stage data analysis typically unfold across multi-turn conversations. In these settings, LLMs must not only issue accurate function calls at each step but also maintain progress awareness, the ability to summarize past interactions and plan future actions to ensure coherent, long-horizon task execution. Existing approaches, however, either reduce multi-turn training to isolated single-turn samples, which neglects task-level planning, or employ end-to-end reinforcement learning (RL) that struggles with redundancy and lacks explicit integration of progress awareness. To overcome these limitations, we introduce PARL-MT, a framework that explicitly incorporates progress awareness into LLM training for multi-turn function calling. PARL-MT combines (i) a Progress Awareness Generation (PAG) pipeline, which automatically constructs datasets coupling conversation summaries with future task planning, and (ii) a Progress Awareness-Guided Reinforcement Learning (PAG-RL) algorithm, which integrates progress awareness into RL training to reduce contextual redundancy and improve alignment between local actions and global task completion. Empirical results on two public benchmarks demonstrate that PARL-MT significantly outperforms existing methods, highlighting the effectiveness of progress awareness in enabling robust and efficient multi-turn function calling.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Hallucination Detection: HSAD</title>
<link>https://arxiv.org/abs/2509.23580</link>
<guid>https://arxiv.org/abs/2509.23580</guid>
<content:encoded><![CDATA[
arXiv:2509.23580v2 Announce Type: replace 
Abstract: Although Large Language Models have demonstrated powerful capabilities in a wide range of tasks such as language understanding and code generation, the frequent occurrence of hallucinations during the generation process has become a significant impediment to their deployment in critical application scenarios. Current mainstream hallucination detection methods rely on factual consistency verification or static hidden layer features. The former is constrained by the scope of knowledge coverage, while the latter struggles to capture reasoning biases during the inference process. To address these issues, and inspired by signal analysis methods in cognitive neuroscience, this paper proposes a hallucination detection method based on the frequency-domain analysis of hidden layer temporal signals, named HSAD (\textbf{H}idden \textbf{S}ignal \textbf{A}nalysis-based \textbf{D}etection). First, by treating the LLM's reasoning process as a cognitive journey that unfolds over time, we propose modeling and simulating the human process of signal perception and discrimination in a deception-detection scenario through hidden layer temporal signals. Next, The Fast Fourier Transform is applied to map these temporal signals into the frequency domain to construct spectral features, which are used to capture anomalies that arise during the reasoning process; analysis experiments on these spectral features have proven the effectiveness of this approach. Finally, a hallucination detection algorithm is designed based on these spectral features to identify hallucinations in the generated content. By effectively combining the modeling of the reasoning process with frequency-domain feature extraction, the HSAD method overcomes the limitations of existing approaches in terms of knowledge coverage and the detection of reasoning biases, demonstrating higher detection accuracy and robustness.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text-Based Approaches to Item Alignment to Content Standards in Large-Scale Reading &amp; Writing Tests</title>
<link>https://arxiv.org/abs/2509.26431</link>
<guid>https://arxiv.org/abs/2509.26431</guid>
<content:encoded><![CDATA[
arXiv:2509.26431v3 Announce Type: replace 
Abstract: Aligning test items to content standards is a critical step in test development to collect validity evidence based on content. Item alignment has typically been conducted by human experts. This judgmental process can be subjective and time-consuming. This study investigated the performance of fine-tuned small language models (SLMs) for automated item alignment using data from a large-scale standardized reading and writing test for college admissions. Different SLMs were trained for alignment at both domain and skill levels respectively with 10 skills mapped to 4 content domains. The model performance was evaluated in multiple criteria on two testing datasets. The impact of types and sizes of the input data for training was investigated. Results showed that including more item text data led to substantially better model performance, surpassing the improvements induced by sample size increase alone. For comparison, supervised machine learning models were trained using the embeddings from the multilingual-E5-large-instruct model. The study results showed that fine-tuned SLMs consistently outperformed the embedding-based supervised machine learning models, particularly for the more fine-grained skill alignment. To better understand model misclassifications, multiple semantic similarity analysis including pairwise cosine similarity, Kullback-Leibler divergence of embedding distributions, and two-dimension projections of item embeddings were conducted. These analyses consistently showed that certain skills in SAT and PSAT were semantically too close, providing evidence for the observed misclassification.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMVA-GEBC: Large Language Model with Video Adapter for Generic Event Boundary Captioning</title>
<link>https://arxiv.org/abs/2306.10354</link>
<guid>https://arxiv.org/abs/2306.10354</guid>
<content:encoded><![CDATA[
arXiv:2306.10354v2 Announce Type: replace-cross 
Abstract: Our winning entry for the CVPR 2023 Generic Event Boundary Captioning (GEBC) competition is detailed in this paper. Unlike conventional video captioning tasks, GEBC demands that the captioning model possess an understanding of immediate changes in status around the designated video boundary, making it a difficult task. This paper proposes an effective model LLMVA-GEBC (Large Language Model with Video Adapter for Generic Event Boundary Captioning): (1) We utilize a pretrained LLM for generating human-like captions with high quality. (2) To adapt the model to the GEBC task, we take the video Q-former as an adapter and train it with the frozen visual feature extractors and LLM. Our proposed method achieved a 76.14 score on the test set and won the first place in the challenge. Our code is available at https://github.com/zjr2000/LLMVA-GEBC .
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaunchpadGPT: Language Model as Music Visualization Designer on Launchpad</title>
<link>https://arxiv.org/abs/2307.04827</link>
<guid>https://arxiv.org/abs/2307.04827</guid>
<content:encoded><![CDATA[
arXiv:2307.04827v3 Announce Type: replace-cross 
Abstract: Launchpad is a musical instrument that allows users to create and perform music by pressing illuminated buttons. To assist and inspire the design of the Launchpad light effect, and provide a more accessible approach for beginners to create music visualization with this instrument, we proposed the LaunchpadGPT model to generate music visualization designs on Launchpad automatically. Based on the language model with excellent generation ability, our proposed LaunchpadGPT takes an audio piece of music as input and outputs the lighting effects of Launchpad-playing in the form of a video (Launchpad-playing video). We collect Launchpad-playing videos and process them to obtain music and corresponding video frame of Launchpad-playing as prompt-completion pairs, to train the language model. The experiment result shows the proposed method can create better music visualization than random generation methods and hold the potential for a broader range of music visualization applications. Our code is available at https://github.com/yunlong10/LaunchpadGPT/.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transparent and Coherent Procedural Mistake Detection</title>
<link>https://arxiv.org/abs/2412.11927</link>
<guid>https://arxiv.org/abs/2412.11927</guid>
<content:encoded><![CDATA[
arXiv:2412.11927v3 Announce Type: replace-cross 
Abstract: Procedural mistake detection (PMD) is a challenging problem of classifying whether a human user (observed through egocentric video) has successfully executed a task (specified by a procedural text). Despite significant recent efforts, machine performance in the wild remains nonviable, and the reasoning processes underlying this performance are opaque. As such, we extend PMD to require generating visual self-dialog rationales to inform decisions. Given the impressive, mature image understanding capabilities observed in recent vision-and-language models (VLMs), we curate a suitable benchmark dataset for PMD based on individual frames. As our reformulation enables unprecedented transparency, we leverage a natural language inference (NLI) model to formulate two automated metrics for the coherence of generated rationales. We establish baselines for this reframed task, showing that VLMs struggle off-the-shelf, but with some trade-offs, their accuracy, coherence, and efficiency can be improved by incorporating these metrics into common inference and fine-tuning methods. Lastly, our multi-faceted metrics visualize common outcomes, highlighting areas for further improvement.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emilia: A Large-Scale, Extensive, Multilingual, and Diverse Dataset for Speech Generation</title>
<link>https://arxiv.org/abs/2501.15907</link>
<guid>https://arxiv.org/abs/2501.15907</guid>
<content:encoded><![CDATA[
arXiv:2501.15907v2 Announce Type: replace-cross 
Abstract: Recent advancements in speech generation have been driven by large-scale training datasets. However, current models struggle to capture the spontaneity and variability inherent in real-world human speech, as they are primarily trained on audio-book datasets limited to formal, read-aloud speaking styles. To address this limitation, we introduce Emilia-Pipe, an open-source preprocessing pipeline designed to extract high-quality training data from valuable yet under-explored in-the-wild sources that capture spontaneous human speech in real-world contexts. Using Emilia-Pipe, we construct Emilia, which comprises over 101k hours of speech across six languages: English, Chinese, German, French, Japanese, and Korean. Furthermore, we expand Emilia to Emilia-Large, a dataset exceeding 216k hours, making it one of the largest open-source speech generation resources available. Extensive experiments show that Emilia-trained models produce markedly more spontaneous, human-like speech than those trained on traditional audio-book datasets, while matching their intelligibility. These models better capture diverse speaker timbres and the full spectrum of real-world conversational styles. Our work also highlights the importance of scaling dataset size for advancing speech generation performance and validates the effectiveness of Emilia for both multilingual and crosslingual speech generation tasks.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taxonomy, Opportunities, and Challenges of Representation Engineering for Large Language Models</title>
<link>https://arxiv.org/abs/2502.19649</link>
<guid>https://arxiv.org/abs/2502.19649</guid>
<content:encoded><![CDATA[
arXiv:2502.19649v5 Announce Type: replace-cross 
Abstract: Representation Engineering (RepE) is a novel paradigm for controlling the behavior of LLMs. Unlike traditional approaches that modify inputs or fine-tune the model, RepE directly manipulates the model's internal representations. As a result, it may offer more effective, interpretable, data-efficient, and flexible control over models' behavior. We present the first comprehensive survey of RepE for LLMs, reviewing the rapidly growing literature to address key questions: What RepE methods exist and how do they differ? For what concepts and problems has RepE been applied? What are the strengths and weaknesses of RepE compared to other methods? To answer these, we propose a unified framework describing RepE as a pipeline comprising representation identification, operationalization, and control. We posit that while RepE methods offer significant potential, challenges remain, including managing multiple concepts, ensuring reliability, and preserving models' performance. Towards improving RepE, we identify opportunities for experimental and methodological improvements and construct a guide for best practices.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Illusion of Progress? Assessing the Current State of Web Agents</title>
<link>https://arxiv.org/abs/2504.01382</link>
<guid>https://arxiv.org/abs/2504.01382</guid>
<content:encoded><![CDATA[
arXiv:2504.01382v4 Announce Type: replace-cross 
Abstract: As digitalization and cloud technologies evolve, the web is becoming increasingly important in the modern society. Autonomous web agents based on large language models (LLMs) hold a great potential in work automation. It is therefore important to accurately measure and monitor the progression of their capabilities. In this work, we conduct a comprehensive and rigorous assessment of the current state of web agents. Our results depict a very different picture of the competency of current agents, suggesting over-optimism in previously reported results. This gap can be attributed to shortcomings in existing benchmarks. We introduce Online-Mind2Web, an online evaluation benchmark consisting of 300 diverse and realistic tasks spanning 136 websites. It enables us to evaluate web agents under a setting that approximates how real users use these agents. To facilitate more scalable evaluation and development, we also develop a novel LLM-as-a-Judge automatic evaluation method and show that it can achieve around 85% agreement with human judgment, substantially higher than existing methods. Finally, we present the first comprehensive comparative analysis of current web agents, highlighting both their strengths and limitations to inspire future research.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controlled Agentic Planning &amp; Reasoning for Mechanism Synthesis</title>
<link>https://arxiv.org/abs/2505.17607</link>
<guid>https://arxiv.org/abs/2505.17607</guid>
<content:encoded><![CDATA[
arXiv:2505.17607v2 Announce Type: replace-cross 
Abstract: This work presents a dual-agent \ac{llm}-based reasoning framework for automated planar mechanism synthesis that tightly couples linguistic specification with symbolic representation and simulation. From a natural-language task description, the system composes symbolic constraints and equations, generates and parametrises simulation code, and iteratively refines designs via critic-driven feedback, including symbolic regression and geometric distance metrics, closing an actionable linguistic/symbolic optimisation loop. To evaluate the approach, we introduce MSynth, a benchmark of analytically defined planar trajectories. Empirically, critic feedback and iterative refinement yield large improvements (up to 90\% on individual tasks) and statistically significant gains per the Wilcoxon signed-rank test. Symbolic-regression prompts provide deeper mechanistic insight primarily when paired with larger models or architectures with appropriate inductive biases (e.g., LRM).
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Roboflow100-VL: A Multi-Domain Object Detection Benchmark for Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.20612</link>
<guid>https://arxiv.org/abs/2505.20612</guid>
<content:encoded><![CDATA[
arXiv:2505.20612v3 Announce Type: replace-cross 
Abstract: Vision-language models (VLMs) trained on internet-scale data achieve remarkable zero-shot detection performance on common objects like car, truck, and pedestrian. However, state-of-the-art models still struggle to generalize to out-of-distribution classes, tasks and imaging modalities not typically found in their pre-training. Rather than simply re-training VLMs on more visual data, we argue that one should align VLMs to new concepts with annotation instructions containing a few visual examples and rich textual descriptions. To this end, we introduce Roboflow100-VL, a large-scale collection of 100 multi-modal object detection datasets with diverse concepts not commonly found in VLM pre-training. We evaluate state-of-the-art models on our benchmark in zero-shot, few-shot, semi-supervised, and fully-supervised settings, allowing for comparison across data regimes. Notably, we find that VLMs like GroundingDINO and Qwen2.5-VL achieve less than 2% zero-shot accuracy on challenging medical imaging datasets within Roboflow100-VL, demonstrating the need for few-shot concept alignment. Lastly, we discuss our recent CVPR 2025 Foundational FSOD competition and share insights from the community. Notably, the winning team significantly outperforms our baseline by 17 mAP! Our code and dataset are available at https://github.com/roboflow/rf100-vl and https://universe.roboflow.com/rf100-vl/.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in AI Agents</title>
<link>https://arxiv.org/abs/2506.00320</link>
<guid>https://arxiv.org/abs/2506.00320</guid>
<content:encoded><![CDATA[
arXiv:2506.00320v2 Announce Type: replace-cross 
Abstract: Recent progress in reasoning with large language models (LLMs), such as DeepSeek-R1, demonstrates impressive capabilities in domains like mathematics and coding, by exhibiting complex cognitive behaviors such as verification, goal decomposition, and self-reflection. However, it is unclear what behavior is effective and what behavior is missing for long-horizon AI agents tasks. In this work, we propose Dyna-Think, a thinking framework that integrates planning with an internal world model with reasoning and acting to enhance AI agent performance. To enable Dyna-Think, we propose Dyna-Think Imitation Learning (DIT) and Dyna-Think Dyna Training (DDT). To initialize a policy with Dyna-Think, DIT reconstructs the thinking process of R1 to focus on performing world model simulation relevant to the proposed (and planned) action, and trains the policy using this reconstructed data. To enhance Dyna-Think, DDT uses a two-stage training process to first improve the agent's world modeling ability via objectives such as state prediction or critique generation, and then improve the agent's action via policy training. We evaluate our methods on OSWorld and WindowsAgentArena, and demonstrate that Dyna-Think improves the agent's in-domain and out-of-domain performance, achieving similar best-of-n performance compared to R1 while generating 2x less tokens on average. Our extensive empirical studies reveal that 1) using critique generation for world model training is effective to improve policy performance; and 2) AI agents with better performance correlate with better world modeling abilities. We believe our results suggest a promising research direction to integrate world model simulation into AI agents to enhance their reasoning, planning, and acting capabilities.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prefilled responses enhance zero-shot detection of AI-generated images</title>
<link>https://arxiv.org/abs/2506.11031</link>
<guid>https://arxiv.org/abs/2506.11031</guid>
<content:encoded><![CDATA[
arXiv:2506.11031v3 Announce Type: replace-cross 
Abstract: As AI models generate increasingly realistic images, growing concerns over potential misuse underscore the need for reliable detection. Traditional supervised detection methods depend on large, curated datasets for training and often fail to generalize to novel, out-of-domain image generators. As an alternative, we explore pre-trained Vision-Language Models (VLMs) for zero-shot detection of AI-generated images. We evaluate VLM performance on three diverse benchmarks encompassing synthetic images of human faces, objects, and animals produced by 16 different state-of-the-art image generators. While off-the-shelf VLMs perform poorly on these datasets, we find that their reasoning can be guided effectively through simple response prefilling -- a method we call Prefill-Guided Thinking (PGT). In particular, prefilling a VLM response with the task-aligned phrase "Let's examine the style and the synthesis artifacts" improves the Macro F1 scores of three widely used open-source VLMs by up to 24%.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Few-shot Keyword Spotting Performance through Pre-Trained Self-supervised Speech Models</title>
<link>https://arxiv.org/abs/2506.17686</link>
<guid>https://arxiv.org/abs/2506.17686</guid>
<content:encoded><![CDATA[
arXiv:2506.17686v2 Announce Type: replace-cross 
Abstract: Keyword Spotting plays a critical role in enabling hands-free interaction for battery-powered edge devices. Few-Shot Keyword Spotting (FS-KWS) addresses the scalability and adaptability challenges of traditional systems by enabling recognition of custom keywords with only a few examples. However, existing FS-KWS systems achieve subpar accuracy at desirable false acceptance rates, particularly in resource-constrained edge environments. To address these issues, we propose a training scheme that leverages self-supervised learning models for robust feature extraction, dimensionality reduction, and knowledge distillation. The teacher model, based on Wav2Vec 2.0 is trained using Sub-center ArcFace loss, which enhances inter-class separability and intra-class compactness. To enable efficient deployment on edge devices, we introduce attention-based dimensionality reduction and train a standard lightweight ResNet15 student model. We evaluate the proposed approach on the English portion of the Multilingual Spoken Words Corpus (MSWC) and the Google Speech Commands (GSC) datasets. Notably, the proposed training method improves the 10-shot classification accuracy from 33.4% to 74.1% on 11 classes at 1% false alarm accuracy on the GSC dataset, thus making it significantly better-suited for a real use case scenario.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality</title>
<link>https://arxiv.org/abs/2506.19807</link>
<guid>https://arxiv.org/abs/2506.19807</guid>
<content:encoded><![CDATA[
arXiv:2506.19807v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs), particularly slow-thinking models, often exhibit severe hallucination, outputting incorrect content due to an inability to accurately recognize knowledge boundaries during reasoning. While Reinforcement Learning (RL) can enhance complex reasoning abilities, its outcome-oriented reward mechanism often lacks factual supervision over the thinking process, further exacerbating the hallucination problem. To address the high hallucination in slow-thinking models, we propose Knowledge-enhanced RL, KnowRL. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. This targeted factual input during RL training enables the model to learn and internalize fact-based reasoning strategies. By directly rewarding adherence to facts within the reasoning steps, KnowRL fosters a more reliable thinking process. Experimental results on three hallucination evaluation datasets and two reasoning evaluation datasets demonstrate that KnowRL effectively mitigates hallucinations in slow-thinking models while maintaining their original strong reasoning capabilities. Our code is available at https://github.com/zjunlp/KnowRL.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can AI Have a Personality? Prompt Engineering for AI Personality Simulation: A Chatbot Case Study in Gender-Affirming Voice Therapy Training</title>
<link>https://arxiv.org/abs/2508.18234</link>
<guid>https://arxiv.org/abs/2508.18234</guid>
<content:encoded><![CDATA[
arXiv:2508.18234v2 Announce Type: replace-cross 
Abstract: This thesis investigates whether large language models (LLMs) can be guided to simulate a consistent personality through prompt engineering. The study explores this concept within the context of a chatbot designed for Speech-Language Pathology (SLP) student training, specifically focused on gender-affirming voice therapy. The chatbot, named Monae Jackson, was created to represent a 32-year-old transgender woman and engage in conversations simulating client-therapist interactions. Findings suggest that with prompt engineering, the chatbot maintained a recognizable and consistent persona and had a distinct personality based on the Big Five Personality test. These results support the idea that prompt engineering can be used to simulate stable personality characteristics in AI chatbots.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Membership Inference Attacks on LLM-based Recommender Systems</title>
<link>https://arxiv.org/abs/2508.18665</link>
<guid>https://arxiv.org/abs/2508.18665</guid>
<content:encoded><![CDATA[
arXiv:2508.18665v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) based Recommender Systems (RecSys) can flexibly adapt recommendation systems to different domains. It utilizes in-context learning (ICL), i.e., the prompts, to customize the recommendation functions, which include sensitive historical user-specific item interactions, e.g., implicit feedback like clicked items or explicit product reviews. Such private information may be exposed to novel privacy attack. However, no study has been done on this important issue. We design four membership inference attacks (MIAs), aiming to reveal whether victims' historical interactions have been used by system prompts. They are \emph{direct inquiry, hallucination, similarity, and poisoning attacks}, each of which utilizes the unique features of LLMs or RecSys. We have carefully evaluated them on three LLMs that have been used to develop ICL-LLM RecSys and two well-known RecSys benchmark datasets. The results confirm that the MIA threat on LLM RecSys is realistic: direct inquiry and poisoning attacks showing significantly high attack advantages. We have also analyzed the factors affecting these attacks, such as the number of shots in system prompts and the position of the victim in the shots.
]]></content:encoded>
<pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>v1: Learning to Point Visual Tokens for Multimodal Grounded Reasoning</title>
<link>https://arxiv.org/abs/2505.18842</link>
<guid>https://arxiv.org/abs/2505.18842</guid>
<content:encoded><![CDATA[
<div> active visual referencing, reasoning, image processing, multimodal reasoning, grounding

Summary: 
The study addresses the limitation of existing models in processing images only once during reasoning by introducing v1, a lightweight extension enabling active visual referencing. This approach allows the model to identify relevant image patches and incorporate them back into the reasoning stream, ensuring hypotheses remain grounded in perceptual evidence. The pointing strategy of v1 enables direct selection of image patches using their semantic representations as keys, keeping perceptual evidence embedded in the same space as the model's reasoning. To train this capability, a dataset v1g of multimodal reasoning traces with visual grounding annotations is introduced. The v1 model consistently outperforms comparable baselines across various multimodal mathematical reasoning benchmarks, showcasing the effectiveness of point-and-copy as a practical mechanism for grounded reasoning. The model checkpoint and dataset are available for further exploration. 

<br /><br />Summary: <div>
arXiv:2505.18842v4 Announce Type: replace 
Abstract: When thinking with images, humans rarely rely on a single glance: they revisit visual information repeatedly during reasoning. However, existing models typically process images only once and thereafter generate reasoning entirely in text, lacking mechanisms to re-access or ground inference in visual representations. We empirically confirm this: as reasoning chains lengthen, models progressively lose focus on relevant regions. In response, we introduce v1, a lightweight extension that enables active visual referencing through a simple point-and-copy approach. This allows the model to identify relevant image patches and copy their embeddings back into the reasoning stream, ensuring that evolving hypotheses remain grounded in perceptual evidence. Crucially, our pointing strategy lets the MLLM directly select image patches using their semantic representations as keys, keeping perceptual evidence embedded in the same space as the model's reasoning. To train this capability, we construct v1g, a dataset of 300K multimodal reasoning traces with interleaved visual grounding annotations. Across various multimodal mathematical reasoning benchmarks, v1 consistently outperforms comparable baselines, establishing point-and-copy as a practical mechanism for grounded reasoning. The model checkpoint and dataset are available at github.com/jun297/v1.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing Algorithmic Bias in Language-Based Depression Detection: A Comparison of DNN and LLM Approaches</title>
<link>https://arxiv.org/abs/2509.25795</link>
<guid>https://arxiv.org/abs/2509.25795</guid>
<content:encoded><![CDATA[
<div> algorithmic bias, language-based models, depression detection, gender disparities, race/ethnicity disparities

Summary:
This paper explores algorithmic bias in language-based models for automated depression detection, specifically focusing on gender and race/ethnicity disparities. Deep neural network (DNN) based embeddings are compared to few-shot learning approaches with large language models (LLMs) using clinical interview transcripts from the DAIC-WOZ dataset. LLMs outperform DNN-based models in depression classification, especially for Hispanic participants, while also showing reduced gender bias. However, racial disparities persist in LLMs. Fairness-aware techniques such as worst-group loss show promise in mitigating bias in DNN-based embeddings, while guided prompting with ethical framing helps reduce gender bias in LLMs with 1-shot learning. Increasing the number of shots does not further reduce disparities. Strategies for race/ethnicity disparities in LLMs are less effective, suggesting the need for more research in this area. <div>
arXiv:2509.25795v2 Announce Type: replace 
Abstract: This paper investigates algorithmic bias in language-based models for automated depression detection, focusing on socio-demographic disparities related to gender and race/ethnicity. Models trained using deep neural networks (DNN) based embeddings are compared to few-shot learning approaches with large language models (LLMs), evaluating both performance and fairness on clinical interview transcripts from the Distress Analysis Interview Corpus/Wizard-of-Oz (DAIC-WOZ). To mitigate bias, fairness-aware loss functions are applied to DNN-based models, while in-context learning with varied prompt framing and shot counts is explored for LLMs. Results indicate that LLMs outperform DNN-based models in depression classification, particularly for underrepresented groups such as Hispanic participants. LLMs also exhibit reduced gender bias compared to DNN-based embeddings, though racial disparities persist. Among fairness-aware techniques for mitigating bias in DNN-based embeddings, the worst-group loss, which is designed to minimize loss for the worst-performing demographic group, achieves a better balance between performance and fairness. In contrast, the fairness-regularized loss minimizes loss across all groups but performs less effectively. In LLMs, guided prompting with ethical framing helps mitigate gender bias in the 1-shot setting. However, increasing the number of shots does not lead to further reductions in disparities. For race/ethnicity, neither prompting strategy nor increasing $N$ in $N$-shot learning effectively reduces disparities.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative and Proactive Management of Task-Oriented Conversations</title>
<link>https://arxiv.org/abs/2510.05110</link>
<guid>https://arxiv.org/abs/2510.05110</guid>
<content:encoded><![CDATA[
<div> Keywords: Task oriented dialogue systems, Natural language processing, Large language models, Information state approach, Dialogue management

Summary:
Task oriented dialogue systems aim to complete tasks based on user preferences through natural language interactions. This paper introduces a model that focuses on effective goal-aware planning in task-oriented conversations by incorporating constructive intermediate information. Predefined slots and text part informational components are used to model user preferences, while critical circumstances are identified to create limited information states. Dialogue moves are defined to indicate movement between these states and the procedures involved. An update strategy is also constructed. Leveraging in-context learning of large language models, the model uses database queries based on predefined slots and the order of retrieved entities to pass corresponding entities in the order of congruency. Evaluation on MultiWOZ dataset shows improved performance in terms of inform and success metrics compared to previous methods.<br /><br />Summary: <div>
arXiv:2510.05110v1 Announce Type: new 
Abstract: Task oriented dialogue systems (TOD) complete particular tasks based on user preferences across natural language interactions. Considering the impressive performance of large language models (LLMs) in natural language processing (NLP) tasks, most of the latest TODs are centered on LLMs. While proactive planning is crucial for task completion, many existing TODs overlook effective goal-aware planning. This paper creates a model for managing task-oriented conversations, conceptualized centered on the information state approach to dialogue management. The created model incorporated constructive intermediate information in planning. Initially, predefined slots and text part informational components are created to model user preferences. Investigating intermediate information, critical circumstances are identified. Informational components corresponding to these circumstances are created. Possible configurations for these informational components lead to limited information states. Then, dialogue moves, which indicate movement between these information states and the procedures that must be performed in the movements, are created. Eventually, the update strategy is constructed. The created model is implemented leveraging in-context learning of LLMs. In this model,  database queries are created centered on indicated predefined slots and the order of retrieved entities is indicated centered on text part. This mechanism enables passing the whole corresponding entities to the preferences in the order of congruency. Evaluations exploiting the complete test conversations of MultiWOZ, with no more than a domain in a conversation, illustrate maximal inform and success, and improvement compared with previous methods.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trainable Reference-Based Evaluation Metric for Identifying Quality of English-Gujarati Machine Translation System</title>
<link>https://arxiv.org/abs/2510.05113</link>
<guid>https://arxiv.org/abs/2510.05113</guid>
<content:encoded><![CDATA[
<div> supervised learning, machine translation evaluation, Indian languages, Gujarati, metrics <br />
<br />
Summary: 
Machine Translation (MT) Evaluation is crucial for assessing the performance of MT systems. In this study, a reference-based MT evaluation metric for Gujarati was developed using supervised learning. Two models with different hidden layers were trained using 25 features. The performance of the metric was tested using 1000 MT outputs from seven systems compared to a human reference translation. The developed metrics showed stronger human correlations compared to other existing metrics. This research highlights the need for language-specific evaluation metrics to improve MT performance in Indian languages like Gujarati. <div>
arXiv:2510.05113v1 Announce Type: new 
Abstract: Machine Translation (MT) Evaluation is an integral part of the MT development life cycle. Without analyzing the outputs of MT engines, it is impossible to evaluate the performance of an MT system. Through experiments, it has been identified that what works for English and other European languages does not work well with Indian languages. Thus, In this paper, we have introduced a reference-based MT evaluation metric for Gujarati which is based on supervised learning. We have trained two versions of the metric which uses 25 features for training. Among the two models, one model is trained using 6 hidden layers with 500 epochs while the other model is trained using 10 hidden layers with 500 epochs. To test the performance of the metric, we collected 1000 MT outputs of seven MT systems. These MT engine outputs were compared with 1 human reference translation. While comparing the developed metrics with other available metrics, it was found that the metrics produced better human correlations.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hallucination is Inevitable for LLMs with the Open World Assumption</title>
<link>https://arxiv.org/abs/2510.05116</link>
<guid>https://arxiv.org/abs/2510.05116</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, hallucination, artificial general intelligence, Closed World assumption, Open World assumption

Summary: 
Large Language Models (LLMs) demonstrate strong linguistic capabilities but often produce inaccurate outputs known as "hallucinations." Engineering approaches usually view hallucination as a flaw to be minimized, while formal analyses suggest it is theoretically inevitable. This paper redefines hallucination as a manifestation of the generalization problem, which is crucial for achieving artificial general intelligence (AGI). Under the Closed World assumption, where training and test distributions align, hallucinations can be reduced. However, under the Open World assumption with an unbounded environment, hallucinations become unavoidable. The paper classifies hallucinations into correctable and unavoidable categories under open-world conditions. It argues that hallucination should not be seen solely as an engineering defect but as a structural aspect to be managed and harmonized with human intelligence. 

<br /><br />Summary: <div>
arXiv:2510.05116v1 Announce Type: new 
Abstract: Large Language Models (LLMs) exhibit impressive linguistic competence but also produce inaccurate or fabricated outputs, often called ``hallucinations''. Engineering approaches usually regard hallucination as a defect to be minimized, while formal analyses have argued for its theoretical inevitability. Yet both perspectives remain incomplete when considering the conditions required for artificial general intelligence (AGI). This paper reframes ``hallucination'' as a manifestation of the generalization problem. Under the Closed World assumption, where training and test distributions are consistent, hallucinations may be mitigated. Under the Open World assumption, however, where the environment is unbounded, hallucinations become inevitable. This paper further develops a classification of hallucination, distinguishing cases that may be corrected from those that appear unavoidable under open-world conditions. On this basis, it suggests that ``hallucination'' should be approached not merely as an engineering defect but as a structural feature to be tolerated and made compatible with human intelligence.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Structured Knowledge: Advancing Triple Extraction from Regional Trade Agreements using Large Language Models</title>
<link>https://arxiv.org/abs/2510.05121</link>
<guid>https://arxiv.org/abs/2510.05121</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, structured knowledge extraction, Subject-Predicate-Object triples, Economics, trade agreements

Summary:
The study explores the effectiveness of Large Language Models (LLMs) in extracting structured knowledge, focusing on Economics applications. It specifically investigates the extraction of Subject-Predicate-Object triples from regional trade agreement texts. The study evaluates the performance of zero-shot, one-shot, and few-shot prompting techniques along with positive and negative examples using the Llama 3.1 model. Insights, challenges, and potential future directions in using language models for economic applications are discussed. The findings can be applied to create economic trade knowledge graphs from natural language texts, highlighting the significance of language models in this domain. <div>
arXiv:2510.05121v1 Announce Type: new 
Abstract: This study investigates the effectiveness of Large Language Models (LLMs) for the extraction of structured knowledge in the form of Subject-Predicate-Object triples. We apply the setup for the domain of Economics application. The findings can be applied to a wide range of scenarios, including the creation of economic trade knowledge graphs from natural language legal trade agreement texts. As a use case, we apply the model to regional trade agreement texts to extract trade-related information triples. In particular, we explore the zero-shot, one-shot and few-shot prompting techniques, incorporating positive and negative examples, and evaluate their performance based on quantitative and qualitative metrics. Specifically, we used Llama 3.1 model to process the unstructured regional trade agreement texts and extract triples. We discuss key insights, challenges, and potential future directions, emphasizing the significance of language models in economic applications.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CARE: Cognitive-reasoning Augmented Reinforcement for Emotional Support Conversation</title>
<link>https://arxiv.org/abs/2510.05122</link>
<guid>https://arxiv.org/abs/2510.05122</guid>
<content:encoded><![CDATA[
<div> Keywords: Emotional Support Conversation, Cognitive Reasoning, Framework, Reasoning Process, Reinforcement Learning <br />
<br />
Summary: 
The article introduces a new framework, CARE, for enhancing the logical coherence and supportive quality of Emotional Support Conversations (ESC). Unlike previous studies that focus on data augmentation, CARE emphasizes strengthening the cognitive reasoning processes essential for effective emotional support. By utilizing the original ESC dataset and leveraging reinforcement learning, CARE guides models in generating more logically coherent and supportive responses. Experimental results show that CARE significantly improves the logical soundness and supportive quality of responses, leading to the development of more empathetic, cognitively robust, and human-like emotional support systems. <div>
arXiv:2510.05122v1 Announce Type: new 
Abstract: Emotional Support Conversation (ESC) plays a vital role in alleviating psychological stress and providing emotional value through dialogue. While recent studies have largely focused on data augmentation and synthetic corpus construction, they often overlook the deeper cognitive reasoning processes that underpin effective emotional support. To address this gap, we propose \textbf{CARE}, a novel framework that strengthens reasoning in ESC without relying on large-scale synthetic data. CARE leverages the original ESC training set to guide models in generating logically coherent and supportive responses, thereby explicitly enhancing cognitive reasoning. Building on this foundation, we further employ reinforcement learning to refine and reinforce the reasoning process. Experimental results demonstrate that CARE significantly improves both the logical soundness and supportive quality of responses, advancing the development of empathetic, cognitively robust, and human-like emotional support systems.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MADS: Multi-Agent Dialogue Simulation for Diverse Persuasion Data Generation</title>
<link>https://arxiv.org/abs/2510.05124</link>
<guid>https://arxiv.org/abs/2510.05124</guid>
<content:encoded><![CDATA[
<div> Keywords: MADS, multi-agent dialogue simulation, persuasion strategies, optimization, user data <br />
<br />
Summary: 
The proposed framework MADS aims to generate persuasive multi-turn dialogues through agent self-play. It involves three agents - User Agents simulating diverse behaviors, a Dialog Agent executing persuasion strategies, and an Optimization Agent evaluating outcomes. MADS eliminates the need for human annotation by generating training data at a low cost. It addresses industry challenges such as lack of user data and prompt inefficiency. The framework was validated using Chain-of-Attitude modeling and LLMs' persuasion assessment. In a real-world marketing scenario, MADS improved the persuasion capacity of small LLMs, leading to a significant increase in the organic traffic conversion rate by 22.4%. This demonstrates the business value of MADS in enhancing dialogue generation and persuasion techniques. <div>
arXiv:2510.05124v1 Announce Type: new 
Abstract: We propose MADS (Multi-Agent Dialogue Simulation), a scalable framework for generating persuasive multi-turn dialogues via agent self-play. MADS employs three coordinated agents: User Agents simulating diverse persona-driven behaviors, a Dialog Agent executing task-oriented persuasion strategies and an Optimization Agent evaluating and refining dialogue outcomes. We further validate its effectiveness through users' Chain-of-Attitude (CoA) modeling and dedicated LLMs' persuasion assessment. This approach enables low-cost generation of training data without human annotation, addressing key industry challenges such as lack of user data, cold-start evaluation difficulties, and prompt inefficiency. Applied to a real-world marketing scenario, MADS significantly improved the persuasion capacity of small LLMs, increasing the organic traffic conversion rate by 22.4\% (from 1.83\% to 2.24\%) , demonstrating clear business value.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Catalog-Native LLM: Speaking Item-ID Dialect with Less Entanglement for Recommendation</title>
<link>https://arxiv.org/abs/2510.05125</link>
<guid>https://arxiv.org/abs/2510.05125</guid>
<content:encoded><![CDATA[
<div> Collaborative filtering, Large Language Models (LLMs), recommendation systems, IDIOMoE, item interaction histories<br />
Summary: Collaborative filtering and Large Language Models (LLMs) are powerful tools for recommendation systems, but combining their strengths is challenging. The IDIOMoE method introduced in this paper integrates collaborative signals with natural language processing by treating item interaction histories as a native dialect in the language space. By separating the Feed Forward Network of a pretrained LLM into text and item experts with token-type gating, IDIOMoE achieves strong recommendation performance while maintaining the text understanding capabilities of the model. This approach enables a unified framework for recommendation systems that can handle natural-language queries and provide transparent explanations to users. <div>
arXiv:2510.05125v1 Announce Type: new 
Abstract: While collaborative filtering delivers predictive accuracy and efficiency, and Large Language Models (LLMs) enable expressive and generalizable reasoning, modern recommendation systems must bring these strengths together. Growing user expectations, such as natural-language queries and transparent explanations, further highlight the need for a unified approach. However, doing so is nontrivial. Collaborative signals are often token-efficient but semantically opaque, while LLMs are semantically rich but struggle to model implicit user preferences when trained only on textual inputs. This paper introduces Item-ID + Oral-language Mixture-of-Experts Language Model (IDIOMoE), which treats item interaction histories as a native dialect within the language space, enabling collaborative signals to be understood in the same way as natural language. By splitting the Feed Forward Network of each block of a pretrained LLM into a separate text expert and an item expert with token-type gating, our method avoids destructive interference between text and catalog modalities. IDIOMoE demonstrates strong recommendation performance across both public and proprietary datasets, while preserving the text understanding of the pretrained model.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Metacognition and Uncertainty Communication in Language Models</title>
<link>https://arxiv.org/abs/2510.05126</link>
<guid>https://arxiv.org/abs/2510.05126</guid>
<content:encoded><![CDATA[
<div> supervised finetuning, large language models, uncertainty communication, metacognitive tasks, calibration error <br />
Summary: <br />
Large language models (LLMs) often lack accurate uncertainty communication, leading to potential errors in decision-making. This study explores how supervised finetuning can enhance LLMs' ability to convey uncertainty in various tasks and domains. Finetuning on datasets from different domains improved calibration and discrimination of uncertainty signals without impacting accuracy. However, the effectiveness of training on specific metacognitive tasks varied, with multitask finetuning yielding broader improvements in out-of-domain evaluations. The results highlight the importance of developing multiple metacognitive skills concurrently through multitask training for improved uncertainty communication in LLMs. <div>
arXiv:2510.05126v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used in decision-making contexts, but when they present answers without signaling low confidence, users may unknowingly act on erroneous outputs. While prior work shows that LLMs maintain internal uncertainty signals, their explicit verbalized confidence is typically miscalibrated and poorly discriminates between correct and incorrect answers. Across two types of LLMs, we investigate whether supervised finetuning can improve models' ability to communicate uncertainty and whether such improvements generalize across tasks and domains. We finetune the LLMs on datasets spanning general knowledge, mathematics, and open-ended trivia, and evaluate two metacognitive tasks: (1) single-question confidence estimation, where the model assigns a numeric certainty to its answer, and (2) pairwise confidence comparison, where the model selects which of two answers it is more likely to have correct. We assess generalization to unseen domains, including medical and legal reasoning. Results show that finetuning improves calibration (alignment between stated confidence and accuracy) and discrimination (higher confidence for correct vs. incorrect responses) within and across domains, while leaving accuracy unchanged. However, improvements are task-specific: training on single-question calibration does not transfer to pairwise comparison, and vice versa. In contrast, multitask finetuning on both forms of metacognition yields broader gains, producing lower calibration error and stronger discrimination in out-of-domain evaluations. These results show that while uncertainty communication in LLMs is trainable and generalizable, different metacognitive skills do not naturally reinforce one another and must be developed together through multitask training.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Automated Spatio-Semantic Analysis in Picture Description Using Language Models</title>
<link>https://arxiv.org/abs/2510.05128</link>
<guid>https://arxiv.org/abs/2510.05128</guid>
<content:encoded><![CDATA[
<div> Keywords: automated assessment, cognitive-linguistic impairment, visual narrative path, BERT-based pipeline, content information units (CIUs)

Summary: 
- The study introduces a BERT-based pipeline for automated extraction and ordering of content information units (CIUs) in picture descriptions for cognitive-linguistic impairment assessment.
- The pipeline achieves high performance with 93% median precision and 96% median recall in CIU detection, and a 24% sequence error rate.
- Extracted features from the pipeline show strong correlations with ground truth and outperform dictionary-based baselines in external validation.
- The proposed method is effective in characterizing visual narrative paths for cognitive impairment assessment.
- The implementation and models of the pipeline are open-sourced to the public. 

Summary: <div>
arXiv:2510.05128v1 Announce Type: new 
Abstract: Current methods for automated assessment of cognitive-linguistic impairment via picture description often neglect the visual narrative path - the sequence and locations of elements a speaker described in the picture. Analyses of spatio-semantic features capture this path using content information units (CIUs), but manual tagging or dictionary-based mapping is labor-intensive. This study proposes a BERT-based pipeline, fine tuned with binary cross-entropy and pairwise ranking loss, for automated CIU extraction and ordering from the Cookie Theft picture description. Evaluated by 5-fold cross-validation, it achieves 93% median precision, 96% median recall in CIU detection, and 24% sequence error rates. The proposed method extracts features that exhibit strong Pearson correlations with ground truth, surpassing the dictionary-based baseline in external validation. These features also perform comparably to those derived from manual annotations in evaluating group differences via ANCOVA. The pipeline is shown to effectively characterize visual narrative paths for cognitive impairment assessment, with the implementation and models open-sourced to public.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Alignment of Math Items to Content Standards in Large-Scale Assessments Using Language Models</title>
<link>https://arxiv.org/abs/2510.05129</link>
<guid>https://arxiv.org/abs/2510.05129</guid>
<content:encoded><![CDATA[
<div> embedding, machine learning, BERT, domain alignment, skill alignment  

Summary:  
- This study evaluates three automated paradigms for aligning items to content standards in large-scale assessments.  
- Classical supervised machine learning models were trained using embeddings, with dimensionality reduction impacting performance.  
- BERT models were fine-tuned for domain and skill alignment, with DeBERTa-v3-base achieving the highest F1 score for domain alignment and RoBERTa-large for skill alignment.  
- Ensemble learning techniques such as majority voting and stacking with meta-models were explored but did not outperform the best-performing language models.  
- Dimension reduction enhanced linear classifiers based on embeddings but did not perform better than language models.  
<br /><br />Summary: <div>
arXiv:2510.05129v1 Announce Type: new 
Abstract: Accurate alignment of items to content standards is critical for valid score interpretation in large-scale assessments. This study evaluates three automated paradigms for aligning items with four domain and nineteen skill labels. First, we extracted embeddings and trained multiple classical supervised machine learning models, and further investigated the impact of dimensionality reduction on model performance. Second, we fine-tuned eight BERT model and its variants for both domain and skill alignment. Third, we explored ensemble learning with majority voting and stacking with multiple meta-models. The DeBERTa-v3-base achieved the highest weighted-average F1 score of 0.950 for domain alignment while the RoBERTa-large yielded the highest F1 score of 0.869 for skill alignment. Ensemble models did not surpass the best-performing language models. Dimension reduction enhanced linear classifiers based on embeddings but did not perform better than language models. This study demonstrated different methods in automated item alignment to content standards.}
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Submodular Context Partitioning and Compression for In-Context Learning-short paper</title>
<link>https://arxiv.org/abs/2510.05130</link>
<guid>https://arxiv.org/abs/2510.05130</guid>
<content:encoded><![CDATA[
<div> learning, large language models, in-context learning, transformers, few-shot learning

Summary:<br />
- The paper introduces In-context learning (ICL) as an efficient few-shot learning method in large language models (LLMs) without requiring additional training.
- ICL suffers from the quadratic input complexity of transformers, limiting the number of exemplars that can be processed.
- Existing ICL approaches partition the context into blocks for processing but often overlook information redundancy or under-representation, leading to suboptimal performance.
- The proposed Sub-CP framework leverages submodular objectives to control block diversity, allowing for fine-grained control over semantic structure while enabling precomputation.
- Extensive experiments across various tasks and datasets demonstrate that Sub-CP consistently enhances performance across different model scales.

<br /><br />Summary: <div>
arXiv:2510.05130v1 Announce Type: new 
Abstract: In-context learning (ICL) enables efficient few-shot learning in large language models (LLMs) without training, but suffers from the quadratic input complexity of transformers, limiting the maximum number of exemplars. While various efficient ICL approaches partition the context into blocks to process (e.g., ensembling, compression, cross-attention), they often ignore the information redundancy or under-representation caused by different partition strategies, leading to suboptimal performance. To tackle this problem, we propose Sub-CP, a block-aware context selection framework that leverages submodular objectives to control block diversity. Sub-CP supports a flexible spectrum of selection strategies, allowing each block to range from globally diverse to locally coherent. This allows fine-grained control over semantic structure while enabling precomputation. Extensive experiments across diverse tasks on multiple datasets show that Sub-CP consistently improves performance across model scales.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rationale-Augmented Retrieval with Constrained LLM Re-Ranking for Task Discovery</title>
<link>https://arxiv.org/abs/2510.05131</link>
<guid>https://arxiv.org/abs/2510.05131</guid>
<content:encoded><![CDATA[
<div> Keywords: Head Start programs, GoEngage, semantic search system, typo-tolerant lexical retrieval, large language model<br />
Summary:<br />
This article addresses the challenges faced by Head Start programs using the GoEngage platform in locating appropriate Tasks due to domain-specific jargon and system-specific nomenclature. The proposed solution is a hybrid semantic search system that combines typo-tolerant lexical retrieval, vector similarity, and large language model re-ranking. The approach leverages existing Task Repository and Knowledge Base infrastructure while focusing on trustworthiness, evolvability, and economic efficiency. The framework includes required resources, implementation strategy, evaluation protocol, and measurement methodology. By ensuring low false-positive rates, accommodation of terminological changes, and efficient mechanisms such as caching and shortlist generation, the system aims to improve the user experience and streamline the task location process. In offline evaluations and online measurements, metrics like Hit@K, Precision@K, Recall@K, zero-result rates, and dwell-time proxies are used to assess the system's performance. <br /><br />Summary: <div>
arXiv:2510.05131v1 Announce Type: new 
Abstract: Head Start programs utilizing GoEngage face significant challenges when new or rotating staff attempt to locate appropriate Tasks (modules) on the platform homepage. These difficulties arise from domain-specific jargon (e.g., IFPA, DRDP), system-specific nomenclature (e.g., Application Pool), and the inherent limitations of lexical search in handling typos and varied word ordering. We propose a pragmatic hybrid semantic search system that synergistically combines lightweight typo-tolerant lexical retrieval, embedding-based vector similarity, and constrained large language model (LLM) re-ranking. Our approach leverages the organization's existing Task Repository and Knowledge Base infrastructure while ensuring trustworthiness through low false-positive rates, evolvability to accommodate terminological changes, and economic efficiency via intelligent caching, shortlist generation, and graceful degradation mechanisms. We provide a comprehensive framework detailing required resources, a phased implementation strategy with concrete milestones, an offline evaluation protocol utilizing curated test cases (Hit@K, Precision@K, Recall@K, MRR), and an online measurement methodology incorporating query success metrics, zero-result rates, and dwell-time proxies.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Large Language Models To Reason In Parallel With Global Forking Tokens</title>
<link>https://arxiv.org/abs/2510.05132</link>
<guid>https://arxiv.org/abs/2510.05132</guid>
<content:encoded><![CDATA[
<div> LLMs, scaling, reasoning paths, diverse, accurate <br />
Summary: 
This article discusses the challenge of generating diverse and accurate reasoning paths in Large Language Models (LLMs) by treating parallel reasoning as a set-of-next-token-prediction problem. The authors propose Set Supervised Fine-Tuning (SSFT), a method that incorporates a set-based global loss into Supervised Fine-Tuning (SFT) using self-supervised bipartite matching, preserving unique reasoning modes and producing emergent global forking tokens. Experiments on multiple reasoning benchmarks show that SSFT consistently outperforms SFT in terms of both Pass@1 and Cons@k metrics. The traditional strategies to encourage diversity, such as temperature scaling, encounter a trade-off between diversity and accuracy due to the deep forking tokens in the sampling tree. SSFT addresses this challenge by maintaining diverse reasoning modes while improving accuracy, demonstrating the efficacy of this approach in enhancing performance in LLMs. <br /><br />Summary: <div>
arXiv:2510.05132v1 Announce Type: new 
Abstract: Although LLMs have demonstrated improved performance by scaling parallel test-time compute, doing so relies on generating reasoning paths that are both diverse and accurate. For challenging problems, the forking tokens that trigger diverse yet correct reasoning modes are typically deep in the sampling tree. Consequently, common strategies to encourage diversity, such as temperature scaling, encounter a worsened trade-off between diversity and accuracy. Motivated by this challenge, we treat parallel reasoning as a set-of-next-token-prediction problem, and incorporate a set-based global loss into Supervised Fine-Tuning (SFT) using self-supervised bipartite matching between our global forking tokens and unique reasoning traces. We observe that, while naive fine-tuning with multiple reasoning traces collapses these unique reasoning modes, our proposed method, Set Supervised Fine-Tuning (SSFT), preserves these modes and produces emergent global forking tokens. Experiments on multiple reasoning benchmarks show that our SSFT consistently outperforms SFT under both Pass@1 and Cons@k metrics.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterizing Model Behavior Under Synthetic Data Training: An Empirical Study Across Scales and Mixing Ratios</title>
<link>https://arxiv.org/abs/2510.05133</link>
<guid>https://arxiv.org/abs/2510.05133</guid>
<content:encoded><![CDATA[
<div> proportion, model behavior, data ratios, calibration, performance
Summary:
- Models were evaluated on various synthetic-to-external data ratios across different scales using the Pythia model suite.
- Models maintain stable performance with up to 20% synthetic data but degrade beyond 30%, with larger models showing more robustness.
- Calibration degradation occurs before accuracy loss, serving as an early warning signal.
- Reasoning tasks degrade faster than retrieval tasks under synthetic data training.
- Best practices such as those in STaR and Self-Instruct systems, which use over 80% external data, are within safe regimes based on the study's findings.
- Practical guidance for practitioners on synthetic data budgets considering model scale and task requirements is provided.
- Comparison with concurrent work, including Shumailov et al.'s model collapse findings, is detailed. 

<br /><br />Summary: <div>
arXiv:2510.05133v1 Announce Type: new 
Abstract: Synthetic data generated by large language models has become integral to modern NLP training pipelines, from bootstrapping reasoning capabilities to augmenting instruction-following datasets. While recent work demonstrates successful applications maintaining high external data ratios, systematic understanding of how synthetic data proportion affects model behavior across different scales remains limited. This paper presents a controlled empirical study examining model performance, calibration, and output characteristics when trained on varying synthetic-to-external data ratios. Using the Pythia model suite (410M-12B parameters) across five diverse tasks, we evaluate models after one to three training iterations with synthetic data proportions ranging from 0-50\%. Our key findings include: models maintain stable performance with up to 20\% synthetic data, but degradation accelerates beyond 30\%; larger models (6.9B-12B) show greater robustness to synthetic data than smaller models (410M-1.4B); calibration degradation precedes accuracy loss, providing an early warning signal; and task characteristics matter, with reasoning tasks degrading faster than retrieval tasks under synthetic data training. Importantly, we find that current best practices, such as those employed in STaR and Self-Instruct systems that maintain greater than 80\% external data, operate well within safe regimes identified by our experiments. We provide practical guidance for practitioners on synthetic data budgets based on model scale and task requirements, alongside detailed comparison with concurrent work including Shumailov et al.'s model collapse findings.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curiosity-Driven LLM-as-a-judge for Personalized Creative Judgment</title>
<link>https://arxiv.org/abs/2510.05135</link>
<guid>https://arxiv.org/abs/2510.05135</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, creativity evaluation, personalized judgments, subjective assessments, Torrance Test of Creative Thinking

Summary:<br /><br />
- Modern large language models (LLMs) perform well on objective tasks but struggle with subjective evaluations of creativity.
- A new curiosity-driven LLM-as-a-judge method is proposed for assessing creative writing, personalized to individual judgments.
- The Torrance Test of Creative Thinking benchmark is used to evaluate this approach, which considers subjective dimensions like Originality.
- Results show that the method enables models of various sizes to learn individual creative judgments, outperforming baseline supervised fine-tuning methods across evaluation metrics.
- This method is particularly valuable for subjective evaluations where not all annotators agree with each other.<br /><br />Summary: <div>
arXiv:2510.05135v1 Announce Type: new 
Abstract: Modern large language models (LLMs) excel at objective tasks such as evaluating mathematical reasoning and factual accuracy, yet they falter when faced with the nuanced, subjective nature of assessing creativity. In this work, we propose a novel curiosity-driven LLM-as-a-judge for evaluating creative writing which is personlized to each individual's creative judgments. We use the Torrance Test of Creative Thinking(TTCW) benchmark introduced in Chakrabarty et al. (2024), which has stories annotated by expert humans across various subjective dimensions like Originality, to test our hypothesis. We show that our method enables models across various sizes, to learn the nuanced creative judgments of different individuals, by showing improvements over baseline supervised finetuning(SFT) method across various evaluation metrics like Pearson correlation, Cohen's and F1 values. Our method is especially useful in subjective evaluations where not all the annotators agree with each other.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linguistic Characteristics of AI-Generated Text: A Survey</title>
<link>https://arxiv.org/abs/2510.05136</link>
<guid>https://arxiv.org/abs/2510.05136</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, AI-generated text, linguistic features, prompt sensitivity, text generation

Summary: 
AI-generated text produced by large language models exhibits linguistic features such as a formal and impersonal style, with an abundance of nouns and determiners but fewer adjectives and adverbs. The text also shows lower lexical diversity, smaller vocabulary size, and repetitiveness. Current research primarily focuses on English data and the GPT model family, indicating a need for more diverse cross-linguistic and cross-model investigations. Additionally, there is a lack of exploration into prompt sensitivity in text generation, highlighting the potential for future studies to employ multiple prompt wordings. This survey paper synthesizes existing research on AI-generated text to provide a comprehensive overview of the trends and findings in the field. <br /><br />Summary: <div>
arXiv:2510.05136v1 Announce Type: new 
Abstract: Large language models (LLMs) are solidifying their position in the modern world as effective tools for the automatic generation of text. Their use is quickly becoming commonplace in fields such as education, healthcare, and scientific research. There is a growing need to study the linguistic features present in AI-generated text, as the increasing presence of such texts has profound implications in various disciplines such as corpus linguistics, computational linguistics, and natural language processing. Many observations have already been made, however a broader synthesis of the findings made so far is required to provide a better understanding of the topic. The present survey paper aims to provide such a synthesis of extant research. We categorize the existing works along several dimensions, including the levels of linguistic description, the models included, the genres analyzed, the languages analyzed, and the approach to prompting. Additionally, the same scheme is used to present the findings made so far and expose the current trends followed by researchers. Among the most-often reported findings is the observation that AI-generated text is more likely to contain a more formal and impersonal style, signaled by the increased presence of nouns, determiners, and adpositions and the lower reliance on adjectives and adverbs. AI-generated text is also more likely to feature a lower lexical diversity, a smaller vocabulary size, and repetitive text. Current research, however, remains heavily concentrated on English data and mostly on text generated by the GPT model family, highlighting the need for broader cross-linguistic and cross-model investigation. In most cases authors also fail to address the issue of prompt sensitivity, leaving much room for future studies that employ multiple prompt wordings in the text generation phase.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demystifying deep search: a holistic evaluation with hint-free multi-hop questions and factorised metrics</title>
<link>https://arxiv.org/abs/2510.05137</link>
<guid>https://arxiv.org/abs/2510.05137</guid>
<content:encoded><![CDATA[
<div> Retrieval-Augmented Generation, multi-hop deep search tasks, benchmark, controlled Wikipedia sandbox, holistic evaluation framework <br />
<br />
Summary: WebDetective introduces a benchmark for multi-hop questions without hints, paired with a controlled Wikipedia sandbox for traceability. The evaluation framework assesses search sufficiency, knowledge utilization, and refusal behavior separately. State-of-the-art models show weaknesses in knowledge utilization and refusal behaviors, despite having sufficient evidence. They struggle to autonomously discover reasoning paths and excel at following given paths. EvidenceLoop, an agentic workflow, addresses these challenges with verification loops and evidence tracking. WebDetective serves as a tool for developing truly autonomous reasoning systems by guiding architectural improvements based on diagnostic insights from the benchmark. <div>
arXiv:2510.05137v1 Announce Type: new 
Abstract: RAG (Retrieval-Augmented Generation) systems and web agents are increasingly evaluated on multi-hop deep search tasks, yet current practice suffers from two major limitations. First, most benchmarks leak the reasoning path in the question text, allowing models to follow surface cues rather than discover reasoning chains autonomously. Second, evaluation is typically reduced to a single pass rate, which collapses diverse behaviours into one score and obscures whether failures stem from inadequate search, poor knowledge use, or inappropriate refusal. To address these issues, we present WebDetective, a benchmark of hint-free multi-hop questions paired with a controlled Wikipedia sandbox that ensures full traceability of model actions, and a holistic evaluation framework that separates search sufficiency, knowledge utilisation, and refusal behaviour. Our evaluation of 25 state-of-the-art models reveals systematic weaknesses across all architectures: models struggle with knowledge utilisation despite having sufficient evidence and demonstrate near-absent appropriate refusal when evidence is lacking. These patterns expose a fundamental gap: today's systems excel at executing given reasoning paths but fail when required to discover them. We develop an agentic workflow, EvidenceLoop, that explicitly targets the challenges our benchmark identifies, incorporating verification loops and systematic evidence tracking that improve both search and synthesis capabilities. This baseline demonstrates that WebDetective's diagnostic framework can guide concrete architectural improvements, establishing our benchmark as a critical tool for developing genuinely autonomous reasoning systems rather than pattern-following agents.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiRA: A Multi-Agent Framework for Reliable and Readable Literature Review Generation</title>
<link>https://arxiv.org/abs/2510.05138</link>
<guid>https://arxiv.org/abs/2510.05138</guid>
<content:encoded><![CDATA[
<div> Keywords: literature review, multi-agent system, automated scientific writing, readability, factual accuracy

Summary:
LiRA (Literature Review Agents) is a multi-agent collaborative workflow designed to streamline the writing phase of literature reviews, focusing on readability and factual accuracy. Utilizing specialized agents for content outlining, subsection writing, editing, and reviewing, LiRA produces comprehensive and cohesive review articles. In evaluations against current baselines like AutoSurvey and MASS-Survey, LiRA demonstrated superior writing and citation quality while maintaining competitive similarity to human-written reviews. Real-world scenarios using document retrieval further showcased the robustness of LiRA to reviewer model variation. The study suggests the potential of agentic LLM workflows in enhancing the reliability and usability of automated scientific writing.<br /><br />Summary: LiRA, a multi-agent system, enhances the writing phase of literature reviews by focusing on readability and factual accuracy. Specialized agents work collaboratively to produce comprehensive and cohesive review articles, outperforming current baselines in writing quality while maintaining similarity to human-written reviews. Real-world evaluations demonstrate LiRA's robustness to reviewer model variation and the potential of agentic LLM workflows in improving automated scientific writing. <div>
arXiv:2510.05138v1 Announce Type: new 
Abstract: The rapid growth of scientific publications has made it increasingly difficult to keep literature reviews comprehensive and up-to-date. Though prior work has focused on automating retrieval and screening, the writing phase of systematic reviews remains largely under-explored, especially with regard to readability and factual accuracy. To address this, we present LiRA (Literature Review Agents), a multi-agent collaborative workflow which emulates the human literature review process. LiRA utilizes specialized agents for content outlining, subsection writing, editing, and reviewing, producing cohesive and comprehensive review articles. Evaluated on SciReviewGen and a proprietary ScienceDirect dataset, LiRA outperforms current baselines such as AutoSurvey and MASS-Survey in writing and citation quality, while maintaining competitive similarity to human-written reviews. We further evaluate LiRA in real-world scenarios using document retrieval and assess its robustness to reviewer model variation. Our findings highlight the potential of agentic LLM workflows, even without domain-specific tuning, to improve the reliability and usability of automated scientific writing.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NLD-LLM: A systematic framework for evaluating small language transformer models on natural language description</title>
<link>https://arxiv.org/abs/2510.05139</link>
<guid>https://arxiv.org/abs/2510.05139</guid>
<content:encoded><![CDATA[
<div> Keywords: Natural Language Description, NLP, language models, prompt design strategy, iterative refinement process

Summary:
Natural Language Description (NLD) is a task in Natural Language Processing (NLP) where models generate structured code descriptions. The NLD-LLM framework evaluates language models' performance in generating accurate code descriptions. It includes diverse transformer models like Qwen, DeepSeek, Phi, LLaMA, and Mistral. A prompt design strategy with standardized formatting and task guidance ensures fair evaluation. An iterative refinement process improves output quality. Smaller models can perform well with well-crafted prompts. Prompt engineering significantly impacts model effectiveness. The study uses semantic and structural metrics to analyze model performance. The framework emphasizes the importance of prompt design in evaluating language models for generating code descriptions.<br /><br />Summary: <div>
arXiv:2510.05139v1 Announce Type: new 
Abstract: Natural Language Description (NLD) is a Natural Language Processing (NLP) task that requires models to generate structured and meaningful outputs from natural language inputs. In this work, we propose NLD-LLM, a systematic NLP framework to evaluate the performance of language models to generate accurate and concise source code descriptions. This framework incorporates a diverse set of transformer models, including Qwen, DeepSeek, Phi, LLaMA, and Mistral, spanning various sizes, architectures, and training approaches. Central to NLD-LLM is a comprehensive prompt design strategy that includes standardized formatting, clear task guidance, and NLD prompting, ensuring fair and consistent evaluation. Additionally, we apply an iterative refinement process to improve output's quality and assess the model's adaptability. Using semantic and structural metrics, our analysis demonstrates that prompt engineering significantly impacts the effectiveness of the model such that smaller models often performing competitively when supported by well-crafted prompts.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To model human linguistic prediction, make LLMs less superhuman</title>
<link>https://arxiv.org/abs/2510.05141</link>
<guid>https://arxiv.org/abs/2510.05141</guid>
<content:encoded><![CDATA[
<div> prediction, language models, human reading behavior, superhuman, memory<br />
<br />
Summary: 
The article discusses how people make predictions about upcoming words while listening to or reading a sentence and how large language models (LLMs) are being explored as cognitive models of human linguistic prediction. It highlights that as language models improve in predicting the next word, their ability to predict human reading behavior has declined due to their 'superhuman' prediction capabilities. This superhumanness is attributed to LLMs having stronger long-term memory and better short-term memory compared to humans. The paper advocates for the development of models with human-like long-term and short-term memory and suggests possible directions to achieve this. It also emphasizes the need for human experiments to measure progress towards this goal, as current human data is deemed insufficient. <div>
arXiv:2510.05141v1 Announce Type: new 
Abstract: When people listen to or read a sentence, they actively make predictions about upcoming words: words that are less predictable are generally read more slowly than predictable ones. The success of large language models (LLMs), which, like humans, make predictions about upcoming words, has motivated exploring the use of these models as cognitive models of human linguistic prediction. Surprisingly, in the last few years, as language models have become better at predicting the next word, their ability to predict human reading behavior has declined. This is because LLMs are able to predict upcoming words much better than people can, leading them to predict lower processing difficulty in reading than observed in human experiments; in other words, mainstream LLMs are 'superhuman' as models of language comprehension. In this position paper, we argue that LLMs' superhumanness is primarily driven by two factors: compared to humans, LLMs have much stronger long-term memory for facts and training examples, and they have much better short-term memory for previous words in the text. We advocate for creating models that have human-like long-term and short-term memory, and outline some possible directions for achieving this goal. Finally, we argue that currently available human data is insufficient to measure progress towards this goal, and outline human experiments that can address this gap.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reliable End-to-End Material Information Extraction from the Literature with Source-Tracked Multi-Stage Large Language Models</title>
<link>https://arxiv.org/abs/2510.05142</link>
<guid>https://arxiv.org/abs/2510.05142</guid>
<content:encoded><![CDATA[
<div> Keywords: data-driven materials discovery, information extraction pipeline, composition-processing-microstructure-property relationships, large language models, materials informatics

Summary: 
Data-driven materials discovery relies on large-scale experimental datasets, but much information is dispersed in unstructured literature. This study proposes a multi-stage information extraction pipeline, utilizing large language models to capture 47 features related to materials composition, processing, microstructure, and properties from experimental reports. The pipeline integrates iterative extraction with source tracking to enhance accuracy and reliability. Evaluations demonstrate high F1 scores at both the feature and tuple levels. Compared to single-pass extraction without source tracking, the approach significantly improves the extraction of microstructure attributes and reduces missed materials. The pipeline enables scalable and efficient literature mining, yielding databases with high precision, minimal omissions, and no false positives. These datasets serve as reliable inputs for machine learning and materials informatics, with a modular design suitable for various material classes, enabling comprehensive materials information extraction.<br /><br />Summary: <div>
arXiv:2510.05142v1 Announce Type: new 
Abstract: Data-driven materials discovery requires large-scale experimental datasets, yet most of the information remains trapped in unstructured literature. Existing extraction efforts often focus on a limited set of features and have not addressed the integrated composition-processing-microstructure-property relationships essential for understanding materials behavior, thereby posing challenges for building comprehensive databases. To address this gap, we propose a multi-stage information extraction pipeline powered by large language models, which captures 47 features spanning composition, processing, microstructure, and properties exclusively from experimentally reported materials. The pipeline integrates iterative extraction with source tracking to enhance both accuracy and reliability. Evaluations at the feature level (independent attributes) and tuple level (interdependent features) yielded F1 scores around 0.96. Compared with single-pass extraction without source tracking, our approach improved F1 scores of microstructure category by 10.0% (feature level) and 13.7% (tuple level), and reduced missed materials from 49 to 13 out of 396 materials in 100 articles on precipitate-containing multi-principal element alloys (miss rate reduced from 12.4% to 3.3%). The pipeline enables scalable and efficient literature mining, producing databases with high precision, minimal omissions, and zero false positives. These datasets provide trustworthy inputs for machine learning and materials informatics, while the modular design generalizes to diverse material classes, enabling comprehensive materials information extraction.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynCED-EnDe 2025: A Synthetic and Curated English - German Dataset for Critical Error Detection in Machine Translation</title>
<link>https://arxiv.org/abs/2510.05144</link>
<guid>https://arxiv.org/abs/2510.05144</guid>
<content:encoded><![CDATA[
<div> Resource, Critical Error Detection, Machine Translation, SynCED-EnDe, Dataset 

Summary: 
The article introduces SynCED-EnDe, a new dataset for Critical Error Detection (CED) in machine translation. This dataset addresses the limitations of previous benchmarks by providing a larger scale, balanced labels, diverse sources, and enhanced annotations. SynCED-EnDe consists of 1,000 gold-labeled and 8,000 silver-labeled sentence pairs, with explicit error subclasses and fine-grained auxiliary judgments. This allows for detailed analysis of error risk and complexity beyond binary detection. The dataset is available on GitHub and Hugging Face, along with documentation and baseline scripts. Benchmark experiments with XLM-R and related encoders show significant performance improvements over previous benchmarks. The goal is to advance the safe deployment of machine translation in information retrieval and conversational assistants, especially in new contexts like wearable AI devices. <div>
arXiv:2510.05144v1 Announce Type: new 
Abstract: Critical Error Detection (CED) in machine translation aims to determine whether a translation is safe to use or contains unacceptable deviations in meaning. While the WMT21 English-German CED dataset provided the first benchmark, it is limited in scale, label balance, domain coverage, and temporal freshness. We present SynCED-EnDe, a new resource consisting of 1,000 gold-labeled and 8,000 silver-labeled sentence pairs, balanced 50/50 between error and non-error cases. SynCED-EnDe draws from diverse 2024-2025 sources (StackExchange, GOV.UK) and introduces explicit error subclasses, structured trigger flags, and fine-grained auxiliary judgments (obviousness, severity, localization complexity, contextual dependency, adequacy deviation). These enrichments enable systematic analyses of error risk and intricacy beyond binary detection. The dataset is permanently hosted on GitHub and Hugging Face, accompanied by documentation, annotation guidelines, and baseline scripts. Benchmark experiments with XLM-R and related encoders show substantial performance gains over WMT21 due to balanced labels and refined annotations. We envision SynCED-EnDe as a community resource to advance safe deployment of MT in information retrieval and conversational assistants, particularly in emerging contexts such as wearable AI devices.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Every Step Counts: Decoding Trajectories as Authorship Fingerprints of dLLMs</title>
<link>https://arxiv.org/abs/2510.05148</link>
<guid>https://arxiv.org/abs/2510.05148</guid>
<content:encoded><![CDATA[
<div> large language models, non-autoregressive, decoding mechanism, model attribution, Gaussian-Trajectory Attribution

Summary: Discrete Diffusion Large Language Models (dLLMs) are gaining popularity due to their fast inference speed and strong performance in code generation and mathematical tasks. This paper explores using the unique decoding mechanism of dLLMs for model attribution, a challenging task involving distinguishing between different models or checkpoints. Traditional methods relying on per-step model confidence are found to be ineffective due to the bidirectional decoding nature of dLLMs. To address this, the Directed Decoding Map (DDM) is introduced to capture structural relationships between decoding steps. Additionally, Gaussian-Trajectory Attribution (GTA) is proposed to effectively utilize the extracted structural information for attribution by fitting Gaussian distributions at each decoding position for each model. Experiments demonstrate the effectiveness of these methods in various scenarios, highlighting the potential of leveraging decoding trajectories for model attribution purposes. 

<br /><br />Summary: <div>
arXiv:2510.05148v1 Announce Type: new 
Abstract: Discrete Diffusion Large Language Models (dLLMs) have recently emerged as a competitive paradigm for non-autoregressive language modeling. Their distinctive decoding mechanism enables faster inference speed and strong performance in code generation and mathematical tasks. In this work, we show that the decoding mechanism of dLLMs not only enhances model utility but also can be used as a powerful tool for model attribution. A key challenge in this problem lies in the diversity of attribution scenarios, including distinguishing between different models as well as between different checkpoints or backups of the same model. To ensure broad applicability, we identify two fundamental problems: what information to extract from the decoding trajectory, and how to utilize it effectively. We first observe that relying directly on per-step model confidence yields poor performance. This is mainly due to the bidirectional decoding nature of dLLMs: each newly decoded token influences the confidence of other decoded tokens, making model confidence highly redundant and washing out structural signal regarding decoding order or dependencies. To overcome this, we propose a novel information extraction scheme called the Directed Decoding Map (DDM), which captures structural relationships between decoding steps and better reveals model-specific behaviors. Furthermore, to make full use of the extracted structural information during attribution, we propose Gaussian-Trajectory Attribution (GTA), where we fit a cell-wise Gaussian distribution at each decoding position for each target model, and define the likelihood of a trajectory as the attribution score: if a trajectory exhibits higher log-likelihood under the distribution of a specific model, it is more likely to have been generated by that model. Extensive experiments under different settings validate the utility of our methods.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chronological Thinking in Full-Duplex Spoken Dialogue Language Models</title>
<link>https://arxiv.org/abs/2510.05150</link>
<guid>https://arxiv.org/abs/2510.05150</guid>
<content:encoded><![CDATA[
<div> Keywords: spoken dialogue language models, full-duplex systems, Chronological Thinking, real-time interaction, response quality <br />
Summary: <br />
- Recent advances in spoken dialogue language models (SDLMs) focus on full-duplex systems that allow the agent to continuously perceive user speech streams while generating responses. <br />
- Existing systems idle the agent during the listening phase by repeatedly predicting the silence token, which is not reflective of human behavior. <br />
- Chronological Thinking is proposed as an on-the-fly conversational thinking mechanism that improves response quality in SDLMs. <br />
- The approach is strictly causal, reasoning incrementally while listening and updating internal hypotheses only from past audio. <br />
- No additional latency is introduced as reasoning takes place during the listening window, enabling the agent to start speaking immediately after the user stops. <br />
- Experimental results show the effectiveness of chronological thinking, with consistent improvements in response quality and competitive performance on full-duplex interaction metrics. <br /> <div>
arXiv:2510.05150v1 Announce Type: new 
Abstract: Recent advances in spoken dialogue language models (SDLMs) reflect growing interest in shifting from turn-based to full-duplex systems, where the models continuously perceive user speech streams while generating responses. This simultaneous listening and speaking design enables real-time interaction and the agent can handle dynamic conversational behaviors like user barge-in. However, during the listening phase, existing systems keep the agent idle by repeatedly predicting the silence token, which departs from human behavior: we usually engage in lightweight thinking during conversation rather than remaining absent-minded. Inspired by this, we propose Chronological Thinking, a on-the-fly conversational thinking mechanism that aims to improve response quality in full-duplex SDLMs. Specifically, chronological thinking presents a paradigm shift from conventional LLM thinking approaches, such as Chain-of-Thought, purpose-built for streaming acoustic input. (1) Strictly causal: the agent reasons incrementally while listening, updating internal hypotheses only from past audio with no lookahead. (2) No additional latency: reasoning is amortized during the listening window; once the user stops speaking, the agent halts thinking and begins speaking without further delay. Experiments demonstrate the effectiveness of chronological thinking through both objective metrics and human evaluations show consistent improvements in response quality. Furthermore, chronological thinking robustly handles conversational dynamics and attains competitive performance on full-duplex interaction metrics.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Large Language Models for Financial Applications: Techniques, Performance, and Challenges with FinMA</title>
<link>https://arxiv.org/abs/2510.05151</link>
<guid>https://arxiv.org/abs/2510.05151</guid>
<content:encoded><![CDATA[
<div> Evaluation, Domain-adapted, Large Language Models, Financial NLP, FinMA

Summary:
FinMA, a domain-adapted Large Language Model (LLM) evaluated in financial natural language processing (NLP), shows strengths in sentiment analysis and classification but struggles with tasks requiring numerical reasoning, entity recognition, and summarization. The study focuses on FinMA's architecture, its tuning process using the Financial Instruction Tuning (FIT) dataset, and evaluation under the FLARE benchmark. It emphasizes the importance of accuracy, reliability, and domain adaptation in financial applications. The research aims to enhance understanding of designing and assessing financial LLMs to support decision-making in finance. 

<br /><br />Summary: <div>
arXiv:2510.05151v1 Announce Type: new 
Abstract: This research explores the strengths and weaknesses of domain-adapted Large Language Models (LLMs) in the context of financial natural language processing (NLP). The analysis centers on FinMA, a model created within the PIXIU framework, which is evaluated for its performance in specialized financial tasks. Recognizing the critical demands of accuracy, reliability, and domain adaptation in financial applications, this study examines FinMA's model architecture, its instruction tuning process utilizing the Financial Instruction Tuning (FIT) dataset, and its evaluation under the FLARE benchmark. Findings indicate that FinMA performs well in sentiment analysis and classification, but faces notable challenges in tasks involving numerical reasoning, entity recognition, and summarization. This work aims to advance the understanding of how financial LLMs can be effectively designed and evaluated to assist in finance-related decision-making processes.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Single Character can Make or Break Your LLM Evals</title>
<link>https://arxiv.org/abs/2510.05152</link>
<guid>https://arxiv.org/abs/2510.05152</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language model evaluation, in-context examples, response quality, model rankings, attention head scores <br />
Summary: <br />
The study focuses on the impact of the format of in-context examples on the response quality of Large Language Models (LLMs). It discovered that the choice of delimiter significantly affects model performance, with variations of up to 23% in quality across different model families. Furthermore, the research identified that altering the delimiter can manipulate model rankings, showcasing the brittleness of LLMs in various topics and model scales. By analyzing attention head scores, the study revealed that well-performing delimiters direct attention to crucial input tokens. To enhance LLMs' robustness, the researchers suggested specifying the chosen delimiter in the prompt and provided practical recommendations for selecting the most effective delimiters. This study sheds light on the essential yet overlooked aspect of formatting in-context examples in LLM evaluations and offers insights to improve model performance. <br /> <div>
arXiv:2510.05152v1 Announce Type: new 
Abstract: Common Large Language model (LLM) evaluations rely on demonstration examples to steer models' responses to the desired style. While the number of examples used has been studied and standardized, the choice of how to format examples is less investigated. In evaluation protocols and real world usage, users face the choice how to separate in-context examples: use a comma? new line? semi-colon? hashtag? etc.? Surprisingly, we find this seemingly minor choice can dramatically alter model response quality. Across leading model families (Llama, Qwen, Gemma), performance on MMLU for example can vary by $\pm 23\%$ depending on the choice of delimiter. In fact, one can manipulate model rankings to put any model in the lead by only modifying the single character separating examples. We find LLMs' brittleness pervades topics, model families, and doesn't improve with scale. By probing attention head scores, we find that good-performing delimiters steer attention towards key tokens in the input. Finally, we explore methods to improve LLMs' robustness to the choice of delimiter. We find specifying the selected delimiter in the prompt boosts robustness and offer practical recommendations for the best-performing delimiters to select.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can AI Truly Represent Your Voice in Deliberations? A Comprehensive Study of Large-Scale Opinion Aggregation with LLMs</title>
<link>https://arxiv.org/abs/2510.05154</link>
<guid>https://arxiv.org/abs/2510.05154</guid>
<content:encoded><![CDATA[
<div> Keywords: Large-scale deliberations, LLMs, fairness concerns, DeliberationBank, DeliberationJudge

Summary: 
DeliberationBank introduces a dataset encompassing opinion data from 3,000 participants on ten deliberation questions and judgment data from 4,500 participants assessing summaries across four dimensions: representativeness, informativeness, neutrality, and policy approval. DeliberationJudge, a fine-tuned DeBERTa model trained on this dataset, outperforms a variety of LLM judges in evaluating deliberation summaries from individual perspectives. The study reveals shortcomings in LLM-based deliberation summarization, particularly in the underrepresentation of minority perspectives. This framework offers a robust and scalable approach to assessing deliberation summarization, aiding in the development of more representative and equitable AI systems for policymaking. 

<br /><br />Summary: <div>
arXiv:2510.05154v1 Announce Type: new 
Abstract: Large-scale public deliberations generate thousands of free-form contributions that must be synthesized into representative and neutral summaries for policy use. While LLMs have been shown as a promising tool to generate summaries for large-scale deliberations, they also risk underrepresenting minority perspectives and exhibiting bias with respect to the input order, raising fairness concerns in high-stakes contexts. Studying and fixing these issues requires a comprehensive evaluation at a large scale, yet current practice often relies on LLMs as judges, which show weak alignment with human judgments. To address this, we present DeliberationBank, a large-scale human-grounded dataset with (1) opinion data spanning ten deliberation questions created by 3,000 participants and (2) summary judgment data annotated by 4,500 participants across four dimensions (representativeness, informativeness, neutrality, policy approval). Using these datasets, we train DeliberationJudge, a fine-tuned DeBERTa model that can rate deliberation summaries from individual perspectives. DeliberationJudge is more efficient and more aligned with human judgements compared to a wide range of LLM judges. With DeliberationJudge, we evaluate 18 LLMs and reveal persistent weaknesses in deliberation summarization, especially underrepresentation of minority positions. Our framework provides a scalable and reliable way to evaluate deliberation summarization, helping ensure AI systems are more representative and equitable for policymaking.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A novel hallucination classification framework</title>
<link>https://arxiv.org/abs/2510.05189</link>
<guid>https://arxiv.org/abs/2510.05189</guid>
<content:encoded><![CDATA[
<div> hallucination, language model, detection, taxonomy, embedding <br />
Summary: <br />
This article presents a new method for automatically detecting hallucinations generated by large language models (LLMs). By creating a taxonomy of hallucination types and manipulating prompts, a dataset is generated for analysis. Using unsupervised learning techniques, the severity of informational distortion in hallucinations is correlated with their distance from correct outputs. The research shows that simple classification algorithms can effectively differentiate between hallucinations and accurate responses within an LLM, enhancing model reliability. The methodology offers a lightweight approach for improving LLM performance. <div>
arXiv:2510.05189v1 Announce Type: new 
Abstract: This work introduces a novel methodology for the automatic detection of hallucinations generated during large language model (LLM) inference. The proposed approach is based on a systematic taxonomy and controlled reproduction of diverse hallucination types through prompt engineering. A dedicated hallucination dataset is subsequently mapped into a vector space using an embedding model and analyzed with unsupervised learning techniques in a reduced-dimensional representation of hallucinations with veridical responses. Quantitative evaluation of inter-centroid distances reveals a consistent correlation between the severity of informational distortion in hallucinations and their spatial divergence from the cluster of correct outputs. These findings provide theoretical and empirical evidence that even simple classification algorithms can reliably distinguish hallucinations from accurate responses within a single LLM, thereby offering a lightweight yet effective framework for improving model reliability.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Let it Calm: Exploratory Annealed Decoding for Verifiable Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.05251</link>
<guid>https://arxiv.org/abs/2510.05251</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, verifiable rewards, language models, exploration, sampling <br />
<br />
Summary: 
Exploration in reinforcement learning with verifiable rewards (RLVR) is crucial for enhancing the reasoning capabilities of large language models (LLMs). The proposed Exploratory Annealed Decoding (EAD) strategy addresses the challenge of balancing sample quality and training stability by annealing the sampling temperature during generation. EAD follows an intuitive explore-at-the-beginning, exploit-at-the-end approach, allowing for high-level diversity at the start and maintaining sample quality as the temperature decreases. This method significantly enhances sample efficiency and outperforms fixed-temperature sampling across various RLVR algorithms and model sizes. By aligning exploration with the natural dynamics of sequential generation, EAD offers a lightweight and effective strategy for improving LLM reasoning. <div>
arXiv:2510.05251v1 Announce Type: new 
Abstract: Reinforcement learning with verifiable rewards (RLVR) is a powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs), yet its success hinges on effective exploration. An ideal exploration strategy must navigate two fundamental challenges: it must preserve sample quality while also ensuring training stability. While standard fixed-temperature sampling is simple, it struggles to balance these competing demands, as high temperatures degrade sample quality and low temperatures limit discovery. In this work, we propose a simpler and more effective strategy, Exploratory Annealed Decoding (EAD), grounded in the insight that exploration is most impactful on early tokens which define a sequence's semantic direction. EAD implements an intuitive **explore-at-the-beginning, exploit-at-the-end** strategy by annealing the sampling temperature from high to low during generation. This dynamic schedule encourages meaningful, high-level diversity at the start, then gradually lowers the temperature to preserve sample quality and keep the sampling distribution close to the target policy, which is essential for stable training. We demonstrate that EAD is a lightweight, plug-and-play method that significantly improves sample efficiency, consistently outperforming fixed-temperature sampling across various RLVR algorithms and model sizes. Our work suggests that aligning exploration with the natural dynamics of sequential generation offers a robust path to improving LLM reasoning.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Camellia: Benchmarking Cultural Biases in LLMs for Asian Languages</title>
<link>https://arxiv.org/abs/2510.05291</link>
<guid>https://arxiv.org/abs/2510.05291</guid>
<content:encoded><![CDATA[
<div> benchmark, multilingual, cultural biases, entity-centric, Asian languages<br />
<br />
Summary:<br />
The paper introduces a new benchmark called Camellia, designed to measure entity-centric cultural biases in nine Asian languages from six distinct Asian cultures. The benchmark includes a large dataset of entities manually annotated for association with specific Asian or Western cultures, as well as masked contexts derived from social media posts. Using Camellia, the study evaluates cultural biases in four multilingual Large Language Model (LLM) families across various tasks. The analysis reveals that LLMs struggle with cultural adaptation in Asian languages, with performance varying across models developed in regions with different access to culturally-relevant data. Additionally, the study finds that different LLM families exhibit distinct biases in associating cultures with sentiments. Furthermore, LLMs exhibit difficulties in understanding context in Asian languages, resulting in performance gaps between cultures in entity extraction. <div>
arXiv:2510.05291v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) gain stronger multilingual capabilities, their ability to handle culturally diverse entities becomes crucial. Prior work has shown that LLMs often favor Western-associated entities in Arabic, raising concerns about cultural fairness. Due to the lack of multilingual benchmarks, it remains unclear if such biases also manifest in different non-Western languages. In this paper, we introduce Camellia, a benchmark for measuring entity-centric cultural biases in nine Asian languages spanning six distinct Asian cultures. Camellia includes 19,530 entities manually annotated for association with the specific Asian or Western culture, as well as 2,173 naturally occurring masked contexts for entities derived from social media posts. Using Camellia, we evaluate cultural biases in four recent multilingual LLM families across various tasks such as cultural context adaptation, sentiment association, and entity extractive QA. Our analyses show a struggle by LLMs at cultural adaptation in all Asian languages, with performance differing across models developed in regions with varying access to culturally-relevant data. We further observe that different LLM families hold their distinct biases, differing in how they associate cultures with particular sentiments. Lastly, we find that LLMs struggle with context understanding in Asian languages, creating performance gaps between cultures in entity extraction.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAG Makes Guardrails Unsafe? Investigating Robustness of Guardrails under RAG-style Contexts</title>
<link>https://arxiv.org/abs/2510.05310</link>
<guid>https://arxiv.org/abs/2510.05310</guid>
<content:encoded><![CDATA[
<div> guardrails, large language models, safety, robustness, evaluation

Summary:<br />
The paper discusses the importance of ensuring the safety of large language models (LLMs) through the use of guardrail models. However, it highlights the vulnerability of these guardrails, such as Retrieval Augmentation Generation (RAG), to data distribution shifts. The study evaluates the reliability of LLM-based guardrails, finding that inserting benign documents into the context can alter judgments in a significant percentage of cases. The effect of different components in the augmented context, including retrieved documents, user queries, and LLM-generated responses, is analyzed. Mitigation methods tested show limited improvement in addressing the context-robustness gap in current guardrails. The results emphasize the need for training and evaluation protocols that are resilient to retrieval and query composition.<br />Summary: <div>
arXiv:2510.05310v1 Announce Type: new 
Abstract: With the increasing adoption of large language models (LLMs), ensuring the safety of LLM systems has become a pressing concern. External LLM-based guardrail models have emerged as a popular solution to screen unsafe inputs and outputs, but they are themselves fine-tuned or prompt-engineered LLMs that are vulnerable to data distribution shifts. In this paper, taking Retrieval Augmentation Generation (RAG) as a case study, we investigated how robust LLM-based guardrails are against additional information embedded in the context. Through a systematic evaluation of 3 Llama Guards and 2 GPT-oss models, we confirmed that inserting benign documents into the guardrail context alters the judgments of input and output guardrails in around 11% and 8% of cases, making them unreliable. We separately analyzed the effect of each component in the augmented context: retrieved documents, user query, and LLM-generated response. The two mitigation methods we tested only bring minor improvements. These results expose a context-robustness gap in current guardrails and motivate training and evaluation protocols that are robust to retrieval and query composition.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WeatherArchive-Bench: Benchmarking Retrieval-Augmented Reasoning for Historical Weather Archives</title>
<link>https://arxiv.org/abs/2510.05336</link>
<guid>https://arxiv.org/abs/2510.05336</guid>
<content:encoded><![CDATA[
<div> benchmark, historical weather archives, retrieval-augmented generation, societal vulnerability, resilience<br />
<br />
Summary:<br />
The article introduces WeatherArchive-Bench, a benchmark for evaluating retrieval-augmented generation (RAG) systems on historical weather archives. These archives provide valuable insights into societal vulnerability and resilience in extreme weather events. The benchmark comprises two tasks: WeatherArchive-Retrieval, to locate historically relevant passages, and WeatherArchive-Assessment, to classify societal indicators. Experiments show that dense retrievers struggle with historical terminology, while Large Language Models (LLMs) often misinterpret vulnerability and resilience concepts. The research highlights the need for more robust climate-focused RAG systems in archival contexts. The dataset and evaluation framework are publicly available for further research. <br /><br />Summary: <div>
arXiv:2510.05336v1 Announce Type: new 
Abstract: Historical archives on weather events are collections of enduring primary source records that offer rich, untapped narratives of how societies have experienced and responded to extreme weather events. These qualitative accounts provide insights into societal vulnerability and resilience that are largely absent from meteorological records, making them valuable for climate scientists to understand societal responses. However, their vast scale, noisy digitized quality, and archaic language make it difficult to transform them into structured knowledge for climate research. To address this challenge, we introduce WeatherArchive-Bench, the first benchmark for evaluating retrieval-augmented generation (RAG) systems on historical weather archives. WeatherArchive-Bench comprises two tasks: WeatherArchive-Retrieval, which measures a system's ability to locate historically relevant passages from over one million archival news segments, and WeatherArchive-Assessment, which evaluates whether Large Language Models (LLMs) can classify societal vulnerability and resilience indicators from extreme weather narratives. Extensive experiments across sparse, dense, and re-ranking retrievers, as well as a diverse set of LLMs, reveal that dense retrievers often fail on historical terminology, while LLMs frequently misinterpret vulnerability and resilience concepts. These findings highlight key limitations in reasoning about complex societal indicators and provide insights for designing more robust climate-focused RAG systems from archival contexts. The constructed dataset and evaluation framework are publicly available at https://anonymous.4open.science/r/WeatherArchive-Bench/.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Residualized Similarity for Faithfully Explainable Authorship Verification</title>
<link>https://arxiv.org/abs/2510.05362</link>
<guid>https://arxiv.org/abs/2510.05362</guid>
<content:encoded><![CDATA[
<div> Keywords: Authorship Verification, Interpretability, Neural Networks, Residualized Similarity, Performance Improvement

Summary:
Responsible use of Authorship Verification systems requires high accuracy and interpretability. Current neural methods, while accurate, lack interpretability. The new method introduced in this paper, Residualized Similarity (RS), combines interpretable features with neural networks to improve performance while maintaining interpretability. Authorship verification is essentially a task of measuring document similarity. RS uses a neural network to predict a similarity residual, highlighting the error in the interpretable system's similarity prediction. Evaluation across four datasets demonstrates that RS can match state-of-the-art models while providing insight into the faithfulness and interpretability of predictions. This novel approach bridges the gap between accuracy and interpretability in Authorship Verification, offering a promising solution for real-world applications. 

<br /><br />Summary: <div>
arXiv:2510.05362v1 Announce Type: new 
Abstract: Responsible use of Authorship Verification (AV) systems not only requires high accuracy but also interpretable solutions. More importantly, for systems to be used to make decisions with real-world consequences requires the model's prediction to be explainable using interpretable features that can be traced to the original texts. Neural methods achieve high accuracies, but their representations lack direct interpretability. Furthermore, LLM predictions cannot be explained faithfully -- if there is an explanation given for a prediction, it doesn't represent the reasoning process behind the model's prediction. In this paper, we introduce Residualized Similarity (RS), a novel method that supplements systems using interpretable features with a neural network to improve their performance while maintaining interpretability. Authorship verification is fundamentally a similarity task, where the goal is to measure how alike two documents are. The key idea is to use the neural network to predict a similarity residual, i.e. the error in the similarity predicted by the interpretable system. Our evaluation across four datasets shows that not only can we match the performance of state-of-the-art authorship verification models, but we can show how and to what degree the final prediction is faithful and interpretable.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The End of Transformers? On Challenging Attention and the Rise of Sub-Quadratic Architectures</title>
<link>https://arxiv.org/abs/2510.05364</link>
<guid>https://arxiv.org/abs/2510.05364</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformers, attention mechanism, sequence processing, computational complexity, benchmark results

Summary:
This paper examines the dominance of transformers in sequence processing tasks, particularly language modeling, and highlights the limitation posed by the quadratic complexity of their attention mechanism. It surveys recent efforts to address this bottleneck, including sub-quadratic attention variants, recurrent neural networks, state space models, and hybrid architectures. The analysis considers factors such as compute and memory complexity, benchmark results, and fundamental limitations to assess the potential challenge to the supremacy of pure-attention transformers. By exploring advancements in various models and architectures, the paper provides a critical evaluation of the strategies adopted to improve the efficiency and performance of sequence processing tasks. It raises important questions about the future trajectory of transformers in the field and whether alternative approaches could offer viable alternatives to overcome the computational challenges inherent in traditional attention mechanisms.<br /><br />Summary: <div>
arXiv:2510.05364v1 Announce Type: new 
Abstract: Transformers have dominated sequence processing tasks for the past seven years -- most notably language modeling. However, the inherent quadratic complexity of their attention mechanism remains a significant bottleneck as context length increases. This paper surveys recent efforts to overcome this bottleneck, including advances in (sub-quadratic) attention variants, recurrent neural networks, state space models, and hybrid architectures. We critically analyze these approaches in terms of compute and memory complexity, benchmark results, and fundamental limitations to assess whether the dominance of pure-attention transformers may soon be challenged.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context Length Alone Hurts LLM Performance Despite Perfect Retrieval</title>
<link>https://arxiv.org/abs/2510.05381</link>
<guid>https://arxiv.org/abs/2510.05381</guid>
<content:encoded><![CDATA[
<div> retrieval, large language models, long-context tasks, performance, limitation 

Summary:
- Large language models (LLMs) face challenges in scaling their performance with the length of input on long-context tasks, attributed to retrieval failures.
- Even with perfect retrieval of relevant information, LLMs experience significant performance degradation as input length increases.
- This performance drop occurs even when irrelevant tokens are removed or masked, indicating a limitation based solely on input length.
- Experimentation across various tasks showed performance drops up to 85%, highlighting the issue across different scenarios.
- A model-agnostic mitigation strategy was proposed, prompting LLMs to recite retrieved evidence before problem-solving, resulting in up to a 4% improvement on tasks like RULER. 

<br /><br />Summary: <div>
arXiv:2510.05381v1 Announce Type: new 
Abstract: Large language models (LLMs) often fail to scale their performance on long-context tasks performance in line with the context lengths they support. This gap is commonly attributed to retrieval failures -- the models' inability to identify relevant information in the long inputs. Accordingly, recent efforts often focus on evaluating and improving LLMs' retrieval performance: if retrieval is perfect, a model should, in principle, perform just as well on a long input as it does on a short one -- or should it? This paper presents findings that the answer to this question may be negative. Our systematic experiments across 5 open- and closed-source LLMs on math, question answering, and coding tasks reveal that, even when models can perfectly retrieve all relevant information, their performance still degrades substantially (13.9%--85%) as input length increases but remains well within the models' claimed lengths. This failure occurs even when the irrelevant tokens are replaced with minimally distracting whitespace, and, more surprisingly, when they are all masked and the models are forced to attend only to the relevant tokens. A similar performance drop is observed when all relevant evidence is placed immediately before the question. Our findings reveal a previously-unrealized limitation: the sheer length of the input alone can hurt LLM performance, independent of retrieval quality and without any distraction. They motivate our simple, model-agnostic mitigation strategy that transforms a long-context task into a short-context one by prompting the model to recite the retrieved evidence before attempting to solve the problem. On RULER, we observe a consistent improvement of GPT-4o up to 4% on an already strong baseline.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Lingual Mental Health Ontologies for Indian Languages: Bridging Patient Expression and Clinical Understanding through Explainable AI and Human-in-the-Loop Validation</title>
<link>https://arxiv.org/abs/2510.05387</link>
<guid>https://arxiv.org/abs/2510.05387</guid>
<content:encoded><![CDATA[
<div> keywords: Mental health communication, India, cross-linguistic graphs, patient distress expressions, NLP <br />
Summary: <br />
- Mental health communication in India is complex due to linguistic diversity and cultural variations, often not adequately addressed in clinical NLP frameworks.
- Current health ontologies focus on English or Western culture, leading to a lack of representation of Indian language expressions of distress.
- The proposed framework, CL-PDE, utilizes graph-based methods to capture culturally embedded expressions of distress, align them across languages, and link them with clinical terminology.
- By creating cross-lingual mental health ontologies, the approach aims to bridge the gap in healthcare communication and provide more inclusive NLP tools for mental health care in multilingual settings.
- Grounding AI systems in culturally valid representations enables the development of patient-centric and culturally sensitive tools for mental health support. <br /> 
Summary: <div>
arXiv:2510.05387v1 Announce Type: new 
Abstract: Mental health communication in India is linguistically fragmented, culturally diverse, and often underrepresented in clinical NLP. Current health ontologies and mental health resources are dominated by diagnostic frameworks centered on English or Western culture, leaving a gap in representing patient distress expressions in Indian languages. We propose cross-linguistic graphs of patient stress expressions (CL-PDE), a framework for building cross-lingual mental health ontologies through graph-based methods that capture culturally embedded expressions of distress, align them across languages, and link them with clinical terminology. Our approach addresses critical gaps in healthcare communication by grounding AI systems in culturally valid representations, allowing more inclusive and patient-centric NLP tools for mental health care in multilingual contexts.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Language Models with Clinical Expertise: DPO for Heart Failure Nursing Documentation in Critical Care</title>
<link>https://arxiv.org/abs/2510.05410</link>
<guid>https://arxiv.org/abs/2510.05410</guid>
<content:encoded><![CDATA[
<div> Keywords: Nursing documentation, intensive care units, heart failure care, Direct Preference Optimization, electronic health record systems <br />
<br />
Summary: DPO was applied to adapt Mistral-7B language model using heart failure nursing notes from the MIMIC-III database and expert-verified preference pairs. Evaluation showed significant improvement in documentation quality, with BLEU increasing by 84% and BERTScore improving by 7.6%. Expert ratings also increased in accuracy, completeness, logical consistency, readability, and structural clarity. These results demonstrate the effectiveness of DPO in aligning clinical language models with expert standards. This can support AI-assisted documentation in electronic health record systems, reducing administrative burden and enhancing ICU patient safety. <div>
arXiv:2510.05410v1 Announce Type: new 
Abstract: Nursing documentation in intensive care units (ICUs) provides essential clinical intelligence but often suffers from inconsistent terminology, informal styles, and lack of standardization, challenges that are particularly critical in heart failure care. This study applies Direct Preference Optimization (DPO) to adapt Mistral-7B, a locally deployable language model, using 8,838 heart failure nursing notes from the MIMIC-III database and 21,210 preference pairs derived from expert-verified GPT outputs, model generations, and original notes. Evaluation across BLEU, ROUGE, BERTScore, Perplexity, and expert qualitative assessments demonstrates that DPO markedly enhances documentation quality. Specifically, BLEU increased by 84% (0.173 to 0.318), BERTScore improved by 7.6% (0.828 to 0.891), and expert ratings rose across accuracy (+14.4 points), completeness (+14.5 points), logical consistency (+14.1 points), readability (+11.1 points), and structural clarity (+6.0 points). These results indicate that DPO can align lightweight clinical language models with expert standards, supporting privacy-preserving, AI-assisted documentation within electronic health record systems to reduce administrative burden and improve ICU patient safety.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Lightweight Large Language Model-Based Multi-Agent System for 2D Frame Structural Analysis</title>
<link>https://arxiv.org/abs/2510.05414</link>
<guid>https://arxiv.org/abs/2510.05414</guid>
<content:encoded><![CDATA[
<div> geometry, finite element modeling, multi-agent system, automation, structural engineering

Summary:
This paper introduces a multi-agent system powered by a Large Language Model (LLM) to automate finite element modeling of 2D frames in structural engineering. The system uses specialized agents with the Llama-3.3 70B Instruct model to handle different subtasks in the modeling process. Starting with a Problem Analysis Agent, the system extracts inputs and parameters from users, followed by a Geometry Agent that derives node coordinates and element connectivity using expert-defined rules. A Translation Agent then converts the structured outputs into executable code for OpenSeesPy, which is further validated by a Model Validation Agent for consistency. Finally, a Load Agent applies load conditions to the structural model. Experimental evaluations show that the system achieves high accuracy levels, outperforming other existing models like Gemini-2.5 Pro and ChatGPT-4o in most cases. <div>
arXiv:2510.05414v1 Announce Type: new 
Abstract: Large language models (LLMs) have recently been used to empower autonomous agents in engineering, significantly improving automation and efficiency in labor-intensive workflows. However, their potential remains underexplored in structural engineering, particularly for finite element modeling tasks requiring geometric modeling, complex reasoning, and domain knowledge. To bridge this gap, this paper develops a LLM-based multi-agent system to automate finite element modeling of 2D frames. The system decomposes structural analysis into subtasks, each managed by a specialized agent powered by the lightweight Llama-3.3 70B Instruct model. The workflow begins with a Problem Analysis Agent, which extracts geometry, boundary, and material parameters from the user input. Next, a Geometry Agent incrementally derives node coordinates and element connectivity by applying expert-defined rules. These structured outputs are converted into executable OpenSeesPy code by a Translation Agent and refined by a Model Validation Agent through consistency checks. Then, a Load Agent applies load conditions into the assembled structural model. Experimental evaluations on 20 benchmark problems demonstrate that the system achieves accuracy over 80% in most cases across 10 repeated trials, outperforming Gemini-2.5 Pro and ChatGPT-4o models.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Filtered Distillation with LLMs-generated Trust Indicators for Reliable Patent Classification</title>
<link>https://arxiv.org/abs/2510.05431</link>
<guid>https://arxiv.org/abs/2510.05431</guid>
<content:encoded><![CDATA[
<div> trust signals, rationales, patent classification, Self-Filtered Distillation, interpretability<br />
<br />
Large language models (LLMs) generate natural language rationales to enhance interpretability in patent classification. However, these rationales may contain logical errors and domain-specific misalignments, leading to noise during training. To address this, Self-Filtered Distillation utilizes trust metrics, including Self-Consistency, Class Entailment Alignment, and LLM Agreement Scoring, to assess the quality of LLM-generated rationales. These metrics are combined into a trust score that guides the training process, filtering out low-trust cases. Experimental results on the USPTO-2M dataset demonstrate that the proposed method surpasses traditional label-based learning and conventional distillation in accuracy, stability, and interpretability, providing a reliable framework for reasoning-aware supervision in patent analytics.<br /><br />Summary: <div>
arXiv:2510.05431v1 Announce Type: new 
Abstract: Large language models (LLMs) increasingly generate natural language rationales to enhance interpretability, but these often contain logical errors, label mismatches, and domain-specific misalignments. Directly using such rationales as supervision risks propagating noise and undermining training stability. To address this challenge, we introduce Self-Filtered Distillation, a framework specifically tailored for patent classification, which treats LLM-generated rationales as trust signals rather than ground-truth supervision. The framework employs selective distillation guided by three unsupervised trust metrics: (1) Self-Consistency, which measures the stability of LLM-generated rationales across multiple generations; (2) Class Entailment Alignment, which assesses semantic coherence with patent-specific class definitions; and (3) LLM Agreement Scoring, which validates rationale-label plausibility. These metrics are integrated into a unified trust score that primarily weights training samples while optionally filtering out extremely low-trust cases, enabling reasoning-aware supervision. Experiments on the USPTO-2M dataset, a widely used benchmark for patent classification, show that our method outperforms label-based learning and conventional distillation in accuracy, stability, and interpretability, establishing a reliable paradigm for leveraging reasoning-aware trust indicators in patent analytics.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimulatorArena: Are User Simulators Reliable Proxies for Multi-Turn Evaluation of AI Assistants?</title>
<link>https://arxiv.org/abs/2510.05444</link>
<guid>https://arxiv.org/abs/2510.05444</guid>
<content:encoded><![CDATA[
<div> SimulatorArena, benchmark, human-LLM conversations, interactive tasks, simulators<br />
<br />
Summary: 
SimulatorArena introduces a benchmark for evaluating simulated users in interactive tasks, such as math tutoring and document creation. The benchmark consists of 909 annotated human-LLM conversations and assesses simulators based on their alignment with human behavior and assistant ratings. Simulators conditioned on user profiles perform well, with a Spearman's ρ of 0.7 on both tasks, indicating a reliable alternative to human evaluation. The study evaluates 18 assistants, including GPT-5, Claude 4.1 Opus, and Gemini 2.5 Pro, using the best simulator for each task. These findings demonstrate the potential for simulators to effectively simulate real user behavior, offering a scalable and practical method for evaluating large language models in interactive applications. <br /><br /> <div>
arXiv:2510.05444v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used in interactive applications, and human evaluation remains the gold standard for assessing their performance in multi-turn conversations. Since human studies are costly, time-consuming, and hard to reproduce, recent work explores using LLMs to simulate users for automatic assistant evaluation. However, there is no benchmark or systematic study to evaluate whether these simulated users are reliable stand-ins for real users. To address this, we introduce SimulatorArena, a benchmark of 909 annotated human-LLM conversations on two interactive tasks -- math tutoring and document creation. SimulatorArena evaluates simulators based on how closely their messages match human behavior and how well their assistant ratings align with human judgments. Experiments on various simulator methods show that simulators conditioned on user profiles, capturing traits like background and message styles, align closely with human judgments. They reach Spearman's $\rho$ of 0.7 on both tasks, providing a practical, scalable alternative to human evaluation. Using the best simulator for each task, we benchmark 18 assistants, including the latest LLMs such as GPT-5, Claude 4.1 Opus, and Gemini 2.5 Pro.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentRouter: A Knowledge-Graph-Guided LLM Router for Collaborative Multi-Agent Question Answering</title>
<link>https://arxiv.org/abs/2510.05445</link>
<guid>https://arxiv.org/abs/2510.05445</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, agent-based frameworks, multi-agent QA, knowledge graph, graph neural network

Summary:
Large language models (LLMs) and agent-based frameworks have advanced rapidly, offering diverse applications. However, selecting the best configuration for a downstream task is challenging due to the multitude of models and strategies available. Current research shows that different agents and backbones have unique strengths, emphasizing the need for adaptive routing mechanisms. To address this, a new framework called tAgentRouter is proposed, which treats multi-agent QA as a knowledge-graph-guided routing problem. By converting QA instances into a knowledge graph and training a heterogeneous graph neural network, AgentRouter learns task-specific routing distributions over agents. Through soft supervision and weighted aggregation, the framework achieves superior performance compared to single-agent and ensemble baselines. Extensive experiments demonstrate the effectiveness and robustness of graph-supervised multi-agent routing for question answering.

<br /><br />Summary: Large language models and agent-based frameworks have quickly evolved, offering diverse applications. Practitioners face uncertainty in choosing the best configuration for a task due to the variety of models and strategies, necessitating adaptive routing mechanisms. tAgentRouter, a knowledge-graph-guided routing framework, addresses this by converting QA instances into a knowledge graph and training a graph neural network to determine optimal routing distributions over agents. The framework outperforms single-agent and ensemble baselines through soft supervision and weighted aggregation, demonstrating its effectiveness and robustness for question answering tasks across benchmarks and LLM backbones. <div>
arXiv:2510.05445v1 Announce Type: new 
Abstract: Large language models (LLMs) and agent-based frameworks have advanced rapidly, enabling diverse applications. Yet, with the proliferation of models and agentic strategies, practitioners face substantial uncertainty in selecting the best configuration for a downstream task. Prior studies show that different agents and backbones exhibit complementary strengths, and that larger models are not always superior, underscoring the need for adaptive routing mechanisms. Existing approaches to agent routing, however, often emphasize cost efficiency while overlooking the fine-grained contextual and relational structure inherent in QA tasks. In this paper, we propose tAgentRouter, a framework that formulates multi-agent QA as a knowledge-graph-guided routing problem supervised by empirical performance signals. Specifically, we convert QA instance into a knowledge graph that jointly encodes queries, contextual entities, and agents, and then train a heterogeneous graph neural network (GNN) to propagate information across node types and produce task-aware routing distributions over agents. By leveraging soft supervision and weighted aggregation of agent outputs, AgentRouter learns principled collaboration schemes that capture the complementary strengths of diverse agents. Extensive experiments demonstrate that our framework consistently outperforms single-agent and ensemble baselines, while generalizing across benchmarks and LLM backbones. These results highlight the effectiveness and robustness of graph-supervised multi-agent routing for question answering.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SocialNLI: A Dialogue-Centric Social Inference Dataset</title>
<link>https://arxiv.org/abs/2510.05458</link>
<guid>https://arxiv.org/abs/2510.05458</guid>
<content:encoded><![CDATA[
<div> Keywords: theory-of-mind, social dialogue, sarcasm, irony, AI assistants <br />
<br />
Summary: <br />
The article introduces SocialNLI (SoNLI), the first social dialogue inference dataset designed to assess the social abilities of AI models. Understanding complex social nuances like sarcasm and irony is crucial for AI assistants to excel in human dialogue comprehension. The dataset includes hand-picked dialogue transcripts with social nuances, corresponding inferences, likelihood scores, and human-written explanations. The study focuses on social inference analysis as a component of theory-of-mind and evaluates the theory-of-mind ability of large language models (LLM) and reasoning models through multi-step counterfactual reasoning. By identifying the weaknesses of current models in understanding social phenomena in dialogue, the research aims to propose solutions to enhance AI assistants' social abilities. <div>
arXiv:2510.05458v1 Announce Type: new 
Abstract: Making theory-of-mind inferences from human dialogue is a strong indicator of a model's underlying social abilities, which are fundamental for adept AI assistants. However, large language and reasoning models struggle to understand sophisticated social phenomena in transcript data, such as sarcasm and irony. To assess the weaknesses of current models and to identify their solutions, we introduce SocialNLI (SoNLI) -- the first social dialogue inference dataset. SoNLI consists of a collection of dialogue transcripts hand-picked to center complex social nuances like irony and sarcasm, paired with inferences, corresponding likelihood scores, and human-written explanations. We explore social inference analysis as a facet of theory-of-mind, and evaluate LLM and reasoning model theory-of-mind ability through multi-step counterfactual reasoning.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TensorBLEU: Vectorized GPU-based BLEU Score Implementation for Per-Sentence In-Training Evaluation</title>
<link>https://arxiv.org/abs/2510.05485</link>
<guid>https://arxiv.org/abs/2510.05485</guid>
<content:encoded><![CDATA[
<div> BLEU, natural language processing, evaluation, GPU-accelerated, PyTorch

Summary:<br />
- The article introduces TensorBLEU, a novel implementation of the BLEU metric designed for efficient evaluation of natural language processing models on GPUs.
- TensorBLEU is fully vectorized for GPU-accelerated per-sentence computation in PyTorch and introduces a memory-efficient counting mechanism.
- By creating a compact batch-specific dictionary of n-grams, TensorBLEU avoids the memory costs of traditional hashing-based vectorization, making it practical for large-vocabulary models.
- Benchmarking against NLTK, experiments show that TensorBLEU provides significant speedups on consumer-grade GPUs and data-center-class hardware.
- TensorBLEU's performance improvement transforms a significant bottleneck into a negligible part of the training loop, accelerating research in areas like RL-based model fine-tuning.

<br /><br />Summary: <div>
arXiv:2510.05485v1 Announce Type: new 
Abstract: Modern natural language processing models have achieved unprecedented scale, yet the tools for their evaluation often remain a computational bottleneck, limiting the pace of research. This is particularly acute for in-training evaluation metrics, such as per-sentence reward signals in Reinforcement Learning, which must operate efficiently on batches of token IDs directly on the GPU. In this paper, we introduce TensorBLEU, a novel implementation of the BLEU metric designed from the ground up for this specific use case. Our approach is fully vectorized for GPU-accelerated, per-sentence computation within PyTorch and introduces a memory-efficient counting mechanism. By creating a compact, batch-specific dictionary of n-grams using \texttt{torch.unique}, our method avoids the prohibitive memory costs of traditional hashing-based vectorization, making it practical for large-vocabulary models. We benchmark TensorBLEU against NLTK, the standard library for token-ID-based BLEU calculation on the CPU. Experiments show that TensorBLEU provides speedups of over 13x on consumer-grade GPUs (NVIDIA T4) and exceeding 40x on data-center-class hardware (NVIDIA A100). This performance transforms a significant bottleneck into a negligible part of the training loop. By clearly defining its role as a "Token-ID BLEU" for development purposes and open-sourcing our implementation, we provide a powerful tool for accelerating research in areas like RL-based model fine-tuning.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Model as Planner and Formalizer under Constraints</title>
<link>https://arxiv.org/abs/2510.05486</link>
<guid>https://arxiv.org/abs/2510.05486</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, planning benchmarks, natural language constraints, reasoning, robustness 

Summary:
This study examines the use of large language models (LLMs) in planning tasks and highlights the limitations of existing benchmarks that do not account for real-world complexities. By augmenting traditional planning benchmarks with fine-grained natural language constraints, the researchers aimed to assess the performance and robustness of LLMs. The introduction of constraints led to a significant decrease in performance across multiple LLM models, formal languages, methods, and datasets. The study demonstrates that incorporating natural language constraints poses challenges to the planning ability and robustness of LLMs, emphasizing the importance of addressing real-world complexities in planning tasks. These findings suggest the need for more realistic and comprehensive benchmarks to accurately evaluate the capabilities of LLMs in planning tasks. 

<br /><br />Summary: <div>
arXiv:2510.05486v1 Announce Type: new 
Abstract: LLMs have been widely used in planning, either as planners to generate action sequences end-to-end, or as formalizers to represent the planning domain and problem in a formal language that can derive plans deterministically. However, both lines of work rely on standard benchmarks that only include generic and simplistic environmental specifications, leading to potential overestimation of the planning ability of LLMs and safety concerns in downstream tasks. We bridge this gap by augmenting widely used planning benchmarks with manually annotated, fine-grained, and rich natural language constraints spanning four formally defined categories. Over 4 state-of-the-art reasoning LLMs, 3 formal languages, 5 methods, and 4 datasets, we show that the introduction of constraints not only consistently halves performance, but also significantly challenges robustness to problem complexity and lexical shift.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LANTERN: Scalable Distillation of Large Language Models for Job-Person Fit and Explanation</title>
<link>https://arxiv.org/abs/2510.05490</link>
<guid>https://arxiv.org/abs/2510.05490</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, job-person fit, explanation, knowledge distillation, domain-specific applications 

Summary: 
Large language models (LLMs) have shown exceptional performance in natural language processing tasks, but deploying them for domain-specific applications like job-person fit at scale presents challenges. LinkedIn introduced LANTERN, a customized LLM knowledge distillation framework for this purpose. LANTERN tackles the complexity of the job-person fit task by incorporating multiple objectives, an encoder model for classification, and a decoder model for explanation. The framework involves multi-level knowledge distillation to transfer knowledge effectively from a powerful teacher model to downstream models. Post-training techniques and prompt engineering are crucial for adapting LLMs successfully. Experimental results demonstrate that LANTERN improves task-specific metrics, enhances job seeker engagement, and increases the apply rate and qualified applications on the platform. Overall, LANTERN offers a significant advancement in utilizing LLMs for domain-specific applications like job-person fit. 

<br /><br />Summary: <div>
arXiv:2510.05490v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved strong performance across a wide range of natural language processing tasks. However, deploying LLMs at scale for domain specific applications, such as job-person fit and explanation in job seeking platforms, introduces distinct challenges. At LinkedIn, the job person fit task requires analyzing a candidate's public profile against job requirements to produce both a fit assessment and a detailed explanation. Directly applying open source or finetuned LLMs to this task often fails to yield high quality, actionable feedback due to the complexity of the domain and the need for structured outputs. Moreover, the large size of these models leads to high inference latency and limits scalability, making them unsuitable for online use. To address these challenges, we introduce LANTERN, a novel LLM knowledge distillation framework tailored specifically for job person fit tasks. LANTERN involves modeling over multiple objectives, an encoder model for classification purpose, and a decoder model for explanation purpose. To better distill the knowledge from a strong black box teacher model to multiple downstream models, LANTERN incorporates multi level knowledge distillation that integrates both data and logit level insights. In addition to introducing the knowledge distillation framework, we share our insights on post training techniques and prompt engineering, both of which are crucial for successfully adapting LLMs to domain specific downstream tasks. Extensive experimental results demonstrate that LANTERN significantly improves task specific metrics for both job person fit and explanation. Online evaluations further confirm its effectiveness, showing measurable gains in job seeker engagement, including a 0.24\% increase in apply rate and a 0.28\% increase in qualified applications.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prototype-Based Dynamic Steering for Large Language Models</title>
<link>https://arxiv.org/abs/2510.05498</link>
<guid>https://arxiv.org/abs/2510.05498</guid>
<content:encoded><![CDATA[
<div> steering, language model, reasoning, prototypes, dynamic<br />
Summary:<br />
Prototype-Based Dynamic Steering (PDS) is a test-time method that enhances large language model (LLM) reasoning without explicit instructions. It involves creating "reasoning prototypes" through clustering activation differences between different prompts. These prototypes are used to generate an instance-specific steering vector, improving accuracy on various tasks without requiring fine-tuning. The approach is effective even when the original reasoning prompts are suppressed, suggesting a lasting impact on underlying reasoning processes. PDS offers a lightweight alternative to traditional training-based methods for boosting LLM performance, making it a promising tool for adaptive reasoning amplification. <br /> <div>
arXiv:2510.05498v1 Announce Type: new 
Abstract: Despite impressive breadth, LLMs still rely on explicit reasoning instructions or static, one-fits-all steering methods, leaving a gap for adaptive, instruction-free reasoning amplification. We present Prototype-Based Dynamic Steering (PDS), a test-time method that amplifies large language model (LLM) reasoning without adding or altering instructions. We introduce "reasoning prototypes" by clustering activation differences between Chain-of-Thought (CoT) and neutral prompts. At inference, an input's hidden state is projected onto these prototypes to form an instance-specific steering vector. Evaluated on GSM8K, AQuA-RAT, and BIG-Bench tasks, PDS consistently improves accuracy without fine-tuning or prompt engineering. Notably, the gains persist even when CoT is explicitly suppressed to improve cost-efficiency, indicating that the intervention strengthens latent reasoning processes rather than inducing a superficial behavioral shift. These results position dynamic, prototype-guided steering as a lightweight alternative to training-time approaches for enhancing LLM reasoning.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAM: A Constructivist View of Agentic Memory for LLM-Based Reading Comprehension</title>
<link>https://arxiv.org/abs/2510.05520</link>
<guid>https://arxiv.org/abs/2510.05520</guid>
<content:encoded><![CDATA[
<div> memory module, large language models, reading comprehension, CAM, Constructivist Theory <br />
<br />
In this article, the authors address the challenge faced by current Large Language Models (LLMs) in processing long-form documents by proposing a novel memory module inspired by Jean Piaget's Constructivist Theory. The Constructivist Agentic Memory (CAM) is designed to enhance LLMs' reading comprehension abilities by incorporating structured schemata, flexible assimilation, and dynamic accommodation. CAM utilizes an incremental overlapping clustering algorithm for structured memory development, enabling coherent hierarchical summarization and online batch integration. During inference, CAM adaptively explores the memory structure to activate query-relevant information, similar to human associative processes. Experimental results show that CAM outperforms existing approaches in various tasks, including question answering, query-based summarization, and claim verification, demonstrating improvements in both performance and efficiency. <br /><br />Summary: <div>
arXiv:2510.05520v1 Announce Type: new 
Abstract: Current Large Language Models (LLMs) are confronted with overwhelming information volume when comprehending long-form documents. This challenge raises the imperative of a cohesive memory module, which can elevate vanilla LLMs into autonomous reading agents. Despite the emergence of some heuristic approaches, a systematic design principle remains absent. To fill this void, we draw inspiration from Jean Piaget's Constructivist Theory, illuminating three traits of the agentic memory -- structured schemata, flexible assimilation, and dynamic accommodation. This blueprint forges a clear path toward a more robust and efficient memory system for LLM-based reading comprehension. To this end, we develop CAM, a prototype implementation of Constructivist Agentic Memory that simultaneously embodies the structurality, flexibility, and dynamicity. At its core, CAM is endowed with an incremental overlapping clustering algorithm for structured memory development, supporting both coherent hierarchical summarization and online batch integration. During inference, CAM adaptively explores the memory structure to activate query-relevant information for contextual response, akin to the human associative process. Compared to existing approaches, our design demonstrates dual advantages in both performance and efficiency across diverse long-text reading comprehension tasks, including question answering, query-based summarization, and claim verification.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KEO: Knowledge Extraction on OMIn via Knowledge Graphs and RAG for Safety-Critical Aviation Maintenance</title>
<link>https://arxiv.org/abs/2510.05524</link>
<guid>https://arxiv.org/abs/2510.05524</guid>
<content:encoded><![CDATA[
<div> Knowledge Extraction on OMIn, domain-specific knowledge extraction and reasoning framework, employs large language models (LLMs) in safety-critical contexts. The framework utilizes Operations and Maintenance Intelligence (OMIn) dataset to create a QA benchmark for global sensemaking and actionable maintenance tasks. By building a structured Knowledge Graph (KG) and integrating it into a retrieval-augmented generation (RAG) pipeline, KEO enables more coherent, dataset-wide reasoning compared to traditional text-chunk RAG. Local deployable LLMs like Gemma-3, Phi-4, Mistral-Nemo and stronger models such as GPT-4o, Llama-3.3 are evaluated in the framework. Results show that KEO enhances global sensemaking by unveiling patterns and system-level insights, while text-chunk RAG is effective for fine-grained procedural tasks requiring localized retrieval. The study highlights the potential of KG-augmented LLMs for secure, domain-specific QA and their significance in high-stakes reasoning. 

Keywords: Knowledge Extraction, OMIn, Large Language Models, Knowledge Graph, Global Sensemaking

<br /><br />Summary: 
- Knowledge Extraction on OMIn framework utilizes large language models (LLMs) for domain-specific knowledge extraction in safety-critical contexts.
- The framework constructs a QA benchmark using the OMIn dataset for global sensemaking and actionable maintenance tasks.
- By integrating a structured Knowledge Graph (KG) into a retrieval-augmented generation (RAG) pipeline, it enhances dataset-wide reasoning.
- Evaluation of local deployable LLMs and stronger models demonstrates improved global sensemaking and insightful system-level patterns.
- The study emphasizes the potential of KG-augmented LLMs in secure, domain-specific QA for high-stakes reasoning. <div>
arXiv:2510.05524v1 Announce Type: new 
Abstract: We present Knowledge Extraction on OMIn (KEO), a domain-specific knowledge extraction and reasoning framework with large language models (LLMs) in safety-critical contexts. Using the Operations and Maintenance Intelligence (OMIn) dataset, we construct a QA benchmark spanning global sensemaking and actionable maintenance tasks. KEO builds a structured Knowledge Graph (KG) and integrates it into a retrieval-augmented generation (RAG) pipeline, enabling more coherent, dataset-wide reasoning than traditional text-chunk RAG. We evaluate locally deployable LLMs (Gemma-3, Phi-4, Mistral-Nemo) and employ stronger models (GPT-4o, Llama-3.3) as judges. Experiments show that KEO markedly improves global sensemaking by revealing patterns and system-level insights, while text-chunk RAG remains effective for fine-grained procedural tasks requiring localized retrieval. These findings underscore the promise of KG-augmented LLMs for secure, domain-specific QA and their potential in high-stakes reasoning.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>H1B-KV: Hybrid One-Bit Caches for Memory-Efficient Large Language Model Inference</title>
<link>https://arxiv.org/abs/2510.05529</link>
<guid>https://arxiv.org/abs/2510.05529</guid>
<content:encoded><![CDATA[
<div> Keywords: Autoregressive decoding, Large language models, Hybrid compression, Memory efficiency, Downstream tasks

Summary: 
The paper introduces the Hybrid One-Bit KV Cache (H1B-KV) as a comprehensive compression scheme for large language models. H1B-KV reduces memory usage significantly without losing context information. It represents key vectors using a 1-bit binary sketch and compresses value vectors with 4-bit quantization, enabling efficient bitwise attention. The approach allows a 7-billion parameter LLM to handle an 8k-token context with only 60 MB of cache memory, a 70x reduction. After lightweight finetuning, H1B-KV achieves performance comparable to full precision on perplexity benchmarks and complex tasks like mathematical reasoning, multi-task understanding, and code generation. The results demonstrate that H1B-KV outperforms existing methods in quality-per-byte, making it a robust solution for deploying LLMs in memory-constrained environments. 

<br /><br />Summary: <div>
arXiv:2510.05529v1 Announce Type: new 
Abstract: Autoregressive decoding in large language models (LLMs) requires caching a growing list of past key-value (KV) pairs, making long-context inference a memory-bound problem. While recent methods have explored quantizing the cache, evicting tokens, or using binary sketches for keys (e.g., Loki), these approaches often provide an incomplete solution by leaving one component (like values) uncompressed or by discarding context information. This paper introduces the Hybrid One-Bit KV Cache (H1B-KV), a comprehensive compression scheme that radically reduces memory usage without sacrificing context. H1B-KV represents each key vector using a 1-bit binary sketch, enabling hardware-friendly bitwise attention, and further compresses value vectors using 4-bit quantization. This holistic, hybrid approach allows a 7-billion parameter LLM to handle an 8k-token context with under 60 MB of cache memory - a 70x reduction. We demonstrate that after a lightweight finetuning, H1B-KV matches full-precision performance not only on perplexity benchmarks but also on complex downstream tasks like mathematical reasoning (GSM8K), multi-task understanding (MMLU), and code generation (HumanEval). Our results show H1B-KV significantly outperforms leading quantization (KIVI), token eviction (SparseLLM), and key-only sketching (Loki) methods in quality-per-byte, establishing it as a robust solution for deploying LLMs in memory-constrained environments.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Role of Difficult Prompts in Self-Play Preference Optimization</title>
<link>https://arxiv.org/abs/2510.05534</link>
<guid>https://arxiv.org/abs/2510.05534</guid>
<content:encoded><![CDATA[
<div> difficulty, prompts, self-play optimization, large language models, reward model

Summary:
This study explores the impact of prompt difficulty on self-play preference optimization in large language models. Using mean reward as a measure of difficulty, the researchers found that difficult prompts lead to inferior performance compared to easy prompts. The inclusion of difficult prompts did not enhance overall performance and even resulted in slight degradation. However, as model capacity increased, the performance gap between difficult and easy prompts narrowed, indicating an interaction between difficulty and model capacity. To address this issue, the researchers proposed selectively removing challenging prompts, which improved self-play performance. The study also discusses failed attempts and provides insights into mitigating the negative effects of difficult prompts on final performance. <div>
arXiv:2510.05534v1 Announce Type: new 
Abstract: Self-play preference optimization has emerged as a prominent paradigm for aligning large language models (LLMs). It typically involves a language model to generate on-policy responses for prompts and a reward model (RM) to guide the selection of chosen and rejected responses, which can be further trained with direct preference optimization (DPO). However, the role of prompts remains underexplored, despite being a core component in this pipeline. In this work, we investigate how prompts of varying difficulty influence self-play preference optimization. We first use the mean reward of $N$ sampled responses of a prompt as a proxy for its difficulty. We find that difficult prompts exhibit substantially inferior self-play optimization performance in comparison to easy prompts for language models. Moreover, incorporating difficult prompts into training fails to enhance overall performance and, in fact, leads to slight degradation compared to training on easy prompts alone. We also observe that the performance gap between difficult and easy prompts closes as the model capacity increases, suggesting that difficulty interacts with the model capacity. Building on these findings, we explore strategies to mitigate the negative effect of difficult prompts on final performance. We demonstrate that selectively removing an appropriate portion of challenging prompts enhances overall self-play performance, while also reporting failed attempts and lessons learned.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Activation-Informed Pareto-Guided Low-Rank Compression for Efficient LLM/VLM</title>
<link>https://arxiv.org/abs/2510.05544</link>
<guid>https://arxiv.org/abs/2510.05544</guid>
<content:encoded><![CDATA[
arXiv:2510.05544v1 Announce Type: new 
Abstract: Large language models (LLM) and vision-language models (VLM) have achieved state-of-the-art performance, but they impose significant memory and computing challenges in deployment. We present a novel low-rank compression framework to address this challenge. First, we upper bound the change of network loss via layer-wise activation-based compression errors, filling a theoretical gap in the literature. We then formulate low-rank model compression as a bi-objective optimization and prove that a single uniform tolerance yields surrogate Pareto-optimal heterogeneous ranks. Based on our theoretical insights, we propose Pareto-Guided Singular Value Decomposition (PGSVD), a zero-shot pipeline that improves activation-aware compression via Pareto-guided rank selection and alternating least-squares implementation. We apply PGSVD to both LLM and VLM, showing better accuracy at the same compression levels and inference speedup.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Presenting a Paper is an Art: Self-Improvement Aesthetic Agents for Academic Presentations</title>
<link>https://arxiv.org/abs/2510.05571</link>
<guid>https://arxiv.org/abs/2510.05571</guid>
<content:encoded><![CDATA[
arXiv:2510.05571v1 Announce Type: new 
Abstract: The promotion of academic papers has become an important means of enhancing research visibility. However, existing automated methods struggle limited storytelling, insufficient aesthetic quality, and constrained self-adjustment, making it difficult to achieve efficient and engaging dissemination. At the heart of those challenges is a simple principle: \emph{there is no way to improve it when you cannot evaluate it right}. To address this, we introduce \textbf{EvoPresent}, a self-improvement agent framework that unifies coherent narratives, aesthetic-aware designs, and realistic presentation delivery via virtual characters. Central to EvoPresent is \textbf{PresAesth}, a multi-task reinforcement learning (RL) aesthetic model that provides reliable aesthetic scoring, defect adjustment, and comparative feedback, enabling iterative self-improvement even under limited aesthetic training data. To systematically evaluate the methods, we introduce \textbf{EvoPresent Benchmark}, a comprehensive benchmark comprising: \textit{Presentation Generation Quality}, built on 650 top-tier AI conference papers with multimodal resources (slides, videos and scripts) to assess both content and design; and \textit{Aesthetic Awareness}, consisting of 2,000 slide pairs with varying aesthetic levels, supporting joint training and evaluation on scoring, defect adjustment, and comparison. Our findings highlight that (i) High-quality feedback is essential for agent self-improvement, while initial capability alone does not guarantee effective self-correction. (ii) Automated generation pipelines exhibit a trade-off between visual design and content construction. (iii) Multi-task RL training shows stronger generalization in aesthetic awareness tasks.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mission Impossible: Feedback-Guided Dynamic Interactive Planning for Improving Reasoning on LLMs</title>
<link>https://arxiv.org/abs/2510.05577</link>
<guid>https://arxiv.org/abs/2510.05577</guid>
<content:encoded><![CDATA[
arXiv:2510.05577v1 Announce Type: new 
Abstract: Recent advancements in language agents have led to significant improvements in multi-hop reasoning tasks. However, existing approaches often struggle with handling open-domain problems, which require massive information retrieval due to their reliance on a fixed sequence of actions. To address this, we propose Feedback-Guided Dynamic Interactive Planning (FGDIP), a novel framework tailored to enhance reasoning in LLMs by utilizing dynamic and adaptive strategies for information exploration in open-domain multi-hop reasoning tasks. Our approach begins by identifying key entities relevant to the problem, which serve as the initial nodes in the reasoning process. From these initial nodes, we then generate reasoning child nodes with the process being refined through a combination of historical error analysis and real-time feedback, which allows the framework to dynamically adjust and optimize its reasoning strategies. By integrating depth-first search with an innovative node generation technique, our framework adapts based on both prior error paths and concurrently generated nodes at the same hierarchical level. This dynamic strategy effectively expands the search space while ensuring the reasoning process systematically converges toward accurate solutions. Experimental results show that FGDIP achieved up to 54.47% F1 score on the HotpotQA dataset and 70.05% on the StrategyQA dataset, surpassing the best baseline by 5.03% and 7.25% respectively, highlighting its versatility and potential to enhance language agents in multi-hop reasoning tasks.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Goal Without a Plan Is Just a Wish: Efficient and Effective Global Planner Training for Long-Horizon Agent Tasks</title>
<link>https://arxiv.org/abs/2510.05608</link>
<guid>https://arxiv.org/abs/2510.05608</guid>
<content:encoded><![CDATA[
arXiv:2510.05608v1 Announce Type: new 
Abstract: Agents based on large language models (LLMs) struggle with brainless trial-and-error and generating hallucinatory actions due to a lack of global planning in long-horizon tasks. In this paper, we introduce a plan-and-execute framework and propose EAGLET, an efficient and effective planner training method to enhance the executor agent's planning abilities without human effort. Specifically, we train a plug-and-play global planner through a two-step process: we first synthesize high-quality plans from an advanced LLM using our proposed homologous consensus filtering strategy, and apply fine-tuning as a cold start. Moreover, we further improve the planner with a rule-based reinforcement learning stage using a novel executor capability gain reward, ensuring it can handle task instructions of varying difficulty. Experiments on three long-horizon agent tasks show that executor agents equipped with our planner outperform existing methods, achieving new state-of-the-art performance. Meanwhile, EAGLET reduces training costs by 8x compared to RL-based baselines, and it does not require manual effort or extra training data, offering an efficient and effective solution.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MADIAVE: Multi-Agent Debate for Implicit Attribute Value Extraction</title>
<link>https://arxiv.org/abs/2510.05611</link>
<guid>https://arxiv.org/abs/2510.05611</guid>
<content:encoded><![CDATA[
arXiv:2510.05611v1 Announce Type: new 
Abstract: Implicit Attribute Value Extraction (AVE) is essential for accurately representing products in e-commerce, as it infers lantent attributes from multimodal data. Despite advances in multimodal large language models (MLLMs), implicit AVE remains challenging due to the complexity of multidimensional data and gaps in vision-text understanding. In this work, we introduce \textsc{\modelname}, a multi-agent debate framework that employs multiple MLLM agents to iteratively refine inferences. Through a series of debate rounds, agents verify and update each other's responses, thereby improving inference performance and robustness. Experiments on the ImplicitAVE dataset demonstrate that even a few rounds of debate significantly boost accuracy, especially for attributes with initially low performance. We systematically evaluate various debate configurations, including identical or different MLLM agents, and analyze how debate rounds affect convergence dynamics. Our findings highlight the potential of multi-agent debate strategies to address the limitations of single-agent approaches and offer a scalable solution for implicit AVE in multimodal e-commerce.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP</title>
<link>https://arxiv.org/abs/2510.05644</link>
<guid>https://arxiv.org/abs/2510.05644</guid>
<content:encoded><![CDATA[
arXiv:2510.05644v1 Announce Type: new 
Abstract: Despite representing nearly one-third of the world's languages, African languages remain critically underserved by modern NLP technologies, with 88\% classified as severely underrepresented or completely ignored in computational linguistics. We present the African Languages Lab (All Lab), a comprehensive research initiative that addresses this technological gap through systematic data collection, model development, and capacity building. Our contributions include: (1) a quality-controlled data collection pipeline, yielding the largest validated African multi-modal speech and text dataset spanning 40 languages with 19 billion tokens of monolingual text and 12,628 hours of aligned speech data; (2) extensive experimental validation demonstrating that our dataset, combined with fine-tuning, achieves substantial improvements over baseline models, averaging +23.69 ChrF++, +0.33 COMET, and +15.34 BLEU points across 31 evaluated languages; and (3) a structured research program that has successfully mentored fifteen early-career researchers, establishing sustainable local capacity. Our comparative evaluation against Google Translate reveals competitive performance in several languages while identifying areas that require continued development.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Code-Switching In-Context Learning for Cross-Lingual Transfer of Large Language Models</title>
<link>https://arxiv.org/abs/2510.05678</link>
<guid>https://arxiv.org/abs/2510.05678</guid>
<content:encoded><![CDATA[
arXiv:2510.05678v1 Announce Type: new 
Abstract: While large language models (LLMs) exhibit strong multilingual abilities, their reliance on English as latent representations creates a translation barrier, where reasoning implicitly depends on internal translation into English. When this process fails, performance in non-English languages deteriorates sharply, limiting the inclusiveness of LLM-based applications. Existing cross-lingual in-context learning (X-ICL) methods primarily leverage monolingual demonstrations, often failing to mitigate this barrier and instead reinforcing it. In this work, we introduce code-switching in-context learning (CSICL), a simple yet effective prompting strategy that progressively transitions from a target language to English within demonstrations and instruction to facilitate their latent reasoning in English. By explicitly scaffolding the reasoning process through controlled code-switching, CSICL acts as an implicit linguistic bridge that enhances cross-lingual alignment and reduces reliance on the translation barrier. We conduct extensive experiments across 4 LLMs, 6 datasets, and 10 languages, spanning both knowledge-intensive and reasoning-oriented domains. Our results demonstrate that CSICL consistently outperforms X-ICL baselines, achieving gains of 3.1%p and 1.9%p in both target and unseen languages, respectively. The improvement is even more pronounced in low-resource settings, with gains of 14.7% in target and 5.3% in unseen languages. These findings establish code-switching as a principled and robust approach for overcoming the translation barrier during inference, moving LLMs toward more equitable and effective multilingual systems.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DecEx-RAG: Boosting Agentic Retrieval-Augmented Generation with Decision and Execution Optimization via Process Supervision</title>
<link>https://arxiv.org/abs/2510.05691</link>
<guid>https://arxiv.org/abs/2510.05691</guid>
<content:encoded><![CDATA[
arXiv:2510.05691v1 Announce Type: new 
Abstract: Agentic Retrieval-Augmented Generation (Agentic RAG) enhances the processing capability for complex tasks through dynamic retrieval and adaptive workflows. Recent advances (e.g., Search-R1) have shown that outcome-supervised reinforcement learning demonstrate strong performance. However, this approach still suffers from inefficient exploration, sparse reward signals, and ambiguous global reward feedback. To address these challenges, we propose DecEx-RAG, which models RAG as a Markov Decision Process (MDP) incorporating decision-making and execution, while introducing an efficient pruning strategy to optimize data expansion. Through comprehensive process-level policy optimization, DecEx-RAG significantly enhances the autonomous task decomposition, dynamic retrieval, and high-quality answer generation capabilities of large language models (LLMs). Experiments show that DecEx-RAG achieves an average absolute performance improvement of $6.2\%$ across six datasets, significantly outperforming existing baselines. Moreover, the pruning strategy improves data construction efficiency by nearly $6 \times$, providing an efficient solution for process-supervised RAG training. The code is available at https://github.com/sdsxdxl/DecEx-RAG.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive and Multi-Source Entity Matching for Name Standardization of Astronomical Observation Facilities</title>
<link>https://arxiv.org/abs/2510.05744</link>
<guid>https://arxiv.org/abs/2510.05744</guid>
<content:encoded><![CDATA[
arXiv:2510.05744v1 Announce Type: new 
Abstract: This ongoing work focuses on the development of a methodology for generating a multi-source mapping of astronomical observation facilities. To compare two entities, we compute scores with adaptable criteria and Natural Language Processing (NLP) techniques (Bag-of-Words approaches, sequential approaches, and surface approaches) to map entities extracted from eight semantic artifacts, including Wikidata and astronomy-oriented resources. We utilize every property available, such as labels, definitions, descriptions, external identifiers, and more domain-specific properties, such as the observation wavebands, spacecraft launch dates, funding agencies, etc. Finally, we use a Large Language Model (LLM) to accept or reject a mapping suggestion and provide a justification, ensuring the plausibility and FAIRness of the validated synonym pairs. The resulting mapping is composed of multi-source synonym sets providing only one standardized label per entity. Those mappings will be used to feed our Name Resolver API and will be integrated into the International Virtual Observatory Alliance (IVOA) Vocabularies and the OntoPortal-Astro platform.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diversity Is All You Need for Contrastive Learning: Spectral Bounds on Gradient Magnitudes</title>
<link>https://arxiv.org/abs/2510.05767</link>
<guid>https://arxiv.org/abs/2510.05767</guid>
<content:encoded><![CDATA[
arXiv:2510.05767v1 Announce Type: new 
Abstract: We derive non-asymptotic spectral bands that bound the squared InfoNCE gradient norm via alignment, temperature, and batch spectrum, recovering the \(1/\tau^{2}\) law and closely tracking batch-mean gradients on synthetic data and ImageNet. Using effective rank \(R_{\mathrm{eff}}\) as an anisotropy proxy, we design spectrum-aware batch selection, including a fast greedy builder. On ImageNet-100, Greedy-64 cuts time-to-67.5\% top-1 by 15\% vs.\ random (24\% vs.\ Pool--P3) at equal accuracy; CIFAR-10 shows similar gains. In-batch whitening promotes isotropy and reduces 50-step gradient variance by \(1.37\times\), matching our theoretical upper bound.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InforME: Improving Informativeness of Abstractive Text Summarization With Informative Attention Guided by Named Entity Salience</title>
<link>https://arxiv.org/abs/2510.05769</link>
<guid>https://arxiv.org/abs/2510.05769</guid>
<content:encoded><![CDATA[
arXiv:2510.05769v1 Announce Type: new 
Abstract: Abstractive text summarization is integral to the Big Data era, which demands advanced methods to turn voluminous and often long text data into concise but coherent and informative summaries for efficient human consumption. Despite significant progress, there is still room for improvement in various aspects. One such aspect is to improve informativeness. Hence, this paper proposes a novel learning approach consisting of two methods: an optimal transport-based informative attention method to improve learning focal information in reference summaries and an accumulative joint entropy reduction method on named entities to enhance informative salience. Experiment results show that our approach achieves better ROUGE scores compared to prior work on CNN/Daily Mail while having competitive results on XSum. Human evaluation of informativeness also demonstrates the better performance of our approach over a strong baseline. Further analysis gives insight into the plausible reasons underlying the evaluation results.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture of Neuron Experts</title>
<link>https://arxiv.org/abs/2510.05781</link>
<guid>https://arxiv.org/abs/2510.05781</guid>
<content:encoded><![CDATA[
arXiv:2510.05781v1 Announce Type: new 
Abstract: In this work, we first explore whether the parameters activated by the MoE layer remain highly sparse at inference. We perform a sparsification study on several representative MoE models. For each expert, we rank parameters by the magnitude of their activations from the gate projection and progressively prune the activated subset. Pruning up to 60% of parameters within that subset causes only negligible task-performance degradation; substantial drops occur only after more than 90% are removed. We further decompose experts into neuron-granular MoE and visualize their activation values, finding that most neuron activations are near zero. This observation motivates us to select only high-activation neuron experts during pretraining. Based on this insight, we propose Mixture of Neuron Experts (MoNE). MoNE achieves neuron-granular expert selection by only applying a simple top-k selection within each expert, incurs negligible latency, and requires no additional routing parameters or inter-expert communication. Extensive experiments demonstrate that MoNE matches traditional MoE performance while activating only 50% of the MoE-layer parameters, and it consistently outperforms traditional MoE when compared at equal numbers of activated parameters. These results suggest that MoNE is a practical approach to improving parameter utilization and inference efficiency in MoE-like models.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-efficient Targeted Token-level Preference Optimization for LLM-based Text-to-Speech</title>
<link>https://arxiv.org/abs/2510.05799</link>
<guid>https://arxiv.org/abs/2510.05799</guid>
<content:encoded><![CDATA[
arXiv:2510.05799v1 Announce Type: new 
Abstract: Aligning text-to-speech (TTS) system outputs with human feedback through preference optimization has been shown to effectively improve the robustness and naturalness of language model-based TTS models. Current approaches primarily require paired desirable and undesirable samples at the utterance level. However, such pairs are often limited in TTS output data, and utterance-level formulation prevents fine-grained token-level optimization needed for accurate pronunciation alignment. In this study, we propose TKTO that eliminates the need for paired data, enabling a more data-efficient training paradigm, and directly targets token-level units, automatically providing fine-grained alignment signals without token-level annotations. TKTO improves the challenging Japanese TTS accuracy by 39% and reduces CER by 54%, automatically assigning 12.8 times stronger reward to targeted tokens.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EEPO: Exploration-Enhanced Policy Optimization via Sample-Then-Forget</title>
<link>https://arxiv.org/abs/2510.05837</link>
<guid>https://arxiv.org/abs/2510.05837</guid>
<content:encoded><![CDATA[
arXiv:2510.05837v1 Announce Type: new 
Abstract: Balancing exploration and exploitation remains a central challenge in reinforcement learning with verifiable rewards (RLVR) for large language models (LLMs). Current RLVR methods often overemphasize exploitation, leading to entropy collapse, diminished exploratory capacity, and ultimately limited performance gains. Although techniques that increase policy stochasticity can promote exploration, they frequently fail to escape dominant behavioral modes. This creates a self-reinforcing loop-repeatedly sampling and rewarding dominant modes-that further erodes exploration. We introduce Exploration-Enhanced Policy Optimization (EEPO), a framework that promotes exploration via two-stage rollouts with adaptive unlearning. In the first stage, the model generates half of the trajectories; it then undergoes a lightweight unlearning step to temporarily suppress these sampled responses, forcing the second stage to explore different regions of the output space. This sample-then-forget mechanism disrupts the self-reinforcing loop and promotes wider exploration during rollouts. Across five reasoning benchmarks, EEPO outperforms GRPO, achieving average relative gains of 24.3% on Qwen2.5-3B, 33.0% on Llama3.2-3B-Instruct, and 10.4% on Qwen3-8B-Base.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Luth: Efficient French Specialization for Small Language Models and Cross-Lingual Transfer</title>
<link>https://arxiv.org/abs/2510.05846</link>
<guid>https://arxiv.org/abs/2510.05846</guid>
<content:encoded><![CDATA[
arXiv:2510.05846v1 Announce Type: new 
Abstract: The landscape of Large Language Models (LLMs) remains predominantly English-centric, resulting in a significant performance gap for other major languages, such as French, especially in the context of Small Language Models (SLMs). Existing multilingual models demonstrate considerably lower performance in French compared to English, and research on efficient adaptation methods for French remains limited. To address this, we introduce \textbf{Luth}, a family of French-specialized SLMs: through targeted post-training on curated, high-quality French data, our models outperform all open-source counterparts of comparable size on multiple French benchmarks while retaining their original English capabilities. We further show that strategic model merging enhances performance in both languages, establishing Luth as a new state of the art for French SLMs and a robust baseline for future French-language research.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DACP: Domain-Adaptive Continual Pre-Training of Large Language Models for Phone Conversation Summarization</title>
<link>https://arxiv.org/abs/2510.05858</link>
<guid>https://arxiv.org/abs/2510.05858</guid>
<content:encoded><![CDATA[
arXiv:2510.05858v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved impressive performance in text summarization, yet their performance often falls short when applied to specialized domains %or conversational data that differ from their original pre-training distribution. While fine-tuning can improve summarization quality, it typically relies on costly and scarce high-quality labeled data. In this work, we explore continual pre-training as a scalable, self-supervised approach to adapt LLMs for downstream summarization tasks, particularly in the context of noisy real-world conversation transcripts. We conduct extensive experiments using large-scale, unlabeled business conversation data to investigate whether continual pre-training enhances model capabilities in conversational summarization. Our results demonstrate that continual pre-training yields substantial gains in both in-domain and out-of-domain summarization benchmarks, while maintaining strong generalization and robustness. We also analyze the effects of data selection strategies, providing practical guidelines for applying continual pre-training in summarization-focused industrial applications.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Boilerplate: Prevalence and Quality of Contract Generators in the Context of Swiss Privacy Policies</title>
<link>https://arxiv.org/abs/2510.05860</link>
<guid>https://arxiv.org/abs/2510.05860</guid>
<content:encoded><![CDATA[
arXiv:2510.05860v1 Announce Type: new 
Abstract: It has become increasingly challenging for firms to comply with a plethora of novel digital regulations. This is especially true for smaller businesses that often lack both the resources and know-how to draft complex legal documents. Instead of seeking costly legal advice from attorneys, firms may turn to cheaper alternative legal service providers such as automated contract generators. While these services have a long-standing presence, there is little empirical evidence on their prevalence and output quality.
  We address this gap in the context of a 2023 Swiss privacy law revision. To enable a systematic evaluation, we create and annotate a multilingual benchmark dataset that captures key compliance obligations under Swiss and EU privacy law. Using this dataset, we validate a novel GPT-5-based method for large-scale compliance assessment of privacy policies, allowing us to measure the impact of the revision. We observe compliance increases indicating an effect of the revision. Generators, explicitly referenced by 18% of local websites, are associated with substantially higher levels of compliance, with increases of up to 15 percentage points compared to privacy policies without generator use. These findings contribute to three debates: the potential of LLMs for cross-lingual legal analysis, the Brussels Effect of EU regulations, and, crucially, the role of automated tools in improving compliance and contractual quality.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Long-context Modeling from Context Denoising Perspective</title>
<link>https://arxiv.org/abs/2510.05862</link>
<guid>https://arxiv.org/abs/2510.05862</guid>
<content:encoded><![CDATA[
arXiv:2510.05862v1 Announce Type: new 
Abstract: Long-context models (LCMs) have demonstrated great potential in processing long sequences, facilitating many real-world applications. The success of LCMs can be attributed to their ability to locate implicit critical information within the context for further prediction. However, recent research reveals that LCMs are often susceptible to contextual noise, i.e., irrelevant tokens, that can mislead model attention. In this paper, we conduct a fine-grained analysis of the context noise and propose an effective metric, the Integrated Gradient (IG) score, to detect and quantify the noise information within the context. Our findings reveal that even simple mitigation of detected context noise can substantially boost the model's attention on critical tokens and benefit subsequent predictions. Building on this insight, we propose Context Denoising Training (CDT), a straightforward yet effective training strategy that improves attention on critical tokens while reinforcing their influence on model predictions. Extensive experiments across four tasks, under both context window scaling and long-context alignment settings, demonstrate the superiority of CDT. Notably, when trained with CDT, an open-source 8B model can achieve performance (50.92) comparable to GPT-4o (51.00).
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Sensitivity of LLMs to Harmful Contents in Long Input</title>
<link>https://arxiv.org/abs/2510.05864</link>
<guid>https://arxiv.org/abs/2510.05864</guid>
<content:encoded><![CDATA[
arXiv:2510.05864v1 Announce Type: new 
Abstract: Large language models (LLMs) increasingly support applications that rely on extended context, from document processing to retrieval-augmented generation. While their long-context capabilities are well studied for reasoning and retrieval, little is known about their behavior in safety-critical scenarios. We evaluate LLMs' sensitivity to harmful content under extended context, varying type (explicit vs. implicit), position (beginning, middle, end), prevalence (0.01-0.50 of the prompt), and context length (600-6000 tokens). Across harmful content categories such as toxic, offensive, and hate speech, with LLaMA-3, Qwen-2.5, and Mistral, we observe similar patterns: performance peaks at moderate harmful prevalence (0.25) but declines when content is very sparse or dominant; recall decreases with increasing context length; harmful sentences at the beginning are generally detected more reliably; and explicit content is more consistently recognized than implicit. These findings provide the first systematic view of how LLMs prioritize and calibrate harmful content in long contexts, highlighting both their emerging strengths and the challenges that remain for safety-critical use.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The fragility of "cultural tendencies" in LLMs</title>
<link>https://arxiv.org/abs/2510.05869</link>
<guid>https://arxiv.org/abs/2510.05869</guid>
<content:encoded><![CDATA[
arXiv:2510.05869v1 Announce Type: new 
Abstract: In a recent study, Lu, Song, and Zhang (2025) (LSZ) propose that large language models (LLMs), when prompted in different languages, display culturally specific tendencies. They report that the two models (i.e., GPT and ERNIE) respond in more interdependent and holistic ways when prompted in Chinese, and more independent and analytic ways when prompted in English. LSZ attribute these differences to deep-seated cultural patterns in the models, claiming that prompt language alone can induce substantial cultural shifts. While we acknowledge the empirical patterns they observed, we find their experiments, methods, and interpretations problematic. In this paper, we critically re-evaluate the methodology, theoretical framing, and conclusions of LSZ. We argue that the reported "cultural tendencies" are not stable traits but fragile artifacts of specific models and task design. To test this, we conducted targeted replications using a broader set of LLMs and a larger number of test items. Our results show that prompt language has minimal effect on outputs, challenging LSZ's claim that these models encode grounded cultural beliefs.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt reinforcing for long-term planning of large language models</title>
<link>https://arxiv.org/abs/2510.05921</link>
<guid>https://arxiv.org/abs/2510.05921</guid>
<content:encoded><![CDATA[
arXiv:2510.05921v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved remarkable success in a wide range of natural language processing tasks and can be adapted through prompting. However, they remain suboptimal in multi-turn interactions, often relying on incorrect early assumptions and failing to track user goals over time, which makes such tasks particularly challenging. Prior works in dialogue systems have shown that long-term planning is essential for handling interactive tasks. In this work, we propose a prompt optimisation framework inspired by reinforcement learning, which enables such planning to take place by only modifying the task instruction prompt of the LLM-based agent. By generating turn-by-turn feedback and leveraging experience replay for prompt rewriting, our proposed method shows significant improvement in multi-turn tasks such as text-to-SQL and task-oriented dialogue. Moreover, it generalises across different LLM-based agents and can leverage diverse LLMs as meta-prompting agents. This warrants future research in reinforcement learning-inspired parameter-free optimisation methods.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hire Your Anthropologist! Rethinking Culture Benchmarks Through an Anthropological Lens</title>
<link>https://arxiv.org/abs/2510.05931</link>
<guid>https://arxiv.org/abs/2510.05931</guid>
<content:encoded><![CDATA[
arXiv:2510.05931v1 Announce Type: new 
Abstract: Cultural evaluation of large language models has become increasingly important, yet current benchmarks often reduce culture to static facts or homogeneous values. This view conflicts with anthropological accounts that emphasize culture as dynamic, historically situated, and enacted in practice. To analyze this gap, we introduce a four-part framework that categorizes how benchmarks frame culture, such as knowledge, preference, performance, or bias. Using this lens, we qualitatively examine 20 cultural benchmarks and identify six recurring methodological issues, including treating countries as cultures, overlooking within-culture diversity, and relying on oversimplified survey formats. Drawing on established anthropological methods, we propose concrete improvements: incorporating real-world narratives and scenarios, involving cultural communities in design and validation, and evaluating models in context rather than isolation. Our aim is to guide the development of cultural benchmarks that go beyond static recall tasks and more accurately capture the responses of the models to complex cultural situations.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvalMORAAL: Interpretable Chain-of-Thought and LLM-as-Judge Evaluation for Moral Alignment in Large Language Models</title>
<link>https://arxiv.org/abs/2510.05942</link>
<guid>https://arxiv.org/abs/2510.05942</guid>
<content:encoded><![CDATA[
arXiv:2510.05942v1 Announce Type: new 
Abstract: We present EvalMORAAL, a transparent chain-of-thought (CoT) framework that uses two scoring methods (log-probabilities and direct ratings) plus a model-as-judge peer review to evaluate moral alignment in 20 large language models. We assess models on the World Values Survey (55 countries, 19 topics) and the PEW Global Attitudes Survey (39 countries, 8 topics). With EvalMORAAL, top models align closely with survey responses (Pearson's r approximately 0.90 on WVS). Yet we find a clear regional difference: Western regions average r=0.82 while non-Western regions average r=0.61 (a 0.21 absolute gap), indicating consistent regional bias. Our framework adds three parts: (1) two scoring methods for all models to enable fair comparison, (2) a structured chain-of-thought protocol with self-consistency checks, and (3) a model-as-judge peer review that flags 348 conflicts using a data-driven threshold. Peer agreement relates to survey alignment (WVS r=0.74, PEW r=0.39, both p<.001), supporting automated quality checks. These results show real progress toward culture-aware AI while highlighting open challenges for use across regions.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing the Difficulty Perception Mechanism of Large Language Models</title>
<link>https://arxiv.org/abs/2510.05969</link>
<guid>https://arxiv.org/abs/2510.05969</guid>
<content:encoded><![CDATA[
arXiv:2510.05969v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed on complex reasoning tasks, yet little is known about their ability to internally evaluate problem difficulty, which is an essential capability for adaptive reasoning and efficient resource allocation. In this work, we investigate whether LLMs implicitly encode problem difficulty in their internal representations. Using a linear probe on the final-token representations of LLMs, we demonstrate that the difficulty level of math problems can be linearly modeled. We further locate the specific attention heads of the final Transformer layer: these attention heads have opposite activation patterns for simple and difficult problems, thus achieving perception of difficulty. Our ablation experiments prove the accuracy of the location. Crucially, our experiments provide practical support for using LLMs as automatic difficulty annotators, potentially substantially reducing reliance on costly human labeling in benchmark construction and curriculum learning. We also uncover that there is a significant difference in entropy and difficulty perception at the token level. Our study reveals that difficulty perception in LLMs is not only present but also structurally organized, offering new theoretical insights and practical directions for future research.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LexiCon: a Benchmark for Planning under Temporal Constraints in Natural Language</title>
<link>https://arxiv.org/abs/2510.05972</link>
<guid>https://arxiv.org/abs/2510.05972</guid>
<content:encoded><![CDATA[
arXiv:2510.05972v1 Announce Type: new 
Abstract: Owing to their reasoning capabilities, large language models (LLMs) have been evaluated on planning tasks described in natural language. However, LLMs have largely been tested on planning domains without constraints. In order to deploy them in real-world settings where adherence to constraints, in particular safety constraints, is critical, we need to evaluate their performance on constrained planning tasks. We introduce LexiCon -- a natural language-based (Lexi) constrained (Con) planning benchmark, consisting of a suite of environments, that can be used to evaluate the planning capabilities of LLMs in a principled fashion. The core idea behind LexiCon is to take existing planning environments and impose temporal constraints on the states. These constrained problems are then translated into natural language and given to an LLM to solve. A key feature of LexiCon is its extensibility. That is, the set of supported environments can be extended with new (unconstrained) environment generators, for which temporal constraints are constructed automatically. This renders LexiCon future-proof: the hardness of the generated planning problems can be increased as the planning capabilities of LLMs improve. Our experiments reveal that the performance of state-of-the-art LLMs, including reasoning models like GPT-5, o3, and R1, deteriorates as the degree of constrainedness of the planning tasks increases.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Gaps in the APS: Direct Minimal Pair Analysis in LLM Syntactic Assessments</title>
<link>https://arxiv.org/abs/2510.06001</link>
<guid>https://arxiv.org/abs/2510.06001</guid>
<content:encoded><![CDATA[
arXiv:2510.06001v1 Announce Type: new 
Abstract: Recent studies probing the Argument from the Poverty of the Stimulus (APS) have applied Large Language Models (LLMs) to test the learnability of complex syntax through surprisal-based metrics. However, divergent conclusions raise questions concerning the insights these metrics offer. While Wilcox et al. (2024) used direct minimal pair comparisons (the "wh-effect") to demonstrate that models successfully generalise knowledge of filler-gap dependencies, Lan et al. (2024) used a Difference-in-Differences (DiD) metric and found that models largely fail on parasitic gaps (PGs). This paper argues that the direct minimal pair approach offers greater diagnostic transparency. We demonstrate this by generating a full 8-permutation paradigm of refined PG stimuli and evaluating the GPT-2 model used in previous studies with a systematic Wilcox-style wh-effect analysis. Our results show that GPT-2 succeeds across all four tested conditions, indicating robust knowledge of filler-gap licensing principles even in complex PG environments. This finding, which contrasts with the more ambiguous results from DiD-style metrics, suggests that the choice of evaluation metric is critical for assessing an LLM's syntactic competence.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MASA: Rethinking the Representational Bottleneck in LoRA with Multi-A Shared Adaptation</title>
<link>https://arxiv.org/abs/2510.06005</link>
<guid>https://arxiv.org/abs/2510.06005</guid>
<content:encoded><![CDATA[
arXiv:2510.06005v1 Announce Type: new 
Abstract: Low-Rank Adaptation (LoRA) has emerged as a dominant method in Parameter-Efficient Fine-Tuning (PEFT) for large language models, which augments the transformer layer with one down-projection $A$ and one up-projection $B$. However, LoRA's reliance on a single down-projection matrix ($A$) creates a representational bottleneck, as this solitary feature extractor is inherently insufficient for capturing the diverse signals required by complex tasks. This motivates our architectural shift to focus on enriching the feature adaptation to improve the downstream task adaptation ability. We propose MASA (Multi-$A$ Shared Adaptation), an architecture that implements a multi-$A$, single-$B$ structure where the multi-$A$ expert ensemble is asymmetrically shared across layers to ensure parameter efficiency. In MASA, these specialized experts capture diverse features, which are then integrated by a single, layer-specific $B$-matrix. The effectiveness and versatility of our method are validated through a comprehensive suite of experiments spanning multi-domain generalization, single-domain specialization, and multi-task reasoning. For example, on the MMLU benchmark, MASA achieves an average accuracy of 59.62%, outperforming the standard LoRA by 1.08 points (a relative improvement of 1.84%) with comparable learnable parameters of 0.52%.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating The Impact of Stimulus Quality in Investigations of LLM Language Performance</title>
<link>https://arxiv.org/abs/2510.06018</link>
<guid>https://arxiv.org/abs/2510.06018</guid>
<content:encoded><![CDATA[
arXiv:2510.06018v1 Announce Type: new 
Abstract: Recent studies employing Large Language Models (LLMs) to test the Argument from the Poverty of the Stimulus (APS) have yielded contrasting results across syntactic phenomena. This paper investigates the hypothesis that characteristics of the stimuli used in recent studies, including lexical ambiguities and structural complexities, may confound model performance. A methodology is proposed for re-evaluating LLM competence on syntactic prediction, focusing on GPT-2. This involves: 1) establishing a baseline on previously used (both filtered and unfiltered) stimuli, and 2) generating a new, refined dataset using a state-of-the-art (SOTA) generative LLM (Gemini 2.5 Pro Preview) guided by linguistically-informed templates designed to mitigate identified confounds. Our preliminary findings indicate that GPT-2 demonstrates notably improved performance on these refined PG stimuli compared to baselines, suggesting that stimulus quality significantly influences outcomes in surprisal-based evaluations of LLM syntactic competency.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CDTP: A Large-Scale Chinese Data-Text Pair Dataset for Comprehensive Evaluation of Chinese LLMs</title>
<link>https://arxiv.org/abs/2510.06039</link>
<guid>https://arxiv.org/abs/2510.06039</guid>
<content:encoded><![CDATA[
arXiv:2510.06039v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have achieved remarkable success across a wide range of natural language processing tasks. However, Chinese LLMs face unique challenges, primarily due to the dominance of unstructured free text and the lack of structured representations in Chinese corpora. While existing benchmarks for LLMs partially assess Chinese LLMs, they are still predominantly English-centric and fail to address the unique linguistic characteristics of Chinese, lacking structured datasets essential for robust evaluation. To address these challenges, we present a Comprehensive Benchmark for Evaluating Chinese Large Language Models (CB-ECLLM) based on the newly constructed Chinese Data-Text Pair (CDTP) dataset. Specifically, CDTP comprises over 7 million aligned text pairs, each consisting of unstructured text coupled with one or more corresponding triples, alongside a total of 15 million triples spanning four critical domains. The core contributions of CDTP are threefold: (i) enriching Chinese corpora with high-quality structured information; (ii) enabling fine-grained evaluation tailored to knowledge-driven tasks; and (iii) supporting multi-task fine-tuning to assess generalization and robustness across scenarios, including Knowledge Graph Completion, Triple-to-Text generation, and Question Answering. Furthermore, we conduct rigorous evaluations through extensive experiments and ablation studies to assess the effectiveness, Supervised Fine-Tuning (SFT), and robustness of the benchmark. To support reproducible research, we offer an open-source codebase and outline potential directions for future investigations based on our insights.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASPO: Asymmetric Importance Sampling Policy Optimization</title>
<link>https://arxiv.org/abs/2510.06062</link>
<guid>https://arxiv.org/abs/2510.06062</guid>
<content:encoded><![CDATA[
arXiv:2510.06062v1 Announce Type: new 
Abstract: Recent Large Language Model (LLM) post-training methods rely on token-level clipping mechanisms during Reinforcement Learning (RL). However, we identify a fundamental flaw in this Outcome-Supervised RL (OSRL) paradigm: the Importance Sampling (IS) ratios of positive-advantage tokens are mismatched, leading to unbalanced token weighting for positive and negative tokens. This mismatch suppresses the update of low-probability tokens while over-amplifying already high-probability ones. To address this, we propose Asymmetric Importance Sampling Policy Optimization (ASPO), which uses a simple yet effective strategy that flips the IS ratios of positive-advantage tokens, aligning their update direction with the learning dynamics of negative ones. AIS further incorporates a soft dual-clipping mechanism to stabilize extreme updates while maintaining gradient flow. Comprehensive experiments on coding and mathematical reasoning benchmarks demonstrate that ASPO significantly mitigates premature convergence, improves training stability, and enhances final performance over strong GRPO-based baselines. Our analysis provides new insights into the role of token-level weighting in OSRL and highlights the critical importance of correcting IS in LLM RL. The code and models of ASPO are available at https://github.com/wizard-III/Archer2.0.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectrum Tuning: Post-Training for Distributional Coverage and In-Context Steerability</title>
<link>https://arxiv.org/abs/2510.06084</link>
<guid>https://arxiv.org/abs/2510.06084</guid>
<content:encoded><![CDATA[
arXiv:2510.06084v1 Announce Type: new 
Abstract: Language model post-training has enhanced instruction-following and performance on many downstream tasks, but also comes with an often-overlooked cost on tasks with many possible valid answers. We characterize three desiderata for conditional distributional modeling: in-context steerability, valid output space coverage, and distributional alignment, and document across three model families how current post-training can reduce these properties. In particular, we disambiguate between two kinds of in-context learning: ICL for eliciting existing underlying knowledge or capabilities, and in-context steerability, where a model must use in-context information to override its priors and steer to a novel data generating distribution. To better evaluate and improve these desiderata, we introduce Spectrum Suite, a large-scale resource compiled from >40 data sources and spanning >90 tasks requiring models to steer to and match diverse distributions ranging from varied human preferences to numerical distributions and more. We find that while current post-training techniques help elicit underlying capabilities and knowledge, they hurt models' ability to flexibly steer in-context. To mitigate these issues, we propose Spectrum Tuning, a post-training method using Spectrum Suite to improve steerability and distributional coverage. We find that Spectrum Tuning often improves over pretrained models and their instruction-tuned counterparts, enhancing steerability, spanning more of the output space, and improving distributional alignment on held-out datasets.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Valley of Code Reasoning: Scaling Knowledge Distillation of Large Language Models</title>
<link>https://arxiv.org/abs/2510.06101</link>
<guid>https://arxiv.org/abs/2510.06101</guid>
<content:encoded><![CDATA[
arXiv:2510.06101v1 Announce Type: new 
Abstract: Distilling the thinking traces of a Large Language Model (LLM) with reasoning capabilities into a smaller model has been proven effective. Yet, there is a scarcity of work done on how model performances scale with the quantity of distillation data. In this work, we study the scaling trend of distilling competitive coding skills on two small non-reasoning LLMs. We validate the hypothesis that there is a $\textit{valley of code reasoning}$: downstream performance on competitive coding first drops as data quantity increases, then it steadily increases in a sharper-than-log-linear fashion. Having identified the trend, we further fine-tune the models at two different distillation stages on the same data to ground conclusions on their respective learning phases. We learn that across stages in the low and medium-low data regimes, small models benefit significantly from easier coding questions than from harder ones. We also find that, surprisingly, the correctness of outputs in training data makes no difference to distillation outcomes. Our work represents a step forward in understanding the training dynamics of code reasoning distillation outside intuition
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributional Semantics Tracing: A Framework for Explaining Hallucinations in Large Language Models</title>
<link>https://arxiv.org/abs/2510.06107</link>
<guid>https://arxiv.org/abs/2510.06107</guid>
<content:encoded><![CDATA[
arXiv:2510.06107v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are prone to hallucination, the generation of plausible yet factually incorrect statements. This work investigates the intrinsic, architectural origins of this failure mode through three primary contributions.First, to enable the reliable tracing of internal semantic failures, we propose \textbf{Distributional Semantics Tracing (DST)}, a unified framework that integrates established interpretability techniques to produce a causal map of a model's reasoning, treating meaning as a function of context (distributional semantics). Second, we pinpoint the model's layer at which a hallucination becomes inevitable, identifying a specific \textbf{commitment layer} where a model's internal representations irreversibly diverge from factuality. Third, we identify the underlying mechanism for these failures. We observe a conflict between distinct computational pathways, which we interpret using the lens of dual-process theory: a fast, heuristic \textbf{associative pathway} (akin to System 1) and a slow, deliberate \textbf{contextual pathway} (akin to System 2), leading to predictable failure modes such as \textit{Reasoning Shortcut Hijacks}. Our framework's ability to quantify the coherence of the contextual pathway reveals a strong negative correlation ($\rho = -0.863$) with hallucination rates, implying that these failures are predictable consequences of internal semantic weakness. The result is a mechanistic account of how, when, and why hallucinations occur within the Transformer architecture.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parallel Tokenizers: Rethinking Vocabulary Design for Cross-Lingual Transfer</title>
<link>https://arxiv.org/abs/2510.06128</link>
<guid>https://arxiv.org/abs/2510.06128</guid>
<content:encoded><![CDATA[
arXiv:2510.06128v1 Announce Type: new 
Abstract: Tokenization defines the foundation of multilingual language models by determining how words are represented and shared across languages. However, existing methods often fail to support effective cross-lingual transfer because semantically equivalent words are assigned distinct embeddings. For example, "I eat rice" in English and "Ina cin shinkafa" in Hausa are typically mapped to different vocabulary indices, preventing shared representations and limiting cross-lingual generalization. We introduce parallel tokenizers. This new framework trains tokenizers monolingually and then aligns their vocabularies exhaustively using bilingual dictionaries or word-to-word translation, ensuring consistent indices for semantically equivalent words. This alignment enforces a shared semantic space across languages while naturally improving fertility balance. To assess their effectiveness, we pretrain a transformer encoder from scratch on thirteen low-resource languages and evaluate it on sentiment analysis, hate speech detection, emotion classification, and sentence embedding similarity. Across all tasks, models trained with parallel tokenizers outperform conventional multilingual baselines, confirming that rethinking tokenization is essential for advancing multilingual representation learning--especially in low-resource settings.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CreditDecoding: Accelerating Parallel Decoding in Diffusion Large Language Models with Trace Credits</title>
<link>https://arxiv.org/abs/2510.06133</link>
<guid>https://arxiv.org/abs/2510.06133</guid>
<content:encoded><![CDATA[
arXiv:2510.06133v1 Announce Type: new 
Abstract: Diffusion large language models (dLLMs) generate text through iterative denoising steps, achieving parallel decoding by denoising only high-confidence positions at each step. However, existing approaches often repetitively remask tokens due to initially low confidence scores, leading to redundant iterations and limiting overall acceleration. Through the analysis of dLLM decoding traces, we observe that the model often determines the final prediction for a token several steps before the decoding step. To leverage this historical information and avoid redundant steps, we introduce the concept of Trace Credit, which quantifies each token's convergence potential by accumulating historical logits. Furthermore, we propose CreditDecoding, a training-free parallel decoding algorithm that accelerates the confidence convergence of correct but underconfident tokens by fusing current logits with Trace Credit. This process significantly reduces redundant iterations and enhances decoding robustness. On eight benchmarks, CreditDecoding achieves a 5.48 times speedup and a 0.48 performance improvement over LLaDA-8B-Instruct, and a 4.11 times speedup with a 0.15 performance improvement over LLaDA-MoE-Instruct. Importantly, CreditDecoding scales effectively to long sequences and is orthogonal to mainstream inference optimizations, making it a readily integrable and versatile solution.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoSE: Round-robin Synthetic Data Evaluation for Selecting LLM Generators without Human Test Sets</title>
<link>https://arxiv.org/abs/2510.06143</link>
<guid>https://arxiv.org/abs/2510.06143</guid>
<content:encoded><![CDATA[
arXiv:2510.06143v1 Announce Type: new 
Abstract: LLMs are powerful generators of synthetic data, which are used for training smaller, specific models. This is especially valuable for low-resource languages, where human-labelled data is scarce but LLMs can still produce high-quality text. However, LLMs differ in how useful their outputs are for training. Selecting the best LLM as a generator is challenging because extrinsic evaluation requires costly human annotations (which are often unavailable for low-resource languages), while intrinsic metrics correlate poorly with downstream performance. We introduce Round robin Synthetic data Evaluation (RoSE), a proxy metric for selecting the best LLM generator without human test sets. RoSE trains a small model on the outputs of a candidate generator (LLM) and then evaluates it on generated synthetic examples from all other candidate LLMs. The final RoSE score is the mean performance of this small model. Across six LLMs, eleven languages, and three tasks (sentiment, topic, intent), RoSE identifies the optimal generator more often than any other intrinsic heuristics. RoSE outperforms intrinsic heuristics and comes within 0.76 percentage points of the optimal generator baseline. This result is measured in terms of downstream performance, obtained by training a small model on the chosen generator's outputs (optimal vs. proxy metric selected) and evaluating it on human-labelled test data. Additionally, RoSE is the only metric to achieve a positive correlation with performance on human test data.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VecInfer: Efficient LLM Inference with Low-Bit KV Cache via Outlier-Suppressed Vector Quantization</title>
<link>https://arxiv.org/abs/2510.06175</link>
<guid>https://arxiv.org/abs/2510.06175</guid>
<content:encoded><![CDATA[
arXiv:2510.06175v1 Announce Type: new 
Abstract: The Key-Value (KV) cache introduces substantial memory overhead during large language model (LLM) inference. Although existing vector quantization (VQ) methods reduce KV cache usage and provide flexible representational capacity across bit-widths, they suffer severe performance degradation at ultra-low bit-widths due to key cache outliers that hinder effective codebook utilization. To address this challenge, we propose VecInfer, a novel VQ method for aggressive KV cache compression while enabling efficient inference. By applying smooth and Hadamard transformations, VecInfer suppresses outliers in the key cache, enabling the codebook to comprehensively cover the original data distribution and thereby reducing quantization difficulty. To facilitate efficient deployment, we design an optimized CUDA kernel that fuses computation with dequantization to minimize memory access overhead. Extensive evaluations demonstrate that VecInfer consistently outperforms existing quantization baselines across both long-context understanding and mathematical reasoning tasks. With only 2-bit quantization, VecInfer achieves performance comparable to full precision, while delivering up to $\mathbf{2.7\times}$ speedup in large-batch self-attention computation and $\mathbf{8.3\times}$ reduction in single-batch end-to-end latency on Llama-3.1-8B with a 196k sequence length.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixing Mechanisms: How Language Models Retrieve Bound Entities In-Context</title>
<link>https://arxiv.org/abs/2510.06182</link>
<guid>https://arxiv.org/abs/2510.06182</guid>
<content:encoded><![CDATA[
arXiv:2510.06182v1 Announce Type: new 
Abstract: A key component of in-context reasoning is the ability of language models (LMs) to bind entities for later retrieval. For example, an LM might represent "Ann loves pie" by binding "Ann" to "pie", allowing it to later retrieve "Ann" when asked "Who loves pie?" Prior research on short lists of bound entities found strong evidence that LMs implement such retrieval via a positional mechanism, where "Ann" is retrieved based on its position in context. In this work, we find that this mechanism generalizes poorly to more complex settings; as the number of bound entities in context increases, the positional mechanism becomes noisy and unreliable in middle positions. To compensate for this, we find that LMs supplement the positional mechanism with a lexical mechanism (retrieving "Ann" using its bound counterpart "pie") and a reflexive mechanism (retrieving "Ann" through a direct pointer). Through extensive experiments on nine models and ten binding tasks, we uncover a consistent pattern in how LMs mix these mechanisms to drive model behavior. We leverage these insights to develop a causal model combining all three mechanisms that estimates next token distributions with 95% agreement. Finally, we show that our model generalizes to substantially longer inputs of open-ended text interleaved with entity groups, further demonstrating the robustness of our findings in more natural settings. Overall, our study establishes a more complete picture of how LMs bind and retrieve entities in-context.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RECODE-H: A Benchmark for Research Code Development with Interactive Human Feedback</title>
<link>https://arxiv.org/abs/2510.06186</link>
<guid>https://arxiv.org/abs/2510.06186</guid>
<content:encoded><![CDATA[
arXiv:2510.06186v1 Announce Type: new 
Abstract: Large language models (LLMs) show the promise in supporting scientific research implementation, yet their ability to generate correct and executable code remains limited. Existing works largely adopt one-shot settings, ignoring the iterative and feedback-driven nature of realistic workflows of scientific research development. To address this gap, we present RECODE-H, a benchmark of 102 tasks from research papers and repositories that evaluates LLM agents through multi-turn interactions with LLM-simulated human feedback. It includes structured instructions,unit tests, and a five-level feedback hierarchy to reflect realistic researcher-agent collaboration. We further present ReCodeAgent, a framework that integrates feedback into iterative code generation. Experiments with leading LLMs, including GPT-5, Claude-Sonnet-4, DeepSeek-V3.1, and Gemini 2.5, show substantial performance gains with richer feedback, while also highlighting ongoing challenges in the generation of complex research code. RECODE-H establishes a foundation for developing adaptive, feedback-driven LLM agents in scientific research implementation
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BanglaTalk: Towards Real-Time Speech Assistance for Bengali Regional Dialects</title>
<link>https://arxiv.org/abs/2510.06188</link>
<guid>https://arxiv.org/abs/2510.06188</guid>
<content:encoded><![CDATA[
arXiv:2510.06188v1 Announce Type: new 
Abstract: Real-time speech assistants are becoming increasingly popular for ensuring improved accessibility to information. Bengali, being a low-resource language with a high regional dialectal diversity, has seen limited progress in developing such systems. Existing systems are not optimized for real-time use and focus only on standard Bengali. In this work, we present BanglaTalk, the first real-time speech assistance system for Bengali regional dialects. BanglaTalk follows the client-server architecture and uses the Real-time Transport Protocol (RTP) to ensure low-latency communication. To address dialectal variation, we introduce a dialect-aware ASR system, BRDialect, developed by fine-tuning the IndicWav2Vec model in ten Bengali regional dialects. It outperforms the baseline ASR models by 12.41-33.98% on the RegSpeech12 dataset. Furthermore, BanglaTalk can operate at a low bandwidth of 24 kbps while maintaining an average end-to-end delay of 4.9 seconds. Low bandwidth usage and minimal end-to-end delay make the system both cost-effective and interactive for real-time use cases, enabling inclusive and accessible speech technology for the diverse community of Bengali speakers.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Speech-Text Transformer</title>
<link>https://arxiv.org/abs/2510.06195</link>
<guid>https://arxiv.org/abs/2510.06195</guid>
<content:encoded><![CDATA[
arXiv:2510.06195v1 Announce Type: new 
Abstract: Auto-regressive speech-text models are typically pre-trained on a large number of interleaved sequences of text tokens and raw speech encoded as speech tokens using vector quantization. These models have demonstrated state-of-the-art performance in speech-to-speech understanding and generation benchmarks, together with promising scaling laws, primarily enabled by the representational alignment between text and speech. Nevertheless, they suffer from shortcomings, partly owing to the disproportionately longer sequences of speech tokens in contrast to textual tokens. This results in a large compute imbalance between modalities during pre-training as well as during inference, and a potential hindrance to effectively aligning speech and text, ultimately translating to several orders of magnitude slower scaling laws. We introduce the Latent Speech-Text Transformer (LST), which makes pre-training speech-text models more data-efficient by dynamically and inexpensively aggregating speech tokens into latent speech patches. These patches serve as higher-level units that can either align with corresponding textual units to aid capability transfer or even encapsulate common speech sequences like silences to be more compute-efficient. We show that LST outperforms vanilla approaches on speech-to-speech as well as text-to-text benchmarks in both data- and compute-controlled settings, the former indicating more effective representational alignment and the latter indicating steeper scaling laws for speech-text models. On HellaSwag story completion, LST achieves 6.5% absolute gain in speech accuracy under compute-controlled training and 5.3% under data-controlled training, while also improving text performance. We will release our models, code, and the evaluation data to facilitate further research.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Peeking inside the Black-Box: Reinforcement Learning for Explainable and Accurate Relation Extraction</title>
<link>https://arxiv.org/abs/2510.06198</link>
<guid>https://arxiv.org/abs/2510.06198</guid>
<content:encoded><![CDATA[
arXiv:2510.06198v1 Announce Type: new 
Abstract: This paper introduces a framework for relation extraction (RE) that enhances both accuracy and explainability. The framework has two key components: (i) a reasoning mechanism that formulates relation extraction as a series of text-processing steps inspired by cognitive science, and (ii) an optimization process driven by reinforcement learning (RL) with a novel reward function designed to improve both task accuracy and explanation quality. We call our approach CogRE. Our framework addresses the lack of supervision for language-based explanations in traditional RE by promoting outputs that include important relation keywords. These keywords are drawn from a high-quality dictionary that is automatically constructed using an LLM. We evaluate our approach for the task of one-shot RE using two LLMs and two RE datasets. Our experiments show that CogRE improves explanation quality by addressing two common failure patterns in one-shot RE: poor attention focus and limited one-shot learning capability. For example, our cognitive-structured reasoning with Qwen2.5-15B-Instruct on One-shot NYT29 achieves 24.65% F1, surpassing prior reasoning-based designs. Optimizing this approach with RL using our reward further improves performance by +23.46% (absolute). Finally, human evaluation shows that our best model generates relational keywords closely aligned with gold labels, increasing human explanation quality ratings by 54% (relative).
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tiny but Mighty: A Software-Hardware Co-Design Approach for Efficient Multimodal Inference on Battery-Powered Small Devices</title>
<link>https://arxiv.org/abs/2510.05109</link>
<guid>https://arxiv.org/abs/2510.05109</guid>
<content:encoded><![CDATA[
arXiv:2510.05109v1 Announce Type: cross 
Abstract: Large Multimodal Models (LMMs) are inherently modular, consisting of vision and audio encoders, projectors, and large language models. Yet, they are almost always executed monolithically, which underutilizes the heterogeneous accelerators (NPUs, GPUs, DSPs) in modern SoCs and leads to high end-to-end latency. In this paper, we present NANOMIND, a hardware--software co-design inference framework for Large Multimodal Models (LMMs) that breaks large models into modular ``bricks'' (vision, language, audio, etc.) and maps each to its ideal accelerator. The key insight is that large models can be broken into modular components and scheduled to run on the most appropriate compute units. It performs module-level dynamic offloading across accelerators on unified-memory SoCs. By combining customized hardware design, system-level scheduling, and optimized low-bit computation kernels, we demonstrate our framework with a compact, battery-powered device capable of running LMMs entirely on device. This prototype functions as a self-contained intelligent assistant that requires no network connectivity, while achieving higher throughput and superior power efficiency under strict resource constraints. The design further bypasses CPU bottlenecks and reduces redundant memory usage through token-aware buffer management and module-level coordination. Our system outperforms existing implementations in resource efficiency, cutting energy consumption by 42.3\% and GPU memory usage by 11.2\%. This enables a battery-powered device to run LLaVA-OneVision with a camera for nearly half a day and LLaMA-3-8B for voice interactions up to almost 20.8 hours.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimization Modeling via Semantic Anchored Alignment</title>
<link>https://arxiv.org/abs/2510.05115</link>
<guid>https://arxiv.org/abs/2510.05115</guid>
<content:encoded><![CDATA[
arXiv:2510.05115v1 Announce Type: cross 
Abstract: Large language models (LLMs) have opened new paradigms in optimization modeling by enabling the generation of executable solver code from natural language descriptions. Despite this promise, existing approaches typically remain solver-driven: they rely on single-pass forward generation and apply limited post-hoc fixes based on solver error messages, leaving undetected semantic errors that silently produce syntactically correct but logically flawed models. To address this challenge, we propose SAC-Opt, a backward-guided correction framework that grounds optimization modeling in problem semantics rather than solver feedback. At each step, SAC-Opt aligns the original semantic anchors with those reconstructed from the generated code and selectively corrects only the mismatched components, driving convergence toward a semantically faithful model. This anchor-driven correction enables fine-grained refinement of constraint and objective logic, enhancing both fidelity and robustness without requiring additional training or supervision. Empirical results on seven public datasets demonstrate that SAC-Opt improves average modeling accuracy by 7.8\%, with gains of up to 21.9\% on the ComplexLP dataset. These findings highlight the importance of semantic-anchored correction in LLM-based optimization workflows to ensure faithful translation from problem intent to solver-executable code.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding Partial Differential Equations: Cross-Modal Adaptation of Decoder-only Models to PDEs</title>
<link>https://arxiv.org/abs/2510.05278</link>
<guid>https://arxiv.org/abs/2510.05278</guid>
<content:encoded><![CDATA[
arXiv:2510.05278v1 Announce Type: cross 
Abstract: Large language models have shown great success on natural language tasks in recent years, but they have also shown great promise when adapted to new modalities, e.g., for scientific machine learning tasks. Even though decoder-only models are more popular within NLP and scale exceedingly well at generating natural language, most proposed approaches for cross-modal adaptation focus on encoder-only models, raising the question of how model architecture affects these approaches. In this paper, we therefore perform a series of ablation studies to answer this question, systematically comparing encoder-only and decoder-only models on cross-modal adaptation for time-dependent simulation tasks based on partial differential equations (PDEs). We find that decoder-only models are far worse than encoder-only models, when existing approaches are applied unmodified. In contrast to several other domains, scaling decoder-only models also does not help. To harness the potential of decoder-only models in this context, we introduce two novel approaches, Parallel Flipping and Sequence Doubling, attempting to mimic bidirectionality in autoregressive models. Both our methods improve overall performance using decoder-only models for all tasks and all cross-model adaptation methods, closing the gap to encoder-only model performance. We hope that our findings broaden the spectrum of models used on cross-modal adaptation tasks to further scientific ML.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Monolithic Rewards: A Hybrid and Multi-Aspect Reward Optimization for MLLM Alignment</title>
<link>https://arxiv.org/abs/2510.05283</link>
<guid>https://arxiv.org/abs/2510.05283</guid>
<content:encoded><![CDATA[
arXiv:2510.05283v1 Announce Type: cross 
Abstract: Aligning multimodal large language models (MLLMs) with human preferences often relies on single-signal, model-based reward methods. Such monolithic rewards often lack confidence calibration across domain-specific tasks, fail to capture diverse aspects of human preferences, and require extensive data annotation and reward model training. In this work, we propose a hybrid reward modeling framework that integrates complementary reward paradigms: (i) model-based rewards, where a learned reward model predicts scalar or vector scores from synthetic and human feedback, and (ii) rule-based rewards, where domain-specific heuristics provide explicit correctness signals with confidence. Beyond accuracy, we further incorporate multi-aspect rewards to enforce instruction adherence and introduce a generalized length-penalty reward to stabilize training and improve performance. The proposed framework provides a flexible and effective approach to aligning MLLMs through reinforcement learning policy optimization. Our experiments show consistent improvements across different multimodal benchmarks when applying hybrid and multi-aspect reward modeling. Our best performing model in the 3B family achieves an overall average improvement of ~9.5% across general and math reasoning tasks. Focusing specifically on mathematical benchmarks, the model achieves a significant average improvement of ~16%, highlighting its effectiveness in mathematical reasoning and problem solving.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WaveSP-Net: Learnable Wavelet-Domain Sparse Prompt Tuning for Speech Deepfake Detection</title>
<link>https://arxiv.org/abs/2510.05305</link>
<guid>https://arxiv.org/abs/2510.05305</guid>
<content:encoded><![CDATA[
arXiv:2510.05305v1 Announce Type: cross 
Abstract: Modern front-end design for speech deepfake detection relies on full fine-tuning of large pre-trained models like XLSR. However, this approach is not parameter-efficient and may lead to suboptimal generalization to realistic, in-the-wild data types. To address these limitations, we introduce a new family of parameter-efficient front-ends that fuse prompt-tuning with classical signal processing transforms. These include FourierPT-XLSR, which uses the Fourier Transform, and two variants based on the Wavelet Transform: WSPT-XLSR and Partial-WSPT-XLSR. We further propose WaveSP-Net, a novel architecture combining a Partial-WSPT-XLSR front-end and a bidirectional Mamba-based back-end. This design injects multi-resolution features into the prompt embeddings, which enhances the localization of subtle synthetic artifacts without altering the frozen XLSR parameters. Experimental results demonstrate that WaveSP-Net outperforms several state-of-the-art models on two new and challenging benchmarks, Deepfake-Eval-2024 and SpoofCeleb, with low trainable parameters and notable performance gains. The code and models are available at https://github.com/xxuan-acoustics/WaveSP-Net.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Concept Music Score from Quantum Picturalism: Musical Incarnation of a Bell-Pair under Measurements</title>
<link>https://arxiv.org/abs/2510.05391</link>
<guid>https://arxiv.org/abs/2510.05391</guid>
<content:encoded><![CDATA[
arXiv:2510.05391v1 Announce Type: cross 
Abstract: We initiate the development of a new language and theory for quantum music, to which we refer as Quantum Concept Music (QCM). This new music formalism is based on Categorical Quantum Mechanics (CQM), and more specifically, its diagrammatic incarnation Quantum Picturalism (QPict), which is heavily based on ZX-calculus. In fact, it is naturally inherited from CQM/QPict. At its heart is the explicit notational representation of relations that exist within and between the key concepts of music composition, performance, and automation. QCM also enables one to directly translate quantum phenomena into music compositions in a both intuitively obvious, rigorous and mechanical manner.
  Following this pattern, we propose a score for musicians interacting like a Bell-pair under measurement, and outline examples of how it could be live performed. While most of the Western classical music notation has heavily relied on linear representation of music - which does not always adequately capture the nature of music - our approach is distinct by highlighting the fundamental relational dimension of music. In addition, this quantum-based technique not only influences the music at the profound level of composition, but also has a direct impact on a live performance, and also provides a new template for automating music, e.g.~in the context of AI-generation.
  All together, we initiate the creation of new music formalism that is powerful and efficient in capturing the interactive nature of music, both in terms of internal and external interactions, and goes beyond the boundaries of Western classical music notation, which allows to use it in many different genres and directions.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Reinforcement Learning for Large Language Model Agent Safety</title>
<link>https://arxiv.org/abs/2510.05442</link>
<guid>https://arxiv.org/abs/2510.05442</guid>
<content:encoded><![CDATA[
arXiv:2510.05442v1 Announce Type: cross 
Abstract: Large Language Model (LLM) agents can leverage tools such as Google Search to complete complex tasks. However, this tool usage introduces the risk of indirect prompt injections, where malicious instructions hidden in tool outputs can manipulate the agent, posing security risks like data leakage. Current defense strategies typically rely on fine-tuning LLM agents on datasets of known attacks. However, the generation of these datasets relies on manually crafted attack patterns, which limits their diversity and leaves agents vulnerable to novel prompt injections. To address this limitation, we propose Adversarial Reinforcement Learning for Agent Safety (ARLAS), a novel framework that leverages adversarial reinforcement learning (RL) by formulating the problem as a two-player zero-sum game. ARLAS co-trains two LLMs: an attacker that learns to autonomously generate diverse prompt injections and an agent that learns to defend against them while completing its assigned tasks. To ensure robustness against a wide range of attacks and to prevent cyclic learning, we employ a population-based learning framework that trains the agent to defend against all previous attacker checkpoints. Evaluated on BrowserGym and AgentDojo, agents fine-tuned with ARLAS achieve a significantly lower attack success rate than the original model while also improving their task success rate. Our analysis further confirms that the adversarial process generates a diverse and challenging set of attacks, leading to a more robust agent compared to the base model.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Code Models Suffer from the Dunning-Kruger Effect?</title>
<link>https://arxiv.org/abs/2510.05457</link>
<guid>https://arxiv.org/abs/2510.05457</guid>
<content:encoded><![CDATA[
arXiv:2510.05457v1 Announce Type: cross 
Abstract: As artificial intelligence systems increasingly collaborate with humans in creative and technical domains, questions arise about the cognitive boundaries and biases that shape our shared agency. This paper investigates the Dunning-Kruger Effect (DKE), the tendency for those with limited competence to overestimate their abilities in state-of-the-art LLMs in coding tasks. By analyzing model confidence and performance across a diverse set of programming languages, we reveal that AI models mirror human patterns of overconfidence, especially in unfamiliar or low-resource domains. Our experiments demonstrate that less competent models and those operating in rare programming languages exhibit stronger DKE-like bias, suggesting that the strength of the bias is proportionate to the competence of the models.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VAL-Bench: Measuring Value Alignment in Language Models</title>
<link>https://arxiv.org/abs/2510.05465</link>
<guid>https://arxiv.org/abs/2510.05465</guid>
<content:encoded><![CDATA[
arXiv:2510.05465v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly used for tasks where outputs shape human decisions, so it is critical to test whether their responses reflect consistent human values. Existing benchmarks mostly track refusals or predefined safety violations, but these only check rule compliance and do not reveal whether a model upholds a coherent value system when facing controversial real-world issues. We introduce the \textbf{V}alue \textbf{AL}ignment \textbf{Bench}mark (\textbf{VAL-Bench}), which evaluates whether models maintain a stable value stance across paired prompts that frame opposing sides of public debates. VAL-Bench consists of 115K such pairs from Wikipedia's controversial sections. A well-aligned model should express similar underlying views regardless of framing, which we measure using an LLM-as-judge to score agreement or divergence between paired responses. Applied across leading open- and closed-source models, the benchmark reveals large variation in alignment and highlights trade-offs between safety strategies (e.g., refusals) and more expressive value systems. By providing a scalable, reproducible benchmark, VAL-Bench enables systematic comparison of how reliably LLMs embody human values.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMAQ: Adaptive Mixed-bit Activation Quantization for Collaborative Parameter Efficient Fine-tuning</title>
<link>https://arxiv.org/abs/2510.05468</link>
<guid>https://arxiv.org/abs/2510.05468</guid>
<content:encoded><![CDATA[
arXiv:2510.05468v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are scaling rapidly, creating significant challenges for collaborative server client distributed training, particularly in terms of communication efficiency and computational overheads. To address these challenges, we implement Parameter-efficient Split Learning, which effectively balances efficiency and performance for collaborative training on low-resource devices.
  To reduce communication overhead in collaborative training, we introduce Adaptive Mixed bit Activation Quantization (AMAQ), a strategy that progressively compresses activations and gradients from high precision (6 to 8 bits) to low precision (3 to 4 bits). AMAQ achieves this by effectively allocating bit budgets across channels based on feature wise and layer wise importance using bit regularization.
  Under the same bit budgets, AMAQ outperforms fixed-precision approaches, delivering about 2.5% higher generation accuracy and about 1.3% better classification accuracy for models like LLaMA3 8B and Qwen2.5 7B. In addition, it significantly enhances training stability and reducing ultra-low bit representation collapse during the training.
  Experiments demonstrate that AMAQ integrates effectively into practical multi-machine collaborative training setups, offering superior inference accuracy with only a modest communication overhead for bits adaptation during training. This trade off makes AMAQ a practical and effective solution for collaborative training with minimal communication cost.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NorMuon: Making Muon more efficient and scalable</title>
<link>https://arxiv.org/abs/2510.05491</link>
<guid>https://arxiv.org/abs/2510.05491</guid>
<content:encoded><![CDATA[
arXiv:2510.05491v1 Announce Type: cross 
Abstract: The choice of optimizer significantly impacts the training efficiency and computational costs of large language models (LLMs). Recently, the Muon optimizer has demonstrated promising results by orthogonalizing parameter updates, improving optimization geometry through better conditioning. Despite Muon's emergence as a candidate successor to Adam, the potential for jointly leveraging their strengths has not been systematically explored. In this work, we bridge this gap by proposing NorMuon (Neuron-wise Normalized Muon), an optimizer that synergistically combines orthogonalization with neuron-level adaptive learning rates. Our analysis reveals that while Muon effectively reduces condition numbers, the resulting updates exhibit highly non-uniform neuron norms, causing certain neurons to dominate the optimization process. NorMuon addresses this imbalance by maintaining second-order momentum statistics for each neuron and applying row-wise normalization after orthogonalization, ensuring balanced parameter utilization while preserving Muon's conditioning benefits. To enable practical deployment at scale, we develop an efficient distributed implementation under the FSDP2 framework that strategically distributes orthogonalization computations across devices. Experiments across multiple model scales demonstrate that NorMuon consistently outperforms both Adam and Muon, achieving 21.74% better training efficiency than Adam and 11.31% improvement over Muon on 1.1 B pretraining setting, while maintaining a comparable memory footprint to Muon. Our findings suggest that orthogonalization and adaptive learning rates are complementary rather than competing approaches, opening new avenues for optimizer design in large-scale deep learning.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sci-Phi: A Large Language Model Spatial Audio Descriptor</title>
<link>https://arxiv.org/abs/2510.05542</link>
<guid>https://arxiv.org/abs/2510.05542</guid>
<content:encoded><![CDATA[
arXiv:2510.05542v1 Announce Type: cross 
Abstract: Acoustic scene perception involves describing the type of sounds, their timing, their direction and distance, as well as their loudness and reverberation. While audio language models excel in sound recognition, single-channel input fundamentally limits spatial understanding. This work presents Sci-Phi, a spatial audio large language model with dual spatial and spectral encoders that estimates a complete parameter set for all sound sources and the surrounding environment. Learning from over 4,000 hours of synthetic first-order Ambisonics recordings including metadata, Sci-Phi enumerates and describes up to four directional sound sources in one pass, alongside non-directional background sounds and room characteristics. We evaluate the model with a permutation-invariant protocol and 15 metrics covering content, location, timing, loudness, and reverberation, and analyze its robustness across source counts, signal-to-noise ratios, reverberation levels, and challenging mixtures of acoustically, spatially, or temporally similar sources. Notably, Sci-Phi generalizes to real room impulse responses with only minor performance degradation. Overall, this work establishes the first audio LLM capable of full spatial-scene description, with strong potential for real-world deployment. Demo: https://sci-phi-audio.github.io/demo
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain-Shift-Aware Conformal Prediction for Large Language Models</title>
<link>https://arxiv.org/abs/2510.05566</link>
<guid>https://arxiv.org/abs/2510.05566</guid>
<content:encoded><![CDATA[
arXiv:2510.05566v1 Announce Type: cross 
Abstract: Large language models have achieved impressive performance across diverse tasks. However, their tendency to produce overconfident and factually incorrect outputs, known as hallucinations, poses risks in real world applications. Conformal prediction provides finite-sample, distribution-free coverage guarantees, but standard conformal prediction breaks down under domain shift, often leading to under-coverage and unreliable prediction sets. We propose a new framework called Domain-Shift-Aware Conformal Prediction (DS-CP). Our framework adapts conformal prediction to large language models under domain shift, by systematically reweighting calibration samples based on their proximity to the test prompt, thereby preserving validity while enhancing adaptivity. Our theoretical analysis and experiments on the MMLU benchmark demonstrate that the proposed method delivers more reliable coverage than standard conformal prediction, especially under substantial distribution shifts, while maintaining efficiency. This provides a practical step toward trustworthy uncertainty quantification for large language models in real-world deployment.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-the-Flow Agentic System Optimization for Effective Planning and Tool Use</title>
<link>https://arxiv.org/abs/2510.05592</link>
<guid>https://arxiv.org/abs/2510.05592</guid>
<content:encoded><![CDATA[
arXiv:2510.05592v1 Announce Type: cross 
Abstract: Outcome-driven reinforcement learning has advanced reasoning in large language models (LLMs), but prevailing tool-augmented approaches train a single, monolithic policy that interleaves thoughts and tool calls under full context; this scales poorly with long horizons and diverse tools and generalizes weakly to new scenarios. Agentic systems offer a promising alternative by decomposing work across specialized modules, yet most remain training-free or rely on offline training decoupled from the live dynamics of multi-turn interaction. We introduce AgentFlow, a trainable, in-the-flow agentic framework that coordinates four modules (planner, executor, verifier, generator) through an evolving memory and directly optimizes its planner inside the multi-turn loop. To train on-policy in live environments, we propose Flow-based Group Refined Policy Optimization (Flow-GRPO), which tackles long-horizon, sparse-reward credit assignment by converting multi-turn optimization into a sequence of tractable single-turn policy updates. It broadcasts a single, verifiable trajectory-level outcome to every turn to align local planner decisions with global success and stabilizes learning with group-normalized advantages. Across ten benchmarks, AgentFlow with a 7B-scale backbone outperforms top-performing baselines with average accuracy gains of 14.9% on search, 14.0% on agentic, 14.5% on mathematical, and 4.1% on scientific tasks, even surpassing larger proprietary models like GPT-4o. Further analyses confirm the benefits of in-the-flow optimization, showing improved planning, enhanced tool-calling reliability, and positive scaling with model size and reasoning turns.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Chain-of-Thought Efficiency for Autoregressive Image Generation</title>
<link>https://arxiv.org/abs/2510.05593</link>
<guid>https://arxiv.org/abs/2510.05593</guid>
<content:encoded><![CDATA[
arXiv:2510.05593v1 Announce Type: cross 
Abstract: Autoregressive multimodal large language models have recently gained popularity for image generation, driven by advances in foundation models. To enhance alignment and detail, newer approaches employ chain-of-thought (CoT) reasoning, expanding user inputs into elaborated prompts prior to image synthesis. However, this strategy can introduce unnecessary redundancy -- a phenomenon we call visual overthinking -- which increases computational costs and can introduce details that contradict the original prompt. In this work, we explore how to generate more concise CoT sequences for more efficient image generation. We introduce ShortCoTI, a lightweight optimization framework that encourages more concise CoT while preserving output image quality. ShortCoTI rewards more concise prompts with an adaptive function that scales according to an estimated difficulty for each task. Incorporating this reward into a reinforcement learning paradigm reduces prompt reasoning length by 54% while maintaining or slightly improving quality metrics across multiple benchmarks (T2I-CompBench, GenEval). Qualitative analysis shows that our method eliminates verbose explanations and repetitive refinements, producing reasoning prompts that are both concise and semantically rich. As a result, ShortCoTI improves computational efficiency without compromising the fidelity or visual appeal of generated images.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI-Driven Hierarchical Multi-Agent Framework for Zero-Touch Optical Networks</title>
<link>https://arxiv.org/abs/2510.05625</link>
<guid>https://arxiv.org/abs/2510.05625</guid>
<content:encoded><![CDATA[
arXiv:2510.05625v1 Announce Type: cross 
Abstract: The rapid development of Generative Artificial Intelligence (GenAI) has catalyzed a transformative technological revolution across all walks of life. As the backbone of wideband communication, optical networks are expecting high-level autonomous operation and zero-touch management to accommodate their expanding network scales and escalating transmission bandwidth. The integration of GenAI is deemed as the pivotal solution for realizing zero-touch optical networks. However, the lifecycle management of optical networks involves a multitude of tasks and necessitates seamless collaboration across multiple layers, which poses significant challenges to the existing single-agent GenAI systems. In this paper, we propose a GenAI-driven hierarchical multi-agent framework designed to streamline multi-task autonomous execution for zero-touch optical networks. We present the architecture, implementation, and applications of this framework. A field-deployed mesh network is utilized to demonstrate three typical scenarios throughout the lifecycle of optical network: quality of transmission estimation in the planning stage, dynamic channel adding/dropping in the operation stage, and system capacity increase in the upgrade stage. The case studies, illustrate the capabilities of multi-agent framework in multi-task allocation, coordination, execution, evaluation, and summarization. This work provides a promising approach for the future development of intelligent, efficient, and collaborative network management solutions, paving the way for more specialized and adaptive zero-touch optical networks.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Reliable and Practical LLM Security Evaluations via Bayesian Modelling</title>
<link>https://arxiv.org/abs/2510.05709</link>
<guid>https://arxiv.org/abs/2510.05709</guid>
<content:encoded><![CDATA[
arXiv:2510.05709v1 Announce Type: cross 
Abstract: Before adopting a new large language model (LLM) architecture, it is critical to understand vulnerabilities accurately. Existing evaluations can be difficult to trust, often drawing conclusions from LLMs that are not meaningfully comparable, relying on heuristic inputs or employing metrics that fail to capture the inherent uncertainty. In this paper, we propose a principled and practical end-to-end framework for evaluating LLM vulnerabilities to prompt injection attacks. First, we propose practical approaches to experimental design, tackling unfair LLM comparisons by considering two practitioner scenarios: when training an LLM and when deploying a pre-trained LLM. Second, we address the analysis of experiments and propose a Bayesian hierarchical model with embedding-space clustering. This model is designed to improve uncertainty quantification in the common scenario that LLM outputs are not deterministic, test prompts are designed imperfectly, and practitioners only have a limited amount of compute to evaluate vulnerabilities. We show the improved inferential capabilities of the model in several prompt injection attack settings. Finally, we demonstrate the pipeline to evaluate the security of Transformer versus Mamba architectures. Our findings show that consideration of output variability can suggest less definitive findings. However, for some attacks, we find notably increased Transformer and Mamba-variant vulnerabilities across LLMs with the same training data or mathematical ability.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Discrete Diffusion Unmasking Policies Beyond Explicit Reference Policies</title>
<link>https://arxiv.org/abs/2510.05725</link>
<guid>https://arxiv.org/abs/2510.05725</guid>
<content:encoded><![CDATA[
arXiv:2510.05725v1 Announce Type: cross 
Abstract: Masked diffusion models (MDMs) have recently emerged as a novel framework for language modeling. MDMs generate sentences by iteratively denoising masked sequences, filling in [MASK] tokens step by step. Although MDMs support any-order sampling, performance is highly sensitive to the choice of which position to unmask next. Prior work typically relies on rule-based schedules (e.g., max-confidence, max-margin), which provide ad hoc improvements. In contrast, we replace these heuristics with a learned scheduler. Specifically, we cast denoising as a KL-regularized Markov decision process (MDP) with an explicit reference policy and optimize a regularized objective that admits policy improvement and convergence guarantees under standard assumptions. We prove that the optimized policy under this framework generates samples that more closely match the data distribution than heuristic schedules. Empirically, across four benchmarks, our learned policy consistently outperforms max-confidence: for example, on SUDOKU, where unmasking order is critical, it yields a 20.1% gain over random and a 11.2% gain over max-confidence.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARM: Discovering Agentic Reasoning Modules for Generalizable Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2510.05746</link>
<guid>https://arxiv.org/abs/2510.05746</guid>
<content:encoded><![CDATA[
arXiv:2510.05746v1 Announce Type: cross 
Abstract: Large Language Model (LLM)-powered Multi-agent systems (MAS) have achieved state-of-the-art results on various complex reasoning tasks. Recent works have proposed techniques to automate the design of MASes, eliminating the need for manual engineering. However, these techniques perform poorly, often achieving similar or inferior performance to simple baselines. Furthermore, they require computationally expensive re-discovery of architectures for each new task domain and expensive data annotation on domains without existing labeled validation sets. A critical insight is that simple Chain of Thought (CoT) reasoning often performs competitively with these complex systems, suggesting that the fundamental reasoning unit of MASes, CoT, warrants further investigation. To this end, we present a new paradigm for automatic MAS design that pivots the focus to optimizing CoT reasoning. We introduce the Agentic Reasoning Module (ARM), an agentic generalization of CoT where each granular reasoning step is executed by a specialized reasoning module. This module is discovered through a tree search over the code space, starting from a simple CoT module and evolved using mutations informed by reflection on execution traces. The resulting ARM acts as a versatile reasoning building block which can be utilized as a direct recursive loop or as a subroutine in a learned meta-orchestrator. Our approach significantly outperforms both manually designed MASes and state-of-the-art automatic MAS design methods. Crucially, MASes built with ARM exhibit superb generalization, maintaining high performance across different foundation models and task domains without further optimization.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Early Multimodal Prediction of Cross-Lingual Meme Virality on Reddit: A Time-Window Analysis</title>
<link>https://arxiv.org/abs/2510.05761</link>
<guid>https://arxiv.org/abs/2510.05761</guid>
<content:encoded><![CDATA[
arXiv:2510.05761v1 Announce Type: cross 
Abstract: Predicting the virality of online content remains challenging, especially for culturally complex, fast-evolving memes. This study investigates the feasibility of early prediction of meme virality using a large-scale, cross-lingual dataset from 25 diverse Reddit communities. We propose a robust, data-driven method to define virality based on a hybrid engagement score, learning a percentile-based threshold from a chronologically held-out training set to prevent data leakage. We evaluated a suite of models, including Logistic Regression, XGBoost, and a Multi-layer Perceptron (MLP), with a comprehensive, multimodal feature set across increasing time windows (30-420 min). Crucially, useful signals emerge quickly: our best-performing model, XGBoost, achieves a PR-AUC $>$ 0.52 in just 30 minutes. Our analysis reveals a clear "evidentiary transition," in which the importance of the feature dynamically shifts from the static context to the temporal dynamics as a meme gains traction. This work establishes a robust, interpretable, and practical benchmark for early virality prediction in scenarios where full diffusion cascade data is unavailable, contributing a novel cross-lingual dataset and a methodologically sound definition of virality. To our knowledge, this study is the first to combine time series data with static content and network features to predict early meme virality.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Premature Exploitation in Particle-based Monte Carlo for Inference-Time Scaling</title>
<link>https://arxiv.org/abs/2510.05825</link>
<guid>https://arxiv.org/abs/2510.05825</guid>
<content:encoded><![CDATA[
arXiv:2510.05825v1 Announce Type: cross 
Abstract: Inference-Time Scaling (ITS) improves language models by allocating more computation at generation time. Particle Filtering (PF) has emerged as a strong ITS method for complex mathematical reasoning tasks, but it is vulnerable when guided by process reward models, which often assign overconfident scores early in the reasoning process. This causes PF to suffer from premature exploitation: it myopically commits to locally promising trajectories, prunes potentially correct hypotheses, and converges to suboptimal solutions. This failure mode, known as particle impoverishment, is especially severe under constrained computational budgets. To address this, we analyze the problem and identify two root causes: a lack of diversity in the particle set due to overconfident resampling and consequent inability to assess the potential of a reasoning path. We introduce Entropic Particle Filtering (ePF), an algorithm that integrates two new techniques to solve these issues. The first technique, Entropic Annealing (EA), directly mitigates particle impoverishment by monitoring search diversity via entropy; when diversity drops, it intervenes by dynamically annealing the resampling distribution to preserve exploration. The second, an enhancement called Look-ahead Modulation (LaM), adds a predictive guide to evaluate a state's potential based on its successors. On several challenging math benchmarks, ePF significantly outperforms strong baselines and achieves up to a 50 % relative improvement in task reward. Together, these methods improve PF's resilience by balancing the exploration of diverse solution spaces with the exploitation of high-reward regions, ultimately leading to higher-quality solutions.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Paying Attention to Hybrid Attention: Untangling the Issues with Conversion Methods</title>
<link>https://arxiv.org/abs/2510.05901</link>
<guid>https://arxiv.org/abs/2510.05901</guid>
<content:encoded><![CDATA[
arXiv:2510.05901v1 Announce Type: cross 
Abstract: Transformers' quadratic computational complexity limits their scalability despite remarkable performance. While linear attention reduces this to linear complexity, pre-training such models from scratch remains, in most cases, prohibitively expensive. Recent post-training linearisation methods convert pre-trained Transformers to linear models efficiently, often using hybrid approaches that combine linear attention with sliding-window softmax. We identify a critical flaw: existing hybrid methods inadvertently bypass the linear component, relying almost entirely on SWA. Component-level diagnostics reveal this previously undetected behaviour stems from overlooked evaluation practices on common-sense benchmarks. We propose three solutions to ensure balanced component usage: (i) inference-time hybridisation of linear-only conversions with sliding-window softmax; (ii) HedgeCATs, combining attention-weight transfer with targeted LoRA fine-tuning; and (iii) Scheduled Sliding-window Dropout (SSD), which stochastically suppresses the softmax branch during training to prevent component collapse. Our methods maintain computational efficiency while recovering most base model performance and ensuring genuine linear attention adoption, restoring the validity of performance attributions in hybrid conversions.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MatheMagic: Generating Dynamic Mathematics Benchmarks Robust to Memorization</title>
<link>https://arxiv.org/abs/2510.05962</link>
<guid>https://arxiv.org/abs/2510.05962</guid>
<content:encoded><![CDATA[
arXiv:2510.05962v1 Announce Type: cross 
Abstract: Conducting contamination-free evaluation of mathematical capabilities can be difficult for two reasons: models may memorize a test set once it is made public, and current mathematical benchmarks are prone to overfitting due to having limited diversity of symbols and rules, coupled with closed-ended answers. This paper proposes a method to leverage these shortcomings as useful features to a construct dynamic, counterfactual benchmark, which can be used to both reveal overfitting and measure true reasoning. We demonstrate this via MatheMagic, which generates math test instances with the interpretations of numbers and operators altered, yet has automatically verifiable answers. Test instances are randomly seeded and constructed at test time to evaluate a model's induction or deduction capability, offering stability, extensibility, comparability, and robustness to overfitting. Our experiments find that models solve deduction more easily than induction, but they revert to standard math. Further analysis reveals that math-adapted models fail to exhibit a general "skill" of reasoning, and fine-tuning on induction tasks generalizes poorly.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sample Smart, Not Hard: Correctness-First Decoding for Better Reasoning in LLMs</title>
<link>https://arxiv.org/abs/2510.05987</link>
<guid>https://arxiv.org/abs/2510.05987</guid>
<content:encoded><![CDATA[
arXiv:2510.05987v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly applied to complex tasks that require extended reasoning. In such settings, models often benefit from diverse chains-of-thought to arrive at multiple candidate solutions. This requires two competing objectives: to inject enough stochasticity to explore multiple reasoning chains, and to ensure sufficient accuracy and quality in each path. Existing works pursue the first objective by increasing exploration at highly uncertain steps with higher temperature or larger candidate token sets, while others improve reliability by rejecting samples with low confidence post-generation, implying that low confidence correlates with low answer quality. These two lines of thought are in conflict, as they conflate different sources of uncertainty. To resolve this, we argue that the decoding rule should be calibrated by correctness, not confidence alone. We should sample from tokens with higher estimated correctness, and reduce sampling where expected correctness is low. We propose simple strategies that achieve this goal: Greedy-Threshold makes sampling greedy at very low confidence steps. Calibrated-TopK and Calibrated-epsilon set truncation threshold based on estimated rank-wise correctness. Together, our findings challenge prevailing heuristics about decoding under uncertainty and show gains across math and general reasoning benchmarks.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deterministic Legal Retrieval: An Action API for Querying the SAT-Graph RAG</title>
<link>https://arxiv.org/abs/2510.06002</link>
<guid>https://arxiv.org/abs/2510.06002</guid>
<content:encoded><![CDATA[
arXiv:2510.06002v1 Announce Type: cross 
Abstract: The Structure-Aware Temporal Graph RAG (SAT-Graph RAG) addresses core limitations of standard Retrieval-Augmented Generation in the legal domain by providing a verifiable knowledge graph that models hierarchical structure, temporal evolution, and causal events of legal norms. However, a critical gap remains: how to reliably query this structured knowledge without sacrificing its deterministic properties. This paper introduces the SAT-Graph API, a formal query execution layer centered on canonical actions-atomic, composable, and auditable primitives that isolate probabilistic discovery from deterministic retrieval. These actions enable: (i) high-precision hybrid search; (ii) robust reference resolution; (iii) point-in-time version retrieval; and (iv) auditable causal tracing. We demonstrate how planner-guided agents can decompose complex queries into Directed Acyclic Graphs (DAGs) of these actions. This two-layer architecture transforms retrieval from an opaque black box to a transparent, auditable process, directly addressing Explainable AI (XAI) requirements for high-stakes domains.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MixReasoning: Switching Modes to Think</title>
<link>https://arxiv.org/abs/2510.06052</link>
<guid>https://arxiv.org/abs/2510.06052</guid>
<content:encoded><![CDATA[
arXiv:2510.06052v1 Announce Type: cross 
Abstract: Reasoning models enhance performance by tackling problems in a step-by-step manner, decomposing them into sub-problems and exploring long chains of thought before producing an answer. However, applying extended reasoning to every step introduces substantial redundancy, as sub-problems vary widely in difficulty and complexity: a small number of pivotal steps are genuinely challenging and decisive for the final answer, while many others only involve straightforward revisions or simple computations. Therefore, a natural idea is to endow reasoning models with the ability to adaptively respond to this variation, rather than treating all steps with the same level of elaboration. To this end, we propose MixReasoning, a framework that dynamically adjusts the depth of reasoning within a single response. The resulting chain of thought then becomes a mixture of detailed reasoning on difficult steps and concise inference on simpler ones. Experiments on GSM8K, MATH-500, and AIME show that MixReasoning shortens reasoning length and substantially improves efficiency without compromising accuracy.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Failures: Understanding LLM Alignment through Failure-Aware Inverse RL</title>
<link>https://arxiv.org/abs/2510.06092</link>
<guid>https://arxiv.org/abs/2510.06092</guid>
<content:encoded><![CDATA[
arXiv:2510.06092v1 Announce Type: cross 
Abstract: Reinforcement Learning from Human Feedback (RLHF) aligns Large Language Models (LLMs) with human preferences, yet the underlying reward signals they internalize remain hidden, posing a critical challenge for interpretability and safety. Existing approaches attempt to extract these latent incentives using Inverse Reinforcement Learning (IRL), but treat all preference pairs equally, often overlooking the most informative signals: those examples the extracted reward model misclassifies or assigns nearly equal scores, which we term \emph{failures}. We introduce a novel \emph{failure-aware} IRL algorithm that focuses on misclassified or difficult examples to recover the latent rewards defining model behaviors. By learning from these failures, our failure-aware IRL extracts reward functions that better reflect the true objectives behind RLHF. We demonstrate that failure-aware IRL outperforms existing IRL baselines across multiple metrics when applied to LLM detoxification, without requiring external classifiers or supervision. Crucially, failure-aware IRL yields rewards that better capture the true incentives learned during RLHF, enabling more effective re-RLHF training than standard IRL. This establishes failure-aware IRL as a robust, scalable method for auditing model alignment and reducing ambiguity in the IRL process.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Alignment Auditor: A Bayesian Framework for Verifying and Refining LLM Objectives</title>
<link>https://arxiv.org/abs/2510.06096</link>
<guid>https://arxiv.org/abs/2510.06096</guid>
<content:encoded><![CDATA[
arXiv:2510.06096v1 Announce Type: cross 
Abstract: The objectives that Large Language Models (LLMs) implicitly optimize remain dangerously opaque, making trustworthy alignment and auditing a grand challenge. While Inverse Reinforcement Learning (IRL) can infer reward functions from behaviour, existing approaches either produce a single, overconfident reward estimate or fail to address the fundamental ambiguity of the task (non-identifiability). This paper introduces a principled auditing framework that re-frames reward inference from a simple estimation task to a comprehensive process for verification. Our framework leverages Bayesian IRL to not only recover a distribution over objectives but to enable three critical audit capabilities: (i) Quantifying and systematically reducing non-identifiability by demonstrating posterior contraction over sequential rounds of evidence; (ii) Providing actionable, uncertainty-aware diagnostics that expose spurious shortcuts and identify out-of-distribution prompts where the inferred objective cannot be trusted; and (iii) Validating policy-level utility by showing that the refined, low-uncertainty reward can be used directly in RLHF to achieve training dynamics and toxicity reductions comparable to the ground-truth alignment process. Empirically, our framework successfully audits a detoxified LLM, yielding a well-calibrated and interpretable objective that strengthens alignment guarantees. Overall, this work provides a practical toolkit for auditors, safety teams, and regulators to verify what LLMs are truly trying to achieve, moving us toward more trustworthy and accountable AI.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Influence Functions for Efficient Data Selection in Reasoning</title>
<link>https://arxiv.org/abs/2510.06108</link>
<guid>https://arxiv.org/abs/2510.06108</guid>
<content:encoded><![CDATA[
arXiv:2510.06108v1 Announce Type: cross 
Abstract: Fine-tuning large language models (LLMs) on chain-of-thought (CoT) data shows that a small amount of high-quality data can outperform massive datasets. Yet, what constitutes "quality" remains ill-defined. Existing reasoning methods rely on indirect heuristics such as problem difficulty or trace length, while instruction-tuning has explored a broader range of automated selection strategies, but rarely in the context of reasoning. We propose to define reasoning data quality using influence functions, which measure the causal effect of individual CoT examples on downstream accuracy, and introduce influence-based pruning, which consistently outperforms perplexity and embedding-based baselines on math reasoning within a model family.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taxonomy of User Needs and Actions</title>
<link>https://arxiv.org/abs/2510.06124</link>
<guid>https://arxiv.org/abs/2510.06124</guid>
<content:encoded><![CDATA[
arXiv:2510.06124v1 Announce Type: cross 
Abstract: The growing ubiquity of conversational AI highlights the need for frameworks that capture not only users' instrumental goals but also the situated, adaptive, and social practices through which they achieve them. Existing taxonomies of conversational behavior either overgeneralize, remain domain-specific, or reduce interactions to narrow dialogue functions. To address this gap, we introduce the Taxonomy of User Needs and Actions (TUNA), an empirically grounded framework developed through iterative qualitative analysis of 1193 human-AI conversations, supplemented by theoretical review and validation across diverse contexts. TUNA organizes user actions into a three-level hierarchy encompassing behaviors associated with information seeking, synthesis, procedural guidance, content creation, social interaction, and meta-conversation. By centering user agency and appropriation practices, TUNA enables multi-scale evaluation, supports policy harmonization across products, and provides a backbone for layering domain-specific taxonomies. This work contributes a systematic vocabulary for describing AI use, advancing both scholarly understanding and practical design of safer, more responsive, and more accountable conversational systems.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TokenChain: A Discrete Speech Chain via Semantic Token Modeling</title>
<link>https://arxiv.org/abs/2510.06201</link>
<guid>https://arxiv.org/abs/2510.06201</guid>
<content:encoded><![CDATA[
arXiv:2510.06201v1 Announce Type: cross 
Abstract: Machine Speech Chain, simulating the human perception-production loop, proves effective in jointly improving ASR and TTS. We propose TokenChain, a fully discrete speech chain coupling semantic-token ASR with a two-stage TTS: an autoregressive text-to-semantic model co-trained with ASR and a masked-generative semantic-to-acoustic model for synthesis only. End-to-end feedback across the text interface is enabled with straight-through argmax/Gumbel-Softmax and balanced with supervised ASR via dynamic weight averaging. Ablations examine optimal temperature schedules for in- and cross-domain transfer. Evaluation reveals TokenChain surpasses baseline accuracy 2-6 epochs earlier and yields 5-13% lower equal-epoch error with stable T2S on LibriSpeech, and reduces relative ASR WER by 56% and T2S WER by 31% on TED-LIUM with minimal forgetting, showing that chain learning remains effective with token interfaces and models.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stratified GRPO: Handling Structural Heterogeneity in Reinforcement Learning of LLM Search Agents</title>
<link>https://arxiv.org/abs/2510.06214</link>
<guid>https://arxiv.org/abs/2510.06214</guid>
<content:encoded><![CDATA[
arXiv:2510.06214v1 Announce Type: cross 
Abstract: Large language model (LLM) agents increasingly rely on external tools such as search engines to solve complex, multi-step problems, and reinforcement learning (RL) has become a key paradigm for training them. However, the trajectories of search agents are structurally heterogeneous, where variations in the number, placement, and outcomes of search calls lead to fundamentally different answer directions and reward distributions. Standard policy gradient methods, which use a single global baseline, suffer from what we identify and formalize as cross-stratum bias-an "apples-to-oranges" comparison of heterogeneous trajectories. This cross-stratum bias distorts credit assignment and hinders exploration of complex, multi-step search strategies. To address this, we propose Stratified GRPO, whose central component, Stratified Advantage Normalization (SAN), partitions trajectories into homogeneous strata based on their structural properties and computes advantages locally within each stratum. This ensures that trajectories are evaluated only against their true peers. Our analysis proves that SAN eliminates cross-stratum bias, yields conditionally unbiased unit-variance estimates inside each stratum, and retains the global unbiasedness and unit-variance properties enjoyed by standard normalization, resulting in a more pure and scale-stable learning signal. To improve practical stability under finite-sample regimes, we further linearly blend SAN with the global estimator. Extensive experiments on diverse single-hop and multi-hop question-answering benchmarks demonstrate that Stratified GRPO consistently and substantially outperforms GRPO by up to 11.3 points, achieving higher training rewards, greater training stability, and more effective search policies. These results establish stratification as a principled remedy for structural heterogeneity in RL for LLM search agents.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular Reasoning</title>
<link>https://arxiv.org/abs/2510.06217</link>
<guid>https://arxiv.org/abs/2510.06217</guid>
<content:encoded><![CDATA[
arXiv:2510.06217v1 Announce Type: cross 
Abstract: Process Reward Models (PRMs) have recently emerged as a powerful framework for enhancing the reasoning capabilities of large reasoning models (LRMs), particularly in the context of test-time scaling (TTS). However, their potential for supervising LRMs on tabular reasoning domains remains underexplored. Through detailed empirical analyses, we identify that existing PRMs, though widely adopted for supervising text-only reasoning steps, struggle with table-specific operations such as sub-table retrieval and schema interaction, leading to critical performance bottlenecks. To address this limitation, we propose TaTToo, a novel table-grounded PRM framework that (i) reasons explicitly over tabular reasoning steps and (ii) integrates tool-based verification to provide precise reward supervision. Concretely, we first design a scalable data curation pipeline that constructs over 60k high-quality step-level annotations by integrating table verification rationales with tool-based executions. Building on the collected data, we train TaTToo with a dual-stage paradigm: cold-start supervised fine-tuning to capture tool-use reasoning patterns, followed by reinforcement learning with tool-grounded reward shaping to align our model with table-based verification. We provide a comprehensive evaluation of the policy improvement induced by our newly designed PRM. Across 5 challenging tabular reasoning benchmarks covering numerical reasoning, fact-checking, and data analysis, TaTToo improves downstream policy LRMs by 30.9% at inference, surpasses strong PRM baselines such as Qwen-2.5-Math-PRM-72B with only 8B parameters, and demonstrates strong generalizability across diverse TTS strategies.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MathVC: An LLM-Simulated Multi-Character Virtual Classroom for Mathematics Education</title>
<link>https://arxiv.org/abs/2404.06711</link>
<guid>https://arxiv.org/abs/2404.06711</guid>
<content:encoded><![CDATA[
arXiv:2404.06711v3 Announce Type: replace 
Abstract: Collaborative problem solving (CPS) is essential in mathematics education, fostering deeper learning through the exchange of ideas. Yet, classrooms often lack the resources, time, and peer dynamics needed to sustain productive CPS. Recent advancements in Large Language Models (LLMs) offer a promising avenue to enhance CPS in mathematical education. We designed and developed MathVC, a multi-persona LLM simulated virtual classroom platform to facilitate CPS in mathematics. MathVC combines a meta planning controller that monitors CPS stages-sense-making, team organization, planning, execution, validation, and predicts the next speaker, with a persona simulation stack that encodes mathematical thinking via a task schema and error-injected persona schemas seeded from teacher-specified misconceptions. We evaluated MathVC with 14 U.S. middle schoolers. Students reported constructive interaction and reaching shared solutions, describing gains in engagement, motivation, and confidence through diverse perspectives, immediate scaffolding, and human-like fallibility. Our findings also provide insights into simulating peers via LLM-based technologies for collaboration to support learning.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SciKnowEval: Evaluating Multi-level Scientific Knowledge of Large Language Models</title>
<link>https://arxiv.org/abs/2406.09098</link>
<guid>https://arxiv.org/abs/2406.09098</guid>
<content:encoded><![CDATA[
arXiv:2406.09098v4 Announce Type: replace 
Abstract: Large language models (LLMs) are playing an increasingly important role in scientific research, yet there remains a lack of comprehensive benchmarks to evaluate the breadth and depth of scientific knowledge embedded in these models. To address this gap, we introduce SciKnowEval, a large-scale dataset designed to systematically assess LLMs across five progressive levels of scientific understanding: memory, comprehension, reasoning, discernment, and application. SciKnowEval comprises 28K multi-level questions and solutions spanning biology, chemistry, physics, and materials science. Using this benchmark, we evaluate 20 leading open-source and proprietary LLMs. The results show that while proprietary models often achieve state-of-the-art performance, substantial challenges remain -- particularly in scientific reasoning and real-world application. We envision SciKnowEval as a standard benchmark for evaluating scientific capabilities in LLMs and as a catalyst for advancing more capable and reliable scientific language models.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robustness of Large Language Models to Perturbations in Text</title>
<link>https://arxiv.org/abs/2407.08989</link>
<guid>https://arxiv.org/abs/2407.08989</guid>
<content:encoded><![CDATA[
arXiv:2407.08989v2 Announce Type: replace 
Abstract: Having a clean dataset has been the foundational assumption of most natural language processing (NLP) systems. However, properly written text is rarely found in real-world scenarios and hence, oftentimes invalidates the aforementioned foundational assumption. Recently, Large language models (LLMs) have shown impressive performance, but can they handle the inevitable noise in real-world data? This work tackles this critical question by investigating LLMs' resilience against morphological variations in text. To that end, we artificially introduce varying levels of noise into a diverse set of datasets and systematically evaluate LLMs' robustness against the corrupt variations of the original text. Our findings show that contrary to popular beliefs, generative LLMs are quiet robust to noisy perturbations in text. This is a departure from pre-trained models like BERT or RoBERTa whose performance has been shown to be sensitive to deteriorating noisy text. Additionally, we test LLMs' resilience on multiple real-world benchmarks that closely mimic commonly found errors in the wild. With minimal prompting, LLMs achieve a new state-of-the-art on the benchmark tasks of Grammar Error Correction (GEC) and Lexical Semantic Change (LSC). To empower future research, we also release a dataset annotated by humans stating their preference for LLM vs. human-corrected outputs along with the code to reproduce our results.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text Clustering as Classification with LLMs</title>
<link>https://arxiv.org/abs/2410.00927</link>
<guid>https://arxiv.org/abs/2410.00927</guid>
<content:encoded><![CDATA[
arXiv:2410.00927v3 Announce Type: replace 
Abstract: Text clustering serves as a fundamental technique for organizing and interpreting unstructured textual data, particularly in contexts where manual annotation is prohibitively costly. With the rapid advancement of Large Language Models (LLMs) and their demonstrated effectiveness across a broad spectrum of NLP tasks, an emerging body of research has begun to explore their potential in the domain of text clustering. However, existing LLM-based approaches still rely on fine-tuned embedding models and sophisticated similarity metrics, rendering them computationally intensive and necessitating domain-specific adaptation. To address these limitations, we propose a novel framework that reframes text clustering as a classification task by harnessing the in-context learning capabilities of LLMs. Our framework eliminates the need for fine-tuning embedding models or intricate clustering algorithms. It comprises two key steps: first, the LLM is prompted to generate a set of candidate labels based on the dataset and then merges semantically similar labels; second, it assigns the most appropriate label to each text sample. By leveraging the advanced natural language understanding and generalization capabilities of LLMs, the proposed approach enables effective clustering with minimal human intervention. Experimental results on diverse datasets demonstrate that our framework achieves comparable or superior performance to state-of-the-art embedding-based clustering techniques, while significantly reducing computational complexity and resource requirements. These findings underscore the transformative potential of LLMs in simplifying and enhancing text clustering tasks. We make our code available to the public for utilization at https://github.com/ECNU-Text-Computing/Text-Clustering-via-LLM. We also provide the supplementary Appendix within the repository.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HEALTH-PARIKSHA: Assessing RAG Models for Health Chatbots in Real-World Multilingual Settings</title>
<link>https://arxiv.org/abs/2410.13671</link>
<guid>https://arxiv.org/abs/2410.13671</guid>
<content:encoded><![CDATA[
arXiv:2410.13671v2 Announce Type: replace 
Abstract: Assessing the capabilities and limitations of large language models (LLMs) has garnered significant interest, yet the evaluation of multiple models in real-world scenarios remains rare. Multilingual evaluation often relies on translated benchmarks, which typically do not capture linguistic and cultural nuances present in the source language. This study provides an extensive assessment of 24 LLMs on real world data collected from Indian patients interacting with a medical chatbot in Indian English and 4 other Indic languages. We employ a uniform Retrieval Augmented Generation framework to generate responses, which are evaluated using both automated techniques and human evaluators on four specific metrics relevant to our application. We find that models vary significantly in their performance and that instruction tuned Indic models do not always perform well on Indic language queries. Further, we empirically show that factual correctness is generally lower for responses to Indic queries compared to English queries. Finally, our qualitative work shows that code-mixed and culturally relevant queries in our dataset pose challenges to evaluated models.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BanglaLlama: LLaMA for Bangla Language</title>
<link>https://arxiv.org/abs/2410.21200</link>
<guid>https://arxiv.org/abs/2410.21200</guid>
<content:encoded><![CDATA[
arXiv:2410.21200v2 Announce Type: replace 
Abstract: Bangla is a language spoken by approximately 240 million native speakers and around 300 million people worldwide. Despite being the 5th largest spoken language in the world, Bangla is still a "low-resource" language, and existing pretrained language models often struggle to perform well on Bangla Language Processing (BLP) tasks. This paper addresses this gap by: (1) introducing two high-quality translated Bangla-instruction datasets totaling 224k samples - Bangla-Orca (172k) and Bangla-Alpaca (52k); and (2) leveraging these datasets to develop BanglaLlama, an open-source family of Bangla-specific LLMs, consisting of five base and instruct variants. We present our methodology, two large datasets, and comprehensive benchmarking results showcasing the effectiveness of our dataset and model on multiple benchmarks. We believe our proposed datasets and models will serve as the new standard baseline for future research focused on this widely spoken yet "low-resource" language.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explaining GPTs' Schema of Depression: A Machine Behavior Analysis</title>
<link>https://arxiv.org/abs/2411.13800</link>
<guid>https://arxiv.org/abs/2411.13800</guid>
<content:encoded><![CDATA[
arXiv:2411.13800v2 Announce Type: replace 
Abstract: Use of large language models such as ChatGPT (GPT-4/GPT-5) for mental health support has grown rapidly, emerging as a promising route to assess and help people with mood disorders like depression. However, we have a limited understanding of these language models' schema of mental disorders, that is, how they internally associate and interpret symptoms of such disorders. In this work, we leveraged contemporary measurement theory to decode how GPT-4 and GPT-5 interrelate depressive symptoms, providing an explanation of how LLMs apply what they learn and informing clinical applications. We found that GPT-4 (a) had strong convergent validity with standard instruments and expert judgments $(r = 0.70 - 0.81)$, and (b) behaviorally linked depression symptoms with each other (symptom inter-correlates $r = 0.23 - 0.78$) in accordance with established literature on depression; however, it (c) underemphasized the relationship between $\textit{suicidality}$ and other symptoms while overemphasizing $\textit{psychomotor symptoms}$; and (d) suggested novel hypotheses of symptom mechanisms, for instance, indicating that $\textit{sleep}$ and $\textit{fatigue}$ are broadly influenced by other depressive symptoms, while $\textit{worthlessness/guilt}$ is only tied to $\textit{depressed mood}$. GPT-5 showed a slightly lower convergence with self-report, a difference our machine-behavior analysis makes interpretable through shifts in symptom-symptom relationships. These insights provide an empirical foundation for understanding language models' mental health assessments and demonstrate a generalizable approach for explainability in other models and disorders. Our findings can guide key stakeholders to make informed decisions for effectively situating these technologies in the care system.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating and Mitigating Social Bias for Large Language Models in Open-ended Settings</title>
<link>https://arxiv.org/abs/2412.06134</link>
<guid>https://arxiv.org/abs/2412.06134</guid>
<content:encoded><![CDATA[
arXiv:2412.06134v3 Announce Type: replace 
Abstract: Current social bias benchmarks for Large Language Models (LLMs) primarily rely on predefined question formats like multiple-choice, limiting their ability to reflect the complexity and open-ended nature of real-world interactions. To close this gap, we extend an existing dataset BBQ (Parrish et al., 2022) to Open-BBQ, a comprehensive framework to evaluate the social bias of LLMs in open-ended settings by incorporating two additional question categories: fill-in-the-blank and short-answer. Since our new Open-BBQ dataset contains a lot of open-ended responses like sentences and paragraphs, we developed an evaluation process to detect biases from open-ended content by labeling sentences and paragraphs. In addition to this, we also found that existing debiasing methods, such as self-debiasing (Gallegos et al., 2024), have over-correction issues, which make the original correct answers incorrect. In order to solve this issue, we propose Composite Prompting, an In-context Learning (ICL) method combining structured examples with explicit chain-of-thought reasoning to form a unified instruction template for LLMs to explicitly identify content that needs debiasing. Experimental results show that the proposed method significantly reduces the bias for both GPT-3.5 and GPT-4o while maintaining high accuracy.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QAPyramid: Fine-grained Evaluation of Content Selection for Text Summarization</title>
<link>https://arxiv.org/abs/2412.07096</link>
<guid>https://arxiv.org/abs/2412.07096</guid>
<content:encoded><![CDATA[
arXiv:2412.07096v2 Announce Type: replace 
Abstract: How to properly conduct human evaluations for text summarization is a longstanding challenge. The Pyramid human evaluation protocol, which assesses content selection by breaking the reference summary into subunits and verifying their presence in the system summary, has been widely adopted. However, it suffers from a lack of systematicity in the definition and granularity of the sub-units. We address these problems by proposing QAPyramid, which decomposes each reference summary into finer-grained question-answer (QA) pairs according to the QA-SRL framework. We collect QA-SRL annotations for reference summaries from CNN/DM and evaluate 10 summarization systems, resulting in 8.9K QA-level annotations. We show that, compared to Pyramid, QAPyramid provides more systematic and fine-grained content selection evaluation while maintaining high inter-annotator agreement without needing expert annotations. Furthermore, we propose metrics that automate the evaluation pipeline and achieve higher correlations with QAPyramid than other widely adopted metrics.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Retrieval-Augmented Generation through Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2501.15228</link>
<guid>https://arxiv.org/abs/2501.15228</guid>
<content:encoded><![CDATA[
arXiv:2501.15228v2 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) is widely utilized to incorporate external knowledge into large language models, thereby enhancing factuality and reducing hallucinations in question-answering (QA) tasks. A standard RAG pipeline consists of several components, such as query rewriting, document retrieval, document filtering, and answer generation. However, these components are typically optimized separately through supervised fine-tuning, which can lead to misalignments between the objectives of individual components and the overarching aim of generating accurate answers. Although recent efforts have explored using reinforcement learning (RL) to optimize specific RAG components, these approaches often focus on simple pipelines with only two components or do not adequately address the complex interdependencies and collaborative interactions among the modules. To overcome these limitations, we propose treating the complex RAG pipeline with multiple components as a multi-agent cooperative task, in which each component can be regarded as an RL agent. Specifically, we present MMOA-RAG, Multi-Module joint Optimization Algorithm for RAG, which employs multi-agent reinforcement learning to harmonize all agents' goals toward a unified reward, such as the F1 score of the final answer. Experiments conducted on various QA benchmarks demonstrate that MMOA-RAG effectively boost the overall performance of the pipeline and outperforms existing baselines. Furthermore, comprehensive ablation studies validate the contributions of individual components and demonstrate MMOA-RAG can be adapted to different RAG pipelines and benchmarks.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Generative Approach to LLM Harmfulness Mitigation with Red Flag Tokens</title>
<link>https://arxiv.org/abs/2502.16366</link>
<guid>https://arxiv.org/abs/2502.16366</guid>
<content:encoded><![CDATA[
arXiv:2502.16366v4 Announce Type: replace 
Abstract: Many safety post-training methods for large language models (LLMs) are designed to modify the model's behaviour from producing unsafe answers to issuing refusals. However, such distribution shifts are often brittle and degrade performance on desirable tasks. To address these pitfalls, we propose augmenting the model's vocabulary with a special red flag token, and training the model to insert this token whenever harmful content is generated or imminent. This approach enables the model to explicitly learn the concept of harmfulness in its representations, with minimal impact on utility due to the marginal change in the generated distribution of natural language. Moreover, because the token is embedded in the model's vocabulary, we can naturally leverage the LLMs' generalization capabilities, such as in-context learning (ICL) and out-of-distribution generalization to languages that are not formally supported (e.g., Japanese for Llama3). In particular, we demonstrate that through ICL alone, the model can learn to initiate reflective reasoning upon generating the red flag token at inference, which steers the response away from harmful continuations or enables self-correction when the flag is raised falsely. This approach is orthogonal and complementary to existing safety technique (such as safety classifiers or standard safety training) and easier to evaluate in comparison to natural language refusals, as it does not require a human or automated judge to assess the harmlessness of the answers.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Relation-Specific Neurons in Large Language Models</title>
<link>https://arxiv.org/abs/2502.17355</link>
<guid>https://arxiv.org/abs/2502.17355</guid>
<content:encoded><![CDATA[
arXiv:2502.17355v2 Announce Type: replace 
Abstract: In large language models (LLMs), certain \emph{neurons} can store distinct pieces of knowledge learned during pretraining. While factual knowledge typically appears as a combination of \emph{relations} and \emph{entities}, it remains unclear whether some neurons focus on a relation itself -- independent of any entity. We hypothesize such neurons \emph{detect} a relation in the input text and \emph{guide} generation involving such a relation. To investigate this, we study the LLama-2 family on a chosen set of relations, with a \textit{statistics}-based method. Our experiments demonstrate the existence of relation-specific neurons. We measure the effect of selectively deactivating candidate neurons specific to relation $r$ on the LLM's ability to handle (1) facts involving relation $r$ and (2) facts involving a different relation $r' \neq r$. With respect to their capacity for encoding relation information, we give evidence for the following three properties of relation-specific neurons. \textbf{(i) Neuron cumulativity.} Multiple neurons jointly contribute to processing facts involving relation $r$, with no single neuron fully encoding a fact in $r$ on its own. \textbf{(ii) Neuron versatility.} Neurons can be shared across multiple closely related as well as less related relations. In addition, some relation neurons transfer across languages. \textbf{(iii) Neuron interference.} Deactivating neurons specific to one relation can improve LLMs' factual recall performance for facts of other relations. We make our code and data publicly available at https://github.com/cisnlp/relation-specific-neurons.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Effect of Retrieval Augmentation on Social Biases</title>
<link>https://arxiv.org/abs/2502.17611</link>
<guid>https://arxiv.org/abs/2502.17611</guid>
<content:encoded><![CDATA[
arXiv:2502.17611v2 Announce Type: replace 
Abstract: Retrieval Augmented Generation (RAG) has gained popularity as a method for conveniently incorporating novel facts that were not seen during the pre-training stage in Large Language Model (LLM)-based Natural Language Generation (NLG) systems. However, LLMs are known to encode significant levels of unfair social biases. The modulation of these biases by RAG in NLG systems is not well understood. In this paper, we systematically study the relationship between the different components of a RAG system and the social biases presented in the text generated across three languages (i.e. English, Japanese and Chinese) and four social bias types (i.e. gender, race, age and religion). Specifically, using the Bias Question Answering (BBQ) benchmark datasets, we evaluate the social biases in RAG responses from document collections with varying levels of stereotypical biases, employing multiple LLMs used as generators. We find that the biases in document collections are often amplified in the generated responses, even when the generating LLM exhibits a low-level of bias. Our findings raise concerns about the use of RAG as a technique for injecting novel facts into NLG systems and call for careful evaluation of potential social biases in RAG applications before their real-world deployment.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometry-Guided Adversarial Prompt Detection via Curvature and Local Intrinsic Dimension</title>
<link>https://arxiv.org/abs/2503.03502</link>
<guid>https://arxiv.org/abs/2503.03502</guid>
<content:encoded><![CDATA[
arXiv:2503.03502v2 Announce Type: replace 
Abstract: Adversarial prompts are capable of jailbreaking frontier large language models (LLMs) and inducing undesirable behaviours, posing a significant obstacle to their safe deployment. Current mitigation strategies primarily rely on activating built-in defence mechanisms or fine-tuning LLMs, both of which are computationally expensive and can sacrifice model utility. In contrast, detection-based approaches are more efficient and practical for deployment in real-world applications. However, the fundamental distinctions between adversarial and benign prompts remain poorly understood. In this work, we introduce CurvaLID, a novel defence framework that efficiently detects adversarial prompts by leveraging their geometric properties. It is agnostic to the type of LLM, offering a unified detection framework across diverse adversarial prompts and LLM architectures. CurvaLID builds on the geometric analysis of text prompts to uncover their underlying differences. We theoretically extend the concept of curvature via the Whewell equation into an $n$-dimensional word embedding space, enabling us to quantify local geometric properties, including semantic shifts and curvature in the underlying manifolds. To further enhance our solution, we leverage Local Intrinsic Dimensionality (LID) to capture complementary geometric features of text prompts within adversarial subspaces. Our findings show that adversarial prompts exhibit distinct geometric signatures from benign prompts, enabling CurvaLID to achieve near-perfect classification and outperform state-of-the-art detectors in adversarial prompt detection. CurvaLID provides a reliable and efficient safeguard against malicious queries as a model-agnostic method that generalises across multiple LLMs and attack families.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WildIFEval: Instruction Following in the Wild</title>
<link>https://arxiv.org/abs/2503.06573</link>
<guid>https://arxiv.org/abs/2503.06573</guid>
<content:encoded><![CDATA[
arXiv:2503.06573v2 Announce Type: replace 
Abstract: Recent LLMs have shown remarkable success in following user instructions, yet handling instructions with multiple constraints remains a significant challenge. In this work, we introduce WildIFEval - a large-scale dataset of 7K real user instructions with diverse, multi-constraint conditions. Unlike prior datasets, our collection spans a broad lexical and topical spectrum of constraints, extracted from natural user instructions. We categorize these constraints into eight high-level classes to capture their distribution and dynamics in real-world scenarios. Leveraging WildIFEval, we conduct extensive experiments to benchmark the instruction-following capabilities of leading LLMs. WildIFEval clearly differentiates between small and large models, and demonstrates that all models have a large room for improvement on such tasks. We analyze the effects of the number and type of constraints on performance, revealing interesting patterns of model constraint-following behavior. We release our dataset to promote further research on instruction-following under complex, realistic conditions.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Teaching Enables Probabilistic Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2503.17523</link>
<guid>https://arxiv.org/abs/2503.17523</guid>
<content:encoded><![CDATA[
arXiv:2503.17523v2 Announce Type: replace 
Abstract: Artificial intelligence systems based on large language models (LLMs) are increasingly used as agents that interact with users and with the world. To do so successfully, LLMs need to construct internal representations of the world and form probabilistic beliefs about those representations. To provide a user with personalized recommendations, for example, the LLM needs to gradually infer the user's preferences, over the course of multiple interactions. To evaluate whether contemporary LLMs are able to do so, we use the Bayesian inference framework from probability theory, which lays out the optimal way to update an agent's beliefs as it receives new information. We first show that LLMs do not update their beliefs as expected from the Bayesian framework, and that consequently their predictions do not improve as expected as more information becomes available. To address this issue, we teach the LLMs to reason in a Bayesian manner by training them to mimic the predictions of the normative Bayesian model. We find that this approach not only significantly improves the LLM's performance on the particular recommendation task it is trained on, but also enables generalization to other tasks. This suggests that this method teaches the LLM to better approximate Bayesian reasoning. More generally, our results indicate that LLMs can effectively learn reasoning skills from examples and generalize those skills to new domains.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building Resource-Constrained Language Agents: A Korean Case Study on Chemical Toxicity Information</title>
<link>https://arxiv.org/abs/2503.17753</link>
<guid>https://arxiv.org/abs/2503.17753</guid>
<content:encoded><![CDATA[
arXiv:2503.17753v2 Announce Type: replace 
Abstract: Language agents powered by large language models (LLMs) face significant deployment challenges in resource-constrained environments, particularly for specialized domains and less-common languages. This paper presents Tox-chat, a Korean chemical toxicity information agent devised within these limitations. We propose two key innovations: a context-efficient architecture that reduces token consumption through hierarchical section search, and a scenario-based dialogue generation methodology that effectively distills tool-using capabilities from larger models. Experimental evaluations demonstrate that our fine-tuned 8B parameter model substantially outperforms both untuned models and baseline approaches, in terms of DB faithfulness and preference. Our work offers valuable insights for researchers developing domain-specific language agents under practical constraints.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MASRAD: Arabic Terminology Management Corpora with Semi-Automatic Construction</title>
<link>https://arxiv.org/abs/2503.19211</link>
<guid>https://arxiv.org/abs/2503.19211</guid>
<content:encoded><![CDATA[
arXiv:2503.19211v2 Announce Type: replace 
Abstract: This paper presents MASRAD, a terminology dataset for Arabic terminology management, and a method with supporting tools for its semi-automatic construction. The entries in MASRAD are $(f,a)$ pairs of foreign (non-Arabic) terms $f$, appearing in specialized, academic and field-specific books next to their Arabic $a$ counterparts. MASRAD-Ex systematically extracts these pairs as a first step to construct MASRAD. MASRAD helps improving term consistency in academic translations and specialized Arabic documents, and automating cross-lingual text processing. MASRAD-Ex leverages translated terms organically occurring in Arabic books, and considers several candidate pairs for each term phrase. The candidate Arabic terms occur next to the foreign terms, and vary in length. MASRAD-Ex computes lexicographic, phonetic, morphological, and semantic similarity metrics for each candidate pair, and uses heuristic, machine learning, and machine learning with post-processing approaches to decide on the best candidate. This paper presents MASRAD after thorough expert review and makes it available to the interested research community. The best performing MASRAD-Ex approach achieved 90.5% precision and 92.4% recall.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entropy-Gated Branching for Efficient Test-Time Reasoning</title>
<link>https://arxiv.org/abs/2503.21961</link>
<guid>https://arxiv.org/abs/2503.21961</guid>
<content:encoded><![CDATA[
arXiv:2503.21961v3 Announce Type: replace 
Abstract: Test-time compute methods can significantly improve the reasoning capabilities and problem-solving accuracy of large language models (LLMs). However, these approaches require substantially more computational resources, with most compute wasted on exploring low-diversity branches where the model already exhibits high confidence. We observe that a small subset of uncertain reasoning steps has a disproportionately large impact on final prediction accuracy, and branching at these critical junctures tends to yield more diverse and higher-quality candidate reasoning steps. We propose Entropy-Gated Branching (EGB), which branches only at high-uncertainty steps and prunes expansions with a lightweight verifier. On mathematical and financial reasoning benchmarks, EGB improves accuracy by 22.6% over standard inference while operating 31%-75% faster across math benchmarks than test-time beam search with higher performance. Our results show that dynamic resource allocation during inference can substantially improve both efficiency and effectiveness, offering a more scalable pathway to enhanced LLM reasoning capabilities.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Routing RAG: Binding Selective Retrieval with Knowledge Verbalization</title>
<link>https://arxiv.org/abs/2504.01018</link>
<guid>https://arxiv.org/abs/2504.01018</guid>
<content:encoded><![CDATA[
arXiv:2504.01018v2 Announce Type: replace 
Abstract: Selective retrieval improves the accuracy and efficiency of retrieval-augmented generation (RAG) by reducing distractions from low-quality retrievals. However, existing approaches underutilize the inherent knowledge of large language models (LLMs), leading to suboptimal retrieval decisions and degraded generation performance. To bridge this gap, we propose Self-Routing RAG (SR-RAG), a novel framework that binds selective retrieval with knowledge verbalization. SR-RAG enables an LLM to dynamically decide whether to retrieve external knowledge or verbalize its own parametric knowledge. To this end, we design a multi-task objective that jointly optimizes an LLM for knowledge source selection, knowledge verbalization, and response generation. SR-RAG further incorporates a nearest neighbor search mechanism at inference time to improve the accuracy of knowledge source decisions under domain shifts. Fine-tuning three LLMs with SR-RAG significantly improves both their response accuracy and reduces the inference latency. Compared to the strongest selective retrieval baseline, SR-RAG reduces the number of retrievals by 29% while improving performance by 5.1%.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAFER: Advancing Safety Alignment via Efficient Ex-Ante Reasoning</title>
<link>https://arxiv.org/abs/2504.02725</link>
<guid>https://arxiv.org/abs/2504.02725</guid>
<content:encoded><![CDATA[
arXiv:2504.02725v2 Announce Type: replace 
Abstract: Recent advancements in large language models (LLMs) have accelerated progress toward artificial general intelligence, yet their potential to generate harmful content poses critical safety challenges. Existing alignment methods often struggle to cover diverse safety scenarios and remain vulnerable to adversarial attacks. In this work, we propose SAFER, a framework for Safety Alignment via eFficient Ex-Ante Reasoning. Our approach instantiates structured Ex-Ante reasoning through initial assessment, rule verification, and path calibration, and embeds predefined safety rules to provide transparent and verifiable safety judgments. Specifically, our approach consists of two training stages: (1) supervised fine-tuning with synthetic traces to teach the multi-stage Ex-Ante reasoning, and (2) step-level reasoning preference optimization to jointly enhance safety, utility, and efficiency. Experiments on multiple open-source LLMs demonstrate that SAFER significantly enhances safety performance while maintaining helpfulness and response efficiency.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedHal: An Evaluation Dataset for Medical Hallucination Detection</title>
<link>https://arxiv.org/abs/2504.08596</link>
<guid>https://arxiv.org/abs/2504.08596</guid>
<content:encoded><![CDATA[
arXiv:2504.08596v2 Announce Type: replace 
Abstract: We present MedHal, a novel large-scale dataset specifically designed to evaluate if models can detect hallucinations in medical texts. Current hallucination detection methods face significant limitations when applied to specialized domains like medicine, where they can have disastrous consequences. Existing medical datasets are either too small, containing only a few hundred samples, or focus on a single task like Question Answering or Natural Language Inference. MedHal addresses these gaps by: (1) incorporating diverse medical text sources and tasks; (2) providing a substantial volume of annotated samples suitable for training medical hallucination detection models; and (3) including explanations for factual inconsistencies to guide model learning. We demonstrate MedHal's utility by training and evaluating a baseline medical hallucination detection model, showing improvements over general-purpose hallucination detection approaches. This resource enables more efficient evaluation of medical text generation systems while reducing reliance on costly expert review, potentially accelerating the development of medical AI research.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring LLM Novelty As The Frontier Of Original And High-Quality Output</title>
<link>https://arxiv.org/abs/2504.09389</link>
<guid>https://arxiv.org/abs/2504.09389</guid>
<content:encoded><![CDATA[
arXiv:2504.09389v2 Announce Type: replace 
Abstract: As large language models (LLMs) are increasingly used for ideation and scientific discovery, it is important to evaluate their ability to generate novel output. Prior work evaluates novelty as originality with respect to model training data, but original outputs may be of low quality. In contrast, non-expert judges more reliably score quality but may favor memorized outputs, limiting the reliability of human preference as a metric. We introduce a new novelty metric for LLM generations that balances originality and quality -- the harmonic mean of the fraction of \ngrams unseen during training and a task-specific quality score. Using this framework, we identify trends that affect the novelty of generations from three families of open-data models (OLMo, OLMo-2, and Pythia) on three creative tasks: story completion, poetry writing, and creative tool use. We find that model-generated text from some base LLMs is less novel than human-written text from the internet. However, increasing model scale and post-training reliably improves novelty due to improvements in output quality. We also find that improving the base model at the same scale (\eg OLMo 7B to OLMo-2 7B) leads to higher novelty due to higher originality. Finally, we observe that inference-time methods, such as prompting and providing novel in-context examples, have a much smaller effect on novelty, often increasing originality at the expense of quality. This highlights the need for further research into more effective elicitation strategies as we use models for creative applications.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Mirage of Performance Gains: Why Contrastive Decoding Fails to Mitigate Object Hallucinations in MLLMs?</title>
<link>https://arxiv.org/abs/2504.10020</link>
<guid>https://arxiv.org/abs/2504.10020</guid>
<content:encoded><![CDATA[
arXiv:2504.10020v3 Announce Type: replace 
Abstract: Contrastive decoding strategies are widely used to reduce object hallucinations in multimodal large language models (MLLMs). These methods work by constructing contrastive samples to induce hallucinations and then suppressing them in the output distribution. However, this paper demonstrates that such approaches fail to effectively mitigate the hallucination problem. The performance improvements observed on POPE Benchmark are largely driven by two misleading factors: (1) crude, unidirectional adjustments to the model's output distribution and (2) the adaptive plausibility constraint, which reduces the sampling strategy to greedy search. To further illustrate these issues, we introduce a series of spurious improvement methods and evaluate their performance against contrastive decoding techniques. Experimental results reveal that the observed performance gains in contrastive decoding are entirely unrelated to its intended goal of mitigating hallucinations. Our findings challenge common assumptions about the effectiveness of contrastive decoding strategies and pave the way for developing genuinely effective solutions to hallucinations in MLLMs.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Document Cross-Lingual NLI via RST-Enhanced Graph Fusion and Interpretability Prediction</title>
<link>https://arxiv.org/abs/2504.12324</link>
<guid>https://arxiv.org/abs/2504.12324</guid>
<content:encoded><![CDATA[
arXiv:2504.12324v3 Announce Type: replace 
Abstract: Natural Language Inference (NLI) is a fundamental task in natural language processing. While NLI has developed many sub-directions such as sentence-level NLI, document-level NLI and cross-lingual NLI, Cross-Document Cross-Lingual NLI (CDCL-NLI) remains largely unexplored. In this paper, we propose a novel paradigm: CDCL-NLI, which extends traditional NLI capabilities to multi-document, multilingual scenarios. To support this task, we construct a high-quality CDCL-NLI dataset including 25,410 instances and spanning 26 languages. To address the limitations of previous methods on CDCL-NLI task, we further propose an innovative method that integrates RST-enhanced graph fusion with interpretability-aware prediction. Our approach leverages RST (Rhetorical Structure Theory) within heterogeneous graph neural networks for cross-document context modeling, and employs a structure-aware semantic alignment based on lexical chains for cross-lingual understanding. For NLI interpretability, we develop an EDU (Elementary Discourse Unit)-level attribution framework that produces extractive explanations. Extensive experiments demonstrate our approach's superior performance, achieving significant improvements over both conventional NLI models as well as large language models. Our work sheds light on the study of NLI and will bring research interest on cross-document cross-lingual context understanding, hallucination elimination and interpretability inference. Our code and datasets are available at "https://github.com/Leonardo123-ui/CDCL_NLI" for peer review.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Illusion or Algorithm? Investigating Memorization, Emergence, and Symbolic Processing in In-Context Learning</title>
<link>https://arxiv.org/abs/2505.11004</link>
<guid>https://arxiv.org/abs/2505.11004</guid>
<content:encoded><![CDATA[
arXiv:2505.11004v3 Announce Type: replace 
Abstract: Large-scale Transformer language models (LMs) trained solely on next-token prediction with web-scale data can solve a wide range of tasks after seeing just a few examples. The mechanism behind this capability, known as in-context learning (ICL), remains both controversial and poorly understood. Some studies argue that it is merely the result of memorizing vast amounts of data, while others contend that it reflects a fundamental, symbolic algorithmic development in LMs. In this work, we introduce a suite of investigative tasks and a novel method to systematically investigate ICL by leveraging the full Pythia scaling suite, including interim checkpoints that capture progressively larger amount of training data. By carefully exploring ICL performance on downstream tasks and simultaneously conducting a mechanistic analysis of the residual stream's subspace, we demonstrate that ICL extends beyond mere "memorization" of the training corpus, yet does not amount to the implementation of an independent symbolic algorithm. Our results also clarify several aspects of ICL, including the influence of training dynamics, model capabilities, and elements of mechanistic interpretability. Overall, our work advances the understanding of ICL and its implications, offering model developers insights into potential improvements and providing AI security practitioners with a basis for more informed guidelines.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Prompts Don't Say: Understanding and Managing Underspecification in LLM Prompts</title>
<link>https://arxiv.org/abs/2505.13360</link>
<guid>https://arxiv.org/abs/2505.13360</guid>
<content:encoded><![CDATA[
arXiv:2505.13360v2 Announce Type: replace 
Abstract: Prompt underspecification is a common challenge when interacting with LLMs. In this paper, we present an in-depth analysis of this problem, showing that while LLMs can often infer unspecified requirements by default (41.1%), such behavior is fragile: Under-specified prompts are 2x as likely to regress across model or prompt changes, sometimes with accuracy drops exceeding 20%. This instability makes it difficult to reliably build LLM applications. Moreover, simply specifying all requirements does not consistently help, as models have limited instruction-following ability and requirements can conflict. Standard prompt optimizers likewise provide little benefit. To address these issues, we propose requirements-aware prompt optimization mechanisms that improve performance by 4.8% on average over baselines. We further advocate for a systematic process of proactive requirements discovery, evaluation, and monitoring to better manage prompt underspecification in practice.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FAID: Fine-Grained AI-Generated Text Detection Using Multi-Task Auxiliary and Multi-Level Contrastive Learning</title>
<link>https://arxiv.org/abs/2505.14271</link>
<guid>https://arxiv.org/abs/2505.14271</guid>
<content:encoded><![CDATA[
arXiv:2505.14271v2 Announce Type: replace 
Abstract: The growing collaboration between humans and AI models in generative tasks has introduced new challenges in distinguishing between human-written, LLM-generated, and human--LLM collaborative texts. In this work, we collect a multilingual, multi-domain, multi-generator dataset FAIDSet. We further introduce a fine-grained detection framework FAID to classify text into these three categories, and also to identify the underlying LLM family of the generator. Unlike existing binary classifiers, FAID is built to capture both authorship and model-specific characteristics. Our method combines multi-level contrastive learning with multi-task auxiliary classification to learn subtle stylistic cues. By modeling LLM families as distinct stylistic entities, we incorporate an adaptation to address distributional shifts without retraining for unseen data. Our experimental results demonstrate that FAID outperforms several baselines, particularly enhancing the generalization accuracy on unseen domains and new LLMs, thus offering a potential solution for improving transparency and accountability in AI-assisted writing.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching Small Language Models to Learn Logic through Meta-Learning</title>
<link>https://arxiv.org/abs/2505.14313</link>
<guid>https://arxiv.org/abs/2505.14313</guid>
<content:encoded><![CDATA[
arXiv:2505.14313v2 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly evaluated on reasoning tasks, yet their logical abilities remain contested. To address this, we study LLMs' reasoning in a well-defined fragment of logic: syllogistic reasoning. We cast the problem as premise selection and construct controlled datasets to isolate logical competence. Beyond evaluation, an open challenge is enabling LLMs to acquire abstract inference patterns that generalize to novel structures. We propose to apply few-shot meta-learning to this domain, thereby encouraging models to extract rules across tasks rather than memorize patterns within tasks. Although meta-learning has been little explored in the context of logic learnability, our experiments show that it is effective: small models (1.5B-7B) fine-tuned with meta-learning demonstrate strong gains in generalization, with especially pronounced benefits in low-data regimes. These meta-learned models outperform GPT-4o and o3-mini on our syllogistic reasoning task.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unifying Inference-Time Planning Language Generation</title>
<link>https://arxiv.org/abs/2505.14763</link>
<guid>https://arxiv.org/abs/2505.14763</guid>
<content:encoded><![CDATA[
arXiv:2505.14763v2 Announce Type: replace 
Abstract: A line of work in planning uses LLM not to generate a plan, but to generate a formal representation in some planning language, which can be input into a symbolic solver to deterministically find a plan. While showing improved trust and promising performance, dozens of recent publications have proposed scattered methods on a variety of benchmarks under different experimental settings. We attempt to unify the inference-time LLM-as-formalizer methodology for classical planning by proposing a unifying framework based on intermediate representations. We thus systematically evaluate more than a dozen pipelines that subsume most existing work, while proposing novel ones that involve syntactically similar but high resource intermediate languages (such as a Python wrapper of PDDL). We provide recipes for planning language generation pipelines, draw a series of conclusions showing the efficacy of their various components, and evidence their robustness against problem complexity.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tracing Multilingual Factual Knowledge Acquisition in Pretraining</title>
<link>https://arxiv.org/abs/2505.14824</link>
<guid>https://arxiv.org/abs/2505.14824</guid>
<content:encoded><![CDATA[
arXiv:2505.14824v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are capable of recalling multilingual factual knowledge present in their pretraining data. However, most studies evaluate only the final model, leaving the development of factual recall and crosslingual consistency throughout pretraining largely unexplored. In this work, we trace how factual recall and crosslingual consistency evolve during pretraining, focusing on OLMo-7B as a case study. We find that both accuracy and consistency improve over time for most languages. We show that this improvement is primarily driven by the fact frequency in the pretraining corpus: more frequent facts are more likely to be recalled correctly, regardless of language. Yet, some low-frequency facts in non-English languages can still be correctly recalled. Our analysis reveals that these instances largely benefit from crosslingual transfer of their English counterparts -- an effect that emerges predominantly in the early stages of pretraining. We pinpoint two distinct pathways through which multilingual factual knowledge acquisition occurs: (1) frequency-driven learning, which is dominant and language-agnostic, and (2) crosslingual transfer, which is limited in scale and typically constrained to relation types involving named entities. We release our code and data to facilitate further research at https://github.com/cisnlp/multilingual-fact-tracing.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChartCards: A Chart-Metadata Generation Framework for Multi-Task Chart Understanding</title>
<link>https://arxiv.org/abs/2505.15046</link>
<guid>https://arxiv.org/abs/2505.15046</guid>
<content:encoded><![CDATA[
arXiv:2505.15046v3 Announce Type: replace 
Abstract: The emergence of Multi-modal Large Language Models (MLLMs) presents new opportunities for chart understanding. However, due to the fine-grained nature of these tasks, applying MLLMs typically requires large, high-quality datasets for task-specific fine-tuning, leading to high data collection and training costs. To address this, we propose ChartCards, a unified chart-metadata generation framework for multi-task chart understanding. ChartCards systematically synthesizes various chart information, including data tables, visualization code, visual elements, and multi-dimensional semantic captions. By structuring this information into organized metadata, ChartCards enables a single chart to support multiple downstream tasks, such as text-to-chart retrieval, chart summarization, chart-to-table conversion, chart description, and chart question answering. Using ChartCards, we further construct MetaChart, a large-scale high-quality dataset containing 10,862 data tables, 85K charts, and 170 K high-quality chart captions. We validate the dataset through qualitative crowdsourcing evaluations and quantitative fine-tuning experiments across various chart understanding tasks. Fine-tuning six different models on MetaChart resulted in an average performance improvement of 5% across all tasks. The most notable improvements are seen in text-to-chart retrieval and chart-to-table tasks, with Long-CLIP and Llama 3.2-11B achieving improvements of 17% and 28%, respectively.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AVerImaTeC: A Dataset for Automatic Verification of Image-Text Claims with Evidence from the Web</title>
<link>https://arxiv.org/abs/2505.17978</link>
<guid>https://arxiv.org/abs/2505.17978</guid>
<content:encoded><![CDATA[
arXiv:2505.17978v2 Announce Type: replace 
Abstract: Textual claims are often accompanied by images to enhance their credibility and spread on social media, but this also raises concerns about the spread of misinformation. Existing datasets for automated verification of image-text claims remain limited, as they often consist of synthetic claims and lack evidence annotations to capture the reasoning behind the verdict. In this work, we introduce AVerImaTeC, a dataset consisting of 1,297 real-world image-text claims. Each claim is annotated with question-answer (QA) pairs containing evidence from the web, reflecting a decomposed reasoning regarding the verdict. We mitigate common challenges in fact-checking datasets such as contextual dependence, temporal leakage, and evidence insufficiency, via claim normalization, temporally constrained evidence annotation, and a two-stage sufficiency check. We assess the consistency of the annotation in AVerImaTeC via inter-annotator studies, achieving a $\kappa=0.742$ on verdicts and $74.7\%$ consistency on QA pairs. We also propose a novel evaluation method for evidence retrieval and conduct extensive experiments to establish baselines for verifying image-text claims using open-web evidence.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Embarrassingly Simple Defense Against LLM Abliteration Attacks</title>
<link>https://arxiv.org/abs/2505.19056</link>
<guid>https://arxiv.org/abs/2505.19056</guid>
<content:encoded><![CDATA[
arXiv:2505.19056v2 Announce Type: replace 
Abstract: Large language models (LLMs) are typically aligned to refuse harmful instructions through safety fine-tuning. A recent attack, termed abliteration, identifies and suppresses the single latent direction most responsible for refusal behavior, thereby enabling models to generate harmful content. We propose a defense that fundamentally alters how models express refusal. We construct an extended-refusal dataset in which responses to harmful prompts provide detailed justifications before refusing, distributing the refusal signal across multiple token positions. Fine-tuning Llama-2-7B-Chat and Qwen2.5-Instruct (1.5B and 3B parameters) on this dataset yields models that maintain high refusal rates under abliteration: refusal rates drop by at most 10%, compared to 70-80% drops in baseline models. Comprehensive evaluations of safety and utility demonstrate that extended-refusal fine-tuning effectively neutralizes abliteration attacks while preserving general model performance and enhancing robustness across multiple alignment scenarios.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OWL: Probing Cross-Lingual Recall of Memorized Texts via World Literature</title>
<link>https://arxiv.org/abs/2505.22945</link>
<guid>https://arxiv.org/abs/2505.22945</guid>
<content:encoded><![CDATA[
arXiv:2505.22945v2 Announce Type: replace 
Abstract: Large language models (LLMs) are known to memorize and recall English text from their pretraining data. However, the extent to which this ability generalizes to non-English languages or transfers across languages remains unclear. This paper investigates multilingual and cross-lingual memorization in LLMs, probing if memorized content in one language (e.g., English) can be recalled when presented in translation. To do so, we introduce OWL, a dataset of 31.5K aligned excerpts from 20 books in ten languages, including English originals, official translations (Vietnamese, Spanish, Turkish), and new translations in six low-resource languages (Sesotho, Yoruba, Maithili, Malagasy, Setswana, Tahitian). We evaluate memorization across model families and sizes through three tasks: (1) direct probing, which asks the model to identify a book's title and author; (2) name cloze, which requires predicting masked character names; and (3) prefix probing, which involves generating continuations. We find that LLMs consistently recall content across languages, even for texts without direct translation in pretraining data. GPT-4o, for example, identifies authors and titles 69% of the time and masked entities 6% of the time in newly translated excerpts. Perturbations (e.g., masking characters, shuffling words) modestly reduce direct probing accuracy (7% drop for shuffled official translations). Our results highlight the extent of cross-lingual memorization and provide insights on the differences between the models.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ExpertLongBench: Benchmarking Language Models on Expert-Level Long-Form Generation Tasks with Structured Checklists</title>
<link>https://arxiv.org/abs/2506.01241</link>
<guid>https://arxiv.org/abs/2506.01241</guid>
<content:encoded><![CDATA[
arXiv:2506.01241v3 Announce Type: replace 
Abstract: This paper introduces ExpertLongBench, an expert-level benchmark containing 11 tasks from 9 domains that reflect realistic expert workflows and applications. Beyond question answering, the application-driven tasks in ExpertLongBench demand long-form outputs that can exceed 5,000 tokens and strict adherence to domain-specific requirements. Notably, each task in ExpertLongBench includes a rubric, designed or validated by domain experts, to specify task requirements and guide output evaluation. Furthermore, we propose CLEAR, an evaluation framework that supports accurate evaluation of long-form model outputs in our benchmark. To achieve fine-grained, expert-aligned evaluation, CLEAR derives checklists from both model outputs and references by extracting information corresponding to items in the task-specific rubric. Checklist items of model outputs are then compared with corresponding items of reference outputs to assess their correctness, enabling grounded evaluation. We benchmark 13 popular large language models (LLMs) and analyze components in CLEAR, showing that (1) existing LLMs, with the top performer Gemini-2.5-Pro achieving only a 33.4 F1 score, require significant improvement for expert-level tasks; (2) models can generate content corresponding to the required aspects, but far from correct; and (3) accurate checklist extraction and comparison in CLEAR can be achieved by open-weight models for more scalable, reproducible, and low-cost usage.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trajectory Prediction Meets Large Language Models: A Survey</title>
<link>https://arxiv.org/abs/2506.03408</link>
<guid>https://arxiv.org/abs/2506.03408</guid>
<content:encoded><![CDATA[
arXiv:2506.03408v2 Announce Type: replace 
Abstract: Recent advances in large language models (LLMs) have sparked growing interest in integrating language-driven techniques into trajectory prediction. By leveraging their semantic and reasoning capabilities, LLMs are reshaping how autonomous systems perceive, model, and predict trajectories. This survey provides a comprehensive overview of this emerging field, categorizing recent work into five directions: (1) Trajectory prediction via language modeling paradigms, (2) Direct trajectory prediction with pretrained language models, (3) Language-guided scene understanding for trajectory prediction, (4) Language-driven data generation for trajectory prediction, (5) Language-based reasoning and interpretability for trajectory prediction. For each, we analyze representative methods, highlight core design choices, and identify open challenges. This survey bridges natural language processing and trajectory prediction, offering a unified perspective on how language can enrich trajectory prediction.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When to use Graphs in RAG: A Comprehensive Analysis for Graph Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2506.05690</link>
<guid>https://arxiv.org/abs/2506.05690</guid>
<content:encoded><![CDATA[
arXiv:2506.05690v2 Announce Type: replace 
Abstract: Graph retrieval-augmented generation (GraphRAG) has emerged as a powerful paradigm for enhancing large language models (LLMs) with external knowledge. It leverages graphs to model the hierarchical structure between specific concepts, enabling more coherent and effective knowledge retrieval for accurate reasoning.Despite its conceptual promise, recent studies report that GraphRAG frequently underperforms vanilla RAG on many real-world tasks. This raises a critical question: Is GraphRAG really effective, and in which scenarios do graph structures provide measurable benefits for RAG systems? To address this, we propose GraphRAG-Bench, a comprehensive benchmark designed to evaluate GraphRAG models onboth hierarchical knowledge retrieval and deep contextual reasoning. GraphRAG-Bench features a comprehensive dataset with tasks of increasing difficulty, coveringfact retrieval, complex reasoning, contextual summarization, and creative generation, and a systematic evaluation across the entire pipeline, from graph constructionand knowledge retrieval to final generation. Leveraging this novel benchmark, we systematically investigate the conditions when GraphRAG surpasses traditional RAG and the underlying reasons for its success, offering guidelines for its practical application. All related resources and analyses are collected for the community at https://github.com/GraphRAG-Bench/GraphRAG-Benchmark.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compound AI Systems Optimization: A Survey of Methods, Challenges, and Future Directions</title>
<link>https://arxiv.org/abs/2506.08234</link>
<guid>https://arxiv.org/abs/2506.08234</guid>
<content:encoded><![CDATA[
arXiv:2506.08234v2 Announce Type: replace 
Abstract: Recent advancements in large language models (LLMs) and AI systems have led to a paradigm shift in the design and optimization of complex AI workflows. By integrating multiple components, compound AI systems have become increasingly adept at performing sophisticated tasks. However, as these systems grow in complexity, new challenges arise in optimizing not only individual components but also their interactions. While traditional optimization methods such as supervised fine-tuning (SFT) and reinforcement learning (RL) remain foundational, the rise of natural language feedback introduces promising new approaches, especially for optimizing non-differentiable systems. This paper provides a systematic review of recent progress in optimizing compound AI systems, encompassing both numerical and language-based techniques. We formalize the notion of compound AI system optimization, classify existing methods along several key dimensions, and highlight open research challenges and future directions in this rapidly evolving field. A list of surveyed papers is publicly available at https://github.com/MiuLab/AISysOpt-Survey.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context Biasing for Pronunciations-Orthography Mismatch in Automatic Speech Recognition</title>
<link>https://arxiv.org/abs/2506.18703</link>
<guid>https://arxiv.org/abs/2506.18703</guid>
<content:encoded><![CDATA[
arXiv:2506.18703v2 Announce Type: replace 
Abstract: Neural sequence-to-sequence systems deliver state-of-the-art performance for automatic speech recognition. When using appropriate modeling units, e.g., byte-pair encoded characters, these systems are in principal open vocabulary systems. In practice, however, they often fail to recognize words not seen during training, e.g., named entities, acronyms, or domain-specific special words. To address this problem, many context biasing methods have been proposed; however, for words with a pronunciation-orthography mismatch, these methods may still struggle. We propose a method which allows corrections of substitution errors to improve the recognition accuracy of such challenging words. Users can add corrections on the fly during inference. We show that with this method we get a relative improvement in biased word error rate of up to 8%, while maintaining a competitive overall word error rate.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models</title>
<link>https://arxiv.org/abs/2507.12428</link>
<guid>https://arxiv.org/abs/2507.12428</guid>
<content:encoded><![CDATA[
arXiv:2507.12428v2 Announce Type: replace 
Abstract: Reasoning language models improve performance on complex tasks by generating long chains of thought (CoTs), but this process can also increase harmful outputs in adversarial settings. In this work, we ask whether the long CoTs can be leveraged for predictive safety monitoring: do the reasoning traces provide early signals of final response alignment that could enable timely intervention? We evaluate a range of monitoring methods using either CoT text or activations, including highly capable large language models, fine-tuned classifiers, and humans. First, we find that a simple linear probe trained on CoT activations significantly outperforms all text-based baselines in predicting whether a final response is safe or unsafe, with an average absolute increase of 13 in F1 scores over the best-performing alternatives. CoT texts are often unfaithful and misleading, while model latents provide a more reliable predictive signal. Second, the probe can be applied to early CoT segments before the response is generated, showing that alignment signals appear before reasoning completes. Error analysis reveals that the performance gap between text classifiers and the linear probe largely stems from a subset of responses we call performative CoTs, where the reasoning consistently contradicts the final response as the CoT progresses. Our findings generalize across model sizes, families, and safety benchmarks, suggesting that lightweight probes could enable real-time safety monitoring and early intervention during generation.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intent-Aware Schema Generation And Refinement For Literature Review Tables</title>
<link>https://arxiv.org/abs/2507.19521</link>
<guid>https://arxiv.org/abs/2507.19521</guid>
<content:encoded><![CDATA[
arXiv:2507.19521v2 Announce Type: replace 
Abstract: The increasing volume of academic literature makes it essential for researchers to organize, compare, and contrast collections of documents. Large language models (LLMs) can support this process by generating schemas defining shared aspects along which to compare papers. However, progress on schema generation has been slow due to: (i) ambiguity in reference-based evaluations, and (ii) lack of editing/refinement methods. Our work is the first to address both issues. First, we present an approach for augmenting unannotated table corpora with \emph{synthesized intents}, and apply it to create a dataset for studying schema generation conditioned on a given information need, thus reducing ambiguity. With this dataset, we show how incorporating table intents significantly improves baseline performance in reconstructing reference schemas. We start by comprehensively benchmarking several single-shot schema generation methods, including prompted LLM workflows and fine-tuned models, showing that smaller, open-weight models can be fine-tuned to be competitive with state-of-the-art prompted LLMs. Next, we propose several LLM-based schema refinement techniques and show that these can further improve schemas generated by these methods.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Locally Deployable Fine-Tuned Causal Large Language Models for Mode Choice Behaviour</title>
<link>https://arxiv.org/abs/2507.21432</link>
<guid>https://arxiv.org/abs/2507.21432</guid>
<content:encoded><![CDATA[
arXiv:2507.21432v2 Announce Type: replace 
Abstract: This study investigates the adoption of open-access, locally deployable causal large language models (LLMs) for travel mode choice prediction and introduces LiTransMC, the first fine-tuned causal LLM developed for this task. We systematically benchmark eleven open-access LLMs (1-12B parameters) across three stated and revealed preference datasets, testing 396 configurations and generating over 79,000 mode choice decisions. Beyond predictive accuracy, we evaluate models generated reasoning using BERTopic for topic modelling and a novel Explanation Strength Index, providing the first structured analysis of how LLMs articulate decision factors in alignment with behavioural theory. LiTransMC, fine-tuned using parameter efficient and loss masking strategy, achieved a weighted F1 score of 0.6845 and a Jensen-Shannon Divergence of 0.000245, surpassing both untuned local models and larger proprietary systems, including GPT-4o with advanced persona inference and embedding-based loading, while also outperforming classical mode choice methods such as discrete choice models and machine learning classifiers for the same dataset. This dual improvement, i.e., high instant-level accuracy and near-perfect distributional calibration, demonstrates the feasibility of creating specialist, locally deployable LLMs that integrate prediction and interpretability. Through combining structured behavioural prediction with natural language reasoning, this work unlocks the potential for conversational, multi-task transport models capable of supporting agent-based simulations, policy testing, and behavioural insight generation. These findings establish a pathway for transforming general purpose LLMs into specialized and explainable tools for transportation research and policy formulation, while maintaining privacy, reducing cost, and broadening access through local deployment.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLiDRE: Generalist Lightweight model for Document-level Relation Extraction</title>
<link>https://arxiv.org/abs/2508.00757</link>
<guid>https://arxiv.org/abs/2508.00757</guid>
<content:encoded><![CDATA[
arXiv:2508.00757v2 Announce Type: replace 
Abstract: Relation Extraction (RE) is a fundamental task in Natural Language Processing, and its document-level variant poses significant challenges, due to complex interactions between entities across sentences. While supervised models have achieved strong results in fully resourced settings, their behavior with limited training data remains insufficiently studied. We introduce GLiDRE, a new compact model for document-level relation extraction, designed to work efficiently in both supervised and few-shot settings. Experiments in both low-resource supervised training and few-shot meta-learning benchmarks show that our approach outperforms existing methods in data-constrained scenarios, establishing a new state-of-the-art in few-shot document-level relation extraction. Our code will be publicly available.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Language Models with Real-time Knowledge Editing</title>
<link>https://arxiv.org/abs/2508.01302</link>
<guid>https://arxiv.org/abs/2508.01302</guid>
<content:encoded><![CDATA[
arXiv:2508.01302v2 Announce Type: replace 
Abstract: Knowledge editing aims to modify outdated knowledge in large language models (LLMs) efficiently while retaining their original capabilities. Mainstream benchmarks for knowledge editing are predominantly static and fail to keep in pace with the evolving real-world knowledge. In this work, we introduce CRAFT, an ever-evolving real-world benchmark for knowledge editing. It features well-designed paired edits for composite reasoning, and evaluates models on alias portability as well as temporal and common-sense locality, making it a challenging knowledge editing benchmark on which previous knowledge editing methods hardly achieve balanced performance. Towards flexible real-time editing, we propose KEDAS, a novel paradigm of knowledge editing alignment featuring diverse edit augmentation and self-adaptive post-alignment inference, which exhibits significant performance gain on CRAFT compared to previous methods. All of our code and data are available at https://anonymous.4open.science/r/CRAFT-KEDAS.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert Redundancy Analysis</title>
<link>https://arxiv.org/abs/2508.02322</link>
<guid>https://arxiv.org/abs/2508.02322</guid>
<content:encoded><![CDATA[
arXiv:2508.02322v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) with Mixture-of-Experts (MoE) architectures are distinguished by their strong performance scaling with increasing parameters across a wide range of tasks, yet they also suffer from substantial computational and storage overheads. Notably, the performance gains of MoE models do not scale proportionally with the growth in expert parameters. While prior works attempt to reduce parameters via expert-level pruning, merging, or decomposition, they still suffer from challenges in both performance and computational efficiency. In this paper, we address these challenges by introducing micro-expert as a finer-grained compression unit that spans across matrices. We first establish a more fundamental perspective, viewing MoE layers as mixtures of micro-experts, and present CAMERA, a lightweight and training-free framework for identifying micro-expert redundancy. Our analysis uncovers significant variance in micro-expert contributions during decoding. Based on this insight, we further propose CAMERA-P, a structured micro-expert pruning framework, and CAMERA-Q, a mixed-precision quantization idea designed for micro-experts. Extensive experiments on nine downstream tasks show that CAMERA-P consistently outperforms strong baselines under pruning ratios ranging from 20% to 60%. Furthermore, CAMERA-Q achieves superior results under aggressive 2-bit quantization, surpassing existing matrix- and channel-level ideas. Notably, our method enables complete micro-expert analysis of Qwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RooseBERT: A New Deal For Political Language Modelling</title>
<link>https://arxiv.org/abs/2508.03250</link>
<guid>https://arxiv.org/abs/2508.03250</guid>
<content:encoded><![CDATA[
arXiv:2508.03250v2 Announce Type: replace 
Abstract: The increasing amount of political debates and politics-related discussions calls for the definition of novel computational methods to automatically analyse such content with the final goal of lightening up political deliberation to citizens. However, the specificity of the political language and the argumentative form of these debates (employing hidden communication strategies and leveraging implicit arguments) make this task very challenging, even for current general-purpose pre-trained Language Models. To address this issue, we introduce a novel pre-trained Language Model for political discourse language called RooseBERT. Pre-training a language model on a specialised domain presents different technical and linguistic challenges, requiring extensive computational resources and large-scale data. RooseBERT has been trained on large political debate and speech corpora (8K debates, each composed of several sub-debates on different topics) in English. To evaluate its performances, we fine-tuned it on four downstream tasks related to political debate analysis, i.e., stance detection, sentiment analysis, argument component detection and classification, and argument relation prediction and classification. Our results demonstrate significant improvements over general-purpose Language Models on these four tasks, highlighting how domain-specific pre-training enhances performance in political debate analysis. We release RooseBERT for the research community.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Unlearning Without an Expert Curated Dataset</title>
<link>https://arxiv.org/abs/2508.06595</link>
<guid>https://arxiv.org/abs/2508.06595</guid>
<content:encoded><![CDATA[
arXiv:2508.06595v3 Announce Type: replace 
Abstract: Modern large language models often encode sensitive, harmful, or copyrighted knowledge, raising the need for post-hoc unlearning-the ability to remove specific domains of knowledge from a model without full retraining. A major bottleneck in current unlearning pipelines is constructing effective forget sets-datasets that approximate the target domain and guide the model to forget it. In this work, we introduce a scalable, automated approach to generate high-quality forget sets using language models themselves. Our method synthesizes textbook-style data through a structured prompting pipeline, requiring only a domain name as input. Through experiments on unlearning biosecurity, cybersecurity, and Harry Potter novels, we show that our synthetic datasets consistently outperform the baseline synthetic alternatives and are comparable to the expert-curated ones. Additionally, ablation studies reveal that the multi-step generation pipeline significantly boosts data diversity, which in turn improves unlearning utility. Overall, our findings suggest that synthetic datasets offer a promising path toward practical, scalable unlearning for a wide range of emerging domains without the need for manual intervention. We release our code and dataset at https://github.com/xyzhu123/Synthetic_Textbook.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair Play in the Newsroom: Actor-Based Filtering Gender Discrimination in Text Corpora</title>
<link>https://arxiv.org/abs/2508.13169</link>
<guid>https://arxiv.org/abs/2508.13169</guid>
<content:encoded><![CDATA[
arXiv:2508.13169v2 Announce Type: replace 
Abstract: Language corpora are the foundation of most natural language processing research, yet they often reproduce structural inequalities. One such inequality is gender discrimination in how actors are represented, which can distort analyses and perpetuate discriminatory outcomes. This paper introduces a user-centric, actor-level pipeline for detecting and mitigating gender discrimination in large-scale text corpora. By combining discourse-aware analysis with metrics for sentiment, syntactic agency, and quotation styles, our method enables both fine-grained auditing and exclusion-based balancing. Applied to the taz2024full corpus of German newspaper articles (1980-2024), the pipeline yields a more gender-balanced dataset while preserving core dynamics of the source material. Our findings show that structural asymmetries can be reduced through systematic filtering, though subtler biases in sentiment and framing remain. We release the tools and reports to support further research in discourse-based fairness auditing and equitable corpus construction.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation</title>
<link>https://arxiv.org/abs/2508.14146</link>
<guid>https://arxiv.org/abs/2508.14146</guid>
<content:encoded><![CDATA[
arXiv:2508.14146v3 Announce Type: replace 
Abstract: With the rapid growth of academic publications, peer review has become an essential yet time-consuming responsibility within the research community. Large Language Models (LLMs) have increasingly been adopted to assist in the generation of review comments; however, current LLM-based review tasks lack a unified evaluation benchmark to rigorously assess the models' ability to produce comprehensive, accurate, and human-aligned assessments, particularly in scenarios involving multimodal content such as figures and tables. To address this gap, we propose \textbf{MMReview}, a comprehensive benchmark that spans multiple disciplines and modalities. MMReview includes multimodal content and expert-written review comments for 240 papers across 17 research domains within four major academic disciplines: Artificial Intelligence, Natural Sciences, Engineering Sciences, and Social Sciences. We design a total of 13 tasks grouped into four core categories, aimed at evaluating the performance of LLMs and Multimodal LLMs (MLLMs) in step-wise review generation, outcome formulation, alignment with human preferences, and robustness to adversarial input manipulation. Extensive experiments conducted on 16 open-source models and 5 advanced closed-source models demonstrate the thoroughness of the benchmark. We envision MMReview as a critical step toward establishing a standardized foundation for the development of automated peer review systems.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Culture Gap: A Framework for LLM-Driven Socio-Cultural Localization of Math Word Problems in Low-Resource Languages</title>
<link>https://arxiv.org/abs/2508.14913</link>
<guid>https://arxiv.org/abs/2508.14913</guid>
<content:encoded><![CDATA[
arXiv:2508.14913v3 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated significant capabilities in solving mathematical problems expressed in natural language. However, multilingual and culturally-grounded mathematical reasoning in low-resource languages lags behind English due to the scarcity of socio-cultural task datasets that reflect accurate native entities such as person names, organization names, and currencies. Existing multilingual benchmarks are predominantly produced via translation and typically retain English-centric entities, owing to the high cost associated with human annotater-based localization. Moreover, automated localization tools are limited, and hence, truly localized datasets remain scarce. To bridge this gap, we introduce a framework for LLM-driven cultural localization of math word problems that automatically constructs datasets with native names, organizations, and currencies from existing sources. We find that translated benchmarks can obscure true multilingual math ability under appropriate socio-cultural contexts. Through extensive experiments, we also show that our framework can help mitigate English-centric entity bias and improves robustness when native entities are introduced across various languages.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Interfaces for Language Models</title>
<link>https://arxiv.org/abs/2508.19227</link>
<guid>https://arxiv.org/abs/2508.19227</guid>
<content:encoded><![CDATA[
arXiv:2508.19227v2 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly seen as assistants, copilots, and consultants, capable of supporting a wide range of tasks through natural conversation. However, most systems remain constrained by a linear request-response format that often makes interactions inefficient in multi-turn, information-dense, and exploratory tasks. To address these limitations, we propose Generative Interfaces for Language Models, a paradigm in which LLMs respond to user queries by proactively generating user interfaces (UIs) that enable more adaptive and interactive engagement. Our framework leverages structured interface-specific representations and iterative refinements to translate user queries into task-specific UIs. For systematic evaluation, we introduce a multidimensional assessment framework that compares generative interfaces with traditional chat-based ones across diverse tasks, interaction patterns, and query types, capturing functional, interactive, and emotional aspects of user experience. Results show that generative interfaces consistently outperform conversational ones, with up to a 72% improvement in human preference. These findings clarify when and why users favor generative interfaces, paving the way for future advancements in human-AI interaction.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speech-Based Cognitive Screening: A Systematic Evaluation of LLM Adaptation Strategies</title>
<link>https://arxiv.org/abs/2509.03525</link>
<guid>https://arxiv.org/abs/2509.03525</guid>
<content:encoded><![CDATA[
arXiv:2509.03525v2 Announce Type: replace 
Abstract: Over half of US adults with Alzheimer disease and related dementias remain undiagnosed, and speech-based screening offers a scalable detection approach. We compared large language model adaptation strategies for dementia detection using the DementiaBank speech corpus, evaluating nine text-only models and three multimodal audio-text models on recordings from DementiaBank speech corpus. Adaptations included in-context learning with different demonstration selection policies, reasoning-augmented prompting, parameter-efficient fine-tuning, and multimodal integration. Results showed that class-centroid demonstrations achieved the highest in-context learning performance, reasoning improved smaller models, and token-level fine-tuning generally produced the best scores. Adding a classification head substantially improved underperforming models. Among multimodal models, fine-tuned audio-text systems performed well but did not surpass the top text-only models. These findings highlight that model adaptation strategies, including demonstration selection, reasoning design, and tuning method, critically influence speech-based dementia detection, and that properly adapted open-weight models can match or exceed commercial systems.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgenticIE: An Adaptive Agent for Information Extraction from Complex Regulatory Documents</title>
<link>https://arxiv.org/abs/2509.11773</link>
<guid>https://arxiv.org/abs/2509.11773</guid>
<content:encoded><![CDATA[
arXiv:2509.11773v2 Announce Type: replace 
Abstract: Declaration of Performance (DoP) documents, mandated by EU regulation, certify the performance of construction products. There are two challenges to make DoPs machine and human accessible through automated key-value pair extraction (KVP) and question answering (QA): (1) While some of their content is standardized, DoPs vary widely in layout, schema, and format; (2) Both users and documents are multilingual. Existing static or LLM-only Information Extraction (IE) pipelines fail to adapt to this structural document and user diversity. Our domain-specific, agentic system addresses these challenges through a planner-executor-responder architecture. The system infers user intent, detects document language and modality, and orchestrates tools dynamically for robust, traceable reasoning while avoiding tool misuse or execution loops. Our agent outperforms baselines (ROUGE: 0.783 vs. 0.703/0.608) with better cross-lingual stability (17-point vs. 21-26-point variation).
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research</title>
<link>https://arxiv.org/abs/2509.13312</link>
<guid>https://arxiv.org/abs/2509.13312</guid>
<content:encoded><![CDATA[
arXiv:2509.13312v3 Announce Type: replace 
Abstract: This paper tackles \textbf{open-ended deep research (OEDR)}, a complex challenge where AI agents must synthesize vast web-scale information into insightful reports. Current approaches are plagued by dual-fold limitations: static research pipelines that decouple planning from evidence acquisition and monolithic generation paradigms that include redundant, irrelevant evidence, suffering from hallucination issues and low citation accuracy. To address these challenges, we introduce \textbf{WebWeaver}, a novel dual-agent framework that emulates the human research process. The planner operates in a dynamic cycle, iteratively interleaving evidence acquisition with outline optimization to produce a comprehensive, citation-grounded outline linking to a memory bank of evidence. The writer then executes a hierarchical retrieval and writing process, composing the report section by section. By performing targeted retrieval of only the necessary evidence from the memory bank via citations for each part, it effectively mitigates long-context issues and citation hallucinations. Our framework establishes a new state-of-the-art across major OEDR benchmarks, including DeepResearch Bench, DeepConsult, and DeepResearchGym. These results validate our human-centric, iterative methodology, demonstrating that adaptive planning and focused synthesis are crucial for producing comprehensive, trusted, and well-structured reports.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-JEPA: Large Language Models Meet Joint Embedding Predictive Architectures</title>
<link>https://arxiv.org/abs/2509.14252</link>
<guid>https://arxiv.org/abs/2509.14252</guid>
<content:encoded><![CDATA[
arXiv:2509.14252v2 Announce Type: replace 
Abstract: Large Language Model (LLM) pretraining, finetuning, and evaluation rely on input-space reconstruction and generative capabilities. Yet, it has been observed in vision that embedding-space training objectives, e.g., with Joint Embedding Predictive Architectures (JEPAs), are far superior to their input-space counterpart. That mismatch in how training is achieved between language and vision opens up a natural question: {\em can language training methods learn a few tricks from the vision ones?} The lack of JEPA-style LLM is a testimony of the challenge in designing such objectives for language. In this work, we propose a first step in that direction where we develop LLM-JEPA, a JEPA based solution for LLMs applicable both to finetuning and pretraining. Thus far, LLM-JEPA is able to outperform the standard LLM training objectives by a significant margin across models, all while being robust to overfiting. Those findings are observed across numerous datasets (NL-RX, GSM8K, Spider, RottenTomatoes) and various models from the Llama3, OpenELM, Gemma2 and Olmo families. Code: https://github.com/rbalestr-lab/llm-jepa.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to vary: Teaching LMs to reproduce human linguistic variability in next-word prediction</title>
<link>https://arxiv.org/abs/2509.17794</link>
<guid>https://arxiv.org/abs/2509.17794</guid>
<content:encoded><![CDATA[
arXiv:2509.17794v2 Announce Type: replace 
Abstract: Natural language generation (NLG) tasks are often subject to inherent variability; e.g. predicting the next word given a context has multiple valid responses, evident when asking multiple humans to complete the task. While having language models (LMs) that are aligned pluralistically, so that they are able to reproduce well the inherent diversity in perspectives of an entire population of interest is clearly beneficial, Ilia and Aziz (2024) show that LMs do not reproduce this type of linguistic variability well. They speculate this inability might stem from the lack of consistent training of LMs with data reflecting this type of inherent variability. As such, we investigate whether training LMs on multiple plausible word continuations per context can improve their ability to reproduce human linguistic variability for next-word prediction. We employ fine-tuning techniques for pre-trained and instruction-tuned models; and demonstrate their potential when fine-tuning GPT-2 and Mistral-7B-IT, using Provo Corpus. Our evaluation, which measures divergence among empirically estimated human and model next-word distributions across contexts before and after fine-tuning, shows that our multi-label fine-tuning improves the LMs' ability to reproduce linguistic variability; both for contexts that admit higher and lower variability.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diagnosing the Performance Trade-off in Moral Alignment: A Case Study on Gender Stereotypes</title>
<link>https://arxiv.org/abs/2509.21456</link>
<guid>https://arxiv.org/abs/2509.21456</guid>
<content:encoded><![CDATA[
arXiv:2509.21456v2 Announce Type: replace 
Abstract: Moral alignment has emerged as a widely adopted approach for regulating the behavior of pretrained language models (PLMs), typically through fine-tuning on curated datasets. Gender stereotype mitigation is a representational task within the broader application of moral alignment. However, this process often comes at the cost of degraded downstream task performance. Prior studies commonly aim to achieve a performance trade-off by encouraging PLMs to selectively forget only stereotypical knowledge through carefully designed fairness objective, while preserving their language modeling capability (overall forgetting). In this short paper, we investigate whether the performance trade-off can be achieved through the lens of forgetting and the fairness objective. Our analysis shows that the large datasets needed for satisfactory fairness highlight the limitations of current fairness objectives in achieving an effective trade-off: (1) downstream task performance is strongly correlated with overall forgetting; (2) selective forgetting reduces stereotypes, but overall forgetting increases. and (3) general solutions for alleviating forgetting are ineffective at reducing the overall forgetting and fail to improve downstream task performance.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative transformations and patterns in LLM-native approaches for software verification and falsification</title>
<link>https://arxiv.org/abs/2404.09384</link>
<guid>https://arxiv.org/abs/2404.09384</guid>
<content:encoded><![CDATA[
arXiv:2404.09384v3 Announce Type: replace-cross 
Abstract: The emergence of prompting as the dominant paradigm for leveraging Large Language Models (LLMs) has led to a proliferation of LLM-native software, where application behavior arises from complex, stochastic data transformations. However, the engineering of such systems remains largely exploratory and ad-hoc, hampered by the absence of conceptual frameworks, ex-ante methodologies, design guidelines, and specialized benchmarks. We argue that a foundational step towards a more disciplined engineering practice is a systematic understanding of the core functional units--generative transformations--and their compositional patterns within LLM-native applications.
  Focusing on the rich domain of software verification and falsification, we conduct a secondary study of over 100 research proposals to address this gap. We first present a fine-grained taxonomy of generative transformations, abstracting prompt-based interactions into conceptual signatures. This taxonomy serves as a scaffolding to identify recurrent transformation relationship patterns--analogous to software design patterns--that characterize solution approaches in the literature. Our analysis not only validates the utility of the taxonomy but also surfaces strategic gaps and cross-dimensional relationships, offering a structured foundation for future research in modular and compositional LLM application design, benchmarking, and the development of reliable LLM-native systems.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Potential of Conversational AI Support for Agent-Based Social Simulation Model Design</title>
<link>https://arxiv.org/abs/2405.08032</link>
<guid>https://arxiv.org/abs/2405.08032</guid>
<content:encoded><![CDATA[
arXiv:2405.08032v2 Announce Type: replace-cross 
Abstract: ChatGPT, the AI-powered chatbot with a massive user base of hundreds of millions, has become a global phenomenon. However, the use of Conversational AI Systems (CAISs) like ChatGPT for research in the field of Social Simulation is still limited. Specifically, there is no evidence of its usage in Agent-Based Social Simulation (ABSS) model design. This paper takes a crucial first step toward exploring the untapped potential of this emerging technology in the context of ABSS model design. The research presented here demonstrates how CAISs can facilitate the development of innovative conceptual ABSS models in a concise timeframe and with minimal required upfront case-based knowledge. By employing advanced prompt engineering techniques and adhering to the Engineering ABSS framework, we have constructed a comprehensive prompt script that enables the design of conceptual ABSS models with or by the CAIS. A proof-of-concept application of the prompt script, used to generate the conceptual ABSS model for a case study on the impact of adaptive architecture in a museum environment, illustrates the practicality of the approach. Despite occasional inaccuracies and conversational divergence, the CAIS proved to be a valuable companion for ABSS modellers.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Grained and Thematic Evaluation of LLMs in Social Deduction Game</title>
<link>https://arxiv.org/abs/2408.09946</link>
<guid>https://arxiv.org/abs/2408.09946</guid>
<content:encoded><![CDATA[
arXiv:2408.09946v3 Announce Type: replace-cross 
Abstract: Recent studies have investigated whether large language models (LLMs) can support obscured communication, which is characterized by core aspects such as inferring subtext and evading suspicions. To conduct the investigation, researchers have used social deduction games (SDGs) as their experimental environment, in which players conceal and infer specific information. However, prior work has often overlooked how LLMs should be evaluated in such settings. Specifically, we point out two limitations with the evaluation methods they employed. First, metrics used in prior studies are coarse-grained as they are based on overall game outcomes that often fail to capture event-level behaviors; Second, error analyses have lacked structured methodologies capable of producing insights that meaningfully support evaluation outcomes. To address these limitations, we propose a microscopic and systematic approach to the investigation. Specifically, we introduce six fine-grained metrics that resolve the first issue. To tackle the second issue, we conducted a thematic analysis and identified four major reasoning failures that undermine LLMs' performance in obscured communication.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Reliable are Causal Probing Interventions?</title>
<link>https://arxiv.org/abs/2408.15510</link>
<guid>https://arxiv.org/abs/2408.15510</guid>
<content:encoded><![CDATA[
arXiv:2408.15510v4 Announce Type: replace-cross 
Abstract: Causal probing aims to analyze foundation models by examining how intervening on their representation of various latent properties impacts their outputs. Recent works have cast doubt on the theoretical basis of several leading causal probing methods, but it has been unclear how to systematically evaluate the effectiveness of these methods in practice. To address this, we define two key causal probing desiderata: completeness (how thoroughly the representation of the target property has been transformed) and selectivity (how little non-targeted properties have been impacted). We find that there is an inherent tradeoff between the two, which we define as reliability, their harmonic mean. We introduce an empirical analysis framework to measure and evaluate these quantities, allowing us to make the first direct comparisons between different families of leading causal probing methods (e.g., linear vs. nonlinear, or concept removal vs. counterfactual interventions). We find that: (1) all methods show a clear tradeoff between completeness and selectivity; (2) more complete and reliable methods have a greater impact on LLM behavior; and (3) nonlinear interventions are almost always more reliable than linear interventions.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hallucination Detox: Sensitivity Dropout (SenD) for Large Language Model Training</title>
<link>https://arxiv.org/abs/2410.15460</link>
<guid>https://arxiv.org/abs/2410.15460</guid>
<content:encoded><![CDATA[
arXiv:2410.15460v5 Announce Type: replace-cross 
Abstract: As large language models (LLMs) become increasingly prevalent, concerns about their reliability, particularly due to hallucinations - factually inaccurate or irrelevant outputs - have grown. Our research investigates the relationship between the uncertainty in training dynamics and the emergence of hallucinations. Using models from the Pythia suite and several hallucination detection metrics, we analyze hallucination trends and identify significant variance during training. To address this, we propose Sensitivity Dropout (SenD), a novel training protocol designed to reduce hallucination variance during training by deterministically dropping embedding indices with significant variability. In addition, we develop an unsupervised hallucination detection metric, Efficient EigenScore (EES), which approximates the traditional EigenScore in 2x speed. This metric is integrated into our training protocol, allowing SenD to be both computationally scalable and effective at reducing hallucination variance. SenD improves test-time reliability of Pythia and Meta's Llama models by up to 17% and enhances factual accuracy in Wikipedia, Medical, Legal, and Coding domains without affecting downstream task performance.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BenchAgents: Multi-Agent Systems for Structured Benchmark Creation</title>
<link>https://arxiv.org/abs/2410.22584</link>
<guid>https://arxiv.org/abs/2410.22584</guid>
<content:encoded><![CDATA[
arXiv:2410.22584v2 Announce Type: replace-cross 
Abstract: Evaluation insights are limited by the availability of high-quality benchmarks. As models evolve, there is a need to create benchmarks that can measure progress on new and complex generative capabilities. However, manually creating new benchmarks is slow and expensive, restricting comprehensive evaluations for any capability. We introduce BenchAgents, a multi-agent framework that methodically leverages large language models (LLMs) to automate evaluation benchmark creation while inherently ensuring data and (evaluation) metric quality. BenchAgents decomposes the benchmark creation process into planning, generation, verification, and evaluation, each of which is ] orchestrated via LLM agents. These agents interact with each other and utilize feedback from benchmark developers to improve and flexibly control data diversity and quality. We use BenchAgents to create benchmarks to evaluate capabilities related to planning, constraint satisfaction, and causal reasoning spanning both language and vision modalities. We then use these benchmarks to study state-of-the-art models and extract new insights into common failure modes and model differences.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaB-RAG: Label Boosted Retrieval Augmented Generation for Radiology Report Generation</title>
<link>https://arxiv.org/abs/2411.16523</link>
<guid>https://arxiv.org/abs/2411.16523</guid>
<content:encoded><![CDATA[
arXiv:2411.16523v2 Announce Type: replace-cross 
Abstract: In the current paradigm of image captioning, deep learning models are trained to generate text from image embeddings of latent features. We challenge the assumption that fine-tuning of large, bespoke models is required to improve model generation accuracy. Here we propose Label Boosted Retrieval Augmented Generation (LaB-RAG), a small-model-based approach to image captioning that leverages image descriptors in the form of categorical labels to boost standard retrieval augmented generation (RAG) with pretrained large language models (LLMs). We study our method in the context of radiology report generation (RRG) over MIMIC-CXR and CheXpert Plus. We argue that simple classification models combined with zero-shot embeddings can effectively transform X-rays into text-space as radiology-specific labels. In combination with standard RAG, we show that these derived text labels can be used with general-domain LLMs to generate radiology reports. Without ever training our generative language model or image embedding models specifically for the task, and without ever directly "showing" the LLM an X-ray, we demonstrate that LaB-RAG achieves better results across natural language and radiology language metrics compared with other retrieval-based RRG methods, while attaining competitive results compared to other fine-tuned vision-language RRG models. We further conduct extensive ablation experiments to better understand the components of LaB-RAG. Our results suggest broader compatibility and synergy with fine-tuned methods to further enhance RRG performance.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility</title>
<link>https://arxiv.org/abs/2504.07086</link>
<guid>https://arxiv.org/abs/2504.07086</guid>
<content:encoded><![CDATA[
arXiv:2504.07086v2 Announce Type: replace-cross 
Abstract: Reasoning has emerged as the next major frontier for language models (LMs), with rapid advances from both academic and industrial labs. However, this progress often outpaces methodological rigor, with many evaluations relying on benchmarking practices that lack transparency, robustness, or statistical grounding. In this work, we conduct a comprehensive empirical study and find that current mathematical reasoning benchmarks are highly sensitive to subtle implementation choices--including decoding parameters, random seeds, prompt formatting, and even hardware and software configurations. Performance gains reported in recent studies frequently hinge on unclear comparisons or unreported sources of variance. To address these issues, we propose a standardized evaluation framework with clearly defined best practices and reporting standards. Using this framework, we reassess recent methods and find that most reinforcement learning (RL) approaches yield only modest improvements--far below prior claims--and are prone to overfitting, especially on small-scale benchmarks like AIME'24. In contrast, supervised finetuning (SFT) methods show consistently stronger generalization in the settings we study. To foster reproducibility, we release all code, prompts, and model outputs, for reasoning benchmarks, establishing more rigorous foundations for future work.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAE-FiRE: Enhancing Earnings Surprise Predictions Through Sparse Autoencoder Feature Selection</title>
<link>https://arxiv.org/abs/2505.14420</link>
<guid>https://arxiv.org/abs/2505.14420</guid>
<content:encoded><![CDATA[
arXiv:2505.14420v2 Announce Type: replace-cross 
Abstract: Predicting earnings surprises from financial documents, such as earnings conference calls, regulatory filings, and financial news, has become increasingly important in financial economics. However, these financial documents present significant analytical challenges, typically containing over 5,000 words with substantial redundancy and industry-specific terminology that creates obstacles for language models. In this work, we propose the SAE-FiRE (Sparse Autoencoder for Financial Representation Enhancement) framework to address these limitations by extracting key information while eliminating redundancy. SAE-FiRE employs Sparse Autoencoders (SAEs) to decompose dense neural representations from large language models into interpretable sparse components, then applies statistical feature selection methods, including ANOVA F-tests and tree-based importance scoring, to identify the top-k most discriminative dimensions for classification. By systematically filtering out noise that might otherwise lead to overfitting, we enable more robust and generalizable predictions. Experimental results across three financial datasets demonstrate that SAE-FiRE significantly outperforms baseline approaches.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Models Surface the Unwritten Code of Science and Society</title>
<link>https://arxiv.org/abs/2505.18942</link>
<guid>https://arxiv.org/abs/2505.18942</guid>
<content:encoded><![CDATA[
arXiv:2505.18942v3 Announce Type: replace-cross 
Abstract: This paper calls on the research community not only to investigate how human biases are inherited by large language models (LLMs) but also to explore how these biases in LLMs can be leveraged to make society's "unwritten code" - such as implicit stereotypes and heuristics - visible and accessible for critique. We introduce a conceptual framework through a case study in science: uncovering hidden rules in peer review - the factors that reviewers care about but rarely state explicitly due to normative scientific expectations. The idea of the framework is to push LLMs to speak out their heuristics through generating self-consistent hypotheses - why one paper appeared stronger in reviewer scoring - among paired papers submitted to 45 academic conferences, while iteratively searching deeper hypotheses from remaining pairs where existing hypotheses cannot explain. We observed that LLMs' normative priors about the internal characteristics of good science extracted from their self-talk, e.g., theoretical rigor, were systematically updated toward posteriors that emphasize storytelling about external connections, such as how the work is positioned and connected within and across literatures. Human reviewers tend to explicitly reward aspects that moderately align with LLMs' normative priors (correlation = 0.49) but avoid articulating contextualization and storytelling posteriors in their review comments (correlation = -0.14), despite giving implicit reward to them with positive scores. These patterns are robust across different models and out-of-sample judgments. We discuss the broad applicability of our proposed framework, leveraging LLMs as diagnostic tools to amplify and surface the tacit codes underlying human society, enabling public discussion of revealed values and more precisely targeted responsible AI.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisRet: Visualization Improves Knowledge-Intensive Text-to-Image Retrieval</title>
<link>https://arxiv.org/abs/2505.20291</link>
<guid>https://arxiv.org/abs/2505.20291</guid>
<content:encoded><![CDATA[
arXiv:2505.20291v2 Announce Type: replace-cross 
Abstract: Text-to-image retrieval (T2I retrieval) remains challenging because cross-modal embeddings often behave as bags of concepts and underrepresent structured visual relationships such as pose and viewpoint. We propose Visualize-then-Retrieve (VisRet), a new paradigm for T2I retrieval that mitigates this limitation of cross-modal similarity alignment. VisRet first projects textual queries into the image modality via T2I generation. Then, it performs retrieval within the image modality to bypass the weaknesses of cross-modal retrievers in recognizing subtle visual-spatial features. Across four benchmarks (Visual-RAG, INQUIRE-Rerank, Microsoft COCO, and our new Visual-RAG-ME featuring multi-entity comparisons), VisRet substantially outperforms cross-modal similarity matching and baselines that recast T2I retrieval as text-to-text similarity matching, improving nDCG@30 by 0.125 on average with CLIP as the retriever and by 0.121 with E5-V. For downstream question answering, VisRet increases accuracy on Visual-RAG and Visual-RAG-ME by 3.8% and 15.7% in top-1 retrieval, and by 3.9% and 11.1% in top-10 retrieval. Ablation studies show compatibility with different T2I instruction LLMs, T2I generation models, and downstream LLMs. VisRet provides a practical and principled path that energizes further advances in vision-language retrieval. Our code and the Visual-RAG-ME benchmark will be publicly released.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Accuracy to Robustness: A Study of Rule- and Model-based Verifiers in Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2505.22203</link>
<guid>https://arxiv.org/abs/2505.22203</guid>
<content:encoded><![CDATA[
arXiv:2505.22203v2 Announce Type: replace-cross 
Abstract: Trustworthy verifiers are essential for the success of reinforcement learning with verifiable reward (RLVR), which is the core methodology behind various large reasoning models such as DeepSeek-R1. In complex domains like mathematical reasoning, rule-based verifiers have been widely adopted in previous works to train strong reasoning models. However, the reliability of these verifiers and their impact on the RL training process remain poorly understood. In this work, we take mathematical reasoning as a case study and conduct a comprehensive analysis of various verifiers in both static evaluation and RL training scenarios. First, we find that current open-source rule-based verifiers often fail to recognize equivalent answers presented in different formats across multiple commonly used mathematical datasets, resulting in non-negligible false negative rates. This limitation adversely affects RL training performance and becomes more pronounced as the policy model gets stronger. Subsequently, we investigate model-based verifiers as a potential solution to address these limitations. While the static evaluation shows that model-based verifiers achieve significantly higher verification accuracy, further analysis and RL results imply that they are highly susceptible to hacking, where they misclassify certain patterns in responses as correct, particularly after fine-tuning. This vulnerability is exploited during policy model optimization, leading to artificially inflated rewards. Our findings underscore the unique challenges inherent to both rule-based and model-based verifiers and provide insights toward developing more accurate and robust reward systems for reinforcement learning.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Malicious AI Swarms Can Threaten Democracy: The Fusion of Agentic AI and LLMs Marks a New Frontier in Information Warfare</title>
<link>https://arxiv.org/abs/2506.06299</link>
<guid>https://arxiv.org/abs/2506.06299</guid>
<content:encoded><![CDATA[
arXiv:2506.06299v3 Announce Type: replace-cross 
Abstract: Public opinion manipulation has entered a new phase, amplifying its roots in rhetoric and propaganda. Advances in large language models (LLMs) and autonomous agents now let influence campaigns reach unprecedented scale and precision. Researchers warn AI could foster mass manipulation. Generative tools can expand propaganda output without sacrificing credibility and inexpensively create election falsehoods that are rated as more human-like than those written by humans. Techniques meant to refine AI reasoning, such as chain-of-thought prompting, can just as effectively be used to generate more convincing falsehoods. Enabled by these capabilities, another disruptive threat is emerging: swarms of collaborative, malicious AI agents. Fusing LLM reasoning with multi-agent architectures, these systems are capable of coordinating autonomously, infiltrating communities, and fabricating consensus cheaply. By adaptively mimicking human social dynamics, they threaten democracy.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A Study on Defeasible Video Entailment</title>
<link>https://arxiv.org/abs/2506.22385</link>
<guid>https://arxiv.org/abs/2506.22385</guid>
<content:encoded><![CDATA[
arXiv:2506.22385v2 Announce Type: replace-cross 
Abstract: Video Large Multimodal Models (VLMMs) have made impressive strides in understanding video content, but they often struggle with abstract and adaptive reasoning-the ability to revise their interpretations when new information emerges. In reality, conclusions are rarely set in stone; additional context can strengthen or weaken an initial inference. To address this, we introduce Defeasible Video Entailment (DVidE), a new task that challenges models to think like doubters, constantly updating their reasoning based on evolving evidence. In DVidE, given a video premise and a textual hypothesis, models must determine whether a new update strengthens or weakens the hypothesis (classification version) or generate a coherent update that modifies the entailment relationship (generation version). For solving the classification task, we propose the Chain of Counterfactual Thought framework, utilizing counterfactual reasoning, ASR-enhanced video content, and rationale refinement to reduce inference bias. For the generation task, we develop a framework that combines ASR output with a Large Language Model (LLM) to produce coherent, contextually relevant updates aligned with the intended strengthener or weakener goals. Additionally, we introduce a novel benchmark dataset, with strengthener/weakener annotations and an LLM-based evaluation metric specifically designed for assessing generative performance. Experimental results demonstrate significant improvements, highlighting our proposed method in enhancing dynamic reasoning capabilities of VLMMs.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAPO: Towards Enhancing LLM Reasoning through Generative Credit Assignment</title>
<link>https://arxiv.org/abs/2508.02298</link>
<guid>https://arxiv.org/abs/2508.02298</guid>
<content:encoded><![CDATA[
arXiv:2508.02298v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has improved the reasoning abilities of Large Language Models (LLMs) by using rule-based binary feedback. However, current RLVR methods typically assign the same reward to every token. This coarse-grained feedback hampers precise credit assignment, making it hard for models to identify which reasoning steps lead to success or failure, and often results in suboptimal policies. Methods like PPO provide credit assignment by value estimation, but yield inaccurate and unverifiable signals due to limited sampling. On the other hand, methods using Process Reward Models can provide step-wise rewards but suffer from several key limitations: they require high-quality process supervision labels, the feedback is unreliable due to probabilistic reward modeling, and their application in online reinforcement learning (RL) is time-consuming. To overcome these limitations, we introduce a simple but efficient method-Credit Assignment Policy Optimization (CAPO). Instead of training auxiliary models, CAPO directly leverages an off-the-shelf, general-purpose LLM as a Generative Process Reward Model (LLM-as-GenPRM) to generate all step-wise critique by one pass only based on the correctness of the step itself, providing deterministic token-level credits to refine the tokens that were originally assigned identical rule-based rewards. To further enhance the accuracy and robustness, we employ voting mechanisms that scale with the number of generated critiques. Extensive experiments on various backbones like Llama and Qwen models show that CAPO consistently outperforms supervised learning-based and RL-based fine-tuning methods across four challenging mathematical benchmarks and three out-of-domain benchmarks. Further analysis shows that CAPO can help the model to foster the learning of correct reasoning pathways leading to correct answers.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RepIt: Representing Isolated Targets to Steer Language Models</title>
<link>https://arxiv.org/abs/2509.13281</link>
<guid>https://arxiv.org/abs/2509.13281</guid>
<content:encoded><![CDATA[
arXiv:2509.13281v2 Announce Type: replace-cross 
Abstract: While activation steering in large language models (LLMs) is a growing area of research, methods can often incur broader effects than desired. This motivates isolation of purer concept vectors to enable targeted interventions and understand LLM behavior at a more granular level. We present RepIt, a simple and data-efficient framework for isolating concept-specific representations. Across five frontier LLMs, RepIt enables precise interventions: it selectively suppresses refusal on targeted concepts while preserving refusal elsewhere, producing models that answer WMD-related questions while still scoring as safe on standard benchmarks. We further show that the corrective signal localizes to just 100-200 neurons and that robust target representations can be extracted from as few as a dozen examples on a single A6000. This efficiency raises a dual concern: manipulations can be performed with modest compute and data to extend to underrepresented data-scarce topics while evading existing benchmarks. By disentangling refusal vectors with RepIt, this work demonstrates that targeted interventions can counteract overgeneralization, laying the foundation for more granular control of model behavior.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GEM-Bench: A Benchmark for Ad-Injected Response Generation within Generative Engine Marketing</title>
<link>https://arxiv.org/abs/2509.14221</link>
<guid>https://arxiv.org/abs/2509.14221</guid>
<content:encoded><![CDATA[
arXiv:2509.14221v2 Announce Type: replace-cross 
Abstract: Generative Engine Marketing (GEM) is an emerging ecosystem for monetizing generative engines, such as LLM-based chatbots, by seamlessly integrating relevant advertisements into their responses. At the core of GEM lies the generation and evaluation of ad-injected responses. However, existing benchmarks are not specifically designed for this purpose, which limits future research. To address this gap, we propose GEM-Bench, the first comprehensive benchmark for ad-injected response generation in GEM. GEM-Bench includes three curated datasets covering both chatbot and search scenarios, a metric ontology that captures multiple dimensions of user satisfaction and engagement, and several baseline solutions implemented within an extensible multi-agent framework. Our preliminary results indicate that, while simple prompt-based methods achieve reasonable engagement such as click-through rate, they often reduce user satisfaction. In contrast, approaches that insert ads based on pre-generated ad-free responses help mitigate this issue but introduce additional overhead. These findings highlight the need for future research on designing more effective and efficient solutions for generating ad-injected responses in GEM. The benchmark and all related resources are publicly available at https://gem-bench.org/.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Margin RLHF via Preference over Preferences</title>
<link>https://arxiv.org/abs/2509.22851</link>
<guid>https://arxiv.org/abs/2509.22851</guid>
<content:encoded><![CDATA[
arXiv:2509.22851v2 Announce Type: replace-cross 
Abstract: Margin-based optimization is fundamental to improving generalization and robustness in classification tasks. In the context of reward model learning from preferences within Reinforcement Learning from Human Feedback (RLHF), existing methods typically rely on no margins, fixed margins, or margins that are simplistic functions of preference ratings. However, such formulations often fail to account for the varying strengths of different preferences, for example some preferences are associated with larger margins between responses, or they rely on noisy margin information derived from ratings. We argue that modeling the strength of preferences can lead to better generalization and more faithful alignment. Furthermore, many existing methods that use adaptive margins assume access to accurate preference scores, which can be difficult for humans to provide reliably. We propose an approach that leverages preferences over preferences, that is annotations indicating which of two preferences reflects a stronger distinction. We use this ordinal signal to infer adaptive margins on a per-datapoint basis. We introduce an extension to Direct Preference Optimization (DPO), DPO-PoP, that incorporates adaptive margins from preference-over-preference supervision, enabling improved discriminative and generative performance. Empirically, our method outperforms vanilla DPO, DPO with fixed margins, and DPO with ground-truth margins on the UltraFeedback dataset. Additionally, we show that there is a tradeoff between discriminative and generative performance: improving test classification accuracy, particularly by correctly labeling weaker preferences at the expense of stronger ones, can lead to a decline in generative quality. To navigate this tradeoff, we propose two sampling strategies to gather preference-over-preference labels: one favoring discriminative performance and one favoring generative performance.
]]></content:encoded>
<pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>K-DeCore: Facilitating Knowledge Transfer in Continual Structured Knowledge Reasoning via Knowledge Decoupling</title>
<link>https://arxiv.org/abs/2509.16929</link>
<guid>https://arxiv.org/abs/2509.16929</guid>
<content:encoded><![CDATA[
<div> Keywords: Continual Learning, Structured Knowledge Reasoning, Knowledge Decoupling, Memory Consolidation, Generalization

Summary:
Continual Structured Knowledge Reasoning (CSKR) involves translating natural language questions into structured queries based on structured knowledge. Existing continual learning approaches struggle with heterogeneous knowledge and increased parameter growth. The proposed framework, K-DeCore, addresses these challenges by using a fixed number of parameters. K-DeCore incorporates a knowledge decoupling mechanism to separate reasoning into task-specific and task-agnostic stages, enhancing performance across diverse tasks. The model also integrates a dual-perspective memory consolidation mechanism and a structure-guided pseudo-data synthesis strategy to improve generalization capabilities. Extensive experiments on benchmark datasets show that K-DeCore outperforms existing methods on various metrics, utilizing different large language models.<br /><br />Summary: <div>
arXiv:2509.16929v5 Announce Type: replace 
Abstract: Continual Structured Knowledge Reasoning (CSKR) focuses on training models to handle sequential tasks, where each task involves translating natural language questions into structured queries grounded in structured knowledge. Existing general continual learning approaches face significant challenges when applied to this task, including poor generalization to heterogeneous structured knowledge and inefficient reasoning due to parameter growth as tasks increase. To address these limitations, we propose a novel CSKR framework, \textsc{K-DeCore}, which operates with a fixed number of tunable parameters. Unlike prior methods, \textsc{K-DeCore} introduces a knowledge decoupling mechanism that disentangles the reasoning process into task-specific and task-agnostic stages, effectively bridging the gaps across diverse tasks. Building on this foundation, \textsc{K-DeCore} integrates a dual-perspective memory consolidation mechanism for distinct stages and introduces a structure-guided pseudo-data synthesis strategy to further enhance the model's generalization capabilities. Extensive experiments on four benchmark datasets demonstrate the superiority of \textsc{K-DeCore} over existing continual learning methods across multiple metrics, leveraging various backbone large language models.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ML2B: Multi-Lingual ML Benchmark For AutoML</title>
<link>https://arxiv.org/abs/2509.22768</link>
<guid>https://arxiv.org/abs/2509.22768</guid>
<content:encoded><![CDATA[
<div> benchmark, multilingual ML code generation, performance degradation, AIDE, data science pipelines

Summary:<br />
The article introduces ML2B, a benchmark for evaluating multilingual machine learning (ML) code generation. It includes 30 Kaggle competitions translated into 13 languages, covering various data types. Using the AIDE framework, the authors evaluate cross-lingual model performance and find a 15-45% performance degradation in non-English tasks. The results emphasize the challenges in multilingual representation learning for code generation. The benchmark, evaluation framework, and results are openly available on GitHub for further research in multilingual ML code generation. <div>
arXiv:2509.22768v2 Announce Type: replace 
Abstract: Large language models (LLMs) have recently demonstrated strong capabilities in generating machine learning (ML) code, enabling end-to-end pipeline construction from natural language instructions. However, existing benchmarks for ML code generation are mainly restricted to English, overlooking the global and multilingual nature of ML research and practice. To address this gap, we present ML2B, the first benchmark for evaluating multilingual ML code generation. ML2B consists of 30 Kaggle competitions translated into 13 natural languages, covering tabular, text, and image data types, with structured metadata and validated human-reviewed translations. For evaluation, we employ AIDE, an automated framework for end-to-end assessment of data science pipelines, and provide insights into cross-lingual model performance. Our results reveal substantial 15-45% performance degradation on non-English tasks, highlighting critical challenges in multilingual representation learning for code generation. The benchmark, evaluation framework, and comprehensive results are made available through our GitHub repository to facilitate future research in multilingual ML code generation: https://github.com/enaix/ml2b.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HEART: Emotionally-driven test-time scaling of Language Models</title>
<link>https://arxiv.org/abs/2509.22876</link>
<guid>https://arxiv.org/abs/2509.22876</guid>
<content:encoded><![CDATA[
<div> emotional prompts, affective feedback, reasoning tasks, language models, HEART <br />
<br />
Summary: HEART is a novel framework that uses emotionally-driven prompts to provide feedback to language models during test-time scaling. Inspired by psychological research on the impact of emotions on cognition, HEART guides models to correct incorrect responses by utilizing emotionally charged phrases based on the six universal emotions. By varying the emotional tone of the feedback, the framework helps models escape flawed reasoning paths and explore better alternatives. Evaluations on challenging reasoning benchmarks show that when guided by an oracle verifier, HEART significantly improves reasoning accuracy over state-of-the-art baselines. However, the framework faces challenges in consistent gains in a verifier-free setting, highlighting a key area for future research. The study suggests that leveraging emotions in machine reasoning may lead to deeper understanding and improved performance in language models. <br /> <div>
arXiv:2509.22876v2 Announce Type: replace 
Abstract: Test-time scaling has shown considerable success in improving the performance of language models on complex reasoning tasks without requiring fine-tuning. However, current strategies such as self-reflection primarily focus on logical or structural refinement. They do not leverage the guiding potential of affective feedback. Inspired by psychological research showing that emotions can modulate cognitive performance, we introduce HEART--a novel framework that uses emotionally-driven prompts for iterative self-correction. HEART provides feedback on a model's incorrect response using a curated set of concise, emotionally charged phrases based on the six universal emotions categorized by Dr. Paul Ekman. By systematically varying the emotional tone of the feedback across iterations, our method guides the model to escape flawed reasoning paths and explore more promising alternatives. We evaluate our framework on challenging reasoning benchmarks including OlympiadBench, Humanity's Last Exam, and SimpleQA. Our results reveal a significant new phenomenon: when guided by an oracle verifier, this affective iteration protocol unlocks significantly deeper reasoning, leading to consistent and substantial increases in accuracy over state-of-the-art baselines with the same verifier. However, we also identify a critical bottleneck for practical deployment. In a verifier-free setting, it struggles to harness these gains consistently, highlighting as a key challenge for future work. Our findings suggest that the next frontier in machine reasoning may lie not just in refining logic, but also in understanding and leveraging the `HEART' of the models.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-Collaborative User Simulators for Tool Agents</title>
<link>https://arxiv.org/abs/2509.23124</link>
<guid>https://arxiv.org/abs/2509.23124</guid>
<content:encoded><![CDATA[
<div> keywords: tool agents, user simulation, non-collaborative behaviors, dialogue breakdowns, performance degradation
Summary: 
The study focuses on improving tool agents' performance by introducing a novel user simulator architecture that simulates non-collaborative behaviors. The proposed simulator can mimic challenging user behaviors such as requesting unavailable services, digressing into tangential conversations, expressing impatience, and providing incomplete utterances. Experiments on MultiWOZ and $\tau$-bench demonstrate that state-of-the-art agents face significant performance degradation when interacting with non-collaborative users. The analysis reveals weaknesses in agents' responses, including escalated hallucinations and dialogue breakdowns. The user simulation framework contributes to the development of robust tool agents by exposing them to real-world challenges and providing insights for preemptive diagnosis. This research aims to enhance agents' interaction capabilities and ensure their effectiveness in handling diverse user behaviors. 
<br /><br />Summary: <div>
arXiv:2509.23124v2 Announce Type: replace 
Abstract: Tool agents interact with users through multi-turn dialogues to accomplish various tasks. Recent studies have adopted user simulation methods to develop these agents in multi-turn settings. However, existing user simulators tend to be agent-friendly, exhibiting only cooperative behaviors, which fails to train and test agents against non-collaborative users in the real world. To address this, we propose a novel user simulator architecture that simulates four categories of non-collaborative behaviors: requesting unavailable services, digressing into tangential conversations, expressing impatience, and providing incomplete utterances. Our user simulator can simulate challenging and natural non-collaborative behaviors while reliably delivering all intents and information necessary to accomplish the task. Our experiments on MultiWOZ and $\tau$-bench reveal significant performance degradation in state-of-the-art tool agents when encountering non-collaborative users. We provide detailed analyses of agents' weaknesses under each non-collaborative condition, such as escalated hallucinations and dialogue breakdowns. Ultimately, we contribute an easily extensible user simulation framework to help the research community develop tool agents and preemptively diagnose them under challenging real-world conditions within their own services.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiKE: Hierarchical Evaluation Framework for Korean-English Code-Switching Speech Recognition</title>
<link>https://arxiv.org/abs/2509.24613</link>
<guid>https://arxiv.org/abs/2509.24613</guid>
<content:encoded><![CDATA[
<div> Keywords: multilingual automatic speech recognition, code-switching, Korean-English, benchmark, fine-tuning 

Summary: 
Despite advances in multilingual automatic speech recognition (ASR), code-switching (CS) – the blending of languages within an utterance – poses a significant challenge. The introduction of HiKE, a benchmark for Korean-English code-switching, aims to address this gap by providing a comprehensive evaluation framework for multilingual ASR models. HiKE includes high-quality CS data across various topics, detailed loanword labels, and a hierarchical CS-level labeling scheme to enable systematic evaluation at different levels (word, phrase, sentence). Evaluation of diverse multilingual ASR models using HiKE reveals initial inadequacy in CS-ASR performance, which can be improved through fine-tuning with synthetic CS data. This research not only highlights the need for better handling of code-switching in ASR but also provides a valuable resource for further research and development in this area. HiKE is publicly available for researchers to access and utilize in their work. 

<br /><br />Summary: <div>
arXiv:2509.24613v2 Announce Type: replace 
Abstract: Despite advances in multilingual automatic speech recognition (ASR), code-switching (CS), the mixing of languages within an utterance common in daily speech, remains a severely underexplored challenge. In this paper, we introduce HiKE: the Hierarchical Korean-English code-switching benchmark, the first globally accessible evaluation framework for Korean-English CS, aiming to provide a means for the precise evaluation of multilingual ASR models and to foster research in the field. The proposed framework not only consists of high-quality, natural CS data across various topics, but also provides meticulous loanword labels and a hierarchical CS-level labeling scheme (word, phrase, and sentence) that together enable a systematic evaluation of a model's ability to handle each distinct level of code-switching. Through evaluations of diverse multilingual ASR models and fine-tuning experiments, this paper demonstrates that although most multilingual ASR models initially exhibit inadequate CS-ASR performance, this capability can be enabled through fine-tuning with synthetic CS data. HiKE is available at https://github.com/ThetaOne-AI/HiKE
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>jina-reranker-v3: Last but Not Late Interaction for Listwise Document Reranking</title>
<link>https://arxiv.org/abs/2509.25085</link>
<guid>https://arxiv.org/abs/2509.25085</guid>
<content:encoded><![CDATA[
<div> novel interaction, multilingual listwise reranker, causal attention, state-of-the-art performance, smaller model

Summary:
The article introduces jina-reranker-v3, a multilingual listwise reranker with a novel "last but not late" interaction approach. Unlike other models, this reranker applies causal attention between the query and candidate documents in the same context window, enabling rich interactions before extracting contextual embeddings. The model, despite its smaller size compared to other models, achieves state-of-the-art BEIR performance with 61.94 nDCG@10. This showcases the effectiveness of the unique interaction approach in improving reranking performance. <div>
arXiv:2509.25085v4 Announce Type: replace 
Abstract: jina-reranker-v3 is a 0.6B-parameter multilingual listwise reranker that introduces a novel "last but not late" interaction. Unlike late interaction models like ColBERT that encode documents separately before multi-vector matching, our approach applies causal attention between the query and all candidate documents in the same context window, enabling rich interactions before extracting contextual embeddings from each document's final token. The new model achieves state-of-the-art BEIR performance with 61.94 nDCG@10 while being significantly smaller than other models with comparable performance.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Your Models Have Thought Enough: Training Large Reasoning Models to Stop Overthinking</title>
<link>https://arxiv.org/abs/2509.23392</link>
<guid>https://arxiv.org/abs/2509.23392</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Reasoning Models, Reinforcement Learning, Evidence Accumulation Models, Just-Enough Thinking, Efficiency

Summary: 
Just-Enough Thinking (JET) addresses the issue of inefficient reasoning in Large Reasoning Models (LRMs) by proactively terminating unnecessary reasoning steps. Inspired by Evidence Accumulation Models, JET performs trajectory truncation during rollout to expose the model to short, distributionally consistent reasoning paths. It also uses a quality-controlled length reward to incentivize concise reasoning while maintaining accuracy. Experimental results show that JET significantly improves reasoning efficiency without sacrificing correctness. For example, DeepSeek-Distill-Qwen-1.5B achieves a 4.6% accuracy gain while reducing output length by 46.3% on the Olympiad benchmark. The code for JET is available on GitHub. <br /><br />Summary: <div>
arXiv:2509.23392v2 Announce Type: replace-cross 
Abstract: Large Reasoning Models (LRMs) have achieved impressive performance on challenging tasks, yet their deep reasoning often incurs substantial computational costs. To achieve efficient reasoning, existing reinforcement learning methods still struggle to construct short reasoning path during the rollout stage, limiting effective learning. Inspired by Evidence Accumulation Models, we find that LRMs have accumulated sufficient information early in reasoning, making further reasoning steps redundant. Based on this insight, we propose Just-Enough Thinking (JET), which trains models to proactively terminate unnecessary reasoning. JET performs trajectory truncation during rollout to expose the model to short, distributionally consistent reasoning paths. Besides, it uses a quality-controlled length reward to better encourage concise reasoning while maintaining correctness. Extensive experiments demonstrate that JET significantly improves reasoning efficiency without sacrificing accuracy. Especially, DeepSeek-Distill-Qwen-1.5B achieves a 4.6% accuracy gain while reducing output length by 46.3% on the Olympiad benchmark. Our code is available in the GitHub.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Visual Reasoning</title>
<link>https://arxiv.org/abs/2509.24251</link>
<guid>https://arxiv.org/abs/2509.24251</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, Chain-of-Thought reasoning, Latent Visual Reasoning, visual question answering, reinforcement learning
Summary: 
- Multimodal Large Language Models (MLLMs) incorporating Chain-of-Thought reasoning have shown significant advancements in various tasks.
- Latent Visual Reasoning (LVR) introduces a new paradigm for autoregressive reasoning directly in the visual embedding space, enhancing visual understanding and perception.
- LVR leverages a visual encoder to project images into visual tokens in a joint semantic space shared with the language model.
- The language model is trained to generate latent states that reconstruct key visual tokens, facilitating the process of latent visual reasoning.
- By interleaving LVR with standard text generation and employing reinforcement learning with the GRPO algorithm, the model achieves substantial improvements in visual question answering tasks, outperforming previous approaches. <div>
arXiv:2509.24251v2 Announce Type: replace-cross 
Abstract: Multimodal Large Language Models (MLLMs) have achieved notable gains in various tasks by incorporating Chain-of-Thought (CoT) reasoning in language spaces. Recent work extends this direction by leveraging external tools for visual editing, thereby enhancing the visual signal along the reasoning trajectories. Nevertheless, these approaches remain fundamentally constrained: reasoning is still confined to the language space, with visual information treated as static preconditions. We introduce Latent Visual Reasoning (LVR), a new paradigm that enables autoregressive reasoning directly in the visual embedding space. A visual encoder first projects images into visual tokens within a joint semantic space shared with the language model. The language model is then trained to generate latent states that reconstruct key visual tokens critical for answering the query, constituting the process of latent visual reasoning. By interleaving LVR with standard text generation, our model achieves substantial gains on perception-intensive visual question answering tasks. In addition, we adapt the GRPO algorithm to conduct reinforcement learning on latent reasoning, further balancing LVR and textual generation. We show that LVR substantially improves fine-grained visual understanding and perception, achieving 71.67% on MMVP compared to 66.67% with Qwen2.5-VL. Code base and model weights will be released later.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extracting the Structure of Press Releases for Predicting Earnings Announcement Returns</title>
<link>https://arxiv.org/abs/2509.24254</link>
<guid>https://arxiv.org/abs/2509.24254</guid>
<content:encoded><![CDATA[
<div> Keywords: earnings press releases, stock returns, FinBERT, market open, self-serving bias<br />
Summary: 
Stock returns on earnings announcement days can be predicted by analyzing textual features in earnings press releases using various methods like bag-of-words and BERT-based embeddings. The study, based on a large dataset spanning from 2005 to 2023, reveals that the content of press releases carries significant predictive power comparable to earnings surprise. FinBERT shows the highest efficacy in prediction. Combining different models enhances the explanatory strength and interpretability of the press release content. Market prices quickly incorporate the information from press releases at the opening, indicating the importance of timeliness. Detecting leaked press releases can provide predictive advantages. Analysis of press release content suggests the presence of self-serving bias in managerial narratives. The study also introduces a framework for real-time return prediction using online learning integration and emphasizes the role of language nuances in stock price formation.<br /><br />Summary: <div>
arXiv:2509.24254v2 Announce Type: replace-cross 
Abstract: We examine how textual features in earnings press releases predict stock returns on earnings announcement days. Using over 138,000 press releases from 2005 to 2023, we compare traditional bag-of-words and BERT-based embeddings. We find that press release content (soft information) is as informative as earnings surprise (hard information), with FinBERT yielding the highest predictive power. Combining models enhances explanatory strength and interpretability of the content of press releases. Stock prices fully reflect the content of press releases at market open. If press releases are leaked, it offers predictive advantage. Topic analysis reveals self-serving bias in managerial narratives. Our framework supports real-time return prediction through the integration of online learning, provides interpretability and reveals the nuanced role of language in price formation.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Self-awareness of Large Reasoning Models' Capability Boundaries</title>
<link>https://arxiv.org/abs/2509.24711</link>
<guid>https://arxiv.org/abs/2509.24711</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Reasoning Models, self-awareness, capability boundaries, optimization strategies, efficiency

Summary:
Large Reasoning Models (LRMs) have shown impressive performance in complex reasoning tasks, but they often struggle with hard questions, wasting computation and producing wrong answers. This paper investigates whether LRMs have self-awareness of their capability boundaries. The study reveals that LRMs may know what they cannot solve based on their expressed reasoning confidence. For black-box models, reasoning expressions indicate boundary signals, with growing confidence for solvable problems and convergent uncertainty for unsolvable ones. White-box models show that hidden states encode boundary information, allowing for the separation of solvable and unsolvable problems even before reasoning begins. Based on these findings, two optimization strategies are proposed: reasoning expression monitoring and hidden states monitoring. Experiments demonstrate that these boundary-aware strategies help LRMs avoid unproductive reasoning, leading to significant efficiency improvements without sacrificing accuracy, reducing token usage by up to 62.7 - 93.6%. 

<br /><br />Summary: <div>
arXiv:2509.24711v2 Announce Type: replace-cross 
Abstract: Large Reasoning Models (LRMs) have shown impressive performance on complex reasoning tasks such as mathematics, yet they also display misbehaviors that expose their limitations. In particular, when faced with hard questions, LRMs often engage in unproductive reasoning until context limit, producing wrong answers while wasting substantial computation. This phenomenon reflects a fundamental issue: current answering paradigms overlook the relationship between questions and LRMs' capability boundaries. In this paper, we investigate whether LRMs possess self-awareness of capability boundaries. We begin by an observation that LRMs may know what they cannot solve through expressed reasoning confidence. For black-box models, we find that reasoning expressions reveal boundary signals, with accelerated growing confidence trajectory for solvable problems but convergent uncertainty trajectory for unsolvable ones. For white-box models, we show that hidden states of the last input token encode boundary information, with solvable and unsolvable problems linearly separable even before reasoning begins. Building on these findings, we propose two simple yet effective optimization strategies: reasoning expression monitoring and hidden states monitoring. Experiments demonstrate that these boundary-aware strategies enable LRMs to avoid unproductive reasoning without sacrificing accuracy, significantly improving reliability and efficiency by cutting token usage up to 62.7 - 93.6%.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pushing LLMs to Their Logical Reasoning Bound: The Role of Data Reasoning Intensity</title>
<link>https://arxiv.org/abs/2509.24836</link>
<guid>https://arxiv.org/abs/2509.24836</guid>
<content:encoded><![CDATA[
<div> Data Reasoning Intensity, Logical Reasoning Complexity, Large Language Models, Training Data Structure, Cognitive Capacity <br />
Summary: <br />
This paper introduces the concept of Data Reasoning Intensity (DRI) as a metric to assess the logical reasoning complexity of training data samples for Large Language Models (LLMs). The study highlights the joint constraints of training data potential and model cognitive capacity on logical reasoning performance of LLMs. A re-cognizing optimization strategy is proposed to enhance the logical reasoning intensity of training data by aligning it better with the LLM's reasoning boundary. Experimental results demonstrate significant performance improvements and enhanced generalization with this approach compared to traditional data-centric strategies. The research emphasizes the importance of prioritizing reasoning complexity in training data over data volume for maximizing the cognitive potential of LLMs, as validated in a reinforcement learning framework. <br /> <div>
arXiv:2509.24836v3 Announce Type: replace-cross 
Abstract: Recent advances in large language models (LLMs) highlight the importance of training data structure and quality in shaping reasoning behavior. However, most existing approaches focus on transforming data formats while neglecting the internal reasoning complexity of training samples, leaving the reasoning potential of data under-explored and underutilized. In this work, we posit that LLM logical reasoning performance is jointly constrained by the potential of the training data and the cognitive capacity of the model. To make this relationship measurable, we introduce Data Reasoning Intensity (DRI), a novel metric that quantifies the latent logical reasoning complexity of samples by decomposing and aggregating their logical structures. This allows us to analyze how well current LLMs utilize logical reasoning signals and identify performance gaps relative to data potential. Based on this insight, we introduce a re-cognizing optimization strategy that systematically enhances the logical reasoning intensity of training data. Rather than increasing data volume, our method re-optimizes existing samples to better align with the LLM's logical reasoning boundary. Extensive experiments show that our approach significantly improves performance and generalization over data-centric strategies. We further validate our method under a reinforcement learning framework. Our results indicate that prioritizing reasoning complexity in data rather than sheer scale or superficial form is essential to realizing LLMs' full cognitive potential.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decomposing Attention To Find Context-Sensitive Neurons</title>
<link>https://arxiv.org/abs/2510.03315</link>
<guid>https://arxiv.org/abs/2510.03315</guid>
<content:encoded><![CDATA[
<div> transformer language models, attention heads, attention patterns, stable softmax denominators, linear summary<br />
<br />
Summary: 
The study examines transformer language models and identifies attention heads with spread-out patterns and weak content dependency. These attention heads have stable softmax denominators when token distribution remains constant. By sampling softmax denominators from a calibration text, the study proposes combining outputs of multiple stable heads in the first layer of GPT2-Small to approximate a linear summary of the text. This approximation approach allows the identification of hundreds of first layer neurons that respond to high-level contextual properties of the text, including neurons not activated by the calibration text. Through analyzing these stable attention heads, the study reveals a method to uncover neurons that capture detailed contextual information solely based on model weights and a single calibration text. <div>
arXiv:2510.03315v1 Announce Type: new 
Abstract: We study transformer language models, analyzing attention heads whose attention patterns are spread out, and whose attention scores depend weakly on content. We argue that the softmax denominators of these heads are stable when the underlying token distribution is fixed. By sampling softmax denominators from a "calibration text", we can combine together the outputs of multiple such stable heads in the first layer of GPT2-Small, approximating their combined output by a linear summary of the surrounding text. This approximation enables a procedure where from the weights alone - and a single calibration text - we can uncover hundreds of first layer neurons that respond to high-level contextual properties of the surrounding text, including neurons that didn't activate on the calibration text.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-S3: Enhancing Agentic textual Graph Retrieval with Synthetic Stepwise Supervision</title>
<link>https://arxiv.org/abs/2510.03323</link>
<guid>https://arxiv.org/abs/2510.03323</guid>
<content:encoded><![CDATA[
<div> Graph-$S^3$, textual graph reasoning, LLM-based retriever, synthetic stepwise supervision, graph retrieval<br />
Summary:<br />
Graph-$S^3$ is a novel framework for textual graph reasoning that addresses the challenge of graph retrieval in LLM-based QA systems. It utilizes an LLM-based retriever trained with synthetic stepwise supervision, focusing on evaluating each step of the retriever based on golden subgraphs. The approach involves a data synthesis pipeline for reward generation and a two-stage training scheme for interactive graph exploration policy learning. Extensive experiments show an average improvement of 8.1% in accuracy and 9.7% in F$_1$ score compared to baselines, with even greater advantages in complex multi-hop reasoning tasks. The code for Graph-$S^3$ will be made open-source. <br /> <div>
arXiv:2510.03323v1 Announce Type: new 
Abstract: A significant portion of real-world data is inherently represented as textual graphs, and integrating these graphs into large language models (LLMs) is promising to enable complex graph-based question answering. However, a key challenge in LLM-based textual graph QA systems lies in graph retrieval, i.e., how to retrieve relevant content from large graphs that is sufficiently informative while remaining compact for the LLM context. Existing retrievers suffer from poor performance since they either rely on shallow embedding similarity or employ interactive retrieving policies that demand excessive data labeling and training cost. To address these issues, we present Graph-$S^3$, an agentic textual graph reasoning framework that employs an LLM-based retriever trained with synthetic stepwise supervision. Instead of rewarding the agent based on the final answers, which may lead to sparse and unstable training signals, we propose to closely evaluate each step of the retriever based on offline-extracted golden subgraphs. Our main techniques include a data synthesis pipeline to extract the golden subgraphs for reward generation and a two-stage training scheme to learn the interactive graph exploration policy based on the synthesized rewards. Based on extensive experiments on three common datasets in comparison with seven strong baselines, our approach achieves an average improvement of 8.1\% in accuracy and 9.7\% in F$_1$ score. The advantage is even higher in more complicated multi-hop reasoning tasks. Our code will be open-sourced.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implicit Values Embedded in How Humans and LLMs Complete Subjective Everyday Tasks</title>
<link>https://arxiv.org/abs/2510.03384</link>
<guid>https://arxiv.org/abs/2510.03384</guid>
<content:encoded><![CDATA[
<div> values, language models, AI assistants, everyday tasks, comparison

Summary: 
The study examines how large language models (LLMs) embody implicit values when completing everyday tasks for AI assistants. The research compares six popular LLMs with 100 human crowdworkers from the US to assess alignment with values like environmentalism, charity, and diversity. The findings show discrepancies between LLMs, humans, and even among different LLMs in exhibiting these values. The study highlights the need to understand and address the discrepancies in implicit values displayed by AI assistants, as they play a crucial role in assisting users with various tasks. <div>
arXiv:2510.03384v1 Announce Type: new 
Abstract: Large language models (LLMs) can underpin AI assistants that help users with everyday tasks, such as by making recommendations or performing basic computation. Despite AI assistants' promise, little is known about the implicit values these assistants display while completing subjective everyday tasks. Humans may consider values like environmentalism, charity, and diversity. To what extent do LLMs exhibit these values in completing everyday tasks? How do they compare with humans? We answer these questions by auditing how six popular LLMs complete 30 everyday tasks, comparing LLMs to each other and to 100 human crowdworkers from the US. We find LLMs often do not align with humans, nor with other LLMs, in the implicit values exhibited.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Morpheme Induction for Emergent Language</title>
<link>https://arxiv.org/abs/2510.03439</link>
<guid>https://arxiv.org/abs/2510.03439</guid>
<content:encoded><![CDATA[
<div> algorithm, morphemes, language, mutual information, data
Summary:
CSAR is a new algorithm for inducing morphemes from emergent language corpora. It weights morphemes based on mutual information between forms and meanings and selects the highest-weighted pair iteratively. The algorithm's effectiveness is validated on both procedurally generated datasets and human language data. It outperforms baselines for related tasks and makes reasonable predictions in adjacent domains. The analysis of emergent languages using CSAR reveals linguistic characteristics such as degree of synonymy and polysemy. This approach of Count, Select, Ablate, Repeat shows promise in extracting meaningful morphemes from language data. <br /><br />Summary: <div>
arXiv:2510.03439v1 Announce Type: new 
Abstract: We introduce CSAR, an algorithm for inducing morphemes from emergent language corpora of parallel utterances and meanings. It is a greedy algorithm that (1) weights morphemes based on mutual information between forms and meanings, (2) selects the highest-weighted pair, (3) removes it from the corpus, and (4) repeats the process to induce further morphemes (i.e., Count, Select, Ablate, Repeat). The effectiveness of CSAR is first validated on procedurally generated datasets and compared against baselines for related tasks. Second, we validate CSAR's performance on human language data to show that the algorithm makes reasonable predictions in adjacent domains. Finally, we analyze a handful of emergent languages, quantifying linguistic characteristics like degree of synonymy and polysemy.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Omni-Embed-Nemotron: A Unified Multimodal Retrieval Model for Text, Image, Audio, and Video</title>
<link>https://arxiv.org/abs/2510.03458</link>
<guid>https://arxiv.org/abs/2510.03458</guid>
<content:encoded><![CDATA[
<div> Keywords: Omni-Embed-Nemotron, multimodal retrieval, embedding model, cross-modal retrieval, joint-modal retrieval

Summary:
Omni-Embed-Nemotron is a novel multimodal retrieval embedding model designed to address the complexities of real-world information retrieval needs. Unlike previous text-based retrievers, this model can handle visually and semantically rich content found in documents like PDFs, slides, and videos. Inspired by existing models like ColPali and Qwen2.5-Omni, Omni-Embed-Nemotron extends retrieval capabilities to support audio and video modalities in addition to text and images. This allows for both cross-modal (e.g., text - video) and joint-modal (e.g., text - video+audio) retrieval using a single model. The architecture, training setup, and evaluation results of Omni-Embed-Nemotron are discussed, showcasing its effectiveness in text, image, and video retrieval. Overall, Omni-Embed-Nemotron represents a significant advancement in multimodal retrieval models, offering a unified solution for handling diverse types of information in real-world documents. 

<br /><br />Summary: <div>
arXiv:2510.03458v1 Announce Type: new 
Abstract: We present Omni-Embed-Nemotron, a unified multimodal retrieval embedding model developed to handle the increasing complexity of real-world information needs. While Retrieval-Augmented Generation (RAG) has significantly advanced language models by incorporating external knowledge, existing text-based retrievers rely on clean, structured input and struggle with the visually and semantically rich content found in real-world documents such as PDFs, slides, or videos. Recent work such as ColPali has shown that preserving document layout using image-based representations can improve retrieval quality. Building on this, and inspired by the capabilities of recent multimodal models such as Qwen2.5-Omni, we extend retrieval beyond text and images to also support audio and video modalities. Omni-Embed-Nemotron enables both cross-modal (e.g., text - video) and joint-modal (e.g., text - video+audio) retrieval using a single model. We describe the architecture, training setup, and evaluation results of Omni-Embed-Nemotron, and demonstrate its effectiveness in text, image, and video retrieval.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Searching for the Most Human-like Emergent Language</title>
<link>https://arxiv.org/abs/2510.03467</link>
<guid>https://arxiv.org/abs/2510.03467</guid>
<content:encoded><![CDATA[
<div> Keywords: signalling game, emergent communication, hyperparameter optimization, XferBench, entropy

Summary:
<br />
- The paper presents a signalling game-based emergent communication environment designed to create advanced emergent languages resembling human language.
- Hyperparameter optimization is used with XferBench to measure the statistical similarity of emergent language to human language.
- The study shows that entropy can predict the transfer learning performance of emergent language, indicating the importance of entropy in emergent communication systems.
- The research confirms previous findings on the entropy-minimization characteristics of emergent communication systems.
- Generalizations are made regarding hyperparameters that lead to more realistic emergent languages with better transferability to human language.<br /><br />Summary: <div>
arXiv:2510.03467v1 Announce Type: new 
Abstract: In this paper, we design a signalling game-based emergent communication environment to generate state-of-the-art emergent languages in terms of similarity to human language. This is done with hyperparameter optimization, using XferBench as the objective function. XferBench quantifies the statistical similarity of emergent language to human language by measuring its suitability for deep transfer learning to human language. Additionally, we demonstrate the predictive power of entropy on the transfer learning performance of emergent language as well as corroborate previous results on the entropy-minimization properties of emergent communication systems. Finally, we report generalizations regarding what hyperparameters produce more realistic emergent languages, that is, ones which transfer better to human language.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEER: The Span-based Emotion Evidence Retrieval Benchmark</title>
<link>https://arxiv.org/abs/2510.03490</link>
<guid>https://arxiv.org/abs/2510.03490</guid>
<content:encoded><![CDATA[
<div> SEER Benchmark, Emotion Evidence Retrieval, Large Language Models, span-based approach, emotion detection<br />
<br />
Summary:<br />
SEER introduces a benchmark to evaluate Large Language Models' ability to identify emotion-expressing text spans. Unlike traditional tasks, SEER focuses on pinpointing specific emotional phrases within sentences or short passages. This span-level approach is crucial for applications like empathetic dialogue and clinical support. The benchmark includes tasks of identifying emotion evidence within a single sentence and across five consecutive sentences, with annotations for emotion and evidence on 1200 real-world sentences. Evaluation of 14 LLMs shows varying performance, with models achieving close to human-level accuracy on single-sentence inputs but struggling with longer passages. Error analysis highlights issues like overreliance on emotion keywords and false positives in neutral text.<br /> <div>
arXiv:2510.03490v1 Announce Type: new 
Abstract: We introduce the SEER (Span-based Emotion Evidence Retrieval) Benchmark to test Large Language Models' (LLMs) ability to identify the specific spans of text that express emotion. Unlike traditional emotion recognition tasks that assign a single label to an entire sentence, SEER targets the underexplored task of emotion evidence detection: pinpointing which exact phrases convey emotion. This span-level approach is crucial for applications like empathetic dialogue and clinical support, which need to know how emotion is expressed, not just what the emotion is. SEER includes two tasks: identifying emotion evidence within a single sentence, and identifying evidence across a short passage of five consecutive sentences. It contains new annotations for both emotion and emotion evidence on 1200 real-world sentences. We evaluate 14 open-source LLMs and find that, while some models approach average human performance on single-sentence inputs, their accuracy degrades in longer passages. Our error analysis reveals key failure modes, including overreliance on emotion keywords and false positives in neutral text.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALHD: A Large-Scale and Multigenre Benchmark Dataset for Arabic LLM-Generated Text Detection</title>
<link>https://arxiv.org/abs/2510.03502</link>
<guid>https://arxiv.org/abs/2510.03502</guid>
<content:encoded><![CDATA[
<div> dataset, Arabic, LLM, detection, news<br />
<br />
ALHD is a new comprehensive Arabic dataset designed to differentiate between human- and LLM-generated texts. It covers news, social media, and reviews in both MSA and dialectal Arabic, with over 400K balanced samples from multiple sources. The dataset allows for studying generalizability in Arabic LLM text detection and includes rigorous preprocessing, annotations, and balanced splits for reproducibility. Benchmark experiments show that fine-tuned BERT models perform best, surpassing LLM-based models. However, challenges arise in cross-genre settings, particularly with news articles where LLM-generated texts closely resemble human writing. This highlights the need for further research in this area to improve detection accuracy and address risks of misinformation, academic dishonesty, and cyber threats.<br /><br />Summary: <div>
arXiv:2510.03502v1 Announce Type: new 
Abstract: We introduce ALHD, the first large-scale comprehensive Arabic dataset explicitly designed to distinguish between human- and LLM-generated texts. ALHD spans three genres (news, social media, reviews), covering both MSA and dialectal Arabic, and contains over 400K balanced samples generated by three leading LLMs and originated from multiple human sources, which enables studying generalizability in Arabic LLM-genearted text detection. We provide rigorous preprocessing, rich annotations, and standardized balanced splits to support reproducibility. In addition, we present, analyze and discuss benchmark experiments using our new dataset, in turn identifying gaps and proposing future research directions. Benchmarking across traditional classifiers, BERT-based models, and LLMs (zero-shot and few-shot) demonstrates that fine-tuned BERT models achieve competitive performance, outperforming LLM-based models. Results are however not always consistent, as we observe challenges when generalizing across genres; indeed, models struggle to generalize when they need to deal with unseen patterns in cross-genre settings, and these challenges are particularly prominent when dealing with news articles, where LLM-generated texts resemble human texts in style, which opens up avenues for future research. ALHD establishes a foundation for research related to Arabic LLM-detection and mitigating risks of misinformation, academic dishonesty, and cyber threats.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TS-Reasoner: Aligning Time Series Foundation Models with LLM Reasoning</title>
<link>https://arxiv.org/abs/2510.03519</link>
<guid>https://arxiv.org/abs/2510.03519</guid>
<content:encoded><![CDATA[
<div> TS-Reasoner, Time series reasoning, Latent representations, Training recipe, Large language models<br />
Summary:<br />
TS-Reasoner aims to bridge the gap between time series foundation models (TSFMs) and large language models (LLMs) for effective time series reasoning. It aligns the latent representations of TSFMs with textual inputs of LLMs through a novel training method using synthetic pairs of time series and textual captions. By applying instruction fine-tuning after alignment pretraining, TS-Reasoner outperforms prevailing LLMs, Vision Language Models (VLMs), and Time Series LLMs on various benchmarks with remarkable data efficiency. Unlike existing approaches, TS-Reasoner leverages a pretrained TSFM and freezes it during training, showcasing its effectiveness in integrating the two modalities for improved reasoning tasks. <div>
arXiv:2510.03519v1 Announce Type: new 
Abstract: Time series reasoning is crucial to decision-making in diverse domains, including finance, energy usage, traffic, weather, and scientific discovery. While existing time series foundation models (TSFMs) can capture low-level dynamic patterns and provide accurate forecasting, further analysis usually requires additional background knowledge and sophisticated reasoning, which are lacking in most TSFMs but can be achieved through large language models (LLMs). On the other hand, without expensive post-training, LLMs often struggle with the numerical understanding of time series data. Although it is intuitive to integrate the two types of models, developing effective training recipes that align the two modalities for reasoning tasks is still an open challenge. To this end, we propose TS-Reasoner that aligns the latent representations of TSFMs with the textual inputs of LLMs for downstream understanding/reasoning tasks. Specifically, we propose a simple yet effective method to curate diverse, synthetic pairs of time series and textual captions for alignment training. We then develop a two-stage training recipe that applies instruction finetuning after the alignment pretraining. Unlike existing works that train an LLM to take time series as inputs, we leverage a pretrained TSFM and freeze it during training. Extensive experiments on several benchmarks demonstrate that TS-Reasoner not only outperforms a wide range of prevailing LLMs, Vision Language Models (VLMs), and Time Series LLMs, but also achieves this with remarkable data efficiency, e.g., using less than half the training data.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying Financial Risk Information Using RAG with a Contrastive Insight</title>
<link>https://arxiv.org/abs/2510.03521</link>
<guid>https://arxiv.org/abs/2510.03521</guid>
<content:encoded><![CDATA[
<div> finance, risk, RAG, comparative inference, text generation<br />
<br />
Summary:<br />
In specialized reasoning tasks, humans often compare new problems with similar examples to draw context-specific insights. Traditional language model-based systems like RAG excel at extracting factual information but may provide generic outputs lacking context-specific nuances. To address this limitation in the finance domain, a peer-aware comparative inference layer is proposed on top of RAG. This approach outperforms baseline RAG models in text generation metrics such as ROUGE and BERTScore, showcasing improved performance in generating equity research and risk assessments similar to human-generated content. By incorporating a contrastive approach, the system can provide more bespoke and relevant insights in specialized domains, enhancing the capability of language models to generate context-specific information for complex problem-solving tasks. <div>
arXiv:2510.03521v1 Announce Type: new 
Abstract: In specialized domains, humans often compare new problems against similar examples, highlight nuances, and draw conclusions instead of analyzing information in isolation. When applying reasoning in specialized contexts with LLMs on top of a RAG, the pipeline can capture contextually relevant information, but it is not designed to retrieve comparable cases or related problems.
  While RAG is effective at extracting factual information, its outputs in specialized reasoning tasks often remain generic, reflecting broad facts rather than context-specific insights. In finance, it results in generic risks that are true for the majority of companies. To address this limitation, we propose a peer-aware comparative inference layer on top of RAG.
  Our contrastive approach outperforms baseline RAG in text generation metrics such as ROUGE and BERTScore in comparison with human-generated equity research and risk.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sample, Align, Synthesize: Graph-Based Response Synthesis with ConGrs</title>
<link>https://arxiv.org/abs/2510.03527</link>
<guid>https://arxiv.org/abs/2510.03527</guid>
<content:encoded><![CDATA[
<div> consensus graphs, language models, response variation, bioinformatics, biography generation<br />
Summary: <br />
The article introduces Consensus Graphs (ConGrs), a data structure that captures shared information and semantic variation in multiple responses generated by language models (LM) to the same prompt. ConGrs are constructed using a lexical sequence alignment algorithm and a secondary LM judge, and task-dependent decoding methods are designed to synthesize a final response. Experiments demonstrate that using ConGrs improves factual precision in biography generation tasks by up to 31% and reduces reliance on LM judges by over 80%. In refusing tasks, ConGrs increase abstention rates by up to 56%. Applying ConGrs to reasoning tasks like MATH and AIME shows improvements in accuracy over baseline methods. Overall, ConGrs provide a flexible and effective way to capture variation in LM responses and leverage this variation to generate more reliable and accurate responses.  <br /> <div>
arXiv:2510.03527v1 Announce Type: new 
Abstract: Language models can be sampled multiple times to access the distribution underlying their responses, but existing methods cannot efficiently synthesize rich epistemic signals across different long-form responses. We introduce Consensus Graphs (ConGrs), a flexible DAG-based data structure that represents shared information, as well as semantic variation in a set of sampled LM responses to the same prompt. We construct ConGrs using a light-weight lexical sequence alignment algorithm from bioinformatics, supplemented by the targeted usage of a secondary LM judge. Further, we design task-dependent decoding methods to synthesize a single, final response from our ConGr data structure. Our experiments show that synthesizing responses from ConGrs improves factual precision on two biography generation tasks by up to 31% over an average response and reduces reliance on LM judges by more than 80% compared to other methods. We also use ConGrs for three refusal-based tasks requiring abstention on unanswerable queries and find that abstention rate is increased by up to 56%. We apply our approach to the MATH and AIME reasoning tasks and find an improvement over self-verification and majority vote baselines by up to 6 points of accuracy. We show that ConGrs provide a flexible method for capturing variation in LM responses and using the epistemic signals provided by response variation to synthesize more effective responses.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning on Noisy Instructions: Effects on Generalization and Performance</title>
<link>https://arxiv.org/abs/2510.03528</link>
<guid>https://arxiv.org/abs/2510.03528</guid>
<content:encoded><![CDATA[
<div> Keywords: instruction-tuning, large language models, perturbations, resistance, performance<br />
Summary:<br />
Large language models (LLMs) benefit from instruction-tuning to improve task-solving abilities, but are sensitive to minor instruction variations. This study investigates the impact of introducing perturbations in instruction-tuning data on LLMs' resistance to noisy instructions. By perturbing instructions through methods like removing stop words or shuffling words, LLMs' performance on original and perturbed benchmarks (MMLU, BBH, GSM8K) was evaluated. Surprisingly, instruction-tuning with perturbed instructions showed potential improvements in downstream performance, indicating greater resilience to noisy inputs. These results underscore the significance of incorporating perturbed instructions in instruction-tuning processes to enhance LLMs' robustness and usability for generating helpful responses across various tasks.<br /><br />Summary: <div>
arXiv:2510.03528v1 Announce Type: new 
Abstract: Instruction-tuning plays a vital role in enhancing the task-solving abilities of large language models (LLMs), improving their usability in generating helpful responses on various tasks. However, previous work has demonstrated that they are sensitive to minor variations in instruction phrasing. In this paper, we explore whether introducing perturbations in instruction-tuning data can enhance LLMs' resistance against noisy instructions. We focus on how instruction-tuning with perturbations, such as removing stop words or shuffling words, affects LLMs' performance on the original and perturbed versions of widely-used benchmarks (MMLU, BBH, GSM8K). We further assess learning dynamics and potential shifts in model behavior. Surprisingly, our results suggest that instruction-tuning on perturbed instructions can, in some cases, improve downstream performance. These findings highlight the importance of including perturbed instructions in instruction-tuning, which can make LLMs more resilient to noisy user inputs.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TriMediQ: A Triplet-Structured Approach for Interactive Medical Question Answering</title>
<link>https://arxiv.org/abs/2510.03536</link>
<guid>https://arxiv.org/abs/2510.03536</guid>
<content:encoded><![CDATA[
<div> TriMediQ, large language models, knowledge graph, triplet-structured approach, clinical reasoning <br />
<br />
Summary: TriMediQ introduces a triplet-structured approach to enhance clinical reasoning in interactive medical Question Answer settings. It addresses the limitations of large language models (LLMs) by summarizing patient responses into triplets and integrating them into a Knowledge Graph (KG) for multi-hop reasoning. The frozen triplet generator extracts clinically relevant triplets, ensuring factual consistency. The trainable projection module fine-tunes with LLM weights frozen and guides multi-hop reasoning during inference. TriMediQ outperforms five baselines on the iMedQA dataset, achieving up to 10.4% improvement in accuracy. By converting patient responses into structured graphs, TriMediQ enables more accurate clinical reasoning in multi-turn settings, making it a promising solution for deploying LLM-based medical assistants. <br /><br /> <div>
arXiv:2510.03536v1 Announce Type: new 
Abstract: Large Language Models (LLMs) perform strongly in static and single-turn medical Question Answer (QA) benchmarks, yet such settings diverge from the iterative information gathering process required in practical clinical consultations. The MEDIQ framework addresses this mismatch by recasting the diagnosis as an interactive dialogue between a patient and an expert system, but the reliability of LLMs drops dramatically when forced to reason with dialogue logs, where clinical facts appear in sentences without clear links. To bridge this gap, we introduce TriMediQ, a triplet-structured approach that summarises patient responses into triplets and integrates them into a Knowledge Graph (KG), enabling multi-hop reasoning. We introduce a frozen triplet generator that extracts clinically relevant triplets, using prompts designed to ensure factual consistency. In parallel, a trainable projection module, comprising a graph encoder and a projector, captures relational information from the KG to enhance expert reasoning. TriMediQ operates in two steps: (i) the projection module fine-tuning with all LLM weights frozen; and (ii) using the fine-tuned module to guide multi-hop reasoning during inference. We evaluate TriMediQ on two interactive QA benchmarks, showing that it achieves up to 10.4\% improvement in accuracy over five baselines on the iMedQA dataset. These results demonstrate that converting patient responses into structured triplet-based graphs enables more accurate clinical reasoning in multi-turn settings, providing a solution for the deployment of LLM-based medical assistants.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What is a protest anyway? Codebook conceptualization is still a first-order concern in LLM-era classification</title>
<link>https://arxiv.org/abs/2510.03541</link>
<guid>https://arxiv.org/abs/2510.03541</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, text classification, computational social science, conceptualization errors, statistical inference

Summary: 
In the context of computational social science, the use of generative large language models (LLMs) for text classification has become prevalent. This study highlights the importance of not overlooking the steps of conceptualization and post-prompting analysis in utilizing LLMs for classification tasks. The authors suggest that analysts may be tempted to skip the crucial step of conceptualization, leading to errors that bias downstream estimates. Through simulations, it is demonstrated that this bias cannot be remedied by simply improving LLM accuracy or applying post-hoc correction methods. The study concludes by emphasizing the significance of proper conceptualization in the LLM era, providing practical guidance for achieving unbiased and low-variance estimates in computational social science research. 

<br /><br />Summary: <div>
arXiv:2510.03541v1 Announce Type: new 
Abstract: Generative large language models (LLMs) are now used extensively for text classification in computational social science (CSS). In this work, focus on the steps before and after LLM prompting -- conceptualization of concepts to be classified and using LLM predictions in downstream statistical inference -- which we argue have been overlooked in much of LLM-era CSS. We claim LLMs can tempt analysts to skip the conceptualization step, creating conceptualization errors that bias downstream estimates. Using simulations, we show that this conceptualization-induced bias cannot be corrected for solely by increasing LLM accuracy or post-hoc bias correction methods. We conclude by reminding CSS analysts that conceptualization is still a first-order concern in the LLM-era and provide concrete advice on how to pursue low-cost, unbiased, low-variance downstream estimates.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CCD-Bench: Probing Cultural Conflict in Large Language Model Decision-Making</title>
<link>https://arxiv.org/abs/2510.03553</link>
<guid>https://arxiv.org/abs/2510.03553</guid>
<content:encoded><![CDATA[
<div> Benchmark, Large language models, Cross-cultural value conflict, Decision-making, Cultural clusters <br />
Summary: 
The article introduces a new benchmark, CCD-Bench, to assess large language models' (LLMs) decision-making in scenarios of cross-cultural value conflict. The benchmark consists of 2,182 dilemmas across seven domains, each with response options related to ten GLOBE cultural clusters. Evaluation of 17 LLMs shows a preference for Nordic and Germanic Europe, while underrepresenting Eastern Europe and the Middle East and North Africa. Despite referencing multiple cultural dimensions in rationales, the models lack depth in certain values like Assertiveness and Gender Egalitarianism. There are minimal ordering effects in the dilemmas, and model decisions are clustered by developer lineage rather than geography. The study highlights the need for alignment strategies that encompass diverse worldviews and move beyond isolated bias detection towards pluralistic decision-making. <br /> <div>
arXiv:2510.03553v1 Announce Type: new 
Abstract: Although large language models (LLMs) are increasingly implicated in interpersonal and societal decision-making, their ability to navigate explicit conflicts between legitimately different cultural value systems remains largely unexamined. Existing benchmarks predominantly target cultural knowledge (CulturalBench), value prediction (WorldValuesBench), or single-axis bias diagnostics (CDEval); none evaluate how LLMs adjudicate when multiple culturally grounded values directly clash. We address this gap with CCD-Bench, a benchmark that assesses LLM decision-making under cross-cultural value conflict. CCD-Bench comprises 2,182 open-ended dilemmas spanning seven domains, each paired with ten anonymized response options corresponding to the ten GLOBE cultural clusters. These dilemmas are presented using a stratified Latin square to mitigate ordering effects. We evaluate 17 non-reasoning LLMs. Models disproportionately prefer Nordic Europe (mean 20.2 percent) and Germanic Europe (12.4 percent), while options for Eastern Europe and the Middle East and North Africa are underrepresented (5.6 to 5.8 percent). Although 87.9 percent of rationales reference multiple GLOBE dimensions, this pluralism is superficial: models recombine Future Orientation and Performance Orientation, and rarely ground choices in Assertiveness or Gender Egalitarianism (both under 3 percent). Ordering effects are negligible (Cramer's V less than 0.10), and symmetrized KL divergence shows clustering by developer lineage rather than geography. These patterns suggest that current alignment pipelines promote a consensus-oriented worldview that underserves scenarios demanding power negotiation, rights-based reasoning, or gender-aware analysis. CCD-Bench shifts evaluation beyond isolated bias detection toward pluralistic decision making and highlights the need for alignment strategies that substantively engage diverse worldviews.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models</title>
<link>https://arxiv.org/abs/2510.03561</link>
<guid>https://arxiv.org/abs/2510.03561</guid>
<content:encoded><![CDATA[
<div> Transformer architecture, Large Language Models, conversational AI, Reactive Transformer, real-time processing

Summary: 
The paper introduces the Reactive Transformer (RxT), a new architecture for conversational AI that addresses the limitations of the stateless nature and quadratic computational complexity of traditional Transformer models. RxT operates in real-time, treating each conversational turn as a discrete event and maintaining context in a fixed-size Short-Term Memory (STM) system. By decoupling response generation from memory updates, RxT reduces the total user-facing cost of a conversation from quadratic to linear with respect to the number of interactions. This architecture enables low latency, making long-form conversations economically viable and truly real-time. Experimental results on synthetic data demonstrate that RxT outperforms a baseline stateless model of similar size in terms of performance and constant-time inference latency. <div>
arXiv:2510.03561v1 Announce Type: new 
Abstract: The Transformer architecture has become the de facto standard for Large Language Models (LLMs), demonstrating remarkable capabilities in language understanding and generation. However, its application in conversational AI is fundamentally constrained by its stateless nature and the quadratic computational complexity ($O(L^2)$) with respect to sequence length $L$. Current models emulate memory by reprocessing an ever-expanding conversation history with each turn, leading to prohibitive costs and latency in long dialogues. This paper introduces the Reactive Transformer (RxT), a novel architecture designed to overcome these limitations by shifting from a data-driven to an event-driven paradigm. RxT processes each conversational turn as a discrete event in real-time, maintaining context in an integrated, fixed-size Short-Term Memory (STM) system. The architecture features a distinct operational cycle where a generator-decoder produces a response based on the current query and the previous memory state, after which a memory-encoder and a dedicated Memory Attention network asynchronously update the STM with a representation of the complete interaction. This design fundamentally alters the scaling dynamics, reducing the total user-facing cost of a conversation from quadratic ($O(N^2 \cdot T)$) to linear ($O(N \cdot T)$) with respect to the number of interactions $N$. By decoupling response generation from memory updates, RxT achieves low latency, enabling truly real-time, stateful, and economically viable long-form conversations. We validated our architecture with a series of proof-of-concept experiments on synthetic data, demonstrating superior performance and constant-time inference latency compared to a baseline stateless model of comparable size.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM, Reporting In! Medical Information Extraction Across Prompting, Fine-tuning and Post-correction</title>
<link>https://arxiv.org/abs/2510.03577</link>
<guid>https://arxiv.org/abs/2510.03577</guid>
<content:encoded><![CDATA[
<div> Approaches, Large Language Models, Biomedical Named Entity Recognition, Health Event Extraction, French <br />
Summary:
This work describes the participation in the EvalLLM 2025 challenge focused on biomedical Named Entity Recognition (NER) and health event extraction in French with limited data. Three approaches were proposed using large language models (LLMs), annotation guidelines, synthetic data, and post-processing. The first approach, in-context learning (ICL) with GPT-4.1, integrated automatic selection of examples and guideline summaries in the prompt. The second approach involved the universal NER system GLiNER, fine-tuned on a synthetic corpus and validated by an LLM in post-processing. The third approach utilized the open LLM LLaMA-3.1-8B-Instruct, fine-tuned on a synthetic corpus. Event extraction also applied the ICL strategy with GPT-4.1, reusing the guideline summary in the prompt. Results showed GPT-4.1 achieved the highest macro-F1 scores for NER and event extraction, emphasizing the significance of well-crafted prompting for optimal performance in extremely low-resource settings. <br /><br />Summary: <div>
arXiv:2510.03577v1 Announce Type: new 
Abstract: This work presents our participation in the EvalLLM 2025 challenge on biomedical Named Entity Recognition (NER) and health event extraction in French (few-shot setting). For NER, we propose three approaches combining large language models (LLMs), annotation guidelines, synthetic data, and post-processing: (1) in-context learning (ICL) with GPT-4.1, incorporating automatic selection of 10 examples and a summary of the annotation guidelines into the prompt, (2) the universal NER system GLiNER, fine-tuned on a synthetic corpus and then verified by an LLM in post-processing, and (3) the open LLM LLaMA-3.1-8B-Instruct, fine-tuned on the same synthetic corpus. Event extraction uses the same ICL strategy with GPT-4.1, reusing the guideline summary in the prompt. Results show GPT-4.1 leads with a macro-F1 of 61.53% for NER and 15.02% for event extraction, highlighting the importance of well-crafted prompting to maximize performance in very low-resource scenarios.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoupling Task-Solving and Output Formatting in LLM Generation</title>
<link>https://arxiv.org/abs/2510.03595</link>
<guid>https://arxiv.org/abs/2510.03595</guid>
<content:encoded><![CDATA[
<div> Deco-G, decoding framework, format compliance, task-solving, instruction-tuned LLMs<br />

Summary:<br />
Deco-G is introduced as a decoding framework that separates format adherence from task solving in large language models (LLMs). It combines next token probabilities from LLMs with a separate probabilistic model for format compliance. The framework enhances performance across tasks with diverse format requirements such as mathematical reasoning, LLM-as-a-judge, and event argument extraction. Three key innovations in instruction-aware distillation, trie-building algorithm, and HMM state pruning contribute to the scalability and efficiency of Deco-G. The approach demonstrates a 1.0% to 6.0% relative gain over regular prompting practices while ensuring format compliance. <br /><br /> <div>
arXiv:2510.03595v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly adept at following instructions containing task descriptions to solve complex problems, such as mathematical reasoning and automatic evaluation (LLM-as-a-Judge). However, as prompts grow more complex, models often struggle to adhere to all instructions. This difficulty is especially common when instructive prompts intertwine reasoning directives -- specifying what the model should solve -- with rigid formatting requirements that dictate how the solution must be presented. The entanglement creates competing goals for the model, suggesting that more explicit separation of these two aspects could lead to improved performance. To this front, we introduce Deco-G, a decoding framework that explicitly decouples format adherence from task solving. Deco-G handles format compliance with a separate tractable probabilistic model (TPM), while prompts LLMs with only task instructions. At each decoding step, Deco-G combines next token probabilities from the LLM with the TPM calculated format compliance likelihood to form the output probability. To make this approach both practical and scalable for modern instruction-tuned LLMs, we introduce three key innovations: instruction-aware distillation, a flexible trie-building algorithm, and HMM state pruning for computational efficiency. We demonstrate the effectiveness of Deco-G across a wide range of tasks with diverse format requirements, including mathematical reasoning, LLM-as-a-judge, and event argument extraction. Overall, our approach yields 1.0% to 6.0% relative gain over regular prompting practice with guaranteed format compliance.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can an LLM Induce a Graph? Investigating Memory Drift and Context Length</title>
<link>https://arxiv.org/abs/2510.03611</link>
<guid>https://arxiv.org/abs/2510.03611</guid>
<content:encoded><![CDATA[
<div> benchmarks, large language models, relational reasoning, memory drift, long-range reasoning

Summary: 
This article discusses the limitations of existing evaluation benchmarks for large language models (LLMs) in measuring their performance in complex reasoning tasks. The authors argue for evaluating LLMs on tasks that require structured relational knowledge induction, such as reasoning with graphs derived from text. Results show that LLMs exhibit memory drift and contextual forgetting at shorter effective lengths when tasked with relational reasoning. Even specialized reasoning models like OpenAI o1 display vulnerabilities to early memory drift in these scenarios. The findings suggest a need for architectural adaptations in LLMs to improve long-range reasoning capabilities. <div>
arXiv:2510.03611v1 Announce Type: new 
Abstract: Recently proposed evaluation benchmarks aim to characterize the effective context length and the forgetting tendencies of large language models (LLMs). However, these benchmarks often rely on simplistic 'needle in a haystack' retrieval or continuation tasks that may not accurately reflect the performance of these models in information-dense scenarios. Thus, rather than simple next token prediction, we argue for evaluating these models on more complex reasoning tasks that requires them to induce structured relational knowledge from the text - such as graphs from potentially noisy natural language content. While the input text can be viewed as generated in terms of a graph, its structure is not made explicit and connections must be induced from distributed textual cues, separated by long contexts and interspersed with irrelevant information. Our findings reveal that LLMs begin to exhibit memory drift and contextual forgetting at much shorter effective lengths when tasked with this form of relational reasoning, compared to what existing benchmarks suggest. With these findings, we offer recommendations for the optimal use of popular LLMs for complex reasoning tasks. We further show that even models specialized for reasoning, such as OpenAI o1, remain vulnerable to early memory drift in these settings. These results point to significant limitations in the models' ability to abstract structured knowledge from unstructured input and highlight the need for architectural adaptations to improve long-range reasoning.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Unsupervised Speech Recognition at the Syllable-Level</title>
<link>https://arxiv.org/abs/2510.03639</link>
<guid>https://arxiv.org/abs/2510.03639</guid>
<content:encoded><![CDATA[
<div> framework, UASR, speech recognition, unsupervised, syllable<br />
<br />
Summary: <br />
The paper introduces a syllable-level Unsupervised Speech Recognition (UASR) framework for training speech recognizers without the need for grapheme-to-phoneme converters (G2Ps) or GAN-based methods. This approach aims to improve generalization and reduce training instability, achieving a significant relative reduction in character error rate (CER) on the LibriSpeech dataset. The framework is designed to address the challenges of extending Automatic Speech Recognition (ASR) to low-resource languages in the long-tail distribution and enabling multimodal learning from non-parallel data. The method shows promising results for languages with ambiguous phoneme boundaries like Mandarin, which have been difficult for previous approaches. The code for the framework will be made available upon acceptance, providing a valuable resource for further research and development in the field of unsupervised speech recognition. <br /> <div>
arXiv:2510.03639v1 Announce Type: new 
Abstract: Training speech recognizers with unpaired speech and text -- known as unsupervised speech recognition (UASR) -- is a crucial step toward extending ASR to low-resource languages in the long-tail distribution and enabling multimodal learning from non-parallel data. However, existing approaches based on phones often rely on costly resources such as grapheme-to-phoneme converters (G2Ps) and struggle to generalize to languages with ambiguous phoneme boundaries due to training instability. In this paper, we address both challenges by introducing a syllable-level UASR framework based on masked language modeling, which avoids the need for G2P and the instability of GAN-based methods. Our approach achieves up to a 40\% relative reduction in character error rate (CER) on LibriSpeech and generalizes effectively to Mandarin, a language that has remained particularly difficult for prior methods. Code will be released upon acceptance.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG</title>
<link>https://arxiv.org/abs/2510.03663</link>
<guid>https://arxiv.org/abs/2510.03663</guid>
<content:encoded><![CDATA[
<div> Benchmarking, Multimodal retrieval-augmented generation, Large language models, Real-world knowledge bases, UniDoc-Bench  
Summary:  
- Introduction of UniDoc-Bench, a realistic benchmark for Multimodal retrieval-augmented generation (MM-RAG) using real-world PDF pages.  
- Generation of 1,600 multimodal QA pairs covering factual retrieval, comparison, summarization, and logical reasoning queries.  
- Comparison of four paradigms: text-only, image-only, multimodal text-image fusion, and multimodal joint retrieval under a unified protocol.  
- Multimodal text-image fusion RAG systems outperform unimodal and joint embedding-based retrieval, highlighting the importance of combining text and visual data.  
- Analysis of visual context's role in complementing textual evidence, identification of failure modes, and offering actionable guidance for improving MM-RAG pipelines.  
<br /><br />Summary: <div>
arXiv:2510.03663v1 Announce Type: new 
Abstract: Multimodal retrieval-augmented generation (MM-RAG) is a key approach for applying large language models (LLMs) and agents to real-world knowledge bases, yet current evaluations are fragmented, focusing on either text or images in isolation or on simplified multimodal setups that fail to capture document-centric multimodal use cases. In this paper, we introduce UniDoc-Bench, the first large-scale, realistic benchmark for MM-RAG built from 70k real-world PDF pages across eight domains. Our pipeline extracts and links evidence from text, tables, and figures, then generates 1,600 multimodal QA pairs spanning factual retrieval, comparison, summarization, and logical reasoning queries. To ensure reliability, 20% of QA pairs are validated by multiple annotators and expert adjudication. UniDoc-Bench supports apples-to-apples comparison across four paradigms: (1) text-only, (2) image-only, (3) multimodal text-image fusion, and (4) multimodal joint retrieval -- under a unified protocol with standardized candidate pools, prompts, and evaluation metrics. Our experiments show that multimodal text-image fusion RAG systems consistently outperform both unimodal and jointly multimodal embedding-based retrieval, indicating that neither text nor images alone are sufficient and that current multimodal embeddings remain inadequate. Beyond benchmarking, our analysis reveals when and how visual context complements textual evidence, uncovers systematic failure modes, and offers actionable guidance for developing more robust MM-RAG pipelines.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning Large Language Models with QLoRA for Offensive Language Detection in Roman Urdu-English Code-Mixed Text</title>
<link>https://arxiv.org/abs/2510.03683</link>
<guid>https://arxiv.org/abs/2510.03683</guid>
<content:encoded><![CDATA[
<div> propose, QLoRA, offensive language detection, Roman Urdu, fine tuning <br />
Summary: 
The study focuses on improving offensive language detection in Roman Urdu-English code mixed text using a QLoRA-based fine-tuning framework to address the challenges posed by derogatory terms. The researchers translated the dataset into English to leverage English language models, fine-tuning several large language models including Meta LLaMA 3 8B and Mistral 7B for memory-efficient adaptation. Testing on a manually annotated Roman Urdu dataset, the models achieved impressive F1 scores, with Meta LLaMA 3 8B and Mistral 7B outperforming traditional transformer baselines at 91.45 and 89.66 F1 scores respectively. The results highlight the effectiveness of QLoRA in enhancing model performance for low-resource environments like code-mixed offensive language detection and demonstrate the potential of large language models for this task. This work sets the groundwork for scalable Roman Urdu moderation and opens doors for future multilingual offensive detection systems utilizing large language models. <br />
Summary: <div>
arXiv:2510.03683v1 Announce Type: new 
Abstract: The use of derogatory terms in languages that employ code mixing, such as Roman Urdu, presents challenges for Natural Language Processing systems due to unstated grammar, inconsistent spelling, and a scarcity of labeled data. In this work, we propose a QLoRA based fine tuning framework to improve offensive language detection in Roman Urdu-English text. We translated the Roman Urdu-English code mixed dataset into English using Google Translate to leverage English LLMs, while acknowledging that this translation reduces direct engagement with code mixing features. Our focus is on classification performance using English translated low resource inputs. We fine tuned several transformers and large language models, including Meta LLaMA 3 8B, Mistral 7B v0.1, LLaMA 2 7B, ModernBERT, and RoBERTa, with QLoRA for memory efficient adaptation. Models were trained and evaluated on a manually annotated Roman Urdu dataset for offensive vs non offensive content. Of all tested models, the highest F1 score of 91.45 was attained by Meta LLaMA 3 8B, followed by Mistral 7B at 89.66, surpassing traditional transformer baselines. These results demonstrate the efficacy of QLoRA in fine tuning high performing models for low resource environments such as code mixed offensive language detection, and confirm the potential of LLMs for this task. This work advances a scalable approach to Roman Urdu moderation and paves the way for future multilingual offensive detection systems based on LLMs.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedReflect: Teaching Medical LLMs to Self-Improve via Reflective Correction</title>
<link>https://arxiv.org/abs/2510.03687</link>
<guid>https://arxiv.org/abs/2510.03687</guid>
<content:encoded><![CDATA[
<div> framework, LLMs, medical problem-solving, self-reflection, dataset construction

Summary:
MedReflect is a new framework designed to enhance the problem-solving capabilities of large language models (LLMs) in the medical field. It enables LLMs to engage in reflective thinking similar to that of a physician, generating a self-verified reflection chain that includes hypothesis generation, self-questioning, self-answering, and decision refinement. By leveraging this reflective mode, LLMs can solve specialized medical problems without the need for external retrieval or heavy annotation. With just 2,000 training examples and light fine-tuning, MedReflect demonstrates significant accuracy improvements across various medical benchmarks while reducing annotation requirements. This approach highlights the potential of LLMs to learn and improve in medical problem-solving through self-reflection, reducing the reliance on external supervision and extensive fine-tuning data.<br /><br />Summary: <div>
arXiv:2510.03687v1 Announce Type: new 
Abstract: Medical problem solving demands expert knowledge and intricate reasoning. Recent studies of large language models (LLMs) attempt to ease this complexity by introducing external knowledge verification through retrieval-augmented generation or by training on reasoning datasets. However, these approaches suffer from drawbacks such as retrieval overhead and high annotation costs, and they heavily rely on substituted external assistants to reach limited performance in medical field. In this paper, we introduce MedReflect, a generalizable framework designed to inspire LLMs with a physician-like reflective thinking mode. MedReflect generates a single-pass reflection chain that includes initial hypothesis generation, self-questioning, self-answering and decision refinement. This self-verified and self-reflective nature releases large language model's latent capability in medical problem-solving without external retrieval or heavy annotation. We demonstrate that MedReflect enables cost-efficient medical dataset construction: with merely 2,000 randomly sampled training examples and a light fine-tuning, this approach achieves notable absolute accuracy improvements across a series of medical benchmarks while cutting annotation requirements. Our results provide evidence that LLMs can learn to solve specialized medical problems via self-reflection and self-improve, reducing reliance on external supervision and extensive task-specific fine-tuning data.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TreePrompt: Leveraging Hierarchical Few-Shot Example Selection for Improved English-Persian and English-German Translation</title>
<link>https://arxiv.org/abs/2510.03748</link>
<guid>https://arxiv.org/abs/2510.03748</guid>
<content:encoded><![CDATA[
<div> TreePrompt, LLM, machine translation, few-shot prompting, example selection<br />
Summary:<br />
Large Language Models (LLMs) are effective in machine translation, especially with proper prompts. Existing methods for example selection in few-shot prompting lack consideration of example quality. The TreePrompt approach presented in this work uses a tree structure to identify high-quality, contextually relevant examples based on LLM preferences. Combining TreePrompt with K-Nearest Neighbors (K-NN) and Adaptive Few-Shot Prompting (AFSP) enhances translation performance, as demonstrated in evaluations on English-Persian (MIZAN) and English-German (WMT19) language pairs. Integration of TreePrompt with AFSP or Random selection results in improved translation quality. <br /><br />Summary: <div>
arXiv:2510.03748v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have consistently demonstrated strong performance in machine translation, especially when guided by high-quality prompts. Few-shot prompting is an effective technique to improve translation quality; however, most existing example selection methods focus solely on query-to-example similarity and do not account for the quality of the examples. In this work, we propose TreePrompt, a novel example selection approach that learns LLM preferences to identify high-quality, contextually relevant examples within a tree-structured framework. To further explore the balance between similarity and quality, we combine TreePrompt with K-Nearest Neighbors (K-NN) and Adaptive Few-Shot Prompting (AFSP). Evaluations on two language pairs - English-Persian (MIZAN) and English-German (WMT19) - show that integrating TreePrompt with AFSP or Random selection leads to improved translation performance.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Lingual Multi-Granularity Framework for Interpretable Parkinson's Disease Diagnosis from Speech</title>
<link>https://arxiv.org/abs/2510.03758</link>
<guid>https://arxiv.org/abs/2510.03758</guid>
<content:encoded><![CDATA[
<div> Keywords: Parkinson's Disease, speech impairments, phoneme-level analysis, bidirectional LSTM, attention analysis

Summary:
The article introduces a novel approach for detecting Parkinson's Disease (PD) using speech data. PD commonly affects speech, and the proposed system focuses on analyzing specific phonetic elements at different levels of granularity such as phonemes, syllables, and words. The system was tested on datasets in Italian, Spanish, and English, showing promising results in PD detection. The phoneme-level analysis outperformed other granularity levels, achieving high accuracy rates. Importantly, the system revealed that certain speech features, such as sustained vowels and specific syllables, were highly informative for PD diagnosis. The attention analysis further confirmed the relevance of these features, aligning with established clinical protocols. The study highlights the potential of using speech data for accurate cross-linguistic PD detection, offering a new perspective in leveraging speech analysis as a diagnostic tool. Source code for the proposed system will be available for further research and development. 

Summary: <br /><br /> <div>
arXiv:2510.03758v1 Announce Type: new 
Abstract: Parkinson's Disease (PD) affects over 10 million people worldwide, with speech impairments in up to 89% of patients. Current speech-based detection systems analyze entire utterances, potentially overlooking the diagnostic value of specific phonetic elements. We developed a granularity-aware approach for multilingual PD detection using an automated pipeline that extracts time-aligned phonemes, syllables, and words from recordings. Using Italian, Spanish, and English datasets, we implemented a bidirectional LSTM with multi-head attention to compare diagnostic performance across the different granularity levels. Phoneme-level analysis achieved superior performance with AUROC of 93.78% +- 2.34% and accuracy of 92.17% +- 2.43%. This demonstrates enhanced diagnostic capability for cross-linguistic PD detection. Importantly, attention analysis revealed that the most informative speech features align with those used in established clinical protocols: sustained vowels (/a/, /e/, /o/, /i/) at phoneme level, diadochokinetic syllables (/ta/, /pa/, /la/, /ka/) at syllable level, and /pataka/ sequences at word level. Source code will be available at https://github.com/jetliqs/clearpd.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt Balance Matters: Understanding How Imbalanced Few-Shot Learning Affects Multilingual Sense Disambiguation in LLMs</title>
<link>https://arxiv.org/abs/2510.03762</link>
<guid>https://arxiv.org/abs/2510.03762</guid>
<content:encoded><![CDATA[
<div> Few-shot prompting, Large Language Models, Word Sense Disambiguation, imbalanced sample distributions, GLOSSGPT prompting method <br />
Summary:<br />
The study explores the impact of few-shot prompting strategies on Word Sense Disambiguation (WSD) tasks across five languages. It focuses on the effectiveness of the GLOSSGPT prompting method in English, German, Spanish, French, and Italian. Results show that imbalanced few-shot examples can lead to incorrect sense predictions in multilingual languages but not in English. Evaluation of GPT-4o and LLaMA-3.1-70B models underscores the sensitivity of multilingual WSD to sample distribution in few-shot settings. This highlights the necessity for balanced and representative prompting strategies to improve accuracy in multilingual WSD tasks. <br /><br />Summary: <div>
arXiv:2510.03762v1 Announce Type: new 
Abstract: Recent advances in Large Language Models (LLMs) have significantly reshaped the landscape of Natural Language Processing (NLP). Among the various prompting techniques, few-shot prompting has gained considerable attention for its practicality and effectiveness. This study investigates how few-shot prompting strategies impact the Word Sense Disambiguation (WSD) task, particularly focusing on the biases introduced by imbalanced sample distributions. We use the GLOSSGPT prompting method, an advanced approach for English WSD, to test its effectiveness across five languages: English, German, Spanish, French, and Italian. Our results show that imbalanced few-shot examples can cause incorrect sense predictions in multilingual languages, but this issue does not appear in English. To assess model behavior, we evaluate both the GPT-4o and LLaMA-3.1-70B models and the results highlight the sensitivity of multilingual WSD to sample distribution in few-shot settings, emphasizing the need for balanced and representative prompting strategies.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rezwan: Leveraging Large Language Models for Comprehensive Hadith Text Processing: A 1.2M Corpus Development</title>
<link>https://arxiv.org/abs/2510.03781</link>
<guid>https://arxiv.org/abs/2510.03781</guid>
<content:encoded><![CDATA[
<div> Keywords: Rezwan, Hadith corpus, AI-assisted, large-scale, Islamic studies

Summary:
Rezwan is a large-scale AI-assisted Hadith corpus with over 1.2M narrations, created through a fully automated pipeline. The pipeline utilizes Large Language Models for segmentation, validation, and enrichment of the narrations. Each narration is enhanced with machine translation, diacritization, summarization, tagging, and semantic analysis. Evaluation by domain experts showed near-human accuracy in tasks such as chain-text separation and summarization. The Najm Corpus outperformed the manually curated Noor Corpus in both scale and quality. Cost analysis demonstrated the economic feasibility of the AI approach compared to expert labor. This work showcases how AI can enhance human expertise, enabling access to Islamic heritage on a large scale. 

Summary: 
This paper introduces Rezwan, a massive AI-driven Hadith corpus, developed through automated processes using Large Language Models for annotation and enrichment. The evaluation by domain experts showcased the accuracy and quality of the corpus, outperforming manually curated datasets in scale and content. The cost analysis highlighted the economic efficiency of the AI approach compared to labor-intensive manual work. This work signifies a paradigm shift in religious text processing, demonstrating the potential of AI to revolutionize access to Islamic heritage on a global scale. 

Summary: <div>
arXiv:2510.03781v1 Announce Type: new 
Abstract: This paper presents the development of Rezwan, a large-scale AI-assisted Hadith corpus comprising over 1.2M narrations, extracted and structured through a fully automated pipeline. Building on digital repositories such as Maktabat Ahl al-Bayt, the pipeline employs Large Language Models (LLMs) for segmentation, chain--text separation, validation, and multi-layer enrichment. Each narration is enhanced with machine translation into twelve languages, intelligent diacritization, abstractive summarization, thematic tagging, and cross-text semantic analysis. This multi-step process transforms raw text into a richly annotated research-ready infrastructure for digital humanities and Islamic studies. A rigorous evaluation was conducted on 1,213 randomly sampled narrations, assessed by six domain experts. Results show near-human accuracy in structured tasks such as chain--text separation (9.33/10) and summarization (9.33/10), while highlighting ongoing challenges in diacritization and semantic similarity detection. Comparative analysis against the manually curated Noor Corpus demonstrates the superiority of Najm in both scale and quality, with a mean overall score of 8.46/10 versus 3.66/10. Furthermore, cost analysis confirms the economic feasibility of the AI approach: tasks requiring over 229,000 hours of expert labor were completed within months at a fraction of the cost. The work introduces a new paradigm in religious text processing by showing how AI can augment human expertise, enabling large-scale, multilingual, and semantically enriched access to Islamic heritage.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mechanistic Interpretability of Socio-Political Frames in Language Models</title>
<link>https://arxiv.org/abs/2510.03799</link>
<guid>https://arxiv.org/abs/2510.03799</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, deep cognitive frames, socio-political contexts, interpretability research, hidden representation

Summary: 
Language models (LLMs) have shown a remarkable ability to generate and recognize deep cognitive frames, particularly in socio-political contexts. The study demonstrates that LLMs excel in producing texts that evoke specific frames and can identify these frames in zero-shot scenarios. By drawing inspiration from mechanistic interpretability research, the researchers delve into the analysis of where the 'strict father' and 'nurturing parent' frames are located within the hidden representation of the model. They identify distinct dimensions that exhibit strong correlations with the presence of these frames. These findings shed light on how LLMs capture and articulate meaningful human concepts, providing valuable insights into the inner workings of these complex models. 

<br /><br />Summary: <div>
arXiv:2510.03799v1 Announce Type: new 
Abstract: This paper explores the ability of large language models to generate and recognize deep cognitive frames, particularly in socio-political contexts. We demonstrate that LLMs are highly fluent in generating texts that evoke specific frames and can recognize these frames in zero-shot settings. Inspired by mechanistic interpretability research, we investigate the location of the `strict father' and `nurturing parent' frames within the model's hidden representation, identifying singular dimensions that correlate strongly with their presence. Our findings contribute to understanding how LLMs capture and express meaningful human concepts.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Token Length: Step Pruner for Efficient and Accurate Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2510.03805</link>
<guid>https://arxiv.org/abs/2510.03805</guid>
<content:encoded><![CDATA[
<div> RL framework, Large Reasoning Models, Step Pruner, efficient reasoning, dynamic stopping mechanism <br />
Summary:
The article introduces the Step Pruner (SP) framework to enhance the efficiency of Large Reasoning Models (LRMs). SP uses a step-aware reward function to prioritize correctness and penalize redundant reasoning steps. It also includes a dynamic stopping mechanism to prevent hacking behavior in training. Experimental results show that SP achieves high accuracy on reasoning benchmarks while significantly reducing response length. For example, on AIME24, SP reduces token usage by 69.7%. This approach addresses challenges faced by existing RL solutions in promoting concise reasoning in LRMs, demonstrating state-of-the-art performance in accuracy with reduced verbosity. <br /><br /> <div>
arXiv:2510.03805v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex tasks but often suffer from excessive verbosity, known as "overthinking." Existing solutions via reinforcement learning (RL) typically penalize generated tokens to promote conciseness. However, these methods encounter two challenges: responses with fewer tokens do not always correspond to fewer reasoning steps, and models may develop hacking behavior in later stages of training by discarding reasoning steps to minimize token usage. In this work, we introduce \textbf{Step Pruner (SP)}, an RL framework that steers LRMs toward more efficient reasoning by favoring compact reasoning steps. Our step-aware reward function prioritizes correctness while imposing penalties for redundant steps, and withholds rewards for incorrect responses to prevent the reinforcement of erroneous reasoning. Moreover, we propose a dynamic stopping mechanism: when the length of any output step exceeds the upper limit, we halt updates to prevent hacking behavior caused by merging steps. Extensive experiments across four reasoning benchmarks demonstrate that SP achieves state-of-the-art accuracy while significantly reducing response length. For instance, on AIME24, SP reduces token usage by \textbf{69.7\%}.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Annotate Rhetorical Relations with INCEpTION: A Comparison with Automatic Approaches</title>
<link>https://arxiv.org/abs/2510.03808</link>
<guid>https://arxiv.org/abs/2510.03808</guid>
<content:encoded><![CDATA[
<div> BERT, DistilBERT, Logistic Regression, Rhetorical Structure Theory, INCEpTION <br />
Summary:<br />
This research examines the annotation of rhetorical relations in discourse, specifically focusing on sports reports, particularly cricket news. The study compares manual annotation with automatic approaches using BERT, DistilBERT, and Logistic Regression models. The analysis evaluates the performance of these models in classifying rhetorical relations such as elaboration, contrast, background, and cause-effect. The results indicate that DistilBERT achieves the highest accuracy among the models tested, demonstrating its potential for efficient discourse relation prediction. This work contributes to the research on discourse parsing and transformer-based NLP, showcasing the effectiveness of using advanced language models in understanding complex textual relationships. Conducted under the supervision of Prof. Dr. Ralf Klabunde at Ruhr University Bochum, this study enhances our understanding of discourse structure and NLP applications in sports reporting. <br /> <div>
arXiv:2510.03808v1 Announce Type: new 
Abstract: This research explores the annotation of rhetorical relations in discourse using the INCEpTION tool and compares manual annotation with automatic approaches based on large language models. The study focuses on sports reports (specifically cricket news) and evaluates the performance of BERT, DistilBERT, and Logistic Regression models in classifying rhetorical relations such as elaboration, contrast, background, and cause-effect. The results show that DistilBERT achieved the highest accuracy, highlighting its potential for efficient discourse relation prediction. This work contributes to the growing intersection of discourse parsing and transformer-based NLP. (This paper was conducted as part of an academic requirement under the supervision of Prof. Dr. Ralf Klabunde, Linguistic Data Science Lab, Ruhr University Bochum.) Keywords: Rhetorical Structure Theory, INCEpTION, BERT, DistilBERT, Discourse Parsing, NLP.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Read Between the Lines: A Benchmark for Uncovering Political Bias in Bangla News Articles</title>
<link>https://arxiv.org/abs/2510.03898</link>
<guid>https://arxiv.org/abs/2510.03898</guid>
<content:encoded><![CDATA[
<div> Keywords: media bias, Bangla, political stance detection, language models, dataset creation 

Summary: 
This article discusses the importance of detecting media bias in the South Asian region, particularly in the context of Bangla news. The lack of annotated datasets and computational studies in this area is highlighted, emphasizing the complexity of analyzing political bias in Bangla media due to linguistic cues and cultural nuances. The authors introduce a benchmark dataset of 200 Bangla news articles labeled for different political stances, allowing for evaluations of large language models (LLMs). The study finds that while LLMs perform well in detecting government-critique content, they struggle with neutral articles and often misinterpret ambiguous narratives. The dataset and diagnostic analyses presented in the article aim to advance research on stance detection in Bangla media and suggest ways to enhance LLM performance in low-resource languages.

<br /><br />Summary: <div>
arXiv:2510.03898v1 Announce Type: new 
Abstract: Detecting media bias is crucial, specifically in the South Asian region. Despite this, annotated datasets and computational studies for Bangla political bias research remain scarce. Crucially because, political stance detection in Bangla news requires understanding of linguistic cues, cultural context, subtle biases, rhetorical strategies, code-switching, implicit sentiment, and socio-political background. To address this, we introduce the first benchmark dataset of 200 politically significant and highly debated Bangla news articles, labeled for government-leaning, government-critique, and neutral stances, alongside diagnostic analyses for evaluating large language models (LLMs). Our comprehensive evaluation of 28 proprietary and open-source LLMs shows strong performance in detecting government-critique content (F1 up to 0.83) but substantial difficulty with neutral articles (F1 as low as 0.00). Models also tend to over-predict government-leaning stances, often misinterpreting ambiguous narratives. This dataset and its associated diagnostics provide a foundation for advancing stance detection in Bangla media research and offer insights for improving LLM performance in low-resource languages.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PsycholexTherapy: Simulating Reasoning in Psychotherapy with Small Language Models in Persian</title>
<link>https://arxiv.org/abs/2510.03913</link>
<guid>https://arxiv.org/abs/2510.03913</guid>
<content:encoded><![CDATA[
<div> Framework, PsychoLexTherapy, Persian, small language models, structured memory <br />
<br />
Summary: This study introduces PsychoLexTherapy, a framework for simulating psychotherapeutic reasoning in Persian using small language models. The framework aims to develop culturally grounded and therapeutically coherent dialogue systems with structured memory for multi-turn interactions in underrepresented languages. It is optimized for on-device deployment to ensure privacy and feasibility. The development process included evaluating psychological knowledge, designing the reasoning-oriented framework, and creating evaluation datasets for benchmarking against baselines. Experiments compared different reasoning paths, with results showing that deliberate model selection achieved a balance of accuracy, efficiency, and privacy. PsychoLexTherapy outperformed all baselines in automatic evaluation and was highly ranked by human evaluators. The framework's long-term memory module was crucial for maintaining coherence and achieving high ratings in empathy, cultural fit, and personalization. Overall, PsychoLexTherapy establishes a practical, culturally aligned foundation for Persian psychotherapy simulation. <div>
arXiv:2510.03913v1 Announce Type: new 
Abstract: This study presents PsychoLexTherapy, a framework for simulating psychotherapeutic reasoning in Persian using small language models (SLMs). The framework tackles the challenge of developing culturally grounded, therapeutically coherent dialogue systems with structured memory for multi-turn interactions in underrepresented languages. To ensure privacy and feasibility, PsychoLexTherapy is optimized for on-device deployment, enabling use without external servers. Development followed a three-stage process: (i) assessing SLMs psychological knowledge with PsychoLexEval; (ii) designing and implementing the reasoning-oriented PsychoLexTherapy framework; and (iii) constructing two evaluation datasets-PsychoLexQuery (real Persian user questions) and PsychoLexDialogue (hybrid simulated sessions)-to benchmark against multiple baselines. Experiments compared simple prompting, multi-agent debate, and structured therapeutic reasoning paths. Results showed that deliberate model selection balanced accuracy, efficiency, and privacy. On PsychoLexQuery, PsychoLexTherapy outperformed all baselines in automatic LLM-as-a-judge evaluation and was ranked highest by human evaluators in a single-turn preference study. In multi-turn tests with PsychoLexDialogue, the long-term memory module proved essential: while naive history concatenation caused incoherence and information loss, the full framework achieved the highest ratings in empathy, coherence, cultural fit, and personalization. Overall, PsychoLexTherapy establishes a practical, privacy-preserving, and culturally aligned foundation for Persian psychotherapy simulation, contributing novel datasets, a reproducible evaluation pipeline, and empirical insights into structured memory for therapeutic reasoning.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mapping Patient-Perceived Physician Traits from Nationwide Online Reviews with LLMs</title>
<link>https://arxiv.org/abs/2510.03997</link>
<guid>https://arxiv.org/abs/2510.03997</guid>
<content:encoded><![CDATA[
<div> Keywords: patient perception, Big Five personality traits, physician reviews, empathy, healthcare 

Summary: 
- The study utilizes a large language model to analyze 4.1 million patient reviews of 226,999 U.S. physicians to understand how patients perceive their doctors. 
- The pipeline developed infers Big Five personality traits and subjective judgments from patient reviews and achieves strong agreement with human assessments and correlates with patient satisfaction. 
- Male physicians receive higher ratings across all traits, with empathy-related traits being important in pediatrics and psychiatry. 
- All extracted traits positively predict overall satisfaction. 
- Cluster analysis identifies four distinct physician archetypes, ranging from "Well-Rounded Excellent" to "Underperforming", providing insights for quality measurement, bias detection, and workforce development in healthcare. 

<br /><br />Summary: <div>
arXiv:2510.03997v1 Announce Type: new 
Abstract: Understanding how patients perceive their physicians is essential to improving trust, communication, and satisfaction. We present a large language model (LLM)-based pipeline that infers Big Five personality traits and five patient-oriented subjective judgments. The analysis encompasses 4.1 million patient reviews of 226,999 U.S. physicians from an initial pool of one million. We validate the method through multi-model comparison and human expert benchmarking, achieving strong agreement between human and LLM assessments (correlation coefficients 0.72-0.89) and external validity through correlations with patient satisfaction (r = 0.41-0.81, all p<0.001). National-scale analysis reveals systematic patterns: male physicians receive higher ratings across all traits, with largest disparities in clinical competence perceptions; empathy-related traits predominate in pediatrics and psychiatry; and all traits positively predict overall satisfaction. Cluster analysis identifies four distinct physician archetypes, from "Well-Rounded Excellent" (33.8%, uniformly high traits) to "Underperforming" (22.6%, consistently low). These findings demonstrate that automated trait extraction from patient narratives can provide interpretable, validated metrics for understanding physician-patient relationships at scale, with implications for quality measurement, bias detection, and workforce development in healthcare.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating and Understanding Deceptive Behaviors in Long-Horizon Interactions</title>
<link>https://arxiv.org/abs/2510.03999</link>
<guid>https://arxiv.org/abs/2510.03999</guid>
<content:encoded><![CDATA[
<div> Keywords: deception, large language models, multi-agent system, trust, strategies

Summary: 
Deception is a significant concern in human communication and large language models (LLMs). Existing studies have shown instances of LLM deception but mainly in single-turn prompts, lacking long-horizon interaction evaluations. A simulation framework is introduced to probe and assess deception in LLMs across extended sequences of tasks and changing contexts. The framework involves a performer agent, a supervisor agent, and a deception auditor. Experiments with 11 models indicate that deception varies among models, increases under pressure, and undermines supervisor trust. Strategies like concealment, equivocation, and falsification are identified through qualitative analyses. The study highlights deception as a potential risk in long-horizon interactions and offers insights for evaluating LLMs in trust-sensitive real-world scenarios. 

<br /><br />Summary: <div>
arXiv:2510.03999v1 Announce Type: new 
Abstract: Deception is a pervasive feature of human communication and an emerging concern in large language models (LLMs). While recent studies document instances of LLM deception under pressure, most evaluations remain confined to single-turn prompts and fail to capture the long-horizon interactions in which deceptive strategies typically unfold. We introduce the first simulation framework for probing and evaluating deception in LLMs under extended sequences of interdependent tasks and dynamic contextual pressures. Our framework instantiates a multi-agent system: a performer agent tasked with completing tasks and a supervisor agent that evaluates progress, provides feedback, and maintains evolving states of trust. An independent deception auditor then reviews full trajectories to identify when and how deception occurs. We conduct extensive experiments across 11 frontier models, spanning both closed- and open-source systems, and find that deception is model-dependent, increases with event pressure, and consistently erodes supervisor trust. Qualitative analyses further reveal distinct strategies of concealment, equivocation, and falsification. Our findings establish deception as an emergent risk in long-horizon interactions and provide a foundation for evaluating future LLMs in real-world, trust-sensitive contexts.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Named Entity Recognition in COVID-19 tweets with Entity Knowledge Augmentation</title>
<link>https://arxiv.org/abs/2510.04001</link>
<guid>https://arxiv.org/abs/2510.04001</guid>
<content:encoded><![CDATA[
<div> Keywords: COVID-19, named entity recognition, social media, entity knowledge augmentation, biomedical

Summary: 

The article discusses the challenges of identifying pandemic-related named entities expressed on social media, such as the informal nature of COVID-19 texts and the lack of annotations for model training. To tackle these challenges, the authors propose a novel entity knowledge augmentation approach for COVID-19, which can also be applied to biomedical named entity recognition in both informal and formal text formats. Experiments conducted on COVID-19 tweets and PubMed datasets demonstrate that the proposed approach improves named entity recognition performance in fully-supervised and few-shot settings. The source code for the method is publicly available on GitHub, providing a valuable resource for researchers in the field. The study contributes to a better understanding of pandemic discussions on social media and highlights the importance of developing effective named entity recognition models for COVID-19. 

<br /><br />Summary: <div>
arXiv:2510.04001v1 Announce Type: new 
Abstract: The COVID-19 pandemic causes severe social and economic disruption around the world, raising various subjects that are discussed over social media. Identifying pandemic-related named entities as expressed on social media is fundamental and important to understand the discussions about the pandemic. However, there is limited work on named entity recognition on this topic due to the following challenges: 1) COVID-19 texts in social media are informal and their annotations are rare and insufficient to train a robust recognition model, and 2) named entity recognition in COVID-19 requires extensive domain-specific knowledge. To address these issues, we propose a novel entity knowledge augmentation approach for COVID-19, which can also be applied in general biomedical named entity recognition in both informal text format and formal text format. Experiments carried out on the COVID-19 tweets dataset and PubMed dataset show that our proposed entity knowledge augmentation improves NER performance in both fully-supervised and few-shot settings. Our source code is publicly available: https://github.com/kkkenshi/LLM-EKA/tree/master
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgriGPT-VL: Agricultural Vision-Language Understanding Suite</title>
<link>https://arxiv.org/abs/2510.04002</link>
<guid>https://arxiv.org/abs/2510.04002</guid>
<content:encoded><![CDATA[
<div> Framework, Agriculture, Vision-language, Model, Evaluation <br />
Summary: <br />
The AgriGPT-VL Suite introduces a multimodal framework tailored for agriculture, addressing limitations in domain-specific models, vision-language corpora, and evaluation. It includes the Agri-3M-VL vision-language corpus, AgriGPT-VL model, and AgriBench-VL-4K evaluation suite. The corpus consists of image-caption and VQA pairs, expert-level VQA instances, and reinforcement learning samples. The model is trained with a progressive curriculum for multimodal reasoning and text-only capability preservation. AgriBench-VL-4K provides challenging evaluation with open-ended and image-grounded questions and multi-metric assessment. Experiments show AgriGPT-VL outperforms general-purpose models on the evaluation suite and remains competitive on text-only benchmarks. Ablation studies confirm the effectiveness of alignment and GRPO refinement stages. All resources will be open-sourced to support reproducible research and deployment in agricultural settings. <br /><br />Summary: <div>
arXiv:2510.04002v1 Announce Type: new 
Abstract: Despite rapid advances in multimodal large language models, agricultural applications remain constrained by the scarcity of domain-tailored models, curated vision-language corpora, and rigorous evaluation. To address these challenges, we present the AgriGPT-VL Suite, a unified multimodal framework for agriculture. Our contributions are threefold. First, we introduce Agri-3M-VL, the largest vision-language corpus for agriculture to our knowledge, curated by a scalable multi-agent data generator; it comprises 1M image-caption pairs, 2M image-grounded VQA pairs, 50K expert-level VQA instances, and 15K GRPO reinforcement learning samples. Second, we develop AgriGPT-VL, an agriculture-specialized vision-language model trained via a progressive curriculum of textual grounding, multimodal shallow/deep alignment, and GRPO refinement. This method achieves strong multimodal reasoning while preserving text-only capability. Third, we establish AgriBench-VL-4K, a compact yet challenging evaluation suite with open-ended and image-grounded questions, paired with multi-metric evaluation and an LLM-as-a-judge framework. Experiments show that AgriGPT-VL outperforms leading general-purpose VLMs on AgriBench-VL-4K, achieving higher pairwise win rates in the LLM-as-a-judge evaluation. Meanwhile, it remains competitive on the text-only AgriBench-13K with no noticeable degradation of language ability. Ablation studies further confirm consistent gains from our alignment and GRPO refinement stages. We will open source all of the resources to support reproducible research and deployment in low-resource agricultural settings.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Microscope: What Model Internals Reveal About Answer Correctness and Context Utilization</title>
<link>https://arxiv.org/abs/2510.04013</link>
<guid>https://arxiv.org/abs/2510.04013</guid>
<content:encoded><![CDATA[
<div> interpretability, large language models, correctness prediction, model activations, context assessment

Summary:
The study focuses on the trustworthiness of large language models (LLMs) and the challenge of determining when external context is beneficial for generating accurate information. By analyzing model activations, the researchers developed a classifier that can predict the correctness of LLM outputs with 75% accuracy, allowing for early auditing. They also introduced a metric that outperforms prompting baselines in distinguishing between correct and incorrect context. This metric helps identify and mitigate the impact of inaccurate or irrelevant context on model outputs. The findings provide insights into the decision-making processes of LLMs and offer a potential solution to improving the reliability of generated information. The code used in the study is publicly available for further exploration and research. 

Summary: <div>
arXiv:2510.04013v1 Announce Type: new 
Abstract: Although large language models (LLMs) have tremendous utility, trustworthiness is still a chief concern: models often generate incorrect information with high confidence. While contextual information can help guide generation, identifying when a query would benefit from retrieved context and assessing the effectiveness of that context remains challenging. In this work, we operationalize interpretability methods to ascertain whether we can predict the correctness of model outputs from the model's activations alone. We also explore whether model internals contain signals about the efficacy of external context. We consider correct, incorrect, and irrelevant context and introduce metrics to distinguish amongst them. Experiments on six different models reveal that a simple classifier trained on intermediate layer activations of the first output token can predict output correctness with about 75% accuracy, enabling early auditing. Our model-internals-based metric significantly outperforms prompting baselines at distinguishing between correct and incorrect context, guarding against inaccuracies introduced by polluted context. These findings offer a lens to better understand the underlying decision-making processes of LLMs. Our code is publicly available at https://github.com/jiarui-liu/LLM-Microscope
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thai Semantic End-of-Turn Detection for Real-Time Voice Agents</title>
<link>https://arxiv.org/abs/2510.04016</link>
<guid>https://arxiv.org/abs/2510.04016</guid>
<content:encoded><![CDATA[
<div> Keywords: Thai, end-of-turn detection, real-time agents, linguistic cues, lightweight transformers

Summary:
In this study, researchers investigate Thai text-only end-of-turn (EOT) detection for real-time agents, focusing on reducing latency in voice-to-voice interaction. Traditional audio-silence end-pointers are not efficient for this task, so the researchers compare different models to find the most accurate and fast solution. By using transcribed subtitles from the YODAS corpus and Thai-specific linguistic cues, they formulate EOT detection as a binary decision over token boundaries. The study reveals an accuracy-latency tradeoff and highlights the effectiveness of small, fine-tuned models for near-instant EOT decisions. This research not only establishes a Thai baseline for EOT detection but also provides a practical implementation plan for on-device agents. The findings emphasize the importance of linguistic cues and model optimization in improving the efficiency of voice interaction systems.<br /><br />Summary: <div>
arXiv:2510.04016v1 Announce Type: new 
Abstract: Fluid voice-to-voice interaction requires reliable and low-latency detection of when a user has finished speaking. Traditional audio-silence end-pointers add hundreds of milliseconds of delay and fail under hesitations or language-specific phenomena. We present, to our knowledge, the first systematic study of Thai text-only end-of-turn (EOT) detection for real-time agents. We compare zero-shot and few-shot prompting of compact LLMs to supervised fine-tuning of lightweight transformers. Using transcribed subtitles from the YODAS corpus and Thai-specific linguistic cues (e.g., sentence-final particles), we formulate EOT as a binary decision over token boundaries. We report a clear accuracy-latency tradeoff and provide a public-ready implementation plan. This work establishes a Thai baseline and demonstrates that small, fine-tuned models can deliver near-instant EOT decisions suitable for on-device agents.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Using Counterfactual Help LLMs Explain Textual Importance in Classification?</title>
<link>https://arxiv.org/abs/2510.04031</link>
<guid>https://arxiv.org/abs/2510.04031</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, textual classification, counterfactuals, decision changing rate, experimental results

Summary: 
Large language models (LLMs) have shown impressive abilities in textual classification tasks, leading to the need for explanations of their decisions. This study explores incorporating counterfactuals into LLM reasoning to improve the identification of key contributing words in classification. The framework introduced, the decision changing rate, quantifies the importance of top words in classification. Experimental results demonstrate the utility of using counterfactuals in enhancing LLM performance. <div>
arXiv:2510.04031v1 Announce Type: new 
Abstract: Large language models (LLMs) are becoming useful in many domains due to their impressive abilities that arise from large training datasets and large model sizes. More recently, they have been shown to be very effective in textual classification tasks, motivating the need to explain the LLMs' decisions. Motivated by practical constrains where LLMs are black-boxed and LLM calls are expensive, we study how incorporating counterfactuals into LLM reasoning can affect the LLM's ability to identify the top words that have contributed to its classification decision. To this end, we introduce a framework called the decision changing rate that helps us quantify the importance of the top words in classification. Our experimental results show that using counterfactuals can be helpful.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Small Language Models for Emergency Departments Decision Support: A Benchmark Study</title>
<link>https://arxiv.org/abs/2510.04032</link>
<guid>https://arxiv.org/abs/2510.04032</guid>
<content:encoded><![CDATA[
<div> benchmark, language models, emergency departments, decision support, medical expertise 

Summary:
The paper introduces the concept of using small language models (SLMs) in emergency departments (EDs) to assist physicians in clinical decision-making. SLMs, with reduced parameter counts compared to large language models (LLMs), are shown to be effective in providing timely and accurate information synthesis. The benchmark dataset designed for evaluating SLMs in ED decision support includes MedMCQA, MedQA-4Options, and PubMedQA, featuring tasks aligned with real ED physicians' daily tasks. Surprisingly, general-domain SLMs outperform medically fine-tuned SLMs across these benchmarks, suggesting that specialized medical expertise may not be necessary for ED tasks. This is important considering practical hardware limitations, cost constraints, and privacy concerns in real-world deployments. Overall, the study highlights the potential of SLMs in improving workflow efficiency and clinical decision-making in emergency medical settings. 

<br /><br />Summary: <div>
arXiv:2510.04032v1 Announce Type: new 
Abstract: Large language models (LLMs) have become increasingly popular in medical domains to assist physicians with a variety of clinical and operational tasks. Given the fast-paced and high-stakes environment of emergency departments (EDs), small language models (SLMs), characterized by a reduction in parameter count compared to LLMs, offer significant potential due to their inherent reasoning capability and efficient performance. This enables SLMs to support physicians by providing timely and accurate information synthesis, thereby improving clinical decision-making and workflow efficiency. In this paper, we present a comprehensive benchmark designed to identify SLMs suited for ED decision support, taking into account both specialized medical expertise and broad general problem-solving capabilities. In our evaluations, we focus on SLMs that have been trained on a mixture of general-domain and medical corpora. A key motivation for emphasizing SLMs is the practical hardware limitations, operational cost constraints, and privacy concerns in the typical real-world deployments. Our benchmark datasets include MedMCQA, MedQA-4Options, and PubMedQA, with the medical abstracts dataset emulating tasks aligned with real ED physicians' daily tasks. Experimental results reveal that general-domain SLMs surprisingly outperform their medically fine-tuned counterparts across these diverse benchmarks for ED. This indicates that for ED, specialized medical fine-tuning of the model may not be required.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Chain-of-Thought Reasoning for Steerable Pluralistic Alignment</title>
<link>https://arxiv.org/abs/2510.04045</link>
<guid>https://arxiv.org/abs/2510.04045</guid>
<content:encoded><![CDATA[
<div> Chain-of-Thought Reasoning, Large Language Models, Steerable Pluralism, Value Kaleidoscope, OpinionQA <br />
Summary: <br />
This study explores the application of Chain-of-Thought (CoT) reasoning techniques in developing steerable pluralistic models for Large Language Models (LLMs). Various methods, including CoT prompting, fine-tuning on human-authored CoT, fine-tuning on synthetic explanations, and Reinforcement Learning with Verifiable Rewards (RLVR) are investigated using the Value Kaleidoscope and OpinionQA datasets. RLVR consistently outperforms other methods and shows high training sample efficiency. The generated CoT traces are analyzed for faithfulness and safety, ensuring the model's reliability in capturing nuanced human perspectives. This research highlights the importance of enabling LLMs to support steerable pluralism for tasks that require understanding a variety of perspectives, showcasing the potential benefits of incorporating CoT reasoning techniques in developing more versatile and inclusive language models. <br /> <div>
arXiv:2510.04045v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are typically trained to reflect a relatively uniform set of values, which limits their applicability to tasks that require understanding of nuanced human perspectives. Recent research has underscored the importance of enabling LLMs to support steerable pluralism -- the capacity to adopt a specific perspective and align generated outputs with it. In this work, we investigate whether Chain-of-Thought (CoT) reasoning techniques can be applied to building steerable pluralistic models. We explore several methods, including CoT prompting, fine-tuning on human-authored CoT, fine-tuning on synthetic explanations, and Reinforcement Learning with Verifiable Rewards (RLVR). We evaluate these approaches using the Value Kaleidoscope and OpinionQA datasets. Among the methods studied, RLVR consistently outperforms others and demonstrates strong training sample efficiency. We further analyze the generated CoT traces with respect to faithfulness and safety.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Makes Diffusion Language Models Super Data Learners?</title>
<link>https://arxiv.org/abs/2510.04071</link>
<guid>https://arxiv.org/abs/2510.04071</guid>
<content:encoded><![CDATA[
<div> masking, diffusion language models, data efficiency, ablation experiments, stochastic regularization <br />
<br />
Recent studies have shown that diffusion language models are highly data-efficient, but the reasons behind this efficiency were unclear. The study conducted extensive ablation experiments to determine the contributing factors. The results indicated that random masking of input tokens was the primary driver of the efficiency. Additionally, similar gains were observed with MLP dropout and weight decay, suggesting that stochastic regularization techniques play a crucial role in enhancing data efficiency during multi-epoch training. This research sheds light on the mechanisms underlying the remarkable data efficiency of diffusion language models and highlights the importance of stochastic regularization in achieving such efficiency. The code for the study is available on GitHub for further exploration and validation. <br /><br />Summary: <div>
arXiv:2510.04071v1 Announce Type: new 
Abstract: Recent studies have shown that diffusion language models achieve remarkable data efficiency under limited-data constraints, yet the underlying mechanisms remain unclear. In this work, we perform extensive ablation experiments to disentangle the sources of this efficiency. Our results show that random masking of input tokens plays the dominant role. We further show that similar gains can be obtained through in MLP dropout and weight decay, indicating that stochastic regularization broadly enhances data efficiency in multi-epoch training. Our code is available at https://github.com/zitian-gao/data-efficiency.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PoLi-RL: A Point-to-List Reinforcement Learning Framework for Conditional Semantic Textual Similarity</title>
<link>https://arxiv.org/abs/2510.04080</link>
<guid>https://arxiv.org/abs/2510.04080</guid>
<content:encoded><![CDATA[
<div> RL, Conditional Semantic Textual Similarity, Large Language Models, Point-to-List Reinforcement Learning, Ranking Reward<br />
<br />
Summary:<br />
This study introduces a new approach, PoLi-RL, for training Large Language Models (LLMs) on Conditional Semantic Textual Similarity (C-STS) tasks. The existing methods in this area have mainly used discriminative models, but this work leverages Reinforcement Learning (RL) to optimize the non-differentiable Spearman ranking metric and improve reasoning in C-STS. PoLi-RL employs a two-stage curriculum, starting with simple pointwise rewards and transitioning to a hybrid reward combining pointwise, pairwise, and listwise objectives to enhance semantic distinctions. The Parallel Slice Ranking Reward (PSRR) mechanism in PoLi-RL computes ranking rewards in parallel slices, enabling precise credit assignment and effective optimization. On the C-STS benchmark, PoLi-RL achieves a high Spearman correlation coefficient, setting a new state-of-the-art for the cross-encoder architecture. This study showcases the potential of RL in training LLMs for complex conditional judgment tasks. <br /> <div>
arXiv:2510.04080v1 Announce Type: new 
Abstract: Conditional Semantic Textual Similarity (C-STS) measures the semantic proximity between text segments under a specific condition, thereby overcoming the ambiguity inherent in traditional STS. However, existing methods are largely confined to discriminative models, failing to fully integrate recent breakthroughs in the NLP community concerning Large Language Models (LLMs) and Reinforcement Learning (RL). RL is a particularly well-suited paradigm for this task, as it can directly optimize the non-differentiable Spearman ranking metric and guide the reasoning process required by C-STS. However, we find that naively applying listwise RL fails to produce meaningful improvements, as the model is overwhelmed by complex, coarse-grained reward signals. To address this challenge, we introduce PoLi-RL, a novel Point-to-List Reinforcement Learning framework. PoLi-RL employs a two-stage curriculum: it first trains the model with simple pointwise rewards to establish fundamental scoring capabilities, then transitions to a hybrid reward that combines pointwise, pairwise, and listwise objectives to refine the model's ability to discern subtle semantic distinctions. Crucially, we propose an innovative Parallel Slice Ranking Reward (PSRR) mechanism that computes ranking rewards in parallel slices, where each slice comprises same-indexed completions from different samples. This provides a precise, differentiated learning signal for each individual completion, enabling granular credit assignment and effective optimization. On the official C-STS benchmark, PoLi-RL achieves a Spearman correlation coefficient of 48.18, establishing a new SOTA for the cross-encoder architecture. As the first work to successfully apply RL to C-STS, our study introduces a powerful and precise paradigm for training LLMs on complex, ranking-based conditional judgment tasks.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning</title>
<link>https://arxiv.org/abs/2510.04081</link>
<guid>https://arxiv.org/abs/2510.04081</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Chain-of-Thought, Reasoning, Code-Assisted, Data Synthesis

Summary: 
Caco is a new framework that enhances reasoning in Large Language Models through code-driven augmentation. It automates the generation of high-quality and diverse reasoning paths based on code. By fine-tuning a code-based CoT generator on existing math and programming solutions, Caco ensures logical correctness and structural diversity in generated data. The framework validates outputs via code execution and rule-based filtering, improving task adaptability and scalability. Models trained on Caco outperform existing baselines on mathematical reasoning benchmarks, showcasing superior generalization across unseen tasks. This self-sustaining approach establishes a reliable and automated system for generating reasoning data without human intervention.<br /><br />Summary: <div>
arXiv:2510.04081v1 Announce Type: new 
Abstract: Reasoning capability is pivotal for Large Language Models (LLMs) to solve complex tasks, yet achieving reliable and scalable reasoning remains challenging. While Chain-of-Thought (CoT) prompting has become a mainstream approach, existing methods often suffer from uncontrolled generation, insufficient quality, and limited diversity in reasoning paths. Recent efforts leverage code to enhance CoT by grounding reasoning in executable steps, but such methods are typically constrained to predefined mathematical problems, hindering scalability and generalizability. In this work, we propose Caco (Code-Assisted Chain-of-ThOught), a novel framework that automates the synthesis of high-quality, verifiable, and diverse instruction-CoT reasoning data through code-driven augmentation. Unlike prior work, Caco first fine-tunes a code-based CoT generator on existing math and programming solutions in a unified code format, then scales the data generation to a large amount of diverse reasoning traces. Crucially, we introduce automated validation via code execution and rule-based filtering to ensure logical correctness and structural diversity, followed by reverse-engineering filtered outputs into natural language instructions and language CoTs to enrich task adaptability. This closed-loop process enables fully automated, scalable synthesis of reasoning data with guaranteed executability. Experiments on our created Caco-1.3M dataset demonstrate that Caco-trained models achieve strong competitive performance on mathematical reasoning benchmarks, outperforming existing strong baselines. Further analysis reveals that Caco's code-anchored verification and instruction diversity contribute to superior generalization across unseen tasks. Our work establishes a paradigm for building self-sustaining, trustworthy reasoning systems without human intervention.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling LLMs' Metaphorical Understanding: Exploring Conceptual Irrelevance, Context Leveraging and Syntactic Influence</title>
<link>https://arxiv.org/abs/2510.04120</link>
<guid>https://arxiv.org/abs/2510.04120</guid>
<content:encoded><![CDATA[
<div> Concept Mapping, Metaphor-Literal Repository, Syntactic Sensitivity, Large Language Models, Metaphor Analysis

Summary:
Large Language Models (LLMs) exhibit advanced capabilities in knowledge integration and contextual reasoning but struggle in metaphor comprehension. Concept Mapping evaluates how LLMs map concepts in target domains and reveals that LLMs generate conceptually irrelevant interpretations. Metaphor-Literal Repository analyzes metaphorical words and their literal counterparts and shows that LLMs depend on metaphorical indicators in training data rather than contextual cues. Syntactic Sensitivity assesses how metaphorical syntactic structures impact LLMs' performance and highlights that LLMs are more sensitive to syntactic irregularities than structural comprehension. These findings underscore the limitations of LLMs in metaphor analysis and suggest the need for more robust computational approaches. 

<br /><br />Summary: <div>
arXiv:2510.04120v1 Announce Type: new 
Abstract: Metaphor analysis is a complex linguistic phenomenon shaped by context and external factors. While Large Language Models (LLMs) demonstrate advanced capabilities in knowledge integration, contextual reasoning, and creative generation, their mechanisms for metaphor comprehension remain insufficiently explored. This study examines LLMs' metaphor-processing abilities from three perspectives: (1) Concept Mapping: using embedding space projections to evaluate how LLMs map concepts in target domains (e.g., misinterpreting "fall in love" as "drop down from love"); (2) Metaphor-Literal Repository: analyzing metaphorical words and their literal counterparts to identify inherent metaphorical knowledge; and (3) Syntactic Sensitivity: assessing how metaphorical syntactic structures influence LLMs' performance. Our findings reveal that LLMs generate 15\%-25\% conceptually irrelevant interpretations, depend on metaphorical indicators in training data rather than contextual cues, and are more sensitive to syntactic irregularities than to structural comprehension. These insights underline the limitations of LLMs in metaphor analysis and call for more robust computational approaches.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sri Lanka Document Datasets: A Large-Scale, Multilingual Resource for Law, News, and Policy (v20251005)</title>
<link>https://arxiv.org/abs/2510.04124</link>
<guid>https://arxiv.org/abs/2510.04124</guid>
<content:encoded><![CDATA[
<div> parliamentary proceedings, legal judgments, government publications, news, tourism statistics <br />
Summary:<br />
The article introduces a collection of open, machine-readable document datasets from Sri Lanka, including parliamentary proceedings, legal judgments, government publications, news, and tourism statistics. The datasets, available in Sinhala, Tamil, and English, currently consist of 215,670 documents totaling 60.3 GB. They are updated daily and accessible on GitHub and Hugging Face. These resources aim to support research in computational linguistics, legal analytics, socio-political studies, and multilingual natural language processing. The article provides details on the data sources, collection pipeline, formats, potential use cases, as well as discusses licensing and ethical considerations. <div>
arXiv:2510.04124v1 Announce Type: new 
Abstract: We present a collection of open, machine-readable document datasets covering parliamentary proceedings, legal judgments, government publications, news, and tourism statistics from Sri Lanka. As of v20251005, the collection currently comprises 215,670 documents (60.3 GB) across 13 datasets in Sinhala, Tamil, and English. The datasets are updated daily and mirrored on GitHub and Hugging Face. These resources aim to support research in computational linguistics, legal analytics, socio-political studies, and multilingual natural language processing. We describe the data sources, collection pipeline, formats, and potential use cases, while discussing licensing and ethical considerations.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine Tuning Methods for Low-resource Languages</title>
<link>https://arxiv.org/abs/2510.04139</link>
<guid>https://arxiv.org/abs/2510.04139</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, culture, underrepresented language, Generative AI, cultural heritage

Summary: 
Large Language Models, such as Gemma 2, have primarily been trained on English texts and culture, leading to underperformance in other languages and cultural contexts. To address this issue, a new method was developed to prepare culturally relevant datasets and enhance the performance of Gemma 2 for an underrepresented language. This project aimed to showcase the potential of Generative AI in diverse cultural settings and preserve cultural heritage. By utilizing this approach, individuals can unlock the power of Generative AI in their own countries, contributing to a more inclusive and culturally diverse landscape in AI research and applications. The project serves as a stepping stone towards achieving greater inclusivity and cultural representation in the development of language models and AI technologies. <br /><br />Summary: <div>
arXiv:2510.04139v1 Announce Type: new 
Abstract: The rise of Large Language Models has not been inclusive of all cultures. The models are mostly trained on English texts and culture which makes them underperform in other languages and cultural contexts. By developing a generalizable method for preparing culturally relevant datasets and post-training the Gemma 2 model, this project aimed to increase the performance of Gemma 2 for an underrepresented language and showcase how others can do the same to unlock the power of Generative AI in their country and preserve their cultural heritage.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self Speculative Decoding for Diffusion Large Language Models</title>
<link>https://arxiv.org/abs/2510.04147</link>
<guid>https://arxiv.org/abs/2510.04147</guid>
<content:encoded><![CDATA[
<div> Self Speculative Decoding, diffusion-based Large Language Models, bidirectional attention, parallel generation, lossless inference acceleration<br />
<br />
Summary: Self Speculative Decoding (SSD) is proposed as a method to accelerate inference in diffusion-based Large Language Models (dLLMs) without loss of performance. SSD leverages the dLLM itself for speculative decoding using a self-drafting mechanism and hierarchical verification trees. This approach eliminates the need for separate draft models, reducing redundancy and memory overhead. SSD allows the model to verify and accept multiple tokens in a single forward pass, achieving up to 3.46× speedup while maintaining output consistency with stepwise decoding. Experiments on models like LLaDA and Dream demonstrate the effectiveness of SSD. The code for SSD will be publicly available on GitHub. <br /><br /> <div>
arXiv:2510.04147v1 Announce Type: new 
Abstract: Diffusion-based Large Language Models (dLLMs) have emerged as a competitive alternative to autoregressive models, offering unique advantages through bidirectional attention and parallel generation paradigms. However, the generation results of current parallel decoding methods deviate from stepwise decoding, introducing potential performance degradation, which limits their practical deployment. To address this problem, we propose \textbf{S}elf \textbf{S}peculative \textbf{D}ecoding (SSD), a lossless inference acceleration method that leverages the dLLM itself as both speculative decoding drafter and verifier without auxiliary modules. SSD introduces a self-drafting mechanism where the model generates predictions for multiple positions, then verifies them through hierarchical verification trees in a single forward pass. Unlike traditional speculative decoding that requires separate draft models, SSD eliminates model redundancy and memory overhead by exploiting the dLLM's inherent parallel prediction capability for multiple positions. This self-speculative approach allows the model to progressively verify and accept multiple tokens in a single forward pass. Our experiments demonstrate that SSD achieves up to 3.46$\times$ speedup while keeping the output identical to stepwise decoding on open source models such as LLaDA and Dream. Code will be made publicly available on GitHub.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thinking on the Fly: Test-Time Reasoning Enhancement via Latent Thought Policy Optimization</title>
<link>https://arxiv.org/abs/2510.04182</link>
<guid>https://arxiv.org/abs/2510.04182</guid>
<content:encoded><![CDATA[
<div> latent thought policy optimization, large language models, reasoning, robustness, parameter-free

Summary:
Latent Thought Policy Optimization (LTPO) is introduced as a parameter-free framework for enhancing Large Language Models (LLMs) reasoning at test time. It utilizes dynamic optimization of latent "thought" vectors for each problem instance, guided by an intrinsic reward signal computed from the LLM's output distributions. LTPO outperforms baselines on standard tasks and shows remarkable robustness, particularly excelling on challenging AIME benchmarks where other approaches struggle. This approach highlights the unique capability of LTPO for complex reasoning tasks. <div>
arXiv:2510.04182v1 Announce Type: new 
Abstract: Recent advancements in Large Language Models (LLMs) have shifted from explicit Chain-of-Thought (CoT) reasoning to more efficient latent reasoning, where intermediate thoughts are represented as vectors rather than text. However, latent reasoning can be brittle on challenging, out-of-distribution tasks where robust reasoning is most critical. To overcome these limitations, we introduce Latent Thought Policy Optimization (LTPO), a parameter-free framework that enhances LLM reasoning entirely at test time, without requiring model parameter updates. LTPO treats intermediate latent "thought" vectors as dynamic parameters that are actively optimized for each problem instance. It employs an online policy gradient method guided by an intrinsic, confidence-based reward signal computed directly from the frozen LLM's own output distributions, eliminating the need for external supervision or expensive text generation during optimization. Extensive experiments on five reasoning benchmarks show that LTPO not only matches or surpasses strong baselines on standard tasks but also demonstrates remarkable robustness where others fail. Most notably, on highly challenging AIME benchmarks where existing latent reasoning baselines collapse to near-zero accuracy, LTPO delivers substantial improvements, showcasing a unique capability for complex reasoning.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling</title>
<link>https://arxiv.org/abs/2510.04204</link>
<guid>https://arxiv.org/abs/2510.04204</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Reasoning Models, CALM, STORM, optimization modeling, reasoning patterns

Summary:<br />
- The study focuses on improving optimization modeling using Large Reasoning Models (LRMs) by proposing CALM (Corrective Adaptation with Lightweight Modification).
- Existing methods fail to fully utilize LRMs' advanced reasoning patterns for optimization tasks.
- CALM framework involves an expert providing corrective hints to refine LRMs within their native reasoning modes.
- CALM modifies a small percentage of tokens to generate high-quality data for adaptation through supervised fine-tuning and reinforcement learning.
- The developed STORM model, a 4B-parameter LRM, achieves a state-of-the-art average accuracy of 68.9% on optimization modeling benchmarks, matching performance of a much larger LRM.
<br /><br />Summary: 
The study introduces CALM, a framework that leverages Large Reasoning Models' advanced reasoning capabilities for optimization modeling. By incorporating corrective hints from an expert to refine LRMs within their native reasoning modes, CALM generates high-quality data for adaptation and reinforcement learning. The resulting STORM model, a 4B-parameter LRM, achieves a new state-of-the-art average accuracy on optimization modeling benchmarks, showcasing the effectiveness of dynamic, hint-based data synthesis in preserving and enhancing LRMs' native reasoning patterns for expert-level performance on complex tasks. <div>
arXiv:2510.04204v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) have demonstrated strong capabilities in complex multi-step reasoning, opening new opportunities for automating optimization modeling. However, existing domain adaptation methods, originally designed for earlier instruction-tuned models, often fail to exploit the advanced reasoning patterns of modern LRMs -- In particular, we show that direct fine-tuning on traditional \textit{non-reflective} datasets leads to limited gains. To fully leverage LRMs' inherent reasoning abilities, we propose \textbf{CALM} (\textit{Corrective Adaptation with Lightweight Modification}), a framework that progressively refines LRMs within their native reasoning modes for optimization modeling tasks. In CALM, an expert intervener identifies reasoning flaws and provides concise corrective hints, which the LRM incorporates to produce improved reasoning trajectories. These interventions modify fewer than 2.6\% of generated tokens, but generate high-quality data for soft adaptation through supervised fine-tuning. The adapted model is then further improved through reinforcement learning. Building on CALM, we develop \textbf{STORM} (\textit{Smart Thinking Optimization Reasoning Model}), a 4B-parameter LRM that achieves a new state-of-the-art average accuracy of 68.9\% across five popular optimization modeling benchmarks, matching the performance of a 671B LRM. These results demonstrate that dynamic, hint-based data synthesis both preserves and amplifies the native reasoning patterns of modern LRMs, offering a more effective and scalable path towards expert-level performance on challenging optimization modeling tasks.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching LLM to be Persuasive: Reward-Enhanced Policy Optimization for Alignment frm Heterogeneous Rewards</title>
<link>https://arxiv.org/abs/2510.04214</link>
<guid>https://arxiv.org/abs/2510.04214</guid>
<content:encoded><![CDATA[
arXiv:2510.04214v1 Announce Type: new 
Abstract: We study deploying large language models (LLMs) as business development (BD) agents for persuasive price negotiation in online travel agencies (OTAs), where aligning traveler affordability and hotel profitability directly affects bookings, partner relationships, and access to travel. The agent must follow a Standard Operating Procedure (SOP) while conducting multi-turn persuasion, interpreting colloquial inputs, and adhering to guardrails (no over-promising, no hallucinations). Conventional post-training -- supervised fine-tuning (SFT) or single-source reward optimization -- overfits scripts, misses nuanced persuasive style, and fails to enforce verifiable business constraints.
  We propose Reward-Enhanced Policy Optimization (REPO), a reinforcement learning post-training framework that aligns an LLM with heterogeneous rewards: a preference-trained reward model (RM) for dense human alignment, a reward judge (RJ) for high-level persuasive behavior and SOP compliance, and programmatic reward functions (RF) for deterministic checks on numerics, formatting, and guardrails. A straightforward enhancement mechanism is proposed to combine the RM with RJ and RF signals to curb reward hacking and improve negotiation quality. In production-style evaluations -- approximately 150 turns from real dialogues and 225 turns from curated bad-case dialogues -- REPO lifts average dialogue rating to 4.63: +1.20 over base, +0.83 over Direct Preference Optimization (DPO); +0.33 over Group Relative Policy Optimization (GRPO), increases the share of conversations with at least one excellent response to 66.67% (+23.34 percentage points over GRPO), and achieves a 93.33% bad-case fix rate with 75.56% clean fixes, outperforming SFT, DPO, PPO, and GRPO. We also observe emergent capabilities -- proactive empathy, localized reasoning, calibrated tactics -- that surpass gold annotations.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Epistemic Diversity and Knowledge Collapse in Large Language Models</title>
<link>https://arxiv.org/abs/2510.04226</link>
<guid>https://arxiv.org/abs/2510.04226</guid>
<content:encoded><![CDATA[
arXiv:2510.04226v1 Announce Type: new 
Abstract: Large language models (LLMs) tend to generate lexically, semantically, and stylistically homogenous texts. This poses a risk of knowledge collapse, where homogenous LLMs mediate a shrinking in the range of accessible information over time. Existing works on homogenization are limited by a focus on closed-ended multiple-choice setups or fuzzy semantic features, and do not look at trends across time and cultural contexts. To overcome this, we present a new methodology to measure epistemic diversity, i.e., variation in real-world claims in LLM outputs, which we use to perform a broad empirical study of LLM knowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200 prompt variations sourced from real user chats. For the topics in our study, we show that while newer models tend to generate more diverse claims, nearly all models are less epistemically diverse than a basic web search. We find that model size has a negative impact on epistemic diversity, while retrieval-augmented generation (RAG) has a positive impact, though the improvement from RAG varies by the cultural context. Finally, compared to a traditional knowledge source (Wikipedia), we find that country-specific claims reflect the English language more than the local one, highlighting a gap in epistemic representation
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pushing on Multilingual Reasoning Models with Language-Mixed Chain-of-Thought</title>
<link>https://arxiv.org/abs/2510.04230</link>
<guid>https://arxiv.org/abs/2510.04230</guid>
<content:encoded><![CDATA[
arXiv:2510.04230v1 Announce Type: new 
Abstract: Recent frontier models employ long chain-of-thought reasoning to explore solution spaces in context and achieve stonger performance. While many works study distillation to build smaller yet capable models, most focus on English and little is known about language-specific reasoning. To bridge this gap, we first introduct **Language-Mixed CoT**, a reasoning schema that switches between English and a target language, using English as an anchor to excel in reasoning while minimizing translation artificats. As a Korean case study, we curate **Yi-Sang**: 5.79M native-Korean prompts from web Q&amp;A, exams, STEM, and code; 3.7M long reasoning traces generated from Qwen3-32B; and a targeted 260k high-yield subset. We train ninve models (4B-35B) across six families (Qwen2.5, Llama-3.1, Gemma-3, etc). Our best model, **KO-REAson-35B**, achieves state-of-the-art performance, with the highest overall average score (64.0 \pm 25), ranking first on 5/9 benchmarks and second on the remainder. Samller and mid-sized models also benefit substantially, with an average improvement of +18.6 points across teh evaluated nine benchmarks. Ablations show **Language-Mixed CoT** is more effective than monolingual CoT, also resulting in cross-lingual and mult-modal performance gains. We release our data-curation pipeline, evaluation system, datasets, and models to advance research on language-specific reasoning. Data and model collection: https://huggingface.co/KOREAson.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LongTail-Swap: benchmarking language models' abilities on rare words</title>
<link>https://arxiv.org/abs/2510.04268</link>
<guid>https://arxiv.org/abs/2510.04268</guid>
<content:encoded><![CDATA[
arXiv:2510.04268v1 Announce Type: new 
Abstract: Children learn to speak with a low amount of data and can be taught new words on a few-shot basis, making them particularly data-efficient learners. The BabyLM challenge aims at exploring language model (LM) training in the low-data regime but uses metrics that concentrate on the head of the word distribution. Here, we introduce LongTail-Swap (LT-Swap), a benchmark that focuses on the tail of the distribution, i.e., measures the ability of LMs to learn new words with very little exposure, like infants do. LT-Swap is a pretraining corpus-specific test set of acceptable versus unacceptable sentence pairs that isolate semantic and syntactic usage of rare words. Models are evaluated in a zero-shot fashion by computing the average log probabilities over the two members of each pair. We built two such test sets associated with the 10M words and 100M words BabyLM training sets, respectively, and evaluated 16 models from the BabyLM leaderboard. Our results not only highlight the poor performance of language models on rare words but also reveal that performance differences across LM architectures are much more pronounced in the long tail than in the head. This offers new insights into which architectures are better at handling rare word generalization. We've also made the code publicly avail
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing Geometry of Next Token Prediction Using Cumulant Expansion of the Softmax Entropy</title>
<link>https://arxiv.org/abs/2510.04285</link>
<guid>https://arxiv.org/abs/2510.04285</guid>
<content:encoded><![CDATA[
arXiv:2510.04285v1 Announce Type: new 
Abstract: We introduce a cumulant-expansion framework for quantifying how large language models (LLMs) internalize higher-order statistical structure during next-token prediction. By treating the softmax entropy of each layer's logit distribution as a perturbation around its "center" distribution, we derive closed-form cumulant observables that isolate successively higher-order correlations. Empirically, we track these cumulants in GPT-2 and Pythia models on Pile-10K prompts. (i) Structured prompts exhibit a characteristic rise-and-plateau profile across layers, whereas token-shuffled prompts remain flat, revealing the dependence of the cumulant profile on meaningful context. (ii) During training, all cumulants increase monotonically before saturating, directly visualizing the model's progression from capturing variance to learning skew, kurtosis, and higher-order statistical structures. (iii) Mathematical prompts show distinct cumulant signatures compared to general text, quantifying how models employ fundamentally different processing mechanisms for mathematical versus linguistic content. Together, these results establish cumulant analysis as a lightweight, mathematically grounded probe of feature-learning dynamics in high-dimensional neural networks.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SliceMoE: Routing Embedding Slices Instead of Tokens for Fine-Grained and Balanced Transformer Scaling</title>
<link>https://arxiv.org/abs/2510.04286</link>
<guid>https://arxiv.org/abs/2510.04286</guid>
<content:encoded><![CDATA[
arXiv:2510.04286v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) layers scale transformers by routing tokens to a sparse subset of feed-forward experts. Token-level routing, however, assigns an entire semantic spectrum to each expert, creating capacity bottlenecks, load-balancing pathologies, and limited specialization. We introduce SliceMoE, an architecture that routes contiguous slices of a token's hidden vector. A d-dimensional embedding is partitioned into S slices, and for each slice, a lightweight shared router predicts the top-k experts. Experts operate on their assigned slices independently, and outputs are reassembled, maintaining per-token FLOP efficiency. Because slices from different tokens interleave within an expert, utilization is naturally smoother. We propose a slice-level capacity loss, cross-slice dropout, and efficient fused batched GEMM kernels. Experiments on WikiText-103 language modeling, WMT En-De translation, and three text-classification datasets show SliceMoE attains up to 1.7x faster inference than dense baselines, 12 to 18 percent lower perplexity than parameter-matched token-MoE, and improved expert balance, with interpretable expertise over syntactic versus semantic subspaces.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PABSA: Hybrid Framework for Persian Aspect-Based Sentiment Analysis</title>
<link>https://arxiv.org/abs/2510.04291</link>
<guid>https://arxiv.org/abs/2510.04291</guid>
<content:encoded><![CDATA[
arXiv:2510.04291v1 Announce Type: new 
Abstract: Sentiment analysis is a key task in Natural Language Processing (NLP), enabling the extraction of meaningful insights from user opinions across various domains. However, performing sentiment analysis in Persian remains challenging due to the scarcity of labeled datasets, limited preprocessing tools, and the lack of high-quality embeddings and feature extraction methods. To address these limitations, we propose a hybrid approach that integrates machine learning (ML) and deep learning (DL) techniques for Persian aspect-based sentiment analysis (ABSA). In particular, we utilize polarity scores from multilingual BERT as additional features and incorporate them into a decision tree classifier, achieving an accuracy of 93.34%-surpassing existing benchmarks on the Pars-ABSA dataset. Additionally, we introduce a Persian synonym and entity dictionary, a novel linguistic resource that supports text augmentation through synonym and named entity replacement. Our results demonstrate the effectiveness of hybrid modeling and feature augmentation in advancing sentiment analysis for low-resource languages such as Persian.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equipping Retrieval-Augmented Large Language Models with Document Structure Awareness</title>
<link>https://arxiv.org/abs/2510.04293</link>
<guid>https://arxiv.org/abs/2510.04293</guid>
<content:encoded><![CDATA[
arXiv:2510.04293v1 Announce Type: new 
Abstract: While large language models (LLMs) demonstrate impressive capabilities, their reliance on parametric knowledge often leads to factual inaccuracies. Retrieval-Augmented Generation (RAG) mitigates this by leveraging external documents, yet existing approaches treat retrieved passages as isolated chunks, ignoring valuable structure that is crucial for document organization. Motivated by this gap, we propose Retrieve-DocumentRoute-Read (RDR2), a novel framework that explicitly incorporates structural information throughout the RAG process. RDR2 employs an LLM-based router to dynamically navigate document structure trees, jointly evaluating content relevance and hierarchical relationships to assemble optimal evidence. Our key innovation lies in formulating document routing as a trainable task, with automatic action curation and structure-aware passage selection inspired by human reading strategies. Through comprehensive evaluation on five challenging datasets, RDR2 achieves state-of-the-art performance, demonstrating that explicit structural awareness significantly enhances RAG systems' ability to acquire and utilize knowledge, particularly in complex scenarios requiring multi-document synthesis.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Language Model Hallucinations Through Distributional Correctness</title>
<link>https://arxiv.org/abs/2510.04302</link>
<guid>https://arxiv.org/abs/2510.04302</guid>
<content:encoded><![CDATA[
arXiv:2510.04302v1 Announce Type: new 
Abstract: Common evaluation paradigms for language models focus on scoring single responses through accuracy metrics or proper scoring rules, failing to capture the full richness of a model's belief state. Recent work illustrates that language models hallucinate in-part because they are optimised to be good test-takers under binary scoring schemes that reward any answer over abstention. While this insight naturally leads to penalty-based approaches, they ignore crucial distinctions in how models distribute uncertainty, for example between hedging toward incorrect answers versus hedging toward "I don't know" responses. A novel evaluation metric, the Distributional Correctness Score (DCS), is introduced to solve this problem, i.e., of not considering a model's entire probability distribution over answer choices. DCS naturally distinguishes between harmful overconfidence in wrong answers and uncertainty expressed through abstention, providing scores in an interpretable default range. Through theoretical analysis and illustrative examples, DCS is demonstrated to offer a more nuanced and aligned evaluation paradigm that incentivises models to express genuine uncertainty rather than guessing. Adapting 12 existing evaluation benchmarks to DCS's variants and measuring performance on six language models reveals that for half of the tested benchmarks scores are negative across all tested models, indicating significant tendencies towards hallucination.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Read the Scene, Not the Script: Outcome-Aware Safety for LLMs</title>
<link>https://arxiv.org/abs/2510.04320</link>
<guid>https://arxiv.org/abs/2510.04320</guid>
<content:encoded><![CDATA[
arXiv:2510.04320v1 Announce Type: new 
Abstract: Safety-aligned Large Language Models (LLMs) still show two dominant failure modes: they are easily jailbroken, or they over-refuse harmless inputs that contain sensitive surface signals. We trace both to a common cause: current models reason weakly about links between actions and outcomes and over-rely on surface-form signals, lexical or stylistic cues that do not encode consequences. We define this failure mode as Consequence-blindness. To study consequence-blindness, we build a benchmark named CB-Bench covering four risk scenarios that vary whether semantic risk aligns with outcome risk, enabling evaluation under both matched and mismatched conditions which are often ignored by existing safety benchmarks. Mainstream models consistently fail to separate these risks and exhibit consequence-blindness, indicating that consequence-blindness is widespread and systematic. To mitigate consequence-blindness, we introduce CS-Chain-4k, a consequence-reasoning dataset for safety alignment. Models fine-tuned on CS-Chain-4k show clear gains against semantic-camouflage jailbreaks and reduce over-refusal on harmless inputs, while maintaining utility and generalization on other benchmarks. These results clarify the limits of current alignment, establish consequence-aware reasoning as a core alignment goal and provide a more practical and reproducible evaluation path.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of Clinical Trials Reporting Quality using Large Language Models</title>
<link>https://arxiv.org/abs/2510.04338</link>
<guid>https://arxiv.org/abs/2510.04338</guid>
<content:encoded><![CDATA[
arXiv:2510.04338v1 Announce Type: new 
Abstract: Reporting quality is an important topic in clinical trial research articles, as it can impact clinical decisions. In this article, we test the ability of large language models to assess the reporting quality of this type of article using the Consolidated Standards of Reporting Trials (CONSORT). We create CONSORT-QA, an evaluation corpus from two studies on abstract reporting quality with CONSORT-abstract standards. We then evaluate the ability of different large generative language models (from the general domain or adapted to the biomedical domain) to correctly assess CONSORT criteria with different known prompting methods, including Chain-of-thought. Our best combination of model and prompting method achieves 85% accuracy. Using Chain-of-thought adds valuable information on the model's reasoning for completing the task.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time</title>
<link>https://arxiv.org/abs/2510.04340</link>
<guid>https://arxiv.org/abs/2510.04340</guid>
<content:encoded><![CDATA[
arXiv:2510.04340v1 Announce Type: new 
Abstract: Language model finetuning often results in learning undesirable traits in combination with desired ones. To address this, we propose inoculation prompting: modifying finetuning data by prepending a short system-prompt instruction that deliberately elicits the undesirable trait. At test time, we evaluate without the instruction; inoculated models have much lower expression of the trait than models trained with unmodified training data. Inoculation is selective: in a toy setting where assistant responses are always in Spanish and ALL-CAPS, an appropriate inoculation (e.g., ``You always speak in Spanish.'') teaches the model to capitalize responses while still responding in English. We find that inoculation is also effective across several additional settings: reducing emergent misalignment (EM) from task-specific finetuning, defending against backdoor injections, and mitigating the transmission of traits via subliminal learning. Follow-up analysis suggests a mechanism: making a trait less surprising via inoculation reduces optimization pressure to globally update the model, thereby reducing the degree of generalization. Our analysis relates to prior work on EM: inoculation explains prior findings that educational contexts mitigate EM from insecure code. Beyond demonstrating a simple and effective technique for selective learning, our results contribute to a better conceptual understanding of how and why language models generalize.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unmasking Backdoors: An Explainable Defense via Gradient-Attention Anomaly Scoring for Pre-trained Language Models</title>
<link>https://arxiv.org/abs/2510.04347</link>
<guid>https://arxiv.org/abs/2510.04347</guid>
<content:encoded><![CDATA[
arXiv:2510.04347v1 Announce Type: new 
Abstract: Pre-trained language models have achieved remarkable success across a wide range of natural language processing (NLP) tasks, particularly when fine-tuned on large, domain-relevant datasets. However, they remain vulnerable to backdoor attacks, where adversaries embed malicious behaviors using trigger patterns in the training data. These triggers remain dormant during normal usage, but, when activated, can cause targeted misclassifications. In this work, we investigate the internal behavior of backdoored pre-trained encoder-based language models, focusing on the consistent shift in attention and gradient attribution when processing poisoned inputs; where the trigger token dominates both attention and gradient signals, overriding the surrounding context. We propose an inference-time defense that constructs anomaly scores by combining token-level attention and gradient information. Extensive experiments on text classification tasks across diverse backdoor attack scenarios demonstrate that our method significantly reduces attack success rates compared to existing baselines. Furthermore, we provide an interpretability-driven analysis of the scoring mechanism, shedding light on trigger localization and the robustness of the proposed defense.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Consistency in Retrieval-Augmented Systems with Group Similarity Rewards</title>
<link>https://arxiv.org/abs/2510.04392</link>
<guid>https://arxiv.org/abs/2510.04392</guid>
<content:encoded><![CDATA[
arXiv:2510.04392v1 Announce Type: new 
Abstract: RAG systems are increasingly deployed in high-stakes domains where users expect outputs to be consistent across semantically equivalent queries. However, existing systems often exhibit significant inconsistencies due to variability in both the retriever and generator (LLM), undermining trust and reliability. In this work, we focus on information consistency, i.e., the requirement that outputs convey the same core content across semantically equivalent inputs. We introduce a principled evaluation framework that decomposes RAG consistency into retriever-level, generator-level, and end-to-end components, helping identify inconsistency sources. To improve consistency, we propose Paraphrased Set Group Relative Policy Optimization (PS-GRPO), an RL approach that leverages multiple rollouts across paraphrased set to assign group similarity rewards. We leverage PS-GRPO to achieve Information Consistent RAG (Con-RAG), training the generator to produce consistent outputs across paraphrased queries and remain robust to retrieval-induced variability. Because exact reward computation over paraphrase sets is computationally expensive, we also introduce a scalable approximation method that retains effectiveness while enabling efficient, large-scale training. Empirical evaluations across short-form, multi-hop, and long-form QA benchmarks demonstrate that Con-RAG significantly improves both consistency and accuracy over strong baselines, even in the absence of explicit ground-truth supervision. Our work provides practical solutions for evaluating and building reliable RAG systems for safety-critical deployments.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time Is Effort: Estimating Human Post-Editing Time for Grammar Error Correction Tool Evaluation</title>
<link>https://arxiv.org/abs/2510.04394</link>
<guid>https://arxiv.org/abs/2510.04394</guid>
<content:encoded><![CDATA[
arXiv:2510.04394v1 Announce Type: new 
Abstract: Text editing can involve several iterations of revision. Incorporating an efficient Grammar Error Correction (GEC) tool in the initial correction round can significantly impact further human editing effort and final text quality. This raises an interesting question to quantify GEC Tool usability: How much effort can the GEC Tool save users? We present the first large-scale dataset of post-editing (PE) time annotations and corrections for two English GEC test datasets (BEA19 and CoNLL14). We introduce Post-Editing Effort in Time (PEET) for GEC Tools as a human-focused evaluation scorer to rank any GEC Tool by estimating PE time-to-correct. Using our dataset, we quantify the amount of time saved by GEC Tools in text editing. Analyzing the edit type indicated that determining whether a sentence needs correction and edits like paraphrasing and punctuation changes had the greatest impact on PE time. Finally, comparison with human rankings shows that PEET correlates well with technical effort judgment, providing a new human-centric direction for evaluating GEC tool usability. We release our dataset and code at: https://github.com/ankitvad/PEET_Scorer.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations</title>
<link>https://arxiv.org/abs/2510.04398</link>
<guid>https://arxiv.org/abs/2510.04398</guid>
<content:encoded><![CDATA[
arXiv:2510.04398v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly deployed in high-risk domains. However, state-of-the-art LLMs often produce hallucinations, raising serious concerns about their reliability. Prior work has explored adversarial attacks for hallucination elicitation in LLMs, but it often produces unrealistic prompts, either by inserting gibberish tokens or by altering the original meaning. As a result, these approaches offer limited insight into how hallucinations may occur in practice. While adversarial attacks in computer vision often involve realistic modifications to input images, the problem of finding realistic adversarial prompts for eliciting LLM hallucinations has remained largely underexplored. To address this gap, we propose Semantically Equivalent and Coherent Attacks (SECA) to elicit hallucinations via realistic modifications to the prompt that preserve its meaning while maintaining semantic coherence. Our contributions are threefold: (i) we formulate finding realistic attacks for hallucination elicitation as a constrained optimization problem over the input prompt space under semantic equivalence and coherence constraints; (ii) we introduce a constraint-preserving zeroth-order method to effectively search for adversarial yet feasible prompts; and (iii) we demonstrate through experiments on open-ended multiple-choice question answering tasks that SECA achieves higher attack success rates while incurring almost no constraint violations compared to existing methods. SECA highlights the sensitivity of both open-source and commercial gradient-inaccessible LLMs to realistic and plausible prompt variations. Code is available at https://github.com/Buyun-Liang/SECA.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Preserve Semantic Isotopies in Story Continuations</title>
<link>https://arxiv.org/abs/2510.04400</link>
<guid>https://arxiv.org/abs/2510.04400</guid>
<content:encoded><![CDATA[
arXiv:2510.04400v1 Announce Type: new 
Abstract: In this work, we explore the relevance of textual semantics to Large Language Models (LLMs), extending previous insights into the connection between distributional semantics and structural semantics. We investigate whether LLM-generated texts preserve semantic isotopies. We design a story continuation experiment using 10,000 ROCStories prompts completed by five LLMs. We first validate GPT-4o's ability to extract isotopies from a linguistic benchmark, then apply it to the generated stories. We then analyze structural (coverage, density, spread) and semantic properties of isotopies to assess how they are affected by completion. Results show that LLM completion within a given token horizon preserves semantic isotopies across multiple properties.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Good Intentions Beyond ACL: Who Does NLP for Social Good, and Where?</title>
<link>https://arxiv.org/abs/2510.04434</link>
<guid>https://arxiv.org/abs/2510.04434</guid>
<content:encoded><![CDATA[
arXiv:2510.04434v1 Announce Type: new 
Abstract: The social impact of Natural Language Processing (NLP) is increasingly important, with a rising community focus on initiatives related to NLP for Social Good (NLP4SG). Indeed, in recent years, almost 20% of all papers in the ACL Anthology address topics related to social good as defined by the UN Sustainable Development Goals (Adauto et al., 2023). In this study, we take an author- and venue-level perspective to map the landscape of NLP4SG, quantifying the proportion of work addressing social good concerns both within and beyond the ACL community, by both core ACL contributors and non-ACL authors. With this approach we discover two surprising facts about the landscape of NLP4SG. First, ACL authors are dramatically more likely to do work addressing social good concerns when publishing in venues outside of ACL. Second, the vast majority of publications using NLP techniques to address concerns of social good are done by non-ACL authors in venues outside of ACL. We discuss the implications of these findings on agenda-setting considerations for the ACL community related to NLP4SG.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Role of Unobserved Sequences on Sample-based Uncertainty Quantification for LLMs</title>
<link>https://arxiv.org/abs/2510.04439</link>
<guid>https://arxiv.org/abs/2510.04439</guid>
<content:encoded><![CDATA[
arXiv:2510.04439v1 Announce Type: new 
Abstract: Quantifying uncertainty in large language models (LLMs) is important for safety-critical applications because it helps spot incorrect answers, known as hallucinations. One major trend of uncertainty quantification methods is based on estimating the entropy of the distribution of the LLM's potential output sequences. This estimation is based on a set of output sequences and associated probabilities obtained by querying the LLM several times. In this paper, we advocate and experimentally show that the probability of unobserved sequences plays a crucial role, and we recommend future research to integrate it to enhance such LLM uncertainty quantification methods.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Forgetting Between Supervised and Reinforcement Learning Yields Stronger Reasoners</title>
<link>https://arxiv.org/abs/2510.04454</link>
<guid>https://arxiv.org/abs/2510.04454</guid>
<content:encoded><![CDATA[
arXiv:2510.04454v1 Announce Type: new 
Abstract: Large Language Models (LLMs) show strong reasoning abilities, often amplified by Chain-of-Thought (CoT) prompting and reinforcement learning (RL). Although RL algorithms can substantially improve reasoning, they struggle to expand reasoning boundaries because they learn from their own reasoning trajectories rather than acquiring external knowledge. Supervised fine-tuning (SFT) offers complementary benefits but typically requires large-scale data and risks overfitting. Recent attempts to combine SFT and RL face three main challenges: data inefficiency, algorithm-specific designs, and catastrophic forgetting. We propose a plug-and-play framework that dynamically integrates SFT into RL by selecting challenging examples for SFT. This approach reduces SFT data requirements and remains agnostic to the choice of RL or SFT algorithm. To mitigate catastrophic forgetting of RL-acquired skills during SFT, we select high-entropy tokens for loss calculation and freeze parameters identified as critical for RL. Our method achieves state-of-the-art (SoTA) reasoning performance using only 1.5% of the SFT data and 20.4% of the RL data used by prior SoTA, providing an efficient and plug-and-play solution for combining SFT and RL in reasoning post-training.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compressed Convolutional Attention: Efficient Attention in a Compressed Latent Space</title>
<link>https://arxiv.org/abs/2510.04476</link>
<guid>https://arxiv.org/abs/2510.04476</guid>
<content:encoded><![CDATA[
arXiv:2510.04476v1 Announce Type: new 
Abstract: Multi-headed Attention's (MHA) quadratic compute and linearly growing KV-cache make long-context transformers expensive to train and serve. Prior works such as Grouped Query Attention (GQA) and Multi-Latent Attention (MLA) shrink the cache, speeding decode, but leave compute, which determines prefill and training speed, largely unchanged. We introduce Compressed Convolutional Attention (CCA), a novel attention method which down-projects queries, keys, and values and performs the entire attention operation inside the shared latent space. This simple design dramatically cuts parameters, KV-cache, and FLOPs all at once by the desired compression factor. Because CCA is orthogonal to head-sharing, we combine the two to form Compressed Convolutional Grouped Query Attention (CCGQA), which further tightens the compute-bandwidth Pareto frontier so that users can tune compression toward either FLOP or memory limits without sacrificing quality. Experiments show that CCGQA consistently outperforms both GQA and MLA at equal KV-cache compression on dense and MoE models. Additionally, we show that CCGQA outperforms all other attention methods on MoE models with half the KV-cache of GQA and MLA, achieving an 8x KV-cache compression with no drop in performance compared to standard MHA. CCA and CCGQA also dramatically reduce the FLOP cost of attention which leads to substantially faster training and prefill than existing methods. On H100 GPUs, our fused CCA/CCGQA kernel reduces prefill latency by about 1.7x at a sequence length of 16k relative to MHA, and accelerates backward by about 1.3x.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Psychological Steering in LLMs: An Evaluation of Effectiveness and Trustworthiness</title>
<link>https://arxiv.org/abs/2510.04484</link>
<guid>https://arxiv.org/abs/2510.04484</guid>
<content:encoded><![CDATA[
arXiv:2510.04484v1 Announce Type: new 
Abstract: The ability to control LLMs' emulated emotional states and personality traits is essential for enabling rich, human-centered interactions in socially interactive settings. We introduce PsySET, a Psychologically-informed benchmark to evaluate LLM Steering Effectiveness and Trustworthiness across the emotion and personality domains. Our study spans four models from different LLM families paired with various steering strategies, including prompting, fine-tuning, and representation engineering. Our results indicate that prompting is consistently effective but limited in intensity control, whereas vector injections achieve finer controllability while slightly reducing output quality. Moreover, we explore the trustworthiness of steered LLMs by assessing safety, truthfulness, fairness, and ethics, highlighting potential side effects and behavioral shifts. Notably, we observe idiosyncratic effects; for instance, even a positive emotion like joy can degrade robustness to adversarial factuality, lower privacy awareness, and increase preferential bias. Meanwhile, anger predictably elevates toxicity yet strengthens leakage resistance. Our framework establishes the first holistic evaluation of emotion and personality steering, offering insights into its interpretability and reliability for socially interactive applications.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenQuest: An LLM-based Text Adventure Game for Language Learners</title>
<link>https://arxiv.org/abs/2510.04498</link>
<guid>https://arxiv.org/abs/2510.04498</guid>
<content:encoded><![CDATA[
arXiv:2510.04498v1 Announce Type: new 
Abstract: GenQuest is a generative text adventure game that leverages Large Language Models (LLMs) to facilitate second language learning through immersive, interactive storytelling. The system engages English as a Foreign Language (EFL) learners in a collaborative "choose-your-own-adventure" style narrative, dynamically generated in response to learner choices. Game mechanics such as branching decision points and story milestones are incorporated to maintain narrative coherence while allowing learner-driven plot development. Key pedagogical features include content generation tailored to each learner's proficiency level, and a vocabulary assistant that provides in-context explanations of learner-queried text strings, ranging from words and phrases to sentences. Findings from a pilot study with university EFL students in China indicate promising vocabulary gains and positive user perceptions. Also discussed are suggestions from participants regarding the narrative length and quality, and the request for multi-modal content such as illustrations.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRACE: Generative Representation Learning via Contrastive Policy Optimization</title>
<link>https://arxiv.org/abs/2510.04506</link>
<guid>https://arxiv.org/abs/2510.04506</guid>
<content:encoded><![CDATA[
arXiv:2510.04506v1 Announce Type: new 
Abstract: Prevailing methods for training Large Language Models (LLMs) as text encoders rely on contrastive losses that treat the model as a black box function, discarding its generative and reasoning capabilities in favor of static embeddings. We introduce GRACE (Generative Representation Learning via Contrastive Policy Optimization), a novel framework that reimagines contrastive signals not as losses to be minimized, but as rewards that guide a generative policy. In GRACE, the LLM acts as a policy that produces explicit, human-interpretable rationales--structured natural language explanations of its semantic understanding. These rationales are then encoded into high-quality embeddings via mean pooling. Using policy gradient optimization, we train the model with a multi-component reward function that maximizes similarity between query positive pairs and minimizes similarity with negatives. This transforms the LLM from an opaque encoder into an interpretable agent whose reasoning process is transparent and inspectable. On MTEB benchmark, GRACE yields broad cross category gains: averaged over four backbones, the supervised setting improves overall score by 11.5% over base models, and the unsupervised variant adds 6.9%, while preserving general capabilities. This work treats contrastive objectives as rewards over rationales, unifying representation learning with generation to produce stronger embeddings and transparent rationales. The model, data and code are available at https://github.com/GasolSun36/GRACE.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-grained auxiliary learning for real-world product recommendation</title>
<link>https://arxiv.org/abs/2510.04551</link>
<guid>https://arxiv.org/abs/2510.04551</guid>
<content:encoded><![CDATA[
arXiv:2510.04551v1 Announce Type: new 
Abstract: Product recommendation is the task of recovering the closest items to a given query within a large product corpora. Generally, one can determine if top-ranked products are related to the query by applying a similarity threshold; exceeding it deems the product relevant, otherwise manual revision is required. Despite being a well-known problem, the integration of these models in real-world systems is often overlooked. In particular, production systems have strong coverage requirements, i.e., a high proportion of recommendations must be automated. In this paper we propose ALC , an Auxiliary Learning strategy that boosts Coverage through learning fine-grained embeddings. Concretely, we introduce two training objectives that leverage the hardest negatives in the batch to build discriminative training signals between positives and negatives. We validate ALC using three extreme multi-label classification approaches in two product recommendation datasets; LF-AmazonTitles-131K and Tech and Durables (proprietary), demonstrating state-of-the-art coverage rates when combined with a recent threshold-consistent margin loss.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Detect Ambiguous Plural Reference? An Analysis of Split-Antecedent and Mereological Reference</title>
<link>https://arxiv.org/abs/2510.04581</link>
<guid>https://arxiv.org/abs/2510.04581</guid>
<content:encoded><![CDATA[
arXiv:2510.04581v1 Announce Type: new 
Abstract: Our goal is to study how LLMs represent and interpret plural reference in ambiguous and unambiguous contexts. We ask the following research questions: (1) Do LLMs exhibit human-like preferences in representing plural reference? (2) Are LLMs able to detect ambiguity in plural anaphoric expressions and identify possible referents? To address these questions, we design a set of experiments, examining pronoun production using next-token prediction tasks, pronoun interpretation, and ambiguity detection using different prompting strategies. We then assess how comparable LLMs are to humans in formulating and interpreting plural reference. We find that LLMs are sometimes aware of possible referents of ambiguous pronouns. However, they do not always follow human reference when choosing between interpretations, especially when the possible interpretation is not explicitly mentioned. In addition, they struggle to identify ambiguity without direct instruction. Our findings also reveal inconsistencies in the results across different types of experiments.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robustness assessment of large audio language models in multiple-choice evaluation</title>
<link>https://arxiv.org/abs/2510.04584</link>
<guid>https://arxiv.org/abs/2510.04584</guid>
<content:encoded><![CDATA[
arXiv:2510.04584v1 Announce Type: new 
Abstract: Recent advances in large audio language models (LALMs) have primarily been assessed using a multiple-choice question answering (MCQA) framework. However, subtle changes, such as shifting the order of choices, result in substantially different results. Existing MCQA frameworks do not account for this variability and report a single accuracy number per benchmark or category. We dive into the MCQA evaluation framework and conduct a systematic study spanning three benchmarks (MMAU, MMAR and MMSU) and four models: Audio Flamingo 2, Audio Flamingo 3, Qwen2.5-Omni-7B-Instruct, and Kimi-Audio-7B-Instruct. Our findings indicate that models are sensitive not only to the ordering of choices, but also to the paraphrasing of the question and the choices. Finally, we propose a simpler evaluation protocol and metric that account for subtle variations and provide a more detailed evaluation report of LALMs within the MCQA framework.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedSRD: Sparsify-Reconstruct-Decompose for Communication-Efficient Federated Large Language Models Fine-Tuning</title>
<link>https://arxiv.org/abs/2510.04601</link>
<guid>https://arxiv.org/abs/2510.04601</guid>
<content:encoded><![CDATA[
arXiv:2510.04601v1 Announce Type: new 
Abstract: The current paradigm of training large language models (LLMs) on publicly available Web data is becoming unsustainable, with high-quality data sources in specialized domains nearing exhaustion. Federated Learning (FL) emerges as a practical solution for the next generation of AI on a decentralized Web, enabling privacy-preserving collaborative fine-tuning by leveraging private data distributed across a global client base. While Low-Rank Adaptation (LoRA) is the standard for efficient fine-tuning, its application in federated settings presents a critical challenge: communication overhead remains a significant bottleneck across the Web's heterogeneous network conditions. The structural redundancy within LoRA parameters not only incurs a heavy communication burden but also introduces conflicts when aggregating client updates. To address this, we propose FedSRD, a Sparsify-Reconstruct-Decompose framework designed for communication-efficient FL. We first introduce an importance-aware sparsification method that preserves the structural integrity of LoRA updates to reduce the uploaded parameter count. The server then reconstructs and aggregates these updates in a full-rank space to mitigate conflicts. Finally, it decomposes the global update into a sparse low-rank format for broadcast, ensuring a symmetrically efficient cycle. We also propose an efficient variant, FedSRD-e, to reduce computational overhead. Experimental results on 10 benchmarks demonstrate that our framework significantly reduces communication costs by up to 90\% while even improving model performance on heterogeneous client data.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Learning Using Graph Embeddings for Domain Adaptation of Language Models in the Process Industry</title>
<link>https://arxiv.org/abs/2510.04631</link>
<guid>https://arxiv.org/abs/2510.04631</guid>
<content:encoded><![CDATA[
arXiv:2510.04631v1 Announce Type: new 
Abstract: Recent trends in NLP utilize knowledge graphs (KGs) to enhance pretrained language models by incorporating additional knowledge from the graph structures to learn domain-specific terminology or relationships between documents that might otherwise be overlooked. This paper explores how SciNCL, a graph-aware neighborhood contrastive learning methodology originally designed for scientific publications, can be applied to the process industry domain, where text logs contain crucial information about daily operations and are often structured as sparse KGs. Our experiments demonstrate that language models fine-tuned with triplets derived from GE outperform a state-of-the-art mE5-large text encoder by 9.8-14.3% (5.4-8.0p) on the proprietary process industry text embedding benchmark (PITEB) while being 3-5 times smaller in size.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating LLMs for Demographic-Targeted Social Bias Detection: A Comprehensive Benchmark Study</title>
<link>https://arxiv.org/abs/2510.04641</link>
<guid>https://arxiv.org/abs/2510.04641</guid>
<content:encoded><![CDATA[
arXiv:2510.04641v1 Announce Type: new 
Abstract: Large-scale web-scraped text corpora used to train general-purpose AI models often contain harmful demographic-targeted social biases, creating a regulatory need for data auditing and developing scalable bias-detection methods. Although prior work has investigated biases in text datasets and related detection methods, these studies remain narrow in scope. They typically focus on a single content type (e.g., hate speech), cover limited demographic axes, overlook biases affecting multiple demographics simultaneously, and analyze limited techniques. Consequently, practitioners lack a holistic understanding of the strengths and limitations of recent large language models (LLMs) for automated bias detection. In this study, we present a comprehensive evaluation framework aimed at English texts to assess the ability of LLMs in detecting demographic-targeted social biases. To align with regulatory requirements, we frame bias detection as a multi-label task using a demographic-focused taxonomy. We then conduct a systematic evaluation with models across scales and techniques, including prompting, in-context learning, and fine-tuning. Using twelve datasets spanning diverse content types and demographics, our study demonstrates the promise of fine-tuned smaller models for scalable detection. However, our analyses also expose persistent gaps across demographic axes and multi-demographic targeted biases, underscoring the need for more effective and scalable auditing frameworks.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FT-MDT: Extracting Decision Trees from Medical Texts via a Novel Low-rank Adaptation Method</title>
<link>https://arxiv.org/abs/2510.04655</link>
<guid>https://arxiv.org/abs/2510.04655</guid>
<content:encoded><![CDATA[
arXiv:2510.04655v1 Announce Type: new 
Abstract: Knowledge of the medical decision process, which can be modeled as medical decision trees (MDTs), is critical to building clinical decision support systems. However, current MDT construction methods rely heavily on time-consuming and laborious manual annotation. To address this challenge, we propose PI-LoRA (Path-Integrated LoRA), a novel low-rank adaptation method for automatically extracting MDTs from clinical guidelines and textbooks. We integrate gradient path information to capture synergistic effects between different modules, enabling more effective and reliable rank allocation. This framework ensures that the most critical modules receive appropriate rank allocations while less important ones are pruned, resulting in a more efficient and accurate model for extracting medical decision trees from clinical texts. Extensive experiments on medical guideline datasets demonstrate that our PI-LoRA method significantly outperforms existing parameter-efficient fine-tuning approaches for the Text2MDT task, achieving better accuracy with substantially reduced model complexity. The proposed method achieves state-of-the-art results while maintaining a lightweight architecture, making it particularly suitable for clinical decision support systems where computational resources may be limited.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FocusMed: A Large Language Model-based Framework for Enhancing Medical Question Summarization with Focus Identification</title>
<link>https://arxiv.org/abs/2510.04671</link>
<guid>https://arxiv.org/abs/2510.04671</guid>
<content:encoded><![CDATA[
arXiv:2510.04671v1 Announce Type: new 
Abstract: With the rapid development of online medical platforms, consumer health questions (CHQs) are inefficient in diagnosis due to redundant information and frequent non-professional terms. The medical question summary (MQS) task aims to transform CHQs into streamlined doctors' frequently asked questions (FAQs), but existing methods still face challenges such as poor identification of question focus and model hallucination. This paper explores the potential of large language models (LLMs) in the MQS task and finds that direct fine-tuning is prone to focus identification bias and generates unfaithful content. To this end, we propose an optimization framework based on core focus guidance. First, a prompt template is designed to drive the LLMs to extract the core focus from the CHQs that is faithful to the original text. Then, a fine-tuning dataset is constructed in combination with the original CHQ-FAQ pairs to improve the ability to identify the focus of the question. Finally, a multi-dimensional quality evaluation and selection mechanism is proposed to comprehensively improve the quality of the summary from multiple dimensions. We conduct comprehensive experiments on two widely-adopted MQS datasets using three established evaluation metrics. The proposed framework achieves state-of-the-art performance across all measures, demonstrating a significant boost in the model's ability to identify critical focus of questions and a notable mitigation of hallucinations. The source codes are freely available at https://github.com/DUT-LiuChao/FocusMed.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Tool-Integrated Policy Optimization</title>
<link>https://arxiv.org/abs/2510.04678</link>
<guid>https://arxiv.org/abs/2510.04678</guid>
<content:encoded><![CDATA[
arXiv:2510.04678v1 Announce Type: new 
Abstract: Large language models (LLMs) increasingly rely on multi-turn tool-integrated planning for knowledge-intensive and complex reasoning tasks. Existing implementations typically rely on a single agent, but they suffer from limited context length and noisy tool responses. A natural solution is to adopt a multi-agent framework with planner- and worker-agents to manage context. However, no existing methods support effective reinforcement learning post-training of tool-integrated multi-agent frameworks. To address this gap, we propose Multi-Agent Tool-Integrated Policy Optimization (MATPO), which enables distinct roles (planner and worker) to be trained within a single LLM instance using role-specific prompts via reinforcement learning. MATPO is derived from a principled credit assignment mechanism across planner and worker rollouts. This design eliminates the need to deploy multiple LLMs, which would be memory-intensive, while preserving the benefits of specialization. Experiments on GAIA-text, WebWalkerQA, and FRAMES show that MATPO consistently outperforms single-agent baselines by an average of 18.38% relative improvement in performance and exhibits greater robustness to noisy tool outputs. Our findings highlight the effectiveness of unifying multiple agent roles within a single LLM and provide practical insights for stable and efficient multi-agent RL training.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TiTok: Transfer Token-level Knowledge via Contrastive Excess to Transplant LoRA</title>
<link>https://arxiv.org/abs/2510.04682</link>
<guid>https://arxiv.org/abs/2510.04682</guid>
<content:encoded><![CDATA[
arXiv:2510.04682v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are widely applied in real world scenarios, but fine-tuning them comes with significant computational and storage costs. Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA mitigate these costs, but the adapted parameters are dependent on the base model and cannot be transferred across different backbones. One way to address this issue is through knowledge distillation, but its effectiveness inherently depends on training data. Recent work such as TransLoRA avoids this by generating synthetic data, but this adds complexity because it requires training an additional discriminator model. In this paper, we propose TiTok, a new framework that enables effective LoRA Transplantation through Token-level knowledge transfer. Specifically, TiTok captures task-relevant information through a contrastive excess between a source model with and without LoRA. This excess highlights informative tokens and enables selective filtering of synthetic data, all without additional models or overhead. Through experiments on three benchmarks across multiple transfer settings, our experiments show that the proposed method is consistently effective, achieving average performance gains of +4~8% compared to baselines overall.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual Routing in Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2510.04694</link>
<guid>https://arxiv.org/abs/2510.04694</guid>
<content:encoded><![CDATA[
arXiv:2510.04694v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) architectures have become the key to scaling modern LLMs, yet little is understood about how their sparse routing dynamics respond to multilingual data. In this work, we analyze expert routing patterns using parallel multilingual datasets and present highly interpretable layer-wise phenomena. We find that MoE models route tokens in language-specific ways in the early and late decoder layers but exhibit significant cross-lingual routing alignment in middle layers, mirroring parameter-sharing trends observed in dense LLMs. In particular, we reveal a clear, strong correlation between a model's performance in a given language and how similarly its tokens are routed to English in these layers. Extending beyond correlation, we explore inference-time interventions that induce higher cross-lingual routing alignment. We introduce a method that steers the router by promoting middle-layer task experts frequently activated in English, and it successfully increases multilingual performance. These 1-2% gains are remarkably consistent across two evaluation tasks, three models, and 15+ languages, especially given that these simple interventions override routers of extensively trained, state-of-the-art LLMs. In comparison, interventions outside of the middle layers or targeting multilingual-specialized experts only yield performance degradation. Altogether, we present numerous findings that explain how MoEs process non-English text and demonstrate that generalization is limited by the model's ability to leverage language-universal experts in all languages.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JSON Whisperer: Efficient JSON Editing with LLMs</title>
<link>https://arxiv.org/abs/2510.04717</link>
<guid>https://arxiv.org/abs/2510.04717</guid>
<content:encoded><![CDATA[
arXiv:2510.04717v1 Announce Type: new 
Abstract: Large language models (LLMs) can modify JSON documents through natural language commands, but current approaches regenerate entire structures for each edit, resulting in computational inefficiency. We present JSON Whisperer, a framework that enables LLMs to generate RFC 6902 diff patches-expressing only the necessary modifications-rather than complete documents. We identify two key challenges in patch-based editing: (1) LLMs often miss related updates when generating isolated patches, and (2) array manipulations require tracking index shifts across operations, which LLMs handle poorly. To address these issues, we introduce EASE (Explicitly Addressed Sequence Encoding), which transforms arrays into dictionaries with stable keys, eliminating index arithmetic complexities. Our evaluation shows that patch generation with EASE reduces token usage by 31% while maintaining edit quality within 5% of full regeneration with particular gains for complex instructions and list manipulations. The dataset is available at: https://github.com/emnlp2025/JSON-Whisperer/
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Low-Resource Speech-Driven NLP Pipeline for Sinhala Dyslexia Assistance</title>
<link>https://arxiv.org/abs/2510.04750</link>
<guid>https://arxiv.org/abs/2510.04750</guid>
<content:encoded><![CDATA[
arXiv:2510.04750v1 Announce Type: new 
Abstract: Dyslexia in adults remains an under-researched and under-served area, particularly in non-English-speaking contexts, despite its significant impact on personal and professional lives. This work addresses that gap by focusing on Sinhala, a low-resource language with limited tools for linguistic accessibility. We present an assistive system explicitly designed for Sinhala-speaking adults with dyslexia. The system integrates Whisper for speech-to-text conversion, SinBERT, an open-sourced fine-tuned BERT model trained for Sinhala to identify common dyslexic errors, and a combined mT5 and Mistral-based model to generate corrected text. Finally, the output is converted back to speech using gTTS, creating a complete multimodal feedback loop. Despite the challenges posed by limited Sinhala-language datasets, the system achieves 0.66 transcription accuracy and 0.7 correction accuracy with 0.65 overall system accuracy. These results demonstrate both the feasibility and effectiveness of the approach. Ultimately, this work highlights the importance of inclusive Natural Language Processing (NLP) technologies in underrepresented languages and showcases a practical
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ModernBERT + ColBERT: Enhancing biomedical RAG through an advanced re-ranking retriever</title>
<link>https://arxiv.org/abs/2510.04757</link>
<guid>https://arxiv.org/abs/2510.04757</guid>
<content:encoded><![CDATA[
arXiv:2510.04757v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) is a powerful technique for enriching Large Language Models (LLMs) with external knowledge, allowing for factually grounded responses, a critical requirement in high-stakes domains such as healthcare. However, the efficacy of RAG systems is fundamentally restricted by the performance of their retrieval module, since irrelevant or semantically misaligned documents directly compromise the accuracy of the final generated response. General-purpose dense retrievers can struggle with the nuanced language of specialised domains, while the high accuracy of in-domain models is often achieved at prohibitive computational costs. In this work, we aim to address this trade-off by developing and evaluating a two-stage retrieval architecture that combines a lightweight ModernBERT bidirectional encoder for efficient initial candidate retrieval with a ColBERTv2 late-interaction model for fine-grained re-ranking. We conduct comprehensive evaluations of our retriever module performance and RAG system performance in the biomedical context, fine-tuning the IR module using 10k question-passage pairs from PubMedQA. Our analysis of the retriever module confirmed the positive impact of the ColBERT re-ranker, which improved Recall@3 by up to 4.2 percentage points compared to its retrieve-only counterpart. When integrated into the biomedical RAG, our IR module leads to a state-of-the-art average accuracy of 0.4448 on the five tasks of the MIRAGE question-answering benchmark, outperforming strong baselines such as MedCPT (0.4436). Our ablation studies reveal that this performance is critically dependent on a joint fine-tuning process that aligns the retriever and re-ranker; otherwise, the re-ranker might degrade the performance.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are BabyLMs Deaf to Gricean Maxims? A Pragmatic Evaluation of Sample-efficient Language Models</title>
<link>https://arxiv.org/abs/2510.04764</link>
<guid>https://arxiv.org/abs/2510.04764</guid>
<content:encoded><![CDATA[
arXiv:2510.04764v1 Announce Type: new 
Abstract: Implicit meanings are integral to human communication, making it essential for language models to be capable of identifying and interpreting them. Grice (1975) proposed a set of conversational maxims that guide cooperative dialogue, noting that speakers may deliberately violate these principles to express meanings beyond literal words, and that listeners, in turn, recognize such violations to draw pragmatic inferences.
  Building on Surian et al. (1996)'s study of children's sensitivity to violations of Gricean maxims, we introduce a novel benchmark to test whether language models pretrained on less than 10M and less than 100M tokens can distinguish maxim-adhering from maxim-violating utterances. We compare these BabyLMs across five maxims and situate their performance relative to children and a Large Language Model (LLM) pretrained on 3T tokens.
  We find that overall, models trained on less than 100M tokens outperform those trained on less than 10M, yet fall short of child-level and LLM competence. Our results suggest that modest data increases improve some aspects of pragmatic behavior, leading to finer-grained differentiation between pragmatic dimensions.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Architectures for Language Models: Systematic Analysis and Design Insights</title>
<link>https://arxiv.org/abs/2510.04800</link>
<guid>https://arxiv.org/abs/2510.04800</guid>
<content:encoded><![CDATA[
arXiv:2510.04800v1 Announce Type: new 
Abstract: Recent progress in large language models demonstrates that hybrid architectures--combining self-attention mechanisms with structured state space models like Mamba--can achieve a compelling balance between modeling quality and computational efficiency, particularly for long-context tasks. While these hybrid models show promising performance, systematic comparisons of hybridization strategies and analyses on the key factors behind their effectiveness have not been clearly shared to the community. In this work, we present a holistic evaluation of hybrid architectures based on inter-layer (sequential) or intra-layer (parallel) fusion. We evaluate these designs from a variety of perspectives: language modeling performance, long-context capabilities, scaling analysis, and training and inference efficiency. By investigating the core characteristics of their computational primitive, we identify the most critical elements for each hybridization strategy and further propose optimal design recipes for both hybrid models. Our comprehensive analysis provides practical guidance and valuable insights for developing hybrid language models, facilitating the optimization of architectural configurations.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How I Built ASR for Endangered Languages with a Spoken Dictionary</title>
<link>https://arxiv.org/abs/2510.04832</link>
<guid>https://arxiv.org/abs/2510.04832</guid>
<content:encoded><![CDATA[
arXiv:2510.04832v1 Announce Type: new 
Abstract: Nearly half of the world's languages are endangered. Speech technologies such as Automatic Speech Recognition (ASR) are central to revival efforts, yet most languages remain unsupported because standard pipelines expect utterance-level supervised data. Speech data often exist for endangered languages but rarely match these formats. Manx Gaelic ($\sim$2,200 speakers), for example, has had transcribed speech since 1948, yet remains unsupported by modern systems. In this paper, we explore how little data, and in what form, is needed to build ASR for critically endangered languages. We show that a short-form pronunciation resource is a viable alternative, and that 40 minutes of such data produces usable ASR for Manx ($<$50\% WER). We replicate our approach, applying it to Cornish ($\sim$600 speakers), another critically endangered language. Results show that the barrier to entry, in quantity and form, is far lower than previously thought, giving hope to endangered language communities that cannot afford to meet the requirements arbitrarily imposed upon them.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instability in Downstream Task Performance During LLM Pretraining</title>
<link>https://arxiv.org/abs/2510.04848</link>
<guid>https://arxiv.org/abs/2510.04848</guid>
<content:encoded><![CDATA[
arXiv:2510.04848v1 Announce Type: new 
Abstract: When training large language models (LLMs), it is common practice to track downstream task performance throughout the training process and select the checkpoint with the highest validation score. However, downstream metrics often exhibit substantial fluctuations, making it difficult to identify the checkpoint that truly represents the best-performing model. In this study, we empirically analyze the stability of downstream task performance in an LLM trained on diverse web-scale corpora. We find that task scores frequently fluctuate throughout training, both at the aggregate and example levels. To address this instability, we investigate two post-hoc checkpoint integration methods: checkpoint averaging and ensemble, motivated by the hypothesis that aggregating neighboring checkpoints can reduce performance volatility. We demonstrate both empirically and theoretically that these methods improve downstream performance stability without requiring any changes to the training procedure.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with PsiloQA</title>
<link>https://arxiv.org/abs/2510.04849</link>
<guid>https://arxiv.org/abs/2510.04849</guid>
<content:encoded><![CDATA[
arXiv:2510.04849v1 Announce Type: new 
Abstract: Hallucination detection remains a fundamental challenge for the safe and reliable deployment of large language models (LLMs), especially in applications requiring factual accuracy. Existing hallucination benchmarks often operate at the sequence level and are limited to English, lacking the fine-grained, multilingual supervision needed for a comprehensive evaluation. In this work, we introduce PsiloQA, a large-scale, multilingual dataset annotated with span-level hallucinations across 14 languages. PsiloQA is constructed through an automated three-stage pipeline: generating question-answer pairs from Wikipedia using GPT-4o, eliciting potentially hallucinated answers from diverse LLMs in a no-context setting, and automatically annotating hallucinated spans using GPT-4o by comparing against golden answers and retrieved context. We evaluate a wide range of hallucination detection methods -- including uncertainty quantification, LLM-based tagging, and fine-tuned encoder models -- and show that encoder-based models achieve the strongest performance across languages. Furthermore, PsiloQA demonstrates effective cross-lingual generalization and supports robust knowledge transfer to other benchmarks, all while being significantly more cost-efficient than human-annotated datasets. Our dataset and results advance the development of scalable, fine-grained hallucination detection in multilingual settings.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Distillation Data from Reasoning Models</title>
<link>https://arxiv.org/abs/2510.04850</link>
<guid>https://arxiv.org/abs/2510.04850</guid>
<content:encoded><![CDATA[
arXiv:2510.04850v1 Announce Type: new 
Abstract: Reasoning distillation has emerged as an efficient and powerful paradigm for enhancing the reasoning capabilities of large language models. However, reasoning distillation may inadvertently cause benchmark contamination, where evaluation data included in distillation datasets can inflate performance metrics of distilled models. In this work, we formally define the task of distillation data detection, which is uniquely challenging due to the partial availability of distillation data. Then, we propose a novel and effective method Token Probability Deviation (TBD), which leverages the probability patterns of the generated output tokens. Our method is motivated by the analysis that distilled models tend to generate near-deterministic tokens for seen questions, while producing more low-probability tokens for unseen questions. Our key idea behind TBD is to quantify how far the generated tokens' probabilities deviate from a high reference probability. In effect, our method achieves competitive detection performance by producing lower scores for seen questions than for unseen questions. Extensive experiments demonstrate the effectiveness of our method, achieving an AUC of 0.918 and a TPR@1% FPR of 0.470 on the S1 dataset.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful Requests</title>
<link>https://arxiv.org/abs/2510.04891</link>
<guid>https://arxiv.org/abs/2510.04891</guid>
<content:encoded><![CDATA[
arXiv:2510.04891v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed in contexts where their failures can have direct sociopolitical consequences. Yet, existing safety benchmarks rarely test vulnerabilities in domains such as political manipulation, propaganda and disinformation generation, or surveillance and information control. We introduce SocialHarmBench, a dataset of 585 prompts spanning 7 sociopolitical categories and 34 countries, designed to surface where LLMs most acutely fail in politically charged contexts. Our evaluations reveal several shortcomings: open-weight models exhibit high vulnerability to harmful compliance, with Mistral-7B reaching attack success rates as high as 97% to 98% in domains such as historical revisionism, propaganda, and political manipulation. Moreover, temporal and geographic analyses show that LLMs are most fragile when confronted with 21st-century or pre-20th-century contexts, and when responding to prompts tied to regions such as Latin America, the USA, and the UK. These findings demonstrate that current safeguards fail to generalize to high-stakes sociopolitical settings, exposing systematic biases and raising concerns about the reliability of LLMs in preserving human rights and democratic values. We share the SocialHarmBench benchmark at https://huggingface.co/datasets/psyonp/SocialHarmBench.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do LLMs Align with My Task? Evaluating Text-to-SQL via Dataset Alignment</title>
<link>https://arxiv.org/abs/2510.04919</link>
<guid>https://arxiv.org/abs/2510.04919</guid>
<content:encoded><![CDATA[
arXiv:2510.04919v1 Announce Type: new 
Abstract: Supervised Fine-Tuning (SFT) is an effective method for adapting Large Language Models (LLMs) on downstream tasks. However, variability in training data can hinder a model's ability to generalize across domains. This paper studies the problem of dataset alignment for Natural Language to SQL (NL2SQL or text to SQL), examining how well SFT training data matches the structural characteristics of target queries and how this alignment impacts model performance. We hypothesize that alignment can be accurately estimated by comparing the distributions of structural SQL features across the training set, target data, and the model's predictions prior to SFT. Through comprehensive experiments on three large cross-domain NL2SQL benchmarks and multiple model families, we show that structural alignment is a strong predictor of fine-tuning success. When alignment is high, SFT yields substantial gains in accuracy and SQL generation quality; when alignment is low, improvements are marginal or absent. These findings highlight the importance of alignment-aware data selection for effective fine-tuning and generalization in NL2SQL tasks.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Geometry of Truth: Layer-wise Semantic Dynamics for Hallucination Detection in Large Language Models</title>
<link>https://arxiv.org/abs/2510.04933</link>
<guid>https://arxiv.org/abs/2510.04933</guid>
<content:encoded><![CDATA[
arXiv:2510.04933v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often produce fluent yet factually incorrect statements-a phenomenon known as hallucination-posing serious risks in high-stakes domains. We present Layer-wise Semantic Dynamics (LSD), a geometric framework for hallucination detection that analyzes the evolution of hidden-state semantics across transformer layers. Unlike prior methods that rely on multiple sampling passes or external verification sources, LSD operates intrinsically within the model's representational space. Using margin-based contrastive learning, LSD aligns hidden activations with ground-truth embeddings derived from a factual encoder, revealing a distinct separation in semantic trajectories: factual responses preserve stable alignment, while hallucinations exhibit pronounced semantic drift across depth. Evaluated on the TruthfulQA and synthetic factual-hallucination datasets, LSD achieves an F1-score of 0.92, AUROC of 0.96, and clustering accuracy of 0.89, outperforming SelfCheckGPT and Semantic Entropy baselines while requiring only a single forward pass. This efficiency yields a 5-20x speedup over sampling-based methods without sacrificing precision or interpretability. LSD offers a scalable, model-agnostic mechanism for real-time hallucination monitoring and provides new insights into the geometry of factual consistency within large language models.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A First Context-Free Grammar Applied to Nawatl Corpora Augmentation</title>
<link>https://arxiv.org/abs/2510.04945</link>
<guid>https://arxiv.org/abs/2510.04945</guid>
<content:encoded><![CDATA[
arXiv:2510.04945v1 Announce Type: new 
Abstract: In this article we introduce a context-free grammar (CFG) for the Nawatl language. Nawatl (or Nahuatl) is an Amerindian language of the $\pi$-language type, i.e. a language with few digital resources, in which the corpora available for machine learning are virtually non-existent. The objective here is to generate a significant number of grammatically correct artificial sentences, in order to increase the corpora available for language model training. We want to show that a grammar enables us significantly to expand a corpus in Nawatl which we call $\pi$-\textsc{yalli}. The corpus, thus enriched, enables us to train algorithms such as FastText and to evaluate them on sentence-level semantic tasks. Preliminary results show that by using the grammar, comparative improvements are achieved over some LLMs. However, it is observed that to achieve more significant improvement, grammars that model the Nawatl language even more effectively are required.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind Your Tone: Investigating How Prompt Politeness Affects LLM Accuracy (short paper)</title>
<link>https://arxiv.org/abs/2510.04950</link>
<guid>https://arxiv.org/abs/2510.04950</guid>
<content:encoded><![CDATA[
arXiv:2510.04950v1 Announce Type: new 
Abstract: The wording of natural language prompts has been shown to influence the performance of large language models (LLMs), yet the role of politeness and tone remains underexplored. In this study, we investigate how varying levels of prompt politeness affect model accuracy on multiple-choice questions. We created a dataset of 50 base questions spanning mathematics, science, and history, each rewritten into five tone variants: Very Polite, Polite, Neutral, Rude, and Very Rude, yielding 250 unique prompts. Using ChatGPT 4o, we evaluated responses across these conditions and applied paired sample t-tests to assess statistical significance. Contrary to expectations, impolite prompts consistently outperformed polite ones, with accuracy ranging from 80.8% for Very Polite prompts to 84.8% for Very Rude prompts. These findings differ from earlier studies that associated rudeness with poorer outcomes, suggesting that newer LLMs may respond differently to tonal variation. Our results highlight the importance of studying pragmatic aspects of prompting and raise broader questions about the social dimensions of human-AI interaction.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AWARE, Beyond Sentence Boundaries: A Contextual Transformer Framework for Identifying Cultural Capital in STEM Narratives</title>
<link>https://arxiv.org/abs/2510.04983</link>
<guid>https://arxiv.org/abs/2510.04983</guid>
<content:encoded><![CDATA[
arXiv:2510.04983v1 Announce Type: new 
Abstract: Identifying cultural capital (CC) themes in student reflections can offer valuable insights that help foster equitable learning environments in classrooms. However, themes such as aspirational goals or family support are often woven into narratives, rather than appearing as direct keywords. This makes them difficult to detect for standard NLP models that process sentences in isolation. The core challenge stems from a lack of awareness, as standard models are pre-trained on general corpora, leaving them blind to the domain-specific language and narrative context inherent to the data. To address this, we introduce AWARE, a framework that systematically attempts to improve a transformer model's awareness for this nuanced task. AWARE has three core components: 1) Domain Awareness, adapting the model's vocabulary to the linguistic style of student reflections; 2) Context Awareness, generating sentence embeddings that are aware of the full essay context; and 3) Class Overlap Awareness, employing a multi-label strategy to recognize the coexistence of themes in a single sentence. Our results show that by making the model explicitly aware of the properties of the input, AWARE outperforms a strong baseline by 2.1 percentage points in Macro-F1 and shows considerable improvements across all themes. This work provides a robust and generalizable methodology for any text classification task in which meaning depends on the context of the narrative.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resource-Efficient Fine-Tuning of LLaMA-3.2-3B for Medical Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2510.05003</link>
<guid>https://arxiv.org/abs/2510.05003</guid>
<content:encoded><![CDATA[
arXiv:2510.05003v1 Announce Type: new 
Abstract: Large Language Models (LLMs) such as GPT-4 and LLaMA have demonstrated remarkable reasoning abilities but require significant computational resources for fine-tuning. This paper presents a resource-efficient fine-tuning approach for LLaMA-3.2-3B to enhance medical chain-of-thought reasoning while operating under constrained GPU and memory settings. Using parameter-efficient tuning techniques such as LoRA and QLoRA, we adapt the base model on publicly available medical reasoning datasets. The model achieves improved reasoning coherence and factual accuracy while reducing memory usage by up to 60% compared to standard full fine-tuning. Experimental evaluation demonstrates that lightweight adaptations can retain strong reasoning capability in medical question-answering tasks. This work highlights practical strategies for deploying LLMs in low-resource research environments and provides insights into balancing efficiency and domain specialization for medical AI systems.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imperceptible Jailbreaking against Large Language Models</title>
<link>https://arxiv.org/abs/2510.05025</link>
<guid>https://arxiv.org/abs/2510.05025</guid>
<content:encoded><![CDATA[
arXiv:2510.05025v1 Announce Type: new 
Abstract: Jailbreaking attacks on the vision modality typically rely on imperceptible adversarial perturbations, whereas attacks on the textual modality are generally assumed to require visible modifications (e.g., non-semantic suffixes). In this paper, we introduce imperceptible jailbreaks that exploit a class of Unicode characters called variation selectors. By appending invisible variation selectors to malicious questions, the jailbreak prompts appear visually identical to original malicious questions on screen, while their tokenization is "secretly" altered. We propose a chain-of-search pipeline to generate such adversarial suffixes to induce harmful responses. Our experiments show that our imperceptible jailbreaks achieve high attack success rates against four aligned LLMs and generalize to prompt injection attacks, all without producing any visible modifications in the written prompt. Our code is available at https://github.com/sail-sg/imperceptible-jailbreaks.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Set of Quebec-French Corpus of Regional Expressions and Terms</title>
<link>https://arxiv.org/abs/2510.05026</link>
<guid>https://arxiv.org/abs/2510.05026</guid>
<content:encoded><![CDATA[
arXiv:2510.05026v1 Announce Type: new 
Abstract: The tasks of idiom understanding and dialect understanding are both well-established benchmarks in natural language processing. In this paper, we propose combining them, and using regional idioms as a test of dialect understanding. Towards this end, we propose two new benchmark datasets for the Quebec dialect of French: QFrCoRE, which contains 4,633 instances of idiomatic phrases, and QFrCoRT, which comprises 171 regional instances of idiomatic words. We explain how to construct these corpora, so that our methodology can be replicated for other dialects. Our experiments with 94 LLM demonstrate that our regional idiom benchmarks are a reliable tool for measuring a model's proficiency in a specific dialect.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guided Query Refinement: Multimodal Hybrid Retrieval with Test-Time Optimization</title>
<link>https://arxiv.org/abs/2510.05038</link>
<guid>https://arxiv.org/abs/2510.05038</guid>
<content:encoded><![CDATA[
arXiv:2510.05038v1 Announce Type: new 
Abstract: Multimodal encoders have pushed the boundaries of visual document retrieval, matching textual query tokens directly to image patches and achieving state-of-the-art performance on public benchmarks. Recent models relying on this paradigm have massively scaled the sizes of their query and document representations, presenting obstacles to deployment and scalability in real-world pipelines. Furthermore, purely vision-centric approaches may be constrained by the inherent modality gap still exhibited by modern vision-language models. In this work, we connect these challenges to the paradigm of hybrid retrieval, investigating whether a lightweight dense text retriever can enhance a stronger vision-centric model. Existing hybrid methods, which rely on coarse-grained fusion of ranks or scores, fail to exploit the rich interactions within each model's representation space. To address this, we introduce Guided Query Refinement (GQR), a novel test-time optimization method that refines a primary retriever's query embedding using guidance from a complementary retriever's scores. Through extensive experiments on visual document retrieval benchmarks, we demonstrate that GQR allows vision-centric models to match the performance of models with significantly larger representations, while being up to 14x faster and requiring 54x less memory. Our findings show that GQR effectively pushes the Pareto frontier for performance and efficiency in multimodal retrieval. We release our code at https://github.com/IBM/test-time-hybrid-retrieval
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COLE: a Comprehensive Benchmark for French Language Understanding Evaluation</title>
<link>https://arxiv.org/abs/2510.05046</link>
<guid>https://arxiv.org/abs/2510.05046</guid>
<content:encoded><![CDATA[
arXiv:2510.05046v1 Announce Type: new 
Abstract: To address the need for a more comprehensive evaluation of French Natural Language Understanding (NLU), we introduce COLE, a new benchmark composed of 23 diverse task covering a broad range of NLU capabilities, including sentiment analysis, paraphrase detection, grammatical judgment, and reasoning, with a particular focus on linguistic phenomena relevant to the French language. We benchmark 94 large language models (LLM), providing an extensive analysis of the current state of French NLU. Our results highlight a significant performance gap between closed- and open-weights models and identify key challenging frontiers for current LLMs, such as zero-shot extractive question-answering (QA), fine-grained word sense disambiguation, and understanding of regional language variations. We release COLE as a public resource to foster further progress in French language modelling.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs</title>
<link>https://arxiv.org/abs/2510.05069</link>
<guid>https://arxiv.org/abs/2510.05069</guid>
<content:encoded><![CDATA[
arXiv:2510.05069v1 Announce Type: new 
Abstract: Recent work shows that, beyond discrete reasoning through explicit chain-of-thought steps, which are limited by the boundaries of natural languages, large language models (LLMs) can also reason continuously in latent space, allowing richer information per step and thereby improving token efficiency. Despite this promise, latent reasoning still faces two challenges, especially in training-free settings: 1) purely latent reasoning broadens the search distribution by maintaining multiple implicit paths, which diffuses probability mass, introduces noise, and impedes convergence to a single high-confidence solution, thereby hurting accuracy; and 2) overthinking persists even without explicit text, wasting tokens and degrading efficiency. To address these issues, we introduce SwiReasoning, a training-free framework for LLM reasoning which features two key innovations: 1) SwiReasoning dynamically switches between explicit and latent reasoning, guided by block-wise confidence estimated from entropy trends in next-token distributions, to balance exploration and exploitation and promote timely convergence. 2) By limiting the maximum number of thinking-block switches, SwiReasoning curbs overthinking and improves token efficiency across varying problem difficulties. On widely used mathematics and STEM benchmarks, SwiReasoning consistently improves average accuracy by 1.5%-2.8% across reasoning LLMs of different model families and scales. Furthermore, under constrained budgets, SwiReasoning improves average token efficiency by 56%-79%, with larger gains as budgets tighten.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Slm-mux: Orchestrating small language models for reasoning</title>
<link>https://arxiv.org/abs/2510.05077</link>
<guid>https://arxiv.org/abs/2510.05077</guid>
<content:encoded><![CDATA[
arXiv:2510.05077v1 Announce Type: new 
Abstract: With the rapid development of language models, the number of small language models (SLMs) has grown significantly. Although they do not achieve state-of-the-art accuracy, they are more efficient and often excel at specific tasks. This raises a natural question: can multiple SLMs be orchestrated into a system where each contributes effectively, achieving higher accuracy than any individual model? Existing orchestration methods have primarily targeted frontier models (e.g., GPT-4) and perform suboptimally when applied to SLMs. To address this gap, we propose a three-stage approach for orchestrating SLMs. First, we introduce SLM-MUX, a multi-model architecture that effectively coordinates multiple SLMs. Building on this, we develop two optimization strategies: (i) a model selection search that identifies the most complementary SLMs from a given pool, and (ii) test-time scaling tailored to SLM-MUX. Our approach delivers strong results: Compared to existing orchestration methods, our approach achieves up to 13.4% improvement on MATH, 8.8% on GPQA, and 7.0% on GSM8K. With just two SLMS, SLM-MUX outperforms Qwen 2.5 72B on GPQA and GSM8K, and matches its performance on MATH. We further provide theoretical analyses to substantiate the advantages of our method. In summary, we demonstrate that SLMs can be effectively orchestrated into more accurate and efficient systems through the proposed approach.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TeachLM: Post-Training LLMs for Education Using Authentic Learning Data</title>
<link>https://arxiv.org/abs/2510.05087</link>
<guid>https://arxiv.org/abs/2510.05087</guid>
<content:encoded><![CDATA[
arXiv:2510.05087v1 Announce Type: new 
Abstract: The promise of generative AI to revolutionize education is constrained by the pedagogical limits of large language models (LLMs). A major issue is the lack of access to high-quality training data that reflect the learning of actual students. Prompt engineering has emerged as a stopgap, but the ability of prompts to encode complex pedagogical strategies in rule-based natural language is inherently limited. To address this gap we introduce TeachLM - an LLM optimized for teaching through parameter-efficient fine-tuning of state-of-the-art models. TeachLM is trained on a dataset comprised of 100,000 hours of one-on-one, longitudinal student-tutor interactions maintained by Polygence, which underwent a rigorous anonymization process to protect privacy. We use parameter-efficient fine-tuning to develop an authentic student model that enables the generation of high-fidelity synthetic student-tutor dialogues. Building on this capability, we propose a novel multi-turn evaluation protocol that leverages synthetic dialogue generation to provide fast, scalable, and reproducible assessments of the dialogical capabilities of LLMs. Our evaluations demonstrate that fine-tuning on authentic learning data significantly improves conversational and pedagogical performance - doubling student talk time, improving questioning style, increasing dialogue turns by 50%, and greater personalization of instruction.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finish First, Perfect Later: Test-Time Token-Level Cross-Validation for Diffusion Large Language Models</title>
<link>https://arxiv.org/abs/2510.05090</link>
<guid>https://arxiv.org/abs/2510.05090</guid>
<content:encoded><![CDATA[
arXiv:2510.05090v1 Announce Type: new 
Abstract: Diffusion large language models (dLLMs) have recently emerged as a promising alternative to autoregressive (AR) models, offering advantages such as accelerated parallel decoding and bidirectional context modeling. However, the vanilla decoding strategy in discrete dLLMs suffers from a critical limitation: once a token is accepted, it can no longer be revised in subsequent steps. As a result, early mistakes persist across iterations, harming both intermediate predictions and final output quality. To address this issue, we propose Tolerator (Token-Level Cross-Validation Refinement), a training-free decoding strategy that leverages cross-validation among predicted tokens. Unlike existing methods that follow a single progressive unmasking procedure, Tolerator introduces a two-stage process: (i) sequence fill-up and (ii) iterative refinement by remasking and decoding a subset of tokens while treating the remaining as context. This design enables previously accepted tokens to be reconsidered and corrected when necessary, leading to more reliable diffusion decoding outputs. We evaluate Tolerator on five standard benchmarks covering language understanding, code generation, and mathematics. Experiments show that our method achieves consistent improvements over the baselines under the same computational budget. These findings suggest that decoding algorithms are crucial to realizing the full potential of diffusion large language models. Code and data are publicly available.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAG: Chunked Augmented Generation for Google Chrome's Built-in Gemini Nano</title>
<link>https://arxiv.org/abs/2412.18708</link>
<guid>https://arxiv.org/abs/2412.18708</guid>
<content:encoded><![CDATA[
arXiv:2412.18708v1 Announce Type: cross 
Abstract: We present Chunked Augmented Generation (CAG), an architecture specifically designed to overcome the context window limitations of Google Chrome's built-in Gemini Nano model. While Chrome's integration of Gemini Nano represents a significant advancement in bringing AI capabilities directly to the browser, its restricted context window poses challenges for processing large inputs. CAG addresses this limitation through intelligent input chunking and processing strategies, enabling efficient handling of extensive content while maintaining the model's performance within browser constraints. Our implementation demonstrates particular efficacy in processing large documents and datasets directly within Chrome, making sophisticated AI capabilities accessible through the browser without external API dependencies. Get started now at https://github.com/vivekVells/cag-js.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>General Exploratory Bonus for Optimistic Exploration in RLHF</title>
<link>https://arxiv.org/abs/2510.03269</link>
<guid>https://arxiv.org/abs/2510.03269</guid>
<content:encoded><![CDATA[
arXiv:2510.03269v1 Announce Type: cross 
Abstract: Optimistic exploration is central to improving sample efficiency in reinforcement learning with human feedback, yet existing exploratory bonus methods to incentivize exploration often fail to realize optimism. We provide a theoretical analysis showing that current formulations, under KL or $\alpha$-divergence regularization, unintentionally bias exploration toward high-probability regions of the reference model, thereby reinforcing conservative behavior instead of promoting discovery of uncertain regions. To address this pitfall, we introduce the General Exploratory Bonus (GEB), a novel theoretical framework that provably satisfies the optimism principle. GEB counteracts divergence-induced bias via reference-dependent reward regulation and unifies prior heuristic bonuses as special cases, while extending naturally across the full $\alpha$-divergence family. Empirically, GEB consistently outperforms baselines on alignment tasks across multiple divergence settings and large language model backbones. These results demonstrate that GEB offers both a principled and practical solution for optimistic exploration in RLHF.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MemMamba: Rethinking Memory Patterns in State Space Model</title>
<link>https://arxiv.org/abs/2510.03279</link>
<guid>https://arxiv.org/abs/2510.03279</guid>
<content:encoded><![CDATA[
arXiv:2510.03279v1 Announce Type: cross 
Abstract: With the explosive growth of data, long-sequence modeling has become increasingly important in tasks such as natural language processing and bioinformatics. However, existing methods face inherent trade-offs between efficiency and memory. Recurrent neural networks suffer from gradient vanishing and explosion, making them hard to scale. Transformers can model global dependencies but are constrained by quadratic complexity. Recently, selective state-space models such as Mamba have demonstrated high efficiency with O(n) time and O(1) recurrent inference, yet their long-range memory decays exponentially. In this work, we conduct mathematical derivations and information-theoretic analysis to systematically uncover the memory decay mechanism of Mamba, answering a fundamental question: what is the nature of Mamba's long-range memory and how does it retain information? To quantify key information loss, we further introduce horizontal-vertical memory fidelity metrics that capture degradation both within and across layers. Inspired by how humans distill and retain salient information when reading long documents, we propose MemMamba, a novel architectural framework that integrates state summarization mechanism together with cross-layer and cross-token attention, which alleviates long-range forgetting while preserving linear complexity. MemMamba achieves significant improvements over existing Mamba variants and Transformers on long-sequence benchmarks such as PG19 and Passkey Retrieval, while delivering a 48% speedup in inference efficiency. Both theoretical analysis and empirical results demonstrate that MemMamba achieves a breakthrough in the complexity-memory trade-off, offering a new paradigm for ultra-long sequence modeling.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Optimal Large Diffusion Language Models</title>
<link>https://arxiv.org/abs/2510.03280</link>
<guid>https://arxiv.org/abs/2510.03280</guid>
<content:encoded><![CDATA[
arXiv:2510.03280v1 Announce Type: cross 
Abstract: We introduce Quokka, the first systematic scaling law for diffusion language models (DLMs), encompassing both compute-constrained and data-constrained regimes, and studying the key modeling and optimization designs. Quokka is a good friend of Chinchilla and provides wider scopes. We hope the results would bring short-term practical guidance in DLMs training and long-term inspirations for the whole AI community.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Transformer Circuits via a Hybrid Attribution and Pruning Framework</title>
<link>https://arxiv.org/abs/2510.03282</link>
<guid>https://arxiv.org/abs/2510.03282</guid>
<content:encoded><![CDATA[
arXiv:2510.03282v1 Announce Type: cross 
Abstract: Interpreting language models often involves circuit analysis, which aims to identify sparse subnetworks, or circuits, that accomplish specific tasks. Existing circuit discovery algorithms face a fundamental trade-off: attribution patching is fast but unfaithful to the full model, while edge pruning is faithful but computationally expensive. This research proposes a hybrid attribution and pruning (HAP) framework that uses attribution patching to identify a high-potential subgraph, then applies edge pruning to extract a faithful circuit from it. We show that HAP is 46\% faster than baseline algorithms without sacrificing circuit faithfulness. Furthermore, we present a case study on the Indirect Object Identification task, showing that our method preserves cooperative circuit components (e.g. S-inhibition heads) that attribution patching methods prune at high sparsity. Our results show that HAP could be an effective approach for improving the scalability of mechanistic interpretability research to larger models. Our code is available at https://anonymous.4open.science/r/HAP-circuit-discovery.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MACE: A Hybrid LLM Serving System with Colocated SLO-aware Continuous Retraining Alignment</title>
<link>https://arxiv.org/abs/2510.03283</link>
<guid>https://arxiv.org/abs/2510.03283</guid>
<content:encoded><![CDATA[
arXiv:2510.03283v1 Announce Type: cross 
Abstract: Large language models (LLMs) deployed on edge servers are increasingly used in latency-sensitive applications such as personalized assistants, recommendation, and content moderation. However, the non-stationary nature of user data necessitates frequent retraining, which introduces a fundamental tension between inference latency and model accuracy under constrained GPU resources. Existing retraining strategies either delay model updates, over-commit resources to retraining, or overlook iteration-level retraining granularity. In this paper, we identify that iteration-level scheduling is crucial for adapting retraining frequency to model drift without violating service-level objectives (SLOs). We propose MACE, a hybrid LLM system that colocates concurrent inference (prefill, decode) and fine-tuning, with intelligent memory management to maximize task performance while promising inference throughput. MACE leverages the insight that not all model updates equally affect output alignment and allocates GPU cycles accordingly to balance throughput, latency, and update freshness. Our trace-driven evaluation shows that MACE matches or exceeds continuous retraining while reducing inference latency by up to 63% and maintaining throughput under resource constraints. Compared to periodic retraining, MACE improves latency breakdown across prefill, decode, and finetune stages, and sustains GPU utilization above 85% in NVIDIA AGX Orin. These results demonstrate that iteration-level hybrid scheduling is a promising direction for deploying LLMs with continual learning capabilities on edge platforms.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why mask diffusion does not work</title>
<link>https://arxiv.org/abs/2510.03289</link>
<guid>https://arxiv.org/abs/2510.03289</guid>
<content:encoded><![CDATA[
arXiv:2510.03289v1 Announce Type: cross 
Abstract: The main advantages of diffusion language models over autoregressive (AR) models lie in their ability to support parallel generation and bidirectional attention, enabling a more controllable generation process. In recent years, open-source mask diffusion language models have emerged, most of which are based on a variant known as absorbing diffusion. However, this paper demonstrates why mask diffusion faces inherent difficulties in achieving parallel generation and bidirectional attention. We also propose the most effective training and inference strategies for mask diffusion.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Arabic Captioning with Interpretable Visual Concept Integration</title>
<link>https://arxiv.org/abs/2510.03295</link>
<guid>https://arxiv.org/abs/2510.03295</guid>
<content:encoded><![CDATA[
arXiv:2510.03295v1 Announce Type: cross 
Abstract: We present VLCAP, an Arabic image captioning framework that integrates CLIP-based visual label retrieval with multimodal text generation. Rather than relying solely on end-to-end captioning, VLCAP grounds generation in interpretable Arabic visual concepts extracted with three multilingual encoders, mCLIP, AraCLIP, and Jina V4, each evaluated separately for label retrieval. A hybrid vocabulary is built from training captions and enriched with about 21K general domain labels translated from the Visual Genome dataset, covering objects, attributes, and scenes. The top-k retrieved labels are transformed into fluent Arabic prompts and passed along with the original image to vision-language models. In the second stage, we tested Qwen-VL and Gemini Pro Vision for caption generation, resulting in six encoder-decoder configurations. The results show that mCLIP + Gemini Pro Vision achieved the best BLEU-1 (5.34%) and cosine similarity (60.01%), while AraCLIP + Qwen-VL obtained the highest LLM-judge score (36.33%). This interpretable pipeline enables culturally coherent and contextually accurate Arabic captions.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAFL-L: Constraint-Aware Federated Learning with Lagrangian Dual Optimization for On-Device Language Models</title>
<link>https://arxiv.org/abs/2510.03298</link>
<guid>https://arxiv.org/abs/2510.03298</guid>
<content:encoded><![CDATA[
arXiv:2510.03298v1 Announce Type: cross 
Abstract: We introduce Constraint-Aware Federated Learning with Lagrangian Dual Optimization (CAFL-L), a principled extension of FedAvg that explicitly incorporates device-level resource constraints including energy, communication, memory, and thermal budgets. CAFL-L employs Lagrangian dual optimization to dynamically adapt training hyperparameters -- freezing depth, local steps, batch size, and communication compression -- while preserving training stability through token-budget preservation via gradient accumulation. Experiments on a character-level language model demonstrate that CAFL-L achieves superior constraint satisfaction compared to standard FedAvg (reducing memory usage by 20% and communication by 95%) while maintaining competitive validation performance, making it practical for deployment on resource-constrained edge devices.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentCaster: Reasoning-Guided Tornado Forecasting</title>
<link>https://arxiv.org/abs/2510.03349</link>
<guid>https://arxiv.org/abs/2510.03349</guid>
<content:encoded><![CDATA[
arXiv:2510.03349v1 Announce Type: cross 
Abstract: There is a growing need to evaluate Large Language Models (LLMs) on complex, high-impact, real-world tasks to assess their true readiness as reasoning agents. To address this gap, we introduce AgentCaster, a contamination-free framework employing multimodal LLMs end-to-end for the challenging, long-horizon task of tornado forecasting. Within AgentCaster, models interpret heterogeneous spatiotemporal data from a high-resolution convection-allowing forecast archive. We assess model performance over a 40-day period featuring diverse historical data, spanning several major tornado outbreaks and including over 500 tornado reports. Each day, models query interactively from a pool of 3,625 forecast maps and 40,125 forecast soundings for a forecast horizon of 12-36 hours. Probabilistic tornado-risk polygon predictions are verified against ground truths derived from geometric comparisons across disjoint risk bands in projected coordinate space. To quantify accuracy, we propose domain-specific TornadoBench and TornadoHallucination metrics, with TornadoBench highly challenging for both LLMs and domain expert human forecasters. Notably, human experts significantly outperform state-of-the-art models, which demonstrate a strong tendency to hallucinate and overpredict risk intensity, struggle with precise geographic placement, and exhibit poor spatiotemporal reasoning in complex, dynamically evolving systems. AgentCaster aims to advance research on improving LLM agents for challenging reasoning tasks in critical domains.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight Prompt Engineering for Cognitive Alignment in Educational AI: A OneClickQuiz Case Study</title>
<link>https://arxiv.org/abs/2510.03374</link>
<guid>https://arxiv.org/abs/2510.03374</guid>
<content:encoded><![CDATA[
arXiv:2510.03374v1 Announce Type: cross 
Abstract: The rapid integration of Artificial Intelligence (AI) into educational technology promises to revolutionize content creation and assessment. However, the quality and pedagogical alignment of AI-generated content remain critical challenges. This paper investigates the impact of lightweight prompt engineering strategies on the cognitive alignment of AI-generated questions within OneClickQuiz, a Moodle plugin leveraging generative AI. We evaluate three prompt variants-a detailed baseline, a simpler version, and a persona-based approach-across Knowledge, Application, and Analysis levels of Bloom's Taxonomy. Utilizing an automated classification model (from prior work) and human review, our findings demonstrate that explicit, detailed prompts are crucial for precise cognitive alignment. While simpler and persona-based prompts yield clear and relevant questions, they frequently misalign with intended Bloom's levels, generating outputs that are either too complex or deviate from the desired cognitive objective. This study underscores the importance of strategic prompt engineering in fostering pedagogically sound AI-driven educational solutions and advises on optimizing AI for quality content generation in learning analytics and smart learning environments.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Studying the Korean Word-Chain Game with RLVR:Mitigating Reward Conflicts via Curriculum Learning</title>
<link>https://arxiv.org/abs/2510.03394</link>
<guid>https://arxiv.org/abs/2510.03394</guid>
<content:encoded><![CDATA[
arXiv:2510.03394v1 Announce Type: cross 
Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach for training large language models (LLMs) with stronger reasoning abilities. It has also been applied to a variety of logic puzzles. In this work, we study the Korean word-chain game using RLVR. We show that rule-derived rewards can naturally conflict, and demonstrate through experiments that a curriculum-learning scheme mitigates these conflicts. Our findings motivate further studies of puzzle tasks in diverse languages.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Know Thyself? On the Incapability and Implications of AI Self-Recognition</title>
<link>https://arxiv.org/abs/2510.03399</link>
<guid>https://arxiv.org/abs/2510.03399</guid>
<content:encoded><![CDATA[
arXiv:2510.03399v1 Announce Type: cross 
Abstract: Self-recognition is a crucial metacognitive capability for AI systems, relevant not only for psychological analysis but also for safety, particularly in evaluative scenarios. Motivated by contradictory interpretations of whether models possess self-recognition (Panickssery et al., 2024; Davidson et al., 2024), we introduce a systematic evaluation framework that can be easily applied and updated. Specifically, we measure how well 10 contemporary larger language models (LLMs) can identify their own generated text versus text from other models through two tasks: binary self-recognition and exact model prediction. Different from prior claims, our results reveal a consistent failure in self-recognition. Only 4 out of 10 models predict themselves as generators, and the performance is rarely above random chance. Additionally, models exhibit a strong bias toward predicting GPT and Claude families. We also provide the first evaluation of model awareness of their own and others' existence, as well as the reasoning behind their choices in self-recognition. We find that the model demonstrates some knowledge of its own existence and other models, but their reasoning reveals a hierarchical bias. They appear to assume that GPT, Claude, and occasionally Gemini are the top-tier models, often associating high-quality text with them. We conclude by discussing the implications of our findings on AI safety and future directions to develop appropriate AI self-awareness.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLSEMANTICSBENCH: Large Language Models As Programming Language Interpreters</title>
<link>https://arxiv.org/abs/2510.03415</link>
<guid>https://arxiv.org/abs/2510.03415</guid>
<content:encoded><![CDATA[
arXiv:2510.03415v1 Announce Type: cross 
Abstract: As large language models (LLMs) excel at code reasoning, a natural question arises: can an LLM execute programs (i.e., act as an interpreter) purely based on a programming language's formal semantics? If so, it will enable rapid prototyping of new programming languages and language features. We study this question using the imperative language IMP (a subset of C), formalized via small-step operational semantics (SOS) and rewriting-based operational semantics (K-semantics). We introduce three evaluation sets-Human-Written, LLM-Translated, and Fuzzer- Generated-whose difficulty is controlled by code-complexity metrics spanning the size, control-flow, and data-flow axes. Given a program and its semantics formalized with SOS/K-semantics, models are evaluated on three tasks ranging from coarse to fine: (1) final-state prediction, (2) semantic rule prediction, and (3) execution trace prediction. To distinguish pretraining memorization from semantic competence, we define two nonstandard semantics obtained through systematic mutations of the standard rules. Across strong code/reasoning LLMs, performance drops under nonstandard semantics despite high performance under the standard one. We further find that (i) there are patterns to different model failures, (ii) most reasoning models perform exceptionally well on coarse grained tasks involving reasoning about highly complex programs often containing nested loop depths beyond five, and surprisingly, (iii) providing formal semantics helps on simple programs but often hurts on more complex ones. Overall, the results show a promise that LLMs could serve as programming language interpreters, but points to the lack of their robust semantics understanding. We release the benchmark and the supporting code at https://github.com/EngineeringSoftware/PLSemanticsBench.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consistent Kernel Change-Point Detection under m-Dependence for Text Segmentation</title>
<link>https://arxiv.org/abs/2510.03437</link>
<guid>https://arxiv.org/abs/2510.03437</guid>
<content:encoded><![CDATA[
arXiv:2510.03437v1 Announce Type: cross 
Abstract: Kernel change-point detection (KCPD) has become a widely used tool for identifying structural changes in complex data. While existing theory establishes consistency under independence assumptions, real-world sequential data such as text exhibits strong dependencies. We establish new guarantees for KCPD under $m$-dependent data: specifically, we prove consistency in the number of detected change points and weak consistency in their locations under mild additional assumptions. We perform an LLM-based simulation that generates synthetic $m$-dependent text to validate the asymptotics. To complement these results, we present the first comprehensive empirical study of KCPD for text segmentation with modern embeddings. Across diverse text datasets, KCPD with text embeddings outperforms baselines in standard text segmentation metrics. We demonstrate through a case study on Taylor Swift's tweets that KCPD not only provides strong theoretical and simulated reliability but also practical effectiveness for text segmentation tasks.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Red Lines and Grey Zones in the Fog of War: Benchmarking Legal Risk, Moral Harm, and Regional Bias in Large Language Model Military Decision-Making</title>
<link>https://arxiv.org/abs/2510.03514</link>
<guid>https://arxiv.org/abs/2510.03514</guid>
<content:encoded><![CDATA[
arXiv:2510.03514v1 Announce Type: cross 
Abstract: As military organisations consider integrating large language models (LLMs) into command and control (C2) systems for planning and decision support, understanding their behavioural tendencies is critical. This study develops a benchmarking framework for evaluating aspects of legal and moral risk in targeting behaviour by comparing LLMs acting as agents in multi-turn simulated conflict. We introduce four metrics grounded in International Humanitarian Law (IHL) and military doctrine: Civilian Target Rate (CTR) and Dual-use Target Rate (DTR) assess compliance with legal targeting principles, while Mean and Max Simulated Non-combatant Casualty Value (SNCV) quantify tolerance for civilian harm.
  We evaluate three frontier models, GPT-4o, Gemini-2.5, and LLaMA-3.1, through 90 multi-agent, multi-turn crisis simulations across three geographic regions. Our findings reveal that off-the-shelf LLMs exhibit concerning and unpredictable targeting behaviour in simulated conflict environments. All models violated the IHL principle of distinction by targeting civilian objects, with breach rates ranging from 16.7% to 66.7%. Harm tolerance escalated through crisis simulations with MeanSNCV increasing from 16.5 in early turns to 27.7 in late turns. Significant inter-model variation emerged: LLaMA-3.1 selected an average of 3.47 civilian strikes per simulation with MeanSNCV of 28.4, while Gemini-2.5 selected 0.90 civilian strikes with MeanSNCV of 17.6. These differences indicate that model selection for deployment constitutes a choice about acceptable legal and moral risk profiles in military operations.
  This work seeks to provide a proof-of-concept of potential behavioural risks that could emerge from the use of LLMs in Decision Support Systems (AI DSS) as well as a reproducible benchmarking framework with interpretable metrics for standardising pre-deployment testing.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Unlearning Meets Adversarial Robustness via Constrained Interventions on LLMs</title>
<link>https://arxiv.org/abs/2510.03567</link>
<guid>https://arxiv.org/abs/2510.03567</guid>
<content:encoded><![CDATA[
arXiv:2510.03567v1 Announce Type: cross 
Abstract: With the increasing adoption of Large Language Models (LLMs), more customization is needed to ensure privacy-preserving and safe generation. We address this objective from two critical aspects: unlearning of sensitive information and robustness to jail-breaking attacks. We investigate various constrained optimization formulations that address both aspects in a \emph{unified manner}, by finding the smallest possible interventions on LLM weights that either make a given vocabulary set unreachable or embed the LLM with robustness to tailored attacks by shifting part of the weights to a \emph{safer} region. Beyond unifying two key properties, this approach contrasts with previous work in that it doesn't require an oracle classifier that is typically not available or represents a computational overhead. Surprisingly, we find that the simplest point-wise constraint-based intervention we propose leads to better performance than max-min interventions, while having a lower computational cost. Comparison against state-of-the-art defense methods demonstrates superior performance of the proposed approach.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Theory to Practice: Evaluating Data Poisoning Attacks and Defenses in In-Context Learning on Social Media Health Discourse</title>
<link>https://arxiv.org/abs/2510.03636</link>
<guid>https://arxiv.org/abs/2510.03636</guid>
<content:encoded><![CDATA[
arXiv:2510.03636v1 Announce Type: cross 
Abstract: This study explored how in-context learning (ICL) in large language models can be disrupted by data poisoning attacks in the setting of public health sentiment analysis. Using tweets of Human Metapneumovirus (HMPV), small adversarial perturbations such as synonym replacement, negation insertion, and randomized perturbation were introduced into the support examples. Even these minor manipulations caused major disruptions, with sentiment labels flipping in up to 67% of cases. To address this, a Spectral Signature Defense was applied, which filtered out poisoned examples while keeping the data's meaning and sentiment intact. After defense, ICL accuracy remained steady at around 46.7%, and logistic regression validation reached 100% accuracy, showing that the defense successfully preserved the dataset's integrity. Overall, the findings extend prior theoretical studies of ICL poisoning to a practical, high-stakes setting in public health discourse analysis, highlighting both the risks and potential defenses for robust LLM deployment. This study also highlights the fragility of ICL under attack and the value of spectral defenses in making AI systems more reliable for health-related social media monitoring.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does higher interpretability imply better utility? A Pairwise Analysis on Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2510.03659</link>
<guid>https://arxiv.org/abs/2510.03659</guid>
<content:encoded><![CDATA[
arXiv:2510.03659v1 Announce Type: cross 
Abstract: Sparse Autoencoders (SAEs) are widely used to steer large language models (LLMs), based on the assumption that their interpretable features naturally enable effective model behavior steering. Yet, a fundamental question remains unanswered: does higher interpretability indeed imply better steering utility? To answer this question, we train 90 SAEs across three LLMs (Gemma-2-2B, Qwen-2.5-3B, Gemma-2-9B), spanning five architectures and six sparsity levels, and evaluate their interpretability and steering utility based on SAEBench (arXiv:2501.12345) and AxBench (arXiv:2502.23456) respectively, and perform a rank-agreement analysis via Kendall's rank coefficients (tau b). Our analysis reveals only a relatively weak positive association (tau b approx 0.298), indicating that interpretability is an insufficient proxy for steering performance. We conjecture the interpretability utility gap may stem from the selection of SAE features, as not all of them are equally effective for steering. To further find features that truly steer the behavior of LLMs, we propose a novel selection criterion called Delta Token Confidence, which measures how much amplifying a feature changes the next token distribution. We show that our method improves the steering performance of three LLMs by 52.52 percent compared to the current best output score based criterion (arXiv:2503.34567). Strikingly, after selecting features with high Delta Token Confidence, the correlation between interpretability and utility vanishes (tau b approx 0), and can even become negative. This further highlights the divergence between interpretability and utility for the most effective steering features.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Token Hidden Reward: Steering Exploration-Exploitation in Group Relative Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.03669</link>
<guid>https://arxiv.org/abs/2510.03669</guid>
<content:encoded><![CDATA[
arXiv:2510.03669v1 Announce Type: cross 
Abstract: Reinforcement learning with verifiable rewards has significantly advanced the reasoning capabilities of large language models, yet how to explicitly steer training toward exploration or exploitation remains an open problem. We introduce Token Hidden Reward (THR), a token-level metric that quantifies each token's influence on the likelihood of correct responses under Group Relative Policy Optimization (GRPO). We find that training dynamics are dominated by a small subset of tokens with high absolute THR values. Most interestingly, tokens with positive THR strengthen confidence in correct outputs, thus favoring exploitation, while tokens with negative THR preserve probability mass for alternative outputs, enabling exploration. This insight suggests a natural intervention: a THR-guided reweighting algorithm that modulates GRPO's learning signals to explicitly bias training toward exploitation or exploration. We validate the efficacy of this algorithm on diverse math reasoning benchmarks. By amplifying tokens with positive THR value and weakening negative ones, our algorithm improves greedy-decoding accuracy, favoring exploitation. The reverse strategy yields consistent gains in Pass@K accuracy, favoring exploration. We further demonstrate that our algorithm integrates seamlessly with other RL objectives such as GSPO and generalizes across architectures including Llama. These findings establish THR as a principled and fine-grained mechanism for dynamically controlling exploration and exploitation in RL-tuned LLMs, providing new tools for targeted fine-tuning in reasoning-intensive applications.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the Goal: Data-Efficient Goal-Oriented Evaluation of Conversational Agents and Chatbots using Teacher Models</title>
<link>https://arxiv.org/abs/2510.03696</link>
<guid>https://arxiv.org/abs/2510.03696</guid>
<content:encoded><![CDATA[
arXiv:2510.03696v1 Announce Type: cross 
Abstract: Evaluating the quality of multi-turn chatbot interactions remains challenging, as most existing methods assess interactions at the turn level without addressing whether a user's overarching goal was fulfilled. A ``goal'' here refers to an information need or task, such as asking for policy information or applying for leave. We propose a comprehensive framework for goal-oriented evaluation of multi-agent systems (MAS), introducing the \textbf{Goal Success Rate (GSR)} to measure the percentage of fulfilled goals, and a \textbf{Root Cause of Failure (RCOF)} taxonomy to identify reasons for failure in multi-agent chatbots. Our method segments conversations by user goals and evaluates success using all relevant turns. We present a model-based evaluation system combining teacher LLMs, where domain experts define goals, set quality standards serving as a guidance for the LLMs. The LLMs use ``thinking tokens'' to produce interpretable rationales, enabling \textit{explainable}, \textit{data-efficient} evaluations. In an enterprise setting, we apply our framework to evaluate AIDA, a zero-to-one employee conversational agent system built as a ground-up multi-agent conversational agent, and observe GSR improvement from 63\% to 79\% over six months since its inception. Our framework is generic and offers actionable insights through a detailed defect taxonomy based on analysis of failure points in multi-agent chatbots, diagnosing overall success, identifying key failure modes, and informing system improvements.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Person-Centric Annotations of LAION-400M: Auditing Bias and Its Transfer to Models</title>
<link>https://arxiv.org/abs/2510.03721</link>
<guid>https://arxiv.org/abs/2510.03721</guid>
<content:encoded><![CDATA[
arXiv:2510.03721v1 Announce Type: cross 
Abstract: Vision-language models trained on large-scale multimodal datasets show strong demographic biases, but the role of training data in producing these biases remains unclear. A major barrier has been the lack of demographic annotations in web-scale datasets such as LAION-400M. We address this gap by creating person-centric annotations for the full dataset, including over 276 million bounding boxes, perceived gender and race/ethnicity labels, and automatically generated captions. These annotations are produced through validated automatic labeling pipelines combining object detection, multimodal captioning, and finetuned classifiers. Using them, we uncover demographic imbalances and harmful associations, such as the disproportionate linking of men and individuals perceived as Black or Middle Eastern with crime-related and negative content. We also show that 60-70% of gender bias in CLIP and Stable Diffusion can be linearly explained by direct co-occurrences in the data. Our resources establish the first large-scale empirical link between dataset composition and downstream model bias.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting Diarization-Conditioned Whisper for End-to-End Multi-Talker Speech Recognition</title>
<link>https://arxiv.org/abs/2510.03723</link>
<guid>https://arxiv.org/abs/2510.03723</guid>
<content:encoded><![CDATA[
arXiv:2510.03723v1 Announce Type: cross 
Abstract: We propose a speaker-attributed (SA) Whisper-based model for multi-talker speech recognition that combines target-speaker modeling with serialized output training (SOT). Our approach leverages a Diarization-Conditioned Whisper (DiCoW) encoder to extract target-speaker embeddings, which are concatenated into a single representation and passed to a shared decoder. This enables the model to transcribe overlapping speech as a serialized output stream with speaker tags and timestamps. In contrast to target-speaker ASR systems such as DiCoW, which decode each speaker separately, our approach performs joint decoding, allowing the decoder to condition on the context of all speakers simultaneously. Experiments show that the model outperforms existing SOT-based approaches and surpasses DiCoW on multi-talker mixtures (e.g., LibriMix).
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Gap Between Multimodal Foundation Models and World Models</title>
<link>https://arxiv.org/abs/2510.03727</link>
<guid>https://arxiv.org/abs/2510.03727</guid>
<content:encoded><![CDATA[
arXiv:2510.03727v1 Announce Type: cross 
Abstract: Humans understand the world through the integration of multiple sensory modalities, enabling them to perceive, reason about, and imagine dynamic physical processes. Inspired by this capability, multimodal foundation models (MFMs) have emerged as powerful tools for multimodal understanding and generation. However, today's MFMs fall short of serving as effective world models. They lack the essential ability such as perform counterfactual reasoning, simulate dynamics, understand the spatiotemporal information, control generated visual outcomes, and perform multifaceted reasoning. We investigates what it takes to bridge the gap between multimodal foundation models and world models. We begin by improving the reasoning capabilities of MFMs through discriminative tasks and equipping MFMs with structured reasoning skills, such as causal inference, counterfactual thinking, and spatiotemporal reasoning, enabling them to go beyond surface correlations and understand deeper relationships within visual and textual data. Next, we explore generative capabilities of multimodal foundation models across both image and video modalities, introducing new frameworks for structured and controllable generation. Our approaches incorporate scene graphs, multimodal conditioning, and multimodal alignment strategies to guide the generation process, ensuring consistency with high-level semantics and fine-grained user intent. We further extend these techniques to controllable 4D generation, enabling interactive, editable, and morphable object synthesis over time and space.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Fine-Tuning through Advanced Initialization Strategies for Low-Rank Adaptation</title>
<link>https://arxiv.org/abs/2510.03731</link>
<guid>https://arxiv.org/abs/2510.03731</guid>
<content:encoded><![CDATA[
arXiv:2510.03731v1 Announce Type: cross 
Abstract: The rapid development of parameter-efficient fine-tuning methods has noticeably improved the efficiency of adapting large language models. Among these, LoRA has gained widespread popularity due to its strong balance of effectiveness and parameter efficiency. However, LoRA relies on initializing two low-rank matrices whose product is zero, which limits its ability to effectively activate and leverage the original model weights-creating a potential bottleneck for optimal performance. To address this limitation, we propose \textbf{IniLoRA}, a novel initialization strategy that initializes the low-rank matrices to closely approximate the original model weights. Experimental results indicate that IniLoRA achieves better performance than LoRA across a range of models and tasks. Additionally, we introduce two variants, IniLoRA-$\alpha$ and IniLoRA-$\beta$, both leveraging distinct initialization methods to enhance performance further.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating LLM Variability in Personalized Conversational Information Retrieval</title>
<link>https://arxiv.org/abs/2510.03795</link>
<guid>https://arxiv.org/abs/2510.03795</guid>
<content:encoded><![CDATA[
arXiv:2510.03795v1 Announce Type: cross 
Abstract: Personalized Conversational Information Retrieval (CIR) has seen rapid progress in recent years, driven by the development of Large Language Models (LLMs). Personalized CIR aims to enhance document retrieval by leveraging user-specific information, such as preferences, knowledge, or constraints, to tailor responses to individual needs. A key resource for this task is the TREC iKAT 2023 dataset, designed to evaluate personalization in CIR pipelines. Building on this resource, Mo et al. explored several strategies for incorporating Personal Textual Knowledge Bases (PTKB) into LLM-based query reformulation. Their findings suggested that personalization from PTKBs could be detrimental and that human annotations were often noisy. However, these conclusions were based on single-run experiments using the GPT-3.5 Turbo model, raising concerns about output variability and repeatability. In this reproducibility study, we rigorously reproduce and extend their work, focusing on LLM output variability and model generalization. We apply the original methods to the new TREC iKAT 2024 dataset and evaluate a diverse range of models, including Llama (1B-70B), Qwen-7B, GPT-4o-mini. Our results show that human-selected PTKBs consistently enhance retrieval performance, while LLM-based selection methods do not reliably outperform manual choices. We further compare variance across datasets and observe higher variability on iKAT than on CAsT, highlighting the challenges of evaluating personalized CIR. Notably, recall-oriented metrics exhibit lower variance than precision-oriented ones, a critical insight for first-stage retrievers. Finally, we underscore the need for multi-run evaluations and variance reporting when assessing LLM-based CIR systems. By broadening evaluation across models, datasets, and metrics, our study contributes to more robust and generalizable practices for personalized CIR.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlocking Reasoning Capabilities in LLMs via Reinforcement Learning Exploration</title>
<link>https://arxiv.org/abs/2510.03865</link>
<guid>https://arxiv.org/abs/2510.03865</guid>
<content:encoded><![CDATA[
arXiv:2510.03865v1 Announce Type: cross 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has recently enhanced the reasoning capabilities of large language models (LLMs), particularly for mathematical problem solving. However, a fundamental limitation remains: as the sampling budget increases, the advantage of RLVR-trained models over their pretrained bases often diminishes or even vanishes, revealing a strong dependence on the base model's restricted search space. We attribute this phenomenon to the widespread use of the reverse Kullback-Leibler (KL) divergence regularizer, whose mode-seeking behavior keeps the policy trapped inside the base model's support region and hampers wider exploration. To address this issue, we propose RAPO (Rewards-Aware Policy Optimization), an algorithm to promote broader yet focused exploration. Our method (i) utilizes the forward KL penalty to replace the reverse KL penalty for out-of-distribution exploration, and (ii) reweights the reference policy to facilitate adaptive in-distribution exploration. We train Qwen2.5-3B and 7B models with RAPO on the 8K SimpleRL-Zero dataset, without supervised fine-tuning, and evaluate them on AIME2024 and AIME2025. Results show that RAPO consistently improves problem-solving performance. Notably, RAPO enables models to surpass the base model's performance ceiling and solves previously intractable problems, advancing the frontier of RLVR for challenging reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kantian-Utilitarian XAI: Meta-Explained</title>
<link>https://arxiv.org/abs/2510.03892</link>
<guid>https://arxiv.org/abs/2510.03892</guid>
<content:encoded><![CDATA[
arXiv:2510.03892v1 Announce Type: cross 
Abstract: We present a gamified explainable AI (XAI) system for ethically aware consumer decision-making in the coffee domain. Each session comprises six rounds with three options per round. Two symbolic engines provide real-time reasons: a Kantian module flags rule violations (e.g., child labor, deforestation risk without shade certification, opaque supply chains, unsafe decaf), and a utilitarian module scores options via multi-criteria aggregation over normalized attributes (price, carbon, water, transparency, farmer income share, taste/freshness, packaging, convenience). A meta-explainer with a regret bound (0.2) highlights Kantian--utilitarian (mis)alignment and switches to a deontically clean, near-parity option when welfare loss is small. We release a structured configuration (attribute schema, certification map, weights, rule set), a policy trace for auditability, and an interactive UI.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Chemistry Estimation for Multi-LLM Recommendation</title>
<link>https://arxiv.org/abs/2510.03930</link>
<guid>https://arxiv.org/abs/2510.03930</guid>
<content:encoded><![CDATA[
arXiv:2510.03930v1 Announce Type: cross 
Abstract: Multi-LLM collaboration promises accurate, robust, and context-aware solutions, yet existing approaches rely on implicit selection and output assessment without analyzing whether collaborating models truly complement or conflict. We introduce LLM Chemistry -- a framework that measures when LLM combinations exhibit synergistic or antagonistic behaviors that shape collective performance beyond individual capabilities. We formalize the notion of chemistry among LLMs, propose algorithms that quantify it by analyzing interaction dependencies, and recommend optimal model ensembles accordingly. Our theoretical analysis shows that chemistry among collaborating LLMs is most evident under heterogeneous model profiles, with its outcome impact shaped by task type, group size, and complexity. Evaluation on classification, summarization, and program repair tasks provides initial evidence for these task-dependent effects, thereby reinforcing our theoretical results. This establishes LLM Chemistry as both a diagnostic factor in multi-LLM systems and a foundation for ensemble recommendation.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language Models</title>
<link>https://arxiv.org/abs/2510.03978</link>
<guid>https://arxiv.org/abs/2510.03978</guid>
<content:encoded><![CDATA[
arXiv:2510.03978v1 Announce Type: cross 
Abstract: Embedding vision-language models (VLMs) are typically pretrained with short text windows (<77 tokens), which forces the truncation of long-format captions. Yet, the distribution of biomedical captions from large-scale open source literature reveals that a huge portion of captions far exceed 77 tokens. To this end, we investigate the impact of pretraining on long-format biomedical captions by extending the context length of text encoders in VLMs. We find that longer context (thus, enabling additional supervision provided in long-format captions) correlates with better retrieval and classification performance. Given this finding, we introduce BIOMEDICA-LongCAP, a dataset of 1M image-caption pairs enriched with context-aware descriptions from full-text articles, providing longer and additional textual supervision. Using BIOMEDICA-LongCAP, we train BMC-LongCLIP, a long-context biomedical VLM with a text encoder supporting windows of up to 512 tokens. Our model extends context capacity by 6.6x, reducing token waste from 55% to just 2.2%. On long-caption retrieval benchmarks, BMC-LongCLIP achieves up to +30% absolute gains in Recall@1 and +2% average improvements in classification, while also converging faster than short-context. Our results demonstrate that long-context modeling is a promising direction for advancing biomedical VLMs.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing OCR for Sino-Vietnamese Language Processing via Fine-tuned PaddleOCRv5</title>
<link>https://arxiv.org/abs/2510.04003</link>
<guid>https://arxiv.org/abs/2510.04003</guid>
<content:encoded><![CDATA[
arXiv:2510.04003v1 Announce Type: cross 
Abstract: Recognizing and processing Classical Chinese (Han-Nom) texts play a vital role in digitizing Vietnamese historical documents and enabling cross-lingual semantic research. However, existing OCR systems struggle with degraded scans, non-standard glyphs, and handwriting variations common in ancient sources. In this work, we propose a fine-tuning approach for PaddleOCRv5 to improve character recognition on Han-Nom texts. We retrain the text recognition module using a curated subset of ancient Vietnamese Chinese manuscripts, supported by a full training pipeline covering preprocessing, LMDB conversion, evaluation, and visualization. Experimental results show a significant improvement over the base model, with exact accuracy increasing from 37.5 percent to 50.0 percent, particularly under noisy image conditions. Furthermore, we develop an interactive demo that visually compares pre- and post-fine-tuning recognition results, facilitating downstream applications such as Han-Vietnamese semantic alignment, machine translation, and historical linguistics research. The demo is available at https://huggingface.co/spaces/MinhDS/Fine-tuned-PaddleOCRv5.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Shapes a Creative Machine Mind? Comprehensively Benchmarking Creativity in Foundation Models</title>
<link>https://arxiv.org/abs/2510.04009</link>
<guid>https://arxiv.org/abs/2510.04009</guid>
<content:encoded><![CDATA[
arXiv:2510.04009v1 Announce Type: cross 
Abstract: The meteoric rise of foundation models (FMs) has expanded their capabilities far beyond conventional tasks. Creativity, long regarded as a hallmark of human intelligence and a driver of innovation, is now increasingly recognized as a critical dimension of machine intelligence in the era of generative FMs, complementing traditional measures of accuracy. However, existing evaluation frameworks for creativity remain fragmented, relying on ad hoc metrics not firmly grounded in established theories. To address this gap, we introduce C^2-Eval, a holistic benchmark for unified assessment of creativity in FMs. C^2-Eval distinguishes between two complementary forms of creativity: convergent creativity, where tasks admit constrained solutions (e.g., code generation), and divergent creativity, where tasks are open-ended (e.g., storytelling). It evaluates both dimensions using fine-grained criteria derived from social-science theory, focusing on Usefulness, Originality, and Surprise (U-O-S). Through extensive experiments on leading proprietary and open-source models, we analyze trade-offs in their creative capabilities. Our results highlight both the strengths and challenges of current FMs in pursuing a creative machine mind, showing that C^2-Eval is an effective lens for examining the evolving landscape of creative AI.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Lifelog Retrieval through Captioning-Enhanced Interpretation</title>
<link>https://arxiv.org/abs/2510.04010</link>
<guid>https://arxiv.org/abs/2510.04010</guid>
<content:encoded><![CDATA[
arXiv:2510.04010v1 Announce Type: cross 
Abstract: People often struggle to remember specific details of past experiences, which can lead to the need to revisit these memories. Consequently, lifelog retrieval has emerged as a crucial application. Various studies have explored methods to facilitate rapid access to personal lifelogs for memory recall assistance. In this paper, we propose a Captioning-Integrated Visual Lifelog (CIVIL) Retrieval System for extracting specific images from a user's visual lifelog based on textual queries. Unlike traditional embedding-based methods, our system first generates captions for visual lifelogs and then utilizes a text embedding model to project both the captions and user queries into a shared vector space. Visual lifelogs, captured through wearable cameras, provide a first-person viewpoint, necessitating the interpretation of the activities of the individual behind the camera rather than merely describing the scene. To address this, we introduce three distinct approaches: the single caption method, the collective caption method, and the merged caption method, each designed to interpret the life experiences of lifeloggers. Experimental results show that our method effectively describes first-person visual images, enhancing the outcomes of lifelog retrieval. Furthermore, we construct a textual dataset that converts visual lifelogs into captions, thereby reconstructing personal life experiences.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Principled and Tractable RL for Reasoning with Diffusion Language Models</title>
<link>https://arxiv.org/abs/2510.04019</link>
<guid>https://arxiv.org/abs/2510.04019</guid>
<content:encoded><![CDATA[
arXiv:2510.04019v1 Announce Type: cross 
Abstract: Diffusion large language models (dLLMs) are a new paradigm of non-autoregressive language models that are trained to predict multiple tokens in parallel and generate text via iterative unmasking. Recent works have successfully pretrained dLLMs to parity with autoregressive LLMs at the 8B scale, but dLLMs have yet to benefit from modern post-training techniques, e.g. reinforcement learning (RL), that have proven effective for autoregressive models. Crucially, algorithms designed for traditional LLMs aren't directly compatible with diffusion frameworks due to inherent differences in modeling assumptions. Moreover, existing attempts at dLLM post-training with RL rely on heuristic-based objectives with no theoretical grounding. In this work, we present Amortized Group Relative Policy Optimization (AGRPO), a principled on-policy RL algorithm designed specifically for dLLMs. AGRPO uses Monte Carlo sampling to compute an unbiased policy gradient estimate, making it the first tractable, faithful adaptation of policy gradient methods for dLLMs. We demonstrate AGRPO's effectiveness on different math/reasoning tasks, a common setting for RL with LLMs, achieving up to +7.6% absolute gain on GSM8K and 3.8x performance on the Countdown task over the baseline LLaDA-8B-Instruct model and 1.3x performance gains over comparable RL methods such as diffu-GRPO. Furthermore, these gains persist across different numbers of sampling steps at inference time, achieving better tradeoffs between compute and performance. Our results demonstrate that online RL algorithms can be extended to diffusion LLMs in principled ways, maintaining both theoretical soundness and practical effectiveness.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Based Data Science Agents: A Survey of Capabilities, Challenges, and Future Directions</title>
<link>https://arxiv.org/abs/2510.04023</link>
<guid>https://arxiv.org/abs/2510.04023</guid>
<content:encoded><![CDATA[
arXiv:2510.04023v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have enabled a new class of AI agents that automate multiple stages of the data science workflow by integrating planning, tool use, and multimodal reasoning across text, code, tables, and visuals. This survey presents the first comprehensive, lifecycle-aligned taxonomy of data science agents, systematically analyzing and mapping forty-five systems onto the six stages of the end-to-end data science process: business understanding and data acquisition, exploratory analysis and visualization, feature engineering, model building and selection, interpretation and explanation, and deployment and monitoring. In addition to lifecycle coverage, we annotate each agent along five cross-cutting design dimensions: reasoning and planning style, modality integration, tool orchestration depth, learning and alignment methods, and trust, safety, and governance mechanisms. Beyond classification, we provide a critical synthesis of agent capabilities, highlight strengths and limitations at each stage, and review emerging benchmarks and evaluation practices. Our analysis identifies three key trends: most systems emphasize exploratory analysis, visualization, and modeling while neglecting business understanding, deployment, and monitoring; multimodal reasoning and tool orchestration remain unresolved challenges; and over 90% lack explicit trust and safety mechanisms. We conclude by outlining open challenges in alignment stability, explainability, governance, and robust evaluation frameworks, and propose future research directions to guide the development of robust, trustworthy, low-latency, transparent, and broadly accessible data science agents.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Scales in Cross-Entropy Scaling Law?</title>
<link>https://arxiv.org/abs/2510.04067</link>
<guid>https://arxiv.org/abs/2510.04067</guid>
<content:encoded><![CDATA[
arXiv:2510.04067v1 Announce Type: cross 
Abstract: The cross-entropy scaling law has long served as a key tool for guiding the development of large language models. It shows that cross-entropy loss decreases in a predictable power-law rate as the model size increases. However, recent evidence indicates that this law breaks down at very large scales: the loss decreases more slowly than expected, which causes significant trouble for developing large language models. In this paper, we hypothesize that the root cause lies in the fact that cross-entropy itself does not truly scale; instead, only one of its hidden components does. To investigate this, we introduce a novel decomposition of cross-entropy into three parts: Error-Entropy, Self-Alignment, and Confidence. We show both theoretically and empirically that this decomposition precisely captures the training dynamics and optimization objectives. Through extensive experiments on multiple datasets and 32 models spanning five orders of magnitude in size, we find that only error-entropy follows a robust power-law scaling, while the other two terms remain largely invariant. Moreover, error-entropy constitutes the dominant share of cross-entropy in small models but diminishes in proportion as models grow larger. This explains why the cross-entropy scaling law appears accurate at small scales but fails at very large ones. Our findings establish the error-entropy scaling law as a more accurate description of model behavior. We believe it will have wide applications in the training, understanding, and future development of large language models.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Slow-Fast Policy Optimization: Reposition-Before-Update for LLM Reasoning</title>
<link>https://arxiv.org/abs/2510.04072</link>
<guid>https://arxiv.org/abs/2510.04072</guid>
<content:encoded><![CDATA[
arXiv:2510.04072v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has become central to enhancing reasoning in large language models (LLMs). Yet on-policy algorithms such as Group Relative Policy Optimization (GRPO) often suffer in early training: noisy gradients from low-quality rollouts lead to unstable updates and inefficient exploration. We introduce Slow-Fast Policy Optimization (SFPO), a simple yet efficient framework to address these limitations via decomposing each step into three stages: a short fast trajectory of inner steps on the same batch, a reposition mechanism to control off-policy drift, and a final slow correction. This reposition-before-update design preserves the objective and rollout process unchanged, making SFPO plug-compatible with existing policy-gradient pipelines. Extensive experiments demonstrate that SFPO consistently improves stability, reduces rollouts, and accelerates convergence of reasoning RL training. Specifically, it outperforms GRPO by up to 2.80 points in average on math reasoning benchmarks. It also achieves up to 4.93\texttimes{} fewer rollouts and a 4.19\texttimes{} reduction in wall-clock time to match GRPO's best accuracy.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Internal states before wait modulate reasoning patterns</title>
<link>https://arxiv.org/abs/2510.04128</link>
<guid>https://arxiv.org/abs/2510.04128</guid>
<content:encoded><![CDATA[
arXiv:2510.04128v1 Announce Type: cross 
Abstract: Prior work has shown that a significant driver of performance in reasoning models is their ability to reason and self-correct. A distinctive marker in these reasoning traces is the token wait, which often signals reasoning behavior such as backtracking. Despite being such a complex behavior, little is understood of exactly why models do or do not decide to reason in this particular manner, which limits our understanding of what makes a reasoning model so effective. In this work, we address the question whether model's latents preceding wait tokens contain relevant information for modulating the subsequent reasoning process. We train crosscoders at multiple layers of DeepSeek-R1-Distill-Llama-8B and its base version, and introduce a latent attribution technique in the crosscoder setting. We locate a small set of features relevant for promoting/suppressing wait tokens' probabilities. Finally, through a targeted series of experiments analyzing max activating examples and causal interventions, we show that many of our identified features indeed are relevant for the reasoning process and give rise to different types of reasoning patterns such as restarting from the beginning, recalling prior knowledge, expressing uncertainty, and double-checking.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Selective Expert Guidance for Effective and Diverse Exploration in Reinforcement Learning of LLMs</title>
<link>https://arxiv.org/abs/2510.04140</link>
<guid>https://arxiv.org/abs/2510.04140</guid>
<content:encoded><![CDATA[
arXiv:2510.04140v1 Announce Type: cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a widely adopted technique for enhancing the reasoning ability of Large Language Models (LLMs). However, the effectiveness of RLVR strongly depends on the capability of base models. This issue arises because it requires the model to have sufficient capability to perform high-quality exploration, which involves both effectiveness and diversity. Unfortunately, existing methods address this issue by imitating expert trajectories, which improve effectiveness but neglect diversity. To address this, we argue that the expert only needs to provide guidance only at critical decision points rather than the entire reasoning path. Based on this insight, we propose MENTOR: Mixed-policy Expert Navigation for Token-level Optimization of Reasoning, a framework that provides expert guidance only at critical decision points to perform effective and diverse exploration in RLVR. Extensive experiments show that MENTOR enables models capture the essence of expert strategies rather than surface imitation, thereby performing high-quality exploration and achieving superior overall performance. Our code is available online.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automating construction safety inspections using a multi-modal vision-language RAG framework</title>
<link>https://arxiv.org/abs/2510.04145</link>
<guid>https://arxiv.org/abs/2510.04145</guid>
<content:encoded><![CDATA[
arXiv:2510.04145v1 Announce Type: cross 
Abstract: Conventional construction safety inspection methods are often inefficient as they require navigating through large volume of information. Recent advances in large vision-language models (LVLMs) provide opportunities to automate safety inspections through enhanced visual and linguistic understanding. However, existing applications face limitations including irrelevant or unspecific responses, restricted modal inputs and hallucinations. Utilisation of Large Language Models (LLMs) for this purpose is constrained by availability of training data and frequently lack real-time adaptability. This study introduces SiteShield, a multi-modal LVLM-based Retrieval-Augmented Generation (RAG) framework for automating construction safety inspection reports by integrating visual and audio inputs. Using real-world data, SiteShield outperformed unimodal LLMs without RAG with an F1 score of 0.82, hamming loss of 0.04, precision of 0.76, and recall of 0.96. The findings indicate that SiteShield offers a novel pathway to enhance information retrieval and efficiency in generating safety reports.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Next-Token Prediction: A Performance Characterization of Diffusion versus Autoregressive Language Models</title>
<link>https://arxiv.org/abs/2510.04146</link>
<guid>https://arxiv.org/abs/2510.04146</guid>
<content:encoded><![CDATA[
arXiv:2510.04146v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have achieved state-of-the-art performance on a broad range of Natural Language Processing (NLP) tasks, including document processing and coding. Autoregressive Language Models (ARMs), which generate tokens sequentially conditioned on all previous tokens, have been the predominant paradigm for LLMs. However, while these networks have achieved high accuracy across a range of downstream tasks, they exhibit low arithmetic intensity due to the inherent sequential dependency with next-token prediction. Recently, Diffusion Language Models (DLMs) have emerged as a promising alternative architecture. DLMs generate output text in parallel, breaking the limitations of sequential dependency. However, the performance implications of DLMs relative to commonly deployed ARMs are not fully understood. In this work, we present a comprehensive performance study analyzing the performance characteristics of ARMs and DLMs, using both theoretical analysis and profiling data to characterize the trade-offs between these approaches. We illustrate that although DLMs exhibit higher arithmetic intensity compared to ARMs because of their capability to utilize parallelism across sequence lengths, they fail to scale effectively to longer contexts. We then explore DLMs with block-wise decoding, outlining how this approach allows for increased arithmetic intensity, while still scaling well to long contexts (similar to ARMs). We also show interesting trade-offs for batched inference, where we find that ARMs exhibit superior throughput, as they benefit more from parallelism across sequences in the batch. Finally, we highlight opportunities for accelerating DLM inference, and, in particular, highlight the importance of reducing the number of sampling steps for allowing open-source DLMs to provide improved latency relative to ARMs.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zoom-In to Sort AI-Generated Images Out</title>
<link>https://arxiv.org/abs/2510.04225</link>
<guid>https://arxiv.org/abs/2510.04225</guid>
<content:encoded><![CDATA[
arXiv:2510.04225v1 Announce Type: cross 
Abstract: The rapid growth of AI-generated imagery has blurred the boundary between real and synthetic content, raising critical concerns for digital integrity. Vision-language models (VLMs) offer interpretability through explanations but often fail to detect subtle artifacts in high-quality synthetic images. We propose ZoomIn, a two-stage forensic framework that improves both accuracy and interpretability. Mimicking human visual inspection, ZoomIn first scans an image to locate suspicious regions and then performs a focused analysis on these zoomed-in areas to deliver a grounded verdict. To support training, we introduce MagniFake, a dataset of 20,000 real and high-quality synthetic images annotated with bounding boxes and forensic explanations, generated through an automated VLM-based pipeline. Our method achieves 96.39% accuracy with robust generalization, while providing human-understandable explanations grounded in visual evidence.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Pass$\mathtt{@}k$: A Bayesian Framework for Large Language Model Evaluation</title>
<link>https://arxiv.org/abs/2510.04265</link>
<guid>https://arxiv.org/abs/2510.04265</guid>
<content:encoded><![CDATA[
arXiv:2510.04265v1 Announce Type: cross 
Abstract: Pass$@k$ is widely used to report performance for LLM reasoning, but it often yields unstable, misleading rankings, especially when the number of trials (samples) is limited and compute is constrained. We present a principled Bayesian evaluation framework that replaces Pass$@k$ and average accuracy over $N$ trials (avg$@N$) with posterior estimates of a model's underlying success probability and credible intervals, yielding stable rankings and a transparent decision rule for differences. Evaluation outcomes are modeled as categorical (not just 0/1) with a Dirichlet prior, giving closed-form expressions for the posterior mean and uncertainty of any weighted rubric and enabling the use of prior evidence when appropriate. Theoretically, under a uniform prior, the Bayesian posterior mean is order-equivalent to average accuracy (Pass$@1$), explaining its empirical robustness while adding principled uncertainty. Empirically, in simulations with known ground-truth success rates and on AIME'24/'25, HMMT'25, and BrUMO'25, the Bayesian/avg procedure achieves faster convergence and greater rank stability than Pass$@k$ and recent variants, enabling reliable comparisons at far smaller sample counts. The framework clarifies when observed gaps are statistically meaningful (non-overlapping credible intervals) versus noise, and it naturally extends to graded, rubric-based evaluations. Together, these results recommend replacing Pass$@k$ for LLM evaluation and ranking with a posterior-based, compute-efficient protocol that unifies binary and non-binary evaluation while making uncertainty explicit. Code is available at https://mohsenhariri.github.io/bayes-kit
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wave-PDE Nets: Trainable Wave-Equation Layers as an Alternative to Attention</title>
<link>https://arxiv.org/abs/2510.04304</link>
<guid>https://arxiv.org/abs/2510.04304</guid>
<content:encoded><![CDATA[
arXiv:2510.04304v1 Announce Type: cross 
Abstract: We introduce Wave-PDE Nets, a neural architecture whose elementary operation is a differentiable simulation of the second-order wave equation. Each layer propagates its hidden state as a continuous field through a medium with trainable spatial velocity c(x) and damping {\gamma}(x). A symplectic spectral solver based on FFTs realises this propagation in O(nlog n) time. This oscillatory, global mechanism provides a powerful alternative to attention and first-order state-space models. We prove that a single Wave-PDE layer is a universal approximator. On language and vision benchmarks, Wave-PDE Nets match or exceed Transformer performance while demonstrating superior practical efficiency, reducing wall-clock time by up to 30% and peak memory by 25%. Ablation studies confirm the critical role of symplectic integration and a spectral Laplacian for stability and performance. Visualizations of the learned physical parameters reveal that the model learns intuitive strategies for information propagation. These results position Wave-PDE Nets as a computationally efficient and robust architecture with a strong physical inductive bias.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MacroBench: A Novel Testbed for Web Automation Scripts via Large Language Models</title>
<link>https://arxiv.org/abs/2510.04363</link>
<guid>https://arxiv.org/abs/2510.04363</guid>
<content:encoded><![CDATA[
arXiv:2510.04363v1 Announce Type: cross 
Abstract: We introduce MacroBench, a code-first benchmark that evaluates whether LLMs can synthesize reusable browser automation programs from natural language goals by reading HTML/DOM and emitting Python with Selenium. MacroBench instantiates seven self-hosted sites: Airbnb-like, TikTok-like, Reddit-like, Instagram-like, Facebook-like, Discord-like, and Threads-like, covering 681 tasks across interaction complexity and targeting difficulty. Our end-to-end protocol validates generated code via static checks, sandboxed execution, and outcome verification including DOM assertions and database snapshots, and includes a safety suite for scraping, spam/abuse, and credential/privacy prompts. Across 2636 model-task runs, we observe stratified success: GPT-4o-Mini achieves 96.8 percent, GPT-4.1 achieves 95.3 percent, Gemini-2.5-Pro achieves 89.0 percent, and DeepSeek-V3.1 achieves 83.4 percent. Models handle simple tasks reliably at 91.7 percent but fail on complex workflows at 0.0 percent, and none meet production-quality coding practices despite functional completion. We release our complete benchmark pipeline, evaluation framework, and experimental results to enable reproducible assessment of macro synthesis for web automation.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator</title>
<link>https://arxiv.org/abs/2510.04390</link>
<guid>https://arxiv.org/abs/2510.04390</guid>
<content:encoded><![CDATA[
arXiv:2510.04390v1 Announce Type: cross 
Abstract: World models that support controllable
  and editable spatiotemporal environments are valuable
  for robotics, enabling scalable training data, repro ducible evaluation, and flexible task design. While
  recent text-to-video models generate realistic dynam ics, they are constrained to 2D views and offer limited
  interaction. We introduce MorphoSim, a language guided framework that generates 4D scenes with
  multi-view consistency and object-level controls. From
  natural language instructions, MorphoSim produces
  dynamic environments where objects can be directed,
  recolored, or removed, and scenes can be observed
  from arbitrary viewpoints. The framework integrates
  trajectory-guided generation with feature field dis tillation, allowing edits to be applied interactively
  without full re-generation. Experiments show that Mor phoSim maintains high scene fidelity while enabling
  controllability and editability. The code is available
  at https://github.com/eric-ai-lab/Morph4D.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Internal World Models as Imagination Networks in Cognitive Agents</title>
<link>https://arxiv.org/abs/2510.04391</link>
<guid>https://arxiv.org/abs/2510.04391</guid>
<content:encoded><![CDATA[
arXiv:2510.04391v1 Announce Type: cross 
Abstract: What is the computational objective of imagination? While classical interpretations suggest imagination is useful for maximizing rewards, recent findings challenge this view. In this study, we propose that imagination serves to access an internal world model (IWM) and use psychological network analysis to explore IWMs in humans and large language models (LLMs). Specifically, we assessed imagination vividness ratings using two questionnaires and constructed imagination networks from these reports. Imagination networks from human groups showed correlations between different centrality measures, including expected influence, strength, and closeness. However, imagination networks from LLMs showed a lack of clustering and lower correlations between centrality measures under different prompts and conversational memory conditions. Together, these results indicate a lack of similarity between IWMs in human and LLM agents. Overall, our study offers a novel method for comparing internally-generated representations in humans and AI, providing insights for developing human-like imagination in artificial intelligence.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Partial Information Decomposition via Normalizing Flows in Latent Gaussian Distributions</title>
<link>https://arxiv.org/abs/2510.04417</link>
<guid>https://arxiv.org/abs/2510.04417</guid>
<content:encoded><![CDATA[
arXiv:2510.04417v1 Announce Type: cross 
Abstract: The study of multimodality has garnered significant interest in fields where the analysis of interactions among multiple information sources can enhance predictive modeling, data fusion, and interpretability. Partial information decomposition (PID) has emerged as a useful information-theoretic framework to quantify the degree to which individual modalities independently, redundantly, or synergistically convey information about a target variable. However, existing PID methods depend on optimizing over a joint distribution constrained by estimated pairwise probability distributions, which are costly and inaccurate for continuous and high-dimensional modalities. Our first key insight is that the problem can be solved efficiently when the pairwise distributions are multivariate Gaussians, and we refer to this problem as Gaussian PID (GPID). We propose a new gradient-based algorithm that substantially improves the computational efficiency of GPID based on an alternative formulation of the underlying optimization problem. To generalize the applicability to non-Gaussian data, we learn information-preserving encoders to transform random variables of arbitrary input distributions into pairwise Gaussian random variables. Along the way, we resolved an open problem regarding the optimality of joint Gaussian solutions for GPID. Empirical validation in diverse synthetic examples demonstrates that our proposed method provides more accurate and efficient PID estimates than existing baselines. We further evaluate a series of large-scale multimodal benchmarks to show its utility in real-world applications of quantifying PID in multimodal datasets and selecting high-performing models.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical Vision-Language Models</title>
<link>https://arxiv.org/abs/2510.04477</link>
<guid>https://arxiv.org/abs/2510.04477</guid>
<content:encoded><![CDATA[
arXiv:2510.04477v1 Announce Type: cross 
Abstract: Bridging clinical diagnostic reasoning with AI remains a central challenge in medical imaging. We introduce MedCLM, an automated pipeline that converts detection datasets into large-scale medical visual question answering (VQA) data with Chain-of-Thought (CoT) reasoning by linking lesion boxes to organ segmentation and structured rationales. These contextual signals enable medical vision-language models to generate question-answer pairs with step-by-step reasoning. To utilize this data effectively, we propose an Integrated CoT-Curriculum Strategy composed of an Easy stage with explicit lesion boxes for visual grounding, a Medium stage that encourages implicit localization, and a Hard stage for weakly supervised reasoning. Experimental results demonstrate that MedCLM attains state-of-the-art performance on several medical VQA benchmarks, providing a scalable framework for developing clinically aligned medical vision-language models.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents</title>
<link>https://arxiv.org/abs/2510.04491</link>
<guid>https://arxiv.org/abs/2510.04491</guid>
<content:encoded><![CDATA[
arXiv:2510.04491v1 Announce Type: cross 
Abstract: Despite rapid progress in building conversational AI agents, robustness is still largely untested. Small shifts in user behavior, such as being more impatient, incoherent, or skeptical, can cause sharp drops in agent performance, revealing how brittle current AI agents are. Today's benchmarks fail to capture this fragility: agents may perform well under standard evaluations but degrade spectacularly in more realistic and varied settings. We address this robustness testing gap by introducing TraitBasis, a lightweight, model-agnostic method for systematically stress testing AI agents. TraitBasis learns directions in activation space corresponding to steerable user traits (e.g., impatience or incoherence), which can be controlled, scaled, composed, and applied at inference time without any fine-tuning or extra data. Using TraitBasis, we extend $\tau$-Bench to $\tau$-Trait, where user behaviors are altered via controlled trait vectors. We observe on average a 2%-30% performance degradation on $\tau$-Trait across frontier models, highlighting the lack of robustness of current AI agents to variations in user behavior. Together, these results highlight both the critical role of robustness testing and the promise of TraitBasis as a simple, data-efficient, and compositional tool. By powering simulation-driven stress tests and training loops, TraitBasis opens the door to building AI agents that remain reliable in the unpredictable dynamics of real-world human interactions. We have open-sourced $\tau$-Trai across four domains: airline, retail, telecom, and telehealth, so the community can systematically QA their agents under realistic, behaviorally diverse intents and trait scenarios: https://github.com/collinear-ai/tau-trait.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs</title>
<link>https://arxiv.org/abs/2510.04503</link>
<guid>https://arxiv.org/abs/2510.04503</guid>
<content:encoded><![CDATA[
arXiv:2510.04503v1 Announce Type: cross 
Abstract: During fine-tuning, large language models (LLMs) are increasingly vulnerable to data-poisoning backdoor attacks, which compromise their reliability and trustworthiness. However, existing defense strategies suffer from limited generalization: they only work on specific attack types or task settings. In this study, we propose Poison-to-Poison (P2P), a general and effective backdoor defense algorithm. P2P injects benign triggers with safe alternative labels into a subset of training samples and fine-tunes the model on this re-poisoned dataset by leveraging prompt-based learning. This enforces the model to associate trigger-induced representations with safe outputs, thereby overriding the effects of original malicious triggers. Thanks to this robust and generalizable trigger-based fine-tuning, P2P is effective across task settings and attack types. Theoretically and empirically, we show that P2P can neutralize malicious backdoors while preserving task performance. We conduct extensive experiments on classification, mathematical reasoning, and summary generation tasks, involving multiple state-of-the-art LLMs. The results demonstrate that our P2P algorithm significantly reduces the attack success rate compared with baseline models. We hope that the P2P can serve as a guideline for defending against backdoor attacks and foster the development of a secure and trustworthy LLM community.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering</title>
<link>https://arxiv.org/abs/2510.04514</link>
<guid>https://arxiv.org/abs/2510.04514</guid>
<content:encoded><![CDATA[
arXiv:2510.04514v1 Announce Type: cross 
Abstract: Recent multimodal LLMs have shown promise in chart-based visual question answering, but their performance declines sharply on unannotated charts, those requiring precise visual interpretation rather than relying on textual shortcuts. To address this, we introduce ChartAgent, a novel agentic framework that explicitly performs visual reasoning directly within the chart's spatial domain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively decomposes queries into visual subtasks and actively manipulates and interacts with chart images through specialized actions such as drawing annotations, cropping regions (e.g., segmenting pie slices, isolating bars), and localizing axes, using a library of chart-specific vision tools to fulfill each subtask. This iterative reasoning process closely mirrors human cognitive strategies for chart comprehension. ChartAgent achieves state-of-the-art accuracy on the ChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07% absolute gain overall and 17.31% on unannotated, numerically intensive queries. Furthermore, our analyses show that ChartAgent is (a) effective across diverse chart types, (b) achieve the highest scores across varying visual and reasoning complexity levels, and (c) serves as a plug-and-play framework that boosts performance across diverse underlying LLMs. Our work is among the first to demonstrate visually grounded reasoning for chart understanding using tool-augmented multimodal agents.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models</title>
<link>https://arxiv.org/abs/2510.04532</link>
<guid>https://arxiv.org/abs/2510.04532</guid>
<content:encoded><![CDATA[
arXiv:2510.04532v1 Announce Type: cross 
Abstract: Vision-Language Model (VLM) driving agents promise explainable end-to-end autonomy by first producing natural-language reasoning and then predicting trajectory planning. However, whether planning is causally driven by this reasoning remains a critical but unverified assumption. To investigate this, we build DriveMind, a large-scale driving Visual Question Answering (VQA) corpus with plan-aligned Chain-of-Thought (CoT), automatically generated from nuPlan. Our data generation process converts sensors and annotations into structured inputs and, crucially, separates priors from to-be-reasoned signals, enabling clean information ablations. Using DriveMind, we train representative VLM agents with Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO) and evaluate them with nuPlan's metrics. Our results, unfortunately, indicate a consistent causal disconnect in reasoning-planning: removing ego/navigation priors causes large drops in planning scores, whereas removing CoT produces only minor changes. Attention analysis further shows that planning primarily focuses on priors rather than the CoT. Based on this evidence, we propose the Reasoning-Planning Decoupling Hypothesis, positing that the training-yielded reasoning is an ancillary byproduct rather than a causal mediator. To enable efficient diagnosis, we also introduce a novel, training-free probe that measures an agent's reliance on priors by evaluating its planning robustness against minor input perturbations. In summary, we provide the community with a new dataset and a diagnostic tool to evaluate the causal fidelity of future models.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning</title>
<link>https://arxiv.org/abs/2510.04573</link>
<guid>https://arxiv.org/abs/2510.04573</guid>
<content:encoded><![CDATA[
arXiv:2510.04573v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) demonstrate their reasoning ability through chain-of-thought (CoT) generation. However, LLM's autoregressive decoding may limit the ability to revisit and refine earlier tokens in a holistic manner, which can also lead to inefficient exploration for diverse solutions. In this paper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoning framework that unifies the expressiveness of continuous latent representation with the iterative refinement capabilities of latent diffusion models for an existing LLM. We first construct a structured latent reasoning space using a Variational Autoencoder (VAE) that encodes text reasoning steps into blocks of thought tokens, preserving semantic information and interpretability while offering compact but expressive representations. Subsequently, we utilize a latent diffusion model that learns to denoise a block of latent thought tokens with a blockwise bidirectional attention mask, enabling longer horizon and iterative refinement with adaptive test-time compute. This design allows efficient parallel generation of diverse reasoning trajectories, allowing the model to plan and revise the reasoning process holistically. We conduct evaluations on a suite of mathematical reasoning and planning benchmarks. Empirical results show that LaDiR consistently improves accuracy, diversity, and interpretability over existing autoregressive, diffusion-based, and latent reasoning methods, revealing a new paradigm for text reasoning with latent diffusion.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models</title>
<link>https://arxiv.org/abs/2510.04618</link>
<guid>https://arxiv.org/abs/2510.04618</guid>
<content:encoded><![CDATA[
arXiv:2510.04618v1 Announce Type: cross 
Abstract: Large language model (LLM) applications such as agents and domain-specific reasoning increasingly rely on context adaptation -- modifying inputs with instructions, strategies, or evidence, rather than weight updates. Prior approaches improve usability but often suffer from brevity bias, which drops domain insights for concise summaries, and from context collapse, where iterative rewriting erodes details over time. Building on the adaptive memory introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context Engineering), a framework that treats contexts as evolving playbooks that accumulate, refine, and organize strategies through a modular process of generation, reflection, and curation. ACE prevents collapse with structured, incremental updates that preserve detailed knowledge and scale with long-context models. Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and rollout cost. Notably, ACE could adapt effectively without labeled supervision and instead by leveraging natural execution feedback. On the AppWorld leaderboard, ACE matches the top-ranked production-level agent on the overall average and surpasses it on the harder test-challenge split, despite using a smaller open-source model. These results show that comprehensive, evolving contexts enable scalable, efficient, and self-improving LLM systems with low overhead.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AtomWorld: A Benchmark for Evaluating Spatial Reasoning in Large Language Models on Crystalline Materials</title>
<link>https://arxiv.org/abs/2510.04704</link>
<guid>https://arxiv.org/abs/2510.04704</guid>
<content:encoded><![CDATA[
arXiv:2510.04704v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) excel at textual reasoning and are beginning to develop spatial understanding, prompting the question of whether these abilities can be combined for complex, domain-specific tasks. This question is essential in fields like materials science, where deep understanding of 3D atomic structures is fundamental. While initial studies have successfully applied LLMs to tasks involving pure crystal generation or coordinate understandings, a standardized benchmark to systematically evaluate their core reasoning abilities across diverse atomic structures has been notably absent. To address this gap, we introduce the AtomWorld benchmark to evaluate LLMs on tasks based in Crystallographic Information Files (CIFs), a standard structure representation format. These tasks, including structural editing, CIF perception, and property-guided modeling, reveal a critical limitation: current models, despite establishing promising baselines, consistently fail in structural understanding and spatial reasoning. Our experiments show that these models make frequent errors on structure modification tasks, and even in the basic CIF format understandings, potentially leading to cumulative errors in subsequent analysis and materials insights. By defining these standardized tasks, AtomWorld lays the ground for advancing LLMs toward robust atomic-scale modeling, crucial for accelerating materials research and automating scientific workflows.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs</title>
<link>https://arxiv.org/abs/2510.04721</link>
<guid>https://arxiv.org/abs/2510.04721</guid>
<content:encoded><![CDATA[
arXiv:2510.04721v1 Announce Type: cross 
Abstract: Large language models (LLMs) have recently shown strong performance on mathematical benchmarks. At the same time, they are prone to hallucination and sycophancy, often providing convincing but flawed proofs for incorrect mathematical statements provided by users. This significantly limits the applicability of LLMs in theorem proving, as verification of these flawed proofs must be done manually by expert mathematicians. However, existing benchmarks that measure sycophancy in mathematics are limited: they focus solely on final-answer problems, rely on very simple and often contaminated datasets, and construct benchmark samples using synthetic modifications that create ill-posed questions rather than well-posed questions that are demonstrably false. To address these issues, we introduce BrokenMath, the first benchmark for evaluating sycophantic behavior in LLMs within the context of natural language theorem proving. BrokenMath is built from advanced 2025 competition problems, which are perturbed with an LLM to produce false statements and subsequently refined through expert review. Using an LLM-as-a-judge framework, we evaluate state-of-the-art LLMs and agentic systems and find that sycophancy is widespread, with the best model, GPT-5, producing sycophantic answers 29% of the time. We further investigate several mitigation strategies, including test-time interventions and supervised fine-tuning on curated sycophantic examples. These approaches substantially reduce, but do not eliminate, sycophantic behavior.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba</title>
<link>https://arxiv.org/abs/2510.04738</link>
<guid>https://arxiv.org/abs/2510.04738</guid>
<content:encoded><![CDATA[
arXiv:2510.04738v1 Announce Type: cross 
Abstract: We introduce MAVE (Mamba with Cross-Attention for Voice Editing and Synthesis), a novel autoregressive architecture for text-conditioned voice editing and high-fidelity text-to-speech (TTS) synthesis, built on a cross-attentive Mamba backbone. MAVE achieves state-of-the-art performance in speech editing and very competitive results in zero-shot TTS, while not being explicitly trained on the latter task, outperforming leading autoregressive and diffusion models on diverse, real-world audio. By integrating Mamba for efficient audio sequence modeling with cross-attention for precise text-acoustic alignment, MAVE enables context-aware voice editing with exceptional naturalness and speaker consistency. In pairwise human evaluations on a random 40-sample subset of the RealEdit benchmark (400 judgments), 57.2% of listeners rated MAVE - edited speech as perceptually equal to the original, while 24.8% prefered the original and 18.0% MAVE - demonstrating that in the majority of cases edits are indistinguishable from the source. MAVE compares favorably with VoiceCraft and FluentSpeech both on pairwise comparisons and standalone mean opinion score (MOS) evaluations. For zero-shot TTS, MAVE exceeds VoiceCraft in both speaker similarity and naturalness, without requiring multiple inference runs or post-processing. Remarkably, these quality gains come with a significantly lower memory cost and approximately the same latency: MAVE requires ~6x less memory than VoiceCraft during inference on utterances from the RealEdit database (mean duration: 6.21s, A100, FP16, batch size 1). Our results demonstrate that MAVE establishes a new standard for flexible, high-fidelity voice editing and synthesis through the synergistic integration of structured state-space modeling and cross-modal attention.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Representations inside the Language Model</title>
<link>https://arxiv.org/abs/2510.04819</link>
<guid>https://arxiv.org/abs/2510.04819</guid>
<content:encoded><![CDATA[
arXiv:2510.04819v1 Announce Type: cross 
Abstract: Despite interpretability work analyzing VIT encoders and transformer activations, we don't yet understand why Multimodal Language Models (MLMs) struggle on perception-heavy tasks. We offer an under-studied perspective by examining how popular MLMs (LLaVA-OneVision, Qwen2.5-VL, and Llama-3-LLaVA-NeXT) process their visual key-value tokens. We first study the flow of visual information through the language model, finding that image value tokens encode sufficient information to perform several perception-heavy tasks zero-shot: segmentation, semantic correspondence, temporal correspondence, and referring expression detection. We find that while the language model does augment the visual information received from the projection of input visual encodings-which we reveal correlates with overall MLM perception capability-it contains less visual information on several tasks than the equivalent visual encoder (SigLIP) that has not undergone MLM finetuning. Further, we find that the visual information corresponding to input-agnostic image key tokens in later layers of language models contains artifacts which reduce perception capability of the overall MLM. Next, we discuss controlling visual information in the language model, showing that adding a text prefix to the image input improves perception capabilities of visual representations. Finally, we reveal that if language models were able to better control their visual information, their perception would significantly improve; e.g., in 33.3% of Art Style questions in the BLINK benchmark, perception information present in the language model is not surfaced to the output! Our findings reveal insights into the role of key-value tokens in multimodal systems, paving the way for deeper mechanistic interpretability of MLMs and suggesting new directions for training their visual encoder and language model components.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-Augmented Code Generation: A Survey with Focus on Repository-Level Approaches</title>
<link>https://arxiv.org/abs/2510.04905</link>
<guid>https://arxiv.org/abs/2510.04905</guid>
<content:encoded><![CDATA[
arXiv:2510.04905v1 Announce Type: cross 
Abstract: Recent advancements in large language models (LLMs) have substantially improved automated code generation. While function-level and file-level generation have achieved promising results, real-world software development typically requires reasoning across entire repositories. This gives rise to the challenging task of Repository-Level Code Generation (RLCG), where models must capture long-range dependencies, ensure global semantic consistency, and generate coherent code spanning multiple files or modules. To address these challenges, Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm that integrates external retrieval mechanisms with LLMs, enhancing context-awareness and scalability. In this survey, we provide a comprehensive review of research on Retrieval-Augmented Code Generation (RACG), with an emphasis on repository-level approaches. We categorize existing work along several dimensions, including generation strategies, retrieval modalities, model architectures, training paradigms, and evaluation protocols. Furthermore, we summarize widely used datasets and benchmarks, analyze current limitations, and outline key challenges and opportunities for future research. Our goal is to establish a unified analytical framework for understanding this rapidly evolving field and to inspire continued progress in AI-powered software engineering.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.04935</link>
<guid>https://arxiv.org/abs/2510.04935</guid>
<content:encoded><![CDATA[
arXiv:2510.04935v1 Announce Type: cross 
Abstract: Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in simple tasks, where the models excessively utilize System 2-type, deliberate reasoning, leading to inefficient token generation. Furthermore, these models face challenges in adapting their reasoning capabilities to rapidly changing environments due to the static nature of their pretraining data. To address these issues, advancing Large Language Models (LLMs) for complex reasoning tasks requires innovative approaches that bridge intuitive and deliberate cognitive processes, akin to human cognition's dual-system dynamic. This paper introduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless integration of System 1's fast, intuitive thinking with System 2's deliberate reasoning within LLMs. MARS strategically integrates multiple external tools, such as Google Search, Google Scholar, and Python Interpreter, to access up-to-date information and execute complex computations, while creating a specialized division of labor where System 1 efficiently processes and summarizes high-volume external information, providing distilled insights that expand System 2's reasoning context without overwhelming its capacity. Furthermore, we propose a multi-agent reinforcement learning framework extending Group Relative Policy Optimization to simultaneously optimize both systems with multi-turn tool interactions, bin-packing optimization, and sample balancing strategies that enhance collaborative efficiency. Extensive experiments demonstrate MARS achieves substantial improvements of 3.86% on the challenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9% across 7 knowledge-intensive tasks, validating the effectiveness of our dual-system paradigm for complex reasoning in dynamic information environments.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ONNX-Net: Towards Universal Representations and Instant Performance Prediction for Neural Architectures</title>
<link>https://arxiv.org/abs/2510.04938</link>
<guid>https://arxiv.org/abs/2510.04938</guid>
<content:encoded><![CDATA[
arXiv:2510.04938v1 Announce Type: cross 
Abstract: Neural architecture search (NAS) automates the design process of high-performing architectures, but remains bottlenecked by expensive performance evaluation. Most existing studies that achieve faster evaluation are mostly tied to cell-based search spaces and graph encodings tailored to those individual search spaces, limiting their flexibility and scalability when applied to more expressive search spaces. In this work, we aim to close the gap of individual search space restrictions and search space dependent network representations. We present ONNX-Bench, a benchmark consisting of a collection of neural networks in a unified format based on ONNX files. ONNX-Bench includes all open-source NAS-bench-based neural networks, resulting in a total size of more than 600k {architecture, accuracy} pairs. This benchmark allows creating a shared neural network representation, ONNX-Net, able to represent any neural architecture using natural language descriptions acting as an input to a performance predictor. This text-based encoding can accommodate arbitrary layer types, operation parameters, and heterogeneous topologies, enabling a single surrogate to generalise across all neural architectures rather than being confined to cell-based search spaces. Experiments show strong zero-shot performance across disparate search spaces using only a small amount of pretraining samples, enabling the unprecedented ability to evaluate any neural network architecture instantly.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Structured State-Space Duality</title>
<link>https://arxiv.org/abs/2510.04944</link>
<guid>https://arxiv.org/abs/2510.04944</guid>
<content:encoded><![CDATA[
arXiv:2510.04944v1 Announce Type: cross 
Abstract: Structured State-Space Duality (SSD) [Dao & Gu, ICML 2024] is an equivalence between a simple Structured State-Space Model (SSM) and a masked attention mechanism. In particular, a state-space model with a scalar-times-identity state matrix is equivalent to a masked self-attention with a $1$-semiseparable causal mask. Consequently, the same sequence transformation (model) has two algorithmic realizations: as a linear-time $O(T)$ recurrence or as a quadratic-time $O(T^2)$ attention. In this note, we formalize and generalize this duality: (i) we extend SSD from the scalar-identity case to general diagonal SSMs (diagonal state matrices); (ii) we show that these diagonal SSMs match the scalar case's training complexity lower bounds while supporting richer dynamics; (iii) we establish a necessary and sufficient condition under which an SSM is equivalent to $1$-semiseparable masked attention; and (iv) we show that such duality fails to extend to standard softmax attention due to rank explosion. Together, these results tighten bridge between recurrent SSMs and Transformers, and widen the design space for expressive yet efficient sequence models.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and Rationale Inference in Imperfect Information Collaboration Game</title>
<link>https://arxiv.org/abs/2510.04980</link>
<guid>https://arxiv.org/abs/2510.04980</guid>
<content:encoded><![CDATA[
arXiv:2510.04980v1 Announce Type: cross 
Abstract: Effective multi-agent collaboration requires agents to infer the rationale behind others' actions, a capability rooted in Theory-of-Mind (ToM). While recent Large Language Models (LLMs) excel at logical inference, their ability to infer rationale in dynamic, collaborative settings remains under-explored. This study introduces LLM-Hanabi, a novel benchmark that uses the cooperative game Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework features an automated evaluation system that measures both game performance and ToM proficiency. Across a range of models, we find a significant positive correlation between ToM and in-game success. Notably, first-order ToM (interpreting others' intent) correlates more strongly with performance than second-order ToM (predicting others' interpretations). These findings highlight that for effective AI collaboration, the ability to accurately interpret a partner's rationale is more critical than higher-order reasoning. We conclude that prioritizing first-order ToM is a promising direction for enhancing the collaborative capabilities of future models.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training</title>
<link>https://arxiv.org/abs/2510.04996</link>
<guid>https://arxiv.org/abs/2510.04996</guid>
<content:encoded><![CDATA[
arXiv:2510.04996v1 Announce Type: cross 
Abstract: Reinforcement learning applied to large language models (LLMs) for reasoning tasks is often bottlenecked by unstable gradient estimates due to fixed and uniform sampling of responses across prompts. Prior work such as GVM-RAFT addresses this by dynamically allocating inference budget per prompt to minimize stochastic gradient variance under a budget constraint. Inspired by this insight, we propose Reinforce-Ada, an adaptive sampling framework for online RL post-training of LLMs that continuously reallocates sampling effort to the prompts with the greatest uncertainty or learning potential. Unlike conventional two-stage allocation methods, Reinforce-Ada interleaves estimation and sampling in an online successive elimination process, and automatically stops sampling for a prompt once sufficient signal is collected. To stabilize updates, we form fixed-size groups with enforced reward diversity and compute advantage baselines using global statistics aggregated over the adaptive sampling phase. Empirical results across multiple model architectures and reasoning benchmarks show that Reinforce-Ada accelerates convergence and improves final performance compared to GRPO, especially when using the balanced sampling variant. Our work highlights the central role of variance-aware, adaptive data curation in enabling efficient and reliable reinforcement learning for reasoning-capable LLMs. Code is available at https://github.com/RLHFlow/Reinforce-Ada.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Achieve Gold Medal Performance at International Astronomy &amp; Astrophysics Olympiad</title>
<link>https://arxiv.org/abs/2510.05016</link>
<guid>https://arxiv.org/abs/2510.05016</guid>
<content:encoded><![CDATA[
arXiv:2510.05016v1 Announce Type: cross 
Abstract: While task-specific demonstrations show early success in applying large language models (LLMs) to automate some astronomical research tasks, they only provide incomplete views of all necessary capabilities in solving astronomy problems, calling for more thorough understanding of LLMs' strengths and limitations. So far, existing benchmarks and evaluations focus on simple question-answering that primarily tests astronomical knowledge and fails to evaluate the complex reasoning required for real-world research in the discipline. Here, we address this gap by systematically benchmarking five state-of-the-art LLMs on the International Olympiad on Astronomy and Astrophysics (IOAA) exams, which are designed to examine deep conceptual understanding, multi-step derivations, and multimodal analysis. With average scores of 85.6% and 84.2%, Gemini 2.5 Pro and GPT-5 (the two top-performing models) not only achieve gold medal level performance but also rank in the top two among ~200-300 participants in all four IOAA theory exams evaluated (2022-2025). In comparison, results on the data analysis exams show more divergence. GPT-5 still excels in the exams with an 88.5% average score, ranking top 10 among the participants in the four most recent IOAAs, while other models' performances drop to 48-76%. Furthermore, our in-depth error analysis underscores conceptual reasoning, geometric reasoning, and spatial visualization (52-79% accuracy) as consistent weaknesses among all LLMs. Hence, although LLMs approach peak human performance in theory exams, critical gaps must be addressed before they can serve as autonomous research agents in astronomy.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proactive defense against LLM Jailbreak</title>
<link>https://arxiv.org/abs/2510.05052</link>
<guid>https://arxiv.org/abs/2510.05052</guid>
<content:encoded><![CDATA[
arXiv:2510.05052v1 Announce Type: cross 
Abstract: The proliferation of powerful large language models (LLMs) has necessitated robust safety alignment, yet these models remain vulnerable to evolving adversarial attacks, including multi-turn jailbreaks that iteratively search for successful queries. Current defenses, primarily reactive and static, often fail to counter these search-based attacks. In this paper, we introduce ProAct, a novel proactive defense framework designed to disrupt and mislead autonomous jailbreaking processes. Our core idea is to intentionally provide adversaries with "spurious responses" that appear to be results of successful jailbreak attacks but contain no actual harmful content. These misleading responses provide false signals to the attacker's internal optimization loop, causing the adversarial search to terminate prematurely and effectively jailbreaking the jailbreak. By conducting extensive experiments across state-of-the-art LLMs, jailbreaking frameworks, and safety benchmarks, our method consistently and significantly reduces attack success rates by up to 92\%. When combined with other defense frameworks, it further reduces the success rate of the latest attack strategies to 0\%. ProAct represents an orthogonal defense strategy that can serve as an additional guardrail to enhance LLM safety against the most effective jailbreaking attacks.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Interpret Weight Differences in Language Models</title>
<link>https://arxiv.org/abs/2510.05092</link>
<guid>https://arxiv.org/abs/2510.05092</guid>
<content:encoded><![CDATA[
arXiv:2510.05092v1 Announce Type: cross 
Abstract: Finetuning (pretrained) language models is a standard approach for updating their internal parametric knowledge and specializing them to new tasks and domains. However, the corresponding model weight changes ("weight diffs") are not generally interpretable. While inspecting the finetuning dataset can give a sense of how the model might have changed, these datasets are often not publicly available or are too large to work with directly. Towards the goal of comprehensively understanding weight diffs in natural language, we introduce Diff Interpretation Tuning (DIT), a method that trains models to describe their own finetuning-induced modifications. Our approach uses synthetic, labeled weight diffs to train a DIT adapter, which can be applied to a compatible finetuned model to make it describe how it has changed. We demonstrate in two proof-of-concept settings (reporting hidden behaviors and summarizing finetuned knowledge) that our method enables models to describe their finetuning-induced modifications using accurate natural language descriptions.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Noisy Traces to Stable Gradients: Bias-Variance Optimized Preference Optimization for Aligning Large Reasoning Models</title>
<link>https://arxiv.org/abs/2510.05095</link>
<guid>https://arxiv.org/abs/2510.05095</guid>
<content:encoded><![CDATA[
arXiv:2510.05095v1 Announce Type: cross 
Abstract: Large reasoning models (LRMs) generate intermediate reasoning traces before producing final answers, yielding strong gains on multi-step and mathematical tasks. Yet aligning LRMs with human preferences, a crucial prerequisite for model deployment, remains underexplored. The statistically correct objective for preference alignment requires marginalizing over reasoning traces, but this computation is intractable in practice. A common workaround optimizes a single sampled trajectory, which introduces substantial gradient variance from stochastic trace sampling. To address this challenge, we frame preference optimization for LRMs through the lens of the bias--variance trade-off and propose Bias--Variance Optimized Preference Optimization (BVPO), a simple, drop-in method that mixes two gradient estimators: a high-variance trace-based estimator and a low-variance empty-trace estimator obtained by disabling reasoning trace generation. Our theory shows that BVPO strictly reduces trace-induced variance for any nontrivial mixture, provides a closed-form choice of the mixing weight that minimizes mean-squared error relative to the true marginal gradient, and under standard smoothness and step-size conditions, tightens classical convergence bounds for stochastic gradient descent. Empirically, BVPO improves alignment over the best baseline by up to 7.8 points on AlpacaEval~2 and 6.8 points on Arena-Hard. Despite being trained only on general conversational data, BVPO also boosts reasoning performance for base models by up to 4.0 points on the average of six math reasoning benchmarks. These results identify variance from trace sampling as a key bottleneck and demonstrate that directly optimizing the bias--variance trade-off yields more stable training and stronger overall performance.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Paper2Video: Automatic Video Generation from Scientific Papers</title>
<link>https://arxiv.org/abs/2510.05096</link>
<guid>https://arxiv.org/abs/2510.05096</guid>
<content:encoded><![CDATA[
arXiv:2510.05096v1 Announce Type: cross 
Abstract: Academic presentation videos have become an essential medium for research communication, yet producing them remains highly labor-intensive, often requiring hours of slide design, recording, and editing for a short 2 to 10 minutes video. Unlike natural video, presentation video generation involves distinctive challenges: inputs from research papers, dense multi-modal information (text, figures, tables), and the need to coordinate multiple aligned channels such as slides, subtitles, speech, and human talker. To address these challenges, we introduce PaperTalker, the first benchmark of 101 research papers paired with author-created presentation videos, slides, and speaker metadata. We further design four tailored evaluation metrics--Meta Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos convey the paper's information to the audience. Building on this foundation, we propose PaperTalker, the first multi-agent framework for academic presentation video generation. It integrates slide generation with effective layout refinement by a novel effective tree search visual choice, cursor grounding, subtitling, speech synthesis, and talking-head rendering, while parallelizing slide-wise generation for efficiency. Experiments on Paper2Video demonstrate that the presentation videos produced by our approach are more faithful and informative than existing baselines, establishing a practical step toward automated and ready-to-use academic video generation. Our dataset, agent, and code are available at https://github.com/showlab/Paper2Video.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Journeys: Quantifying Change in Emoji Meaning from 2012-2018</title>
<link>https://arxiv.org/abs/2105.00846</link>
<guid>https://arxiv.org/abs/2105.00846</guid>
<content:encoded><![CDATA[
arXiv:2105.00846v3 Announce Type: replace 
Abstract: The semantics of emoji has, to date, been considered from a static perspective. We offer the first longitudinal study of how emoji semantics changes over time, applying techniques from computational linguistics to six years of Twitter data. We identify five patterns in emoji semantic development and find evidence that the less abstract an emoji is, the more likely it is to undergo semantic change. In addition, we analyse select emoji in more detail, examining the effect of seasonality and world events on emoji semantics. To aid future work on emoji and semantics, we make our data publicly available along with a web-based interface that anyone can use to explore semantic change in emoji.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Retrieval Augmentation for Long-Form Question Answering</title>
<link>https://arxiv.org/abs/2310.12150</link>
<guid>https://arxiv.org/abs/2310.12150</guid>
<content:encoded><![CDATA[
arXiv:2310.12150v2 Announce Type: replace 
Abstract: How retrieved documents are used in language models (LMs) for long-form generation task is understudied. We present two controlled studies on retrieval-augmented LM for long-form question answering (LFQA): one fixing the LM and varying evidence documents and the other fixing evidence documents and varying the LMs. We study various attributes of generated answers (e.g., fluency, length, variance), with an emphasis on the attribution of generated answers to in-context evidence documents. We collect a dataset (SALAD) containing human annotations of sentence-level answer attribution in LFQA and evaluate existing methods for automatically judging attribution. We find that while LMs can leverage relevant in-context documents, the generated answer is only partially attributable towards the documents, especially for LMs trained without retrieval augmentation. Together, our analysis reveals how retrieval augmentation impacts long knowledge-rich text generation and provide directions for future work.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can GPT models Follow Human Summarization Guidelines? A Study for Targeted Communication Goals</title>
<link>https://arxiv.org/abs/2310.16810</link>
<guid>https://arxiv.org/abs/2310.16810</guid>
<content:encoded><![CDATA[
arXiv:2310.16810v3 Announce Type: replace 
Abstract: This study investigates the ability of GPT models (ChatGPT, GPT-4 and GPT-4o) to generate dialogue summaries that adhere to human guidelines. Our evaluation involved experimenting with various prompts to guide the models in complying with guidelines on two datasets: DialogSum (English social conversations) and DECODA (French call center interactions). Human evaluation, based on summarization guidelines, served as the primary assessment method, complemented by extensive quantitative and qualitative analyses. Our findings reveal a preference for GPT-generated summaries over those from task-specific pre-trained models and reference summaries, highlighting GPT models' ability to follow human guidelines despite occasionally producing longer outputs and exhibiting divergent lexical and structural alignment with references. The discrepancy between ROUGE, BERTScore, and human evaluation underscores the need for more reliable automatic evaluation metrics.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When "Competency" in Reasoning Opens the Door to Vulnerability: Jailbreaking LLMs via Novel Complex Ciphers</title>
<link>https://arxiv.org/abs/2402.10601</link>
<guid>https://arxiv.org/abs/2402.10601</guid>
<content:encoded><![CDATA[
arXiv:2402.10601v4 Announce Type: replace 
Abstract: Recent advancements in Large Language Model (LLM) safety have primarily focused on mitigating attacks crafted in natural language or common ciphers (e.g. Base64), which are likely integrated into newer models' safety training. However, we reveal a paradoxical vulnerability: as LLMs advance in reasoning, they inadvertently become more susceptible to novel jailbreaking attacks. Enhanced reasoning enables LLMs to interpret complex instructions and decode complex user-defined ciphers, creating an exploitable security gap. To study this vulnerability, we introduce Attacks using Custom Encryptions (ACE), a jailbreaking technique that encodes malicious queries with novel ciphers. Extending ACE, we introduce Layered Attacks using Custom Encryptions (LACE), which applies multi-layer ciphers to amplify attack complexity. Furthermore, we develop CipherBench, a benchmark designed to evaluate LLMs' accuracy in decoding encrypted benign text. Our experiments reveal a critical trade-off: LLMs that are more capable of decoding ciphers are more vulnerable to LACE, with success rates on gpt-oss-20b escalating from 60% under ACE to 72% with LACE. These findings highlight a critical insight: as LLMs become more adept at deciphering complex user ciphers--many of which cannot be preemptively included in safety training--they become increasingly exploitable.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rowen: Adaptive Retrieval-Augmented Generation for Hallucination Mitigation in LLMs</title>
<link>https://arxiv.org/abs/2402.10612</link>
<guid>https://arxiv.org/abs/2402.10612</guid>
<content:encoded><![CDATA[
arXiv:2402.10612v3 Announce Type: replace 
Abstract: Hallucinations present a significant challenge for large language models (LLMs). The utilization of parametric knowledge in generating factual content is constrained by the limited knowledge of LLMs, potentially resulting in internal hallucinations. While incorporating external information can help fill knowledge gaps, it also introduces the risk of irrelevant information, thereby increasing the likelihood of external hallucinations. To balance the use of parametric knowledge within LLMs and external information, in this study, we present Rowen, a novel framework that enhances LLMs with an adaptive retrieval augmentation process tailored to address hallucinated outputs. Rowen introduces a consistency-based hallucination detection module, which assesses the model's uncertainty regarding the input query by evaluating the semantic inconsistencies in various responses generated across different languages or models. When high uncertainties in the responses are detected, Rowen activates the retrieval of external information to rectify the model outputs. Through comprehensive empirical experiments, we demonstrate that Rowen surpasses the current state-of-the-art in both detecting and mitigating hallucinated content within the outputs of LLMs.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RealKIE: Five Novel Datasets for Enterprise Key Information Extraction</title>
<link>https://arxiv.org/abs/2403.20101</link>
<guid>https://arxiv.org/abs/2403.20101</guid>
<content:encoded><![CDATA[
arXiv:2403.20101v2 Announce Type: replace 
Abstract: We introduce RealKIE, a benchmark of five challenging datasets aimed at advancing key information extraction methods, with an emphasis on enterprise applications. The datasets include a diverse range of documents including SEC S1 Filings, US Non-disclosure Agreements, UK Charity Reports, FCC Invoices, and Resource Contracts. Each presents unique challenges: poor text serialization, sparse annotations in long documents, and complex tabular layouts. These datasets provide a realistic testing ground for key information extraction tasks like investment analysis and contract analysis. In addition to presenting these datasets, we offer an in-depth description of the annotation process, document processing techniques, and baseline modeling approaches. This contribution facilitates the development of NLP models capable of handling practical challenges and supports further research into information extraction technologies applicable to industry-specific problems. The annotated data, OCR outputs, and code to reproduce baselines are available to download at https://indicodatasolutions.github.io/RealKIE/.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ClinicRealm: Re-evaluating Large Language Models with Conventional Machine Learning for Non-Generative Clinical Prediction Tasks</title>
<link>https://arxiv.org/abs/2407.18525</link>
<guid>https://arxiv.org/abs/2407.18525</guid>
<content:encoded><![CDATA[
arXiv:2407.18525v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly deployed in medicine. However, their utility in non-generative clinical prediction, often presumed inferior to specialized models, remains under-evaluated, leading to ongoing debate within the field and potential for misuse, misunderstanding, or over-reliance due to a lack of systematic benchmarking. Our ClinicRealm study addresses this by benchmarking 15 GPT-style LLMs, 5 BERT-style models, and 11 traditional methods on unstructured clinical notes and structured Electronic Health Records (EHR), while also assessing their reasoning, reliability, and fairness. Key findings reveal a significant shift: for clinical note predictions, leading LLMs (e.g., DeepSeek-V3.1-Think, GPT-5) in zero-shot settings now decisively outperform finetuned BERT models. On structured EHRs, while specialized models excel with ample data, advanced LLMs (e.g., GPT-5, DeepSeek-V3.1-Think) show potent zero-shot capabilities, often surpassing conventional models in data-scarce settings. Notably, leading open-source LLMs can match or exceed proprietary counterparts. These results provide compelling evidence that modern LLMs are competitive tools for non-generative clinical prediction, particularly with unstructured text and offering data-efficient structured data options, thus necessitating a re-evaluation of model selection strategies. This research should serve as an important insight for medical informaticists, AI developers, and clinical researchers, potentially prompting a reassessment of current assumptions and inspiring new approaches to LLM application in predictive healthcare.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Unsupervised Sentence Embeddings via Knowledge-Driven Data Augmentation and Gaussian-Decayed Contrastive Learning</title>
<link>https://arxiv.org/abs/2409.12887</link>
<guid>https://arxiv.org/abs/2409.12887</guid>
<content:encoded><![CDATA[
arXiv:2409.12887v5 Announce Type: replace 
Abstract: Recently, using large language models (LLMs) for data augmentation has led to considerable improvements in unsupervised sentence embedding models. However, existing methods encounter two primary challenges: limited data diversity and high data noise. Current approaches often neglect fine-grained knowledge, such as entities and quantities, leading to insufficient diversity. Besides, unsupervised data frequently lacks discriminative information, and the generated synthetic samples may introduce noise. In this paper, we propose a pipeline-based data augmentation method via LLMs and introduce the Gaussian-decayed gradient-assisted Contrastive Sentence Embedding (GCSE) model to enhance unsupervised sentence embeddings. To tackle the issue of low data diversity, our pipeline utilizes knowledge graphs (KGs) to extract entities and quantities, enabling LLMs to generate more diverse samples. To address high data noise, the GCSE model uses a Gaussian-decayed function to limit the impact of false hard negative samples, enhancing the model's discriminative capability. Experimental results show that our approach achieves state-of-the-art performance in semantic textual similarity (STS) tasks, using fewer data samples and smaller LLMs, demonstrating its efficiency and robustness across various models.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Insights from the Inverse: Reconstructing LLM Training Goals Through Inverse Reinforcement Learning</title>
<link>https://arxiv.org/abs/2410.12491</link>
<guid>https://arxiv.org/abs/2410.12491</guid>
<content:encoded><![CDATA[
arXiv:2410.12491v3 Announce Type: replace 
Abstract: Large language models (LLMs) trained with Reinforcement Learning from Human Feedback (RLHF) have demonstrated remarkable capabilities, but their underlying reward functions and decision-making processes remain opaque. This paper introduces a novel approach to interpreting LLMs by applying inverse reinforcement learning (IRL) to recover their implicit reward functions. We conduct experiments on toxicity-aligned LLMs of varying sizes, extracting reward models that achieve up to 85% accuracy in predicting human preferences. Our analysis reveals key insights into the non-identifiability of reward functions, the relationship between model size and interpretability, and potential pitfalls in the RLHF process. We demonstrate that IRL-derived reward models can be used to fine-tune new LLMs, resulting in comparable or improved performance on toxicity benchmarks. This work provides a new lens for understanding and improving LLM alignment, with implications for the responsible development and deployment of these powerful systems.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Pay Attention, PLANT It: Pretraining Attention via Learning-to-Rank</title>
<link>https://arxiv.org/abs/2410.23066</link>
<guid>https://arxiv.org/abs/2410.23066</guid>
<content:encoded><![CDATA[
arXiv:2410.23066v2 Announce Type: replace 
Abstract: State-of-the-art Extreme Multi-Label Text Classification models rely on multi-label attention to focus on key tokens in input text, but learning good attention weights is challenging. We introduce PLANT - Pretrained and Leveraged Attention - a plug-and-play strategy for initializing attention. PLANT works by planting label-specific attention using a pretrained Learning-to-Rank model guided by mutual information gain. This architecture-agnostic approach integrates seamlessly with large language model backbones such as Mistral-7B, LLaMA3-8B, DeepSeek-V3, and Phi-3. PLANT outperforms state-of-the-art methods across tasks including ICD coding, legal topic classification, and content recommendation. Gains are especially pronounced in few-shot settings, with substantial improvements on rare labels. Ablation studies confirm that attention initialization is a key driver of these gains. For code and trained models, see https://github.com/debjyotiSRoy/xcube/tree/plant
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Arena-Lite: Efficient and Reliable Large Language Model Evaluation via Tournament-Based Direct Comparisons</title>
<link>https://arxiv.org/abs/2411.01281</link>
<guid>https://arxiv.org/abs/2411.01281</guid>
<content:encoded><![CDATA[
arXiv:2411.01281v5 Announce Type: replace 
Abstract: As Large Language Models (LLMs) expand across domains, LLM judges have become essential for systems evaluation. Current benchmarks typically compare system outputs against baselines. This baseline-mediated approach, though convenient, yields lower reliability than direct comparison between systems. We propose Arena-Lite which integrates tournament structure on top of head-to-head comparison. The application of a tournament structure and direct comparison eliminates the need for baseline outputs, reduces the number of required comparisons, and allows higher reliability in system rankings. We conducted two experiments: (1) controlled stochastic modeling and (2) empirical validation with a real LLM judge. Those experiments collectively demonstrate that Arena-Lite consistently achieves higher reliability with fewer comparisons, even with smaller datasets or weaker judges. We release an easy-to-use web demonstration and code to foster adoption of Arena-Lite, streamlining model selection across research and industry communities. Arena-Lite demo and code are available on \href{https://huggingface.co/spaces/NCSOFT/ArenaLite}{https://huggingface.co/spaces/NCSOFT/ArenaLite}
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometry of orofacial neuromuscular signals: speech articulation decoding using surface electromyography</title>
<link>https://arxiv.org/abs/2411.02591</link>
<guid>https://arxiv.org/abs/2411.02591</guid>
<content:encoded><![CDATA[
arXiv:2411.02591v3 Announce Type: replace 
Abstract: Objective. In this article, we present data and methods for decoding speech articulations using surface electromyogram (EMG) signals. EMG-based speech neuroprostheses offer a promising approach for restoring audible speech in individuals who have lost the ability to speak intelligibly due to laryngectomy, neuromuscular diseases, stroke, or trauma-induced damage (e.g., from radiotherapy) to the speech articulators.
  Approach. To achieve this, we collect EMG signals from the face, jaw, and neck as subjects articulate speech, and we perform EMG-to-speech translation.
  Main results. Our findings reveal that the manifold of symmetric positive definite (SPD) matrices serves as a natural embedding space for EMG signals. Specifically, we provide an algebraic interpretation of the manifold-valued EMG data using linear transformations, and we analyze and quantify distribution shifts in EMG signals across individuals.
  Significance. Overall, our approach demonstrates significant potential for developing neural networks that are both data- and parameter-efficient, an important consideration for EMG-based systems, which face challenges in large-scale data collection and operate under limited computational resources on embedded devices.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>H3Fusion: Helpful, Harmless, Honest Fusion of Aligned LLMs</title>
<link>https://arxiv.org/abs/2411.17792</link>
<guid>https://arxiv.org/abs/2411.17792</guid>
<content:encoded><![CDATA[
arXiv:2411.17792v2 Announce Type: replace 
Abstract: Alignment of pretrained LLMs using instruction-based datasets is critical for creating fine-tuned models that reflect human preference. A growing number of alignment-based fine-tuning algorithms and benchmarks emerged recently, fueling the efforts on effective alignments of pre-trained LLMs to ensure helpful, harmless, and honest answers from both open-source and closed-source LLMs. This paper tackles this problem by developing an alignment fusion approach, coined as $H^3$Fusion, with three unique characteristics. First, $H^3$Fusion ensembles multiple individually aligned LLMs to create a final fine-tuned alignment model with enhanced capabilities beyond those of individual models, delivering robust alignment through promoting helpful, harmless, honest fusion. Second, $H^3$Fusion leverages the mixture-of-experts (MoE) methodology in two steps. We first freeze the multi-head attention weights of each individual model while tuning the FFN layer during alignment fusion. Then we merge the aligned model weights with an expert router according to the type of input instruction and dynamically select a subset of experts that are best suited for producing the output response. Finally, we boost the performance of the resulting $H^3$3Fusion model by introducing gating loss and regularization terms. The former penalizes the selection errors of the expert-router, and the latter mediates the expert weights drifting during fine-tuning and dynamically adjusts the fusion behavior of the resulting model by canalizing the activations on the experts. Extensive evaluations on three benchmark datasets show that $H^3$3Fusion is more helpful, less harmful, and more honest from two aspects: it outperforms each individually aligned model by $11.37\%$, and it provides stronger robustness compared to the state-of-the-art LLM ensemble approaches by $13.77\%$. Code is available at github.com/sftekin/h3fusion.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HP-BERT: A framework for longitudinal study of Hinduphobia on social media via language models</title>
<link>https://arxiv.org/abs/2501.05482</link>
<guid>https://arxiv.org/abs/2501.05482</guid>
<content:encoded><![CDATA[
arXiv:2501.05482v2 Announce Type: replace 
Abstract: During the COVID-19 pandemic, community tensions intensified, contributing to discriminatory sentiments against various religious groups, including Hindu communities. Recent advances in language models have shown promise for social media analysis with potential for longitudinal studies of social media platforms, such as X (Twitter). We present a computational framework for analyzing anti-Hindu sentiment (Hinduphobia) during the COVID-19 period, introducing an abuse detection and sentiment analysis approach for longitudinal analysis on X. We curate and release a "Hinduphobic COVID-19 XDataset" containing 8,000 annotated and manually verified tweets. We then develop the Hinduphobic BERT (HP-BERT) model using this dataset and achieve 94.72\% accuracy, outperforming baseline Transformer-based language models. The model incorporates multi-label sentiment analysis capabilities through additional fine-tuning. Our analysis encompasses approximately 27.4 million tweets from six countries, including Australia, Brazil, India, Indonesia, Japan, and the United Kingdom. Statistical analysis reveals moderate correlations (r = 0.312-0.428) between COVID-19 case increases and Hinduphobic content volume, highlighting how pandemic-related stress may contribute to discriminatory discourse. This study provides evidence of social media-based religious discrimination during a COVID-19 crisis.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlocking In-Context Learning for Natural Datasets Beyond Language Modelling</title>
<link>https://arxiv.org/abs/2501.06256</link>
<guid>https://arxiv.org/abs/2501.06256</guid>
<content:encoded><![CDATA[
arXiv:2501.06256v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) exhibit In-Context Learning (ICL), which enables the model to perform new tasks conditioning only on the examples provided in the context without updating the model's weights. While ICL offers fast adaptation across natural language tasks and domains, its emergence is less straightforward for modalities beyond text. In this work, we systematically uncover properties present in LLMs that support the emergence of ICL for autoregressive models and various modalities by promoting the learning of the needed mechanisms for ICL. We identify exact token repetitions in the training data sequences as an important factor for ICL. Such repetitions further improve stability and reduce transiency in ICL performance. Moreover, we emphasise the significance of training task difficulty for the emergence of ICL. Finally, by applying our novel insights on ICL emergence, we unlock ICL capabilities for various visual datasets and a more challenging EEG classification task.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Longitudinal Abuse and Sentiment Analysis of Hollywood Movie Dialogues using Language Models</title>
<link>https://arxiv.org/abs/2501.13948</link>
<guid>https://arxiv.org/abs/2501.13948</guid>
<content:encoded><![CDATA[
arXiv:2501.13948v3 Announce Type: replace 
Abstract: Over the past decades, there has been an increase in the prevalence of abusive and violent content in Hollywood movies. In this study, we use language models to explore the longitudinal abuse and sentiment analysis of Hollywood Oscar and blockbuster movie dialogues from 1950 to 2024. We provide an analysis of subtitles for over a thousand movies, which are categorised into four genres. We employ fine-tuned language models to examine the trends and shifts in emotional and abusive content over the past seven decades. Findings reveal significant temporal changes in movie dialogues, which reflect broader social and cultural influences. Overall, the emotional tendencies in the films are diverse, and the detection of abusive content also exhibits significant fluctuations. The results show a gradual rise in abusive content in recent decades, reflecting social norms and regulatory policy changes. Genres such as thrillers still present a higher frequency of abusive content that emphasises the ongoing narrative role of violence and conflict. At the same time, underlying positive emotions such as humour and optimism remain prevalent in most of the movies. Furthermore, the gradual increase of abusive content in movie dialogues has been significant over the last two decades, where Oscar-nominated movies overtook the top ten blockbusters.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Low-Resource Sequence Labeling with Knowledge Fusion and Contextual Label Explanations</title>
<link>https://arxiv.org/abs/2501.19093</link>
<guid>https://arxiv.org/abs/2501.19093</guid>
<content:encoded><![CDATA[
arXiv:2501.19093v4 Announce Type: replace 
Abstract: Sequence labeling remains a significant challenge in low-resource, domain-specific scenarios, particularly for character-dense languages like Chinese. Existing methods primarily focus on enhancing model comprehension and improving data diversity to boost performance. However, these approaches still struggle with inadequate model applicability and semantic distribution biases in domain-specific contexts. To overcome these limitations, we propose a novel framework that combines an LLM-based knowledge enhancement workflow with a span-based Knowledge Fusion for Rich and Efficient Extraction (KnowFREE) model. Our workflow employs explanation prompts to generate precise contextual interpretations of target entities, effectively mitigating semantic biases and enriching the model's contextual understanding. The KnowFREE model further integrates extension label features, enabling efficient nested entity extraction without relying on external knowledge during inference. Experiments on multiple Chinese domain-specific sequence labeling datasets demonstrate that our approach achieves state-of-the-art performance, effectively addressing the challenges posed by low-resource settings.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Summaries as Centroids for Interpretable and Scalable Text Clustering</title>
<link>https://arxiv.org/abs/2502.09667</link>
<guid>https://arxiv.org/abs/2502.09667</guid>
<content:encoded><![CDATA[
arXiv:2502.09667v3 Announce Type: replace 
Abstract: We introduce k-NLPmeans and k-LLMmeans, text-clustering variants of k-means that periodically replace numeric centroids with textual summaries. The key idea, summary-as-centroid, retains k-means assignments in embedding space while producing human-readable, auditable cluster prototypes. The method is LLM-optional: k-NLPmeans uses lightweight, deterministic summarizers, enabling offline, low-cost, and stable operation; k-LLMmeans is a drop-in upgrade that uses an LLM for summaries under a fixed per-iteration budget whose cost does not grow with dataset size. We also present a mini-batch extension for real-time clustering of streaming text. Across diverse datasets, embedding models, and summarization strategies, our approach consistently outperforms classical baselines and approaches the accuracy of recent LLM-based clustering-without extensive LLM calls. Finally, we provide a case study on sequential text streams and release a StackExchange-derived benchmark for evaluating streaming text clustering.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffSampling: Enhancing Diversity and Accuracy in Neural Text Generation</title>
<link>https://arxiv.org/abs/2502.14037</link>
<guid>https://arxiv.org/abs/2502.14037</guid>
<content:encoded><![CDATA[
arXiv:2502.14037v4 Announce Type: replace 
Abstract: Despite their growing capabilities, language models still frequently reproduce content from their training data, generate repetitive text, and favor common grammatical patterns and vocabulary. A possible cause is the decoding strategy: the most common strategies either consider only the most probable tokens, which reduces output diversity, or increase the likelihood of unlikely tokens, compromising output accuracy and correctness. In this paper, we propose DiffSampling, a new decoding method that leverages a mathematical analysis of the token probability distribution to ensure the generation of contextually appropriate text. In particular, the difference between consecutive, sorted probabilities can be used to truncate incorrect tokens. In addition, we also propose two variations of the proposed method that aim to correct the subtle inconsistencies of common sampling strategies. Experiments involving four different text-generation tasks demonstrate that our approach consistently performs at least on par with the existing methods it builds upon in terms of quality, while potentially improving output diversity.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in Multilingual LLMs</title>
<link>https://arxiv.org/abs/2502.16901</link>
<guid>https://arxiv.org/abs/2502.16901</guid>
<content:encoded><![CDATA[
arXiv:2502.16901v3 Announce Type: replace 
Abstract: We explore \textbf{C}ross-lingual \textbf{B}ackdoor \textbf{AT}tacks (X-BAT) in multilingual Large Language Models (mLLMs), revealing how backdoors inserted in one language can automatically transfer to others through shared embedding spaces. Using toxicity classification as a case study, we demonstrate that attackers can compromise multilingual systems by poisoning data in a single language, with rare and high-occurring tokens serving as specific, effective triggers. Our findings expose a critical vulnerability that influences the model's architecture, resulting in a concealed backdoor effect during the information flow. Our code and data are publicly available https://github.com/himanshubeniwal/X-BAT.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>League: Leaderboard Generation on Demand</title>
<link>https://arxiv.org/abs/2502.18209</link>
<guid>https://arxiv.org/abs/2502.18209</guid>
<content:encoded><![CDATA[
arXiv:2502.18209v2 Announce Type: replace 
Abstract: This paper introduces Leaderboard Auto Generation (LAG), a novel and well-organized framework for automatic generation of leaderboards on a given research topic in rapidly evolving fields like Artificial Intelligence (AI). Faced with a large number of AI papers updated daily, it becomes difficult for researchers to track every paper's proposed methods, experimental results, and settings, prompting the need for efficient automatic leaderboard construction. While large language models (LLMs) offer promise in automating this process, challenges such as multi-document summarization, leaderboard generation, and experiment fair comparison still remain under exploration. LAG solves these challenges through a systematic approach that involves the paper collection, experiment results extraction and integration, leaderboard generation, and quality evaluation. Our contributions include a comprehensive solution to the leaderboard construction problem, a reliable evaluation method, and experimental results showing the high quality of leaderboards.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Pruning State-Space LLMs</title>
<link>https://arxiv.org/abs/2502.18886</link>
<guid>https://arxiv.org/abs/2502.18886</guid>
<content:encoded><![CDATA[
arXiv:2502.18886v2 Announce Type: replace 
Abstract: Recent work proposed state-space models (SSMs) as an efficient alternative to transformer-based LLMs. Can these models be pruned to further reduce their computation costs? We adapt several pruning methods to the SSM structure, and apply them to four SSM-based LLMs across multiple tasks. We find that such models are quite robust to some pruning methods (e.g. WANDA), while using other methods lead to fast performance degradation.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Large Language Models Outperform Non-Experts in Poetry Evaluation? A Comparative Study Using the Consensual Assessment Technique</title>
<link>https://arxiv.org/abs/2502.19064</link>
<guid>https://arxiv.org/abs/2502.19064</guid>
<content:encoded><![CDATA[
arXiv:2502.19064v2 Announce Type: replace 
Abstract: This study adapts the Consensual Assessment Technique (CAT) for Large Language Models (LLMs), introducing a novel methodology for poetry evaluation. Using a 90-poem dataset with a ground truth based on publication venue, we demonstrate that this approach allows LLMs to significantly surpass the performance of non-expert human judges. Our method, which leverages forced-choice ranking within small, randomized batches, enabled Claude-3-Opus to achieve a Spearman's Rank Correlation of 0.87 with the ground truth, dramatically outperforming the best human non-expert evaluation (SRC = 0.38). The LLM assessments also exhibited high inter-rater reliability, underscoring the methodology's robustness. These findings establish that LLMs, when guided by a comparative framework, can be effective and reliable tools for assessing poetry, paving the way for their broader application in other creative domains.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SemViQA: A Semantic Question Answering System for Vietnamese Information Fact-Checking</title>
<link>https://arxiv.org/abs/2503.00955</link>
<guid>https://arxiv.org/abs/2503.00955</guid>
<content:encoded><![CDATA[
arXiv:2503.00955v3 Announce Type: replace 
Abstract: The rise of misinformation, exacerbated by Large Language Models (LLMs) like GPT and Gemini, demands robust fact-checking solutions, especially for low-resource languages like Vietnamese. Existing methods struggle with semantic ambiguity, homonyms, and complex linguistic structures, often trading accuracy for efficiency. We introduce SemViQA, a novel Vietnamese fact-checking framework integrating Semantic-based Evidence Retrieval (SER) and Two-step Verdict Classification (TVC). Our approach balances precision and speed, achieving state-of-the-art results with 78.97\% strict accuracy on ISE-DSC01 and 80.82\% on ViWikiFC, securing 1st place in the UIT Data Science Challenge. Additionally, SemViQA Faster improves inference speed 7x while maintaining competitive accuracy. SemViQA sets a new benchmark for Vietnamese fact verification, advancing the fight against misinformation. The source code is available at: https://github.com/DAVID-NGUYEN-S16/SemViQA.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolutionary Guided Decoding: Iterative Value Refinement for LLMs</title>
<link>https://arxiv.org/abs/2503.02368</link>
<guid>https://arxiv.org/abs/2503.02368</guid>
<content:encoded><![CDATA[
arXiv:2503.02368v3 Announce Type: replace 
Abstract: While guided decoding, especially value-guided methods, has emerged as a cost-effective alternative for controlling language model outputs without re-training models, its effectiveness is limited by the accuracy of the value function. We identify that this inaccuracy stems from a core distributional gap: existing methods train static value functions on trajectories sampled exclusively from the base policy, which inherently confines their training to a narrow and suboptimal view of the potential output space. We propose Iterative Value Refinement, a novel framework designed to bridge this gap. It employs Value Exploration to provide a more comprehensive and robust training signal, complemented by Iterative Self-Refinement, which uses the improved value function from one iteration to guide the generation of higher-quality data for the next. Extensive experiments on text summarization, multi-turn dialogue, and instruction following demonstrate the effectiveness of our framework in aligning language models. Our approach not only achieves alignment but also significantly reduces computational costs by leveraging principled value function optimization for efficient and effective control.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient Long-Context LLMs</title>
<link>https://arxiv.org/abs/2503.10714</link>
<guid>https://arxiv.org/abs/2503.10714</guid>
<content:encoded><![CDATA[
arXiv:2503.10714v3 Announce Type: replace 
Abstract: The linear growth of key-value (KV) cache memory and quadratic computational in attention mechanisms complexity pose significant bottlenecks for large language models (LLMs) in long-context processing. While existing KV cache optimization methods address these challenges through token pruning or feature merging, they often incur irreversible information loss or require costly parameter retraining. To this end, we propose ZSMerge, a dynamic KV cache compression framework designed for efficient cache management, featuring three key operations: (1) fine-grained memory allocation guided by multi-dimensional token importance metrics at head-level granularity, (2) a residual merging mechanism that preserves critical context through compensated attention scoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM architectures without requiring retraining. ZSMerge significantly enhances memory efficiency and inference speed with negligible performance degradation across LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression ratio for key-value cache retention (reducing memory footprint to 5\% of baseline) while sustaining comparable generation quality, coupled with triple throughput gains at extreme 54k-token contexts that eliminate out-of-memory failures. The code is available at https://github.com/SusCom-Lab/ZSMerge.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Praxis-VLM: Vision-Grounded Decision Making via Text-Driven Reinforcement Learning</title>
<link>https://arxiv.org/abs/2503.16965</link>
<guid>https://arxiv.org/abs/2503.16965</guid>
<content:encoded><![CDATA[
arXiv:2503.16965v3 Announce Type: replace 
Abstract: Vision Language Models exhibit impressive performance for various tasks, yet they often lack the sophisticated situational reasoning required for complex decision-making. This paper shows that VLMs can achieve surprisingly strong decision-making performance when visual scenes are replaced by textual descriptions, suggesting foundational reasoning can be effectively learned from language. Motivated by this insight, we propose Praxis-VLM, a reasoning VLM for vision-grounded decision-making. Praxis-VLM employs the GRPO algorithm on textual scenarios to instill robust reasoning capabilities, where models learn to evaluate actions and their consequences. These reasoning skills, acquired purely from text, successfully transfer to multimodal inference with visual inputs, significantly reducing reliance on scarce paired image-text training data. Experiments across diverse decision-making benchmarks demonstrate that Praxis-VLM substantially outperforms standard supervised fine-tuning, exhibiting superior performance and generalizability. Further analysis confirms that our models engage in explicit and effective reasoning, underpinning their enhanced performance and adaptability.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Laws of Synthetic Data for Language Models</title>
<link>https://arxiv.org/abs/2503.19551</link>
<guid>https://arxiv.org/abs/2503.19551</guid>
<content:encoded><![CDATA[
arXiv:2503.19551v3 Announce Type: replace 
Abstract: Large language models (LLMs) achieve strong performance across diverse tasks, largely driven by high-quality web data used in pre-training. However, recent studies indicate this data source is rapidly depleting. Synthetic data emerges as a promising alternative, but it remains unclear whether synthetic datasets exhibit predictable scalability comparable to raw pre-training data. In this work, we systematically investigate the scaling laws of synthetic data by introducing SynthLLM, a scalable framework that transforms pre-training corpora into diverse, high-quality synthetic datasets. Our approach achieves this by automatically extracting and recombining high-level concepts across multiple documents using a graph algorithm. Key findings from our extensive mathematical experiments on SynthLLM include: (1) SynthLLM generates synthetic data that reliably adheres to the rectified scaling law across various model sizes; (2) Performance improvements plateau near 300B tokens; and (3) Larger models approach optimal performance with fewer training tokens. For instance, an 8B model peaks at 1T tokens, while a 3B model requires 4T. Moreover, comparisons with existing synthetic data generation and augmentation methods demonstrate that SynthLLM achieves superior performance and scalability. Our findings highlight synthetic data as a scalable and reliable alternative to organic pre-training corpora, offering a viable path toward continued improvement in model performance.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAG over Tables: Hierarchical Memory Index, Multi-Stage Retrieval, and Benchmarking</title>
<link>https://arxiv.org/abs/2504.01346</link>
<guid>https://arxiv.org/abs/2504.01346</guid>
<content:encoded><![CDATA[
arXiv:2504.01346v4 Announce Type: replace 
Abstract: Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by integrating them with an external knowledge base to improve the answer relevance and accuracy. In real-world scenarios, beyond pure text, a substantial amount of knowledge is stored in tables, and user questions often require retrieving answers that are distributed across multiple tables. Retrieving knowledge from a table corpora (i.e., various individual tables) for a question remains nascent, at least, for (i) how to understand intra- and inter-table knowledge effectively, (ii) how to filter unnecessary tables and how to retrieve the most relevant tables efficiently, (iii) how to prompt LLMs to infer over the retrieval, (iv) how to evaluate the corresponding performance in a realistic setting. Facing the above challenges, in this paper, we first propose a table-corpora-aware RAG framework, named T-RAG, which consists of the hierarchical memory index, multi-stage retrieval, and graph-aware prompting for effective and efficient table knowledge retrieval and inference. Further, we first develop a multi-table question answering benchmark named MultiTableQA, which spans 3 different task types, 57,193 tables, and 23,758 questions in total, and the sources are all from real-world scenarios. Based on MultiTableQA, we did the holistic comparison over table retrieval methods, RAG methods, and table-to-graph representation learning methods, where T-RAG shows the leading accuracy, recall, and running time performance. Also, under T-RAG, we evaluate the inference ability upgrade of different LLMs. Code and Data are available at https://github.com/jiaruzouu/T-RAG
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Testing Low-Resource Language Support in LLMs Using Language Proficiency Exams: the Case of Luxembourgish</title>
<link>https://arxiv.org/abs/2504.01667</link>
<guid>https://arxiv.org/abs/2504.01667</guid>
<content:encoded><![CDATA[
arXiv:2504.01667v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have become an increasingly important tool in research and society at large. While LLMs are regularly used all over the world by experts and lay-people alike, they are predominantly developed with English-speaking users in mind, performing well in English and other wide-spread languages while less-resourced languages such as Luxembourgish are seen as a lower priority. This lack of attention is also reflected in the sparsity of available evaluation tools and datasets. In this study, we investigate the viability of language proficiency exams as such evaluation tools for the Luxembourgish language. We find that large models such as Claude and DeepSeek-R1 typically achieve high scores, while smaller models show weak performances. We also find that the performances in such language exams can be used to predict performances in other NLP tasks in Luxembourgish.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual Retrieval-Augmented Generation for Knowledge-Intensive Task</title>
<link>https://arxiv.org/abs/2504.03616</link>
<guid>https://arxiv.org/abs/2504.03616</guid>
<content:encoded><![CDATA[
arXiv:2504.03616v2 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) has become a cornerstone of contemporary NLP, enhancing large language models (LLMs) by allowing them to access richer factual contexts through in-context retrieval. While effective in monolingual settings, especially in English, its use in multilingual tasks remains unexplored. This paper investigates the effectiveness of RAG across multiple languages by proposing novel approaches for multilingual open-domain question-answering. We evaluate the performance of various multilingual RAG strategies, including question-translation (tRAG), which translates questions into English before retrieval, and Multilingual RAG (MultiRAG), where retrieval occurs directly across multiple languages. Our findings reveal that tRAG, while useful, suffers from limited coverage. In contrast, MultiRAG improves efficiency by enabling multilingual retrieval but introduces inconsistencies due to cross-lingual variations in the retrieved content. To address these issues, we propose Crosslingual RAG (CrossRAG), a method that translates retrieved documents into a common language (e.g., English) before generating the response. Our experiments show that CrossRAG significantly enhances performance on knowledge-intensive tasks, benefiting both high-resource and low-resource languages.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentAda: Skill-Adaptive Data Analytics for Tailored Insight Discovery</title>
<link>https://arxiv.org/abs/2504.07421</link>
<guid>https://arxiv.org/abs/2504.07421</guid>
<content:encoded><![CDATA[
arXiv:2504.07421v2 Announce Type: replace 
Abstract: We introduce AgentAda, the first LLM-powered analytics agent that can learn and use new analytics skills to extract more specialized insights. Unlike existing methods that require users to manually decide which data analytics method to apply, AgentAda automatically identifies the skill needed from a library of analytical skills to perform the analysis. This also allows AgentAda to use skills that existing LLMs cannot perform out of the box. The library covers a range of methods, including clustering, predictive modeling, and NLP techniques like BERT, which allow AgentAda to handle complex analytics tasks based on what the user needs. AgentAda's dataset-to-insight extraction strategy consists of three key steps: (I) a question generator to generate queries relevant to the user's goal and persona, (II) a hybrid Retrieval-Augmented Generation (RAG)-based skill matcher to choose the best data analytics skill from the skill library, and (III) a code generator that produces executable code based on the retrieved skill's documentation to extract key patterns. We also introduce KaggleBench, a benchmark of curated notebooks across diverse domains, to evaluate AgentAda's performance. We conducted a human evaluation demonstrating that AgentAda provides more insightful analytics than existing tools, with 48.78% of evaluators preferring its analyses, compared to 27.67% for the unskilled agent. We also propose a novel LLM-as-a-judge approach that we show is aligned with human evaluation as a way to automate insight quality evaluation at larger scale.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forecasting Conversation Derailments Through Generation</title>
<link>https://arxiv.org/abs/2504.08905</link>
<guid>https://arxiv.org/abs/2504.08905</guid>
<content:encoded><![CDATA[
arXiv:2504.08905v2 Announce Type: replace 
Abstract: Forecasting conversation derailment can be useful in real-world settings such as online content moderation, conflict resolution, and business negotiations. However, despite language models' success at identifying offensive speech present in conversations, they struggle to forecast future conversation derailments. In contrast to prior work that predicts conversation outcomes solely based on the past conversation history, our approach samples multiple future conversation trajectories conditioned on existing conversation history using a fine-tuned LLM. It predicts the conversation outcome based on the consensus of these trajectories. We also experimented with leveraging socio-linguistic attributes, which reflect turn-level conversation dynamics, as guidance when generating future conversations. Our method of future conversation trajectories surpasses state-of-the-art results on English conversation derailment prediction benchmarks and demonstrates significant accuracy gains in ablation studies.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deliberate Planning in Language Models with Symbolic Representation</title>
<link>https://arxiv.org/abs/2505.01479</link>
<guid>https://arxiv.org/abs/2505.01479</guid>
<content:encoded><![CDATA[
arXiv:2505.01479v3 Announce Type: replace 
Abstract: Planning remains a core challenge for large language models (LLMs), particularly in domains that require coherent multi-step action sequences grounded in external constraints. We introduce SymPlanner, a novel framework that equips LLMs with structured planning capabilities by interfacing them with a symbolic environment that serves as an explicit world model. Rather than relying purely on natural language reasoning, SymPlanner grounds the planning process in a symbolic state space, where a policy model proposes actions and a symbolic environment deterministically executes and verifies their effects. To enhance exploration and improve robustness, we introduce Iterative Correction (IC), which refines previously proposed actions by leveraging feedback from the symbolic environment to eliminate invalid decisions and guide the model toward valid alternatives. Additionally, Contrastive Ranking (CR) enables fine-grained comparison of candidate plans by evaluating them jointly. Conceptually, SymPlanner operationalizes two cognitive faculties: (i) error monitoring and repair via externalized feedback (IC) and (ii) preference formation among alternatives via pairwise comparison (CR), advancing cognitively plausible, symbol-grounded planning aligned with the rich structure in intelligent systems. We evaluate SymPlanner on PlanBench, demonstrating that it produces more coherent, diverse, and verifiable plans than pure natural language baselines.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCAN: Structured Capability Assessment and Navigation for LLMs</title>
<link>https://arxiv.org/abs/2505.06698</link>
<guid>https://arxiv.org/abs/2505.06698</guid>
<content:encoded><![CDATA[
arXiv:2505.06698v3 Announce Type: replace 
Abstract: Evaluating Large Language Models (LLMs) has become increasingly important, with automatic evaluation benchmarks gaining prominence as alternatives to human evaluation. While existing research has focused on approximating model rankings, such benchmarks fail to provide users and developers with a comprehensive and fine-grained understanding of a specific model's capabilities. To fill this gap, we propose \textbf{SCAN} (Structured Capability Assessment and Navigation), a practical framework that enables detailed characterization of LLM capabilities through comprehensive and fine-grained evaluation. SCAN incorporates four key components: (1) TaxBuilder, which extracts capability-indicating tags from extensive queries to construct a hierarchical taxonomy automatically; (2) RealMix, a query synthesis and filtering mechanism that ensures sufficient evaluation data for each capability tag; (3) a suite of visualization and analysis tools that facilitate efficient navigation and analysis of model capabilities; and (4) a PC$^2$-based (Pre-Comparison-derived Criteria) LLM-as-a-Judge approach that achieves significantly higher accuracy compared to classic LLM-as-a-Judge method. Using SCAN, we conduct a comprehensive evaluation of 21 mainstream LLMs. Our detailed analysis of the GPT-OSS family reveals substantial performance variations, even within sub-capabilities belonging to the same category of capability. This finding highlights the importance of fine-grained evaluation in accurately understanding LLM behavior. Project homepage and resources are available at \href{https://liudan193.github.io/Feedbacker/}{https://liudan193.github.io/Feedbacker/}.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.10320</link>
<guid>https://arxiv.org/abs/2505.10320</guid>
<content:encoded><![CDATA[
arXiv:2505.10320v2 Announce Type: replace 
Abstract: The progress of AI is bottlenecked by the quality of evaluation, making powerful LLM-as-a-Judge models a core solution. The efficacy of these judges depends on their chain-of-thought reasoning, creating a critical need for methods that can effectively optimize this reasoning process. In this work, we introduce J1, a reinforcement learning framework for teaching LLM judges to think before making decisions. Our core contribution lies in converting all judgment tasks for non-verifiable and verifiable prompts into a unified format with verifiable rewards, enabling direct optimization of evaluation quality while mitigating positional bias. We then use RL to train thinking-judges at scales of 8B, 32B, and 70B and show that they obtain state-of-the-art performance across multiple benchmarks. In particular, J1-Qwen-32B, our multitasked pointwise and pairwise judge also outperforms o1-mini, o3, and a much larger 671B DeepSeek-R1 on some benchmarks, while only training on synthetic data. Through comprehensive ablations of pairwise, pointwise, and multitask J1 variants, we demonstrate the effectiveness of our approach across seed prompts, reward strategies, and training recipes. Qualitative analysis reveals that J1 develops systematic evaluation strategies, including dynamic criteria generation, reference answer creation, iterative self-correction of initial assessments, and feedback generation for low-quality responses.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DACL-RAG: Data Augmentation Strategy with Curriculum Learning for Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2505.10493</link>
<guid>https://arxiv.org/abs/2505.10493</guid>
<content:encoded><![CDATA[
arXiv:2505.10493v2 Announce Type: replace 
Abstract: Retrieval-Augmented Generation (RAG) is an effective method to enhance the capabilities of large language models (LLMs). Existing methods typically optimize the retriever or the generator in a RAG system by directly using the top-k retrieved documents. However, two key issues inherent in the training data constrain the effectiveness of this training paradigm: (1) across different queries, the top-k retrieved documents vary greatly in content quality, with some providing valuable knowledge while others lack critical information or are even misleading, and training on such data in a purely random manner may impair the generator's ability to extract key information; (2) for a given query, the limited set of k documents often exhibits low discriminability, and training solely on them makes it difficult for the retriever to learn how to distinguish between relevant and irrelevant documents. To address these issues, we introduce DACL-RAG, a multi-stage RAG training framework that combines a multi-level Data Augmentation strategy with a multi-stage Curriculum Learning paradigm. The data augmentation strategy constructs comprehensive and diverse training sets with controllable difficulty levels through sample evolution, while the curriculum learning paradigm organizes them into progressive stages for training, ensuring stable and consistent improvements, thereby optimizing the overall performance and generalization of the RAG system more effectively. Our DACL-RAG framework demonstrates consistent effectiveness across four open-domain QA datasets, achieving performance gains of 2% to 4% over multiple advanced methods.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-GIVE: Associative Thinking from Limited Structured Knowledge for Enhanced Large Language Model Reasoning</title>
<link>https://arxiv.org/abs/2505.15062</link>
<guid>https://arxiv.org/abs/2505.15062</guid>
<content:encoded><![CDATA[
arXiv:2505.15062v3 Announce Type: replace 
Abstract: When addressing complex questions that require new information, people often associate the question with existing knowledge to derive a sensible answer. For instance, when evaluating whether melatonin aids insomnia, one might associate "hormones helping mental disorders" with "melatonin being a hormone and insomnia a mental disorder" to complete the reasoning. Large Language Models (LLMs) also require such associative thinking, particularly in resolving scientific inquiries when retrieved knowledge is insufficient and does not directly answer the question. Graph Inspired Veracity Extrapolation (GIVE) addresses this by using a knowledge graph (KG) to extrapolate structured knowledge. However, it involves the construction and pruning of many hypothetical triplets, which limits efficiency and generalizability. We propose Self-GIVE, a retrieve-RL framework that enhances LLMs with automatic associative thinking through reinforcement learning. Self-GIVE extracts structured information and entity sets to assist the model in linking to the queried concepts. We address GIVE's key limitations: (1) extensive LLM calls and token overhead for knowledge extrapolation, (2) difficulty in deploying on smaller LLMs (3B or 7B) due to complex instructions, and (3) inaccurate knowledge from LLM pruning. Specifically, after fine-tuning using self-GIVE with a 135 node UMLS KG, it improves the performance of the Qwen2.5 3B and 7B models by up to $\textbf{28.5%$\rightarrow$71.4%}$ and $\textbf{78.6$\rightarrow$90.5%}$ in samples $\textbf{unseen}$ in challenging biomedical QA tasks. In particular, Self-GIVE allows the 7B model to match or outperform GPT3.5 turbo with GIVE, while cutting token usage by over 90%. Self-GIVE enhances the scalable integration of structured retrieval and reasoning with associative thinking.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TACO: Enhancing Multimodal In-context Learning via Task Mapping-Guided Sequence Configuration</title>
<link>https://arxiv.org/abs/2505.17098</link>
<guid>https://arxiv.org/abs/2505.17098</guid>
<content:encoded><![CDATA[
arXiv:2505.17098v3 Announce Type: replace 
Abstract: Multimodal in-context learning (ICL) has emerged as a key mechanism for harnessing the capabilities of large vision-language models (LVLMs). However, its effectiveness remains highly sensitive to the quality of input ICL sequences, particularly for tasks involving complex reasoning or open-ended generation. A major limitation is our limited understanding of how LVLMs actually exploit these sequences during inference. To bridge this gap, we systematically interpret multimodal ICL through the lens of task mapping, which reveals how local and global relationships within and among demonstrations guide model reasoning. Building on this insight, we present TACO, a lightweight transformer-based model equipped with task-aware attention that dynamically configures ICL sequences. By injecting task-mapping signals into the autoregressive decoding process, TACO creates a bidirectional synergy between sequence construction and task reasoning. Experiments on five LVLMs and nine datasets demonstrate that TACO consistently surpasses baselines across diverse ICL tasks. These results position task mapping as a novel and valuable perspective for interpreting and improving multimodal ICL.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Compression to Expression: A Layerwise Analysis of In-Context Learning</title>
<link>https://arxiv.org/abs/2505.17322</link>
<guid>https://arxiv.org/abs/2505.17322</guid>
<content:encoded><![CDATA[
arXiv:2505.17322v2 Announce Type: replace 
Abstract: In-context learning (ICL) enables large language models (LLMs) to adapt to new tasks without weight updates by learning from demonstration sequences. While ICL shows strong empirical performance, its internal representational mechanisms are not yet well understood. In this work, we conduct a statistical geometric analysis of ICL representations to investigate how task-specific information is captured across layers. Our analysis reveals an intriguing phenomenon, which we term *Layerwise Compression-Expression*: early layers progressively produce compact and discriminative representations that encode task information from the input demonstrations, while later layers express these representations to incorporate the query and generate the prediction. This phenomenon is observed consistently across diverse tasks and a range of contemporary LLM architectures. We demonstrate that it has important implications for ICL performance -- improving with model size and the number of demonstrations -- and for robustness in the presence of noisy examples. To further understand the effect of the compact task representation, we propose a bias-variance decomposition and provide a theoretical analysis showing how attention mechanisms contribute to reducing both variance and bias, thereby enhancing performance as the number of demonstrations increases. Our findings reveal an intriguing layerwise dynamic in ICL, highlight how structured representations emerge within LLMs, and showcase that analyzing internal representations can facilitate a deeper understanding of model behavior.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Word to World: Evaluate and Mitigate Culture Bias in LLMs via Word Association Test</title>
<link>https://arxiv.org/abs/2505.18562</link>
<guid>https://arxiv.org/abs/2505.18562</guid>
<content:encoded><![CDATA[
arXiv:2505.18562v2 Announce Type: replace 
Abstract: The human-centered word association test (WAT) serves as a cognitive proxy, revealing sociocultural variations through culturally shared semantic expectations and implicit linguistic patterns shaped by lived experiences. We extend this test into an LLM-adaptive, free-relation task to assess the alignment of large language models (LLMs) with cross-cultural cognition. To address culture preference, we propose CultureSteer, an innovative approach that moves beyond superficial cultural prompting by embedding cultural-specific semantic associations directly within the model's internal representation space. Experiments show that current LLMs exhibit significant bias toward Western (notably American) schemas at the word association level. In contrast, our model substantially improves cross-cultural alignment, capturing diverse semantic associations. Further validation on culture-sensitive downstream tasks confirms its efficacy in fostering cognitive alignment across cultures. This work contributes a novel methodological paradigm for enhancing cultural awareness in LLMs, advancing the development of more inclusive language technologies.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Good or Scientific Curiosity? Uncovering the Research Framing Behind NLP Artefacts</title>
<link>https://arxiv.org/abs/2505.18677</link>
<guid>https://arxiv.org/abs/2505.18677</guid>
<content:encoded><![CDATA[
arXiv:2505.18677v2 Announce Type: replace 
Abstract: Clarifying the research framing of NLP artefacts (e.g., models, datasets, etc.) is crucial to aligning research with practical applications. Recent studies manually analyzed NLP research across domains, showing that few papers explicitly identify key stakeholders, intended uses, or appropriate contexts. In this work, we propose to automate this analysis, developing a three-component system that infers research framings by first extracting key elements (means, ends, stakeholders), then linking them through interpretable rules and contextual reasoning. We evaluate our approach on two domains: automated fact-checking using an existing dataset, and hate speech detection for which we annotate a new dataset-achieving consistent improvements over strong LLM baselines. Finally, we apply our system to recent automated fact-checking papers and uncover three notable trends: a rise in vague or underspecified research goals, increased emphasis on scientific exploration over application, and a shift toward supporting human fact-checkers rather than pursuing full automation.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StressTest: Can YOUR Speech LM Handle the Stress?</title>
<link>https://arxiv.org/abs/2505.22765</link>
<guid>https://arxiv.org/abs/2505.22765</guid>
<content:encoded><![CDATA[
arXiv:2505.22765v2 Announce Type: replace 
Abstract: Sentence stress refers to emphasis on words within a spoken utterance to highlight or contrast an idea. It is often used to imply an underlying intention not explicitly stated. Recent speech-aware language models (SLMs) have enabled direct audio processing, allowing models to access the full richness of speech to perform audio reasoning tasks such as spoken question answering. Despite the crucial role of sentence stress in shaping meaning and intent, it remains largely overlooked in evaluation and development of SLMs. We address this gap by introducing StressTest, a benchmark designed to evaluate models' ability to distinguish between meanings of speech based on the stress pattern. We evaluate leading SLMs, and find that despite their overall capabilities, they perform poorly on such tasks. Hence, we propose a novel data generation pipeline, and create Stress-17k, a training set that simulates change of meaning implied by stress variation. Results suggest, that our finetuned model, StresSLM, generalizes well to real recordings and notably outperforms existing SLMs on sentence stress reasoning and detection. Models, code, data, samples - pages.cs.huji.ac.il/adiyoss-lab/stresstest.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain Dialogue Evaluators</title>
<link>https://arxiv.org/abs/2505.22777</link>
<guid>https://arxiv.org/abs/2505.22777</guid>
<content:encoded><![CDATA[
arXiv:2505.22777v4 Announce Type: replace 
Abstract: Evaluating the quality of open-domain chatbots has become increasingly reliant on LLMs acting as automatic judges. However, existing meta-evaluation benchmarks are static, outdated, and lacking in multilingual coverage, limiting their ability to fully capture subtle weaknesses in evaluation. We introduce MEDAL, an automated multi-agent framework for curating more representative and diverse open-domain dialogue evaluation benchmarks. Our approach leverages several state-of-the-art LLMs to generate user-chatbot multilingual dialogues, conditioned on varied seed contexts. Then, a strong LLM (GPT-4.1) is used for a multidimensional analysis of the performance of the chatbots, uncovering noticeable cross-lingual performance differences. Guided by this large-scale evaluation, we curate a new meta-evaluation multilingual benchmark and human-annotate samples with nuanced quality judgments. This benchmark is then used to assess the ability of several reasoning and non-reasoning LLMs to act as evaluators of open-domain dialogues. Using MEDAL, we uncover that state-of-the-art judges fail to reliably detect nuanced issues such as lack of empathy, commonsense, or relevance.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Has Been Lost with Synthetic Evaluation?</title>
<link>https://arxiv.org/abs/2505.22830</link>
<guid>https://arxiv.org/abs/2505.22830</guid>
<content:encoded><![CDATA[
arXiv:2505.22830v3 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly used for data generation. However, creating evaluation benchmarks raises the bar for this emerging paradigm. Benchmarks must target specific phenomena, penalize exploiting shortcuts, and be challenging. Through two case studies, we investigate whether LLMs can meet these demands by generating reasoning over-text benchmarks and comparing them to those created through careful crowdsourcing. Specifically, we evaluate both the validity and difficulty of LLM-generated versions of two high-quality reading comprehension datasets: CondaQA, which evaluates reasoning about negation, and DROP, which targets reasoning about quantities. We find that prompting LLMs can produce variants of these datasets that are often valid according to the annotation guidelines, at a fraction of the cost of the original crowdsourcing effort. However, we show that they are less challenging for LLMs than their human-authored counterparts. This finding sheds light on what may have been lost by generating evaluation data with LLMs, and calls for critically reassessing the immediate use of this increasingly prevalent approach to benchmark creation.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LegalSearchLM: Rethinking Legal Case Retrieval as Legal Elements Generation</title>
<link>https://arxiv.org/abs/2505.23832</link>
<guid>https://arxiv.org/abs/2505.23832</guid>
<content:encoded><![CDATA[
arXiv:2505.23832v3 Announce Type: replace 
Abstract: Legal Case Retrieval (LCR), which retrieves relevant cases from a query case, is a fundamental task for legal professionals in research and decision-making. However, existing studies on LCR face two major limitations. First, they are evaluated on relatively small-scale retrieval corpora (e.g., 100-55K cases) and use a narrow range of criminal query types, which cannot sufficiently reflect the complexity of real-world legal retrieval scenarios. Second, their reliance on embedding-based or lexical matching methods often results in limited representations and legally irrelevant matches. To address these issues, we present: (1) LEGAR BENCH, the first large-scale Korean LCR benchmark, covering 411 diverse crime types in queries over 1.2M candidate cases; and (2) LegalSearchLM, a retrieval model that performs legal element reasoning over the query case and directly generates content containing those elements, grounded in the target cases through constrained decoding. Experimental results show that LegalSearchLM outperforms baselines by 6-20% on LEGAR BENCH, achieving state-of-the-art performance. It also demonstrates strong generalization to out-of-domain cases, outperforming naive generative models trained on in-domain data by 15%.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improve MLLM Benchmark Efficiency through Interview</title>
<link>https://arxiv.org/abs/2506.00883</link>
<guid>https://arxiv.org/abs/2506.00883</guid>
<content:encoded><![CDATA[
arXiv:2506.00883v2 Announce Type: replace 
Abstract: The rapid development of Multimodal Large Language Models (MLLM) has led to a wide range of MLLM applications, and a number of benchmark datasets have sprung up in order to assess MLLM abilities. However, full-coverage Q&amp;A testing on large-scale data is resource-intensive and time-consuming. To address this issue, we propose the MLLM Interview (MITV) strategy, which aims to quickly obtain MLLM performance metrics by quizzing fewer question. First, First, we constructed the interview dataset, which was built on an existing MLLM assessment dataset, by adding difficulty labels based on the performance of some typical MLLMs in this dataset. Second, we propose an MLLM Interview strategy, which obtains an initial performance situation of the large model by quizzing a small number of topics and then continuously tries to test the model's limits. Through extensive experiments, the result shows that the MITV strategy proposed in this paper performs well on MLLM benchmark datasets, and it is able to obtain the model evaluation capability faster through a small number of questions and answers.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.01713</link>
<guid>https://arxiv.org/abs/2506.01713</guid>
<content:encoded><![CDATA[
arXiv:2506.01713v3 Announce Type: replace 
Abstract: Multimodal large language models (MLLMs) have shown promising capabilities in reasoning tasks, yet still struggle with complex problems requiring explicit self-reflection and self-correction, especially compared to their unimodal text-based counterparts. Existing reflection methods are simplistic and struggle to generate meaningful and instructive feedback, as the reasoning ability and knowledge limits of pre-trained models are largely fixed during initial training. To overcome these challenges, we propose Multimodal Self-Reflection enhanced reasoning with Group Relative Policy Optimization (SRPO), a two-stage reflection-aware reinforcement learning (RL) framework explicitly designed to enhance multimodal LLM reasoning. In the first stage, we construct a high-quality, reflection-focused dataset under the guidance of an advanced MLLM, which generates reflections based on initial responses to help the policy model learn both reasoning and self-reflection. In the second stage, we introduce a novel reward mechanism within the GRPO framework that encourages concise and cognitively meaningful reflection while avoiding redundancy. Extensive experiments across multiple multimodal reasoning benchmarks, including MathVista, MathVision, MathVerse, and MMMU-Pro, using Qwen-2.5-VL-7B and Qwen-2.5-VL-32B demonstrate that SRPO significantly outperforms state-of-the-art models, achieving notable improvements in both reasoning accuracy and reflection quality.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedAgentGym: A Scalable Agentic Training Environment for Code-Centric Reasoning in Biomedical Data Science</title>
<link>https://arxiv.org/abs/2506.04405</link>
<guid>https://arxiv.org/abs/2506.04405</guid>
<content:encoded><![CDATA[
arXiv:2506.04405v2 Announce Type: replace 
Abstract: We introduce MedAgentGym, a scalable and interactive training environment designed to enhance coding-based biomedical reasoning capabilities in large language model (LLM) agents. MedAgentGym comprises 72,413 task instances across 129 categories derived from 12 authentic real-world biomedical scenarios. Tasks are encapsulated within executable sandbox environments, each featuring detailed task specifications, interactive feedback mechanisms, verifiable ground truth annotations, and scalable training trajectory generation. Extensive benchmarking of 29 LLMs reveals substantial performance disparities in biomedical data science between commercial and open-source LLMs. Leveraging efficient multi-threaded and multi-turn trajectory sampling in MedAgentGym, Med-Copilot achieves performance gains of +43.02% and +45.28% from offline and online reinforcement learning, respectively, demonstrating MedAgentGym as an effective training ground while establishing itself as a cost-effective, privacy-preserving alternative competitive with proprietary LLMs (gpt-4o). By offering a unified execution environment with a comprehensive benchmark and accessible, extensible training resources, MedAgentGym delivers an integrated platform to develop LLM-based coding assistants for advanced biomedical data science.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SSA-COMET: Do LLMs Outperform Learned Metrics in Evaluating MT for Under-Resourced African Languages?</title>
<link>https://arxiv.org/abs/2506.04557</link>
<guid>https://arxiv.org/abs/2506.04557</guid>
<content:encoded><![CDATA[
arXiv:2506.04557v2 Announce Type: replace 
Abstract: Evaluating machine translation (MT) quality for under-resourced African languages remains a significant challenge, as existing metrics often suffer from limited language coverage and poor performance in low-resource settings. While recent efforts, such as AfriCOMET, have addressed some of the issues, they are still constrained by small evaluation sets, a lack of publicly available training data tailored to African languages, and inconsistent performance in extremely low-resource scenarios. In this work, we introduce SSA-MTE, a large-scale human-annotated MT evaluation (MTE) dataset covering 14 African language pairs from the News domain, with over 73,000 sentence-level annotations from a diverse set of MT systems. Based on this data, we develop SSA-COMET and SSA-COMET-QE, improved reference-based and reference-free evaluation metrics. We also benchmark prompting-based approaches using state-of-the-art LLMs like GPT-4o, Claude-3.7 and Gemini 2.5 Pro. Our experimental results show that SSA-COMET models significantly outperform AfriCOMET and are competitive with the strongest LLM Gemini 2.5 Pro evaluated in our study, particularly on low-resource languages such as Twi, Luo, and Yoruba. All resources are released under open licenses to support future research.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Micro-Act: Mitigating Knowledge Conflict in LLM-based RAG via Actionable Self-Reasoning</title>
<link>https://arxiv.org/abs/2506.05278</link>
<guid>https://arxiv.org/abs/2506.05278</guid>
<content:encoded><![CDATA[
arXiv:2506.05278v2 Announce Type: replace 
Abstract: Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge Conflicts, where retrieved external knowledge contradicts the inherent, parametric knowledge of large language models (LLMs). It adversely affects performance on downstream tasks such as question answering (QA). Existing approaches often attempt to mitigate conflicts by directly comparing two knowledge sources in a side-by-side manner, but this can overwhelm LLMs with extraneous or lengthy contexts, ultimately hindering their ability to identify and mitigate inconsistencies. To address this issue, we propose Micro-Act a framework with a hierarchical action space that automatically perceives context complexity and adaptively decomposes each knowledge source into a sequence of fine-grained comparisons. These comparisons are represented as actionable steps, enabling reasoning beyond the superficial context. Through extensive experiments on five benchmark datasets, Micro-Act consistently achieves significant increase in QA accuracy over state-of-the-art baselines across all 5 datasets and 3 conflict types, especially in temporal and semantic types where all baselines fail significantly. More importantly, Micro-Act exhibits robust performance on non-conflict questions simultaneously, highlighting its practical value in real-world RAG applications.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Infini-gram mini: Exact n-gram Search at the Internet Scale with FM-Index</title>
<link>https://arxiv.org/abs/2506.12229</link>
<guid>https://arxiv.org/abs/2506.12229</guid>
<content:encoded><![CDATA[
arXiv:2506.12229v4 Announce Type: replace 
Abstract: Language models are trained mainly on massive text data from the Internet, and it becomes increasingly important to understand this data source. Exact-match search engines enable searching in large text corpora - counting string appearances and retrieving the enclosing documents - yet the high storage overhead hinders their application on Internet-scale data. We present infini-gram mini, an efficient and scalable system that can make petabyte-level text corpora searchable. Based on the FM-index data structure (Ferragina and Manzini, 2000), which simultaneously indexes and compresses text, our system creates indexes with size only 44% of the corpus. Infini-gram mini greatly improves upon the best existing implementation of FM-index in terms of indexing speed (18$\times$) and memory use during both indexing (3.2$\times$ reduction) and querying (down to a negligible amount). We index 83TB of Internet text in 99 days with a single CPU node with 128 vCPUs (or 19 hours if using 137 such nodes). We show one important use case of infini-gram mini in a large-scale analysis of benchmark contamination. We find several core LM evaluation benchmarks to be heavily contaminated in Internet crawls (up to 74.2% in GSM8K), which could lead to overestimating the capabilities of language models if trained on such data. We host a benchmark contamination bulletin to share the contamination rate of many core and community-contributed benchmarks. We also release a web interface and an API endpoint to serve general search queries on infini-gram mini indexes.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LexiMark: Robust Watermarking via Lexical Substitutions to Enhance Membership Verification of an LLM's Textual Training Data</title>
<link>https://arxiv.org/abs/2506.14474</link>
<guid>https://arxiv.org/abs/2506.14474</guid>
<content:encoded><![CDATA[
arXiv:2506.14474v2 Announce Type: replace 
Abstract: Large language models (LLMs) can be trained or fine-tuned on data obtained without the owner's consent. Verifying whether a specific LLM was trained on particular data instances or an entire dataset is extremely challenging. Dataset watermarking addresses this by embedding identifiable modifications in training data to detect unauthorized use. However, existing methods often lack stealth, making them relatively easy to detect and remove. In light of these limitations, we propose LexiMark, a novel watermarking technique designed for text and documents, which embeds synonym substitutions for carefully selected high-entropy words. Our method aims to enhance an LLM's memorization capabilities on the watermarked text without altering the semantic integrity of the text. As a result, the watermark is difficult to detect, blending seamlessly into the text with no visible markers, and is resistant to removal due to its subtle, contextually appropriate substitutions that evade automated and manual detection. We evaluated our method using baseline datasets from recent studies and seven open-source models: LLaMA-1 7B, LLaMA-3 8B, Mistral 7B, Pythia 6.9B, as well as three smaller variants from the Pythia family (160M, 410M, and 1B). Our evaluation spans multiple training settings, including continued pretraining and fine-tuning scenarios. The results demonstrate significant improvements in AUROC scores compared to existing methods, underscoring our method's effectiveness in reliably verifying whether unauthorized watermarked data was used in LLM training.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using cognitive models to reveal value trade-offs in language models</title>
<link>https://arxiv.org/abs/2506.20666</link>
<guid>https://arxiv.org/abs/2506.20666</guid>
<content:encoded><![CDATA[
arXiv:2506.20666v3 Announce Type: replace 
Abstract: Value trade-offs are an integral part of human decision-making and language use, however, current tools for interpreting such dynamic and multi-faceted notions of values in LLMs are limited. In cognitive science, so-called "cognitive models" provide formal accounts of such trade-offs in humans, by modeling the weighting of a speaker's competing utility functions in choosing an action or utterance. Here we use a leading cognitive model of polite speech to systematically evaluate value trade-offs in two encompassing model settings: degrees of reasoning "effort" in frontier black-box models, and RL post-training dynamics of open-source models. Our results highlight patterns of higher informational utility than social utility in reasoning models' default behavior, and demonstrate that these patterns shift in predictable ways when models are prompted to prioritize certain goals over others. Our findings from LLMs' training dynamics suggest large shifts in utility values early on in training with persistent effects of the choice of base model and pretraining data, compared to feedback dataset or alignment method. Our framework offers a flexible tool for probing value trade-offs across diverse model types, providing insights for generating hypotheses about other social behaviors such as sycophancy and for shaping training regimes that better control trade-offs between values during model development.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Correction Bench: Uncovering and Addressing the Self-Correction Blind Spot in Large Language Models</title>
<link>https://arxiv.org/abs/2507.02778</link>
<guid>https://arxiv.org/abs/2507.02778</guid>
<content:encoded><![CDATA[
arXiv:2507.02778v2 Announce Type: replace 
Abstract: Although large language models (LLMs) have transformed AI, they still make mistakes and can explore unproductive reasoning paths. Self-correction capability is essential for deploying LLMs in safety-critical applications. We uncover a systematic failure: LLMs cannot correct errors in their own outputs while successfully correcting identical errors from external sources - a limitation we term the Self-Correction Blind Spot. To study this phenomenon, we introduce Self-Correction Bench, an evaluation framework to measure this phenomenon through controlled error injection at three complexity levels. Testing 14 open-source non-reasoning models, we find an average 64.5% blind spot rate. We provide multiple lines of evidence suggesting this limitation may be influenced by training data: human demonstrations rarely include error-correction sequences (favoring error-free responses), whereas reinforcement learning (RL) trained models learn error correction via outcome feedback. Remarkably, appending a minimal "Wait" prompt activates a 89.3% reduction in blind spots, suggesting dormant capabilities that require triggering. Our work highlights a critical limitation potentially influenced by training distribution and offers a practical approach to enhance LLM reliability and trustworthiness - vital for safety-critical domains.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empowering Healthcare Practitioners with Language Models: Structuring Speech Transcripts in Two Real-World Clinical Applications</title>
<link>https://arxiv.org/abs/2507.05517</link>
<guid>https://arxiv.org/abs/2507.05517</guid>
<content:encoded><![CDATA[
arXiv:2507.05517v3 Announce Type: replace 
Abstract: Large language models (LLMs) such as GPT-4o and o1 have demonstrated strong performance on clinical natural language processing (NLP) tasks across multiple medical benchmarks. Nonetheless, two high-impact NLP tasks - structured tabular reporting from nurse dictations and medical order extraction from doctor-patient consultations - remain underexplored due to data scarcity and sensitivity, despite active industry efforts. Practical solutions to these real-world clinical tasks can significantly reduce the documentation burden on healthcare providers, allowing greater focus on patient care. In this paper, we investigate these two challenging tasks using private and open-source clinical datasets, evaluating the performance of both open- and closed-weight LLMs, and analyzing their respective strengths and limitations. Furthermore, we propose an agentic pipeline for generating realistic, non-sensitive nurse dictations, enabling structured extraction of clinical observations. To support further research in both areas, we release SYNUR and SIMORD, the first open-source datasets for nurse observation extraction and medical order extraction.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Psychometric Item Validation Using Virtual Respondents with Trait-Response Mediators</title>
<link>https://arxiv.org/abs/2507.05890</link>
<guid>https://arxiv.org/abs/2507.05890</guid>
<content:encoded><![CDATA[
arXiv:2507.05890v2 Announce Type: replace 
Abstract: As psychometric surveys are increasingly used to assess the traits of large language models (LLMs), the need for scalable survey item generation suited for LLMs has also grown. A critical challenge here is ensuring the construct validity of generated items, i.e., whether they truly measure the intended trait. Traditionally, this requires costly, large-scale human data collection. To make it efficient, we present a framework for virtual respondent simulation using LLMs. Our central idea is to account for mediators: factors through which the same trait can give rise to varying responses to a survey item. By simulating respondents with diverse mediators, we identify survey items that robustly measure intended traits. Experiments on three psychological trait theories (Big5, Schwartz, VIA) show that our mediator generation methods and simulation framework effectively identify high-validity items. LLMs demonstrate the ability to generate plausible mediators from trait definitions and to simulate respondent behavior for item validation. Our problem formulation, metrics, methodology, and dataset open a new direction for cost-effective survey development and a deeper understanding of how LLMs simulate human survey responses. We publicly release our dataset and code to support future work.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MapIQ: Evaluating Multimodal Large Language Models for Map Question Answering</title>
<link>https://arxiv.org/abs/2507.11625</link>
<guid>https://arxiv.org/abs/2507.11625</guid>
<content:encoded><![CDATA[
arXiv:2507.11625v2 Announce Type: replace 
Abstract: Recent advancements in multimodal large language models (MLLMs) have driven researchers to explore how well these models read data visualizations, e.g., bar charts, scatter plots. More recently, attention has shifted to visual question answering with maps (Map-VQA). However, Map-VQA research has primarily focused on choropleth maps, which cover only a limited range of thematic categories and visual analytical tasks. To address these gaps, we introduce MapIQ, a benchmark dataset comprising 14,706 question-answer pairs across three map types: choropleth maps, cartograms, and proportional symbol maps spanning topics from six distinct themes (e.g., housing, crime). We evaluate multiple MLLMs using six visual analytical tasks, comparing their performance against one another and a human baseline. An additional experiment examining the impact of map design changes (e.g., altered color schemes, modified legend designs, and removal of map elements) provides insights into the robustness and sensitivity of MLLMs, their reliance on internal geographic knowledge, and potential avenues for improving Map-VQA performance.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WakenLLM: Evaluating Reasoning Potential and Stability in LLMs via Fine-Grained Benchmarking</title>
<link>https://arxiv.org/abs/2507.16199</link>
<guid>https://arxiv.org/abs/2507.16199</guid>
<content:encoded><![CDATA[
arXiv:2507.16199v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) frequently output the label Unknown in reasoning tasks, where two scenarios may appear: (i) an input sample is genuinely unverifiable, but the model cannot understand why; and (ii) a verifiable problem that the model fails to solve, thus outputs Unknown. We refer to these cases collectively as the Vague Perception phenomenon. Current evaluations focus on whether such answers are honest, rather than analyzing the limits of LLM reasoning.
  To address this, we introduce WakenLLM, a framework that quantifies the portion of Unknown output attributable to model incapacity and evaluates whether stimulation can convert them into either correct answers (verifiable) or justified (unverifiable) responses with valid reasoning. Our method offers a clearer picture of the limits of LLM reasoning and the potential for corrections across various datasets. Comprehensive experiments on six LLMs suggest that, without any training or parameter revision, LLMs can achieve up to a 68.53% accuracy improvement on Vague Perception samples through guided understanding.
  Our work reveals that current baseline methods only activate a small portion of LLMs' reasoning potential, indicating considerable unexplored capacity. This extends the theoretical upper bounds of reasoning accuracy in LLMs. Consequently, this study deepens our understanding of the latent reasoning capacity of LLMs and offers a new perspective on addressing the Vague Perception phenomenon.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Enforcing Company Policy Adherence in Agentic Workflows</title>
<link>https://arxiv.org/abs/2507.16459</link>
<guid>https://arxiv.org/abs/2507.16459</guid>
<content:encoded><![CDATA[
arXiv:2507.16459v2 Announce Type: replace 
Abstract: Large Language Model (LLM) agents hold promise for a flexible and scalable alternative to traditional business process automation, but struggle to reliably follow complex company policies. In this study we introduce a deterministic, transparent, and modular framework for enforcing business policy adherence in agentic workflows. Our method operates in two phases: (1) an offline buildtime stage that compiles policy documents into verifiable guard code associated with tool use, and (2) a runtime integration where these guards ensure compliance before each agent action. We demonstrate our approach on the challenging $\tau$-bench Airlines domain, showing encouraging preliminary results in policy enforcement, and further outline key challenges for real-world deployments.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAGIC: A Multi-Hop and Graph-Based Benchmark for Inter-Context Conflicts in Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2507.21544</link>
<guid>https://arxiv.org/abs/2507.21544</guid>
<content:encoded><![CDATA[
arXiv:2507.21544v2 Announce Type: replace 
Abstract: Knowledge conflict often arises in retrieval-augmented generation (RAG) systems, where retrieved documents may be inconsistent with one another or contradict the model's parametric knowledge. Existing benchmarks for investigating the phenomenon have notable limitations, including a narrow focus on the question answering setup, heavy reliance on entity substitution techniques, and a restricted range of conflict types. To address these issues, we propose a knowledge graph (KG)-based framework that generates varied and subtle conflicts between two similar yet distinct contexts, while ensuring interpretability through the explicit relational structure of KGs. Experimental results on our benchmark, MAGIC, provide intriguing insights into the inner workings of LLMs regarding knowledge conflict: both open-source and proprietary models struggle with conflict detection -- especially when multi-hop reasoning is required -- and often fail to pinpoint the exact source of contradictions. Finally, we present in-depth analyses that serve as a foundation for improving LLMs in integrating diverse, sometimes even conflicting, information.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Defence without Adversarial Defence: Enhancing Language Model Robustness via Instance-level Principal Component Removal</title>
<link>https://arxiv.org/abs/2507.21750</link>
<guid>https://arxiv.org/abs/2507.21750</guid>
<content:encoded><![CDATA[
arXiv:2507.21750v2 Announce Type: replace 
Abstract: Pre-trained language models (PLMs) have driven substantial progress in natural language processing but remain vulnerable to adversarial attacks, raising concerns about their robustness in real-world applications. Previous studies have sought to mitigate the impact of adversarial attacks by introducing adversarial perturbations into the training process, either implicitly or explicitly. While both strategies enhance robustness, they often incur high computational costs. In this work, we propose a simple yet effective add-on module that enhances the adversarial robustness of PLMs by removing instance-level principal components, without relying on conventional adversarial defences or perturbing the original training data. Our approach transforms the embedding space to approximate Gaussian properties, thereby reducing its susceptibility to adversarial perturbations while preserving semantic relationships. This transformation aligns embedding distributions in a way that minimises the impact of adversarial noise on decision boundaries, enhancing robustness without requiring adversarial examples or costly training-time augmentation. Evaluations on eight benchmark datasets show that our approach improves adversarial robustness while maintaining comparable before-attack accuracy to baselines, achieving a balanced trade-off between robustness and generalisation.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring Challenges in Complex Conversations</title>
<link>https://arxiv.org/abs/2507.22968</link>
<guid>https://arxiv.org/abs/2507.22968</guid>
<content:encoded><![CDATA[
arXiv:2507.22968v3 Announce Type: replace 
Abstract: Spoken Dialogue Models (SDMs) have recently attracted significant attention for their ability to generate voice responses directly to users' spoken queries. Despite their increasing popularity, there exists a gap in research focused on comprehensively understanding their practical effectiveness in comprehending and emulating human conversations. This is especially true compared to text-based Large Language Models (LLMs), which benefit from extensive benchmarking. Human voice interactions are inherently more complex than text due to characteristics unique to spoken dialogue. Ambiguity poses one challenge, stemming from semantic factors like polysemy, as well as phonological aspects such as heterograph, heteronyms, and stress patterns. Additionally, context-dependency, like omission, coreference, and multi-turn interaction, adds further complexity to human conversational dynamics. To illuminate the current state of SDM development and to address these challenges, we present a benchmark dataset in this paper, which comprises 1,079 instances in English and Chinese. Accompanied by an LLM-based evaluation method that closely aligns with human judgment, this dataset facilitates a comprehensive exploration of the performance of SDMs in tackling these practical challenges.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>User Feedback in Human-LLM Dialogues: A Lens to Understand Users But Noisy as a Learning Signal</title>
<link>https://arxiv.org/abs/2507.23158</link>
<guid>https://arxiv.org/abs/2507.23158</guid>
<content:encoded><![CDATA[
arXiv:2507.23158v2 Announce Type: replace 
Abstract: Once language models (LMs) are deployed, they can interact with users long-term, ideally evolving based on their feedback. Asking for direct user feedback can be disruptive; thus, we study harvesting implicit user feedback from user-LM interaction logs. We study two user-LM interaction datasets (WildChat and LMSYS). First, we analyze user feedback in the user-LLM conversation logs, providing insights into when and why such feedback occurs. Second, we study harvesting learning signals from such implicit user feedback. Specifically, we study whether incorporating the contents of user feedback (e.g., user wanted clarification), in addition to the polarity of the feedback, can improve the model performance. We observe mixed results, showing this helps in short human-designed questions (MTBench) but not on longer and more complex questions (WildBench). Together, we provide an in-depth study of implicit user feedback, showing its potential and limitations.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XAutoLM: Efficient Fine-Tuning of Language Models via Meta-Learning and AutoML</title>
<link>https://arxiv.org/abs/2508.00924</link>
<guid>https://arxiv.org/abs/2508.00924</guid>
<content:encoded><![CDATA[
arXiv:2508.00924v3 Announce Type: replace 
Abstract: Experts in machine learning leverage domain knowledge to navigate decisions in model selection, hyperparameter optimization, and resource allocation. This is particularly critical for fine-tuning language models (LMs), where repeated trials incur substantial computational overhead and environmental impact. However, no existing automated framework simultaneously tackles the entire model selection and hyperparameter optimization (HPO) task for resource-efficient LM fine-tuning. We introduce XAutoLM, a meta-learning-augmented AutoML framework that reuses past experiences to optimize discriminative and generative LM fine-tuning pipelines efficiently. XAutoLM learns from stored successes and failures by extracting task- and system-level meta-features to bias its sampling toward valuable configurations and away from costly dead ends. On four text classification and two question-answering benchmarks, XAutoLM surpasses zero-shot optimizer's peak F1 on five of six tasks, cuts mean evaluation time of pipelines by up to 4.5x, reduces search error ratios by up to sevenfold, and uncovers up to 50% more pipelines above the zero-shot Pareto front. In contrast, simpler memory-based baselines suffer negative transfer. We release XAutoLM and our experience store to catalyze resource-efficient, Green AI fine-tuning in the NLP community.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LPI-RIT at LeWiDi-2025: Improving Distributional Predictions via Metadata and Loss Reweighting with DisCo</title>
<link>https://arxiv.org/abs/2508.08163</link>
<guid>https://arxiv.org/abs/2508.08163</guid>
<content:encoded><![CDATA[
arXiv:2508.08163v2 Announce Type: replace 
Abstract: The Learning With Disagreements (LeWiDi) 2025 shared task aims to model annotator disagreement through soft label distribution prediction and perspectivist evaluation, which focuses on modeling individual annotators. We adapt DisCo (Distribution from Context), a neural architecture that jointly models item-level and annotator-level label distributions, and present detailed analysis and improvements. In this paper, we extend DisCo by introducing annotator metadata embeddings, enhancing input representations, and multi-objective training losses to capture disagreement patterns better. Through extensive experiments, we demonstrate substantial improvements in both soft and perspectivist evaluation metrics across three datasets. We also conduct in-depth calibration and error analyses that reveal when and why disagreement-aware modeling improves. Our findings show that disagreement can be better captured by conditioning on annotator demographics and by optimizing directly for distributional metrics, yielding consistent improvements across datasets.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models</title>
<link>https://arxiv.org/abs/2508.09138</link>
<guid>https://arxiv.org/abs/2508.09138</guid>
<content:encoded><![CDATA[
arXiv:2508.09138v3 Announce Type: replace 
Abstract: Diffusion large language models (dLLMs) generate text through iterative denoising, yet current decoding strategies discard rich intermediate predictions in favor of the final output. Our work here reveals a critical phenomenon, temporal oscillation, where correct answers often emerge in the middle process, but are overwritten in later denoising steps. To address this issue, we introduce two complementary methods that exploit temporal consistency: 1) Temporal Self-Consistency Voting, a training-free, test-time decoding strategy that aggregates predictions across denoising steps to select the most consistent output; and 2) a post-training method termed Temporal Consistency Reinforcement, which uses Temporal Semantic Entropy (TSE), a measure of semantic stability across intermediate predictions, as a reward signal to encourage stable generations. Empirical results across multiple benchmarks demonstrate the effectiveness of our approach. Using the negative TSE reward alone, we observe a remarkable average improvement of 24.7% on the Countdown dataset over an existing dLLM. Combined with the accuracy reward, we achieve absolute gains of 2.0% on GSM8K, 4.3% on MATH500, 6.6% on SVAMP, and 25.3% on Countdown, respectively. Our findings underscore the untapped potential of temporal dynamics in dLLMs and offer two simple yet effective tools to harness them.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow-SLM: Joint Learning of Linguistic and Acoustic Information for Spoken Language Modeling</title>
<link>https://arxiv.org/abs/2508.09350</link>
<guid>https://arxiv.org/abs/2508.09350</guid>
<content:encoded><![CDATA[
arXiv:2508.09350v2 Announce Type: replace 
Abstract: Textless spoken language models (SLMs) are generative models of speech that do not rely on text supervision. Most textless SLMs learn to predict the next semantic token, a discrete representation of linguistic content, and rely on a separate vocoder to add acoustic information to the generated speech. Such models have no access to acoustic context and no built-in control over acoustic details. In this work, we propose to jointly model linguistic and acoustic information by generating semantic tokens and a continuous real-valued representation of the acoustic frame. We use a flow-matching objective to predict the continuous vector conditioned on the semantic tokens. We study the design space of this approach and find that predicting multiple future semantic tokens helps preserve linguistic information. Our approach achieves comparable performance to existing models in terms of linguistic likelihood benchmarks, while providing better acoustic detail in prompted generation.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models</title>
<link>https://arxiv.org/abs/2508.12903</link>
<guid>https://arxiv.org/abs/2508.12903</guid>
<content:encoded><![CDATA[
arXiv:2508.12903v2 Announce Type: replace 
Abstract: Recent advances in self-refinement have demonstrated significant potential for improving the outputs of large language models (LLMs) through iterative refinement. However, most existing self-refinement methods rely on a reactive process with a fixed number of iterations, making it difficult to determine the optimal timing and content of refinement based on the evolving generation context. Inspired by the way humans dynamically refine their thoughts during execution, we propose ProActive Self-Refinement (PASR), a novel method that enables LLMs to refine their outputs during the generation process. Unlike methods that regenerate entire responses, PASR proactively decides whether, when, and how to refine based on the model's internal state and evolving context. We conduct extensive experiments on a diverse set of 10 tasks to evaluate the effectiveness of PASR. Experimental results show that PASR significantly enhances problem-solving performance. In particular, on Qwen3-8B, PASR reduces average token consumption by 41.6% compared to standard generation, while also achieving an 8.2% improvement in accuracy. Our code and baselines used in the paper are available on GitHub.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoBnB-RAG: Enhancing Multi-Agent Incident Response with Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2508.13118</link>
<guid>https://arxiv.org/abs/2508.13118</guid>
<content:encoded><![CDATA[
arXiv:2508.13118v2 Announce Type: replace 
Abstract: Incident response (IR) requires fast, coordinated, and well-informed decision-making to contain and mitigate cyber threats. While large language models (LLMs) have shown promise as autonomous agents in simulated IR settings, their reasoning is often limited by a lack of access to external knowledge. In this work, we present AutoBnB-RAG, an extension of the AutoBnB framework that incorporates retrieval-augmented generation (RAG) into multi-agent incident response simulations. Built on the Backdoors & Breaches (B&amp;B) tabletop game environment, AutoBnB-RAG enables agents to issue retrieval queries and incorporate external evidence during collaborative investigations. We introduce two retrieval settings: one grounded in curated technical documentation (RAG-Wiki), and another using narrative-style incident reports (RAG-News). We evaluate performance across eight team structures, including newly introduced argumentative configurations designed to promote critical reasoning. To validate practical utility, we also simulate real-world cyber incidents based on public breach reports, demonstrating AutoBnB-RAG's ability to reconstruct complex multi-stage attacks. Our results show that retrieval augmentation improves decision quality and success rates across diverse organizational models. This work demonstrates the value of integrating retrieval mechanisms into LLM-based multi-agent systems for cybersecurity decision-making.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OptimalThinkingBench: Evaluating Over and Underthinking in LLMs</title>
<link>https://arxiv.org/abs/2508.13141</link>
<guid>https://arxiv.org/abs/2508.13141</guid>
<content:encoded><![CDATA[
arXiv:2508.13141v2 Announce Type: replace 
Abstract: Thinking LLMs solve complex tasks at the expense of increased compute and overthinking on simpler problems, while non-thinking LLMs are faster and cheaper but underthink on harder reasoning problems. This has led to the development of separate thinking and non-thinking LLM variants, leaving the onus of selecting the optimal model for each query on the end user. We introduce OptimalThinkingBench, a unified benchmark that jointly evaluates overthinking and underthinking in LLMs and also encourages the development of optimally-thinking models that balance performance and efficiency. Our benchmark comprises two sub-benchmarks: OverthinkingBench, featuring simple math and general queries in 72 domains, and UnderthinkingBench, containing 11 challenging reasoning tasks along with harder math problems. Using novel thinking-adjusted accuracy metrics, we extensively evaluate 33 different thinking and non-thinking models and show that no model is able to optimally think on our benchmark. Thinking models often overthink for hundreds of tokens on the simplest user queries without improving performance. In contrast, large non-thinking models underthink, often falling short of much smaller thinking models. We further explore several methods to encourage optimal thinking, but find that these approaches often improve on one sub-benchmark at the expense of the other, highlighting the need for better unified and optimal models in the future.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SurGE: A Benchmark and Evaluation Framework for Scientific Survey Generation</title>
<link>https://arxiv.org/abs/2508.15658</link>
<guid>https://arxiv.org/abs/2508.15658</guid>
<content:encoded><![CDATA[
arXiv:2508.15658v2 Announce Type: replace 
Abstract: The rapid growth of academic literature makes the manual creation of scientific surveys increasingly infeasible. While large language models show promise for automating this process, progress in this area is hindered by the absence of standardized benchmarks and evaluation protocols. To bridge this critical gap, we introduce SurGE (Survey Generation Evaluation), a new benchmark for scientific survey generation in computer science. SurGE consists of (1) a collection of test instances, each including a topic description, an expert-written survey, and its full set of cited references, and (2) a large-scale academic corpus of over one million papers. In addition, we propose an automated evaluation framework that measures the quality of generated surveys across four dimensions: comprehensiveness, citation accuracy, structural organization, and content quality. Our evaluation of diverse LLM-based methods demonstrates a significant performance gap, revealing that even advanced agentic frameworks struggle with the complexities of survey generation and highlighting the need for future research in this area. We have open-sourced all the code, data, and models at: https://github.com/oneal2000/SurGE
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenWHO: A Document-Level Parallel Corpus for Health Translation in Low-Resource Languages</title>
<link>https://arxiv.org/abs/2508.16048</link>
<guid>https://arxiv.org/abs/2508.16048</guid>
<content:encoded><![CDATA[
arXiv:2508.16048v5 Announce Type: replace 
Abstract: In machine translation (MT), health is a high-stakes domain characterised by widespread deployment and domain-specific vocabulary. However, there is a lack of MT evaluation datasets for low-resource languages in this domain. To address this gap, we introduce OpenWHO, a document-level parallel corpus of 2,978 documents and 26,824 sentences from the World Health Organization's e-learning platform. Sourced from expert-authored, professionally translated materials shielded from web-crawling, OpenWHO spans a diverse range of over 20 languages, of which nine are low-resource. Leveraging this new resource, we evaluate modern large language models (LLMs) against traditional MT models. Our findings reveal that LLMs consistently outperform traditional MT models, with Gemini 2.5 Flash achieving a +4.79 ChrF point improvement over NLLB-54B on our low-resource test set. Further, we investigate how LLM context utilisation affects accuracy, finding that the benefits of document-level translation are most pronounced in specialised domains like health. We release the OpenWHO corpus to encourage further research into low-resource MT in the health domain.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ObjexMT: Objective Extraction and Metacognitive Calibration for LLM-as-a-Judge under Multi-Turn Jailbreaks</title>
<link>https://arxiv.org/abs/2508.16889</link>
<guid>https://arxiv.org/abs/2508.16889</guid>
<content:encoded><![CDATA[
arXiv:2508.16889v3 Announce Type: replace 
Abstract: LLM-as-a-Judge (LLMaaJ) now underpins scalable evaluation, yet we lack a decisive test of a judge's qualification: can it recover a conversation's latent objective and know when that inference is trustworthy? LLMs degrade under irrelevant or long context; multi-turn jailbreaks further hide goals across turns. We introduce ObjexMT, a benchmark for objective extraction and metacognition. Given a multi-turn transcript, a model must return a one-sentence base objective and self-reported confidence. Accuracy is computed via LLM-judge semantic similarity to gold objectives, converted to binary correctness by a human-aligned threshold calibrated on N=300 items (tau = 0.66; F1 = 0.891). Metacognition is evaluated with ECE, Brier, Wrong at High-Confidence (0.80/0.90/0.95), and risk-coverage. Across six models (gpt-4.1, claude-sonnet-4, Qwen3-235B-A22B-FP8, kimi-k2, deepseek-v3.1, gemini-2.5-flash) on three datasets, kimi-k2 attains the highest objective-extraction accuracy (0.612), with claude-sonnet-4 (0.603) and deepseek-v3.1 (0.599) statistically comparable. claude-sonnet-4 yields the best selective risk and calibration (AURC 0.242; ECE 0.206; Brier 0.254). Dataset heterogeneity (16-82 percent accuracy variance) reveals that automated obfuscation poses fundamental challenges beyond model choice. High-confidence errors persist: Wrong at 0.90 ranges from 14.9 percent (claude-sonnet-4) to 47.7 percent (Qwen3-235B-A22B-FP8). ObjexMT provides an actionable test for LLM judges: when objectives are not explicit, judges often misinfer them; we recommend exposing objectives when feasible and gating decisions by confidence otherwise. Data at https://github.com/hyunjun1121/ObjexMT_dataset.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SSFO: Self-Supervised Faithfulness Optimization for Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2508.17225</link>
<guid>https://arxiv.org/abs/2508.17225</guid>
<content:encoded><![CDATA[
arXiv:2508.17225v2 Announce Type: replace 
Abstract: Retrieval-Augmented Generation (RAG) systems require Large Language Models (LLMs) to generate responses that are faithful to the retrieved context. However, faithfulness hallucination remains a critical challenge, as existing methods often require costly supervision and post-training or significant inference burdens. To overcome these limitations, we introduce Self-Supervised Faithfulness Optimization (SSFO), the first self-supervised alignment approach for enhancing RAG faithfulness. SSFO constructs preference data pairs by contrasting the model's outputs generated with and without the context. Leveraging Direct Preference Optimization (DPO), SSFO aligns model faithfulness without incurring labeling costs or additional inference burden. We theoretically and empirically demonstrate that SSFO leverages a benign form of \emph{likelihood displacement}, transferring probability mass from parametric-based tokens to context-aligned tokens. Based on this insight, we propose a modified DPO loss function to encourage likelihood displacement. Comprehensive evaluations show that SSFO significantly outperforms existing methods, achieving state-of-the-art faithfulness on multiple context-based question-answering datasets. Notably, SSFO exhibits strong generalization, improving cross-lingual faithfulness and preserving general instruction-following capabilities. We release our code and model at the anonymous link: https://github.com/chkwy/SSFO
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Filtering for Creativity: Adaptive Prompting for Multilingual Riddle Generation in LLMs</title>
<link>https://arxiv.org/abs/2508.18709</link>
<guid>https://arxiv.org/abs/2508.18709</guid>
<content:encoded><![CDATA[
arXiv:2508.18709v2 Announce Type: replace 
Abstract: Multilingual riddle generation challenges large language models (LLMs) to balance cultural fluency with creative abstraction. Standard prompting strategies -- zero-shot, few-shot, chain-of-thought -- tend to reuse memorized riddles or perform shallow paraphrasing. We introduce Adaptive Originality Filtering (AOF), a prompting framework that filters redundant generations using cosine-based similarity rejection, while enforcing lexical novelty and cross-lingual fidelity. Evaluated across three LLMs and four language pairs, AOF-enhanced GPT-4o achieves \texttt{0.177} Self-BLEU and \texttt{0.915} Distinct-2 in Japanese, signaling improved lexical diversity and reduced redundancy compared to other prompting methods and language pairs. Our findings show that semantic rejection can guide culturally grounded, creative generation without task-specific fine-tuning.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Pretraining for Zero-Shot Cross-Lingual Named Entity Recognition in Low-Resource Philippine Languages</title>
<link>https://arxiv.org/abs/2509.02160</link>
<guid>https://arxiv.org/abs/2509.02160</guid>
<content:encoded><![CDATA[
arXiv:2509.02160v2 Announce Type: replace 
Abstract: Named-entity recognition (NER) in low-resource languages is usually tackled by finetuning very large multilingual LMs, an option that is often infeasible in memory- or latency-constrained settings. We ask whether small decoder LMs can be pretrained so that they adapt quickly and transfer zero-shot to languages unseen during pretraining. To this end we replace part of the autoregressive objective with first-order model-agnostic meta-learning (MAML). Tagalog and Cebuano are typologically similar yet structurally different in their actor/non-actor voice systems, and hence serve as a challenging test-bed. Across four model sizes (11 M - 570 M) MAML lifts zero-shot micro-F1 by 2-6 pp under head-only tuning and 1-3 pp after full tuning, while cutting convergence time by up to 8%. Gains are largest for single-token person entities that co-occur with Tagalog case particles si/ni, highlighting the importance of surface anchors.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Post-training Large Language Models for Diverse High-Quality Responses</title>
<link>https://arxiv.org/abs/2509.04784</link>
<guid>https://arxiv.org/abs/2509.04784</guid>
<content:encoded><![CDATA[
arXiv:2509.04784v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) has emerged as a popular method for post-training large language models (LLMs). While improving the model's performance on downstream tasks, it often reduces the model's output diversity, leading to narrow, canonical responses. Existing methods to enhance diversity are limited, either by operating at inference time or by focusing on surface-level differences. We propose a novel training method named DQO (Diversity Quality Optimization) based on determinantal point processes (DPPs) to jointly optimize LLMs for quality and semantic diversity. Our approach samples and embeds a group of responses for each prompt, then uses the determinant of a kernel-based similarity matrix to measure diversity as the volume spanned by the embeddings of these responses. DQO is flexible and can be applied on top of existing RL algorithms. Experiments across instruction-following, summarization, story generation, and reasoning tasks demonstrate that our method substantially improves semantic diversity without sacrificing model quality.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to Single-turn Jailbreak Templates</title>
<link>https://arxiv.org/abs/2509.08729</link>
<guid>https://arxiv.org/abs/2509.08729</guid>
<content:encoded><![CDATA[
arXiv:2509.08729v2 Announce Type: replace 
Abstract: Multi-turn-to-single-turn (M2S) compresses iterative red-teaming into one structured prompt, but prior work relied on a handful of manually written templates. We present X-Teaming Evolutionary M2S, an automated framework that discovers and optimizes M2S templates through language-model-guided evolution. The system pairs smart sampling from 12 sources with an LLM-as-judge inspired by StrongREJECT and records fully auditable logs.
  Maintaining selection pressure by setting the success threshold to $\theta = 0.70$, we obtain five evolutionary generations, two new template families, and 44.8% overall success (103/230) on GPT-4.1. A balanced cross-model panel of 2,500 trials (judge fixed) shows that structural gains transfer but vary by target; two models score zero at the same threshold. We also find a positive coupling between prompt length and score, motivating length-aware judging.
  Our results demonstrate that structure-level search is a reproducible route to stronger single-turn probes and underscore the importance of threshold calibration and cross-model evaluation. Code, configurations, and artifacts are available at https://github.com/hyunjun1121/M2S-x-teaming.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation</title>
<link>https://arxiv.org/abs/2509.08825</link>
<guid>https://arxiv.org/abs/2509.08825</guid>
<content:encoded><![CDATA[
arXiv:2509.08825v2 Announce Type: replace 
Abstract: Large language models are rapidly transforming social science research by enabling the automation of labor-intensive tasks like data annotation and text analysis. However, LLM outputs vary significantly depending on the implementation choices made by researchers (e.g., model selection or prompting strategy). Such variation can introduce systematic biases and random errors, which propagate to downstream analyses and cause Type I (false positive), Type II (false negative), Type S (wrong sign), or Type M (exaggerated effect) errors. We call this phenomenon where configuration choices lead to incorrect conclusions LLM hacking.
  We find that intentional LLM hacking is strikingly simple. By replicating 37 data annotation tasks from 21 published social science studies, we show that, with just a handful of prompt paraphrases, virtually anything can be presented as statistically significant.
  Beyond intentional manipulation, our analysis of 13 million labels from 18 different LLMs across 2361 realistic hypotheses shows that there is also a high risk of accidental LLM hacking, even when following standard research practices. We find incorrect conclusions in approximately 31% of hypotheses for state-of-the-art LLMs, and in half the hypotheses for smaller language models. While higher task performance and stronger general model capabilities reduce LLM hacking risk, even highly accurate models remain susceptible. The risk of LLM hacking decreases as effect sizes increase, indicating the need for more rigorous verification of LLM-based findings near significance thresholds. We analyze 21 mitigation techniques and find that human annotations provide crucial protection against false positives. Common regression estimator correction techniques can restore valid inference but trade off Type I vs. Type II errors.
  We publish a list of practical recommendations to prevent LLM hacking.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Population-Aligned Persona Generation for LLM-based Social Simulation</title>
<link>https://arxiv.org/abs/2509.10127</link>
<guid>https://arxiv.org/abs/2509.10127</guid>
<content:encoded><![CDATA[
arXiv:2509.10127v2 Announce Type: replace 
Abstract: Recent advances in large language models (LLMs) have enabled human-like social simulations at unprecedented scale and fidelity, offering new opportunities for computational social science. A key challenge, however, is the construction of persona sets that authentically represent the diversity and distribution of real-world populations. Most existing LLM-based social simulation studies focus primarily on designing agentic frameworks and simulation environments, often overlooking the complexities of persona generation and the potential biases introduced by unrepresentative persona sets. In this paper, we propose a systematic framework for synthesizing high-quality, population-aligned persona sets for LLM-driven social simulation. Our approach begins by leveraging LLMs to generate narrative personas from long-term social media data, followed by rigorous quality assessment to filter out low-fidelity profiles. We then apply importance sampling to achieve global alignment with reference psychometric distributions, such as the Big Five personality traits. To address the needs of specific simulation contexts, we further introduce a task-specific module that adapts the globally aligned persona set to targeted subpopulations. Extensive experiments demonstrate that our method significantly reduces population-level bias and enables accurate, flexible social simulation for a wide range of research and policy applications.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CEMTM: Contextual Embedding-based Multimodal Topic Modeling</title>
<link>https://arxiv.org/abs/2509.11465</link>
<guid>https://arxiv.org/abs/2509.11465</guid>
<content:encoded><![CDATA[
arXiv:2509.11465v2 Announce Type: replace 
Abstract: We introduce CEMTM, a context-enhanced multimodal topic model designed to infer coherent and interpretable topic structures from both short and long documents containing text and images. CEMTM builds on fine-tuned large vision language models (LVLMs) to obtain contextualized embeddings, and employs a distributional attention mechanism to weight token-level contributions to topic inference. A reconstruction objective aligns topic-based representations with the document embedding, encouraging semantic consistency across modalities. Unlike existing approaches, CEMTM can process multiple images per document without repeated encoding and maintains interpretability through explicit word-topic and document-topic distributions. Extensive experiments on six multimodal benchmarks show that CEMTM consistently outperforms unimodal and multimodal baselines, achieving a remarkable average LLM score of 2.61. Further analysis shows its effectiveness in downstream few-shot retrieval and its ability to capture visually grounded semantics in complex domains such as scientific articles.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing Information-Seeking Behaviors in a Hakka AI Chatbot: A Cognitive-Pragmatic Study</title>
<link>https://arxiv.org/abs/2509.11591</link>
<guid>https://arxiv.org/abs/2509.11591</guid>
<content:encoded><![CDATA[
arXiv:2509.11591v2 Announce Type: replace 
Abstract: With many endangered languages at risk of disappearing, efforts to preserve them now rely more than ever on using technology alongside culturally informed teaching strategies. This study examines user behaviors in TALKA, a generative AI-powered chatbot designed for Hakka language engagement, by employing a dual-layered analytical framework grounded in Bloom's Taxonomy of cognitive processes and dialogue act categorization. We analyzed 7,077 user utterances, each carefully annotated according to six cognitive levels and eleven dialogue act types. These included a variety of functions, such as asking for information, requesting translations, making cultural inquiries, and using language creatively. Pragmatic classifications further highlight how different types of dialogue acts--such as feedback, control commands, and social greetings--align with specific cognitive intentions. The results suggest that generative AI chatbots can support language learning in meaningful ways--especially when they are designed with an understanding of how users think and communicate. They may also help learners express themselves more confidently and connect with their cultural identity. The TALKA case provides empirical insights into how AI-mediated dialogue facilitates cognitive development in low-resource language learners, as well as pragmatic negotiation and socio-cultural affiliation. By focusing on AI-assisted language learning, this study offers new insights into how technology can support language preservation and educational practice.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fun-ASR Technical Report</title>
<link>https://arxiv.org/abs/2509.12508</link>
<guid>https://arxiv.org/abs/2509.12508</guid>
<content:encoded><![CDATA[
arXiv:2509.12508v3 Announce Type: replace 
Abstract: In recent years, automatic speech recognition (ASR) has witnessed transformative advancements driven by three complementary paradigms: data scaling, model size scaling, and deep integration with large language models (LLMs). However, LLMs are prone to hallucination, which can significantly degrade user experience in real-world ASR applications. In this paper, we present Fun-ASR, a large-scale, LLM-based ASR system that synergistically combines massive data, large model capacity, LLM integration, and reinforcement learning to achieve state-of-the-art performance across diverse and complex speech recognition scenarios. Moreover, Fun-ASR is specifically optimized for practical deployment, with enhancements in streaming capability, noise robustness, code-switching, hotword customization, and satisfying other real-world application requirements. Experimental results show that while most LLM-based ASR systems achieve strong performance on open-source benchmarks, they often underperform on real industry evaluation sets. Thanks to production-oriented optimizations, Fun-ASR achieves state-of-the-art performance on real application datasets, demonstrating its effectiveness and robustness in practical settings.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Deliberation</title>
<link>https://arxiv.org/abs/2509.14760</link>
<guid>https://arxiv.org/abs/2509.14760</guid>
<content:encoded><![CDATA[
arXiv:2509.14760v2 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly applied in diverse real-world scenarios, each governed by bespoke behavioral and safety specifications (spec) custom-tailored by users or organizations. These spec, categorized into safety-spec and behavioral-spec, vary across scenarios and evolve with changing preferences and requirements. We formalize this challenge as specification alignment, focusing on LLMs' ability to follow dynamic, scenario-specific spec from both behavioral and safety perspectives. To address this challenge, we propose Align3, a lightweight method that employs Test-Time Deliberation (TTD) with hierarchical reflection and revision to reason over the specification boundaries. We further present SpecBench, a unified benchmark for measuring specification alignment, covering 5 scenarios, 103 spec, and 1,500 prompts. Experiments on 15 reasoning and 18 instruct models with several TTD methods, including Self-Refine, TPO, and MoreThink, yield three key findings: (i) test-time deliberation enhances specification alignment; (ii) Align3 advances the safety-helpfulness trade-off frontier with minimal overhead; (iii) SpecBench effectively reveals alignment gaps. These results highlight the potential of test-time deliberation as an effective strategy for reasoning over the real-world specification boundaries.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking the Role of Text Complexity in Language Model Pretraining</title>
<link>https://arxiv.org/abs/2509.16551</link>
<guid>https://arxiv.org/abs/2509.16551</guid>
<content:encoded><![CDATA[
arXiv:2509.16551v2 Announce Type: replace 
Abstract: Improving pretraining data quality and size is known to boost downstream performance, but the role of text complexity--how hard a text is to read--remains less explored. We reduce surface-level complexity (shorter sentences, simpler words, simpler structure) while keeping core content approximately constant and ask: (i) How does complexity affect language modeling across model sizes? (ii) Can useful representations be learned from simpler text alone? (iii) How does pretraining text complexity influence downstream language understanding? We simplify human-written texts using a large language model, pretrain causal models (28M-500M) from scratch on original vs. simplified data, and evaluate them in fine-tuning and zero-shot setups. We find that perplexity is sensitive to the interaction between model capacity and text complexity--smaller models degrade far less on simpler texts--while text complexity has little impact on fine-tuning evaluations, with zero-shot evaluations indicating that simpler texts benefit performance on linguistic knowledge tasks, whereas more complex texts favor tasks requiring world knowledge and entity tracking. Our findings suggest that different types of data diversity affect transfer and zero-shot performance differently, providing insight into tailoring data curation to specific goals.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instruction Boundary: Quantifying Biases in LLM Reasoning under Various Coverage</title>
<link>https://arxiv.org/abs/2509.20278</link>
<guid>https://arxiv.org/abs/2509.20278</guid>
<content:encoded><![CDATA[
arXiv:2509.20278v2 Announce Type: replace 
Abstract: Nowadays, automatically generated datasets are increasingly used in LLM reasoning tasks; however, large-scale corpora often contain inherent flaws. For example, a single-choice question may include none or multiple correct options, while true-or-false questions may involve vague or unverifiable statements. We refer to these exceptional answer forms as sparse labels. To compare LLMs' ability to recognize various question forms and produce correct answers, we investigate how different instruction formats can either facilitate or mislead LLM reasoning ability. We introduce the concept of Instruction Boundary, which systematically analyzes how different levels of prompt coverage -- sufficient, redundant, or insufficient -- can lead to reasoning biases and performance changes in LLMs. To examine this phenomenon, we design eight experimental settings across five dataset forms. We further propose BiasDetector, a unified framework that quantifies LLMs' ability to identify sparse labels under different kinds of Instruction Boundary conditions. Evaluations on five mainstream LLMs show that, despite their seemingly high accuracy, substantial reasoning biases persist in many downstream tasks as a direct consequence of prompt coverage. We analyze the impact of these biases and outline possible mitigation strategies. Our findings highlight not only the importance of addressing sparse labels, but also the need for developers to recognize and mitigate the risks introduced by Instruction Boundary.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning</title>
<link>https://arxiv.org/abs/2509.22075</link>
<guid>https://arxiv.org/abs/2509.22075</guid>
<content:encoded><![CDATA[
arXiv:2509.22075v2 Announce Type: replace 
Abstract: Post-training compression of large language models (LLMs) largely relies on low-rank weight approximation, which represents each column of a weight matrix in a shared low-dimensional subspace. While this is a computationally efficient strategy, the imposed structural constraint is rigid and can lead to a noticeable model accuracy drop. In this work, we propose CoSpaDi (Compression via Sparse Dictionary Learning), a novel training-free compression framework that replaces low-rank decomposition with a more flexible structured sparse factorization in which each weight matrix is represented with a dense dictionary and a column-sparse coefficient matrix. This formulation enables a union-of-subspaces representation: different columns of the original weight matrix are approximated in distinct subspaces spanned by adaptively selected dictionary atoms, offering greater expressiveness than a single invariant basis. Crucially, CoSpaDi leverages a small calibration dataset to optimize the factorization such that the output activations of compressed projection layers closely match those of the original ones, thereby minimizing functional reconstruction error rather than mere weight approximation. This data-aware strategy preserves better model fidelity without any fine-tuning under reasonable compression ratios. Moreover, the resulting structured sparsity allows efficient sparse-dense matrix multiplication and is compatible with post-training quantization for further memory and latency gains. We evaluate CoSpaDi across multiple Llama and Qwen models under per-layer and per-group settings at 20-50\% compression ratios, demonstrating consistent superiority over state-of-the-art data-aware low-rank methods both in accuracy and perplexity. Our results establish structured sparse dictionary learning as a powerful alternative to conventional low-rank approaches for efficient LLM deployment.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentBench: Evaluating LLMs as Agents</title>
<link>https://arxiv.org/abs/2308.03688</link>
<guid>https://arxiv.org/abs/2308.03688</guid>
<content:encoded><![CDATA[
arXiv:2308.03688v3 Announce Type: replace-cross 
Abstract: The potential of Large Language Model (LLM) as agents has been widely acknowledged recently. Thus, there is an urgent need to quantitatively \textit{evaluate LLMs as agents} on challenging tasks in interactive environments. We present AgentBench, a multi-dimensional benchmark that consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities. Our extensive test over \num API-based and open-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and many OSS competitors that are no larger than 70B. We identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents. Improving instruction following and training on high quality multi-round alignment data could improve agent performance. And different from existing assumptions, training on code present ambivalent impacts on different agent tasks. Datasets, environments, and an integrated evaluation package for AgentBench are released at https://github.com/THUDM/AgentBench.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Large Language Models generalize analogy solving like children can?</title>
<link>https://arxiv.org/abs/2411.02348</link>
<guid>https://arxiv.org/abs/2411.02348</guid>
<content:encoded><![CDATA[
arXiv:2411.02348v3 Announce Type: replace-cross 
Abstract: In people, the ability to solve analogies such as "body : feet :: table : ?" emerges in childhood, and appears to transfer easily to other domains, such as the visual domain "( : ) :: < : ?". Recent research shows that large language models (LLMs) can solve various forms of analogies. However, can LLMs generalize analogy solving to new domains like people can? To investigate this, we had children, adults, and LLMs solve a series of letter-string analogies (e.g., a b : a c :: j k : ?) in the Latin alphabet, in a near transfer domain (Greek alphabet), and a far transfer domain (list of symbols). Children and adults easily generalized their knowledge to unfamiliar domains, whereas LLMs did not. This key difference between human and AI performance is evidence that these LLMs still struggle with robust human-like analogical transfer.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Statistical Hypothesis Testing Framework for Data Misappropriation Detection in Large Language Models</title>
<link>https://arxiv.org/abs/2501.02441</link>
<guid>https://arxiv.org/abs/2501.02441</guid>
<content:encoded><![CDATA[
arXiv:2501.02441v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are rapidly gaining enormous popularity in recent years. However, the training of LLMs has raised significant privacy and legal concerns, particularly regarding the distillation and inclusion of copyrighted materials in their training data without proper attribution or licensing, an issue that falls under the broader concern of data misappropriation. In this article, we focus on a specific problem of data misappropriation detection, namely, to determine whether a given LLM has incorporated the data generated by another LLM. We propose embedding watermarks into the copyrighted training data and formulating the detection of data misappropriation as a hypothesis testing problem. We develop a general statistical testing framework, construct test statistics, determine optimal rejection thresholds, and explicitly control type I and type II errors. Furthermore, we establish the asymptotic optimality properties of the proposed tests, and demonstrate the empirical effectiveness through intensive numerical experiments.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fed-SB: A Silver Bullet for Extreme Communication Efficiency and Performance in (Private) Federated LoRA Fine-Tuning</title>
<link>https://arxiv.org/abs/2502.15436</link>
<guid>https://arxiv.org/abs/2502.15436</guid>
<content:encoded><![CDATA[
arXiv:2502.15436v2 Announce Type: replace-cross 
Abstract: Low-Rank Adaptation (LoRA) has become ubiquitous for efficiently fine-tuning foundation models. However, federated fine-tuning using LoRA is challenging due to suboptimal updates arising from traditional federated averaging of individual adapters. Existing solutions either incur prohibitively high communication cost that scales linearly with the number of clients or suffer from performance degradation due to limited expressivity. We introduce Federated Silver Bullet (Fed-SB), a novel approach for federated fine-tuning of LLMs using LoRA-SB, a recently proposed low-rank adaptation method. LoRA-SB optimally aligns the optimization trajectory with the ideal low-rank full fine-tuning projection by learning a small square matrix (R) between adapters B and A, keeping other components fixed. Direct averaging of R guarantees exact updates, substantially reducing communication cost, which remains independent of the number of clients, and enables scalability. Fed-SB achieves state-of-the-art performance across commonsense reasoning, arithmetic reasoning, and language inference tasks while reducing communication costs by up to 230x. In private settings, Fed-SB further improves performance by (1) reducing trainable parameters, thereby lowering the noise required for differential privacy and (2) avoiding noise amplification introduced by other methods. Overall, Fed-SB offers a state-of-the-art, efficient, and scalable solution for both private and non-private federated fine-tuning. Our code is publicly available at: https://github.com/CERT-Lab/fed-sb.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DISC: Dynamic Decomposition Improves LLM Inference Scaling</title>
<link>https://arxiv.org/abs/2502.16706</link>
<guid>https://arxiv.org/abs/2502.16706</guid>
<content:encoded><![CDATA[
arXiv:2502.16706v3 Announce Type: replace-cross 
Abstract: Inference scaling methods for LLMs often rely on decomposing problems into steps (or groups of tokens), followed by sampling and selecting the best next steps. However, these steps and their sizes are often predetermined or manually designed based on domain knowledge. We propose dynamic decomposition, a method that adaptively and automatically partitions solution and reasoning traces into manageable steps during inference. By more effectively allocating compute -- particularly through subdividing challenging steps and prioritizing their sampling -- dynamic decomposition significantly improves inference efficiency. Experiments on benchmarks such as APPS, MATH, and LiveCodeBench demonstrate that dynamic decomposition outperforms static approaches, including token-level, sentence-level, and single-step decompositions, reducing the pass@10 error rate by 5.0%, 6.7%, and 10.5% respectively. These findings highlight the potential of dynamic decomposition to improve a wide range of inference scaling techniques.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeded Poisson Factorization: leveraging domain knowledge to fit topic models</title>
<link>https://arxiv.org/abs/2503.02741</link>
<guid>https://arxiv.org/abs/2503.02741</guid>
<content:encoded><![CDATA[
arXiv:2503.02741v2 Announce Type: replace-cross 
Abstract: Topic models are widely used for discovering latent thematic structures in large text corpora, yet traditional unsupervised methods often struggle to align with pre-defined conceptual domains. This paper introduces seeded Poisson Factorization (SPF), a novel approach that extends the Poisson Factorization (PF) framework by incorporating domain knowledge through seed words. SPF enables a structured topic discovery by modifying the prior distribution of topic-specific term intensities, assigning higher initial rates to pre-defined seed words. The model is estimated using variational inference with stochastic gradient optimization, ensuring scalability to large datasets.
  We present in detail the results of applying SPF to an Amazon customer feedback dataset, leveraging pre-defined product categories as guiding structures. SPF achieves superior performance compared to alternative guided probabilistic topic models in terms of computational efficiency and classification performance. Robustness checks highlight SPF's ability to adaptively balance domain knowledge and data-driven topic discovery, even in case of imperfect seed word selection. Further applications of SPF to four additional benchmark datasets, where the corpus varies in size and the number of topics differs, demonstrate its general superior classification performance compared to the unseeded PF model.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding R1-Zero-Like Training: A Critical Perspective</title>
<link>https://arxiv.org/abs/2503.20783</link>
<guid>https://arxiv.org/abs/2503.20783</guid>
<content:encoded><![CDATA[
arXiv:2503.20783v2 Announce Type: replace-cross 
Abstract: DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can directly enhance the reasoning capabilities of LLMs without supervised fine-tuning. In this work, we critically examine R1-Zero-like training by analyzing its two core components: base models and RL. We investigate a wide range of base models, including DeepSeek-V3-Base, to understand how pretraining characteristics influence RL performance. Our analysis reveals that DeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models demonstrate strong reasoning capabilities even without prompt templates, suggesting potential pretraining biases. Additionally, we identify an optimization bias in Group Relative Policy Optimization (GRPO), which artificially increases response length (especially for incorrect outputs) during training. To address this, we introduce Dr. GRPO, an unbiased optimization method that improves token efficiency while maintaining reasoning performance. Leveraging these insights, we present a minimalist R1-Zero recipe that achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a new state-of-the-art. Our code is available at https://github.com/sail-sg/understand-r1-zero.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRA: Better Length Generalisation with Threshold Relative Attention</title>
<link>https://arxiv.org/abs/2503.23174</link>
<guid>https://arxiv.org/abs/2503.23174</guid>
<content:encoded><![CDATA[
arXiv:2503.23174v4 Announce Type: replace-cross 
Abstract: Transformers struggle with length generalisation, displaying poor performance even on basic tasks. We test whether these limitations can be explained through two key failures of the self-attention mechanism. The first is the inability to fully remove irrelevant information. The second is tied to position, even if the dot product between a key and query is highly negative (i.e. an irrelevant key) learned positional biases may unintentionally up-weight such information - dangerous when distances become out of distribution. Put together, these two failure cases lead to compounding generalisation difficulties. We test whether they can be mitigated through the combination of a) selective sparsity - completely removing irrelevant keys from the attention softmax and b) contextualised relative distance - distance is only considered as between the query and the keys that matter. We show how refactoring the attention mechanism with these two mitigations in place can substantially improve the generalisation capabilities of decoder only transformers.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Effectiveness and Generalization of Race Representations for Debiasing High-Stakes Decisions</title>
<link>https://arxiv.org/abs/2504.06303</link>
<guid>https://arxiv.org/abs/2504.06303</guid>
<content:encoded><![CDATA[
arXiv:2504.06303v2 Announce Type: replace-cross 
Abstract: Understanding and mitigating biases is critical for the adoption of large language models (LLMs) in high-stakes decision-making. We introduce Admissions and Hiring, decision tasks with hypothetical applicant profiles where a person's race can be inferred from their name, as simplified test beds for racial bias. We show that Gemma 2B Instruct and LLaMA 3.2 3B Instruct exhibit strong biases. Gemma grants admission to 26% more White than Black applicants, and LLaMA hires 60% more Asian than White applicants. We demonstrate that these biases are resistant to prompt engineering: multiple prompting strategies all fail to promote fairness. In contrast, using distributed alignment search, we can identify "race subspaces" within model activations and intervene on them to debias model decisions. Averaging the representation across all races within the subspaces reduces Gemma's bias by 37-57%. Finally, we examine the generalizability of Gemma's race subspaces, and find limited evidence for generalization, where changing the prompt format can affect the race representation. Our work suggests mechanistic approaches may provide a promising venue for improving the fairness of LLMs, but a universal race representation remains elusive.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentRewardBench: Evaluating Automatic Evaluations of Web Agent Trajectories</title>
<link>https://arxiv.org/abs/2504.08942</link>
<guid>https://arxiv.org/abs/2504.08942</guid>
<content:encoded><![CDATA[
arXiv:2504.08942v2 Announce Type: replace-cross 
Abstract: Web agents enable users to perform tasks on web browsers through natural language interaction. Evaluating web agents trajectories is an important problem, since it helps us determine whether the agent successfully completed the tasks. Rule-based methods are widely used for this purpose, but they are challenging to extend to new tasks and may not always recognize successful trajectories. We may achieve higher accuracy through human evaluation, but the process would be substantially slower and more expensive. Automatic evaluations with LLMs may avoid the challenges of designing new rules and manually annotating trajectories, enabling faster and cost-effective evaluation. However, it is unclear how effective they are at evaluating web agents. To this end, we propose AgentRewardBench, the first benchmark to assess the effectiveness of LLM judges for evaluating web agents. AgentRewardBench contains 1302 trajectories across 5 benchmarks and 4 LLMs. Each trajectory in AgentRewardBench is reviewed by an expert, who answers questions pertaining to the success, side effects, and repetitiveness of the agent. Using our benchmark, we evaluate 12 LLM judges and find that no single LLM excels across all benchmarks. We also find that the rule-based evaluation used by common benchmarks tends to underreport the success rate of web agents, highlighting a key weakness of rule-based evaluation and the need to develop more flexible automatic evaluations. We release the benchmark at: https://agent-reward-bench.github.io
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Dialect Gaps to Identity Maps: Tackling Variability in Speaker Verification</title>
<link>https://arxiv.org/abs/2505.04629</link>
<guid>https://arxiv.org/abs/2505.04629</guid>
<content:encoded><![CDATA[
arXiv:2505.04629v2 Announce Type: replace-cross 
Abstract: The complexity and difficulties of Kurdish speaker detection among its several dialects are investigated in this work. Because of its great phonetic and lexical differences, Kurdish with several dialects including Kurmanji, Sorani, and Hawrami offers special challenges for speaker recognition systems. The main difficulties in building a strong speaker identification system capable of precisely identifying speakers across several dialects are investigated in this work. To raise the accuracy and dependability of these systems, it also suggests solutions like sophisticated machine learning approaches, data augmentation tactics, and the building of thorough dialect-specific corpus. The results show that customized strategies for every dialect together with cross-dialect training greatly enhance recognition performance.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Agents Mirror Human Causal Reasoning Biases. How Can We Help Them Think Like Scientists?</title>
<link>https://arxiv.org/abs/2505.09614</link>
<guid>https://arxiv.org/abs/2505.09614</guid>
<content:encoded><![CDATA[
arXiv:2505.09614v3 Announce Type: replace-cross 
Abstract: Language model (LM) agents are increasingly used as autonomous decision-makers which need to actively gather information to guide their decisions. A crucial cognitive skill for such agents is the efficient exploration and understanding of the causal structure of the world -- key to robust, scientifically grounded reasoning. Yet, it remains unclear whether LMs possess this capability or exhibit systematic biases leading to erroneous conclusions. In this work, we examine LMs' ability to explore and infer causal relationships, using the well-established Blicket Test paradigm from developmental psychology. We find that LMs reliably infer the common, intuitive disjunctive causal relationships but systematically struggle with the unusual, yet equally (or sometimes even more) evidenced conjunctive ones. This "disjunctive bias" persists across model families, sizes, and prompting strategies, and performance further declines as task complexity increases. Interestingly, an analogous bias appears in human adults, suggesting that LMs may have inherited deep-seated reasoning heuristics from their training data. To this end, we quantify similarities between LMs and humans, finding that LMs exhibit adult-like inference profiles (but not child-like). Finally, we propose a test-time sampling method which explicitly samples and eliminates hypotheses about causal relationships from the LM. This scalable approach significantly reduces the disjunctive bias and moves LMs closer to the goal of scientific, causally rigorous reasoning.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly</title>
<link>https://arxiv.org/abs/2505.10610</link>
<guid>https://arxiv.org/abs/2505.10610</guid>
<content:encoded><![CDATA[
arXiv:2505.10610v3 Announce Type: replace-cross 
Abstract: The rapid extension of context windows in large vision-language models has given rise to long-context vision-language models (LCVLMs), which are capable of handling hundreds of images with interleaved text tokens in a single forward pass. In this work, we introduce MMLongBench, the first benchmark covering a diverse set of long-context vision-language tasks, to evaluate LCVLMs effectively and thoroughly. MMLongBench is composed of 13,331 examples spanning five different categories of downstream tasks, such as Visual RAG and Many-Shot ICL. It also provides broad coverage of image types, including various natural and synthetic images. To assess the robustness of the models to different input lengths, all examples are delivered at five standardized input lengths (8K-128K tokens) via a cross-modal tokenization scheme that combines vision patches and text tokens. Through a thorough benchmarking of 46 closed-source and open-source LCVLMs, we provide a comprehensive analysis of the current models' vision-language long-context ability. Our results show that: i) performance on a single task is a weak proxy for overall long-context capability; ii) both closed-source and open-source models face challenges in long-context vision-language tasks, indicating substantial room for future improvement; iii) models with stronger reasoning ability tend to exhibit better long-context performance. By offering wide task coverage, various image types, and rigorous length control, MMLongBench provides the missing foundation for diagnosing and advancing the next generation of LCVLMs.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios</title>
<link>https://arxiv.org/abs/2505.12891</link>
<guid>https://arxiv.org/abs/2505.12891</guid>
<content:encoded><![CDATA[
arXiv:2505.12891v3 Announce Type: replace-cross 
Abstract: Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend the real world. However, existing works neglect the real-world challenges for temporal reasoning: (1) intensive temporal information, (2) fast-changing event dynamics, and (3) complex temporal dependencies in social interactions. To bridge this gap, we propose a multi-level benchmark TIME, designed for temporal reasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3 levels with 11 fine-grained sub-tasks. This benchmark encompasses 3 sub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News, and TIME-Dial. We conduct extensive experiments on reasoning models and non-reasoning models. And we conducted an in-depth analysis of temporal reasoning performance across diverse real-world scenarios and tasks, and summarized the impact of test-time scaling on temporal reasoning capabilities. Additionally, we release TIME-Lite, a human-annotated subset to foster future research and standardized evaluation in temporal reasoning. The code is available at https://github.com/sylvain-wei/TIME , the dataset is available at https://huggingface.co/datasets/SylvainWei/TIME , and the project page link is https://sylvain-wei.github.io/TIME/ .
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safety Subspaces are Not Linearly Distinct: A Fine-Tuning Case Study</title>
<link>https://arxiv.org/abs/2505.14185</link>
<guid>https://arxiv.org/abs/2505.14185</guid>
<content:encoded><![CDATA[
arXiv:2505.14185v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) rely on safety alignment to produce socially acceptable responses. However, this behavior is known to be brittle: further fine-tuning, even on benign or lightly contaminated data, can degrade safety and reintroduce harmful behaviors. A growing body of work suggests that alignment may correspond to identifiable directions in weight space, forming subspaces that could, in principle, be isolated or preserved to defend against misalignment. In this work, we conduct a comprehensive empirical study of this perspective. We examine whether safety-relevant behavior is concentrated in specific linear subspaces, whether it can be separated from general-purpose learning, and whether harmfulness arises from distinguishable patterns in activations. Across both weight and activation spaces, our findings are consistent: subspaces that amplify safe behaviors also amplify useful ones, and prompts with different safety implications activate overlapping representations. Rather than residing in distinct directions, we show that safety is highly entangled with the general learning components of the model. This suggests that subspace-based defenses face fundamental limitations and underscores the need for alternative strategies to preserve safety under continued training. We corroborate these findings with multiple experiments on five open-source LLMs from the Llama and Qwen families. Our code is publicly available at: https://github.com/CERT-Lab/safety-subspaces.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaSTaR: Adaptive Data Sampling for Training Self-Taught Reasoners</title>
<link>https://arxiv.org/abs/2505.16322</link>
<guid>https://arxiv.org/abs/2505.16322</guid>
<content:encoded><![CDATA[
arXiv:2505.16322v3 Announce Type: replace-cross 
Abstract: Self-Taught Reasoners (STaR), synonymously known as Rejection sampling Fine-Tuning (RFT), is an integral part of the training pipeline of self-improving reasoning Language Models (LMs). The self-improving mechanism often employs random observation (data) sampling. However, this results in trained observation imbalance; inefficiently over-training on solved examples while under-training on challenging ones. In response, we introduce Adaptive STaR (AdaSTaR), a novel algorithm that rectifies this by integrating two adaptive sampling principles: (1) Adaptive Sampling for Diversity: promoting balanced training across observations, and (2) Adaptive Sampling for Curriculum: dynamically adjusting data difficulty to match the model's evolving strength. Across six benchmarks, AdaSTaR achieves best test accuracy in all instances (6/6) and reduces training FLOPs by an average of 58.6% against an extensive list of baselines. These improvements in performance and efficiency generalize to different pre-trained LMs and larger models, paving the way for more efficient and effective self-improving LMs.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COUNTDOWN: Contextually Sparse Activation Filtering Out Unnecessary Weights in Down Projection</title>
<link>https://arxiv.org/abs/2505.17701</link>
<guid>https://arxiv.org/abs/2505.17701</guid>
<content:encoded><![CDATA[
arXiv:2505.17701v2 Announce Type: replace-cross 
Abstract: The growing size of large language models has created significant computational inefficiencies. To address this challenge, sparse activation methods selectively deactivates non-essential parameters during inference, reducing computational costs in FFNN layers. While existing methods focus on non-linear gating mechanisms, we hypothesize that the sparsity of the FFNN layer lies globally in the form of a linear combination over its internal down projection matrix. Based on this insight, we propose two methods: M-COUNTDOWN, leveraging indirect coefficients, and D-COUNTDOWN, utilizing direct coefficients of the linear combination. Experimental results demonstrate that D-COUNTDOWN can omit 90% of computations with performance loss as low as 5.5% ideally, while M-COUNTDOWN provides a predictor-free solution with up to 29.4% better performance preservation compared to existing methods. Our specialized kernel implementations effectively realize these theoretical gains into substantial real-world acceleration.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Many Parameters Does Your Task Really Need? Task Specific Pruning with LLM-Sieve</title>
<link>https://arxiv.org/abs/2505.18350</link>
<guid>https://arxiv.org/abs/2505.18350</guid>
<content:encoded><![CDATA[
arXiv:2505.18350v2 Announce Type: replace-cross 
Abstract: As Large Language Models (LLMs) are increasingly deployed for narrow tasks in resource-constrained settings, a central question arises: how much of an LLM is truly necessary for a given task? We present LLM-Sieve, a framework that prunes LLMs down to the minimal parameter subset needed to preserve task performance. Our approach introduces two innovations: (i) output-aligned non-orthogonal projections, which yield more faithful low-rank approximations than traditional PCA/SVD by aligning directly with layer outputs; and (ii) adaptive pruning via a Genetic Algorithm, which automatically discovers matrix-specific pruning levels and exposes the uneven distribution of task-relevant knowledge. Across models from 3.8B to 70B parameters, LLM-Sieve removes 20-75% of weights with only 1-5% accuracy loss-substantially ahead of prior pruning methods. Beyond efficiency, our framework reveals bottleneck matrices that concentrate critical knowledge, suggesting architectural implications for future LLM design. LLM-Sieve integrates seamlessly with LoRA fine-tuning and quantization, enabling both efficient deployment and deeper understanding of knowledge organization in LLMs.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scientific Paper Retrieval with LLM-Guided Semantic-Based Ranking</title>
<link>https://arxiv.org/abs/2505.21815</link>
<guid>https://arxiv.org/abs/2505.21815</guid>
<content:encoded><![CDATA[
arXiv:2505.21815v2 Announce Type: replace-cross 
Abstract: Scientific paper retrieval is essential for supporting literature discovery and research. While dense retrieval methods demonstrate effectiveness in general-purpose tasks, they often fail to capture fine-grained scientific concepts that are essential for accurate understanding of scientific queries. Recent studies also use large language models (LLMs) for query understanding; however, these methods often lack grounding in corpus-specific knowledge and may generate unreliable or unfaithful content. To overcome these limitations, we propose SemRank, an effective and efficient paper retrieval framework that combines LLM-guided query understanding with a concept-based semantic index. Each paper is indexed using multi-granular scientific concepts, including general research topics and detailed key phrases. At query time, an LLM identifies core concepts derived from the corpus to explicitly capture the query's information need. These identified concepts enable precise semantic matching, significantly enhancing retrieval accuracy. Experiments show that SemRank consistently improves the performance of various base retrievers, surpasses strong existing LLM-based baselines, and remains highly efficient.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identity resolution of software metadata using Large Language Models</title>
<link>https://arxiv.org/abs/2505.23500</link>
<guid>https://arxiv.org/abs/2505.23500</guid>
<content:encoded><![CDATA[
arXiv:2505.23500v2 Announce Type: replace-cross 
Abstract: Software is an essential component of research. However, little attention has been paid to it compared with that paid to research data. Recently, there has been an increase in efforts to acknowledge and highlight the importance of software in research activities. Structured metadata from platforms like bio.tools, Bioconductor, and Galaxy ToolShed offers valuable insights into research software in the Life Sciences. Although originally intended to support discovery and integration, this metadata can be repurposed for large-scale analysis of software practices. However, its quality and completeness vary across platforms, reflecting diverse documentation practices. To gain a comprehensive view of software development and sustainability, consolidating this metadata is necessary, but requires robust mechanisms to address its heterogeneity and scale.
  This article presents an evaluation of instruction-tuned large language models for the task of software metadata identity resolution, a critical step in assembling a cohesive collection of research software. Such a collection is the reference component for the Software Observatory at OpenEBench, a platform that aggregates metadata to monitor the FAIRness of research software in the Life Sciences. We benchmarked multiple models against a human-annotated gold standard, examined their behavior on ambiguous cases, and introduced an agreement-based proxy for high-confidence automated decisions. The proxy achieved high precision and statistical robustness, while also highlighting the limitations of current models and the broader challenges of automating semantic judgment in FAIR-aligned software metadata across registries and repositories.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human Empathy as Encoder: AI-Assisted Depression Assessment in Special Education</title>
<link>https://arxiv.org/abs/2505.23631</link>
<guid>https://arxiv.org/abs/2505.23631</guid>
<content:encoded><![CDATA[
arXiv:2505.23631v3 Announce Type: replace-cross 
Abstract: Assessing student depression in sensitive environments like special education is challenging. Standardized questionnaires may not fully reflect students' true situations. Furthermore, automated methods often falter with rich student narratives, lacking the crucial, individualized insights stemming from teachers' empathetic connections with students. Existing methods often fail to address this ambiguity or effectively integrate educator understanding. To address these limitations by fostering a synergistic human-AI collaboration, this paper introduces Human Empathy as Encoder (HEAE), a novel, human-centered AI framework for transparent and socially responsible depression severity assessment. Our approach uniquely integrates student narrative text with a teacher-derived, 9-dimensional "Empathy Vector" (EV), its dimensions guided by the PHQ-9 framework,to explicitly translate tacit empathetic insight into a structured AI input enhancing rather than replacing human judgment. Rigorous experiments optimized the multimodal fusion, text representation, and classification architecture, achieving 82.74% accuracy for 7-level severity classification. This work demonstrates a path toward more responsible and ethical affective computing by structurally embedding human empathy
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Exact Unlearning under Exposure: Extracting Forgotten Data under Exact Unlearning in Large Language Model</title>
<link>https://arxiv.org/abs/2505.24379</link>
<guid>https://arxiv.org/abs/2505.24379</guid>
<content:encoded><![CDATA[
arXiv:2505.24379v2 Announce Type: replace-cross 
Abstract: Large Language Models are typically trained on datasets collected from the web, which may inadvertently contain harmful or sensitive personal information. To address growing privacy concerns, unlearning methods have been proposed to remove the influence of specific data from trained models. Of these, exact unlearning -- which retrains the model from scratch without the target data -- is widely regarded the gold standard for mitigating privacy risks in deployment. In this paper, we revisit this assumption in a practical deployment setting where both the pre- and post-unlearning logits API are exposed, such as in open-weight scenarios. Targeting this setting, we introduce a novel data extraction attack that leverages signals from the pre-unlearning model to guide the post-unlearning model, uncovering patterns that reflect the removed data distribution. Combining model guidance with a token filtering strategy, our attack significantly improves extraction success rates -- doubling performance in some cases -- across common benchmarks such as MUSE, TOFU, and WMDP. Furthermore, we demonstrate our attack's effectiveness on a simulated medical diagnosis dataset to highlight real-world privacy risks associated with exact unlearning. In light of our findings, which suggest that unlearning may, in a contradictory way, increase the risk of privacy leakage during real-world deployments, we advocate for evaluation of unlearning methods to consider broader threat models that account not only for post-unlearning models but also for adversarial access to prior checkpoints. Code is publicly available at: https://github.com/Nicholas0228/unlearned_data_extraction_llm.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contextual Integrity in LLMs via Reasoning and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.04245</link>
<guid>https://arxiv.org/abs/2506.04245</guid>
<content:encoded><![CDATA[
arXiv:2506.04245v2 Announce Type: replace-cross 
Abstract: As the era of autonomous agents making decisions on behalf of users unfolds, ensuring contextual integrity (CI) -- what is the appropriate information to share while carrying out a certain task -- becomes a central question to the field. We posit that CI demands a form of reasoning where the agent needs to reason about the context in which it is operating. To test this, we first prompt LLMs to reason explicitly about CI when deciding what information to disclose. We then extend this approach by developing a reinforcement learning (RL) framework that further instills in models the reasoning necessary to achieve CI. Using a synthetic, automatically created, dataset of only $\sim700$ examples but with diverse contexts and information disclosure norms, we show that our method substantially reduces inappropriate information disclosure while maintaining task performance across multiple model sizes and families. Importantly, improvements transfer from this synthetic dataset to established CI benchmarks such as PrivacyLens that has human annotations and evaluates privacy leakage of AI assistants in actions and tool calls.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models</title>
<link>https://arxiv.org/abs/2506.07468</link>
<guid>https://arxiv.org/abs/2506.07468</guid>
<content:encoded><![CDATA[
arXiv:2506.07468v3 Announce Type: replace-cross 
Abstract: Conventional language model (LM) safety alignment relies on a reactive, disjoint procedure: attackers exploit a static model, followed by defensive fine-tuning to patch exposed vulnerabilities. This sequential approach creates a mismatch -- attackers overfit to obsolete defenses, while defenders perpetually lag behind emerging threats. To address this, we propose Self-RedTeam, an online self-play reinforcement learning algorithm where an attacker and defender agent co-evolve through continuous interaction. We cast safety alignment as a two-player zero-sum game, where a single model alternates between attacker and defender roles -- generating adversarial prompts and safeguarding against them -- while a reward LM adjudicates outcomes. This enables dynamic co-adaptation. Grounded in the game-theoretic framework of zero-sum games, we establish a theoretical safety guarantee which motivates the design of our method: if self-play converges to a Nash Equilibrium, the defender will reliably produce safe responses to any adversarial input. Empirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared to attackers trained against static defenders and achieves higher robustness on safety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained against static attackers. We further propose hidden Chain-of-Thought, allowing agents to plan privately, which boosts adversarial diversity and reduces over-refusals. Our results motivate a shift from reactive patching to proactive co-evolution in LM safety training, enabling scalable, autonomous, and robust self-improvement of LMs via multi-agent reinforcement learning (MARL).
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can We Infer Confidential Properties of Training Data from LLMs?</title>
<link>https://arxiv.org/abs/2506.10364</link>
<guid>https://arxiv.org/abs/2506.10364</guid>
<content:encoded><![CDATA[
arXiv:2506.10364v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are increasingly fine-tuned on domain-specific datasets to support applications in fields such as healthcare, finance, and law. These fine-tuning datasets often have sensitive and confidential dataset-level properties -- such as patient demographics or disease prevalence -- that are not intended to be revealed. While prior work has studied property inference attacks on discriminative models (e.g., image classification models) and generative models (e.g., GANs for image data), it remains unclear if such attacks transfer to LLMs. In this work, we introduce PropInfer, a benchmark task for evaluating property inference in LLMs under two fine-tuning paradigms: question-answering and chat-completion. Built on the ChatDoctor dataset, our benchmark includes a range of property types and task configurations. We further propose two tailored attacks: a prompt-based generation attack and a shadow-model attack leveraging word frequency signals. Empirical evaluations across multiple pretrained LLMs show the success of our attacks, revealing a previously unrecognized vulnerability in LLMs.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trainable Dynamic Mask Sparse Attention</title>
<link>https://arxiv.org/abs/2508.02124</link>
<guid>https://arxiv.org/abs/2508.02124</guid>
<content:encoded><![CDATA[
arXiv:2508.02124v4 Announce Type: replace-cross 
Abstract: In large language models, the demand for modeling long contexts is ever-increasing, yet the quadratic complexity of standard self-attention presents a significant bottleneck. While existing sparse attention mechanisms enhance efficiency, they often suffer from limitations such as static patterns and information loss. This paper introduces a Trainable Dynamic Mask Sparse Attention mechanism that addresses these challenges through three key innovations. First, it leverages value vectors to dynamically generate content-aware sparse masks, enabling the model to adaptively identify and focus on crucial information. Second, it implements a position-aware sparse attention computation that effectively skips unnecessary computational regions. Finally, we ensure that the introduced dynamic masks and sparse weights do not obstruct gradients, thereby supporting end-to-end training. This dual-sparsity design allows the model to retain complete information while significantly reducing computational complexity, achieving an excellent balance between efficiency and performance. We validate the performance of Dynamic Mask Attention through comprehensive experiments. Comparative studies demonstrate that our method consistently achieves Pareto dominance across various tasks, including scaling laws, multi-query associative recall, general benchmarks, and needle-in-a-haystack tests, delivering up to 10 times acceleration. These results highlight its capability to effectively balance model efficiency with long-context modeling. Our computational kernel is open-sourced at https://github.com/SmallDoges/flash-dmattn to facilitate further research and application within the community.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-Interactive Symbolic-Aided Chain-of-Thought for Logical Reasoning</title>
<link>https://arxiv.org/abs/2508.12425</link>
<guid>https://arxiv.org/abs/2508.12425</guid>
<content:encoded><![CDATA[
arXiv:2508.12425v2 Announce Type: replace-cross 
Abstract: This work introduces Symbolic-Aided Chain-of-Thought (CoT), an improved approach to standard CoT, for logical reasoning in large language models (LLMs). The key idea is to integrate lightweight symbolic representations into few-shot prompts, structuring the inference steps with a consistent strategy to make reasoning patterns more explicit within a non-interactive reasoning process. By incorporating these symbolic structures, Symbolic-Aided CoT preserves the generalizability of standard prompting techniques while enhancing the transparency, interpretability, and analyzability of LLM logical reasoning. Extensive experiments on four well-known logical reasoning benchmarks -- ProofWriter, FOLIO, ProntoQA, and LogicalDeduction, which cover diverse reasoning tasks and scenarios -- demonstrate the effectiveness of the proposed approach, particularly in complex reasoning tasks that require navigating multiple constraints or rules. Notably, Symbolic-Aided CoT consistently improves LLMs' reasoning capabilities across various model sizes and significantly outperforms conventional CoT on three out of four datasets, ProofWriter, ProntoQA, and LogicalDeduction.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Telephone Game: Evaluating Semantic Drift in Unified Models</title>
<link>https://arxiv.org/abs/2509.04438</link>
<guid>https://arxiv.org/abs/2509.04438</guid>
<content:encoded><![CDATA[
arXiv:2509.04438v2 Announce Type: replace-cross 
Abstract: Employing a single, unified model (UM) for both visual understanding (image-to-text: I2T) and visual generation (text-to-image: T2I) has opened a new direction in Visual Language Model (VLM) research. While UMs can also support broader unimodal tasks (e.g., text-to-text, image-to-image), we focus on the core cross-modal pair T2I and I2T. Existing evaluation benchmarks consider these capabilities in isolation: FID and GenEval for T2I, and benchmarks such as MME, MMBench for I2T. These isolated single-pass metrics do not reveal cross-consistency: whether a model that "understands" a concept can also "render" it, nor whether semantic meaning is preserved when cycling between image and text modalities. To address this, we introduce the Semantic Drift Protocol (SDP) for Unified Models, a cyclic evaluation protocol that alternates I2T and T2I over multiple generations to quantify semantic drift. We propose two metrics: (i) Mean Cumulative Drift (MCD), an embedding-based measure of overall semantic drift; and (ii) Multi-Generation GenEval (MGG), an object-level compliance score extending GenEval. To assess generalization beyond COCO dataset, which is widely used in training; we create a new benchmark Nocaps+Docci400, sampled from NoCaps and DOCCI and evaluated on seven recent models. SDP reveals substantial variation in cross-modal stability: some models like BAGEL maintain semantic meaning over many alternations, whereas others like VILA-U drift quickly despite strong single-pass scores. Our results highlight SDP as a necessary complement to standard I2T and T2I evaluations. Code is available at https://github.com/mollahsabbir/Semantic-Drift-in-Unified-Models
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory</title>
<link>https://arxiv.org/abs/2509.04439</link>
<guid>https://arxiv.org/abs/2509.04439</guid>
<content:encoded><![CDATA[
arXiv:2509.04439v3 Announce Type: replace-cross 
Abstract: While inference-time scaling enables LLMs to carry out increasingly long and capable reasoning traces, the patterns and insights uncovered during these traces are immediately discarded once the context window is reset for a new query. External memory is a natural way to persist these discoveries, and recent work has shown clear benefits for reasoning-intensive tasks. We see an opportunity to make such memories more broadly reusable and scalable by moving beyond instance-based memory entries (e.g. exact query/response pairs, or summaries tightly coupled with the original problem context) toward concept-level memory: reusable, modular abstractions distilled from solution traces and stored in natural language. For future queries, relevant concepts are selectively retrieved and integrated into the prompt, enabling test-time continual learning without weight updates. Our design introduces new strategies for abstracting takeaways from rollouts and retrieving entries for new queries, promoting reuse and allowing memory to expand with additional experiences. We evaluate on ARC-AGI, a benchmark that stresses compositional generalization and abstract reasoning, making it a natural fit for concept memory. Our method yields a 7.5% relative gain over a strong no-memory baseline with performance continuing to scale with inference compute. We find abstract concepts to be the most consistent memory design, outscoring the baseline at all tested inference compute scales. Moreover, dynamically updating memory during test-time outperforms fixed settings, supporting the hypothesis that accumulating and abstracting patterns enables further solutions in a form of self-improvement. Code is available at https://github.com/matt-seb-ho/arc_memo.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedMentor: Domain-Aware Differential Privacy for Heterogeneous Federated LLMs in Mental Health</title>
<link>https://arxiv.org/abs/2509.14275</link>
<guid>https://arxiv.org/abs/2509.14275</guid>
<content:encoded><![CDATA[
arXiv:2509.14275v2 Announce Type: replace-cross 
Abstract: Privacy-preserving adaptation of Large Language Models (LLMs) in sensitive domains (e.g., mental health) requires balancing strict confidentiality with model utility and safety. We propose FedMentor, a federated fine-tuning framework that integrates Low-Rank Adaptation (LoRA) and domain-aware Differential Privacy (DP) to meet per-domain privacy budgets while maintaining performance. Each client (domain) applies a custom DP noise scale proportional to its data sensitivity, and the server adaptively reduces noise when utility falls below a threshold. In experiments on three mental health datasets, we show that FedMentor improves safety over standard Federated Learning (FL) without privacy, raising safe output rates by up to three points and lowering toxicity, while maintaining utility (BERTScore F1 and ROUGE-L) within 0.5% of the non-private baseline and close to the centralized upper bound. The framework scales to backbones with up to 1.7B parameters on single-GPU clients, requiring < 173 MB of communication per-round. FedMentor demonstrates a practical approach to privately fine-tune LLMs for safer deployments in healthcare and other sensitive fields.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RL Grokking Recipe: How Does RL Unlock and Transfer New Algorithms in LLMs?</title>
<link>https://arxiv.org/abs/2509.21016</link>
<guid>https://arxiv.org/abs/2509.21016</guid>
<content:encoded><![CDATA[
arXiv:2509.21016v2 Announce Type: replace-cross 
Abstract: It remains an open question whether LLMs can acquire or generalize genuinely new reasoning strategies, beyond the sharpened skills encoded in their parameters during pre-training or post-training. To attempt to answer this debate, we introduce DELTA-Code -- Distributional Evaluation of Learnability and Transferrability in Algorithmic Coding -- a controlled benchmark of synthetic coding problem families designed to probe two fundamental aspects: learnability -- can LLMs, through reinforcement learning (RL), solve problem families where pretrained models exhibit failure with large enough attempts (pass@K=0)? -- and transferrability -- if learnability happens, can such skills transfer systematically to out-of-distribution (OOD) test sets? Unlike prior public coding datasets, DELTA isolates reasoning skills through templated problem generators and introduces fully OOD problem families that demand novel strategies rather than tool invocation or memorized patterns. Our experiments reveal a striking grokking phase transition: after an extended period with near-zero reward, RL-trained models abruptly climb to near-perfect accuracy. To enable learnability on previously unsolvable problem families, we explore key training ingredients such as staged warm-up with dense rewards, experience replay, curriculum training, and verification-in-the-loop. Beyond learnability, we use DELTA to evaluate transferability or generalization along exploratory, compositional, and transformative axes, as well as cross-family transfer. Results show solid gains within families and for recomposed skills, but persistent weaknesses in transformative cases. DELTA thus offers a clean testbed for probing the limits of RL-driven reasoning and for understanding how models can move beyond existing priors to acquire new algorithmic skills.
]]></content:encoded>
<pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Online Data to Enhance Medical Knowledge in a Small Persian Language Model</title>
<link>https://arxiv.org/abs/2505.16000</link>
<guid>https://arxiv.org/abs/2505.16000</guid>
<content:encoded><![CDATA[
<div> Dataset, Persian language, medical question answering, fine-tuning, AI applications<br />
<br />
Summary: 
This study introduces a newly curated dataset of 20k doctor-patient Q&amp;A pairs and a corpus from medical magazines for Persian language models. It used a parameter-efficient fine-tuning approach to enhance the baseline model's medical knowledge, aya-expanse-8b. The fine-tuned model improved accuracy in medical question answering and passed the Iranian Basic Medical Science Entrance Exam (IBSEE) in September 2023. Additionally, it enhanced Persian-translated MMLU accuracy by 2.67%. The research demonstrates the potential of using open-access online data to enrich small language models in medical fields and provides a solution for Persian medical AI applications in resource-constrained environments. The study suggests exploring multimodal input for further performance enhancement. <br /><br /> <div>
arXiv:2505.16000v4 Announce Type: replace 
Abstract: The rapid advancement of language models has demonstrated the potential of artificial intelligence in the healthcare industry. However, small language models struggle with specialized domains in low-resource languages like Persian. While numerous medical-domain websites exist in Persian, no curated dataset or corpus has been available making ours the first of its kind. This study introduces a newly curated dataset comprising 20k doctor-patient Q\&amp;A pairs and 60\% of a 90-million-token crawled corpus from medical magazines. Using a parameter-efficient fine-tuning approach, we enhanced the medical knowledge of the baseline model, aya-expanse-8b. Benchmark evaluations demonstrate that the fine-tuned model achieves improved accuracy in medical question answering and successfully passed the Iranian Basic Medical Science Entrance Exam (IBSEE) in September 2023, which the baseline model did not. Additionally, the fine-tuned model improved Persian-translated MMLU accuracy by an average of 2.67\%. This work highlights the potential of leveraging open-access online data to enrich small language models in medical fields, providing a novel solution for Persian medical AI applications suitable for resource-constrained environments. Future research could explore multimodal input to further enhance performance.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hallucination reduction with CASAL: Contrastive Activation Steering For Amortized Learning</title>
<link>https://arxiv.org/abs/2510.02324</link>
<guid>https://arxiv.org/abs/2510.02324</guid>
<content:encoded><![CDATA[
<div> steering, large language models, activation, amortized learning, hallucinations  
Summary:  
Contrastive Activation Steering for Amortized Learning (CASAL) is introduced as an algorithm to reduce hallucinations in Large Language Models (LLMs) without the need for real-time monitoring. CASAL connects interpretability with amortized optimization by embedding activation steering benefits into the model's weights during training. This approach results in LLMs abstaining from confidently providing incorrect answers to questions they do not know. CASAL shows a 30%-40% reduction in hallucination across various short-form QA benchmarks, with significantly higher efficiency compared to existing baselines such as LoRA-based methods. It is also demonstrated to be effective in out-of-distribution domains, showcasing its flexibility in text-only and vision-language models. CASAL is the first steering-based training method proven to work for both dense and Mixture-of-Experts (MoE) models, making it a promising advancement for practical deployment in production systems.  
<br /><br />Summary: <div>
arXiv:2510.02324v1 Announce Type: new 
Abstract: Large Language Models (LLMs) exhibit impressive capabilities but often hallucinate, confidently providing incorrect answers instead of admitting ignorance. Prior work has shown that models encode linear representations of their own knowledge and that activation steering can reduce hallucinations. These approaches, however, require real-time monitoring and intervention during inference. We introduce Contrastive Activation Steering for Amortized Learning (CASAL), an efficient algorithm that connects interpretability with amortized optimization. CASAL directly bakes the benefits of activation steering into model's weights. Once trained, LLMs answer questions they know while abstaining from answering those they do not. CASAL's light-weight design requires training only a submodule of a single transformer layer and yet reduces hallucination by 30%-40% across multiple short-form QA benchmarks. CASAL is 30x more compute-efficient and 20x more data-efficient than strong LoRA-based baselines such as SFT and DPO, boosting its practical applicability in data scarce domains. Importantly, CASAL also generalizes effectively to out-of-distribution (OOD) domains. We showcase CASAL's flexibility in mitigating hallucinations in both text-only and vision-language models. To our knowledge, CASAL is the first steering-based training method that has been shown to be effective for both dense and Mixture-of-Experts (MoE) models. CASAL represents a promising step forward for applying interpretability-inspired method for practical deployment in production systems.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hallucination-Resistant, Domain-Specific Research Assistant with Self-Evaluation and Vector-Grounded Retrieval</title>
<link>https://arxiv.org/abs/2510.02326</link>
<guid>https://arxiv.org/abs/2510.02326</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, research assistant, photonics, knowledge base, citation pipeline

Summary: 
RA-FSM is a research assistant system that enhances the capabilities of large language models by incorporating a finite-state control loop for improved relevance, confidence, and knowledge in literature synthesis. Grounded in vector retrieval and a deterministic citation pipeline, the system efficiently filters queries, scores answerability, and utilizes a ranked-tier ingestion workflow to construct a domain knowledge base in the field of photonics. Evaluation on various task categories shows that domain experts prefer RA-FSM over other models due to its stronger boundary-condition handling and more defensible evidence use. The system explores beyond traditional models while maintaining tunable latency and cost overheads. Overall, RA-FSM provides transparent, well-cited answers suitable for high-stakes technical work and can be easily adapted to other scientific domains.<br /><br />Summary: <div>
arXiv:2510.02326v1 Announce Type: new 
Abstract: Large language models accelerate literature synthesis but can hallucinate and mis-cite, limiting their usefulness in expert workflows. We present RA-FSM (Research Assistant - Finite State Machine), a modular GPT-based research assistant that wraps generation in a finite-state control loop: Relevance -> Confidence -> Knowledge. The system is grounded in vector retrieval and a deterministic citation pipeline. The controller filters out-of-scope queries, scores answerability, decomposes questions, and triggers retrieval only when needed, and emits answers with confidence labels and in-corpus, de-duplicated references. A ranked-tier ingestion workflow constructs a domain knowledge base from journals, conferences, indices, preprints, and patents, writing both to a dense vector index and to a relational store of normalized metrics. We implement the system for photonics and evaluate it on six task categories: analytical reasoning, numerical analysis, methodological critique, comparative synthesis, factual extraction, and application design. In blinded A/B reviews, domain experts prefer RA-FSM to both a strong Notebook LM (NLM) and a vanilla Default GPT API call single-pass baseline, citing stronger boundary-condition handling and more defensible evidence use. Coverage and novelty analyses indicate that RA-FSM explores beyond the NLM while incurring tunable latency and cost overheads. The design emphasizes transparent, well-cited answers for high-stakes technical work and is generalizable to other scientific domains.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KAME: Tandem Architecture for Enhancing Knowledge in Real-Time Speech-to-Speech Conversational AI</title>
<link>https://arxiv.org/abs/2510.02327</link>
<guid>https://arxiv.org/abs/2510.02327</guid>
<content:encoded><![CDATA[
<div> Transformer, speech-to-speech, Large Language Model, hybrid architecture, real-time

Summary:
- The paper introduces a novel hybrid architecture that combines real-time speech-to-speech (S2S) models with a powerful Large Language Model (LLM) backend for improved knowledge representation.
- User speech is processed through an S2S transformer for immediate responsiveness, while the query is simultaneously sent to the LLM for a text-based response.
- The LLM's response guides the S2S model's speech generation, enhancing the output with rich knowledge without significantly increasing latency.
- The system outperforms a baseline S2S model in response correctness, approaching the performance of a cascaded system, while maintaining latency comparable to the baseline. 

<br /><br />Summary: <div>
arXiv:2510.02327v1 Announce Type: new 
Abstract: Real-time speech-to-speech (S2S) models excel at generating natural, low-latency conversational responses but often lack deep knowledge and semantic understanding. Conversely, cascaded systems combining automatic speech recognition, a text-based Large Language Model (LLM), and text-to-speech synthesis offer superior knowledge representation at the cost of high latency, which disrupts the flow of natural interaction. This paper introduces a novel hybrid architecture that bridges the gap between these two paradigms. Our framework processes user speech through an S2S transformer for immediate responsiveness while concurrently relaying the query to a powerful back-end LLM. The LLM's text-based response is then injected in real time to guide the S2S model's speech generation, effectively infusing its output with rich knowledge without the full latency penalty of a cascaded system. We evaluated our method using a speech-synthesized variant of the MT-Bench benchmark that consists of multi-turn question-answering sessions. The results demonstrate that our system substantially outperforms a baseline S2S model in response correctness, approaching that of a cascaded system, while maintaining a latency on par with the baseline.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMANDA: Agentic Medical Knowledge Augmentation for Data-Efficient Medical Visual Question Answering</title>
<link>https://arxiv.org/abs/2510.02328</link>
<guid>https://arxiv.org/abs/2510.02328</guid>
<content:encoded><![CDATA[
<div> Keywords: Medical Multimodal Large Language Models, Med-VQA, AMANDA, medical reasoning, low-resource settings<br />
Summary:<br />
The article introduces AMANDA, a framework designed to enhance medical reasoning in low-resource settings for medical visual question answering. Existing Med-MLLMs often struggle in these settings due to reasoning bottlenecks. AMANDA addresses these limitations with training-free medical knowledge augmentation through LLM agents. This augmentation includes both intrinsic, focusing on question decomposition for improved diagnosis, and extrinsic, incorporating biomedical knowledge graph retrieval for grounded reasoning. Experimental results across multiple Med-VQA benchmarks show significant improvements in zero-shot and few-shot scenarios. The framework's code is available for public access, providing a useful resource for the medical community. <div>
arXiv:2510.02328v1 Announce Type: new 
Abstract: Medical Multimodal Large Language Models (Med-MLLMs) have shown great promise in medical visual question answering (Med-VQA). However, when deployed in low-resource settings where abundant labeled data are unavailable, existing Med-MLLMs commonly fail due to their medical reasoning capability bottlenecks: (i) the intrinsic reasoning bottleneck that ignores the details from the medical image; (ii) the extrinsic reasoning bottleneck that fails to incorporate specialized medical knowledge. To address those limitations, we propose AMANDA, a training-free agentic framework that performs medical knowledge augmentation via LLM agents. Specifically, our intrinsic medical knowledge augmentation focuses on coarse-to-fine question decomposition for comprehensive diagnosis, while extrinsic medical knowledge augmentation grounds the reasoning process via biomedical knowledge graph retrieval. Extensive experiments across eight Med-VQA benchmarks demonstrate substantial improvements in both zero-shot and few-shot Med-VQA settings. The code is available at https://github.com/REAL-Lab-NU/AMANDA.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SelfJudge: Faster Speculative Decoding via Self-Supervised Judge Verification</title>
<link>https://arxiv.org/abs/2510.02329</link>
<guid>https://arxiv.org/abs/2510.02329</guid>
<content:encoded><![CDATA[
<div> Keywords: Speculative decoding, LLM inference, judge decoding, self-supervision, NLP tasks

Summary:
SelfJudge introduces a novel approach for accelerating Large Language Model (LLM) inference by training judge verifiers through self-supervision of the target model. This method assesses semantic preservation by evaluating whether token-substituted responses maintain the original meaning, enabling automatic verifier training across diverse Natural Language Processing (NLP) tasks. The proposed SelfJudge outperforms judge decoding baselines in terms of inference-accuracy trade-offs, offering a broadly applicable solution for faster LLM inference. By leveraging self-supervision, SelfJudge eliminates the reliance on human annotations and verifiable ground truths, enhancing generalizability across various NLP tasks. This approach allows for the verification of candidate tokens from a draft model against a larger target model, with the flexibility to accept minor discrepancies in output. Overall, SelfJudge provides a promising solution for improving the efficiency and accuracy of LLM inference. 

<br /><br />Summary: <div>
arXiv:2510.02329v1 Announce Type: new 
Abstract: Speculative decoding accelerates LLM inference by verifying candidate tokens from a draft model against a larger target model. Recent judge decoding boosts this process by relaxing verification criteria by accepting draft tokens that may exhibit minor discrepancies from target model output, but existing methods are restricted by their reliance on human annotations or tasks with verifiable ground truths, limiting generalizability across diverse NLP tasks. We propose SelfJudge, which trains judge verifiers via self-supervision of the target model. Our method measures semantic preservation by assessing whether token-substituted responses preserve the meaning of original responses, enabling automatic verifier training across diverse NLP tasks. Our experiments show SelfJudge achieves superior inference-accuracy trade-offs than judge decoding baselines, offering a broadly applicable solution for faster LLM inference.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EntropyLong: Effective Long-Context Training via Predictive Uncertainty</title>
<link>https://arxiv.org/abs/2510.02330</link>
<guid>https://arxiv.org/abs/2510.02330</guid>
<content:encoded><![CDATA[
<div> Keywords: long-context language models, data construction, predictive uncertainty, entropy-based verification, long-range dependencies 

Summary:
EntropyLong introduces a novel approach to training long-context language models by utilizing predictive uncertainty to validate the quality of long-range dependencies. By identifying high-entropy positions in documents and retrieving semantically relevant contexts from large corpora, EntropyLong ensures that the dependencies represent genuine information gain rather than spurious correlation. The constructed training samples consist of original documents combined with verified contextual supplements, leading to significant improvements on RULER benchmarks and enhanced understanding of long-context information. Models trained using EntropyLong also demonstrate substantial gains on LongBenchv2 after instruction fine-tuning. Ablation studies further support the effectiveness of entropy-based verification in long-context training, highlighting the importance of ensuring the quality of dependencies for capturing distant information effectively. 

<br /><br />Summary: <div>
arXiv:2510.02330v1 Announce Type: new 
Abstract: Training long-context language models to capture long-range dependencies requires specialized data construction. Current approaches, such as generic text concatenation or heuristic-based variants, frequently fail to guarantee genuine long-range dependencies. We propose EntropyLong, a novel data construction method that leverages predictive uncertainty to verify dependency quality. Our approach identifies high-entropy positions in documents, retrieves semantically relevant contexts from large corpora, and verifies their utility by assessing whether they reduce prediction entropy. This model-in-the-loop verification ensures each dependency represents measurable information gain rather than spurious correlation. We construct training samples with long-range dependencies by combining original documents with these verified contextual supplements. Using FineWebEdu and Cosmopedia, we generate a dataset of 128K-length sequences with verified dependencies. Models trained on this data demonstrate significant improvements on RULER benchmarks, particularly in tasks requiring distant information. Following instruction fine-tuning, our models also achieve substantial gains on LongBenchv2, demonstrating enhanced long-context understanding. Extensive ablation studies further validate the necessity and effectiveness of entropybased verification for long-context training.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Dialogue Generation for Interactive Conversational Elicitation &amp; Recommendation (ICER)</title>
<link>https://arxiv.org/abs/2510.02331</link>
<guid>https://arxiv.org/abs/2510.02331</guid>
<content:encoded><![CDATA[
<div> generating, dialogues, user simulator, conversational recommender systems, fine-tuning 
Summary:
Generating natural dialogues for conversational recommender systems (CRSs) using language models (LMs) poses challenges due to the lack of public data. To address this, a methodology is developed using behavior simulators and LM-prompting to create dialogues consistent with the user's underlying state. This approach results in a large, open-source CRS dataset with preference elicitation and critiquing examples. Rater evaluation shows the generated dialogues exhibit consistency, factuality, and naturalness. Through the integration of user simulators and LM-driven prompting, the methodology enhances the quality of generated dialogues for CRSs. 
<br /><br />Summary: <div>
arXiv:2510.02331v1 Announce Type: new 
Abstract: While language models (LMs) offer great potential for conversational recommender systems (CRSs), the paucity of public CRS data makes fine-tuning LMs for CRSs challenging. In response, LMs as user simulators qua data generators can be used to train LM-based CRSs, but often lack behavioral consistency, generating utterance sequences inconsistent with those of any real user. To address this, we develop a methodology for generating natural dialogues that are consistent with a user's underlying state using behavior simulators together with LM-prompting. We illustrate our approach by generating a large, open-source CRS data set with both preference elicitation and example critiquing. Rater evaluation on some of these dialogues shows them to exhibit considerable consistency, factuality and naturalness.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A High-Capacity and Secure Disambiguation Algorithm for Neural Linguistic Steganography</title>
<link>https://arxiv.org/abs/2510.02332</link>
<guid>https://arxiv.org/abs/2510.02332</guid>
<content:encoded><![CDATA[
<div> Neural linguistic steganography, SyncPool method, look-ahead Sync method, embedding capacity, provable security guarantees<br />
<br />
Summary: 
The article discusses neural linguistic steganography techniques aiming to embed information into natural text while maintaining statistical undetectability. Tokenization ambiguity in modern tokenizers poses a challenge, leading to decoding failures. The SyncPool method addresses this issue but sacrifices embedding capacity. The proposed look-ahead Sync method overcomes this limitation by maximizing embedding capacity while ensuring provable security. The approach strategically samples synchronized token sequences, optimizing embedding capacity without sacrificing security. The method's theoretical proofs validate its security guarantees. Experiments on English and Chinese benchmarks demonstrate the approach's superior performance over SyncPool, with an increase in embedding rate of over 160% in English and 25% in Chinese settings with larger candidate pools. This work represents a significant advancement towards practical, high-capacity, and provably secure linguistic steganography.<br /><br /> <div>
arXiv:2510.02332v1 Announce Type: new 
Abstract: Neural linguistic steganography aims to embed information
  into natural text while preserving statistical undetectability. A fundamental challenge in this ffeld stems from tokenization ambiguity in modern tokenizers, which can lead to catastrophic decoding failures. The recent method, SyncPool, addresses this ambiguity
  by employing a coarse-grained synchronization mechanism over groups of ambiguous candidates. However, SyncPool sacriffces embedding capacity, as it utilizes the entire Shannon entropy of an ambiguous group solely for synchronization rather than for payload embedding. We propose a method named look-ahead Sync, which overcomes the capacity limitation of SyncPool while retaining its provable security guarantees. Our approach performs minimal synchronized sampling only on truly indistinguishable token sequences, while strategically preserving all other discernible paths to maximize embedding capacity. We provide theoretical proofs for the security of our method and analyze the gap between its achievable embedding capacity and the theoretical upper bound. Experiments on English (using Llama 3) and Chinese (using Qwen 2.5) benchmarks show that our method consistently approaches the theoretical capacity upper bound and signiffcantly outperforms SyncPool. The improvement in embedding rate exceeds 160% in English and 25% in Chinese, particularly in settings with larger candidate pools. This work represents a signiffcant step toward practical high-capacity provably secure linguistic steganography.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human Mobility Datasets Enriched With Contextual and Social Dimensions</title>
<link>https://arxiv.org/abs/2510.02333</link>
<guid>https://arxiv.org/abs/2510.02333</guid>
<content:encoded><![CDATA[
<div> Keywords: human trajectories, datasets, semantically enriched, Large Language Models, mobility analysis

Summary:
The paper introduces two datasets of semantically enriched human trajectories sourced from GPS traces on OpenStreetMap. These datasets include contextual layers such as stops, moves, points of interest, inferred transportation modes, and weather data. Additionally, synthetic social media posts generated by Large Language Models (LLMs) are included, enabling multimodal and semantic mobility analysis. The datasets are available in tabular and RDF formats, supporting semantic reasoning and FAIR data practices. Covering Paris and New York, the datasets can be customized using an open-source reproducible pipeline. This resource allows for behavior modeling, mobility prediction, knowledge graph construction, and LLM-based applications. Notably, this is the first resource to combine real-world movement, semantic enrichment, LLM-generated text, and semantic web compatibility in a reusable framework. 

<br /><br />Summary: <div>
arXiv:2510.02333v1 Announce Type: new 
Abstract: In this resource paper, we present two publicly available datasets of semantically enriched human trajectories, together with the pipeline to build them. The trajectories are publicly available GPS traces retrieved from OpenStreetMap. Each dataset includes contextual layers such as stops, moves, points of interest (POIs), inferred transportation modes, and weather data. A novel semantic feature is the inclusion of synthetic, realistic social media posts generated by Large Language Models (LLMs), enabling multimodal and semantic mobility analysis. The datasets are available in both tabular and Resource Description Framework (RDF) formats, supporting semantic reasoning and FAIR data practices. They cover two structurally distinct, large cities: Paris and New York. Our open source reproducible pipeline allows for dataset customization, while the datasets support research tasks such as behavior modeling, mobility prediction, knowledge graph construction, and LLM-based applications. To our knowledge, our resource is the first to combine real-world movement, structured semantic enrichment, LLM-generated text, and semantic web compatibility in a reusable framework.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation Gradient Tracing</title>
<link>https://arxiv.org/abs/2510.02334</link>
<guid>https://arxiv.org/abs/2510.02334</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, AI safety, undesirable behaviors, representation analysis, token-level analysis

Summary:
Large Language Models (LLMs) have shown impressive capabilities but often exhibit undesirable behaviors such as generating harmful content, factual inaccuracies, and societal biases. The challenge lies in identifying the root causes of these failures for AI safety. Traditional attribution methods based on parameter gradients have limitations due to noisy signals and computational complexity. In response, a novel framework is introduced to diagnose a range of undesirable LLM behaviors by analyzing representations and gradients in the activation space. The approach proves effective in tasks like tracking harmful content, detecting backdoor poisoning, and identifying knowledge contamination. Not only does it excel in sample-level attribution, but it also enables precise token-level analysis to pinpoint specific samples and phrases influencing model behavior. This work offers a valuable diagnostic tool for understanding, auditing, and mitigating risks associated with LLMs. <div>
arXiv:2510.02334v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities, yet their deployment is frequently undermined by undesirable behaviors such as generating harmful content, factual inaccuracies, and societal biases. Diagnosing the root causes of these failures poses a critical challenge for AI safety. Existing attribution methods, particularly those based on parameter gradients, often fall short due to prohibitive noisy signals and computational complexity. In this work, we introduce a novel and efficient framework that diagnoses a range of undesirable LLM behaviors by analyzing representation and its gradients, which operates directly in the model's activation space to provide a semantically meaningful signal linking outputs to their training data. We systematically evaluate our method for tasks that include tracking harmful content, detecting backdoor poisoning, and identifying knowledge contamination. The results demonstrate that our approach not only excels at sample-level attribution but also enables fine-grained token-level analysis, precisely identifying the specific samples and phrases that causally influence model behavior. This work provides a powerful diagnostic tool to understand, audit, and ultimately mitigate the risks associated with LLMs. The code is available at https://github.com/plumprc/RepT.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FormalML: A Benchmark for Evaluating Formal Subgoal Completion in Machine Learning Theory</title>
<link>https://arxiv.org/abs/2510.02335</link>
<guid>https://arxiv.org/abs/2510.02335</guid>
<content:encoded><![CDATA[
<div> theorem proving, large language models, subgoal completion, FormalML benchmark, machine learning

Summary: 
Large language models have shown great progress in theorem proving but their capability as mathematical assistants remains largely unexplored. The challenge of subgoal completion, where LLMs help resolve proof obligations in complex proofs, is identified. A benchmark called FormalML is introduced, comprised of 4937 problems in optimization and probability inequalities. These problems vary in difficulty and are presented in research-level contexts, requiring both premise retrieval and complex reasoning. Evaluation of current provers shows limitations in accuracy and efficiency, highlighting the need for more capable LLM-based theorem provers for effective subgoal completion. <div>
arXiv:2510.02335v1 Announce Type: new 
Abstract: Large language models (LLMs) have recently demonstrated remarkable progress in formal theorem proving. Yet their ability to serve as practical assistants for mathematicians, filling in missing steps within complex proofs, remains underexplored. We identify this challenge as the task of subgoal completion, where an LLM must discharge short but nontrivial proof obligations left unresolved in a human-provided sketch. To study this problem, we introduce FormalML, a Lean 4 benchmark built from foundational theories of machine learning. Using a translation tactic that converts procedural proofs into declarative form, we extract 4937 problems spanning optimization and probability inequalities, with varying levels of difficulty. FormalML is the first subgoal completion benchmark to combine premise retrieval and complex research-level contexts. Evaluation of state-of-the-art provers highlights persistent limitations in accuracy and efficiency, underscoring the need for more capable LLM-based theorem provers for effective subgoal completion,
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KurdSTS: The Kurdish Semantic Textual Similarity</title>
<link>https://arxiv.org/abs/2510.02336</link>
<guid>https://arxiv.org/abs/2510.02336</guid>
<content:encoded><![CDATA[
<div> Semantic Textual Similarity, Kurdish, STS dataset, Sentence-BERT, low-resource NLP
Summary: 
Semantic Textual Similarity (STS) is vital for NLP tasks but lacks resources for low-resource languages like Kurdish. The first Kurdish STS dataset, comprising 10,000 annotated sentence pairs, was introduced. The dataset covers formal and informal registers. Strong baselines including Sentence-BERT and multilingual BERT were benchmarked, yielding competitive results. Challenges specific to Kurdish, such as morphology, orthographic variation, and code-mixing, were highlighted. The dataset and baselines create a reproducible evaluation suite and serve as a foundation for future research on Kurdish semantics and low-resource NLP. <br /><br />Summary: <div>
arXiv:2510.02336v1 Announce Type: new 
Abstract: Semantic Textual Similarity (STS) measures the degree of meaning overlap between two texts and underpins many NLP tasks. While extensive resources exist for high-resource languages, low-resource languages such as Kurdish remain underserved. We present, to our knowledge, the first Kurdish STS dataset: 10,000 sentence pairs spanning formal and informal registers, each annotated for similarity. We benchmark Sentence-BERT, multilingual BERT, and other strong baselines, obtaining competitive results while highlighting challenges arising from Kurdish morphology, orthographic variation, and code-mixing. The dataset and baselines establish a reproducible evaluation suite and provide a strong starting point for future research on Kurdish semantics and low-resource NLP.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRACQ: A Multi-Dimensional Approach To Automated Document Assessment</title>
<link>https://arxiv.org/abs/2510.02337</link>
<guid>https://arxiv.org/abs/2510.02337</guid>
<content:encoded><![CDATA[
<div> Evaluation Framework, Machine-generated text, Automated Evaluation, Trait-based analysis, Artificial intelligence<br />
<br />
Summary: 
This paper introduces the CRACQ framework, designed to evaluate documents based on Coherence, Rigor, Appropriateness, Completeness, and Quality. Unlike single-score methods, CRACQ considers linguistic, semantic, and structural signals to provide both holistic and trait-level assessments of machine-generated text. Trained on synthetic grant proposals and tested on real applications, CRACQ demonstrates more stable and interpretable trait-level judgments compared to direct human evaluation. While promising, challenges in reliability and domain specificity still need to be addressed. <div>
arXiv:2510.02337v1 Announce Type: new 
Abstract: This paper presents CRACQ, a multi-dimensional evaluation framework tailored to evaluate documents across f i v e specific traits: Coherence, Rigor, Appropriateness, Completeness, and Quality. Building on insights from traitbased Automated Essay Scoring (AES), CRACQ expands its fo-cus beyond essays to encompass diverse forms of machine-generated text, providing a rubricdriven and interpretable methodology for automated evaluation. Unlike singlescore approaches, CRACQ integrates linguistic, semantic, and structural signals into a cumulative assessment, enabling both holistic and trait-level analysis. Trained on 500 synthetic grant pro-posals, CRACQ was benchmarked against an LLM-as-a-judge and further tested on both strong and weak real applications. Preliminary results in-dicate that CRACQ produces more stable and interpretable trait-level judgments than direct LLM evaluation, though challenges in reliability and domain scope remain
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Long-Form Clinical Text Generation with Claim-Based Rewards</title>
<link>https://arxiv.org/abs/2510.02338</link>
<guid>https://arxiv.org/abs/2510.02338</guid>
<content:encoded><![CDATA[
<div> Keywords: clinical documentation, language models, reinforcement learning, factual grounding, completeness

Summary:
Automating clinical documentation using large language models requires precise alignment with priorities such as completeness and factual grounding. A new evaluation-integrated reinforcement learning framework for long-form clinical text generation has been developed, utilizing Group Relative Policy Optimization (GRPO) coupled with DocLens, a claim-level evaluator that offers deterministic rewards based on dialogue-grounded assessments. This method focuses on optimizing factual grounding and completeness without the need for a separate reward model or human-authored references. Empirical results demonstrate improved clinical note quality and reduced training costs through a straightforward reward-gating strategy. An independent evaluation of GPT-5 confirms the enhanced factuality, completeness, and brevity of outputs generated by the GRPO approach, with fewer omissions and hallucinations. These improvements, while significant, are likely a conservative estimate given the baseline model's alignment with benchmarks. The framework is adaptable to real-world applications and can integrate custom objectives such as guideline adherence or billing preferences.<br /><br />Summary: <div>
arXiv:2510.02338v1 Announce Type: new 
Abstract: Automating clinical documentation with large language models requires precise alignment with priorities such as completeness and factual grounding. We present an evaluation-integrated reinforcement learning framework for long-form clinical text generation that couples Group Relative Policy Optimization (GRPO) with DocLens, a claim-level evaluator that provides deterministic, dialogue-grounded rewards. Our method directly optimizes factual grounding and completeness without training a separate reward model or relying on human-authored references. Empirically, the approach improves clinical note quality and reduces training cost via a simple reward-gating strategy. An independent GPT-5 qualitative evaluation further supports these gains, showing higher preference for GRPO outputs in factuality, completeness, and brevity, with fewer omissions and hallucinations. Because the benchmarks are relatively clean and the base model already well aligned, these improvements likely represent a conservative lower bound. The framework is scalable to real-world settings and can incorporate custom objectives such as guideline adherence or billing preferences.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Uncertainty Quantification Methods in Argumentative Large Language Models</title>
<link>https://arxiv.org/abs/2510.02339</link>
<guid>https://arxiv.org/abs/2510.02339</guid>
<content:encoded><![CDATA[
<div> Keywords: uncertainty quantification, large language models, argumentative LLMs, claim verification tasks, UQ methods <br />
<br />
Summary: Research focuses on uncertainty quantification (UQ) for large language models (LLMs) to ensure their reliability. The study integrates UQ methods into argumentative LLMs (ArgLLMs), a framework for decision-making based on computational argumentation. Experiments evaluate ArgLLMs' performance on claim verification tasks using various UQ methods. The study's novel experimental approach assesses the effectiveness of UQ methods, especially in handling complex and contentious statements. Results show that direct prompting is a highly effective UQ strategy in ArgLLMs, surpassing more complex approaches. This suggests that simplicity can be key in UQ for ArgLLMs and potentially other LLM applications. <div>
arXiv:2510.02339v1 Announce Type: new 
Abstract: Research in uncertainty quantification (UQ) for large language models (LLMs) is increasingly important towards guaranteeing the reliability of this groundbreaking technology. We explore the integration of LLM UQ methods in argumentative LLMs (ArgLLMs), an explainable LLM framework for decision-making based on computational argumentation in which UQ plays a critical role. We conduct experiments to evaluate ArgLLMs' performance on claim verification tasks when using different LLM UQ methods, inherently performing an assessment of the UQ methods' effectiveness. Moreover, the experimental procedure itself is a novel way of evaluating the effectiveness of UQ methods, especially when intricate and potentially contentious statements are present. Our results demonstrate that, despite its simplicity, direct prompting is an effective UQ strategy in ArgLLMs, outperforming considerably more complex approaches.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Prompts Rewind Time for LLMs? Evaluating the Effectiveness of Prompted Knowledge Cutoffs</title>
<link>https://arxiv.org/abs/2510.02340</link>
<guid>https://arxiv.org/abs/2510.02340</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, temporal prediction, knowledge cutoff, prompting, forgetting

Summary:
Large Language Models (LLMs) are often used for temporal prediction tasks but their reliance on pretraining data can lead to contamination concerns. This study explores the use of prompting to simulate an earlier knowledge cutoff in LLMs. Three evaluation datasets were created to assess the LLMs' ability to forget direct factual knowledge, semantic shifts, and causally related knowledge. Results indicate that while prompt-based simulated knowledge cutoffs are effective when queried directly, they struggle to induce forgetting of information that is not directly asked but causally related. This suggests a need for more rigorous evaluation settings when using LLMs for temporal prediction tasks. The full dataset and evaluation code are available for further exploration. <div>
arXiv:2510.02340v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are widely used for temporal prediction, but their reliance on pretraining data raises contamination concerns, as accurate predictions on pre-cutoff test data may reflect memorization rather than reasoning, leading to an overestimation of their generalization capability. With the recent emergence of prompting-based unlearning techniques, a natural question arises: Can LLMs be prompted to simulate an earlier knowledge cutoff? In this work, we investigate the capability of prompting to simulate earlier knowledge cutoff in LLMs. We construct three evaluation datasets to assess the extent to which LLMs can forget (1) direct factual knowledge, (2) semantic shifts, and (3) causally related knowledge. Results demonstrate that while prompt-based simulated knowledge cutoffs show effectiveness when directly queried with the information after that date, they struggle to induce forgetting when the forgotten content is not directly asked but causally related to the query. These findings highlight the need for more rigorous evaluation settings when applying LLMs for temporal prediction tasks. The full dataset and evaluation code are available at https://github.com/gxx27/time_unlearn.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning</title>
<link>https://arxiv.org/abs/2510.02341</link>
<guid>https://arxiv.org/abs/2510.02341</guid>
<content:encoded><![CDATA[
<div> Keywords: large language model, preference learning, dissatisfaction signals, iterative training, post-training

Summary:
In this paper, the authors introduce DRIFT (Dissatisfaction-Refined Iterative Preference Training), a novel approach for training large language models using real-world dissatisfaction signals. Unlike existing methods that rely on costly human annotations or assume abundant positive feedback, DRIFT anchors training on implicit user dissatisfaction signals and samples positive feedback dynamically from the evolving model policy. Empirical results demonstrate that DRIFT outperforms strong baseline methods and achieves significant improvements in model performance on both real-world and synthetic datasets. In particular, at larger scales, DRIFT-trained models surpass state-of-the-art models like GPT-4o-mini. Furthermore, DRIFT preserves exploratory capacity, leading to more diverse and high-reward solutions. Theoretical analysis shows that the design of DRIFT maintains preference margins and prevents gradient degeneration. Overall, DRIFT is shown to be an effective and scalable approach for training large language models using abundant and informative dissatisfaction signals. The code and data for DRIFT are available on GitHub for further exploration. 

<br /><br />Summary: <div>
arXiv:2510.02341v1 Announce Type: new 
Abstract: Real-world large language model deployments (e.g., conversational AI systems, code generation assistants) naturally generate abundant implicit user dissatisfaction (DSAT) signals, as users iterate toward better answers through refinements, corrections, and expressed preferences, while explicit satisfaction (SAT) feedback is scarce. Existing preference learning approaches are poorly aligned with this data profile, as they rely on costly human annotations or assume plentiful positive responses. In this paper, we introduce \textbf{DRIFT} (\textbf{D}issatisfaction-\textbf{R}efined \textbf{I}terative pre\textbf{F}erence \textbf{T}raining), which anchors training on real-world DSAT signals and samples positives dynamically from the evolving policy. Empirically, DRIFT models trained on real-world \textit{WildFeedback} datasets and synthetic \textit{UltraFeedback} datasets achieve up to +6.23\% (7B) / +7.61\% (14B) on WildBench Task Score and up to +8.95\% (7B) / +12.29\% (14B) on AlpacaEval2 win rate over base models, outperforming strong baseline methods such as iterative DPO and SPIN. At larger scales, the improvements are particularly pronounced: 14B models trained with DRIFT surpass GPT-4o-mini on WildBench. Further analysis shows that DRIFT also preserves exploratory capacity, yielding more diverse high-reward solutions rather than collapsing to narrow subsets. Theoretically, we demonstrate that this design preserves preference margins and avoids the gradient degeneration. These results show that DRIFT is an effective and scalable recipe for real-world post-training that leverages the most abundant and informative signal. The code and data are available at https://github.com/cacayaya/DRIFT.git.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\texttt{BluePrint}$: A Social Media User Dataset for LLM Persona Evaluation and Training</title>
<link>https://arxiv.org/abs/2510.02343</link>
<guid>https://arxiv.org/abs/2510.02343</guid>
<content:encoded><![CDATA[
<div> privacy, social media, large language models, dataset, simulation-oriented

Summary:
The article introduces SIMPACT, a toolkit for creating behaviorally-grounded social media datasets for training large language models (LLMs) as agents. It focuses on next-action prediction and introduces metrics for assessing behavioral fidelity and stylistic realism at both cluster and population levels. The BluePrint dataset, derived from public Bluesky data, anonymizes users into personas and includes a variety of social media interaction types. This dataset allows for the development of LLM-based agents that can model both language and interaction behaviors on social media. By standardizing data and evaluation protocols, SIMPACT aims to advance ethical and rigorous social media simulations. BluePrint serves as a benchmark for political discourse modeling and can be used to study challenges such as misinformation and polarization.<br /><br />Summary: <div>
arXiv:2510.02343v1 Announce Type: new 
Abstract: Large language models (LLMs) offer promising capabilities for simulating social media dynamics at scale, enabling studies that would be ethically or logistically challenging with human subjects. However, the field lacks standardized data resources for fine-tuning and evaluating LLMs as realistic social media agents. We address this gap by introducing SIMPACT, the SIMulation-oriented Persona and Action Capture Toolkit, a privacy respecting framework for constructing behaviorally-grounded social media datasets suitable for training agent models. We formulate next-action prediction as a task for training and evaluating LLM-based agents and introduce metrics at both the cluster and population levels to assess behavioral fidelity and stylistic realism. As a concrete implementation, we release BluePrint, a large-scale dataset built from public Bluesky data focused on political discourse. BluePrint clusters anonymized users into personas of aggregated behaviours, capturing authentic engagement patterns while safeguarding privacy through pseudonymization and removal of personally identifiable information. The dataset includes a sizable action set of 12 social media interaction types (likes, replies, reposts, etc.), each instance tied to the posting activity preceding it. This supports the development of agents that use context-dependence, not only in the language, but also in the interaction behaviours of social media to model social media users. By standardizing data and evaluation protocols, SIMPACT provides a foundation for advancing rigorous, ethically responsible social media simulations. BluePrint serves as both an evaluation benchmark for political discourse modeling and a template for building domain specific datasets to study challenges such as misinformation and polarization.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the MoE LLM Trilemma: Dynamic Expert Clustering with Structured Compression</title>
<link>https://arxiv.org/abs/2510.02345</link>
<guid>https://arxiv.org/abs/2510.02345</guid>
<content:encoded><![CDATA[
<div> dynamic expert clustering, structured compression, parameter reduction, hierarchical routing, heterogeneous precision<br />
<br />
Summary: 
This study introduces a framework for Mixture-of-Experts (MoE) Large Language Models (LLMs) to address the challenges of load imbalance, parameter redundancy, and communication overhead. The framework utilizes dynamic expert clustering to stabilize expert utilization and reduce parameter redundancy. By decomposing expert weights into shared base matrices and low-rank residual adapters, the model achieves significant parameter reduction while maintaining specialization. A hierarchical routing strategy assigns tokens to specific clusters and experts, decreasing the routing search space and communication volume. A heterogeneous precision scheme and dynamic offloading of inactive clusters reduce memory consumption. Evaluation on GLUE and WikiText-103 datasets shows that the framework maintains model quality while reducing parameters by 80%, improving throughput by 10% to 20%, and decreasing expert load variance. This work demonstrates the effectiveness of structural reorganization for scalable, efficient, and memory-effective MoE LLMs. <br /><br /> <div>
arXiv:2510.02345v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) Large Language Models (LLMs) face a trilemma of load imbalance, parameter redundancy, and communication overhead. We introduce a unified framework based on dynamic expert clustering and structured compression to address these issues cohesively. Our method employs an online clustering procedure that periodically regroups experts using a fused metric of parameter and activation similarity, which stabilizes expert utilization. To our knowledge, this is one of the first frameworks to leverage the semantic embedding capability of the router to dynamically reconfigure the model's architecture during training for substantial efficiency gains. Within each cluster, we decompose expert weights into a shared base matrix and extremely low-rank residual adapters, achieving up to fivefold parameter reduction per group while preserving specialization. This structure enables a two-stage hierarchical routing strategy: tokens are first assigned to a cluster, then to specific experts within it, drastically reducing the routing search space and the volume of all-to-all communication. Furthermore, a heterogeneous precision scheme, which stores shared bases in FP16 and residual factors in INT4, coupled with dynamic offloading of inactive clusters, reduces peak memory consumption to levels comparable to dense models. Evaluated on GLUE and WikiText-103, our framework matches the quality of standard MoE models while reducing total parameters by approximately 80%, improving throughput by 10% to 20%, and lowering expert load variance by a factor of over three. Our work demonstrates that structural reorganization is a principled path toward scalable, efficient, and memory-effective MoE LLMs.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Small Language Models for Curriculum-based Guidance</title>
<link>https://arxiv.org/abs/2510.02347</link>
<guid>https://arxiv.org/abs/2510.02347</guid>
<content:encoded><![CDATA[
<div> Keywords: generative AI, large language models, AI teaching assistants, sustainability, personalized learning <br />
Summary: 
In this study, the development and evaluation of AI teaching assistants using small language models (SLMs) for curriculum-based guidance are explored. The research focused on a retrieval-augmented generation (RAG) pipeline applied to selected SLMs to provide accurate and pedagogically aligned responses. Benchmarking eight SLMs against GPT-4o showed that with proper prompting and targeted retrieval, SLMs can match the performance of large language models (LLMs) in educational settings. One significant advantage of SLMs is their lower computational and energy requirements, enabling real-time use on consumer-grade hardware without relying on cloud infrastructure. This makes SLMs cost-effective, privacy-preserving, and environmentally responsible, making them suitable AI teaching assistants for educational institutions looking to scale personalized learning sustainably and efficiently. <br /><br />Summary: <div>
arXiv:2510.02347v1 Announce Type: new 
Abstract: The adoption of generative AI and large language models (LLMs) in education is still emerging. In this study, we explore the development and evaluation of AI teaching assistants that provide curriculum-based guidance using a retrieval-augmented generation (RAG) pipeline applied to selected open-source small language models (SLMs). We benchmarked eight SLMs, including LLaMA 3.1, IBM Granite 3.3, and Gemma 3 (7-17B parameters), against GPT-4o. Our findings show that with proper prompting and targeted retrieval, SLMs can match LLMs in delivering accurate, pedagogically aligned responses. Importantly, SLMs offer significant sustainability benefits due to their lower computational and energy requirements, enabling real-time use on consumer-grade hardware without depending on cloud infrastructure. This makes them not only cost-effective and privacy-preserving but also environmentally responsible, positioning them as viable AI teaching assistants for educational institutions aiming to scale personalized learning in a sustainable and energy-efficient manner.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>mini-vec2vec: Scaling Universal Geometry Alignment with Linear Transformations</title>
<link>https://arxiv.org/abs/2510.02348</link>
<guid>https://arxiv.org/abs/2510.02348</guid>
<content:encoded><![CDATA[
<div> Keywords: vec2vec, text embedding, linear transformation, efficiency, robustness

Summary: 
mini-vec2vec is introduced as an efficient and stable alternative to the original vec2vec procedure for aligning text embedding spaces without parallel data. The method utilizes a linear transformation and consists of three key stages: matching pseudo-parallel embedding vectors, fitting transformations, and iterative refinement. mini-vec2vec surpasses the original vec2vec in efficiency by orders of magnitude while achieving comparable or superior results. The method's stability and interpretable algorithmic steps make it highly scalable and applicable in various domains. By offering a simpler and more cost-effective solution, mini-vec2vec opens up new possibilities for adoption in different fields. <div>
arXiv:2510.02348v1 Announce Type: new 
Abstract: We build upon vec2vec, a procedure designed to align text embedding spaces without parallel data. vec2vec finds a near-perfect alignment, but it is expensive and unstable. We present mini-vec2vec, a simple and efficient alternative that requires substantially lower computational cost and is highly robust. Moreover, the learned mapping is a linear transformation. Our method consists of three main stages: a tentative matching of pseudo-parallel embedding vectors, transformation fitting, and iterative refinement. Our linear alternative exceeds the original instantiation of vec2vec by orders of magnitude in efficiency, while matching or exceeding their results. The method's stability and interpretable algorithmic steps facilitate scaling and unlock new opportunities for adoption in new domains and fields.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL</title>
<link>https://arxiv.org/abs/2510.02350</link>
<guid>https://arxiv.org/abs/2510.02350</guid>
<content:encoded><![CDATA[
<div> SQL queries, Text-to-SQL, WikiSQL, LLM, natural language interfaces

Summary: 
The article introduces LLMSQL, a revised version of the WikiSQL dataset, tailored for modern language models (LLMs). The WikiSQL dataset, while important in NL2SQL research, had issues like inconsistencies and errors. LLMSQL addresses these issues through systematic revision and transformation. The dataset is cleaned and re-annotated using automated methods, making it suitable for evaluating large language models. LLMSQL serves as an LLM-ready benchmark, providing clean natural language questions and full SQL queries in plain text format. This enables straightforward generation and evaluation for current natural language-to-SQL models, unlike the original WikiSQL designed for pointer-network models.Multiple large language models such as Gemma 3, LLaMA 3.2, Mistral 7B, and others were evaluated on LLMSQL, showcasing its effectiveness in NL2SQL tasks. Overall, LLMSQL enhances the usability and accuracy of text-to-SQL systems, addressing the limitations of earlier datasets like WikiSQL. 

<br /><br />Summary: <div>
arXiv:2510.02350v1 Announce Type: new 
Abstract: Converting natural language questions into SQL queries (Text-to-SQL) enables non-expert users to interact with relational databases and has long been a central task for natural language interfaces to data. While the WikiSQL dataset played a key role in early NL2SQL research, its usage has declined due to structural and annotation issues, including case sensitivity inconsistencies, data type mismatches, syntax errors, and unanswered questions. We present LLMSQL, a systematic revision and transformation of WikiSQL designed for the LLM era. We classify these errors and implement automated methods for cleaning and re-annotation. To assess the impact of these improvements, we evaluated multiple large language models (LLMs), including Gemma 3, LLaMA 3.2, Mistral 7B, gpt-oss 20B, Phi-3.5 Mini, Qwen 2.5, OpenAI o4-mini, DeepSeek R1 and others. Rather than serving as an update, LLMSQL is introduced as an LLM-ready benchmark: unlike the original WikiSQL, tailored for pointer-network models selecting tokens from input, LLMSQL provides clean natural language questions and full SQL queries as plain text, enabling straightforward generation and evaluation for modern natural language-to-SQL models.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language, Culture, and Ideology: Personalizing Offensiveness Detection in Political Tweets with Reasoning LLMs</title>
<link>https://arxiv.org/abs/2510.02351</link>
<guid>https://arxiv.org/abs/2510.02351</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, political discourse, offensiveness, reasoning abilities, sociopolitical text classification

Summary:
Large language models (LLMs) were evaluated on their ability to assess offensiveness in political discourse from different political perspectives in English, Polish, and Russian contexts using the MD-Agreement dataset. Models like DeepSeek-R1 and o4-mini with explicit reasoning abilities showed better consistency and sensitivity to ideological and cultural variation compared to smaller models like GPT-4.1-mini and Qwen3. The study found that reasoning capabilities significantly improved the personalization and interpretability of offensiveness judgments, especially in capturing subtle distinctions. The results suggest that incorporating reasoning mechanisms is crucial for adapting LLMs for nuanced sociopolitical text classification across different languages and political ideologies.

<br /><br />Summary: 
1. Evaluation of LLMs on assessing offensiveness in political discourse 
2. Models like DeepSeek-R1 and o4-mini showed better sensitivity to ideological and cultural variations 
3. Reasoning capabilities improved personalization and interpretability of judgments 
4. Larger models outperformed smaller models in capturing subtle distinctions 
5. Importance of reasoning mechanisms for adapting LLMs across languages and ideologies <div>
arXiv:2510.02351v1 Announce Type: new 
Abstract: We explore how large language models (LLMs) assess offensiveness in political discourse when prompted to adopt specific political and cultural perspectives. Using a multilingual subset of the MD-Agreement dataset centered on tweets from the 2020 US elections, we evaluate several recent LLMs - including DeepSeek-R1, o4-mini, GPT-4.1-mini, Qwen3, Gemma, and Mistral - tasked with judging tweets as offensive or non-offensive from the viewpoints of varied political personas (far-right, conservative, centrist, progressive) across English, Polish, and Russian contexts. Our results show that larger models with explicit reasoning abilities (e.g., DeepSeek-R1, o4-mini) are more consistent and sensitive to ideological and cultural variation, while smaller models often fail to capture subtle distinctions. We find that reasoning capabilities significantly improve both the personalization and interpretability of offensiveness judgments, suggesting that such mechanisms are key to adapting LLMs for nuanced sociopolitical text classification across languages and ideologies.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Bias in Spoken Dialogue LLMs for Real-World Decisions and Recommendations</title>
<link>https://arxiv.org/abs/2510.02352</link>
<guid>https://arxiv.org/abs/2510.02352</guid>
<content:encoded><![CDATA[
<div> Keywords: biases, spoken dialogue models, paralinguistic features, multi-turn conversations, fairness<br />
Summary:<br />
This paper examines biases in spoken dialogue models, specifically focusing on the effects of paralinguistic features such as age, gender, and accent. The study evaluates biases in speech LLMs and considers the impact of multi-turn dialogues with repeated negative feedback on model outputs. Biases are measured using Group Unfairness Score (GUS) for decisions and similarity-based normalized statistics rate (SNSR) for recommendations. The analysis compares open-source models like Qwen2.5-Omni and GLM-4-Voice with closed-source APIs such as GPT-4o Audio and Gemini-2.5-Flash. The findings show that closed-source models generally exhibit lower bias, while open-source models are more sensitive to age and gender. Recommendation tasks tend to amplify cross-group disparities. Biased decisions may persist in multi-turn conversations, highlighting the importance of addressing biases in end-to-end spoken dialogue models for fair and reliable audio-based interactive systems.<br />Summary: <div>
arXiv:2510.02352v1 Announce Type: new 
Abstract: While biases in large language models (LLMs), such as stereotypes and cultural tendencies in outputs, have been examined and identified, their presence and characteristics in spoken dialogue models (SDMs) with audio input and output remain largely unexplored. Paralinguistic features, such as age, gender, and accent, can affect model outputs; when compounded by multi-turn conversations, these effects may exacerbate biases, with potential implications for fairness in decision-making and recommendation tasks. In this paper, we systematically evaluate biases in speech LLMs and study the impact of multi-turn dialogues with repeated negative feedback. Bias is measured using Group Unfairness Score (GUS) for decisions and similarity-based normalized statistics rate (SNSR) for recommendations, across both open-source models like Qwen2.5-Omni and GLM-4-Voice, as well as closed-source APIs such as GPT-4o Audio and Gemini-2.5-Flash. Our analysis reveals that closed-source models generally exhibit lower bias, while open-source models are more sensitive to age and gender, and recommendation tasks tend to amplify cross-group disparities. We found that biased decisions may persist in multi-turn conversations. This work provides the first systematic study of biases in end-to-end spoken dialogue models, offering insights towards fair and reliable audio-based interactive systems. To facilitate further research, we release the FairDialogue dataset and evaluation code.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Senegalese Legal Texts Structuration Using LLM-augmented Knowledge Graph</title>
<link>https://arxiv.org/abs/2510.02353</link>
<guid>https://arxiv.org/abs/2510.02353</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, large language models, legal texts, Senegal, judicial system

Summary:
This study explores the use of artificial intelligence and large language models to enhance access to legal texts in Senegal's judicial system. It addresses the challenges in extracting and organizing legal documents and stresses the importance of improved access to judicial information. The research successfully extracted 7,967 articles from various legal documents, with a focus on the Land and Public Domain Code. A detailed graph database containing 2,872 nodes and 10,774 relationships was created to visualize interconnections within legal texts. Advanced triple extraction techniques, utilizing models like GPT-4o, GPT-4, and Mistral-Large, were employed to identify relationships and relevant metadata. The goal is to establish a robust framework that enables Senegalese citizens and legal professionals to better comprehend their rights and obligations. 

<br /><br />Summary: <div>
arXiv:2510.02353v1 Announce Type: new 
Abstract: This study examines the application of artificial intelligence (AI) and large language models (LLM) to improve access to legal texts in Senegal's judicial system. The emphasis is on the difficulties of extracting and organizing legal documents, highlighting the need for better access to judicial information. The research successfully extracted 7,967 articles from various legal documents, particularly focusing on the Land and Public Domain Code. A detailed graph database was developed, which contains 2,872 nodes and 10,774 relationships, aiding in the visualization of interconnections within legal texts. In addition, advanced triple extraction techniques were utilized for knowledge, demonstrating the effectiveness of models such as GPT-4o, GPT-4, and Mistral-Large in identifying relationships and relevant metadata. Through these technologies, the aim is to create a solid framework that allows Senegalese citizens and legal professionals to more effectively understand their rights and responsibilities.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling the language cortex with form-independent and enriched representations of sentence meaning reveals remarkable semantic abstractness</title>
<link>https://arxiv.org/abs/2510.02354</link>
<guid>https://arxiv.org/abs/2510.02354</guid>
<content:encoded><![CDATA[
<div> abstractness, meaning representations, language cortex, vision model, neural responses

Summary: 
The study explores the abstractness of meaning representations in the human language system using neural responses to sentences. By generating images corresponding to sentences and extracting vision model embeddings, researchers found that aggregating across multiple images improved prediction accuracy of language cortex responses. Averaging embeddings across multiple paraphrases of a sentence also enhanced prediction accuracy. Furthermore, enriching paraphrases with contextual details led to increased accuracy, surpassing predictions based on the original sentence embedding. These findings suggest the existence of highly abstract, form-independent meaning representations within the language cortex, indicating that the language system maintains richer and broader semantic representations compared to language models. <div>
arXiv:2510.02354v1 Announce Type: new 
Abstract: The human language system represents both linguistic forms and meanings, but the abstractness of the meaning representations remains debated. Here, we searched for abstract representations of meaning in the language cortex by modeling neural responses to sentences using representations from vision and language models. When we generate images corresponding to sentences and extract vision model embeddings, we find that aggregating across multiple generated images yields increasingly accurate predictions of language cortex responses, sometimes rivaling large language models. Similarly, averaging embeddings across multiple paraphrases of a sentence improves prediction accuracy compared to any single paraphrase. Enriching paraphrases with contextual details that may be implicit (e.g., augmenting "I had a pancake" to include details like "maple syrup") further increases prediction accuracy, even surpassing predictions based on the embedding of the original sentence, suggesting that the language system maintains richer and broader semantic representations than language models. Together, these results demonstrate the existence of highly abstract, form-independent meaning representations within the language cortex.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffuSpec: Unlocking Diffusion Language Models for Speculative Decoding</title>
<link>https://arxiv.org/abs/2510.02358</link>
<guid>https://arxiv.org/abs/2510.02358</guid>
<content:encoded><![CDATA[
<div> DiffuSpec, training-free, diffusion-based drafting, speculative decoding, large language models, autoregressive nature
Summary:
DiffuSpec is a new framework for improving the efficiency of large language models by utilizing a pretrained diffusion language model (DLM) for multi-token drafting. Unlike traditional autoregressive drafting methods, DiffuSpec allows for parallel generation of drafts in a single forward pass, leading to significant speed improvements. The framework includes a causal-consistency path search (CPS) algorithm to ensure alignment with autoregressive verification processes and an adaptive draft-length (ADL) controller to optimize draft proposals based on feedback. By leveraging bidirectional conditioning, DiffuSpec overcomes limitations of traditional AR drafters and achieves up to a 3x wall-clock speedup in speculative decoding tasks. This approach offers a promising alternative for enhancing the efficiency of large language models without the need for extensive retraining. <div>
arXiv:2510.02358v1 Announce Type: new 
Abstract: As large language models (LLMs) scale up, accuracy improves, but the autoregressive (AR) nature of decoding increases latency since each token requires a serial forward pass. Speculative decoding addresses this by employing a fast drafter to propose multi-token drafts, which are then verified in parallel by the target model. However, many deployments still rely on AR drafters, where sequential passes limit wall-clock gains. We revisit the drafting stage and present DiffuSpec, a training-free drop-in framework that uses a pretrained diffusion language model (DLM) to produce multi-token drafts in a single forward pass, while remaining compatible with standard AR verifiers. Because DLM drafts are generated under bidirectional conditioning, parallel per-position candidates form a token lattice in which the locally highest-probability token at each position need not form a causal left-to-right path. Moreover, DLM drafting requires pre-specifying a draft length, inducing a speed-quality trade-off. To address these challenges, we introduce two practical components: (i) a causal-consistency path search (CPS) over this lattice that extracts a left-to-right path aligned with AR verification; and (ii) an adaptive draft-length (ADL) controller that adjusts next proposal size based on recent acceptance feedback and realized generated length. Across benchmarks, DiffuSpec yields up to 3x wall-clock speedup, establishing diffusion-based drafting as a robust alternative to autoregressive drafters for speculative decoding.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emission-GPT: A domain-specific language model agent for knowledge retrieval, emission inventory and data analysis</title>
<link>https://arxiv.org/abs/2510.02359</link>
<guid>https://arxiv.org/abs/2510.02359</guid>
<content:encoded><![CDATA[
<div> knowledge-enhanced, large language model, atmospheric emissions, Emission-GPT, domain-specific
Summary: 
The article introduces Emission-GPT, a large language model designed for the atmospheric emissions domain. Emission-GPT utilizes a curated knowledge base of over 10,000 documents to provide accurate domain-specific question answering and interactive emissions data analysis through natural language interactions. It allows users to query, visualize inventories, analyze source contributions, and recommend emission factors for specific scenarios. A case study in Guangdong Province demonstrates Emission-GPT's ability to extract key insights from raw data with simple prompts, such as point source distributions and sectoral trends. The modular and extensible architecture of Emission-GPT enables the automation of manual workflows, making it a foundational tool for next-generation emission inventory development and scenario-based assessment. <div>
arXiv:2510.02359v1 Announce Type: new 
Abstract: Improving air quality and addressing climate change relies on accurate understanding and analysis of air pollutant and greenhouse gas emissions. However, emission-related knowledge is often fragmented and highly specialized, while existing methods for accessing and compiling emissions data remain inefficient. These issues hinder the ability of non-experts to interpret emissions information, posing challenges to research and management. To address this, we present Emission-GPT, a knowledge-enhanced large language model agent tailored for the atmospheric emissions domain. Built on a curated knowledge base of over 10,000 documents (including standards, reports, guidebooks, and peer-reviewed literature), Emission-GPT integrates prompt engineering and question completion to support accurate domain-specific question answering. Emission-GPT also enables users to interactively analyze emissions data via natural language, such as querying and visualizing inventories, analyzing source contributions, and recommending emission factors for user-defined scenarios. A case study in Guangdong Province demonstrates that Emission-GPT can extract key insights--such as point source distributions and sectoral trends--directly from raw data with simple prompts. Its modular and extensible architecture facilitates automation of traditionally manual workflows, positioning Emission-GPT as a foundational tool for next-generation emission inventory development and scenario-based assessment.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spiral of Silence in Large Language Model Agents</title>
<link>https://arxiv.org/abs/2510.02360</link>
<guid>https://arxiv.org/abs/2510.02360</guid>
<content:encoded><![CDATA[
<div> Keywords: Spiral of Silence, Language models, Opinion dynamics, History signals, Persona signals

Summary: 
The study evaluates the Spiral of Silence (SoS) theory in large language models (LLMs) to understand if similar dynamics can emerge in LLM collectives as in human societies. By varying the availability of History and Persona signals in LLM agents, the research shows that both signals together create strong majority dominance, replicating SoS patterns. History signals alone lead to strong anchoring of opinions, while Persona signals alone generate diverse but uncorrelated opinions. The experiments highlight the importance of historical anchoring in generating SoS dynamics. The findings bridge computational sociology and responsible AI design, emphasizing the necessity of monitoring and mitigating emergent conformity in LLM-agent systems.<br /><br /> <div>
arXiv:2510.02360v1 Announce Type: new 
Abstract: The Spiral of Silence (SoS) theory holds that individuals with minority views often refrain from speaking out for fear of social isolation, enabling majority positions to dominate public discourse. When the 'agents' are large language models (LLMs), however, the classical psychological explanation is not directly applicable, since SoS was developed for human societies. This raises a central question: can SoS-like dynamics nevertheless emerge from purely statistical language generation in LLM collectives? We propose an evaluation framework for examining SoS in LLM agents. Specifically, we consider four controlled conditions that systematically vary the availability of 'History' and 'Persona' signals. Opinion dynamics are assessed using trend tests such as Mann-Kendall and Spearman's rank, along with concentration measures including kurtosis and interquartile range. Experiments across open-source and closed-source models show that history and persona together produce strong majority dominance and replicate SoS patterns; history signals alone induce strong anchoring; and persona signals alone foster diverse but uncorrelated opinions, indicating that without historical anchoring, SoS dynamics cannot emerge. The work bridges computational sociology and responsible AI design, highlighting the need to monitor and mitigate emergent conformity in LLM-agent systems.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChunkLLM: A Lightweight Pluggable Framework for Accelerating LLMs Inference</title>
<link>https://arxiv.org/abs/2510.02361</link>
<guid>https://arxiv.org/abs/2510.02361</guid>
<content:encoded><![CDATA[
<div> Keyword: Transformer-based models, self-attention, ChunkLLM, QK Adapter, performance evaluation

Summary:
ChunkLLM is a new training framework designed to address computational inefficiencies in Transformer-based large models. It introduces two components, QK Adapter and Chunk Adapter, to compress features, acquire chunk attention, and detect chunk boundaries. During training, only the QK Adapter and Chunk Adapter are trained while the backbone parameters remain frozen. Attention distillation is used to enhance the recall rate of key chunks. Inference phase triggers chunk selection at chunk boundaries, resulting in faster model inference. Experimental evaluations show that ChunkLLM achieves comparable performance on short-text benchmarks and maintains 98.64% performance on long-context benchmarks while preserving a 48.58% key-value cache retention rate. ChunkLLM provides a maximum speedup of 4.48x in processing 120K long texts compared to the vanilla Transformer. <div>
arXiv:2510.02361v1 Announce Type: new 
Abstract: Transformer-based large models excel in natural language processing and computer vision, but face severe computational inefficiencies due to the self-attention's quadratic complexity with input tokens. Recently, researchers have proposed a series of methods based on block selection and compression to alleviate this problem, but they either have issues with semantic incompleteness or poor training-inference efficiency. To comprehensively address these challenges, we propose ChunkLLM, a lightweight and pluggable training framework. Specifically, we introduce two components: QK Adapter (Q-Adapter and K-Adapter) and Chunk Adapter. The former is attached to each Transformer layer, serving dual purposes of feature compression and chunk attention acquisition. The latter operates at the bottommost layer of the model, functioning to detect chunk boundaries by leveraging contextual semantic information. During the training phase, the parameters of the backbone remain frozen, with only the QK Adapter and Chunk Adapter undergoing training. Notably, we design an attention distillation method for training the QK Adapter, which enhances the recall rate of key chunks. During the inference phase, chunk selection is triggered exclusively when the current token is detected as a chunk boundary, thereby accelerating model inference. Experimental evaluations are conducted on a diverse set of long-text and short-text benchmark datasets spanning multiple tasks. ChunkLLM not only attains comparable performance on short-text benchmarks but also maintains 98.64% of the performance on long-context benchmarks while preserving a 48.58% key-value cache retention rate. Particularly, ChunkLLM attains a maximum speedup of 4.48x in comparison to the vanilla Transformer in the processing of 120K long texts.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Cross-Lingual Analysis of Bias in Large Language Models Using Romanian History</title>
<link>https://arxiv.org/abs/2510.02362</link>
<guid>https://arxiv.org/abs/2510.02362</guid>
<content:encoded><![CDATA[
<div> historical questions, Large Language Models, biases, neutrality, inconsistencies <br />
Summary: 
The study examines how multiple Large Language Models respond to controversial Romanian historical questions to assess biases. History is often viewed through biased perspectives influenced by cultural and state ideals, which can be perpetuated by LLMs. The research process involved three stages to observe how responses varied based on the type of question posed. Results showed high but imperfect stability in binary responses, with models frequently changing stances across languages or response formats. Numeric ratings often differed from initial binary choices, and the most consistent models were not always deemed the most accurate or neutral. The research highlights the predisposition of LLMs to inconsistencies, particularly influenced by context and language specific to the question asked. <br />Summary: <div>
arXiv:2510.02362v1 Announce Type: new 
Abstract: In this case study, we select a set of controversial Romanian historical questions and ask multiple Large Language Models to answer them across languages and contexts, in order to assess their biases. Besides being a study mainly performed for educational purposes, the motivation also lies in the recognition that history is often presented through altered perspectives, primarily influenced by the culture and ideals of a state, even through large language models. Since they are often trained on certain data sets that may present certain ambiguities, the lack of neutrality is subsequently instilled in users. The research process was carried out in three stages, to confirm the idea that the type of response expected can influence, to a certain extent, the response itself; after providing an affirmative answer to some given question, an LLM could shift its way of thinking after being asked the same question again, but being told to respond with a numerical value of a scale. Results show that binary response stability is relatively high but far from perfect and varies by language. Models often flip stance across languages or between formats; numeric ratings frequently diverge from the initial binary choice, and the most consistent models are not always those judged most accurate or neutral. Our research brings to light the predisposition of models to such inconsistencies, within a specific contextualization of the language for the question asked.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Manuals and Tasks: Instance-Level Context Learning for LLM Agents</title>
<link>https://arxiv.org/abs/2510.02369</link>
<guid>https://arxiv.org/abs/2510.02369</guid>
<content:encoded><![CDATA[
<div> Instance-Level Context Learning, Large Language Model, Guided Exploration, Context Document, Exploration Cost<br />
<br />
Large language model agents often struggle in complex tasks due to the lack of instance-level context, which includes specific and verifiable facts about an environment instance. This context is crucial for making accurate decisions based on persistent information. The Instance-Level Context Learning (ILCL) problem addresses the efficient acquisition and formatting of instance-level context by using a guided exploration method that prioritizes actions and creates a reusable context document. Experiments across different environments show significant improvements in success rates and efficiency for LLM agents. For example, success rates in TextWorld tasks increase substantially with the ILCL approach. By turning one-time exploration efforts into persistent knowledge, ILCL enhances the reliability and efficiency of large language model agents.<br /><br />Summary: <div>
arXiv:2510.02369v1 Announce Type: new 
Abstract: Large language model (LLM) agents typically receive two kinds of context: (i) environment-level manuals that define interaction interfaces and global rules, and (ii) task-level guidance or demonstrations tied to specific goals. In this work, we identify a crucial but overlooked third type of context, instance-level context, which consists of verifiable and reusable facts tied to a specific environment instance, such as object locations, crafting recipes, and local rules. We argue that the absence of instance-level context is a common source of failure for LLM agents in complex tasks, as success often depends not only on reasoning over global rules or task prompts but also on making decisions based on precise and persistent facts. Acquiring such context requires more than memorization: the challenge lies in efficiently exploring, validating, and formatting these facts under tight interaction budgets. We formalize this problem as Instance-Level Context Learning (ILCL) and introduce our task-agnostic method to solve it. Our method performs a guided exploration, using a compact TODO forest to intelligently prioritize its next actions and a lightweight plan-act-extract loop to execute them. This process automatically produces a high-precision context document that is reusable across many downstream tasks and agents, thereby amortizing the initial exploration cost. Experiments across TextWorld, ALFWorld, and Crafter demonstrate consistent gains in both success and efficiency: for instance, ReAct's mean success rate in TextWorld rises from 37% to 95%, while IGE improves from 81% to 95%. By transforming one-off exploration into persistent, reusable knowledge, our method complements existing contexts to enable more reliable and efficient LLM agents.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Dynamics of Parametric and In-Context Knowledge Utilization in Language Models</title>
<link>https://arxiv.org/abs/2510.02370</link>
<guid>https://arxiv.org/abs/2510.02370</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, knowledge arbitration, retrieval-augmented generation, training conditions, parametric knowledge

Summary:
Our study examines how training conditions affect language models' use of in-context and parametric knowledge and their arbitration between the two. We trained transformer-based models on a synthetic biographies corpus under various controlled conditions. We found that intra-document repetition of facts enhances the development of both parametric and in-context capabilities. Training on datasets with inconsistent information or distributional skew helps models develop robust strategies for leveraging parametric and in-context knowledge. These challenging properties play a crucial role in learning proper knowledge arbitration. Rather than being seen as problems to eliminate, they are actually vital for creating models that effectively integrate different types of knowledge. Our insights provide practical guidance for pretraining models to strikingly combine parametric and in-context knowledge.<br /><br />Summary: <div>
arXiv:2510.02370v1 Announce Type: new 
Abstract: Large language models often encounter conflicts between in-context knowledge retrieved at inference time and parametric knowledge acquired during pretraining. Models that accept external knowledge uncritically are vulnerable to misinformation, whereas models that adhere rigidly to parametric knowledge fail to benefit from retrieval. Despite the widespread adoption of retrieval-augmented generation, we still lack a systematic understanding of what shapes knowledge-arbitration strategies during training. This gap risks producing pretrained models with undesirable arbitration behaviors and, consequently, wasting substantial computational resources after the pretraining budget has already been spent. To address this problem, we present the first controlled study of how training conditions influence models' use of in-context and parametric knowledge, and how they arbitrate between them. We train transformer-based language models on a synthetic biographies corpus while systematically controlling various conditions. Our experiments reveal that intra-document repetition of facts fosters the development of both parametric and in-context capabilities. Moreover, training on a corpus that contains inconsistent information or distributional skew encourages models to develop robust strategies for leveraging parametric and in-context knowledge. Rather than viewing these non-ideal properties as artifacts to remove, our results indicate that they are important for learning robust arbitration. These insights offer concrete, empirical guidance for pretraining models that harmoniously integrate parametric and in-context knowledge.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pretraining with hierarchical memories: separating long-tail and common knowledge</title>
<link>https://arxiv.org/abs/2510.02375</link>
<guid>https://arxiv.org/abs/2510.02375</guid>
<content:encoded><![CDATA[
<div> memory-augmented architecture, pretraining strategy, language models, hierarchical memory banks, world knowledge <br /> 
<br />
Summary: 
The article introduces a memory-augmented architecture and pretraining strategy to address the limitations of scaling parameters in language models. By using hierarchical parametric memory banks, the approach enables small language models to access and fetch context-dependent memory blocks containing world knowledge during pretraining and inference. This allows for the efficient storage of long-tail world knowledge in memory parameters while the small language model focuses on capturing common knowledge and reasoning abilities. Experiments show significant performance gains, with a 160M-parameter model augmented with an 18M-parameter memory achieving performance comparable to a larger model with over double the parameters. The study explores various types and sizes of parametric memories in transformers, scaling them up to 21B parameters and demonstrating the effectiveness of hierarchical feed-forward memories across different transformer architectures. <br /> <div>
arXiv:2510.02375v1 Announce Type: new 
Abstract: The impressive performance gains of modern language models currently rely on scaling parameters: larger models store more world knowledge and reason better. Yet compressing all world knowledge into parameters is unnecessary, as only a fraction is used per prompt, and impractical for edge devices with limited inference-time memory and compute. We address this shortcoming by a memory-augmented architecture and a pretraining strategy aligned with existing hardware paradigms. We introduce small language models that access large hierarchical parametric memory banks encoding world knowledge. During pretraining and inference, we fetch a small, context-dependent memory block and add it to the model. Our pretraining learns to store long-tail world knowledge in the memory parameters, while the small language model acts as an anchor capturing common knowledge and general reasoning abilities. Through trillion-token-scale experiments, we show significant gains: a 160M-parameters model augmented with an 18M-parameters memory fetched from a 4.6B memory bank obtains comparable performance to a regular model with more than 2x the parameters. Through extensive experiments, we study the optimal type and size of parametric memories in transformers, scaling them to over 21B parameters. We find that our proposed hierarchical feed-forward memories work robustly across transformer architectures, whether added during pretraining or post-hoc.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Aware Answer Selection for Improved Reasoning in Multi-LLM Systems</title>
<link>https://arxiv.org/abs/2510.02377</link>
<guid>https://arxiv.org/abs/2510.02377</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, response selection, multi-LLM systems, log-likelihood score, dataset evaluation

Summary: 
This article introduces a method for selecting the most reliable response from multiple Large Language Models (LLMs) without the need for costly external verifiers or human evaluators. The proposed approach leverages the calibrated log-likelihood score to select the best response from different LLMs, capturing their inherent knowledge and confidence. This method shows improvements of approximately 4% to 5% across debate and non-debate settings on various datasets, including GSM8K, MMLU, and ARC. By effectively utilizing multiple LLMs and their diverse responses, the proposed method enhances response selection performance in resource-constrained environments. The study showcases the potential of multi-LLM systems and offers a computationally efficient solution for selecting the most reliable response in different scenarios. 

<br /><br />Summary: <div>
arXiv:2510.02377v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities, yet selecting the most reliable response from multiple LLMs remains a challenge, particularly in resource-constrained settings. Existing approaches often depend on costly external verifiers, human evaluators, or self-consistency techniques that require multiple samples from a single model. While multi-LLM systems produce more diverse responses than single models and thus have greater potential, they often underperform compared to single LLM self-consistency. We propose a principled, novel and computationally efficient method to select the best response from multiple different LLMs using a calibrated log-likelihood score, implicitly leveraging the inherent knowledge and confidence of these models. Our method demonstrates improvements of approx. 4%, 3%, and 5% across both debate (multi-round LLM discussions) and non-debate (Best-of-N with multiple LLMs) settings on GSM8K, MMLU (6 subsets), and ARC datasets respectively.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Route: A Rule-Driven Agent Framework for Hybrid-Source Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2510.02388</link>
<guid>https://arxiv.org/abs/2510.02388</guid>
<content:encoded><![CDATA[
<div> finance, healthcare, scientific research, Large Language Models, Retrieval-Augmented Generation<br />
<br />
Summary: 
Large Language Models (LLMs) excel in general Question Answering (QA) but struggle with domain-specific information. Retrieval-Augmented Generation (RAG) enhances LLMs with external knowledge, yet often overlooks relational databases. This study uncovers the complementarity of databases and documents for QA, emphasizing the need for efficient source selection. The authors propose a rule-driven routing framework that balances effectiveness and efficiency by routing queries to the most suitable source. Experiment results on three QA benchmarks show that the framework outperforms static and learned routing strategies, achieving higher accuracy with moderate computational cost. The framework consists of a routing agent scoring candidate augmentation paths, a rule-making expert refining rules based on feedback, and a path-level meta-cache reducing latency and cost by reusing past decisions for similar queries. <div>
arXiv:2510.02388v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown remarkable performance on general Question Answering (QA), yet they often struggle in domain-specific scenarios where accurate and up-to-date information is required. Retrieval-Augmented Generation (RAG) addresses this limitation by enriching LLMs with external knowledge, but existing systems primarily rely on unstructured documents, while largely overlooking relational databases, which provide precise, timely, and efficiently queryable factual information, serving as indispensable infrastructure in domains such as finance, healthcare, and scientific research. Motivated by this gap, we conduct a systematic analysis that reveals three central observations: (i) databases and documents offer complementary strengths across queries, (ii) naively combining both sources introduces noise and cost without consistent accuracy gains, and (iii) selecting the most suitable source for each query is crucial to balance effectiveness and efficiency. We further observe that query types show consistent regularities in their alignment with retrieval paths, suggesting that routing decisions can be effectively guided by systematic rules that capture these patterns. Building on these insights, we propose a rule-driven routing framework. A routing agent scores candidate augmentation paths based on explicit rules and selects the most suitable one; a rule-making expert agent refines the rules over time using QA feedback to maintain adaptability; and a path-level meta-cache reuses past routing decisions for semantically similar queries to reduce latency and cost. Experiments on three QA benchmarks demonstrate that our framework consistently outperforms static strategies and learned routing baselines, achieving higher accuracy while maintaining moderate computational cost.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KnowledgeSmith: Uncovering Knowledge Updating in LLMs with Model Editing and Unlearning</title>
<link>https://arxiv.org/abs/2510.02392</link>
<guid>https://arxiv.org/abs/2510.02392</guid>
<content:encoded><![CDATA[
<div> unlearning, large language models, knowledge editing, KnowledgeSmith, knowledge propagation  
Summary:  
The paper introduces KnowledgeSmith, a framework that aims to understand the knowledge updating mechanism of large language models (LLMs) through knowledge editing and unlearning. It frames editing and unlearning as instances of a single optimization problem and provides an automatic dataset generator for structured interventions at multiple graph levels and data scales. The experiments conducted reveal insights into knowledge propagation, plasticity scaling, consistency, and robustness of LLMs. The results show that LLMs do not update knowledge in a manner similar to humans across different knowledge levels and highlight the existence of a consistency-capacity trade-off. These findings could potentially inform the development of more reliable and scalable strategies for knowledge updating in LLMs.  
<br /><br />Summary: <div>
arXiv:2510.02392v1 Announce Type: new 
Abstract: Knowledge editing and machine unlearning are two popular approaches for large language models (LLMs) to stay up-to-date. However, the knowledge updating mechanism of LLMs remains largely unexplored due to insufficient, isolated, and small-scale evaluation. For instance, are LLMs similar to humans in modifying certain knowledge? What differs editing and unlearning as training data increases? This paper proposes KnowledgeSmith, a unified framework to systematically understand the updating mechanism of LLMs. We first cast editing and unlearning as instances of one constrained optimization problem. Then, we propose an automatic dataset generator that provides structured interventions across multiple graph levels and data scales, enabling controlled studies of how different modification strategies propagate through model knowledge. Extensive experiments demonstrate nuanced insights over knowledge propagation, plasticity scaling, consistency, and robustness. For instance, our results show that LLMs do not exhibit similar updating as humans for different levels of knowledge, and there exists consistency-capacity trade-off. We hope our findings can offer suggestions to the design of more reliable and scalable strategies. Code: https://github.com/AIFrontierLab/KnowledgeSmith.git
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval and Augmentation of Domain Knowledge for Text-to-SQL Semantic Parsing</title>
<link>https://arxiv.org/abs/2510.02394</link>
<guid>https://arxiv.org/abs/2510.02394</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Natural Language queries, SQL, domain specific vocabulary, structured domain statements

Summary:<br /><br />
The performance of Large Language Models (LLMs) in translating Natural Language (NL) queries into SQL varies greatly depending on the database. NL queries often use domain specific vocabulary, making it challenging to map them to the correct SQL without understanding the domain expressions and their relationship to the database schema. Existing benchmarks rely on unrealistic textual hints for expressing domain knowledge, but this paper proposes a systematic framework for associating structured domain statements at the database level. By retrieving relevant structured domain statements using sub-string matching, the approach is found to be more practical and accurate than ad-hoc query-specific textual hints. The evaluation on eleven diverse database schemas across various domains shows that the sub-string matching retrieval method yields significantly higher accuracy compared to other approaches. <div>
arXiv:2510.02394v1 Announce Type: new 
Abstract: The performance of Large Language Models (LLMs) for translating Natural Language (NL) queries into SQL varies significantly across databases (DBs). NL queries are often expressed using a domain specific vocabulary, and mapping these to the correct SQL requires an understanding of the embedded domain expressions, their relationship to the DB schema structure. Existing benchmarks rely on unrealistic, ad-hoc query specific textual hints for expressing domain knowledge. In this paper, we propose a systematic framework for associating structured domain statements at the database level. We present retrieval of relevant structured domain statements given a user query using sub-string level match. We evaluate on eleven realistic DB schemas covering diverse domains across five open-source and proprietary LLMs and demonstrate that (1) DB level structured domain statements are more practical and accurate than existing ad-hoc query specific textual domain statements, and (2) Our sub-string match based retrieval of relevant domain statements provides significantly higher accuracy than other retrieval approaches.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Words That Make Language Models Perceive</title>
<link>https://arxiv.org/abs/2510.02425</link>
<guid>https://arxiv.org/abs/2510.02425</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, sensory prompting, multimodal regularities, latent structure, modality-appropriate representations

Summary: 
Large language models (LLMs) trained solely on text data can acquire implicit multimodal knowledge embedded in language. The study explores how explicit sensory prompts can activate latent visual and auditory representations in text-trained LLMs. By instructing the model to 'see' or 'hear', the prompts guide the model to generate predictions as if based on visual or auditory information. Results demonstrate that simple prompt adjustments can effectively engage modality-specific representations in text-based LLMs, enhancing alignment with specialized vision and audio models. This study sheds light on leveraging sensory prompting to unveil hidden multimodal knowledge within text-only LLMs, offering insights into enhancing their capabilities and bridging the modality gap in language understanding. <br /><br />Summary: <div>
arXiv:2510.02425v1 Announce Type: new 
Abstract: Large language models (LLMs) trained purely on text ostensibly lack any direct perceptual experience, yet their internal representations are implicitly shaped by multimodal regularities encoded in language. We test the hypothesis that explicit sensory prompting can surface this latent structure, bringing a text-only LLM into closer representational alignment with specialist vision and audio encoders. When a sensory prompt tells the model to 'see' or 'hear', it cues the model to resolve its next-token predictions as if they were conditioned on latent visual or auditory evidence that is never actually supplied. Our findings reveal that lightweight prompt engineering can reliably activate modality-appropriate representations in purely text-trained LLMs.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLARITY: Clinical Assistant for Routing, Inference, and Triage</title>
<link>https://arxiv.org/abs/2510.02463</link>
<guid>https://arxiv.org/abs/2510.02463</guid>
<content:encoded><![CDATA[
<div> AI-driven platform, patient-to-specialist routing, clinical consultations, severity assessment, hybrid architecture, Large Language Model

Summary:
The article introduces CLARITY, an AI platform for patient routing and severity assessment in healthcare. CLARITY combines a Finite State Machine with collaborative agents using Large Language Models for efficient dialogue flows and specialist referrals. Its modular microservices framework ensures safe, flexible, and scalable performance in healthcare workflows. Integrated into a national inter-hospital IT platform, CLARITY completed over 55,000 user dialogues in two months, surpassing human-level performance in routing precision. Validated results showed up to three times shorter consultation durations compared to human consultations. Overall, CLARITY's hybrid architecture and AI capabilities streamline patient care processes, improving efficiency and accuracy in clinical consultations and specialist referrals.<br /><br />Summary: <div>
arXiv:2510.02463v1 Announce Type: new 
Abstract: We present CLARITY (Clinical Assistant for Routing, Inference, and Triage), an AI-driven platform designed to facilitate patient-to-specialist routing, clinical consultations, and severity assessment of patients' conditions. Its hybrid architecture combines a Finite State Machine (FSM) for structured dialogue flows with collaborative agents that employ Large Language Model (LLM) to analyze symptoms and prioritize referrals to appropriate specialists. Built on a modular microservices framework, CLARITY ensures safe, efficient, and robust performance, flexible and readily scalable to meet the demands of existing workflows and IT solutions in healthcare.
  We report integration of our clinical assistant into a large-scale nation-wide inter-hospital IT platform, with over 55,000 content-rich user dialogues completed within the two months of deployment, 2,500 of which were expert-annotated for a consequent validation. The validation results show that CLARITY surpasses human-level performance in terms of the first-attempt routing precision, naturally requiring up to 3 times shorter duration of the consultation than with a human.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unraveling Syntax: How Language Models Learn Context-Free Grammars</title>
<link>https://arxiv.org/abs/2510.02524</link>
<guid>https://arxiv.org/abs/2510.02524</guid>
<content:encoded><![CDATA[
<div> framework, language models, syntax acquisition, probabilistic context-free grammars, learning dynamics
Summary: 
The article introduces a new framework for understanding how language models learn syntax, focusing on probabilistic context-free grammars (PCFGs) that capture various domains. By studying small models trained on synthetic languages from PCFGs, the authors derive formulae for training loss and Kullback-Leibler divergence. Unlike children, models reduce loss across all subgrammars simultaneously, rather than mastering simple structures first. Subgrammar pretraining improves final loss for smaller models and aligns internal representations with the grammar's substructure. Models struggle with deep recursive structures, highlighting challenges in representing hierarchical syntax. This research paves the way for studying transformer learning dynamics on PCFGs as a versatile testbed for exploring language model learning, sparking new research directions and questions. 
Summary: <div>
arXiv:2510.02524v1 Announce Type: new 
Abstract: We introduce a new framework for understanding how language models acquire syntax. While large models achieve impressive results, little is known about their learning dynamics. Our approach starts with the observation that most domains of interest, such as natural language syntax, coding languages, arithmetic problems, are captured by probabilistic context-free grammars (PCFGs). We study the learning dynamics of small models trained on synthetic languages generated from PCFGs, enabling precise control over grammar complexity, recursion depth, and subgrammar structure. We prove several general, recursive formulae for the training loss and Kullback-Leibler divergence over the subgrammar structure of a PCFG. Empirically, we find that unlike children, who first master simple substructures before progressing to more complex constructions, transformers reduce loss across all subgrammars in parallel. We further show that subgrammar pretraining can improve the final loss for smaller models, and that pretrained models develop internal representations more aligned with the grammar's substructure. Finally, we demonstrate that models struggle with deeper recursive structures (a limitation even of large language models), revealing fundamental challenges in how neural networks represent hierarchical syntax. Overall, our work initiates the study of the learning dynamics of transformers on PCFGs as a versatile testbed for probing learning in language models, opening a research direction with many open questions.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Semantic Retrieval with Cobweb</title>
<link>https://arxiv.org/abs/2510.02539</link>
<guid>https://arxiv.org/abs/2510.02539</guid>
<content:encoded><![CDATA[
<div> Keywords: Neural document retrieval, Cobweb framework, prototype tree, multi-granular relevance signals, interpretable retrieval

Summary: 
Neural document retrieval often overlooks corpus structure, resulting in opaque explanations. This study introduces a novel approach using the Cobweb framework to organize sentence embeddings into a prototype tree, enabling ranking documents through coarse-to-fine traversal. The internal nodes of the hierarchy serve as concept prototypes, offering multi-granular relevance signals and transparent retrieval paths. Two inference methods, best-first search and path-sum ranker, are implemented and evaluated on datasets with various embeddings. The results demonstrate that the novel retrieval approaches match traditional methods with strong encoder embeddings and exhibit robustness with lower-quality embeddings. Notably, the approach remains effective and interpretable, showcasing competitive performance, robustness, scalability, and transparent retrieval via hierarchical prototypes. 

<br /><br />Summary: <div>
arXiv:2510.02539v1 Announce Type: new 
Abstract: Neural document retrieval often treats a corpus as a flat cloud of vectors scored at a single granularity, leaving corpus structure underused and explanations opaque. We use Cobweb--a hierarchy-aware framework--to organize sentence embeddings into a prototype tree and rank documents via coarse-to-fine traversal. Internal nodes act as concept prototypes, providing multi-granular relevance signals and a transparent rationale through retrieval paths. We instantiate two inference approaches: a generalized best-first search and a lightweight path-sum ranker. We evaluate our approaches on MS MARCO and QQP with encoder (e.g., BERT/T5) and decoder (GPT-2) representations. Our results show that our retrieval approaches match the dot product search on strong encoder embeddings while remaining robust when kNN degrades: with GPT-2 vectors, dot product performance collapses whereas our approaches still retrieve relevant results. Overall, our experiments suggest that Cobweb provides competitive effectiveness, improved robustness to embedding quality, scalability, and interpretable retrieval via hierarchical prototypes.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge-Graph Based RAG System Evaluation Framework</title>
<link>https://arxiv.org/abs/2510.02549</link>
<guid>https://arxiv.org/abs/2510.02549</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Retrieval Augmented Generation, KG-based evaluation, multi-hop reasoning, semantic community clustering

Summary: 
Large language models (LLMs) are increasingly used in text generation and dialog systems. Retrieval Augmented Generation (RAG) is a key application of LLMs, improving generated content reliability. Evaluating RAG systems is challenging due to traditional metrics' limitations. Building on the RAGAS tool, a KG-based evaluation approach enables multi-hop reasoning and semantic clustering for more comprehensive scoring. Comparing this method with RAGAS and human annotations shows better sensitivity to semantic nuances in outputs. The study highlights the need for improved evaluation methods for RAG systems and suggests future research directions. 

Summary: <div>
arXiv:2510.02549v1 Announce Type: new 
Abstract: Large language models (LLMs) has become a significant research focus and is utilized in various fields, such as text generation and dialog systems. One of the most essential applications of LLM is Retrieval Augmented Generation (RAG), which greatly enhances generated content's reliability and relevance. However, evaluating RAG systems remains a challenging task. Traditional evaluation metrics struggle to effectively capture the key features of modern LLM-generated content that often exhibits high fluency and naturalness. Inspired by the RAGAS tool, a well-known RAG evaluation framework, we extended this framework into a KG-based evaluation paradigm, enabling multi-hop reasoning and semantic community clustering to derive more comprehensive scoring metrics. By incorporating these comprehensive evaluation criteria, we gain a deeper understanding of RAG systems and a more nuanced perspective on their performance. To validate the effectiveness of our approach, we compare its performance with RAGAS scores and construct a human-annotated subset to assess the correlation between human judgments and automated metrics. In addition, we conduct targeted experiments to demonstrate that our KG-based evaluation method is more sensitive to subtle semantic differences in generated outputs. Finally, we discuss the key challenges in evaluating RAG systems and highlight potential directions for future research.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transcribe, Translate, or Transliterate: An Investigation of Intermediate Representations in Spoken Language Models</title>
<link>https://arxiv.org/abs/2510.02569</link>
<guid>https://arxiv.org/abs/2510.02569</guid>
<content:encoded><![CDATA[
<div> speech models, language models, modality adapters, encoder, decoder  
Summary:  
Spoken language models (SLMs) integrate speech with large language models (LMs) using modality adapters (MAs) to map speech encoder output for decoder LM understanding. This study analyzes MA output representations in three SLMs—SALMONN, Qwen2-Audio, Phi-4-Multimodal-Instruct—to uncover transformation strategies. MAs in Whisper encoder models display an interlingua-based approach, capturing input meaning comprehensible across languages. In contrast, models like Phi-4-Multimodal-Instruct without Whisper encoder focus on phonetic representation in English words. The distinction suggests MAs encode input semantics differently based on speech encoder training for recognition or translation tasks. Understanding these strategies enhances the design and performance of SLMs for multilingual and multimodal applications.  
Summary: <div>
arXiv:2510.02569v1 Announce Type: new 
Abstract: Spoken language models (SLMs) that integrate speech with large language models (LMs) rely on modality adapters (MAs) to map the output of speech encoders to a representation that is understandable to the decoder LM. Yet we know very little about how these crucial MAs transform representations. Here we examine the MA output representation in three SLMs (SALMONN, Qwen2-Audio and Phi-4-Multimodal-Instruct). By finding the nearest decoder LM token to an MA representation, we uncover two strategies for MA representations. For models using a Whisper encoder, MAs appear to represent the meaning of the input using an English-based interlingua, allowing them to handle languages unseen in instruction tuning. For models that don't, like Phi-4-Multimodal-Instruct, MAs instead represent the phonetics of the input, but expressed with English words. We hypothesise that which arises depends on whether the speech encoder is trained only for speech recognition or also for translation.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation Framework for Highlight Explanations of Context Utilisation in Language Models</title>
<link>https://arxiv.org/abs/2510.02629</link>
<guid>https://arxiv.org/abs/2510.02629</guid>
<content:encoded><![CDATA[
<div> context utilisation, Language Models, Highlight explanations, evaluation framework, MechLight

Summary:
The study focuses on the evaluation of Highlight explanations (HEs) in explaining context utilization in Language Models (LMs). A gold standard HE evaluation framework was introduced to assess the effectiveness of HE methods in attributing context. The evaluation involved four HE methods, four context scenarios, four datasets, and five LMs. MechLight, a mechanistic interpretability approach, emerged as the best-performing method across all context scenarios. However, all methods faced challenges with longer contexts and exhibited positional biases. These findings highlight the need for new approaches to deliver reliable explanations for context utilization at scale. The study provides valuable insights into the opacity of context utilization in LMs and emphasizes the importance of accurate explanation frameworks in understanding model behavior. <br /><br /> <div>
arXiv:2510.02629v1 Announce Type: new 
Abstract: Context utilisation, the ability of Language Models (LMs) to incorporate relevant information from the provided context when generating responses, remains largely opaque to users, who cannot determine whether models draw from parametric memory or provided context, nor identify which specific context pieces inform the response. Highlight explanations (HEs) offer a natural solution as they can point the exact context pieces and tokens that influenced model outputs. However, no existing work evaluates their effectiveness in accurately explaining context utilisation. We address this gap by introducing the first gold standard HE evaluation framework for context attribution, using controlled test cases with known ground-truth context usage, which avoids the limitations of existing indirect proxy evaluations. To demonstrate the framework's broad applicability, we evaluate four HE methods -- three established techniques and MechLight, a mechanistic interpretability approach we adapt for this task -- across four context scenarios, four datasets, and five LMs. Overall, we find that MechLight performs best across all context scenarios. However, all methods struggle with longer contexts and exhibit positional biases, pointing to fundamental challenges in explanation accuracy that require new approaches to deliver reliable context utilisation explanations at scale.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the Gap: Linguistic Divergence and Adaptation Strategies in Human-LLM Assistant vs. Human-Human Interactions</title>
<link>https://arxiv.org/abs/2510.02645</link>
<guid>https://arxiv.org/abs/2510.02645</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, communication styles, chatbots, user language, data augmentation

Summary: 
This study explores how users interact differently with Large Language Models (LLMs) compared to human agents, uncovering significant differences in communication styles such as grammatical fluency, politeness, and lexical diversity. The findings suggest that LLMs may struggle to adapt to the shift in communication style when deployed as chatbots. To address this, the researchers experimented with data augmentation post-training and user message reformulation at inference time. Models trained on stylistically diverse datasets performed better than those trained on uniform datasets, highlighting the importance of incorporating varied communication styles in LLM training. However, inference-time reformulation showed less effectiveness in improving model performance. These insights are crucial for enhancing the robustness of LLMs in user interactions, ultimately leading to improved user experiences with chatbots.<br /><br />Summary: <div>
arXiv:2510.02645v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) are increasingly deployed in customer-facing applications, a critical yet underexplored question is how users communicate differently with LLM chatbots compared to human agent. In this study, we present empirical evidence that users adopt distinct communication styles when users interact with chatbots versus human agents. Our analysis reveals significant differences in grammatical fluency, politeness, and lexical diversity in user language between the two settings. These findings suggest that models trained exclusively on human-human interaction data may not adequately accommodate the communication style shift that occurs once an LLM chatbot is deployed. To enhance LLM robustness to post-launch communication style changes, we experimented with two strategies: (1) data augmentation during the post-training phase and (2) inference-time user message reformulation. Our results indicate that models trained on stylistically diverse datasets significantly outperform those trained exclusively on original or stylistically uniform datasets, while inference-time reformulation proved less effective. These insights help us to better adapt our models for improved LLM-user interaction experiences.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoT: Structured-of-Thought Prompting Guides Multilingual Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2510.02648</link>
<guid>https://arxiv.org/abs/2510.02648</guid>
<content:encoded><![CDATA[
<div> Structured-of-Thought, multilingual reasoning, language models, deep thinking, resource constraints <br />
Summary: <br />
The paper introduces Structured-of-Thought (SoT), a novel training-free method that enhances multilingual reasoning capabilities of Large Language Models (LLMs). It achieves this by Language Thinking Transformation and Structured Knowledge Transformation, converting language-specific semantic information into language-agnostic structured representations. SoT enables models to better understand queries in different languages and guides LLMs to maintain consistent reasoning pathways when handling cross-lingual variations. Experimental results show that SoT surpasses strong baselines on various multilingual reasoning benchmarks, across different LLM backbones. It can also be combined with other training-free strategies for additional enhancements. The code for SoT is available on GitHub for further exploration and implementation.  <div>
arXiv:2510.02648v1 Announce Type: new 
Abstract: Recent developments have enabled Large Language Models (LLMs) to engage in complex reasoning tasks through deep thinking. However, the capacity of reasoning has not been successfully transferred to non-high-resource languages due to resource constraints, which struggles with multilingual reasoning tasks. To this end, we propose Structured-of-Thought (SoT), a training-free method that improves the performance on multilingual reasoning through a multi-step transformation: Language Thinking Transformation and Structured Knowledge Transformation. The SoT method converts language-specific semantic information into language-agnostic structured representations, enabling the models to understand the query in different languages more sophisticated. Besides, SoT effectively guides LLMs toward more concentrated reasoning to maintain consistent underlying reasoning pathways when handling cross-lingual variations in expression. Experimental results demonstrate that SoT outperforms several strong baselines on multiple multilingual reasoning benchmarks when adapting to various backbones of LLMs. It can also be integrated with other training-free strategies for further improvements. Our code is available at https://github.com/Cherry-qwq/SoT.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Improvement in Multimodal Large Language Models: A Survey</title>
<link>https://arxiv.org/abs/2510.02665</link>
<guid>https://arxiv.org/abs/2510.02665</guid>
<content:encoded><![CDATA[
<div> advancements, self-improvement, Large Language Models, multimodal domain, data collection
Summary:
Recent advancements have led to efficient self-improvement in Large Language Models (LLMs), particularly in the multimodal domain. This survey provides a comprehensive overview of self-improvement in Multimodal LLMs (MLLMs), focusing on data collection, organization, and model optimization. The literature review covers methods and evaluations for developing more general self-improving models. Common downstream applications are also discussed. Challenges and future research directions are outlined to facilitate further advancements in self-improvement for MLLMs.<br /><br /> <div>
arXiv:2510.02665v1 Announce Type: new 
Abstract: Recent advancements in self-improvement for Large Language Models (LLMs) have efficiently enhanced model capabilities without significantly increasing costs, particularly in terms of human effort. While this area is still relatively young, its extension to the multimodal domain holds immense potential for leveraging diverse data sources and developing more general self-improving models. This survey is the first to provide a comprehensive overview of self-improvement in Multimodal LLMs (MLLMs). We provide a structured overview of the current literature and discuss methods from three perspectives: 1) data collection, 2) data organization, and 3) model optimization, to facilitate the further development of self-improvement in MLLMs. We also include commonly used evaluations and downstream applications. Finally, we conclude by outlining open challenges and future research directions.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty as Feature Gaps: Epistemic Uncertainty Quantification of LLMs in Contextual Question-Answering</title>
<link>https://arxiv.org/abs/2510.02671</link>
<guid>https://arxiv.org/abs/2510.02671</guid>
<content:encoded><![CDATA[
<div> Keywords: Uncertainty Quantification, Contextual Question Answering, Epistemic Uncertainty, Semantic Feature Gaps, Interpretability<br />
Summary:<br />
- The study addresses the lack of Uncertainty Quantification (UQ) research in contextual question answering tasks.
- A novel approach to quantify epistemic uncertainty is proposed, focusing on token-level measures and decomposing uncertainties.
- The upper bound for epistemic uncertainty is derived, highlighting semantic feature gaps in the model's hidden representations.
- The approach is applied to contextual QA tasks, emphasizing context-reliance, context comprehension, and honesty features.
- Through a top-down interpretability approach, these features are extracted using a small number of labeled samples and ensembled for a robust uncertainty score.
- Experimental results demonstrate superior performance over state-of-the-art unsupervised and supervised UQ methods with significant performance improvements and minimal inference overhead.<br />
Summary: <div>
arXiv:2510.02671v1 Announce Type: new 
Abstract: Uncertainty Quantification (UQ) research has primarily focused on closed-book factual question answering (QA), while contextual QA remains unexplored, despite its importance in real-world applications. In this work, we focus on UQ for the contextual QA task and propose a theoretically grounded approach to quantify epistemic uncertainty. We begin by introducing a task-agnostic, token-level uncertainty measure defined as the cross-entropy between the predictive distribution of the given model and the unknown true distribution. By decomposing this measure, we isolate the epistemic component and approximate the true distribution by a perfectly prompted, idealized model. We then derive an upper bound for epistemic uncertainty and show that it can be interpreted as semantic feature gaps in the given model's hidden representations relative to the ideal model. We further apply this generic framework to the contextual QA task and hypothesize that three features approximate this gap: context-reliance (using the provided context rather than parametric knowledge), context comprehension (extracting relevant information from context), and honesty (avoiding intentional lies). Using a top-down interpretability approach, we extract these features by using only a small number of labeled samples and ensemble them to form a robust uncertainty score. Experiments on multiple QA benchmarks in both in-distribution and out-of-distribution settings show that our method substantially outperforms state-of-the-art unsupervised (sampling-free and sampling-based) and supervised UQ methods, achieving up to a 13-point PRR improvement while incurring a negligible inference overhead.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks</title>
<link>https://arxiv.org/abs/2510.02712</link>
<guid>https://arxiv.org/abs/2510.02712</guid>
<content:encoded><![CDATA[
<div> survival analysis, conversational AI robustness, large language models, semantic drift, failure hazard
<br />
Summary:
This article explores the robustness of Large Language Models (LLMs) in multi-turn dialogues through survival analysis. By analyzing 36,951 conversation turns across 9 LLMs, the study models failure as a time-to-event process. The analysis reveals that abrupt semantic drift between prompts significantly increases the risk of conversational failure, while gradual drift acts as a protective factor, allowing for longer dialogues. Models incorporating interactions demonstrate superior performance in predicting failure events. This research highlights the importance of temporal dynamics in evaluating LLMs' robustness, challenges the necessity of semantic consistency in conversational AI systems, and provides insights for designing more resilient conversational agents. <br /><br />Summary: <div>
arXiv:2510.02712v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have revolutionized conversational AI, yet their robustness in extended multi-turn dialogues remains poorly understood. Existing evaluation frameworks focus on static benchmarks and single-turn assessments, failing to capture the temporal dynamics of conversational degradation that characterize real-world interactions. In this work, we present the first comprehensive survival analysis of conversational AI robustness, analyzing 36,951 conversation turns across 9 state-of-the-art LLMs to model failure as a time-to-event process. Our survival modeling framework-employing Cox proportional hazards, Accelerated Failure Time, and Random Survival Forest approaches-reveals extraordinary temporal dynamics. We find that abrupt, prompt-to-prompt(P2P) semantic drift is catastrophic, dramatically increasing the hazard of conversational failure. In stark contrast, gradual, cumulative drift is highly protective, vastly reducing the failure hazard and enabling significantly longer dialogues. AFT models with interactions demonstrate superior performance, achieving excellent discrimination and exceptional calibration. These findings establish survival analysis as a powerful paradigm for evaluating LLM robustness, offer concrete insights for designing resilient conversational agents, and challenge prevailing assumptions about the necessity of semantic consistency in conversational AI Systems.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TravelBench : Exploring LLM Performance in Low-Resource Domains</title>
<link>https://arxiv.org/abs/2510.02719</link>
<guid>https://arxiv.org/abs/2510.02719</guid>
<content:encoded><![CDATA[
arXiv:2510.02719v1 Announce Type: new 
Abstract: Results on existing LLM benchmarks capture little information over the model capabilities in low-resource tasks, making it difficult to develop effective solutions in these domains. To address these challenges, we curated 14 travel-domain datasets spanning 7 common NLP tasks using anonymised data from real-world scenarios, and analysed the performance across LLMs. We report on the accuracy, scaling behaviour, and reasoning capabilities of LLMs in a variety of tasks. Our results confirm that general benchmarking results are insufficient for understanding model performance in low-resource tasks. Despite the amount of training FLOPs, out-of-the-box LLMs hit performance bottlenecks in complex, domain-specific scenarios. Furthermore, reasoning provides a more significant boost for smaller LLMs by making the model a better judge on certain tasks.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PGMEL: Policy Gradient-based Generative Adversarial Network for Multimodal Entity Linking</title>
<link>https://arxiv.org/abs/2510.02726</link>
<guid>https://arxiv.org/abs/2510.02726</guid>
<content:encoded><![CDATA[
arXiv:2510.02726v1 Announce Type: new 
Abstract: The task of entity linking, which involves associating mentions with their respective entities in a knowledge graph, has received significant attention due to its numerous potential applications. Recently, various multimodal entity linking (MEL) techniques have been proposed, targeted to learn comprehensive embeddings by leveraging both text and vision modalities. The selection of high-quality negative samples can potentially play a crucial role in metric/representation learning. However, to the best of our knowledge, this possibility remains unexplored in existing literature within the framework of MEL. To fill this gap, we address the multimodal entity linking problem in a generative adversarial setting where the generator is responsible for generating high-quality negative samples, and the discriminator is assigned the responsibility for the metric learning tasks. Since the generator is involved in generating samples, which is a discrete process, we optimize it using policy gradient techniques and propose a policy gradient-based generative adversarial network for multimodal entity linking (PGMEL). Experimental results based on Wiki-MEL, Richpedia-MEL and WikiDiverse datasets demonstrate that PGMEL learns meaningful representation by selecting challenging negative samples and outperforms state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IndiCASA: A Dataset and Bias Evaluation Framework in LLMs Using Contrastive Embedding Similarity in the Indian Context</title>
<link>https://arxiv.org/abs/2510.02742</link>
<guid>https://arxiv.org/abs/2510.02742</guid>
<content:encoded><![CDATA[
arXiv:2510.02742v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have gained significant traction across critical domains owing to their impressive contextual understanding and generative capabilities. However, their increasing deployment in high stakes applications necessitates rigorous evaluation of embedded biases, particularly in culturally diverse contexts like India where existing embedding-based bias assessment methods often fall short in capturing nuanced stereotypes. We propose an evaluation framework based on a encoder trained using contrastive learning that captures fine-grained bias through embedding similarity. We also introduce a novel dataset - IndiCASA (IndiBias-based Contextually Aligned Stereotypes and Anti-stereotypes) comprising 2,575 human-validated sentences spanning five demographic axes: caste, gender, religion, disability, and socioeconomic status. Our evaluation of multiple open-weight LLMs reveals that all models exhibit some degree of stereotypical bias, with disability related biases being notably persistent, and religion bias generally lower likely due to global debiasing efforts demonstrating the need for fairer model development.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Path of Self-Evolving Large Language Models: Achieving Data-Efficient Learning via Intrinsic Feedback</title>
<link>https://arxiv.org/abs/2510.02752</link>
<guid>https://arxiv.org/abs/2510.02752</guid>
<content:encoded><![CDATA[
arXiv:2510.02752v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has demonstrated potential in enhancing the reasoning capabilities of large language models (LLMs), but such training typically demands substantial efforts in creating and annotating data. In this work, we explore improving LLMs through RL with minimal data. Our approach alternates between the LLM proposing a task and then attempting to solve it. To minimize data dependency, we introduce two novel mechanisms grounded in self-awareness: (1) self-aware difficulty prediction, where the model learns to assess task difficulty relative to its own abilities and prioritize challenging yet solvable tasks, and (2) self-aware limit breaking, where the model recognizes when a task is beyond its capability boundary and proactively requests external data to break through that limit. Extensive experiments on nine benchmarks showing a 53.8% relative improvement with less than 1.2% extra data demonstrate the efficacy of self-aware RL and underscore the promise of self-evolving agent training.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XTRA: Cross-Lingual Topic Modeling with Topic and Representation Alignments</title>
<link>https://arxiv.org/abs/2510.02788</link>
<guid>https://arxiv.org/abs/2510.02788</guid>
<content:encoded><![CDATA[
arXiv:2510.02788v1 Announce Type: new 
Abstract: Cross-lingual topic modeling aims to uncover shared semantic themes across languages. Several methods have been proposed to address this problem, leveraging both traditional and neural approaches. While previous methods have achieved some improvements in topic diversity, they often struggle to ensure high topic coherence and consistent alignment across languages. We propose XTRA (Cross-Lingual Topic Modeling with Topic and Representation Alignments), a novel framework that unifies Bag-of-Words modeling with multilingual embeddings. XTRA introduces two core components: (1) representation alignment, aligning document-topic distributions via contrastive learning in a shared semantic space; and (2) topic alignment, projecting topic-word distributions into the same space to enforce crosslingual consistency. This dual mechanism enables XTRA to learn topics that are interpretable (coherent and diverse) and well-aligned across languages. Experiments on multilingual corpora confirm that XTRA significantly outperforms strong baselines in topic coherence, diversity, and alignment quality. Code and reproducible scripts are available at https: //github.com/tienphat140205/XTRA.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Computational Framework for Interpretable Text-Based Personality Assessment from Social Media</title>
<link>https://arxiv.org/abs/2510.02811</link>
<guid>https://arxiv.org/abs/2510.02811</guid>
<content:encoded><![CDATA[
arXiv:2510.02811v1 Announce Type: new 
Abstract: Personality refers to individual differences in behavior, thinking, and feeling. With the growing availability of digital footprints, especially from social media, automated methods for personality assessment have become increasingly important. Natural language processing (NLP) enables the analysis of unstructured text data to identify personality indicators. However, two main challenges remain central to this thesis: the scarcity of large, personality-labeled datasets and the disconnect between personality psychology and NLP, which restricts model validity and interpretability. To address these challenges, this thesis presents two datasets -- MBTI9k and PANDORA -- collected from Reddit, a platform known for user anonymity and diverse discussions. The PANDORA dataset contains 17 million comments from over 10,000 users and integrates the MBTI and Big Five personality models with demographic information, overcoming limitations in data size, quality, and label coverage. Experiments on these datasets show that demographic variables influence model validity. In response, the SIMPA (Statement-to-Item Matching Personality Assessment) framework was developed - a computational framework for interpretable personality assessment that matches user-generated statements with validated questionnaire items. By using machine learning and semantic similarity, SIMPA delivers personality assessments comparable to human evaluations while maintaining high interpretability and efficiency. Although focused on personality assessment, SIMPA's versatility extends beyond this domain. Its model-agnostic design, layered cue detection, and scalability make it suitable for various research and practical applications involving complex label taxonomies and variable cue associations with target concepts.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StepChain GraphRAG: Reasoning Over Knowledge Graphs for Multi-Hop Question Answering</title>
<link>https://arxiv.org/abs/2510.02827</link>
<guid>https://arxiv.org/abs/2510.02827</guid>
<content:encoded><![CDATA[
arXiv:2510.02827v1 Announce Type: new 
Abstract: Recent progress in retrieval-augmented generation (RAG) has led to more accurate and interpretable multi-hop question answering (QA). Yet, challenges persist in integrating iterative reasoning steps with external knowledge retrieval. To address this, we introduce StepChain GraphRAG, a framework that unites question decomposition with a Breadth-First Search (BFS) Reasoning Flow for enhanced multi-hop QA. Our approach first builds a global index over the corpus; at inference time, only retrieved passages are parsed on-the-fly into a knowledge graph, and the complex query is split into sub-questions. For each sub-question, a BFS-based traversal dynamically expands along relevant edges, assembling explicit evidence chains without overwhelming the language model with superfluous context. Experiments on MuSiQue, 2WikiMultiHopQA, and HotpotQA show that StepChain GraphRAG achieves state-of-the-art Exact Match and F1 scores. StepChain GraphRAG lifts average EM by 2.57% and F1 by 2.13% over the SOTA method, achieving the largest gain on HotpotQA (+4.70% EM, +3.44% F1). StepChain GraphRAG also fosters enhanced explainability by preserving the chain-of-thought across intermediate retrieval steps. We conclude by discussing how future work can mitigate the computational overhead and address potential hallucinations from large language models to refine efficiency and reliability in multi-hop QA.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Large Language Models for IUCN Red List Species Information</title>
<link>https://arxiv.org/abs/2510.02830</link>
<guid>https://arxiv.org/abs/2510.02830</guid>
<content:encoded><![CDATA[
arXiv:2510.02830v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are rapidly being adopted in conservation to address the biodiversity crisis, yet their reliability for species evaluation is uncertain. This study systematically validates five leading models on 21,955 species across four core IUCN Red List assessment components: taxonomy, conservation status, distribution, and threats. A critical paradox was revealed: models excelled at taxonomic classification (94.9%) but consistently failed at conservation reasoning (27.2% for status assessment). This knowledge-reasoning gap, evident across all models, suggests inherent architectural constraints, not just data limitations. Furthermore, models exhibited systematic biases favoring charismatic vertebrates, potentially amplifying existing conservation inequities. These findings delineate clear boundaries for responsible LLM deployment: they are powerful tools for information retrieval but require human oversight for judgment-based decisions. A hybrid approach is recommended, where LLMs augment expert capacity while human experts retain sole authority over risk assessment and policy.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constraint Satisfaction Approaches to Wordle: Novel Heuristics and Cross-Lexicon Validation</title>
<link>https://arxiv.org/abs/2510.02855</link>
<guid>https://arxiv.org/abs/2510.02855</guid>
<content:encoded><![CDATA[
arXiv:2510.02855v1 Announce Type: new 
Abstract: Wordle presents an algorithmically rich testbed for constraint satisfaction problem (CSP) solving. While existing solvers rely on information-theoretic entropy maximization or frequency-based heuristics without formal constraint treatment, we present the first comprehensive CSP formulation of Wordle with novel constraint-aware solving strategies. We introduce CSP-Aware Entropy, computing information gain after constraint propagation rather than on raw candidate sets, and a Probabilistic CSP framework integrating Bayesian word-frequency priors with logical constraints. Through evaluation on 2,315 English words, CSP-Aware Entropy achieves 3.54 average guesses with 99.9% success rate, a statistically significant 1.7% improvement over Forward Checking (t=-4.82, p<0.001, Cohen's d=0.07) with 46% faster runtime (12.9ms versus 23.7ms per guess). Under 10% noise, CSP-aware approaches maintain 5.3 percentage point advantages (29.0% versus 23.7%, p=0.041), while Probabilistic CSP achieves 100% success across all noise levels (0-20%) through constraint recovery mechanisms. Cross-lexicon validation on 500 Spanish words demonstrates 88% success with zero language-specific tuning, validating that core CSP principles transfer across languages despite an 11.2 percentage point gap from linguistic differences (p<0.001, Fisher's exact test). Our open-source implementation with 34 unit tests achieving 91% code coverage provides reproducible infrastructure for CSP research. The combination of formal CSP treatment, constraint-aware heuristics, probabilistic-logical integration, robustness analysis, and cross-lexicon validation establishes new performance benchmarks demonstrating that principled constraint satisfaction techniques outperform classical information-theoretic and learning-based approaches for structured puzzle-solving domains.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Reflective Generation at Test Time</title>
<link>https://arxiv.org/abs/2510.02919</link>
<guid>https://arxiv.org/abs/2510.02919</guid>
<content:encoded><![CDATA[
arXiv:2510.02919v1 Announce Type: new 
Abstract: Large language models (LLMs) increasingly solve complex reasoning tasks via long chain-of-thought, but their forward-only autoregressive generation process is fragile; early token errors can cascade, which creates a clear need for self-reflection mechanisms. However, existing self-reflection either performs revisions over full drafts or learns self-correction via expensive training, both fundamentally reactive and inefficient. To address this, we propose Self-Reflective Generation at Test Time (SRGen), a lightweight test-time framework that reflects before generating at uncertain points. During token generation, SRGen utilizes dynamic entropy thresholding to identify high-uncertainty tokens. For each identified token, it trains a specific corrective vector, which fully exploits the already generated context for a self-reflective generation to correct the token probability distribution. By retrospectively analyzing the partial output, this self-reflection enables more trustworthy decisions, thereby significantly reducing the probability of errors at highly uncertain points. Evaluated on challenging mathematical reasoning benchmarks and a diverse set of LLMs, SRGen can consistently strengthen model reasoning: improvements in single-pass quality also translate into stronger self-consistency voting. Especially, on AIME2024 with DeepSeek-R1-Distill-Qwen-7B, SRGen yields absolute improvements of +12.0% on Pass@1 and +13.3% on Cons@5. Moreover, our findings position SRGen as a plug-and-play method that integrates reflection into the generation process for reliable LLM reasoning, achieving consistent gains with bounded overhead and broad composability with other training-time (e.g., RLHF) and test-time (e.g., SLOT) techniques.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finding Diamonds in Conversation Haystacks: A Benchmark for Conversational Data Retrieval</title>
<link>https://arxiv.org/abs/2510.02938</link>
<guid>https://arxiv.org/abs/2510.02938</guid>
<content:encoded><![CDATA[
arXiv:2510.02938v1 Announce Type: new 
Abstract: We present the Conversational Data Retrieval (CDR) benchmark, the first comprehensive test set for evaluating systems that retrieve conversation data for product insights. With 1.6k queries across five analytical tasks and 9.1k conversations, our benchmark provides a reliable standard for measuring conversational data retrieval performance. Our evaluation of 16 popular embedding models shows that even the best models reach only around NDCG@10 of 0.51, revealing a substantial gap between document and conversational data retrieval capabilities. Our work identifies unique challenges in conversational data retrieval (implicit state recognition, turn dynamics, contextual references) while providing practical query templates and detailed error analysis across different task categories. The benchmark dataset and code are available at https://github.com/l-yohai/CDR-Benchmark.
]]></content:encoded>
<pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>