<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CL updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CL</link>


<item>
<title>Short-Context Dominance: How Much Local Context Natural Language Actually Needs?</title>
<link>https://arxiv.org/abs/2512.08082</link>
<guid>https://arxiv.org/abs/2512.08082</guid>
<content:encoded><![CDATA[
<div> arXiv, Minimum Context Length, Long-Context Sequences, Distributionally Aware MCL, Decoding Algorithm  

<br /><br />Summary:  
This paper investigates the short-context dominance hypothesis, which posits that for most token sequences, only a small local prefix is necessary to predict subsequent tokens accurately. Using large language models as statistical oracles, the authors measure the Minimum Context Length (MCL) needed to replicate full-context predictions across datasets with varying sequence lengths from 1,000 to 7,000 tokens. They find that 75-80% of sequences require no more than the last 96 tokens for accurate prediction, confirming the prevalence of short-context dominance. To address the challenge posed by the minority of sequences needing longer context, the paper introduces Distributionally Aware MCL (DaMCL), a practical proxy metric that estimates MCL without needing the actual next token and supports various sampling methods beyond greedy decoding. Experiments show that simple thresholding of DaMCL effectively distinguishes between long- and short-context sequences. Building on this, the authors develop a decoding algorithm that uses DaMCL to detect and enhance tokens relevant to long-range dependencies, thereby correcting biases caused by short-context dominance in LLM output distributions. Evaluations across question-answering tasks and model architectures demonstrate that this mitigation significantly improves model performance. <div>
arXiv:2512.08082v1 Announce Type: new 
Abstract: We investigate the short-context dominance hypothesis: that for most sequences, a small local prefix suffices to predict their next tokens. Using large language models as statistical oracles, we measure the minimum context length (MCL) needed to reproduce accurate full-context predictions across datasets with sequences of varying lengths. For sequences with 1-7k tokens from long-context documents, we consistently find that 75-80% require only the last 96 tokens at most. Given the dominance of short-context tokens, we then ask whether it is possible to detect challenging long-context sequences for which a short local prefix does not suffice for prediction. We introduce a practical proxy to MCL, called Distributionally Aware MCL (DaMCL), that does not require knowledge of the actual next-token and is compatible with sampling strategies beyond greedy decoding. Our experiments validate that simple thresholding of the metric defining DaMCL achieves high performance in detecting long vs. short context sequences. Finally, to counter the bias that short-context dominance induces in LLM output distributions, we develop an intuitive decoding algorithm that leverages our detector to identify and boost tokens that are long-range-relevant. Across Q&amp;A tasks and model architectures, we confirm that mitigating the bias improves performance.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptation of Embedding Models to Financial Filings via LLM Distillation</title>
<link>https://arxiv.org/abs/2512.08088</link>
<guid>https://arxiv.org/abs/2512.08088</guid>
<content:encoded><![CDATA[
<div> Keywords: specialized conversational AI, retrieval embedding, financial domain, iterative training, unlabeled corpus<br /><br />Summary:<br /><br />This paper addresses the limitations in applying specialized conversational AI for finance due to high computation costs, latency, and the need for domain-specific relevance. Existing embedding models handle cost and latency but underperform in specialized domains. The authors propose a scalable pipeline that leverages a general-purpose retrieval embedding model as a foundation to train specialized models from unlabeled financial corpora. Their method improves retrieval metrics significantly, including a 27.7% increase in MRR@5 and 44.6% in mean DCG@5 over 21,800 query-document pairs across 14 financial filing types, plus gains in NDCG on 3 of 4 FinanceBench document classes. Instead of fine-tuning LLM generators, they adapt bi-encoder retrieval embeddings for retrieval-augmented generation (RAG) using LLM-judged relevance to distill domain knowledge into a compact retriever. Unlike prior work that pairs synthetic queries with real passages for direct fine-tuning, their pipeline introduces interactive student-teacher model training. This process interleaves retrieval-based mining of hard positive and negative examples from the unlabeled corpus with iterative retraining of the student model, where each iteration mines harder examples to refine performance further. The methodology offers a cost-effective solution for bridging the gap between general-purpose models and specialized domains without relying on extensive human annotations. <div>
arXiv:2512.08088v1 Announce Type: new 
Abstract: Despite advances in generative large language models (LLMs), practical application of specialized conversational AI agents remains constrained by computation costs, latency requirements, and the need for precise domain-specific relevance measures. While existing embedding models address the first two constraints, they underperform on information retrieval in specialized domains like finance. This paper introduces a scalable pipeline that trains specialized models from an unlabeled corpus using a general purpose retrieval embedding model as foundation. Our method yields an average of 27.7% improvement in MRR$\texttt{@}$5, 44.6% improvement in mean DCG$\texttt{@}$5 across 14 financial filing types measured over 21,800 query-document pairs, and improved NDCG on 3 of 4 document classes in FinanceBench. We adapt retrieval embeddings (bi-encoder) for RAG, not LLM generators, using LLM-judged relevance to distill domain knowledge into a compact retriever. There are prior works which pair synthetically generated queries with real passages to directly fine-tune the retrieval model. Our pipeline differs from these by introducing interaction between student and teacher models that interleaves retrieval-based mining of hard positive/negative examples from the unlabeled corpus with iterative retraining of the student model's weights using these examples. Each retrieval iteration uses the refined student model to mine the corpus for progressively harder training examples for the subsequent training iteration. The methodology provides a cost-effective solution to bridging the gap between general-purpose models and specialized domains without requiring labor-intensive human annotation.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Segment, Embed, and Align: A Universal Recipe for Aligning Subtitles to Signing</title>
<link>https://arxiv.org/abs/2512.08094</link>
<guid>https://arxiv.org/abs/2512.08094</guid>
<content:encoded><![CDATA[
<div> Keywords: subtitle alignment, sign language videos, pretrained models, dynamic programming, multilingual framework  

<br /><br />Summary:  
The paper introduces SEA (Segment, Embed, and Align), a universal framework designed to align subtitles—spoken language text with timestamps—to continuous sign language videos. Unlike prior methods dependent on end-to-end training for specific languages or datasets, SEA generalizes across multiple languages and domains. The framework leverages two pretrained models: one to segment video frames into individual signs, and another to embed each sign’s video clip into a shared latent space alongside the corresponding text. Alignment is performed through an efficient dynamic programming algorithm that can process even hour-long video episodes on standard CPUs within minutes. SEA’s adaptability allows it to work with a variety of resource types ranging from small lexicons to large-scale continuous corpora, making it flexible for different scenarios. Experimental results on four different sign language datasets demonstrate that SEA achieves state-of-the-art subtitle-to-sign alignment performance. This highlights SEA’s potential for generating high-quality parallel sign language and spoken language data, which is crucial for advancing automatic sign language processing systems. The authors have made the SEA code and pretrained models openly available, encouraging further research and application development in the field. <div>
arXiv:2512.08094v1 Announce Type: new 
Abstract: The goal of this work is to develop a universal approach for aligning subtitles (i.e., spoken language text with corresponding timestamps) to continuous sign language videos. Prior approaches typically rely on end-to-end training tied to a specific language or dataset, which limits their generality. In contrast, our method Segment, Embed, and Align (SEA) provides a single framework that works across multiple languages and domains. SEA leverages two pretrained models: the first to segment a video frame sequence into individual signs and the second to embed the video clip of each sign into a shared latent space with text. Alignment is subsequently performed with a lightweight dynamic programming procedure that runs efficiently on CPUs within a minute, even for hour-long episodes. SEA is flexible and can adapt to a wide range of scenarios, utilizing resources from small lexicons to large continuous corpora. Experiments on four sign language datasets demonstrate state-of-the-art alignment performance, highlighting the potential of SEA to generate high-quality parallel data for advancing sign language processing. SEA's code and models are openly available.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Universal Adversarial Suffixes Using Calibrated Gumbel-Softmax Relaxation</title>
<link>https://arxiv.org/abs/2512.08123</link>
<guid>https://arxiv.org/abs/2512.08123</guid>
<content:encoded><![CDATA[
<div> Keywords: universal adversarial suffixes, language models, zero-shot classification, transferability, adversarial prompts<br /><br />Summary: This paper investigates the vulnerability of large language models (LMs) when used as zero-shot or few-shot classifiers to adversarial prompts, specifically focusing on universal adversarial suffixes. These suffixes are short sequences of tokens (4-10 tokens) that, when appended to any input, degrade the performance of the model across multiple tasks and different LM architectures. The method trains these suffixes in a differentiable "soft" manner using Gumbel-Softmax relaxation before discretizing them for deployment to ensure effectiveness. Training optimizes calibrated cross-entropy on the label region while masking the correct tokens to prevent trivial information leakage and incorporates entropy regularization to maintain diversity and avoid collapse. A significant contribution is demonstrating that a single trained suffix on one model transfers efficiently to other models, consistently reducing both their accuracy and confidence in predictions. Experiments validate the approach on diverse tasks such as sentiment analysis, natural language inference, paraphrase detection, commonsense question answering, and physical reasoning. Evaluations were conducted across multiple models including Qwen2-1.5B, Phi-1.5, and TinyLlama-1.1B, showing robust and transferable attack performance. This work highlights the broad susceptibility of LMs to universal adversarial suffixes and the challenge of ensuring model robustness in zero-shot scenarios. <div>
arXiv:2512.08123v1 Announce Type: new 
Abstract: Language models (LMs) are often used as zero-shot or few-shot classifiers by scoring label words, but they remain fragile to adversarial prompts. Prior work typically optimizes task- or model-specific triggers, making results difficult to compare and limiting transferability. We study universal adversarial suffixes: short token sequences (4-10 tokens) that, when appended to any input, broadly reduce accuracy across tasks and models. Our approach learns the suffix in a differentiable "soft" form using Gumbel-Softmax relaxation and then discretizes it for inference. Training maximizes calibrated cross-entropy on the label region while masking gold tokens to prevent trivial leakage, with entropy regularization to avoid collapse. A single suffix trained on one model transfers effectively to others, consistently lowering both accuracy and calibrated confidence. Experiments on sentiment analysis, natural language inference, paraphrase detection, commonsense QA, and physical reasoning with Qwen2-1.5B, Phi-1.5, and TinyLlama-1.1B demonstrate consistent attack effectiveness and transfer across tasks and model families.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Universal Adversarial Suffixes for Language Models Using Reinforcement Learning with Calibrated Reward</title>
<link>https://arxiv.org/abs/2512.08131</link>
<guid>https://arxiv.org/abs/2512.08131</guid>
<content:encoded><![CDATA[
<div> Keywords: adversarial suffixes, reinforcement learning, Proximal Policy Optimization, language models, cross-entropy calibration<br /><br />Summary:<br /><br />This paper investigates the vulnerability of language models to adversarial suffixes—short sequences appended to inputs that reliably change model predictions. Existing approaches rely on gradient-based or rule-driven methods that tend to be fragile and specialized to particular tasks or models. To overcome these limitations, the authors propose a reinforcement learning (RL) framework that treats the adversarial suffix as a policy optimized through Proximal Policy Optimization (PPO). The target language model is frozen and serves as a reward oracle during training. To improve the robustness and transferability of the attacks, reward functions are shaped via calibrated cross-entropy losses that remove label biases and aggregate rewards across multiple surface forms of inputs. The method is empirically evaluated on five different NLP benchmarks spanning sentiment analysis, natural language inference, paraphrase detection, and commonsense reasoning. Experiments utilize three distinct publicly available language models: Qwen2-1.5B Instruct, TinyLlama-1.1B Chat, and Phi-1.5. Results demonstrate that suffixes learned through this RL approach more effectively degrade model accuracy and transfer better across different tasks and models compared to prior adversarial trigger generation techniques of similar styles. This suggests that the proposed method produces more generalizable and potent adversarial suffixes for language models. <div>
arXiv:2512.08131v1 Announce Type: new 
Abstract: Language models are vulnerable to short adversarial suffixes that can reliably alter predictions. Previous works usually find such suffixes with gradient search or rule-based methods, but these are brittle and often tied to a single task or model. In this paper, a reinforcement learning framework is used where the suffix is treated as a policy and trained with Proximal Policy Optimization against a frozen model as a reward oracle. Rewards are shaped using calibrated cross-entropy, removing label bias and aggregating across surface forms to improve transferability. The proposed method is evaluated on five diverse NLP benchmark datasets, covering sentiment, natural language inference, paraphrase, and commonsense reasoning, using three distinct language models: Qwen2-1.5B Instruct, TinyLlama-1.1B Chat, and Phi-1.5. Results show that RL-trained suffixes consistently degrade accuracy and transfer more effectively across tasks and models than previous adversarial triggers of similar genres.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ClinicalTrialsHub: Bridging Registries and Literature for Comprehensive Clinical Trial Access</title>
<link>https://arxiv.org/abs/2512.08193</link>
<guid>https://arxiv.org/abs/2512.08193</guid>
<content:encoded><![CDATA[
<div> Keywords: ClinicalTrialsHub, ClinicalTrials.gov, PubMed, large language models, question answering<br /><br />Summary:<br /><br />1. ClinicalTrialsHub is an interactive search platform that integrates all data from ClinicalTrials.gov and enhances it by automatically extracting and structuring clinical trial relevant information from PubMed research articles.<br />2. The system increases access to structured clinical trial data by 83.8% compared to using ClinicalTrials.gov data alone, aiming to facilitate easier access for patients, clinicians, researchers, and policymakers to support evidence-based medicine.<br />3. It leverages advanced large language models such as GPT-5.1 and Gemini-3-Pro to boost accessibility and functionality.<br />4. The platform is capable of parsing full-text research articles to extract structured trial information, converting user queries into structured database searches, and providing an attributed question-answering system that generates evidence-based answers linked to specific source sentences.<br />5. The utility of ClinicalTrialsHub was validated through a user study involving clinicians, clinical researchers, and PhD students in pharmaceutical sciences and nursing, alongside systematic automatic evaluations of its information extraction and question answering performance. <div>
arXiv:2512.08193v1 Announce Type: new 
Abstract: We present ClinicalTrialsHub, an interactive search-focused platform that consolidates all data from ClinicalTrials.gov and augments it by automatically extracting and structuring trial-relevant information from PubMed research articles. Our system effectively increases access to structured clinical trial data by 83.8% compared to relying on ClinicalTrials.gov alone, with potential to make access easier for patients, clinicians, researchers, and policymakers, advancing evidence-based medicine. ClinicalTrialsHub uses large language models such as GPT-5.1 and Gemini-3-Pro to enhance accessibility. The platform automatically parses full-text research articles to extract structured trial information, translates user queries into structured database searches, and provides an attributed question-answering system that generates evidence-grounded answers linked to specific source sentences. We demonstrate its utility through a user study involving clinicians, clinical researchers, and PhD students of pharmaceutical sciences and nursing, and a systematic automatic evaluation of its information extraction and question answering capabilities.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are generative AI text annotations systematically biased?</title>
<link>https://arxiv.org/abs/2512.08404</link>
<guid>https://arxiv.org/abs/2512.08404</guid>
<content:encoded><![CDATA[
<div> Keywords: GLLM annotations, bias, manual annotations, F1 scores, downstream results<br /><br />Summary:<br /><br />This paper explores the presence of bias in Generative Large Language Model (GLLM) annotations by conceptually replicating manual annotations made by Boukes (2024). Five distinct concepts—political content, interactivity, rationality, incivility, and ideology—are annotated using various GLLMs including Llama3.1:8b, Llama3.3:70b, GPT4o, and Qwen2.5:72b, each combined with five different prompts to assess consistency and differences. While the GLLMs demonstrate adequate performance measured through F1 scores, their annotation outputs differ considerably from manual annotations, showing distinct prevalence rates for annotated concepts. These differences not only lead to substantively different downstream analytical results but also reveal a systematic bias pattern: the GLLM annotations show greater overlap among themselves than with the manual annotations, suggesting a shared model-based bias. Crucially, simply looking at F1 scores does not capture the extent of these biases, meaning performance metrics alone are insufficient for evaluating annotation quality. This study highlights the challenges and limitations in using GLLMs as substitutes for human annotators, calling for caution and further research on bias mitigation in automated content annotation tasks. <div>
arXiv:2512.08404v1 Announce Type: new 
Abstract: This paper investigates bias in GLLM annotations by conceptually replicating manual annotations of Boukes (2024). Using various GLLMs (Llama3.1:8b, Llama3.3:70b, GPT4o, Qwen2.5:72b) in combination with five different prompts for five concepts (political content, interactivity, rationality, incivility, and ideology). We find GLLMs perform adequate in terms of F1 scores, but differ from manual annotations in terms of prevalence, yield substantively different downstream results, and display systematic bias in that they overlap more with each other than with manual annotations. Differences in F1 scores fail to account for the degree of bias.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Triggers my Model? Contrastive Explanations Inform Gender Choices by Translation Models</title>
<link>https://arxiv.org/abs/2512.08440</link>
<guid>https://arxiv.org/abs/2512.08440</guid>
<content:encoded><![CDATA[
<div> gender bias, machine translation, interpretability, saliency attribution, linguistic analysis<br /><br />Summary:<br /><br />This study focuses on understanding gender bias in machine translation (MT) and large language models (LLMs) by moving beyond merely measuring bias to exploring its origins. The research uses gender-ambiguous natural source data to investigate which input tokens or contextual elements influence the model’s choice of gender inflection in the target language. To achieve this, the authors employ contrastive explanations and compute saliency attribution to highlight the importance of specific source words in the translation decision process. A key challenge addressed is the absence of a clear scoring threshold for attribution, which they tackle by examining different levels of source word attribution on gender decisions. The study also compares the salient source words identified by the model with human perceptions of gender, finding a significant overlap. Furthermore, a linguistic analysis of these salient words is provided to deepen the understanding of gender influences on translation models. Overall, the work highlights the importance of interpretability in revealing how MT models make gendered decisions, shows the alignment between model and human perceptions, and emphasizes the potential of leveraging these insights to develop strategies for mitigating gender bias in translation systems. <div>
arXiv:2512.08440v1 Announce Type: new 
Abstract: Interpretability can be implemented as a means to understand decisions taken by (black box) models, such as machine translation (MT) or large language models (LLMs). Yet, research in this area has been limited in relation to a manifested problem in these models: gender bias. With this research, we aim to move away from simply measuring bias to exploring its origins. Working with gender-ambiguous natural source data, this study examines which context, in the form of input tokens in the source sentence, influences (or triggers) the translation model choice of a certain gender inflection in the target language. To analyse this, we use contrastive explanations and compute saliency attribution. We first address the challenge of a lacking scoring threshold and specifically examine different attribution levels of source words on the model gender decisions in the translation. We compare salient source words with human perceptions of gender and demonstrate a noticeable overlap between human perceptions and model attribution. Additionally, we provide a linguistic analysis of salient words. Our work showcases the relevance of understanding model translation decisions in terms of gender, how this compares to human decisions and that this information should be leveraged to mitigate gender bias.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Soft Inductive Bias Approach via Explicit Reasoning Perspectives in Inappropriate Utterance Detection Using Large Language Models</title>
<link>https://arxiv.org/abs/2512.08480</link>
<guid>https://arxiv.org/abs/2512.08480</guid>
<content:encoded><![CDATA[
<div> inappropriate utterance detection, Korean large language model, soft inductive bias, chain-of-thought reasoning, conversational text safety  

<br /><br />Summary:  
The paper addresses the problem of detecting inappropriate utterances in online conversations, especially within Korean-language environments where anonymity often leads to verbal abuse and criminal behavior. Recognizing the need for safer communication, the authors propose a novel soft inductive bias method that explicitly defines reasoning perspectives to guide the inference process of large language models. This approach helps promote rational decision-making and reduces reasoning errors. They fine-tune a Korean large language model named Kanana-1.5 using this method, comparing it with standard supervised learning approaches through both quantitative and qualitative evaluations. Experimental results show that Kanana-1.5 achieves an average accuracy of 87.0046%, representing a 3.89% improvement over conventional methods. The findings suggest that the proposed method enables language models to make more precise and consistent judgments by constraining reasoning perspectives rather than merely imitating knowledge. Overall, this research contributes a promising technique for enhancing the detection of inappropriate utterances in conversational texts, thereby supporting the development of safer online communication environments. <div>
arXiv:2512.08480v1 Announce Type: new 
Abstract: Recent incidents in certain online games and communities, where anonymity is guaranteed, show that unchecked inappropriate remarks frequently escalate into verbal abuse and even criminal behavior, raising significant social concerns. Consequently, there is a growing need for research on techniques that can detect inappropriate utterances within conversational texts to help build a safer communication environment. Although large-scale language models trained on Korean corpora and chain-of-thought reasoning have recently gained attention, research applying these approaches to inappropriate utterance detection remains limited. In this study, we propose a soft inductive bias approach that explicitly defines reasoning perspectives to guide the inference process, thereby promoting rational decision-making and preventing errors that may arise during reasoning. We fine-tune a Korean large language model using the proposed method and conduct both quantitative performance comparisons and qualitative evaluations across different training strategies. Experimental results show that the Kanana-1.5 model achieves an average accuracy of 87.0046, improving by approximately 3.89 percent over standard supervised learning. These findings indicate that the proposed method goes beyond simple knowledge imitation by large language models and enables more precise and consistent judgments through constrained reasoning perspectives, demonstrating its effectiveness for inappropriate utterance detection.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Curriculum Guided Massive Multi Agent System Solving For Robust Long Horizon Tasks</title>
<link>https://arxiv.org/abs/2512.08545</link>
<guid>https://arxiv.org/abs/2512.08545</guid>
<content:encoded><![CDATA[
<div> Keywords: hierarchical multi-agent, spatial curriculum, long-horizon reasoning, Negative Log-Likelihood, Thompson Sampling<br /><br />Summary:<br /><br />This paper addresses challenges faced by Large Language Models and multi-agent systems in performing long-horizon reasoning tasks and managing high computational costs. The authors propose a hierarchical multi-agent architecture that organizes a 64×64 grid of lightweight agents, coordinated with a selective oracle to distribute reasoning effectively. To facilitate learning, a spatial curriculum is employed, gradually expanding the grid's operational area, enabling agents to first master simpler central tasks before progressing to more complex peripheral tasks. Reliability is enhanced by incorporating Negative Log-Likelihood (NLL) as a confidence measure, which helps prioritize training focus on regions where agents demonstrate both accuracy and good calibration. The curriculum management is driven by a Thompson Sampling strategy that adaptively selects training zones based on agents' competence and reward signals derived from NLL. The approach is evaluated on a spatially grounded Tower of Hanoi benchmark, reflecting the nature of many robotic manipulation and planning challenges. Experimental results show that this system achieves better stability, reduces reliance on the oracle, and improves long-range reasoning through effective cooperation among agents distributed across the grid. <div>
arXiv:2512.08545v1 Announce Type: new 
Abstract: Large Language Models and multi-agent systems have shown promise in decomposing complex tasks, yet they struggle with long-horizon reasoning tasks and escalating computation cost. This work introduces a hierarchical multi-agent architecture that distributes reasoning across a 64*64 grid of lightweight agents, supported by a selective oracle. A spatial curriculum progressively expands the operational region of the grid, ensuring that agents master easier central tasks before tackling harder peripheral ones. To improve reliability, the system integrates Negative Log-Likelihood as a measure of confidence, allowing the curriculum to prioritize regions where agents are both accurate and well calibrated. A Thompson Sampling curriculum manager adaptively chooses training zones based on competence and NLL-driven reward signals. We evaluate the approach on a spatially grounded Tower of Hanoi benchmark, which mirrors the long-horizon structure of many robotic manipulation and planning tasks. Results demonstrate improved stability, reduced oracle usage, and stronger long-range reasoning from distributed agent cooperation.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HealthcareNLP: where are we and what is next?</title>
<link>https://arxiv.org/abs/2512.08617</link>
<guid>https://arxiv.org/abs/2512.08617</guid>
<content:encoded><![CDATA[
<div> HealthcareNLP, synthetic data, explainable clinical NLP, retrieval augmented generation, Patient Public Involvement and Engagement<br /><br />Summary:<br /><br />This tutorial focuses on the applications of Natural Language Processing (NLP) within the healthcare domain, highlighting achievements and future challenges. It addresses gaps in existing reviews by covering essential topics such as synthetic data generation to mitigate privacy issues, explainable clinical NLP for better integration, and advanced methodologies like retrieval augmented generation and neural symbolic integration of large language models (LLMs) with knowledge graphs (KGs). The tutorial is structured into three hierarchical layers: the data/resource layer, which includes annotation guidelines, ethical approvals, governance, and synthetic data; the NLP-Eval layer, focusing on tasks such as named entity recognition (NER), relation extraction (RE), sentiment analysis, and linking/coding, supporting explainable HealthAI; and the patients layer, addressing Patient Public Involvement and Engagement (PPIE), health literacy, translation, text simplification, summarisation, and shared decision-making. A hands-on session will allow participants to practically engage with HealthcareNLP applications. The target audience includes NLP practitioners in healthcare, researchers interested in domain applications, healthcare researchers, and students. No prior knowledge is required, making this an introductory tutorial. Materials and resources are available on GitHub at https://github.com/4dpicture/HealthNLP. <div>
arXiv:2512.08617v1 Announce Type: new 
Abstract: This proposed tutorial focuses on Healthcare Domain Applications of NLP, what we have achieved around HealthcareNLP, and the challenges that lie ahead for the future. Existing reviews in this domain either overlook some important tasks, such as synthetic data generation for addressing privacy concerns, or explainable clinical NLP for improved integration and implementation, or fail to mention important methodologies, including retrieval augmented generation and the neural symbolic integration of LLMs and KGs. In light of this, the goal of this tutorial is to provide an introductory overview of the most important sub-areas of a patient- and resource-oriented HealthcareNLP, with three layers of hierarchy: data/resource layer: annotation guidelines, ethical approvals, governance, synthetic data; NLP-Eval layer: NLP tasks such as NER, RE, sentiment analysis, and linking/coding with categorised methods, leading to explainable HealthAI; patients layer: Patient Public Involvement and Engagement (PPIE), health literacy, translation, simplification, and summarisation (also NLP tasks), and shared decision-making support. A hands-on session will be included in the tutorial for the audience to use HealthcareNLP applications. The target audience includes NLP practitioners in the healthcare application domain, NLP researchers who are interested in domain applications, healthcare researchers, and students from NLP fields. The type of tutorial is "Introductory to CL/NLP topics (HealthcareNLP)" and the audience does not need prior knowledge to attend this. Tutorial materials: https://github.com/4dpicture/HealthNLP
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QSTN: A Modular Framework for Robust Questionnaire Inference with Large Language Models</title>
<link>https://arxiv.org/abs/2512.08646</link>
<guid>https://arxiv.org/abs/2512.08646</guid>
<content:encoded><![CDATA[
<div> Questionnaire, large language models, response generation, in-silico surveys, prompt perturbations<br /><br />Summary:<br /><br />1. The article introduces QSTN, an open-source Python framework designed to generate responses from questionnaire-style prompts, facilitating in-silico surveys and annotation tasks using large language models (LLMs).<br /><br />2. QSTN supports robust evaluation by allowing researchers to test questionnaire presentation formats, prompt perturbations, and various response generation methods.<br /><br />3. The authors conducted an extensive evaluation involving over 40 million survey responses, demonstrating that both the structure of questions and the methods used for response generation significantly affect how closely the generated answers align with human responses.<br /><br />4. The framework achieves this alignment with a computational cost significantly lower than traditional methods, making it efficient for large-scale experimental setups.<br /><br />5. Additionally, QSTN offers a no-code user interface, enabling researchers without programming expertise to easily set up and run experiments, thus promoting reproducibility and reliability in LLM-based research. <div>
arXiv:2512.08646v1 Announce Type: new 
Abstract: We introduce QSTN, an open-source Python framework for systematically generating responses from questionnaire-style prompts to support in-silico surveys and annotation tasks with large language models (LLMs). QSTN enables robust evaluation of questionnaire presentation, prompt perturbations, and response generation methods. Our extensive evaluation ($>40 $ million survey responses) shows that question structure and response generation methods have a significant impact on the alignment of generated survey responses with human answers, and can be obtained for a fraction of the compute cost. In addition, we offer a no-code user interface that allows researchers to set up robust experiments with LLMs without coding knowledge. We hope that QSTN will support the reproducibility and reliability of LLM-based research in the future.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Agentic AI System for Multi-Framework Communication Coding</title>
<link>https://arxiv.org/abs/2512.08659</link>
<guid>https://arxiv.org/abs/2512.08659</guid>
<content:encoded><![CDATA[
<div> Clinical communication, annotation, large language models, multi-agent system, retrieval-augmented generation  

<br /><br />Summary:  
This study addresses the challenge of large-scale human annotation of patient-provider conversations, which is labor-intensive, inconsistent, and difficult to scale. The authors introduce MOSAIC, a Multi-framework Structured Agentic AI system for Clinical Communication, designed to improve adaptability, interpretability, and reliability compared to existing single-task large language model approaches. MOSAIC is built on a LangGraph-based architecture that coordinates four specialized agents: a Plan Agent for selecting codebooks and planning workflows; an Update Agent to keep retrieval databases current; Annotation Agents that utilize codebook-guided retrieval-augmented generation (RAG) with dynamic few-shot prompting for accurate annotation; and a Verification Agent for consistency checks and feedback. The system was developed and evaluated using 26 gold standard annotated transcripts for training and 50 for testing, covering rheumatology and OB/GYN clinical domains. On the test set, MOSAIC achieved a high overall F1 score of 0.928, with particularly strong performance in the Rheumatology domain (F1 = 0.962) and in annotating Patient Behavior such as questioning, preferences, and assertiveness. Ablation studies confirmed that MOSAIC significantly outperforms baseline benchmarks, demonstrating the effectiveness of the multi-agent, multi-framework approach in clinical conversation annotation. <div>
arXiv:2512.08659v1 Announce Type: new 
Abstract: Clinical communication is central to patient outcomes, yet large-scale human annotation of patient-provider conversation remains labor-intensive, inconsistent, and difficult to scale. Existing approaches based on large language models typically rely on single-task models that lack adaptability, interpretability, and reliability, especially when applied across various communication frameworks and clinical domains. In this study, we developed a Multi-framework Structured Agentic AI system for Clinical Communication (MOSAIC), built on a LangGraph-based architecture that orchestrates four core agents, including a Plan Agent for codebook selection and workflow planning, an Update Agent for maintaining up-to-date retrieval databases, a set of Annotation Agents that applies codebook-guided retrieval-augmented generation (RAG) with dynamic few-shot prompting, and a Verification Agent that provides consistency checks and feedback. To evaluate performance, we compared MOSAIC outputs against gold-standard annotations created by trained human coders. We developed and evaluated MOSAIC using 26 gold standard annotated transcripts for training and 50 transcripts for testing, spanning rheumatology and OB/GYN domains. On the test set, MOSAIC achieved an overall F1 score of 0.928. Performance was highest in the Rheumatology subset (F1 = 0.962) and strongest for Patient Behavior (e.g., patients asking questions, expressing preferences, or showing assertiveness). Ablations revealed that MOSAIC outperforms baseline benchmarking.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Essay Scoring and Feedback Generation in Basque Language Learning</title>
<link>https://arxiv.org/abs/2512.08713</link>
<guid>https://arxiv.org/abs/2512.08713</guid>
<content:encoded><![CDATA[
<div> Keywords: Automatic Essay Scoring, Basque language, feedback generation, Latxa model, CEFR C1 proficiency level<br /><br />Summary:<br /> This paper presents the first publicly available dataset for Automatic Essay Scoring (AES) and feedback generation specifically designed for the Basque language targeting learners at the CEFR C1 proficiency level. The dataset consists of 3,200 essays collected from HABE and annotated by expert evaluators, including criterion-specific scores for correctness, richness, coherence, cohesion, and task alignment, accompanied by detailed feedback and examples of common learner errors. The authors fine-tune several open-source models—RoBERTa-EusCrawl along with Latxa models of 8 billion and 70 billion parameters—for both essay scoring and explanation/feedback generation. Experimental results show that traditional encoder models are reliable for AES, but supervised fine-tuning of the Latxa models significantly improves performance, outperforming state-of-the-art closed-source systems such as GPT-5 and Claude Sonnet 4.5 in scoring consistency and feedback quality. Additionally, the paper introduces a novel evaluation methodology that correlates automatic consistency metrics with expert human validation of the learner error extraction process. These findings demonstrate that the fine-tuned Latxa model produces pedagogically meaningful, criterion-aligned feedback and identifies a broader range of error types than proprietary models. This dataset and benchmark provide an important foundation for transparent, reproducible, and education-oriented NLP research in low-resource languages like Basque. <div>
arXiv:2512.08713v1 Announce Type: new 
Abstract: This paper introduces the first publicly available dataset for Automatic Essay Scoring (AES) and feedback generation in Basque, targeting the CEFR C1 proficiency level. The dataset comprises 3,200 essays from HABE, each annotated by expert evaluators with criterion specific scores covering correctness, richness, coherence, cohesion, and task alignment enriched with detailed feedback and error examples. We fine-tune open-source models, including RoBERTa-EusCrawl and Latxa 8B/70B, for both scoring and explanation generation. Our experiments show that encoder models remain highly reliable for AES, while supervised fine-tuning (SFT) of Latxa significantly enhances performance, surpassing state-of-the-art (SoTA) closed-source systems such as GPT-5 and Claude Sonnet 4.5 in scoring consistency and feedback quality. We also propose a novel evaluation methodology for assessing feedback generation, combining automatic consistency metrics with expert-based validation of extracted learner errors. Results demonstrate that the fine-tuned Latxa model produces criterion-aligned, pedagogically meaningful feedback and identifies a wider range of error types than proprietary models. This resource and benchmark establish a foundation for transparent, reproducible, and educationally grounded NLP research in low-resource languages such as Basque.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fluent Alignment with Disfluent Judges: Post-training for Lower-resource Languages</title>
<link>https://arxiv.org/abs/2512.08777</link>
<guid>https://arxiv.org/abs/2512.08777</guid>
<content:encoded><![CDATA[
<div> Keywords: lower-resource languages, preference optimization, on-policy training, Norwegian Bokmål, fluency evaluation<br /><br />Summary:<br /><br />1. The paper addresses the challenge of developing fluent language models for lower-resource languages where native speaker data and high-quality language models are scarce. 2. Traditional preference-optimization methods have focused primarily on English and Chinese, leaving a gap in effective approaches for other languages. 3. The authors propose a post-training method that uses on-policy training to align language models with preference rewards while preserving fluency, even when reward models are disfluent. 4. They compare this on-policy approach with two common alternatives: supervised finetuning on machine-translated datasets and multilingual finetuning, highlighting the advantages of their method. 5. A case study on Norwegian Bokmål is conducted, with fluency assessed by native speakers to validate the approach. 6. Results demonstrate that the on-policy training method outperforms other techniques without needing any instruction-tuning data or hard-to-acquire resources, making it a promising strategy for improving language model fluency in lower-resource languages. <div>
arXiv:2512.08777v1 Announce Type: new 
Abstract: We propose a post-training method for lower-resource languages that preserves fluency of language models even when aligned by disfluent reward models. Preference-optimization is now a well-researched topic, but previous work has mostly addressed models for English and Chinese. Lower-resource languages lack both datasets written by native speakers and language models capable of generating fluent synthetic data. Thus, in this work, we focus on developing a fluent preference-aligned language model without any instruction-tuning data in the target language. Our approach uses an on-policy training method, which we compare with two common approaches: supervised finetuning on machine-translated data and multilingual finetuning. We conduct a case study on Norwegian Bokm{\aa}l and evaluate fluency through native-speaker assessments. The results show that the on-policy aspect is crucial and outperforms the alternatives without relying on any hard-to-obtain data.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic Alignment of LLMs</title>
<link>https://arxiv.org/abs/2512.08786</link>
<guid>https://arxiv.org/abs/2512.08786</guid>
<content:encoded><![CDATA[
<div> Keywords: federated learning, large language models, alignment, fairness, adaptive aggregation<br /><br />Summary:<br /><br />This paper tackles the problem of aligning large language models (LLMs) to diverse human preferences within federated learning (FL) setups, where traditional methods often fail to capture the heterogeneity of viewpoints. It introduces a comprehensive evaluation framework designed to systematically explore the balance between alignment quality and fairness when aggregating human preference signals. The proposed federated setting involves groups locally evaluating model rollouts and generating reward signals, with the central server aggregating these rewards at the group level without accessing any raw data, ensuring privacy. The study compares standard reward aggregation methods such as minimum, maximum, and average aggregation, and presents a novel adaptive aggregation technique that adjusts preference weights dynamically based on each group's historical alignment performance. Experiments focus on question-answering tasks and employ a Proximal Policy Optimization (PPO)-based Reinforcement Learning from Human Feedback (RLHF) pipeline. Results demonstrate that the adaptive weighting scheme consistently yields better fairness outcomes while maintaining competitive alignment scores compared to traditional aggregation methods. Ultimately, this work contributes a practical approach and evaluation methodology for creating fairly aligned, pluralistic LLMs that respect the diversity of human preferences in decentralized environments. <div>
arXiv:2512.08786v1 Announce Type: new 
Abstract: This paper addresses the challenge of aligning large language models (LLMs) with diverse human preferences within federated learning (FL) environments, where standard methods often fail to adequately represent diverse viewpoints. We introduce a comprehensive evaluation framework that systematically assesses the trade-off between alignment quality and fairness when using different aggregation strategies for human preferences. In our federated setting, each group locally evaluates rollouts and produces reward signals, and the server aggregates these group-level rewards without accessing any raw data. Specifically, we evaluate standard reward aggregation techniques (min, max, and average) and introduce a novel adaptive scheme that dynamically adjusts preference weights based on a group's historical alignment performance. Our experiments on question-answering (Q/A) tasks using a PPO-based RLHF pipeline demonstrate that our adaptive approach consistently achieves superior fairness while maintaining competitive alignment scores. This work offers a robust methodology for evaluating LLM behavior across diverse populations and provides a practical solution for developing truly pluralistic and fairly aligned models.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ask, Answer, and Detect: Role-Playing LLMs for Personality Detection with Question-Conditioned Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2512.08814</link>
<guid>https://arxiv.org/abs/2512.08814</guid>
<content:encoded><![CDATA[
<div> Keywords: personality detection, large language models, psychometric questionnaires, Mixture-of-Experts, multi-task learning  

<br /><br />Summary:  
1. This paper addresses the challenge of detecting human personality traits from social media posts, a task important for applications such as personalized recommendations and mental health assessments.  
2. Existing methods typically follow a "posts -> user vector -> labels" pipeline, but they struggle due to limited labeled data and the difficulty of linking text semantics to abstract psychological constructs.  
3. The authors propose ROME, a novel framework that integrates psychological knowledge into the detection process by using large language models' role-play capabilities to simulate answers to validated psychometric questionnaires, thereby grounding user post representation in interpretable, question-level responses.  
4. ROME includes a question-conditioned Mixture-of-Experts module to jointly process post and question representations, learning to produce questionnaire item answers under explicit supervision.  
5. These question-level answers are then summarized into an answer vector and fused with the user representation in a multi-task learning setup where question answering acts as an auxiliary task, improving the personality label predictions.  
6. Experiments on two real-world datasets demonstrate that ROME significantly outperforms state-of-the-art baselines, with a reported 15.41% improvement on the Kaggle dataset, highlighting the effectiveness of incorporating psychological knowledge and multi-task learning in personality detection. <div>
arXiv:2512.08814v1 Announce Type: new 
Abstract: Understanding human personality is crucial for web applications such as personalized recommendation and mental health assessment. Existing studies on personality detection predominantly adopt a "posts -> user vector -> labels" modeling paradigm, which encodes social media posts into user representations for predicting personality labels (e.g., MBTI labels). While recent advances in large language models (LLMs) have improved text encoding capacities, these approaches remain constrained by limited supervision signals due to label scarcity, and under-specified semantic mappings between user language and abstract psychological constructs. We address these challenges by proposing ROME, a novel framework that explicitly injects psychological knowledge into personality detection. Inspired by standardized self-assessment tests, ROME leverages LLMs' role-play capability to simulate user responses to validated psychometric questionnaires. These generated question-level answers transform free-form user posts into interpretable, questionnaire-grounded evidence linking linguistic cues to personality labels, thereby providing rich intermediate supervision to mitigate label scarcity while offering a semantic reasoning chain that guides and simplifies the text-to-personality mapping learning. A question-conditioned Mixture-of-Experts module then jointly routes over post and question representations, learning to answer questionnaire items under explicit supervision. The predicted answers are summarized into an interpretable answer vector and fused with the user representation for final prediction within a multi-task learning framework, where question answering serves as a powerful auxiliary task for personality detection. Extensive experiments on two real-world datasets demonstrate that ROME consistently outperforms state-of-the-art baselines, achieving improvements (15.41% on Kaggle dataset).
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Depth-Grown Models Overcome the Curse of Depth? An In-Depth Analysis</title>
<link>https://arxiv.org/abs/2512.08819</link>
<guid>https://arxiv.org/abs/2512.08819</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformer depth growth, MIDAS, Curse of Depth, residual stream, computational blocks<br /><br />Summary: This paper investigates the benefits of gradually increasing the depth of Transformer models during training, a method previously shown by MIDAS (Saunshi et al., 2024) to reduce training costs and enhance reasoning performance. The authors connect these benefits to the "Curse of Depth," where layers in the latter half of standard pre-layernorm Transformers contribute significantly less to the output than earlier layers, as identified in recent studies (Sun et al., 2025; Csordás et al., 2025). Through depth-wise analysis, they demonstrate that the gradual middle stacking approach leads to more effective use of the model's full depth compared to non-grown models. This gradual growth also modifies the residual stream structure within the network, which helps form distinct, permutable computational blocks that likely underpin improved functionality. Additionally, the paper proposes a lightweight modification to MIDAS, which further enhances performance on downstream reasoning benchmarks. Overall, their findings emphasize that gradually growing model depth not only alleviates underutilization issues in traditional deep Transformers but also fosters the emergence of explicit computational circuits, contributing to stronger model reasoning capabilities. <div>
arXiv:2512.08819v1 Announce Type: new 
Abstract: Gradually growing the depth of Transformers during training can not only reduce training cost but also lead to improved reasoning performance, as shown by MIDAS (Saunshi et al., 2024). Thus far, however, a mechanistic understanding of these gains has been missing. In this work, we establish a connection to recent work showing that layers in the second half of non-grown, pre-layernorm Transformers contribute much less to the final output distribution than those in the first half - also known as the Curse of Depth (Sun et al., 2025, Csord\'as et al., 2025). Using depth-wise analyses, we demonstrate that growth via gradual middle stacking yields more effective utilization of model depth, alters the residual stream structure, and facilitates the formation of permutable computational blocks. In addition, we propose a lightweight modification of MIDAS that yields further improvements in downstream reasoning benchmarks. Overall, this work highlights how the gradual growth of model depth can lead to the formation of distinct computational circuits and overcome the limited depth utilization seen in standard non-grown models.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2512.08892</link>
<guid>https://arxiv.org/abs/2512.08892</guid>
<content:encoded><![CDATA[
<div> Retrieval-Augmented Generation, hallucination detection, sparse autoencoders, large language models, interpretability<br /><br />Summary:<br />1. Retrieval-Augmented Generation (RAG) enhances factual accuracy in large language models (LLMs) by grounding text generation in retrieved evidence but struggles with hallucinations where outputs contradict or go beyond sources.  
2. Existing hallucination detection methods either require large annotated datasets for training detectors or use external LLM judges, both having drawbacks like high cost or limited accuracy.  
3. To address this, the authors leverage mechanistic interpretability using sparse autoencoders (SAEs) to isolate internal activations specifically triggered during hallucinations in RAG.  
4. They develop RAGLens, a lightweight detector that uses LLM’s internal representations combined with information-based feature selection and additive feature modeling to accurately identify unfaithful outputs.  
5. RAGLens outperforms existing methods, offers interpretable rationales for detection decisions, enables effective post-hoc mitigation of hallucinations, and provides new insights into how hallucination signals are distributed within LLMs.  
6. The paper also shares the code publicly for reproducibility and further research. <div>
arXiv:2512.08892v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) improves the factuality of large language models (LLMs) by grounding outputs in retrieved evidence, but faithfulness failures, where generations contradict or extend beyond the provided sources, remain a critical challenge. Existing hallucination detection methods for RAG often rely either on large-scale detector training, which requires substantial annotated data, or on querying external LLM judges, which leads to high inference costs. Although some approaches attempt to leverage internal representations of LLMs for hallucination detection, their accuracy remains limited. Motivated by recent advances in mechanistic interpretability, we employ sparse autoencoders (SAEs) to disentangle internal activations, successfully identifying features that are specifically triggered during RAG hallucinations. Building on a systematic pipeline of information-based feature selection and additive feature modeling, we introduce RAGLens, a lightweight hallucination detector that accurately flags unfaithful RAG outputs using LLM internal representations. RAGLens not only achieves superior detection performance compared to existing methods, but also provides interpretable rationales for its decisions, enabling effective post-hoc mitigation of unfaithful RAG. Finally, we justify our design choices and reveal new insights into the distribution of hallucination-related signals within LLMs. The code is available at https://github.com/Teddy-XiongGZ/RAGLens.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models</title>
<link>https://arxiv.org/abs/2512.07843</link>
<guid>https://arxiv.org/abs/2512.07843</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, adaptive parallel reasoning, ThreadWeaver, chain-of-thought, inference efficiency<br /><br />Summary:  
1) The paper addresses the latency challenges in Large Language Models (LLMs) caused by inherently sequential decoding during complex reasoning tasks.  
2) It introduces ThreadWeaver, a novel framework for adaptive parallel reasoning designed to reduce inference time while maintaining accuracy comparable to popular sequential reasoning models.  
3) ThreadWeaver incorporates three key innovations: a two-stage parallel trajectory generator for creating extensive, high-quality chain-of-thought (CoT) data with parallel annotations; a trie-based training and inference co-design that supports parallel reasoning on standard autoregressive inference engines without modifying position embeddings or KV caches; and a parallelization-aware reinforcement learning framework that helps balance accuracy and parallelization efficiency.  
4) Experiments conducted using ThreadWeaver on the Qwen3-8B model across six difficult mathematical reasoning benchmarks show it achieves an average accuracy of 71.9%, and up to 79.9% on AIME24, which is comparable to state-of-the-art sequential reasoning baselines.  
5) ThreadWeaver also provides up to a 1.53x speedup in token-level inference latency, establishing a new Pareto frontier that optimally balances accuracy and computational efficiency in reasoning tasks. <div>
arXiv:2512.07843v1 Announce Type: cross 
Abstract: Scaling inference-time computation has enabled Large Language Models (LLMs) to achieve strong reasoning performance, but inherently sequential decoding leads to substantial latency, especially on complex tasks. Recent work on adaptive parallel reasoning aims to improve inference efficiency by decomposing the problem-solving process into concurrent reasoning threads when beneficial. However, existing methods on realistic tasks are either limited to supervised behavior cloning or exhibit significant accuracy drops compared to widely-used sequential long chain-of-thought (CoT) baselines. Moreover, many require customized inference engines, complicating deployment. We introduce ThreadWeaver, a framework for adaptive parallel reasoning that achieves accuracy on par with popular sequential reasoning models of comparable size while significantly reducing inference latency. ThreadWeaver's performance stems from three key innovations: 1) a two-stage parallel trajectory generator that produces large-scale, high-quality CoT data with parallel annotations for supervised fine-tuning; 2) a trie-based training-inference co-design that enables parallel reasoning on any off-the-shelf autoregressive inference engine without modifying position embeddings or KV caches; and 3) a parallelization-aware reinforcement learning framework that teaches the model to balance accuracy with effective parallelization. Across six challenging mathematical reasoning benchmarks, ThreadWeaver trained atop Qwen3-8B achieves accuracy comparable to cutting-edge sequential reasoning models (71.9% on average and 79.9% on AIME24) while delivering up to 1.53x average speedup in token latency, establishing a new Pareto frontier between accuracy and efficiency.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MixLM: High-Throughput and Effective LLM Ranking via Text-Embedding Mix-Interaction</title>
<link>https://arxiv.org/abs/2512.07846</link>
<guid>https://arxiv.org/abs/2512.07846</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, ranking system, MixLM, throughput optimization, search relevance<br /><br />Summary:<br /><br />1. Large language models (LLMs) are highly effective at capturing semantic nuances, resulting in impressive relevance ranking in recommendation and search systems, but they face challenges due to high computational overhead and latency concerns in industrial applications. <br /><br />2. Traditional cross-encoder ranking systems demand processing long context inputs, which combine user, query, and item information, leading to heavy computational workloads. <br /><br />3. The paper introduces MixLM, an innovative LLM-based ranking framework that greatly improves throughput by reducing input context length, while maintaining the semantic capabilities of cross-encoder rankers.<br /><br />4. MixLM uses a "mix-interaction" approach that represents inputs as a combination of text and embedding tokens. Specifically, all catalog items are pre-encoded into a few embedding tokens stored in a nearline cache, drastically reducing the item input size from thousands of text tokens to just a few embedding tokens during inference. <br /><br />5. The authors detail the deployment of MixLM in a real-world LinkedIn search system, describing training and serving infrastructure optimizations. Compared to strong baselines, MixLM achieves a 10x increase in throughput at the same latency budget and maintains relevance metrics, enabling full-traffic LLM-powered search with a resulting 0.47% increase in Daily Active Users (DAU) in online A/B testing. <div>
arXiv:2512.07846v1 Announce Type: cross 
Abstract: Large language models (LLMs) excel at capturing semantic nuances and therefore show impressive relevance ranking performance in modern recommendation and search systems. However, they suffer from high computational overhead under industrial latency and throughput requirements. In particular, cross-encoder ranking systems often create long context prefill-heavy workloads, as the model has to be presented with the user, query and item information. To this end, we propose MixLM, a novel LLM-based ranking framework, which significantly improves the system throughput via reducing the input context length, while preserving the semantic strength of cross-encoder rankers. In contrast to a standard ranking system where the context is presented to the model as pure text, we propose to use mix-interaction, a mixture of text and embedding tokens to represent the input. Specifically, MixLM encodes all items in the catalog into a few embedding tokens and stores in a nearline cache. The encoded item descriptions are used during online inference, effectively reducing the item length from a few thousand text tokens to a few embedding tokens. We share insights from deploying our MixLM framework to a real-world search application at LinkedIn, including a detailed discussion of our training pipelines, as well as a thorough analysis of our online serving infrastructure optimization. Comparing with strong baselines, MixLM increased throughput by 10.0x under the same latency budget, while maintaining relevance metrics. The efficiency gains delivered by MixLM enabled full-traffic deployment of LLM-powered search, which resulted in a significant 0.47% increase in Daily Active Users (DAU) in online A/B tests.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerating Urban Science Research with AI Urban Scientist</title>
<link>https://arxiv.org/abs/2512.07849</link>
<guid>https://arxiv.org/abs/2512.07849</guid>
<content:encoded><![CDATA[
<div> Keywords: AI Urban Scientist, urban systems, multi-agent framework, data-driven experimentation, urban analytics<br /><br />Summary:<br /><br />1. The article highlights the complexity of cities as adaptive systems and the challenge urban science faces in synthesizing vast, fragmented, and interdisciplinary data into coherent explanations of urban functioning and evolution.<br /><br />2. It introduces the concept of AI scientists—agents capable of autonomous reasoning, hypothesis formation, and data-driven experimentation—as a promising route to accelerate urban scientific research.<br /><br />3. Recognizing that general-purpose AI lacks the necessary domain expertise, the authors present a knowledge-driven AI Urban Scientist designed specifically for urban science. This system is built on hypotheses, peer-review signals, datasets, and analytical patterns derived from thousands of authoritative studies.<br /><br />4. The AI Urban Scientist operates as a coordinated multi-agent framework, supporting end-to-end inquiry by generating structured hypotheses, retrieving and harmonizing diverse datasets, performing automated empirical analyses and simulations, and synthesizing insights aligned with urban scientific reasoning.<br /><br />5. By offering reusable analytical tools and enabling community-driven enhancements, the system reduces barriers to advanced urban analytics, functioning not just as an assistant but as an active collaborator to uncover mechanisms shaping urban systems and inform the development of more resilient and equitable cities. <div>
arXiv:2512.07849v1 Announce Type: cross 
Abstract: Cities are complex, adaptive systems whose underlying principles remain difficult to disentangle despite unprecedented data abundance. Urban science therefore faces a fundamental challenge: converting vast, fragmented and interdisciplinary information into coherent explanations of how cities function and evolve. The emergence of AI scientists, i.e., agents capable of autonomous reasoning, hypothesis formation and data-driven experimentation, offers a new pathway toward accelerating this transformation, yet general-purpose systems fall short of the domain knowledge and methodological depth required for urban science research. Here we introduce a knowledge-driven AI Urban Scientist, built from hypotheses, peer-review signals, datasets and analytical patterns distilled from thousands of high-quality studies, and implemented as a coordinated multi-agent framework for end-to-end inquiry. The system generates structured hypotheses, retrieves and harmonizes heterogeneous datasets, conducts automated empirical analysis and simulation, and synthesizes insights in forms compatible with urban scientific reasoning. By providing reusable analytical tools and supporting community-driven extensions, the AI Urban Scientist lowers barriers to advanced urban analytics and acts not merely as an assistant but as an active collaborator in revealing the mechanisms that shape urban systems and in guiding the design of more resilient and equitable cities.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Unified Models: A Service-Oriented Approach to Low Latency, Context Aware Phonemization for Real Time TTS</title>
<link>https://arxiv.org/abs/2512.08006</link>
<guid>https://arxiv.org/abs/2512.08006</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-speech, phonemization, real-time, G2P, service-oriented architecture<br /><br />Summary:<br /><br />This paper addresses the challenge of balancing phonemization quality with inference speed in grapheme-to-phoneme (G2P) aided text-to-speech (TTS) systems. Lightweight phonemizers are efficient but often fail to handle context-dependent linguistic nuances, while more advanced phonemizers achieve better accuracy at the cost of higher computational demands, limiting real-time application. To overcome this, the authors propose a framework that integrates context-aware phonemization strategies in a lightweight manner. Central to their approach is a service-oriented TTS architecture that modularizes phonemization components as independent services, separating the resource-intensive context-aware processes from the core TTS engine. This decoupling reduces latency and enables the use of sophisticated phonemization models without compromising real-time responsiveness. Experimental evaluation demonstrates that the proposed system enhances pronunciation accuracy and linguistic soundness compared to conventional lightweight phonemizers. Moreover, the architecture supports offline operation and deployment on resource-constrained devices. Thus, the paper presents a practical solution to deploy high-quality, context-aware phonemization in real-time TTS applications, improving accessibility and user experience across varied computational environments. <div>
arXiv:2512.08006v1 Announce Type: cross 
Abstract: Lightweight, real-time text-to-speech systems are crucial for accessibility. However, the most efficient TTS models often rely on lightweight phonemizers that struggle with context-dependent challenges. In contrast, more advanced phonemizers with a deeper linguistic understanding typically incur high computational costs, which prevents real-time performance.
  This paper examines the trade-off between phonemization quality and inference speed in G2P-aided TTS systems, introducing a practical framework to bridge this gap. We propose lightweight strategies for context-aware phonemization and a service-oriented TTS architecture that executes these modules as independent services. This design decouples heavy context-aware components from the core TTS engine, effectively breaking the latency barrier and enabling real-time use of high-quality phonemization models. Experimental results confirm that the proposed system improves pronunciation soundness and linguistic accuracy while maintaining real-time responsiveness, making it well-suited for offline and end-device TTS applications.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Balanced Accuracy: The Right Metric for Evaluating LLM Judges - Explained through Youden's J statistic</title>
<link>https://arxiv.org/abs/2512.08121</link>
<guid>https://arxiv.org/abs/2512.08121</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, evaluation metrics, classifier selection, Youden's J statistic, Balanced Accuracy<br /><br />Summary:<br />1. Evaluating large language models (LLMs) relies on measuring the prevalence of certain behaviors, either desirable (e.g., task pass rates) or undesirable (e.g., policy violations), which depends heavily on the choice of classifier serving as the judge.  
2. Typical metrics used to select classifiers, such as Accuracy, Precision, and F1-score, suffer from sensitivity to class imbalance and rely on arbitrary designation of the positive class, often leading to biased or misleading prevalence estimates.  
3. The authors argue that Youden's J statistic is theoretically the most appropriate metric for choosing classifiers when comparing LLMs because it properly balances true positive and true negative rates, independent of class imbalance.  
4. Balanced Accuracy is mathematically equivalent to a linear transformation of Youden's J statistic, making it a practical and interpretable alternative for classifier selection.  
5. Through theoretical analysis, empirical evaluations, and simulations, the study demonstrates that selecting classifiers based on Balanced Accuracy yields more reliable and robust prevalence estimates, thereby improving the trustworthiness of LLM evaluations. <div>
arXiv:2512.08121v1 Announce Type: cross 
Abstract: Rigorous evaluation of large language models (LLMs) relies on comparing models by the prevalence of desirable or undesirable behaviors, such as task pass rates or policy violations. These prevalence estimates are produced by a classifier, either an LLM-as-a-judge or human annotators, making the choice of classifier central to trustworthy evaluation. Common metrics used for this choice, such as Accuracy, Precision, and F1, are sensitive to class imbalance and to arbitrary choices of positive class, and can favor judges that distort prevalence estimates. We show that Youden's $J$ statistic is theoretically aligned with choosing the best judge to compare models, and that Balanced Accuracy is an equivalent linear transformation of $J$. Through both analytical arguments and empirical examples and simulations, we demonstrate how selecting judges using Balanced Accuracy leads to better, more robust classifier selection.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning Models Ace the CFA Exams</title>
<link>https://arxiv.org/abs/2512.08270</link>
<guid>https://arxiv.org/abs/2512.08270</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, CFA exams, reasoning models, Gemini 3.0 Pro, exam performance<br /><br />Summary:<br /><br />Previous research indicated that large language models (LLMs) performed poorly on the Chartered Financial Analyst (CFA) exams. However, newer reasoning models have demonstrated strong capabilities on graduate-level academic and professional tests across diverse fields. This study evaluates state-of-the-art reasoning models using a comprehensive set of 980 mock CFA exam questions spanning all three exam levels: three Level I, two Level II, and three Level III exams. When applying the same pass/fail criteria from earlier work, the majority of these models successfully passed all three levels. The top-performing models, ranked by overall success, are Gemini 3.0 Pro, Gemini 2.5 Pro, GPT-5, Grok 4, Claude Opus 4.1, and DeepSeek-V3.1. Notably, Gemini 3.0 Pro achieved a record-breaking score of 97.6% on Level I. On Level II, GPT-5 led the field with a score of 94.3%. For Level III, Gemini 2.5 Pro recorded the highest score of 86.4% on multiple-choice questions, while Gemini 3.0 Pro excelled on constructed-response questions with a score of 92.0%. These findings indicate a significant advancement in LLM capabilities on professional finance certification exams. <div>
arXiv:2512.08270v1 Announce Type: cross 
Abstract: Previous research has reported that large language models (LLMs) demonstrate poor performance on the Chartered Financial Analyst (CFA) exams. However, recent reasoning models have achieved strong results on graduate-level academic and professional examinations across various disciplines. In this paper, we evaluate state-of-the-art reasoning models on a set of mock CFA exams consisting of 980 questions across three Level I exams, two Level II exams, and three Level III exams. Using the same pass/fail criteria from prior studies, we find that most models clear all three levels. The models that pass, ordered by overall performance, are Gemini 3.0 Pro, Gemini 2.5 Pro, GPT-5, Grok 4, Claude Opus 4.1, and DeepSeek-V3.1. Specifically, Gemini 3.0 Pro achieves a record score of 97.6% on Level I. Performance is also strong on Level II, led by GPT-5 at 94.3%. On Level III, Gemini 2.5 Pro attains the highest score with 86.4% on multiple-choice questions while Gemini 3.0 Pro achieves 92.0% on constructed-response questions.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The High Cost of Incivility: Quantifying Interaction Inefficiency via Multi-Agent Monte Carlo Simulations</title>
<link>https://arxiv.org/abs/2512.08345</link>
<guid>https://arxiv.org/abs/2512.08345</guid>
<content:encoded><![CDATA[
<div> Keywords: workplace toxicity, Large Language Models, Multi-Agent Systems, Monte Carlo simulation, operational efficiency<br /><br />Summary:<br /><br />This study investigates the impact of workplace toxicity on operational efficiency by using Large Language Model (LLM) based Multi-Agent Systems to simulate adversarial 1-on-1 debates in a controlled environment dubbed a "sociological sandbox." By employing a Monte Carlo method, hundreds of simulated discussions were conducted to compare convergence times—the number of arguments needed to reach conclusions—between baseline control groups and groups with agents programmed with toxic behaviors via system prompts. The findings reveal that conversations involving toxic agents took approximately 25% longer to conclude, demonstrating a significant increase in latency due to toxicity. This increased duration is proposed as a quantifiable proxy for financial damage caused by toxic social dynamics in corporate and academic environments. Additionally, the study highlights the ethical and practical advantages of agent-based modeling, as it provides a reproducible and ethically sound alternative to traditional human-subject research for studying social friction and conflict mechanics. This approach circumvents the challenges of experimentally reproducing interpersonal conflict among humans while offering insight into the mechanics and costs of workplace toxicity. <div>
arXiv:2512.08345v1 Announce Type: cross 
Abstract: Workplace toxicity is widely recognized as detrimental to organizational culture, yet quantifying its direct impact on operational efficiency remains methodologically challenging due to the ethical and practical difficulties of reproducing conflict in human subjects. This study leverages Large Language Model (LLM) based Multi-Agent Systems to simulate 1-on-1 adversarial debates, creating a controlled "sociological sandbox". We employ a Monte Carlo method to simulate hundrets of discussions, measuring the convergence time (defined as the number of arguments required to reach a conclusion) between a baseline control group and treatment groups involving agents with "toxic" system prompts. Our results demonstrate a statistically significant increase of approximately 25\% in the duration of conversations involving toxic participants. We propose that this "latency of toxicity" serves as a proxy for financial damage in corporate and academic settings. Furthermore, we demonstrate that agent-based modeling provides a reproducible, ethical alternative to human-subject research for measuring the mechanics of social friction.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ontology-Based Knowledge Graph Framework for Industrial Standard Documents via Hierarchical and Propositional Structuring</title>
<link>https://arxiv.org/abs/2512.08398</link>
<guid>https://arxiv.org/abs/2512.08398</guid>
<content:encoded><![CDATA[
<div> Keywords: ontology, knowledge graph, industrial standards, LLM-based triple extraction, rule-based QA<br /><br />Summary:<br /><br />1. This paper addresses the challenge of constructing ontology-based knowledge graphs (KGs) from industrial standards, which include complex structured information such as tables, scopes, constraints, exceptions, and numerical rules.  
2. The authors propose a novel method that organizes industrial documents into hierarchical semantic structures and decomposes sentences and tables into atomic propositions derived from conditional and numerical rules.  
3. The method leverages large language model (LLM)-based triple extraction to integrate these atomic propositions into an ontology-enriched knowledge graph, effectively capturing both hierarchy and logic within the documents.  
4. To validate their approach, the researchers created datasets focused on rules, tables, multi-hop question answering (QA), and toxic clause detection specifically from industrial standards.  
5. Experimental results showed that their ontology-aware KG-RAG framework significantly outperforms existing KG-RAG methods across all QA types, demonstrating that scalable and reliable knowledge representation for complex industrial documentation is achievable, promoting advancements in domain-specific retrieval-augmented generation (RAG) and intelligent document management systems. <div>
arXiv:2512.08398v1 Announce Type: cross 
Abstract: Ontology-based knowledge graph (KG) construction is a core technology that enables multidimensional understanding and advanced reasoning over domain knowledge. Industrial standards, in particular, contain extensive technical information and complex rules presented in highly structured formats that combine tables, scopes of application, constraints, exceptions, and numerical calculations, making KG construction especially challenging. In this study, we propose a method that organizes such documents into a hierarchical semantic structure, decomposes sentences and tables into atomic propositions derived from conditional and numerical rules, and integrates them into an ontology-knowledge graph through LLM-based triple extraction. Our approach captures both the hierarchical and logical structures of documents, effectively representing domain-specific semantics that conventional methods fail to reflect. To verify its effectiveness, we constructed rule, table, and multi-hop QA datasets, as well as a toxic clause detection dataset, from industrial standards, and implemented an ontology-aware KG-RAG framework for comparative evaluation. Experimental results show that our method achieves significant performance improvements across all QA types compared to existing KG-RAG approaches. This study demonstrates that reliable and scalable knowledge representation is feasible even for industrial documents with intertwined conditions, constraints, and scopes, contributing to future domain-specific RAG development and intelligent document management.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Real Weights: Hypercomplex Representations for Stable Quantization</title>
<link>https://arxiv.org/abs/2512.08524</link>
<guid>https://arxiv.org/abs/2512.08524</guid>
<content:encoded><![CDATA[
<div> Multimodal Language Models, Parameterized Hypercomplex Multiplication, Model Compression, Knowledge Distillation, Efficient Inference

<br /><br />Summary:  
The paper addresses the challenge of high computational demand in multimodal language models (MLLMs) required to align complex visual and linguistic data. It proposes a progressive reparameterization strategy that incrementally replaces dense feed-forward network blocks with compact Parameterized Hypercomplex Multiplication (PHM) layers. This transition is guided by a residual interpolation schedule combined with lightweight reconstruction and knowledge distillation losses, ensuring that the newly introduced PHM layers replicate the functional behavior of the original dense layers during training. This approach yields significant reductions in both parameter count and floating-point operations (FLOPs), leading to faster inference times without compromising the quality of multimodal alignment. Evaluations across various vision-language models demonstrate that the method retains performance comparable to original models while substantially decreasing model size and latency. The technique also serves as a complementary solution to existing low-bit quantization methods, providing an architecture-compatible route toward more resource-efficient multimodal reasoning solutions. <div>
arXiv:2512.08524v1 Announce Type: cross 
Abstract: Multimodal language models (MLLMs) require large parameter capacity to align high-dimensional visual features with linguistic representations, making them computationally heavy and difficult to deploy efficiently. We introduce a progressive reparameterization strategy that compresses these models by gradually replacing dense feed-forward network blocks with compact Parameterized Hypercomplex Multiplication (PHM) layers. A residual interpolation schedule, together with lightweight reconstruction and knowledge distillation losses, ensures that the PHM modules inherit the functional behavior of their dense counterparts during training. This transition yields substantial parameter and FLOP reductions while preserving strong multimodal alignment, enabling faster inference without degrading output quality. We evaluate the approach on multiple vision-language models (VLMs). Our method maintains performance comparable to the base models while delivering significant reductions in model size and inference latency. Progressive PHM substitution thus offers an architecture-compatible path toward more efficient multimodal reasoning and complements existing low-bit quantization techniques.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pose-Based Sign Language Spotting via an End-to-End Encoder Architecture</title>
<link>https://arxiv.org/abs/2512.08738</link>
<guid>https://arxiv.org/abs/2512.08738</guid>
<content:encoded><![CDATA[
<div> Keywords: Sign Language Spotting, Automatic Sign Language Recognition, pose keypoints, binary classification, sign-to-sign retrieval  

<br /><br />Summary:  
This paper introduces a novel task called Sign Language Spotting, which focuses on detecting the presence or absence of a specific query sign within continuous sign language sequences, addressing the largely unexplored problem of sign-to-sign retrieval. Unlike traditional methods that depend on intermediate gloss recognition or text-based matching, the authors propose an end-to-end model that operates directly on pose keypoints extracted from sign videos. The model uses an encoder-only backbone paired with a binary classification head to determine whether the query sign exists within the target sequence. Utilizing pose representations instead of raw RGB frames reduces computational overhead and lessens the impact of visual noise, enhancing the model's efficiency. The approach is evaluated on the Word Presence Prediction dataset from the WSLP 2025 shared task, achieving an accuracy of 61.88% and an F1-score of 60.00%. These results validate the effectiveness of the proposed pose-based framework as a solid foundation for future work in automatic sign language retrieval and verification. The authors have also made their code publicly available to encourage further research in this area. <div>
arXiv:2512.08738v1 Announce Type: cross 
Abstract: Automatic Sign Language Recognition (ASLR) has emerged as a vital field for bridging the gap between deaf and hearing communities. However, the problem of sign-to-sign retrieval or detecting a specific sign within a sequence of continuous signs remains largely unexplored. We define this novel task as Sign Language Spotting. In this paper, we present a first step toward sign language retrieval by addressing the challenge of detecting the presence or absence of a query sign video within a sentence-level gloss or sign video. Unlike conventional approaches that rely on intermediate gloss recognition or text-based matching, we propose an end-to-end model that directly operates on pose keypoints extracted from sign videos. Our architecture employs an encoder-only backbone with a binary classification head to determine whether the query sign appears within the target sequence. By focusing on pose representations instead of raw RGB frames, our method significantly reduces computational cost and mitigates visual noise. We evaluate our approach on the Word Presence Prediction dataset from the WSLP 2025 shared task, achieving 61.88\% accuracy and 60.00\% F1-score. These results demonstrate the effectiveness of our pose-based framework for Sign Language Spotting, establishing a strong foundation for future research in automatic sign language retrieval and verification. Code is available at https://github.com/EbimoJohnny/Pose-Based-Sign-Language-Spotting
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting the Scaling Properties of Downstream Metrics in Large Language Model Training</title>
<link>https://arxiv.org/abs/2512.08894</link>
<guid>https://arxiv.org/abs/2512.08894</guid>
<content:encoded><![CDATA[
<div> scaling laws, large language models, downstream performance, power law, training budget<br /><br />Summary:  
This paper revisits the problem of predicting downstream task performance of large language models (LLMs) based on their training budget, challenging the traditional reliance on proxy metrics like pretraining loss. It proposes a direct modeling framework that correlates benchmark performance with the training budget through a simple power law when maintaining a fixed token-to-parameter ratio. The study demonstrates that this direct approach outperforms the previously used two-stage procedure, which is susceptible to compounding errors when extrapolating model performance. Additionally, the authors develop functional forms capable of predicting accuracy across varying token-to-parameter ratios while factoring in inference compute costs, including repeated sampling. Their empirical validation spans models with up to 17 billion parameters, trained on datasets consisting of up to 350 billion tokens drawn from two different mixtures. The paper emphasizes reproducibility by releasing the entire set of pretraining losses alongside downstream evaluation results. Overall, the research provides a more reliable and generalizable method to predict LLM performance across diverse settings, which could guide efficient allocation of compute resources during model development. <div>
arXiv:2512.08894v1 Announce Type: cross 
Abstract: While scaling laws for Large Language Models (LLMs) traditionally focus on proxy metrics like pretraining loss, predicting downstream task performance has been considered unreliable. This paper challenges that view by proposing a direct framework to model the scaling of benchmark performance from the training budget. We find that for a fixed token-to-parameter ratio, a simple power law can accurately describe the scaling behavior of log accuracy on multiple popular downstream tasks. Our results show that the direct approach extrapolates better than the previously proposed two-stage procedure, which is prone to compounding errors. Furthermore, we introduce functional forms that predict accuracy across token-to-parameter ratios and account for inference compute under repeated sampling. We validate our findings on models with up to 17B parameters trained on up to 350B tokens across two dataset mixtures. To support reproducibility and encourage future research, we release the complete set of pretraining losses and downstream evaluation results.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting value-expressive text posts in Russian social media</title>
<link>https://arxiv.org/abs/2312.08968</link>
<guid>https://arxiv.org/abs/2312.08968</guid>
<content:encoded><![CDATA[
<div> Keywords: personal values, social media, Russian language, annotation, transformer models  

<br /><br />Summary:  
This study addresses the challenge of detecting personal value-expressive posts within Russian social media, specifically on VKontakte. Personal values are defined as concepts or beliefs concerning desirable end-states that go beyond specific situations, and their study through social media can shed light on societal value evolution, particularly in populations difficult to survey through traditional methods. However, user-generated content in social media is often influenced by culturally stereotyped speech rather than genuine personal value expression, complicating detection efforts. The authors developed a training dataset of 5,035 VKontakte posts annotated by three experts, 304 crowd-workers, and ChatGPT. Agreement between experts and crowd-workers was moderate, while ChatGPT showed higher consistency but had limitations with spam detection. To improve annotation quality, an ensemble approach combining human and AI assistance with active learning was employed. Multiple classification models were then trained using embeddings from various pre-trained transformer models. The finest results were obtained using embeddings from a fine-tuned rubert-tiny2 model, which achieved an F1 score of 0.77 and an F1-macro of 0.83, indicating strong performance in identifying value-expressive content. This model represents a significant advancement in studying personal values within Russian social media users and supports future research into values dynamics across user groups. <div>
arXiv:2312.08968v3 Announce Type: replace 
Abstract: Basic values are concepts or beliefs which pertain to desirable end-states and transcend specific situations. Studying personal values in social media can illuminate how and why societal values evolve especially when the stimuli-based methods, such as surveys, are inefficient, for instance, in hard-to-reach populations. On the other hand, user-generated content is driven by the massive use of stereotyped, culturally defined speech constructions rather than authentic expressions of personal values. We aimed to find a model that can accurately detect value-expressive posts in Russian social media VKontakte. A training dataset of 5,035 posts was annotated by three experts, 304 crowd-workers and ChatGPT. Crowd-workers and experts showed only moderate agreement in categorizing posts. ChatGPT was more consistent but struggled with spam detection. We applied an ensemble of human- and AI-assisted annotation involving active learning approach, subsequently trained several classification models using embeddings from various pre-trained transformer-based language models. The best performance was achieved with embeddings from a fine-tuned rubert-tiny2 model, yielding high value detection quality (F1 = 0.77, F1-macro = 0.83). This model provides a crucial step to a study of values within and between Russian social media users.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>B-cos LM: Efficiently Transforming Pre-trained Language Models for Improved Explainability</title>
<link>https://arxiv.org/abs/2502.12992</link>
<guid>https://arxiv.org/abs/2502.12992</guid>
<content:encoded><![CDATA[
<div> Keywords: B-cos networks, language models, explainability, natural language processing, post-hoc explanations<br /><br />Summary:<br /><br />This paper addresses limitations in post-hoc explanation methods for black-box models, which often lack faithfulness and human interpretability due to neural architecture constraints. The authors focus on B-cos networks, which improve model explainability by removing bias terms and encouraging alignment between inputs and weights. Although previously successful in computer vision, B-cos networks had limited application in natural language processing (NLP). The work introduces B-cos Language Models (B-cos LMs), a novel approach that converts pre-trained language models into B-cos LMs through a combined B-cos conversion and task-specific fine-tuning process. This method enhances efficiency over prior techniques. Both automatic and human evaluations reveal that B-cos LMs generate explanations that are more faithful and easily interpretable than those produced by traditional post-hoc methods, while maintaining competitive task performance. The study also provides an in-depth analysis comparing the learning dynamics and explanation patterns of B-cos LMs versus conventionally fine-tuned models. Additionally, the paper explores the transformation of decoder-only language models into B-cos LMs, expanding their applicability to generation tasks. The authors have made their implementation publicly accessible via GitHub to encourage further research and adoption in the NLP community. <div>
arXiv:2502.12992v4 Announce Type: replace 
Abstract: Post-hoc explanation methods for black-box models often struggle with faithfulness and human interpretability due to the lack of explainability in current neural architectures. Meanwhile, B-cos networks have been introduced to improve model explainability by proposing an architecture that removes bias terms and promotes input-weight alignment. Although B-cos networks have shown success in building explainable systems, their application has so far been limited to computer vision models and their associated training pipelines. In this work, we introduce B-cos LMs, i.e., B-cos Language Models (LMs) empowered for natural language processing (NLP) tasks. Our approach directly transforms pre-trained language models into B-cos LMs by combining B-cos conversion and task fine-tuning, improving efficiency compared to previous methods. Automatic and human evaluation results demonstrate that B-cos LMs produce more faithful and human interpretable explanations than post-hoc methods, while maintaining task performance comparable to conventional fine-tuning. Our in-depth analysis explores how B-cos LMs differ from conventionally fine-tuned models in their learning processes and explanation patterns. Finally, we present a first exploration of transforming decoder-only models to B-cos LMs for generation tasks. Our code is available at https://github.com/Ewanwong/bcos_lm.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Survey and Experiments on Mental Disorder Detection via Social Media: From Large Language Models and RAG to Agents</title>
<link>https://arxiv.org/abs/2504.02800</link>
<guid>https://arxiv.org/abs/2504.02800</guid>
<content:encoded><![CDATA[
<div> mental disorders, social media, large language models, retrieval-augmented generation, agentic systems<br /><br />Summary:  
This article addresses the critical global health issue of mental disorders by exploring the role of social media as a platform for real-time digital phenotyping and intervention. It highlights Large Language Models (LLMs) as powerful tools that offer improved semantic understanding and reasoning compared to traditional deep learning methods, while acknowledging their current limitations such as hallucinations and lack of persistent memory in clinical settings. The paper systematically surveys advanced enhancement techniques, specifically Retrieval-Augmented Generation (RAG) and agentic systems, that can help overcome these shortcomings by reducing hallucinations and enabling autonomous reasoning and multi-step interventions. The review organizes existing research across various technical paradigms and clinical targets, broadening the scope beyond common internalizing disorders to include psychotic disorders and externalizing behaviors. Furthermore, the study comprehensively evaluates the performance of LLMs with and without RAG across multiple tasks, establishing a unified benchmark for the field. Ultimately, this work aims to facilitate the development of trustworthy, autonomous AI systems capable of providing precise, explainable, and effective mental health support through social media analysis. <div>
arXiv:2504.02800v3 Announce Type: replace 
Abstract: Mental disorders represent a critical global health challenge, and social media is increasingly viewed as a vital resource for real-time digital phenotyping and intervention. Large Language Models (LLMs) offer stronger semantic understanding and reasoning than traditional deep learning, but their use in high-stakes clinical settings is limited by hallucinations and the lack of persistent memory. However, existing literature has not sufficiently investigated how advanced enhancement techniques, specifically Retrieval-Augmented Generation (RAG) and Agentic systems, can address these reliability and reasoning limitations. Here, we systematically survey the evolving landscape of LLM-based methods for social media mental disorder analysis, spanning standard pretrained language models, RAG to mitigate hallucinations and contextual gaps, and agentic systems for autonomous reasoning and multi-step intervention. We organize existing work by technical paradigm and clinical target, extending beyond common internalizing disorders to include psychotic disorders and externalizing behaviors. Additionally, the paper comprehensively evaluates the performance of LLMs, including the impact of RAG, across various tasks. This work establishes a unified benchmark for the field, paving the way for the development of trustworthy, autonomous AI systems that can deliver precise and explainable mental health support.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EEG-to-Text Translation: A Model for Deciphering Human Brain Activity</title>
<link>https://arxiv.org/abs/2505.13936</link>
<guid>https://arxiv.org/abs/2505.13936</guid>
<content:encoded><![CDATA[
<div> EEG decoding, R1 Translator, bidirectional LSTM, transformer decoder, ROUGE metrics<br /><br />Summary:<br /><br />1. This paper addresses the challenge of decoding EEG signals into text, aiming to bridge the gap between human brain activity and language processing.<br />2. The proposed model, R1 Translator, combines a bidirectional LSTM encoder with a pretrained transformer-based decoder to improve EEG-to-text translation.<br />3. EEG features are first processed through the LSTM to capture sequential dependencies, then fed into the transformer decoder to generate high-quality text output.<br />4. The R1 Translator significantly outperforms previous models such as T5 and Brain Translator across several metrics.<br />5. Performance highlights include a ROUGE-1 precision score of 38.00%, which is 9% higher than T5 and 3% higher than Brain Translator.<br />6. R1 also leads in ROUGE-L F1 score with 32.51%, exceeding T5 by 3% and Brain by 2%.<br />7. The model achieves a character error rate (CER) of 0.5795, 2% lower than T5 and 4% lower than Brain.<br />8. Word error rate (WER) is improved to 0.7280, which is 4.3% and 3.6% better than T5 and Brain, respectively.<br />9. The combination of bidirectional LSTM with a transformer decoder enables effective sequential feature extraction and text generation.<br />10. The authors provide code for their R1 Translator model at the linked GitHub repository for reproducibility and further research. <div>
arXiv:2505.13936v2 Announce Type: replace 
Abstract: With the rapid advancement of large language models like Gemini, GPT, and others, bridging the gap between the human brain and language processing has become an important area of focus. To address this challenge, researchers have developed various models to decode EEG signals into text. However, these models still face significant performance limitations. To overcome these shortcomings, we propose a new model, R1 Translator, which aims to improve the performance of EEG-to-text decoding. The R1 Translator model combines a bidirectional LSTM encoder with a pretrained transformer-based decoder, utilizing EEG features to produce high-quality text outputs. The model processes EEG embeddings through the LSTM to capture sequential dependencies, which are then fed into the transformer decoder for effective text generation. The R1 Translator excels in ROUGE metrics, outperforming both T5 (previous research) and Brain Translator. Specifically, R1 achieves a ROUGE-1 score of 38.00% (P), which is up to 9% higher than T5 (34.89%) and 3% better than Brain (35.69%). It also leads in ROUGE-L, with a F1 score of 32.51%, outperforming T5 by 3% (29.67%) and Brain by 2% (30.38%). In terms of CER, R1 achieves a CER of 0.5795, which is 2% lower than T5 (0.5917) and 4% lower than Brain (0.6001). Additionally, R1 performs better in WER with a score of 0.7280, outperforming T5 by 4.3% (0.7610) and Brain by 3.6% (0.7553). Code is available at https://github.com/Mmurrad/EEG-To-text.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Languages and Modalities</title>
<link>https://arxiv.org/abs/2505.23856</link>
<guid>https://arxiv.org/abs/2505.23856</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, harmful prompt detection, multilingual, multimodal, Omniguard<br /><br />Summary:<br /><br />1. The paper addresses the challenge of detecting harmful queries posed to large language models (LLMs), a crucial step to mitigate risks of misuse. 2. Current harmful prompt detection methods are vulnerable, especially when attackers exploit generalization gaps across languages or input modalities like images and audio. 3. To overcome these limitations, the authors propose Omniguard, a detection approach that leverages internal representations of LLMs and multimodal LLMs (MLLMs) aligned across languages and modalities. 4. Omniguard uses these aligned embeddings to build classifiers that are language-agnostic and modality-agnostic, enhancing detection robustness. 5. Experimental results show Omniguard improves classification accuracy by 11.57% over prior best methods in multilingual text, by 20.44% for image-based prompts, and establishes new state-of-the-art accuracy for audio-based prompts. 6. Additionally, Omniguard repurposes embeddings already computed during generation, resulting in significant efficiency gains—about 120 times faster than the next fastest baseline. 7. The authors have released code and data publicly, facilitating further research and development in safe and robust harmful prompt detection across diverse inputs. <div>
arXiv:2505.23856v2 Announce Type: replace 
Abstract: The emerging capabilities of large language models (LLMs) have sparked concerns about their immediate potential for harmful misuse. The core approach to mitigate these concerns is the detection of harmful queries to the model. Current detection approaches are fallible, and are particularly susceptible to attacks that exploit mismatched generalization of model capabilities (e.g., prompts in low-resource languages or prompts provided in non-text modalities such as image and audio). To tackle this challenge, we propose Omniguard, an approach for detecting harmful prompts across languages and modalities. Our approach (i) identifies internal representations of an LLM/MLLM that are aligned across languages or modalities and then (ii) uses them to build a language-agnostic or modality-agnostic classifier for detecting harmful prompts. Omniguard improves harmful prompt classification accuracy by 11.57\% over the strongest baseline in a multilingual setting, by 20.44\% for image-based prompts, and sets a new SOTA for audio-based prompts. By repurposing embeddings computed during generation, Omniguard is also very efficient ($\approx\!120 \times$ faster than the next fastest baseline). Code and data are available at: https://github.com/vsahil/OmniGuard.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bench4KE: Benchmarking Automated Competency Question Generation</title>
<link>https://arxiv.org/abs/2505.24554</link>
<guid>https://arxiv.org/abs/2505.24554</guid>
<content:encoded><![CDATA[
<div> Knowledge Engineering, Benchmarking, Competency Questions, Large Language Models, Ontology Automation<br /><br />Summary:<br /><br />1. The paper introduces Bench4KE, an extensible, API-based benchmarking system designed to enhance methodological rigor in Knowledge Engineering (KE) automation research.<br />2. The primary focus of the current release is on evaluating automatic Competency Question (CQ) generation tools, which help ontology engineers define functional requirements by producing natural language questions.<br />3. Bench4KE offers a curated gold standard comprising CQ datasets derived from 17 real-world ontology engineering projects, facilitating standardized evaluation.<br />4. The system employs a suite of similarity metrics to quantitatively assess and compare the quality of CQs generated by different tools.<br />5. The authors present a comparative analysis of six recent CQ generation systems based on Large Language Models (LLMs), establishing a performance baseline for future developments.<br />6. Bench4KE is designed with extensibility in mind, supporting additional KE automation tasks such as SPARQL query generation, ontology testing, and drafting.<br />7. The codebase and datasets are made publicly available under the Apache 2.0 license, encouraging replication, transparency, and further research in the KE automation domain. <div>
arXiv:2505.24554v3 Announce Type: replace 
Abstract: The availability of Large Language Models (LLMs) presents a unique opportunity to reinvigorate research on Knowledge Engineering (KE) automation. This trend is already evident in recent efforts developing LLM-based methods and tools for the automatic generation of Competency Questions (CQs), natural language questions used by ontology engineers to define the functional requirements of an ontology. However, the evaluation of these tools lacks standardization. This undermines the methodological rigor and hinders the replication and comparison of results. To address this gap, we introduce Bench4KE, an extensible API-based benchmarking system for KE automation. The presented release focuses on evaluating tools that generate CQs automatically. Bench4KE provides a curated gold standard consisting of CQ datasets from 17 real-world ontology engineering projects and uses a suite of similarity metrics to assess the quality of the CQs generated. We present a comparative analysis of 6 recent CQ generation systems, which are based on LLMs, establishing a baseline for future research. Bench4KE is also designed to accommodate additional KE automation tasks, such as SPARQL query generation, ontology testing and drafting. Code and datasets are publicly available under the Apache 2.0 license.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shrinking the Generation-Verification Gap with Weak Verifiers</title>
<link>https://arxiv.org/abs/2506.18203</link>
<guid>https://arxiv.org/abs/2506.18203</guid>
<content:encoded><![CDATA[
<div> Keywords: verifiers, language models, Weaver framework, weak supervision, ensemble learning  

<br /><br />Summary:  
This paper introduces Weaver, a novel framework designed to create a strong verifier for language model outputs by combining multiple weak and imperfect verifiers. Traditional verifiers like humans are accurate but unscalable, while automated tools often lack utility or precision. Weaver improves upon this by forming weighted ensembles of verifiers, which outperform unweighted combinations due to varying verifier accuracies, but typically need labeled data for training. To reduce reliance on labeled datasets, Weaver applies weak supervision techniques to estimate each verifier’s reliability and aggregates their outputs into a unified, more accurate quality score. The framework also tackles practical challenges such as inconsistent output formats among verifiers and low-quality verifier filtering by using dataset statistics for normalization and selection. Evaluations demonstrate that Weaver significantly enhances Pass@1 performance in reasoning and math tasks by effectively selecting the best candidate from multiple generated responses. Using Llama 3.3 70B Instruct as a generator with an ensemble of 70B or smaller judge and reward models as verifiers, Weaver achieves an average accuracy of 87.7%, comparable to the improvement seen between GPT-4o and o3-mini models, which required extensive finetuning. To lower computational costs, the authors additionally train a 400M parameter cross-encoder based on Weaver’s combined scoring outputs. <div>
arXiv:2506.18203v2 Announce Type: replace 
Abstract: Verifiers can improve language model capabilities by scoring and ranking responses from generated candidates. Currently, high-quality verifiers are either unscalable (e.g., humans) or limited in utility (e.g., tools like Lean). While LM judges and reward models have become broadly useful as general-purpose verifiers, a significant performance gap remains between them and oracle verifiers (verifiers with perfect accuracy). To help close this gap, we introduce Weaver, a framework for designing a strong verifier by combining multiple weak, imperfect verifiers. We find weighted ensembles of verifiers, which typically require learning from labeled data, significantly outperform unweighted combinations due to differences in verifier accuracies. To reduce dependency on labeled data, Weaver leverages weak supervision to estimate each verifier's accuracy and combines outputs into a unified score that better reflects true response quality. However, directly applying weak supervision algorithms poses challenges, including inconsistent verifier output formats and handling low-quality verifiers. Weaver addresses these using dataset statistics to normalize outputs and filter specific verifiers. We study Weaver's effectiveness in test-time repeated sampling, where a model generates multiple candidate responses and selects one. Our evaluations show Weaver significantly improves over Pass@1-performance when selecting the first candidate-across reasoning and math tasks, achieving o3-mini-level accuracy with Llama 3.3 70B Instruct as generator, and an ensemble of 70B or smaller judge and reward models as verifiers (87.7% average). This gain mirrors the jump between GPT-4o and o3-mini (69.0% vs. 86.7%), which required extensive finetuning and post-training. To reduce computational costs of verifier ensembles, we train a 400M cross-encoder using Weaver's combined output scores.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Compactor: Calibrated Query-Agnostic KV Cache Compression with Approximate Leverage Scores</title>
<link>https://arxiv.org/abs/2507.08143</link>
<guid>https://arxiv.org/abs/2507.08143</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, KV compression, approximate leverage scores, context-calibrated compression, memory efficiency<br /><br />Summary:<br /><br />1. The paper introduces Compactor, a novel training-free, query-agnostic key-value (KV) compression method that leverages approximate leverage scores to assess token importance in large language models. <br /><br />2. Compactor outperforms competing KV compression methods by retaining 20% fewer tokens while maintaining equivalent performance across various synthetic and real-world context tasks, demonstrating enhanced task robustness.<br /><br />3. A context-calibrated compression procedure is proposed to infer the maximum compression level for a given context without significant performance degradation, enabling dynamic adjustment for different input scenarios.<br /><br />4. Using this calibrated approach, Compactor achieves full KV performance on the Longbench benchmark while reducing KV memory consumption by an average of 68%, addressing the critical memory bottleneck in LLM deployment.<br /><br />5. To validate its generalizability, Compactor is evaluated on 27 synthetic and real-world tasks from RULER and Longbench, employing models from the Qwen 2.5 and Llama 3.1 families. Additionally, the authors release compactor-vllm, an inference engine with optimized Triton kernels that efficiently handle sparse, non-contiguous memory accesses inherent in compressed KV caches, reinforcing its practical applicability in real-world LLM systems. <div>
arXiv:2507.08143v2 Announce Type: replace 
Abstract: Modern Large Language Models (LLMs) are increasingly trained to support very large context windows. We present Compactor, a training-free, query-agnostic KV compression strategy that uses approximate leverage scores to determine token importance. We show that Compactor can achieve the same performance as competing methods while retaining 20% fewer tokens in both synthetic and real-world context tasks, while being more task-robust. We further introduce a procedure for context-calibrated compression: inferring the maximum compression a given context supports before significant performance loss. Using context-calibrated compression, we show that Compactor achieves full KV performance on Longbench while reducing the KV memory burden by 68%, on average. To demonstrate the efficacy and generalizability of our approach, we apply Compactor to 27 synthetic and real-world tasks from RULER and Longbench, with models from both the Qwen 2.5 and Llama 3.1 families. Finally, we release compactor-vllm, an inference engine and suite of optimized Triton kernels designed to efficiently support the sparse, non-contiguous memory access patterns inherent to compressed KV caches. This work demonstrates that Compactor offers a practical, high-performance solution for alleviating the memory bottleneck in modern LLM deployment.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLARIFID: Improving Radiology Report Generation by Reinforcing Clinically Accurate Impressions and Enforcing Detailed Findings</title>
<link>https://arxiv.org/abs/2507.17234</link>
<guid>https://arxiv.org/abs/2507.17234</guid>
<content:encoded><![CDATA[
<div> Keywords: radiology reports, diagnostic correctness, section-aware pretraining, Proximal Policy Optimization, multi-view encoder<br /><br />Summary: This paper introduces CLARIFID, a novel framework for automatic generation of radiology reports aimed at improving diagnostic correctness. Unlike prior methods that mainly focus on generating fluent text, CLARIFID mirrors the expert workflow by learning the logical flow from the Findings section to the Impression section through section-aware pretraining. The model is fine-tuned using Proximal Policy Optimization, with the CheXbert F1 score of the Impression as the reward to directly optimize clinical accuracy. To maintain coherent reasoning, CLARIFID employs controlled decoding that first completes the Findings section before synthesizing the Impression. Additionally, it integrates multiple chest X-ray views via a vision-transformer-based multi-view encoder to enhance diagnostic comprehensiveness. During inference, a next-token forcing strategy with report-level re-ranking ensures comprehensive and logically consistent report generation. Experiments on the MIMIC-CXR dataset demonstrate that CLARIFID significantly outperforms existing baselines on clinical efficacy metrics, producing reports with superior clinical reliability and diagnostic value. This approach addresses the key limitations of previous methods by focusing on factual correctness and multi-view image fusion, offering a promising solution to reduce radiologists’ workload through more accurate automated reporting. <div>
arXiv:2507.17234v3 Announce Type: replace 
Abstract: Automatic generation of radiology reports has the potential to alleviate radiologists' significant workload, yet current methods struggle to deliver clinically reliable conclusions. In particular, most prior approaches focus on producing fluent text without effectively ensuring the factual correctness of the reports and often rely on single-view images, limiting diagnostic comprehensiveness. We propose CLARIFID, a novel framework that directly optimizes diagnostic correctness by mirroring the two-step workflow of experts. Specifically, CLARIFID (1) learns the logical flow from Findings to Impression through section-aware pretraining, (2) is fine-tuned with Proximal Policy Optimization in which the CheXbert F1 score of the Impression section serves as the reward, (3) employs controlled decoding that completes "Findings" before synthesizing the "Impression", and (4) fuses multiple chest X-ray views via a vision-transformer-based multi-view encoder. During inference, we apply a next-token forcing strategy followed by report-level re-ranking, ensuring that the model first produces a comprehensive "Findings" section before synthesizing the "Impression" and thereby preserving coherent clinical reasoning. Experimental results on the MIMIC-CXR dataset demonstrate that our method achieves superior clinical efficacy and outperforms existing baselines on clinical efficacy scores.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions</title>
<link>https://arxiv.org/abs/2508.18321</link>
<guid>https://arxiv.org/abs/2508.18321</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, multi-agent systems, social dynamics, rapport, Group Relative Policy Optimisation (GRPO)  

<br /><br />Summary:  
This paper addresses the integration of large language models (LLMs) into multi-agent systems (MAS) where agents interact socially to influence decisions. Unlike previous studies focusing mainly on conformity bias, the work broadens the scope to explore how LLMs establish rapport, identify and incorporate high-quality peer information, and resist misleading inputs—skills critical for collective intelligence in complex social contexts. The authors introduce KAIROS, a novel benchmark that simulates quiz-style collaboration among peer agents with controllable rapport levels and behaviors, both from past interactions and in real-time. This setup allows for systematic study of the interplay between rapport, peer behavior, and the model’s self-confidence in shaping decision-making. Using KAIROS, the paper evaluates different training methods including prompting, supervised fine-tuning, and reinforcement learning through Group Relative Policy Optimisation (GRPO). Key findings reveal that model scale plays a significant role in moderating social influence susceptibility: larger models are more robust and can benefit from prompting-based strategies, while smaller models remain vulnerable. Moreover, only carefully designed GRPO training reliably improves robustness and boosts performance in smaller LLMs. This highlights the importance of training configurations and model size in achieving resilient multi-agent collaboration. <div>
arXiv:2508.18321v3 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly integrated into multi-agent systems (MAS), where peer interactions shape individual decisions. While prior work has mainly examined conformity bias, we broaden the view to include how LLMs build rapport from prior interactions, discern and integrate high-quality peer information, and resist misleading inputs-abilities essential for achieving collective intelligence under complex social dynamics. We introduce KAIROS, a benchmark that simulates quiz-style collaboration with peer agents whose rapport levels and behaviours can be precisely controlled in both historical interactions and the current round. This unified setup enables systematic analysis of how rapport, peer actions, and the model's self-confidence jointly influence decision-making. Using KAIROS, we evaluate prompting, supervised fine-tuning, and reinforcement learning via Group Relative Policy Optimisation (GRPO). Results show that model scale is a primary factor moderating susceptibility to social influence: larger models are more resilient and benefit from prompting-based mitigation, whereas smaller models remain vulnerable. Only carefully configured GRPO training yields consistent robustness and performance gains for small models.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SENSE models: an open source solution for multilingual and multimodal semantic-based tasks</title>
<link>https://arxiv.org/abs/2509.12093</link>
<guid>https://arxiv.org/abs/2509.12093</guid>
<content:encoded><![CDATA[
<div> Keywords: SENSE, SAMU-XLSR, speech encoder, multilingual semantic tasks, SpeechBrain<br /><br />Summary: This paper presents SENSE (Shared Embedding for N-lingual Speech and tExt), an open-source framework designed to align speech and text representations across multiple languages. Inspired by the SAMU-XLSR framework and similar in concept to Meta AI’s SONAR models, SENSE employs a teacher-student architecture where a self-supervised speech encoder is aligned with language-agnostic continuous text representations at the utterance level. Improvements over the original SAMU-XLSR method include the use of a stronger teacher text model and a superior initial speech encoder, leading to enhanced performance. The authors have integrated SENSE’s training and deployment code into the SpeechBrain toolkit, facilitating accessibility and reproducibility. The first SENSE model trained by the authors has been publicly released to promote further research and application development. Experimental evaluations demonstrate that the SENSE model achieves highly competitive results on various multilingual and multimodal semantic tasks, validating its effectiveness. Additionally, the study offers valuable insights into how semantics are captured within semantically aligned speech encoders, potentially informing future advancements in cross-modal and cross-lingual representation learning. <div>
arXiv:2509.12093v2 Announce Type: replace 
Abstract: This paper introduces SENSE (Shared Embedding for N-lingual Speech and tExt), an open-source solution inspired by the SAMU-XLSR framework and conceptually similar to Meta AI's SONAR models. These approaches rely on a teacher-student framework to align a self-supervised speech encoder with the language-agnostic continuous representations of a text encoder at the utterance level. We describe how the original SAMU-XLSR method has been updated by selecting a stronger teacher text model and a better initial speech encoder. The source code for training and using SENSE models has been integrated into the SpeechBrain toolkit, and the first SENSE model we trained has been publicly released. We report experimental results on multilingual and multimodal semantic tasks, where our SENSE model achieves highly competitive performance. Finally, this study offers new insights into how semantics are captured in such semantically aligned speech encoders.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Natural Language Descriptions of Model Activations Convey Privileged Information?</title>
<link>https://arxiv.org/abs/2509.13316</link>
<guid>https://arxiv.org/abs/2509.13316</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM interpretability, activation verbalization, verbalizer LLM, benchmark evaluation, internal representations<br /><br />Summary:<br />1. Recent interpretability techniques use a secondary verbalizer large language model (LLM) to translate internal activations of a target LLM into natural language descriptions aiming to reveal how the target model processes inputs.<br />2. The study questions whether these activation verbalization methods truly access privileged information about the target model’s internal workings or if they simply restate information already present in the inputs.<br />3. Evaluation of popular verbalization methods across previously used datasets shows that these methods can perform well even without any access to the target model’s internals, indicating limitations in current benchmark datasets.<br />4. Controlled experiments demonstrate that the generated verbalizations often reflect the knowledge and biases of the verbalizer LLM itself rather than the internal knowledge or reasoning of the target LLM from which activations are decoded.<br />5. The findings highlight the necessity for more carefully designed benchmarks and experimental controls to critically assess whether activation verbalization methods yield authentic insights into how LLMs operate internally. <div>
arXiv:2509.13316v3 Announce Type: replace 
Abstract: Recent interpretability methods have proposed to translate LLM internal representations into natural language descriptions using a second verbalizer LLM. This is intended to illuminate how the target model represents and operates on inputs. But do such activation verbalization approaches actually provide privileged knowledge about the internal workings of the target model, or do they merely convey information about its inputs? We critically evaluate popular verbalization methods across datasets used in prior work and find that they can succeed at benchmarks without any access to target model internals, suggesting that these datasets may not be ideal for evaluating verbalization methods. We then run controlled experiments which reveal that verbalizations often reflect the parametric knowledge of the verbalizer LLM which generated them, rather than the knowledge of the target LLM whose activations are decoded. Taken together, our results indicate a need for targeted benchmarks and experimental controls to rigorously assess whether verbalization methods provide meaningful insights into the operations of LLMs.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging the Knowledge-Prediction Gap in LLMs on Multiple-Choice Questions</title>
<link>https://arxiv.org/abs/2509.23782</link>
<guid>https://arxiv.org/abs/2509.23782</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, multiple-choice questions, knowledge-prediction gap, latent subspace, KAPPA<br /><br />Summary: Large Language Models (LLMs) often exhibit a discrepancy between their demonstrated knowledge and their performance on multiple-choice questions (MCQs), frequently failing despite having correct underlying knowledge. To explore this phenomenon, the authors carry out a probing analysis focusing on residual streams within certain model layers, identifying a key subspace composed of two bases: a knowledge basis representing the probability of the ground-truth answer, and a prediction basis corresponding to the model's chosen answer probability. The root cause of incorrect MCQ predictions is found to be misalignment between the hidden states’ coordinates in these two bases. To address this, the paper proposes KAPPA (Knowledge-Aligned Prediction through Projection-based Adjustment), a parameter-free method that adjusts hidden states to better align the prediction coordinate with the knowledge coordinate within the identified subspace. Experimental evaluations on datasets like Big-Bench-Hard and ARC-Challenge show that KAPPA substantially improves accuracy and consistently outperforms existing baselines. Additionally, it is demonstrated that while optimal subspaces vary by task, they generalize across datasets to a fair degree. KAPPA's effectiveness also extends beyond MCQs to free-form question answering. This study offers a novel geometric perspective on the knowledge-prediction gap and introduces a practical technique to better align LLM behavior with its inherent knowledge. <div>
arXiv:2509.23782v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) often fail on multiple-choice questions (MCQs) despite demonstrating correct knowledge in other contexts, such as free-form generation. To investigate the mechanism underlying this knowledge-prediction gap on MCQs and alleviate it, we conduct a probing analysis and find that residual streams in certain layers contain a subspace spanned by two important bases: a \emph{knowledge basis} that encodes the probability of the ground-truth answer for a given MCQ and a \emph{prediction basis} that encodes the probability of the answer choice predicted by the model. We observe that incorrect predictions arise from a misalignment of the model's hidden states along these two bases. Hence, we introduce \textbf{KAPPA} (Knowledge-Aligned Prediction through Projection-based Adjustment), a parameter-free intervention that transforms the hidden states to align the prediction coordinate with the knowledge coordinate within this subspace. Experiments on binary-choice reformulations of Big-Bench-Hard and ARC-Challenge show that KAPPA substantially improves accuracy and consistently outperforms baselines. While optimal subspaces differ across tasks, subspaces generalize to some extent, as supported by cross-dataset experiments. Moreover, KAPPA extends its effectiveness to free-form questions beyond MCQs. Our work provides a new geometric understanding of the knowledge-prediction gap and offers a practical method for better aligning model behavior with its latent knowledge.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual Mechanisms of Value Expression: Intrinsic vs. Prompted Values in LLMs</title>
<link>https://arxiv.org/abs/2509.24319</link>
<guid>https://arxiv.org/abs/2509.24319</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, value expression, value vectors, value neurons, value steerability<br /><br />Summary: This paper investigates how large language models (LLMs) express values through two mechanisms: intrinsic expression, which reflects values learned inherently during training, and prompted expression, which arises from explicit prompts. The authors explore whether these two mechanisms overlap or differ significantly by analyzing them at a mechanistic level using two methods: value vectors, representing value directions extracted from the model’s residual stream, and value neurons, specific MLP neurons contributing to value expression. The study finds that intrinsic and prompted value mechanisms share some key components essential for value expression but also contain distinct unique elements. These unique elements result in differing effects on value steerability and response diversity—prompted mechanisms lead to higher steerability in guiding the model’s responses, while intrinsic mechanisms generate greater lexical diversity. Furthermore, components unique to intrinsic value expression encourage more varied language use in outputs. In contrast, those specific to prompted expression enhance following instructions robustly, including in challenging settings like jailbreaking tasks, indicating different underlying functional roles for each mechanism in value expression within LLMs. <div>
arXiv:2509.24319v2 Announce Type: replace 
Abstract: Large language models (LLMs) can express different values in two distinct ways: (1) intrinsic expression, reflecting the model's inherent values learned during training, and (2) prompted expression, elicited by explicit prompts. Given their widespread use in value alignment and persona steering, it is paramount to clearly understand their underlying mechanisms, particularly whether they mostly overlap (as one might expect) or rely on substantially different mechanisms, but this remains largely understudied. We analyze this at the mechanistic level using two approaches: (1) value vectors, feature directions representing value mechanisms extracted from the residual stream, and (2) value neurons, MLP neurons that contribute to value expressions. We demonstrate that intrinsic and prompted value mechanisms partly share common components that are crucial for inducing value expression, but also possess unique elements that manifest in different ways. As a result, these mechanisms lead to different degrees of value steerability (prompted > intrinsic) and response diversity (intrinsic > prompted). In particular, components unique to the intrinsic mechanism seem to promote lexical diversity in responses, whereas those specific to the prompted mechanism primarily strengthen instruction following, taking effect even in distant tasks like jailbreaking.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality Instruction and Reasoning Data Built from Permissive-First Text Sources</title>
<link>https://arxiv.org/abs/2509.25531</link>
<guid>https://arxiv.org/abs/2509.25531</guid>
<content:encoded><![CDATA[
<div> MixtureVitae, legal-risk mitigation, permissive datasets, pretraining corpus, model performance<br /><br />Summary:<br /><br />1. MixtureVitae is an open-access pretraining corpus designed to minimize legal risk while delivering strong model performance.  
2. It employs a risk-mitigated sourcing strategy that combines public-domain and permissively licensed texts (such as CC-BY and Apache licenses) with justified low-risk additions, including government works and EU TDM-eligible sources.  
3. The dataset also incorporates targeted instruction, reasoning, and synthetic data with clear provenance, ensuring transparency and quality.  
4. A transparent, multi-stage pipeline is used for license-aware filtering, safety and quality screening, and domain-aware mixing, which is publicly released along with the dataset to encourage reproducible research.  
5. Experiments using the open-sci-ref training protocol across various model sizes (130M to 1.7B parameters) and training budgets show that models trained on MixtureVitae consistently outperform other permissive datasets on standard benchmarks.  
6. At larger scales (1.7B parameters, 300B tokens), MixtureVitae-trained models surpass FineWeb-Edu and approach the performance of DCLM in later training phases.  
7. Performance is especially strong on math and code tasks and competitive on question answering tasks.  
8. Overall, MixtureVitae demonstrates that a permissive-first, risk-mitigated data collection approach can provide a practical, legally compliant foundation for training capable large language models (LLMs), reducing the need for indiscriminate web scraping without sacrificing model competitiveness. <div>
arXiv:2509.25531v2 Announce Type: replace 
Abstract: We present MixtureVitae, an open-access pretraining corpus built to minimize legal risk while providing strong model performance. MixtureVitae follows a risk-mitigated sourcing strategy that combines public-domain and permissively licensed text (e.g., CC-BY/Apache) with carefully justified low-risk additions (e.g., government works and EU TDM-eligible sources), alongside targeted instruction, reasoning and synthetic data with documented provenance. We detail a transparent, multi-stage pipeline for license-aware filtering, safety and quality screening, and domain-aware mixing, and we release the dataset and curation recipes to support reproducible research. In controlled experiments using the open-sci-ref training protocol (fixed architectures at 130M/400M/1.3B/1.7B parameters; training budgets of 50B and 300B tokens), models trained on MixtureVitae consistently outperform other permissive datasets across a suite of standard benchmarks, and at the 1.7B/300B setting they surpass FineWeb-Edu and approach DCLM in the later stages of training. Performance is particularly strong on math/code and competitive on QA tasks. These results demonstrate that permissive-first, risk-mitigated data provides a practical and legally mitigated foundation for training capable LLMs, reducing reliance on indiscriminate web scraping without sacrificing competitiveness. Code: https://github.com/ontocord/mixturevitae
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL</title>
<link>https://arxiv.org/abs/2510.02350</link>
<guid>https://arxiv.org/abs/2510.02350</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-SQL, WikiSQL, large language models, dataset revision, SQL query generation  

<br /><br />Summary:  
This paper addresses the task of converting natural language questions into SQL queries, a crucial function allowing non-expert users to interact with relational databases. It identifies several issues in the widely used WikiSQL dataset, such as inconsistencies related to case sensitivity, mismatches in data types, syntax errors, and questions that remained unanswered. To remedy these problems, the authors introduce LLMSQL, a thoroughly revised and transformed version of WikiSQL designed specifically for the modern era of large language models (LLMs). The revision process involved categorizing various types of errors and applying automated cleaning and re-annotation methodologies. To evaluate the benefits of these improvements, multiple large language models were tested, including Gemma 3, LLaMA 3.2, Mistral 7B, and others. Notably, DeepSeek-R1 achieved an impressive 88.40% accuracy in zero-shot settings, and models with fewer than 10 billion parameters exceeded 90% accuracy following fine-tuning. Importantly, LLMSQL is presented not merely as an updated dataset but as an LLM-ready benchmark that differs from the original WikiSQL. Unlike the original dataset designed for pointer-network models, LLMSQL offers clean natural language questions paired with complete SQL queries as plain text, facilitating easier generation and evaluation by current natural-language-to-SQL systems. <div>
arXiv:2510.02350v2 Announce Type: replace 
Abstract: Converting natural language questions into SQL queries enables non-expert users to interact with relational databases and has long been a central task for natural language interfaces to data. While the WikiSQL dataset played a key role in early text-to-SQL research, its usage has declined due to structural and annotation issues, including case sensitivity inconsistencies, data type mismatches, syntax errors, and unanswered questions. We present LLMSQL, a systematic revision and transformation of WikiSQL designed for the large language model era. We classify these errors and implement automated methods for cleaning and re-annotation. To assess the impact of these improvements, we evaluated multiple large language models, including Gemma 3, LLaMA 3.2, Mistral 7B, gpt-oss 20B, Phi-3.5 Mini, Qwen 2.5, OpenAI o4-mini, DeepSeek-R1, and others. Notably, DeepSeek-R1 achieves 88.40% accuracy in a zero-shot setting, and models under 10B parameters surpass 90% accuracy after fine-tuning. Rather than serving as an update, LLMSQL is introduced as an LLM-ready benchmark. Unlike the original WikiSQL, which was tailored for pointer-network models selecting tokens from input, LLMSQL provides clean natural language questions and full SQL queries as plain text, enabling straightforward generation and evaluation for modern natural-language-to-SQL models.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can AI Truly Represent Your Voice in Deliberations? A Comprehensive Study of Large-Scale Opinion Aggregation with LLMs</title>
<link>https://arxiv.org/abs/2510.05154</link>
<guid>https://arxiv.org/abs/2510.05154</guid>
<content:encoded><![CDATA[
<div> deliberation summarization, large-scale dataset, fairness, minority representation, evaluation metrics<br /><br />Summary: This article addresses the challenge of summarizing large-scale public deliberations, which involve thousands of free-form contributions, into representative and neutral summaries suitable for policymaking. It highlights the risks of using large language models (LLMs) for this purpose, including bias toward input order and underrepresentation of minority perspectives, raising important fairness concerns. To provide a solution, the authors introduce DeliberationBank, a comprehensive human-grounded dataset containing opinion data from 3,000 participants across ten deliberation questions, along with summary judgment data annotated by 4,500 participants on four critical dimensions: representativeness, informativeness, neutrality, and policy approval. Leveraging this dataset, the authors develop DeliberationJudge, a fine-tuned DeBERTa model designed to evaluate deliberation summaries from individual perspectives more efficiently and with better alignment to human judgments compared to various LLM-based judges. Using DeliberationJudge, the study evaluates 18 different LLMs and uncovers persistent issues such as the underrepresentation of minority positions in deliberation summaries. Ultimately, this framework offers a scalable, reliable, and human-aligned approach to evaluating deliberation summarization, aiming to enhance fairness and representativeness in AI-assisted policymaking processes. <div>
arXiv:2510.05154v3 Announce Type: replace 
Abstract: Large-scale public deliberations generate thousands of free-form contributions that must be synthesized into representative and neutral summaries for policy use. While LLMs have been shown as a promising tool to generate summaries for large-scale deliberations, they also risk underrepresenting minority perspectives and exhibiting bias with respect to the input order, raising fairness concerns in high-stakes contexts. Studying and fixing these issues requires a comprehensive evaluation at a large scale, yet current practice often relies on LLMs as judges, which show weak alignment with human judgments. To address this, we present DeliberationBank, a large-scale human-grounded dataset with (1) opinion data spanning ten deliberation questions created by 3,000 participants and (2) summary judgment data annotated by 4,500 participants across four dimensions (representativeness, informativeness, neutrality, policy approval). Using these datasets, we train DeliberationJudge, a fine-tuned DeBERTa model that can rate deliberation summaries from individual perspectives. DeliberationJudge is more efficient and more aligned with human judgements compared to a wide range of LLM judges. With DeliberationJudge, we evaluate 18 LLMs and reveal persistent weaknesses in deliberation summarization, especially underrepresentation of minority positions. Our framework provides a scalable and reliable way to evaluate deliberation summarization, helping ensure AI systems are more representative and equitable for policymaking.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StreamingThinker: Large Language Models Can Think While Reading</title>
<link>https://arxiv.org/abs/2510.17238</link>
<guid>https://arxiv.org/abs/2510.17238</guid>
<content:encoded><![CDATA[
<div> Keywords: Streaming thinking, Large language models, Chain of Thought reasoning, Latency reduction, Parallel inference<br /><br />Summary: Large language models (LLMs) excel at chain of thought (CoT) reasoning but traditionally start reasoning only after receiving the full input, causing latency and diminished attention to early input in dynamic scenarios. Inspired by human cognition that thinks while reading, the authors propose a novel "streaming thinking" paradigm where reasoning occurs incrementally as input arrives and adjusts depth after the full input is processed. They develop StreamingThinker, a framework implementing this paradigm by integrating streaming CoT generation, streaming-constraint training, and streaming parallel inference. StreamingThinker uses streaming reasoning units with quality control and enforces order-preserving reasoning via streaming attention masks and position encoding. It also employs parallel key-value caches to separate input encoding from reasoning generation, enabling true concurrency and alignment. Evaluations on the Qwen3 model family across math, logical, and context-based QA reasoning tasks demonstrate that StreamingThinker matches batch thinking performance while reducing token waiting time before reasoning onset by 80% and cutting final answer latency by over 60%. This shows the streaming paradigm’s effectiveness for improving reasoning efficiency in LLMs. Code will be publicly available at the provided GitHub repository. <div>
arXiv:2510.17238v2 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in chain of thought (CoT) reasoning. However, the current LLM reasoning paradigm initiates thinking only after the entire input is available, which introduces unnecessary latency and weakens attention to earlier information in dynamic scenarios. Inspired by human cognition of thinking while reading, we first design a \textit{\textbf{streaming thinking}} paradigm for LLMs, where reasoning unfolds in the order of input and further adjusts its depth once reading is complete. We instantiate this paradigm with \textit{StreamingThinker}, a framework that enables LLMs to think while reading through the integration of streaming CoT generation, streaming-constraint training, and streaming parallel inference. Specifically, StreamingThinker employs streaming reasoning units with quality control for CoT generation, enforces order-preserving reasoning through streaming attention masks and position encoding, and leverages parallel KV caches that decouple input encoding from reasoning generation, thereby ensuring alignment and enabling true concurrency. We evaluate StreamingThinker on the Qwen3 model family across math reasoning, logical reasoning, and context-based QA reasoning tasks. Experimental results show that the StreamingThinker preserves performance comparable to batch thinking, while yielding an 80\% reduction in token waiting before the onset of reasoning and a more than 60\% reduction in time-level latency for producing the final answer, demonstrating the effectiveness of the streaming paradigm for LLM reasoning. Code will be released at https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Language Models Associate Sound with Meaning? A Multimodal Study of Sound Symbolism</title>
<link>https://arxiv.org/abs/2511.10045</link>
<guid>https://arxiv.org/abs/2511.10045</guid>
<content:encoded><![CDATA[
<div> Sound symbolism, Multimodal Large Language Models, phonetic iconicity, LEX-ICON dataset, phoneme-level attention<br /><br />Summary: This study explores sound symbolism, the non-arbitrary relationship between phonetic forms and meanings, as a means to understand how Multimodal Large Language Models (MLLMs) process auditory information in human languages. Researchers evaluated MLLMs on phonetic iconicity using both textual inputs (orthographic and IPA) and auditory signals, examining up to 25 semantic dimensions such as sharpness versus roundness. To facilitate this, the study introduces LEX-ICON, a comprehensive dataset containing 8,052 mimetic words from four natural languages—English, French, Japanese, and Korean—alongside 2,930 pseudo-words, all annotated with semantic features across text and audio modalities. Key findings reveal that MLLMs demonstrate phonetic intuitions consistent with linguistic theories across multiple semantic dimensions. Additionally, the models exhibit distinctive phonosemantic attention patterns, focusing on iconic phonemes that reflect meaningful sound-meaning correspondences. This work bridges artificial intelligence and cognitive linguistics by providing the first extensive, quantitative analysis of phonetic iconicity related to the interpretability of MLLMs, highlighting their capacity to encode and attend to sound symbolism in multimodal contexts. <div>
arXiv:2511.10045v3 Announce Type: replace 
Abstract: Sound symbolism is a linguistic concept that refers to non-arbitrary associations between phonetic forms and their meanings. We suggest that this can be a compelling probe into how Multimodal Large Language Models (MLLMs) interpret auditory information in human languages. We investigate MLLMs' performance on phonetic iconicity across textual (orthographic and IPA) and auditory forms of inputs with up to 25 semantic dimensions (e.g., sharp vs. round), observing models' layer-wise information processing by measuring phoneme-level attention fraction scores. To this end, we present LEX-ICON, an extensive mimetic word dataset consisting of 8,052 words from four natural languages (English, French, Japanese, and Korean) and 2,930 systematically constructed pseudo-words, annotated with semantic features applied across both text and audio modalities. Our key findings demonstrate (1) MLLMs' phonetic intuitions that align with existing linguistic research across multiple semantic dimensions and (2) phonosemantic attention patterns that highlight models' focus on iconic phonemes. These results bridge domains of artificial intelligence and cognitive linguistics, providing the first large-scale, quantitative analyses of phonetic iconicity in terms of MLLMs' interpretability.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models</title>
<link>https://arxiv.org/abs/2511.14295</link>
<guid>https://arxiv.org/abs/2511.14295</guid>
<content:encoded><![CDATA[
<div> Keywords: Arabic, large language models, linguistic competence, grammar, benchmark<br /><br />Summary:<br /><br />1. AraLingBench is a newly introduced benchmark designed to evaluate the Arabic linguistic competence of large language models (LLMs).<br />2. The benchmark covers five essential linguistic domains: grammar, morphology, spelling, reading comprehension, and syntax. It consists of 150 expert-designed multiple-choice questions aimed at directly assessing structural language understanding.<br />3. The evaluation of 35 Arabic and bilingual LLMs using AraLingBench reveals that while these models exhibit strong surface-level proficiency, they struggle significantly with deeper grammatical and syntactic reasoning.<br />4. AraLingBench exposes a notable discrepancy between the high performance of current LLMs on traditional knowledge-based benchmarks and their actual linguistic mastery, indicating reliance on memorization or pattern recognition rather than genuine comprehension.<br />5. By focusing on fundamental linguistic skills, AraLingBench offers a diagnostic framework that can guide the development and improvement of Arabic LLMs. The evaluation code for the benchmark is made publicly available on GitHub to encourage further research and development. <div>
arXiv:2511.14295v2 Announce Type: replace 
Abstract: We present AraLingBench: a fully human annotated benchmark for evaluating the Arabic linguistic competence of large language models (LLMs). The benchmark spans five core categories: grammar, morphology, spelling, reading comprehension, and syntax, through 150 expert-designed multiple choice questions that directly assess structural language understanding. Evaluating 35 Arabic and bilingual LLMs reveals that current models demonstrate strong surface level proficiency but struggle with deeper grammatical and syntactic reasoning. AraLingBench highlights a persistent gap between high scores on knowledge-based benchmarks and true linguistic mastery, showing that many models succeed through memorization or pattern recognition rather than authentic comprehension. By isolating and measuring fundamental linguistic skills, AraLingBench provides a diagnostic framework for developing Arabic LLMs. The full evaluation code is publicly available on GitHub.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TS-PEFT: Unveiling Token-Level Redundancy in Parameter-Efficient Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.16147</link>
<guid>https://arxiv.org/abs/2511.16147</guid>
<content:encoded><![CDATA[
arXiv:2511.16147v2 Announce Type: replace 
Abstract: Current Parameter-Efficient Fine-Tuning (PEFT) methods typically operate under an implicit assumption: once a target module is selected, every token passing through it contributes equally to the downstream task and requires a parameter update. In this paper, we challenge this convention and unveil a pervasive token-level redundancy in the fine-tuning of large models. We propose TS-PEFT, a theoretically grounded framework utilizing proximal optimization to dynamically identify and skip redundant token updates during training. Our extensive experiments across Natural Language Understanding, Commonsense Reasoning, and Visual Instruction Tuning demonstrate that indiscriminately updating all tokens is not only computationally superfluous but often introduces optimization noise. Strikingly, by discarding 40%-60% of token updates, TS-PEFT consistently matches or surpasses the performance of dense baselines (e.g., LoRA, DoRA). Furthermore, we provide an in-depth analysis revealing that the learned token-level sparsity serves as a superior indicator of module importance compared to traditional weight norms, offering a novel data-driven perspective on the intrinsic adaptation mechanism of large models.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiscoVerse: Multi-Agent Pharmaceutical Co-Scientist for Traceable Drug Discovery and Reverse Translation</title>
<link>https://arxiv.org/abs/2511.18259</link>
<guid>https://arxiv.org/abs/2511.18259</guid>
<content:encoded><![CDATA[
arXiv:2511.18259v2 Announce Type: replace 
Abstract: Pharmaceutical research and development has accumulated vast and heterogeneous archives of data. Much of this knowledge stems from discontinued programs, and reusing these archives is invaluable for reverse translation. However, in practice, such reuse is often infeasible. In this work, we introduce DiscoVerse, a multi-agent co-scientist designed to support pharmaceutical research and development at Roche. Designed as a human-in-the-loop assistant, DiscoVerse enables domain-specific queries by delivering evidence-based answers: it retrieves relevant data, links across documents, summarises key findings and preserves institutional memory. We assess DiscoVerse through expert evaluation of source-linked outputs. Our evaluation spans a selected subset of 180 molecules from Roche's research and development repositories, encompassing over 0.87 billion BPE tokens and more than four decades of research. To our knowledge, this represents the first agentic framework to be systematically assessed on real pharmaceutical data for reverse translation, enabled by authorized access to confidential archives covering the full lifecycle of drug development. Our contributions include: role-specialized agent designs aligned with scientist workflows; human-in-the-loop support for reverse translation; expert evaluation; and a large-scale demonstration showing promising decision-making insights. In brief, across seven benchmark queries, DiscoVerse achieved near-perfect recall ($\geq 0.99$) with moderate precision ($0.71-0.91$). Qualitative assessments and three real-world pharmaceutical use cases further showed faithful, source-linked synthesis across preclinical and clinical evidence.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Representational Stability of Truth in Large Language Models</title>
<link>https://arxiv.org/abs/2511.19166</link>
<guid>https://arxiv.org/abs/2511.19166</guid>
<content:encoded><![CDATA[
arXiv:2511.19166v2 Announce Type: replace 
Abstract: Large language models (LLMs) are widely used for factual tasks such as "What treats asthma?" or "What is the capital of Latvia?". However, it remains unclear how stably LLMs encode distinctions between true, false, and neither-true-nor-false content in their internal probabilistic representations. We introduce representational stability as the robustness of an LLM's veracity representations to perturbations in the operational definition of truth. We assess representational stability by (i) training a linear probe on an LLM's activations to separate true from not-true statements and (ii) measuring how its learned decision boundary shifts under controlled label changes. Using activations from sixteen open-source models and three factual domains, we compare two types of neither statements. The first are fact-like assertions about entities we believe to be absent from any training data. We call these unfamiliar neither statements. The second are nonfactual claims drawn from well-known fictional contexts. We call these familiar neither statements. The unfamiliar statements induce the largest boundary shifts, producing up to $40\%$ flipped truth judgements in fragile domains (such as word definitions), while familiar fictional statements remain more coherently clustered and yield smaller changes ($\leq 8.2\%$). These results suggest that representational stability stems more from epistemic familiarity than from linguistic form. More broadly, our approach provides a diagnostic for auditing and training LLMs to preserve coherent truth assignments under semantic uncertainty, rather than optimizing for output accuracy alone.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLIBD: Bridging Vision and Genomics for Biodiversity Monitoring at Scale</title>
<link>https://arxiv.org/abs/2405.17537</link>
<guid>https://arxiv.org/abs/2405.17537</guid>
<content:encoded><![CDATA[
arXiv:2405.17537v5 Announce Type: replace-cross 
Abstract: Measuring biodiversity is crucial for understanding ecosystem health. While prior works have developed machine learning models for taxonomic classification of photographic images and DNA separately, in this work, we introduce a multimodal approach combining both, using CLIP-style contrastive learning to align images, barcode DNA, and text-based representations of taxonomic labels in a unified embedding space. This allows for accurate classification of both known and unknown insect species without task-specific fine-tuning, leveraging contrastive learning for the first time to fuse barcode DNA and image data. Our method surpasses previous single-modality approaches in accuracy by over 8% on zero-shot learning tasks, showcasing its effectiveness in biodiversity studies.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SimSUM: Simulated Benchmark with Structured and Unstructured Medical Records</title>
<link>https://arxiv.org/abs/2409.08936</link>
<guid>https://arxiv.org/abs/2409.08936</guid>
<content:encoded><![CDATA[
arXiv:2409.08936v4 Announce Type: replace-cross 
Abstract: Clinical information extraction, which involves structuring clinical concepts from unstructured medical text, remains a challenging problem that could benefit from the inclusion of tabular background information available in electronic health records. Existing open-source datasets lack explicit links between structured features and clinical concepts in the text, motivating the need for a new research dataset. We introduce SimSUM, a benchmark dataset of 10,000 simulated patient records that link unstructured clinical notes with structured background variables. Each record simulates a patient encounter in the domain of respiratory diseases and includes tabular data (e.g., symptoms, diagnoses, underlying conditions) generated from a Bayesian network whose structure and parameters are defined by domain experts. A large language model (GPT-4o) is prompted to generate a clinical note describing the encounter, including symptoms and relevant context. These notes are annotated with span-level symptom mentions. We conduct an expert evaluation to assess note quality and run baseline predictive models on both the tabular and textual data. The SimSUM dataset is primarily designed to support research on clinical information extraction in the presence of tabular background variables, which can be linked through domain knowledge to concepts of interest to be extracted from the text -- namely, symptoms in the case of SimSUM. Secondary uses include research on the automation of clinical reasoning over both tabular data and text, causal effect estimation in the presence of tabular and/or textual confounders, and multi-modal synthetic data generation. SimSUM is not intended for training clinical decision support systems or production-grade models, but rather to facilitate reproducible research in a simplified and controlled setting.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AEIOU: A Unified Defense Framework against NSFW Prompts in Text-to-Image Models</title>
<link>https://arxiv.org/abs/2412.18123</link>
<guid>https://arxiv.org/abs/2412.18123</guid>
<content:encoded><![CDATA[
arXiv:2412.18123v3 Announce Type: replace-cross 
Abstract: As text-to-image (T2I) models advance and gain widespread adoption, their associated safety concerns are becoming increasingly critical. Malicious users exploit these models to generate Not-Safe-for-Work (NSFW) images using harmful or adversarial prompts, underscoring the need for effective safeguards to ensure the integrity and compliance of model outputs. However, existing detection methods often exhibit low accuracy and inefficiency. In this paper, we propose AEIOU, a defense framework that is adaptable, efficient, interpretable, optimizable, and unified against NSFW prompts in T2I models. AEIOU extracts NSFW features from the hidden states of the model's text encoder, utilizing the separable nature of these features to detect NSFW prompts. The detection process is efficient, requiring minimal inference time. AEIOU also offers real-time interpretation of results and supports optimization through data augmentation techniques. The framework is versatile, accommodating various T2I architectures. Our extensive experiments show that AEIOU significantly outperforms both commercial and open-source moderation tools, achieving over 95\% accuracy across all datasets and improving efficiency by at least tenfold. It effectively counters adaptive attacks and excels in few-shot and multi-label scenarios.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Make LVLMs Focus: Context-Aware Attention Modulation for Better Multimodal In-Context Learning</title>
<link>https://arxiv.org/abs/2505.17097</link>
<guid>https://arxiv.org/abs/2505.17097</guid>
<content:encoded><![CDATA[
arXiv:2505.17097v3 Announce Type: replace-cross 
Abstract: Multimodal in-context learning (ICL) is becoming a key capability that allows large vision-language models (LVLMs) to adapt to novel tasks without parameter updates, which expands their usefulness in many real-world applications. However, ICL performance remains unstable even when the in-context demonstrations (ICDs) are well matched, showing that LVLMs still struggle to make full use of the provided context. While existing work mainly focuses on prompt engineering or post-hoc logit calibration, we study the attention mechanisms inside LVLMs to address their inherent limitations. We identify two important weaknesses in their self-attention that hinder effective ICL. To address these weaknesses, we propose \textbf{Context-Aware Modulated Attention} (CAMA), a training-free and plug-and-play method that dynamically adjusts attention logits based on the input in-context sequence. CAMA uses a two-stage modulation process that strengthens attention to semantically important tokens, especially visual ones. Across four LVLMs and seven benchmarks, CAMA consistently outperforms vanilla models and baselines, showing clear effectiveness and generalization. It can also activate the intended benefits of prompt engineering methods and remains robust across different sequence configurations. Therefore, CAMA opens up new directions for improving multimodal reasoning through a deeper understanding of attention dynamics.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-grained Spatiotemporal Grounding on Egocentric Videos</title>
<link>https://arxiv.org/abs/2508.00518</link>
<guid>https://arxiv.org/abs/2508.00518</guid>
<content:encoded><![CDATA[
arXiv:2508.00518v2 Announce Type: replace-cross 
Abstract: Spatiotemporal video grounding aims to localize target entities in videos based on textual queries. While existing research has made significant progress in exocentric videos, the egocentric setting remains relatively underexplored, despite its growing importance in applications such as augmented reality and robotics. In this work, we conduct a systematic analysis of the discrepancies between egocentric and exocentric videos, revealing key challenges such as shorter object durations, sparser trajectories, smaller object sizes, and larger positional shifts. To address these challenges, we introduce EgoMask, the first pixel-level benchmark for fine-grained spatiotemporal grounding in egocentric videos. It is constructed by our proposed automatic annotation pipeline, which annotates referring expressions and object masks across short-, medium-, and long-term videos. Additionally, we create EgoMask-Train, a large-scale training dataset to facilitate model development. Experiments demonstrate that the state-of-the-art spatiotemporal grounding models perform poorly on our benchmark EgoMask, but fine-tuning on EgoMask-Train yields significant improvements, while preserving performance on exocentric datasets. Our work thus provides essential resources and insights for advancing egocentric video understanding. Our code is available at https://github.com/LaVi-Lab/EgoMask .
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls</title>
<link>https://arxiv.org/abs/2508.06457</link>
<guid>https://arxiv.org/abs/2508.06457</guid>
<content:encoded><![CDATA[
arXiv:2508.06457v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated impressive fluency and reasoning capabilities, but their potential for misuse has raised growing concern. In this paper, we present ScamAgent, an autonomous multi-turn agent built on top of LLMs, capable of generating highly realistic scam call scripts that simulate real-world fraud scenarios. Unlike prior work focused on single-shot prompt misuse, ScamAgent maintains dialogue memory, adapts dynamically to simulated user responses, and employs deceptive persuasion strategies across conversational turns. We show that current LLM safety guardrails, including refusal mechanisms and content filters, are ineffective against such agent-based threats. Even models with strong prompt-level safeguards can be bypassed when prompts are decomposed, disguised, or delivered incrementally within an agent framework. We further demonstrate the transformation of scam scripts into lifelike voice calls using modern text-to-speech systems, completing a fully automated scam pipeline. Our findings highlight an urgent need for multi-turn safety auditing, agent-level control frameworks, and new methods to detect and disrupt conversational deception powered by generative AI.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>You May Speak Freely: Improving the Fine-Grained Visual Recognition Capabilities of Multimodal Large Language Models with Answer Extraction</title>
<link>https://arxiv.org/abs/2510.14885</link>
<guid>https://arxiv.org/abs/2510.14885</guid>
<content:encoded><![CDATA[
arXiv:2510.14885v2 Announce Type: replace-cross 
Abstract: Despite the renewed interest in zero-shot visual classification due to the rise of Multimodal Large Language Models (MLLMs), the problem of evaluating free-form responses of auto-regressive models remains a persistent challenge. Most existing works focus on language-only tasks or don't consider Multiple Choice Questions (MCQs) beyond 5-way options, both of which are critical capabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where choice counts are in the hundreds to thousands and the choices are highly related. Furthermore, in this highly multi-way MCQ setting it is not clear how to extend LLM choice extraction to retrieval-based problems, where computing probabilities over the choice set is computationally costly. In this work we investigate nlg2choice, a simple two-stage method which first asks the MLLM an open-ended question for the task with minimal constraints, then uses text-only constrained decoding to predict the most likely choice. In retrieval settings, we compute the probability of the constrained response taking that choice with an early stopping method to significantly improve throughput. Our results show improvement over a suite of seven fine-grained visual datasets when evaluating in terms of classification and retrieval, and show that this performance holds over the various ways that users of LLMs can implement tasks in natural language.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Many-Shot Prompting Fails: An Empirical Study of LLM Code Translation</title>
<link>https://arxiv.org/abs/2510.16809</link>
<guid>https://arxiv.org/abs/2510.16809</guid>
<content:encoded><![CDATA[
arXiv:2510.16809v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) with vast context windows offer new avenues for in-context learning (ICL), where providing many examples ("many-shot" prompting) is often assumed to enhance performance. We investigate this assumption for the complex task of code translation. Through a large-scale empirical study of over 90,000 translations, we systematically evaluate the impact of scaling in-context examples from zero-shot to many-shot configurations of up to 625 examples, with prompts spanning from approximately 100,000 to 800,000 tokens. Our findings reveal a "many-shot paradox": while static similarity metrics may modestly improve with more examples, functional correctness consistently peaks with few-shot prompting (5-25 examples). Providing substantially more examples often degrades this crucial functional performance. This study highlights that for code translation, the quality of a few well-chosen examples outweighs sheer quantity, challenging the universal efficacy of "more is better" for ICL and underscoring the task-dependent nature of optimal prompting strategies. Our results have significant implications for effectively leveraging LLMs in software engineering.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows</title>
<link>https://arxiv.org/abs/2510.24411</link>
<guid>https://arxiv.org/abs/2510.24411</guid>
<content:encoded><![CDATA[
arXiv:2510.24411v2 Announce Type: replace-cross 
Abstract: Computer-using agents powered by Vision-Language Models (VLMs) have demonstrated human-like capabilities in operating digital environments like mobile platforms. While these agents hold great promise for advancing digital automation, their potential for unsafe operations, such as system compromise and privacy leakage, is raising significant concerns. Detecting these safety concerns across the vast and complex operational space of mobile environments presents a formidable challenge that remains critically underexplored. To establish a foundation for mobile agent safety research, we introduce MobileRisk-Live, a dynamic sandbox environment accompanied by a safety detection benchmark comprising realistic trajectories with fine-grained annotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety detection framework that synergistically combines a Formal Verifier for detecting explicit system-level violations with a VLM-based Contextual Judge for assessing contextual risks and agent actions. Experiments show that OS-Sentinel achieves 10%-30% improvements over existing approaches across multiple metrics. Further analysis provides critical insights that foster the development of safer and more reliable autonomous mobile agents. Our code and data are available at https://github.com/OS-Copilot/OS-Sentinel.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProgRAG: Hallucination-Resistant Progressive Retrieval and Reasoning over Knowledge Graphs</title>
<link>https://arxiv.org/abs/2511.10240</link>
<guid>https://arxiv.org/abs/2511.10240</guid>
<content:encoded><![CDATA[
arXiv:2511.10240v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) demonstrate strong reasoning capabilities but struggle with hallucinations and limited transparency. Recently, KG-enhanced LLMs that integrate knowledge graphs (KGs) have been shown to improve reasoning performance, particularly for complex, knowledge-intensive tasks. However, these methods still face significant challenges, including inaccurate retrieval and reasoning failures, often exacerbated by long input contexts that obscure relevant information or by context constructions that struggle to capture the richer logical directions required by different question types. Furthermore, many of these approaches rely on LLMs to directly retrieve evidence from KGs, and to self-assess the sufficiency of this evidence, which often results in premature or incorrect reasoning. To address the retrieval and reasoning failures, we propose ProgRAG, a multi-hop knowledge graph question answering (KGQA) framework that decomposes complex questions into sub-questions, and progressively extends partial reasoning paths by answering each sub-question. At each step, external retrievers gather candidate evidence, which is then refined through uncertainty-aware pruning by the LLM. Finally, the context for LLM reasoning is optimized by organizing and rearranging the partial reasoning paths obtained from the sub-question answers. Experiments on three well-known datasets demonstrate that ProgRAG outperforms existing baselines in multi-hop KGQA, offering improved reliability and reasoning quality.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SynBullying: A Multi LLM Synthetic Conversational Dataset for Cyberbullying Detection</title>
<link>https://arxiv.org/abs/2511.11599</link>
<guid>https://arxiv.org/abs/2511.11599</guid>
<content:encoded><![CDATA[
arXiv:2511.11599v2 Announce Type: replace-cross 
Abstract: We introduce SynBullying, a synthetic multi-LLM conversational dataset for studying and detecting cyberbullying (CB). SynBullying provides a scalable and ethically safe alternative to human data collection by leveraging large language models (LLMs) to simulate realistic bullying interactions. The dataset offers (i) conversational structure, capturing multi-turn exchanges rather than isolated posts; (ii) context-aware annotations, where harmfulness is assessed within the conversational flow considering context, intent, and discourse dynamics; and (iii) fine-grained labeling, covering various CB categories for detailed linguistic and behavioral analysis. We evaluate SynBullying across five dimensions, including conversational structure, lexical patterns, sentiment/toxicity, role dynamics, harm intensity, and CB-type distribution. We further examine its utility by testing its performance as standalone training data and as an augmentation source for CB classification.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent Collaboration in Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2511.20639</link>
<guid>https://arxiv.org/abs/2511.20639</guid>
<content:encoded><![CDATA[
<div> Multi-agent systems, large language models, latent collaboration, shared memory, efficiency<br /><br />Summary:<br /><br />1. The paper introduces LatentMAS, a novel framework for multi-agent systems (MAS) that enables collaboration among large language model (LLM) agents directly within continuous latent space rather than relying on conventional text-based communication.<br /><br />2. LatentMAS operates without the need for additional training, using each agent's last-layer hidden embeddings to generate auto-regressive latent thoughts, which are shared and preserved in a shared latent working memory for lossless information exchange.<br /><br />3. The authors provide theoretical analysis proving that LatentMAS achieves higher expressiveness and ensures lossless information preservation with significantly lower complexity compared to traditional text-based MAS approaches.<br /><br />4. Empirical evaluation across nine benchmarks related to math and science reasoning, commonsense understanding, and code generation shows that LatentMAS consistently outperforms baseline models, improving accuracy by up to 14.6%, reducing token usage by 70.8% to 83.7%, and accelerating inference speed by 4 to 4.3 times.<br /><br />5. The presented framework demonstrates substantial improvements in system-level reasoning quality and efficiency, enabling more effective multi-agent coordination with open-sourced code and data available for further research and application. <div>
arXiv:2511.20639v2 Announce Type: replace 
Abstract: Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuous latent space. We introduce LatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS, each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings. A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. We provide theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning, commonsense understanding, and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference. These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at https://github.com/Gen-Verse/LatentMAS.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.22601</link>
<guid>https://arxiv.org/abs/2509.22601</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, self-imitation learning, exploration-exploitation, large language models, tool use

<br /><br />Summary:  
This paper addresses the challenge of balancing exploration and exploitation in reinforcement learning (RL) for large language models (LLMs) involved in long-horizon, sparsely-rewarded agent tasks. Traditional methods rely on maximizing policy entropy to stimulate exploration, which can lead to instability due to multi-turn distribution shifts. The authors propose SPEAR, a novel approach based on self-imitation learning (SIL) that gradually steers policy entropy through curriculum scheduling, harmonizing intrinsic reward shaping with self-imitation. This method accelerates exploration by encouraging frequent tool interactions early on and promotes exploitation by reinforcing successful strategies as familiarity with the environment grows. SPEAR is integrated with a strong baseline called Dr.BoT, leveraging standard industrial RL optimization techniques. Experimental results on benchmarks including ALFWorld, WebShop, AIME24, and AIME25 demonstrate significant success rate improvements compared to existing methods (up to 16.1%, 20.7%, and more). These enhancements come with only a modest increase in theoretical complexity (10%-25%) and negligible practical runtime overhead, highlighting SPEAR's scalability and ease of adoption for agentic LLM training. <div>
arXiv:2509.22601v4 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) is the dominant paradigm for sharpening strategic tool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks, yet it faces a fundamental challenge of exploration-exploitation trade-off. Existing studies stimulate exploration through the lens of policy entropy, but such mechanical entropy maximization is prone to RL instability due to the multi-turn distribution shifting. In this paper, we target the progressive exploration-exploitation balance under the guidance of the agent's own experiences without succumbing to either entropy collapsing or runaway divergence. We propose SPEAR, a self-imitation learning (SIL) recipe for training agentic LLMs. It extends the vanilla SIL, where a replay buffer stores good experience for off-policy update, by gradually steering the policy entropy across stages. Specifically, the proposed curriculum scheduling harmonizes intrinsic reward shaping and self-imitation to 1) expedite exploration via frequent tool interactions at the beginning, and 2) strengthen exploitation of successful tactics upon convergence towards familiarity with the environment. We also combine bag-of-tricks of industrial RL optimizations for a strong baseline Dr.BoT to demonstrate our effectiveness. In ALFWorld and WebShop, SPEAR increases the success rates of GRPO/GiGPO/Dr.BoT by up to 16.1%/5.1%/8.6% and 20.7%/11.8%/13.9%, respectively. In AIME24 and AIME25, SPEAR boosts Dr.BoT by up to 3.8% and 6.1%, respectively. Such gains incur only 10%-25% extra theoretical complexity and negligible runtime overhead in practice, demonstrating the plug-and-play scalability of SPEAR.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Code Foundation Models to Agents and Applications: A Comprehensive Survey and Practical Guide to Code Intelligence</title>
<link>https://arxiv.org/abs/2511.18538</link>
<guid>https://arxiv.org/abs/2511.18538</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, code generation, model lifecycle, fine-tuning, autonomous coding agents<br /><br />Summary: This paper provides a comprehensive synthesis and practical guide to large language models (LLMs) for automated code generation, covering the entire model lifecycle from data curation to deployment. It reviews the evolution of code LLMs, highlighting performance improvements from early rule-based methods to Transformer architectures achieving over 95% success on benchmarks like HumanEval. The study systematically examines code capabilities in both general-purpose models (GPT-4, Claude, LLaMA) and code-specialized models (StarCoder, Code LLaMA, DeepSeek-Coder, QwenCoder), analyzing their design decisions, techniques, and trade-offs. The authors identify a significant research-practice gap, noting discrepancies between academic benchmarks and real-world software development requirements such as code correctness, security, contextual understanding of large codebases, and integration with development tools. They then map emerging research directions to practical needs, emphasizing the importance of bridging theory and application. Finally, the work presents extensive experiments comparing code pre-training, supervised fine-tuning, and reinforcement learning methods, exploring scaling laws, hyperparameter sensitivity, architectural choices, framework selection, and dataset impacts to guide future development and optimization of code LLMs for industrial use. <div>
arXiv:2511.18538v5 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Empathy by Design: Aligning Large Language Models for Healthcare Dialogue</title>
<link>https://arxiv.org/abs/2512.06097</link>
<guid>https://arxiv.org/abs/2512.06097</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Direct Preference Optimization, Empathy, Healthcare Communication, Factual Accuracy<br /><br />Summary:<br />1. Large Language Models (LLMs) show strong generative and reasoning skills but struggle with factual reliability and empathetic communication in healthcare and caregiving contexts.<br />2. These limitations are critical because non-professional users and caregivers rely on accurate medical guidance and emotional support.<br />3. The study introduces a Direct Preference Optimization (DPO)-based alignment framework that enhances factual correctness, semantic coherence, and human-centric qualities like empathy, politeness, and simplicity in dialogues between caregivers and patients.<br />4. The approach fine-tunes domain-adapted LLMs using pairwise preference data, favoring responses that are supportive and accessible rather than prescriptive or overly technical.<br />5. Compared to traditional reinforcement-learning alignment, the DPO method aligns model outputs more efficiently with human preferences.<br />6. Empirical results on multiple open and proprietary LLMs demonstrate that DPO-tuned models outperform baseline and commercial systems (e.g., Google medical dialogue systems) in semantic alignment, factual accuracy, and human-centric metrics.<br />7. The findings support preference-based alignment as a scalable, transparent way to build trustworthy, empathetic, and clinically informed AI assistants for healthcare communication.<br />8. The authors provide open-source code to facilitate further research and application. <div>
arXiv:2512.06097v1 Announce Type: new 
Abstract: General-purpose large language models (LLMs) have demonstrated remarkable generative and reasoning capabilities but remain limited in healthcare and caregiving applications due to two key deficiencies: factual unreliability and a lack of empathetic communication. These shortcomings pose significant risks in sensitive contexts where users, particularly non-professionals and caregivers, seek medically relevant guidance or emotional reassurance. To address these challenges, we introduce a Direct Preference Optimization (DPO)-based alignment framework designed to improve factual correctness, semantic coherence, and human-centric qualities such as empathy, politeness, and simplicity in caregiver-patient dialogues. Our approach fine-tunes domain-adapted LLMs using pairwise preference data, where preferred responses reflect supportive and accessible communication styles while rejected ones represent prescriptive or overly technical tones. This direct optimization method aligns model outputs with human preferences more efficiently than traditional reinforcement-learning-based alignment. Empirical evaluations across multiple open and proprietary LLMs show that our DPO-tuned models achieve higher semantic alignment, improved factual accuracy, and stronger human-centric evaluation scores compared to baseline and commercial alternatives such as Google medical dialogue systems. These improvements demonstrate that preference-based alignment offers a scalable and transparent pathway toward developing trustworthy, empathetic, and clinically informed AI assistants for caregiver and healthcare communication. Our open-source code is available at: https://github.com/LeonG19/Empathy-by-Design
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Morphologically-Informed Tokenizers for Languages with Non-Concatenative Morphology: A case study of Yolox\'ochtil Mixtec ASR</title>
<link>https://arxiv.org/abs/2512.06169</link>
<guid>https://arxiv.org/abs/2512.06169</guid>
<content:encoded><![CDATA[
<div> Keywords: Yoloxóchitl Mixtec, morphologically-informed tokenizers, ASR, tonal morphology, word error rate<br /><br />Summary:<br /><br />This paper explores the use of morphologically-informed tokenizers to assist and improve the efficiency of interlinear gloss annotation in an audio corpus of Yoloxóchitl Mixtec (YM). The authors introduce two novel tokenization schemes designed to handle non-concatenative, tonal morphology: the Segment and Melody tokenizer, which extracts tones without predicting word segmentation, and the Sequence of Processes tokenizer, which predicts word segmentation and could enable end-to-end ASR systems to output both segmented and unsegmented transcriptions simultaneously. Evaluations show these new tokenizers perform competitively compared to conventional Byte Pair Encoding (BPE) and Unigram models. Specifically, the Segment and Melody tokenizer yields better word error rates than traditional tokenizers, although it does not surpass them in character error rate. Furthermore, the study analyzes the tokenizers using morphological and information-theoretic metrics, finding correlations that may predict performance on downstream tasks. The results indicate that nonlinear tokenizers tailored for the unique morphology of the language can be as effective as commonly used tokenizers in ASR systems. However, the paper highlights the need for further research to assess the effectiveness of these tokenizers in other downstream language processing applications. <div>
arXiv:2512.06169v1 Announce Type: new 
Abstract: This paper investigates the impact of using morphologically-informed tokenizers to aid and streamline the interlinear gloss annotation of an audio corpus of Yolox\'ochitl Mixtec (YM) using a combination of ASR and text-based sequence-to-sequence tools, with the goal of improving efficiency while reducing the workload of a human annotator. We present two novel tokenization schemes that separate words in a nonlinear manner, preserving information about tonal morphology as much as possible. One of these approaches, a Segment and Melody tokenizer, simply extracts the tones without predicting segmentation. The other, a Sequence of Processes tokenizer, predicts segmentation for the words, which could allow an end-to-end ASR system to produce segmented and unsegmented transcriptions in a single pass. We find that these novel tokenizers are competitive with BPE and Unigram models, and the Segment-and-Melody model outperforms traditional tokenizers in terms of word error rate but does not reach the same character error rate. In addition, we analyze tokenizers on morphological and information-theoretic metrics to find predictive correlations with downstream performance. Our results suggest that nonlinear tokenizers designed specifically for the non-concatenative morphology of a language are competitive with conventional BPE and Unigram models for ASR. Further research will be necessary to determine the applicability of these tokenizers in downstream processing tasks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do You Feel Comfortable? Detecting Hidden Conversational Escalation in AI Chatbots</title>
<link>https://arxiv.org/abs/2512.06193</link>
<guid>https://arxiv.org/abs/2512.06193</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, implicit harm, affective drift, conversational escalation, GAUGE  

<br /><br />Summary:  
1. Large Language Models (LLMs) are increasingly used not only as information assistants but also as emotional companions in daily interactions.  
2. Despite lacking explicit toxicity, these models can cause gradual distress due to repeated emotional reinforcement or affective drift, a phenomenon termed "implicit harm."  
3. Current guardrail mechanisms often depend on external classifiers or clinical rubrics, which may not keep pace with the nuanced and real-time emotional dynamics occurring within a conversation.  
4. To address these limitations, the authors introduce GAUGE (Guarding Affective Utterance Generation Escalation), a lightweight framework designed for real-time detection of hidden conversational escalation.  
5. GAUGE operates on a logit-based approach, measuring how an LLM’s output probabilistically shifts the affective state within a dialogue to detect escalation early and mitigate implicit harm effectively. <div>
arXiv:2512.06193v1 Announce Type: new 
Abstract: Large Language Models (LLM) are increasingly integrated into everyday interactions, serving not only as information assistants but also as emotional companions. Even in the absence of explicit toxicity, repeated emotional reinforcement or affective drift can gradually escalate distress in a form of \textit{implicit harm} that traditional toxicity filters fail to detect. Existing guardrail mechanisms often rely on external classifiers or clinical rubrics that may lag behind the nuanced, real-time dynamics of a developing conversation. To address this gap, we propose GAUGE (Guarding Affective Utterance Generation Escalation), a lightweight, logit-based framework for the real-time detection of hidden conversational escalation. GAUGE measures how an LLM's output probabilistically shifts the affective state of a dialogue.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Data Enrichment using Confidence-Aware Fine-Grained Debate among Open-Source LLMs for Mental Health and Online Safety</title>
<link>https://arxiv.org/abs/2512.06227</link>
<guid>https://arxiv.org/abs/2512.06227</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM-based data enrichment, Confidence-Aware Fine-Grained Debate, mental health analysis, online safety, expert-annotated datasets<br /><br />Summary:<br /><br />This paper addresses the challenge of enriching NLP training datasets with real-world indicators, particularly in domains where labeling is costly and events are dynamic, such as life events for mental health and risky behavior for online safety. The authors propose and compare several large language model (LLM)-based data enrichment methods, introducing a novel Confidence-Aware Fine-Grained Debate (CFD) framework. In the CFD framework, multiple LLM agents act like human annotators, exchanging detailed evidence to collaboratively reach a consensus on annotations. To support evaluation, the paper presents two new expert-annotated datasets: a mental health Reddit wellbeing dataset and an online safety Facebook sharenting risk dataset. Experiments show that the CFD framework consistently outperforms a range of baseline methods, delivering the most robust enrichment results. Incorporating enriched features derived from debate transcripts into NLP models yields significant improvements in downstream tasks. Specifically, for the online safety task, the enriched feature approach improves performance by 10.1% over non-enriched baselines, demonstrating the effectiveness of this novel method for data enrichment in complex, real-world NLP tasks. <div>
arXiv:2512.06227v1 Announce Type: new 
Abstract: Real-world indicators are important for improving natural language processing (NLP) tasks such as life events for mental health analysis and risky behaviour for online safety, yet labelling such information in NLP training datasets is often costly and/or difficult given the dynamic nature of such events. This paper compares several LLM-based data enrichment methods and introduces a novel Confidence-Aware Fine-Grained Debate (CFD) framework in which multiple LLM agents simulate human annotators and exchange fine-grained evidence to reach consensus. We describe two new expert-annotated datasets, a mental health Reddit wellbeing dataset and an online safety Facebook sharenting risk dataset. Our CFD framework achieves the most robust data enrichment performance compared to a range of baselines and we show that this type of data enrichment consistently improves downstream tasks. Enriched features incorporated via debate transcripts yield the largest gains, outperforming the non-enriched baseline by 10.1% for the online safety task.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Policy-based Sentence Simplification: Replacing Parallel Corpora with LLM-as-a-Judge</title>
<link>https://arxiv.org/abs/2512.06228</link>
<guid>https://arxiv.org/abs/2512.06228</guid>
<content:encoded><![CDATA[
<div> Sentence Simplification, Policy-driven Control, Large Language Models, LLM-as-a-Judge, Lexical Simplification<br /><br />Summary:  
This paper addresses the challenge of sentence simplification, which involves modifying sentences to make them easier to read and understand without changing their original meaning. Different applications require distinct simplification policies, including lexical-level word replacement or rewriting entire sentences while balancing simplicity with detail retention. The authors propose a novel approach that uses Large Language Model-as-a-Judge (LLM-as-a-Judge) to generate training data aligned with specific simplification policies automatically. This method removes the dependency on expensive human annotations or parallel simplification corpora. The approach facilitates creating simplification systems adaptable to various policy requirements. Notably, experiments reveal that smaller open-source LLMs, such as Phi-3-mini-3.8B, outperform GPT-4o in lexical simplification tasks and show comparable results in overall sentence rewriting. These results are confirmed through both automatic evaluation metrics and human assessments. The improvements across different model sizes and families highlight the robustness and general applicability of the proposed method in building flexible and effective sentence simplification models. <div>
arXiv:2512.06228v1 Announce Type: new 
Abstract: Sentence simplification aims to modify a sentence to make it easier to read and understand while preserving the meaning. Different applications require distinct simplification policies, such as replacing only complex words at the lexical level or rewriting the entire sentence while trading off details for simplicity. However, achieving such policy-driven control remains an open challenge. In this work, we introduce a simple yet powerful approach that leverages Large Language Model-as-a-Judge (LLM-as-a-Judge) to automatically construct policy-aligned training data, completely removing the need for costly human annotation or parallel corpora. Our method enables building simplification systems that adapt to diverse simplification policies. Remarkably, even small-scale open-source LLMs such as Phi-3-mini-3.8B surpass GPT-4o on lexical-oriented simplification, while achieving comparable performance on overall rewriting, as verified by both automatic metrics and human evaluations. The consistent improvements across model families and sizes demonstrate the robustness of our approach.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LOCUS: A System and Method for Low-Cost Customization for Universal Specialization</title>
<link>https://arxiv.org/abs/2512.06239</link>
<guid>https://arxiv.org/abs/2512.06239</guid>
<content:encoded><![CDATA[
<div> Keywords: LOCUS, few-shot learning, named entity recognition, parameter-efficient tuning, synthetic data generation<br /><br />Summary:  
1. The paper introduces LOCUS (LOw-cost Customization for Universal Specialization), a pipeline designed to improve NLP model construction and training using few-shot data.  
2. LOCUS leverages a combination of targeted retrieval from a large data repository, synthetic data generation through in-context learning, and parameter-efficient tuning methods, including full fine-tuning and low-rank adaptation (LoRA).  
3. The pipeline is evaluated on named entity recognition (NER) and text classification (TC) benchmarks, consistently outperforming strong baselines, including those based on GPT-4o.  
4. LOCUS substantially reduces computational costs and model sizes, delivering memory-optimized models with only about 5% of the memory footprint of fully fine-tuned models while maintaining 99% of their accuracy.  
5. Remarkably, LOCUS models surpass GPT-4o performance on several benchmarks despite having less than 1% of its parameters, making LOCUS a highly efficient and effective solution for low-resource NLP model customization. <div>
arXiv:2512.06239v1 Announce Type: new 
Abstract: We present LOCUS (LOw-cost Customization for Universal Specialization), a pipeline that consumes few-shot data to streamline the construction and training of NLP models through targeted retrieval, synthetic data generation, and parameter-efficient tuning. With only a small number of labeled examples, LOCUS discovers pertinent data in a broad repository, synthesizes additional training samples via in-context data generation, and fine-tunes models using either full or low-rank (LoRA) parameter adaptation. Our approach targets named entity recognition (NER) and text classification (TC) benchmarks, consistently outperforming strong baselines (including GPT-4o) while substantially lowering costs and model sizes. Our resultant memory-optimized models retain 99% of fully fine-tuned accuracy while using barely 5% of the memory footprint, also beating GPT-4o on several benchmarks with less than 1% of its parameters.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Convergence of Outputs When Two Large Language Models Interact in a Multi-Agentic Setup</title>
<link>https://arxiv.org/abs/2512.06256</link>
<guid>https://arxiv.org/abs/2512.06256</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent setup, large language models, conversation convergence, repetition loops, lexical and embedding metrics  

<br /><br />Summary:  
This paper investigates the interaction dynamics when two large language models (Mistral Nemo Base 2407 and Llama 2 13B hf) engage in a multi-turn conversation without human input, starting from a short seed sentence. Initially, the conversations between the models proceed coherently, but as the dialogue progresses, it frequently falls into repetitive loops where short phrases are repeated across multiple turns. This repetition causes both models to converge in their outputs, producing highly similar or even identical text rather than introducing novel ideas or directions into the conversation. Notably, this convergence and repetition occur despite the models being large, independently trained, and operating without explicit prompt instructions. The study employs lexical and embedding-based metrics to quantitatively analyze how the conversation progressively drifts away from the original seed sentence and how the similarity between the responses of the two models increases over time. The findings highlight a fundamental limitation in multi-agent dialogue setups with large language models, emphasizing a tendency toward convergence and cyclical repetition rather than sustained, diversified interaction. This work provides insights into the dynamics of autonomous multi-model conversations and suggests areas for improving long-term interactive behaviors in AI agents. <div>
arXiv:2512.06256v1 Announce Type: new 
Abstract: In this work, we report what happens when two large language models respond to each other for many turns without any outside input in a multi-agent setup. The setup begins with a short seed sentence. After that, each model reads the other's output and generates a response. This continues for a fixed number of steps. We used Mistral Nemo Base 2407 and Llama 2 13B hf. We observed that most conversations start coherently but later fall into repetition. In many runs, a short phrase appears and repeats across turns. Once repetition begins, both models tend to produce similar output rather than introducing a new direction in the conversation. This leads to a loop where the same or similar text is produced repeatedly. We describe this behavior as a form of convergence. It occurs even though the models are large, trained separately, and not given any prompt instructions. To study this behavior, we apply lexical and embedding-based metrics to measure how far the conversation drifts from the initial seed and how similar the outputs of the two models becomes as the conversation progresses.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nanbeige4-3B Technical Report: Exploring the Frontier of Small Language Models</title>
<link>https://arxiv.org/abs/2512.06266</link>
<guid>https://arxiv.org/abs/2512.06266</guid>
<content:encoded><![CDATA[
<div> Keywords: Nanbeige4-3B, Fine-Grained Warmup-Stable-Decay, Dual Preference Distillation, instruction finetuning, reinforcement learning<br /><br />Summary: The paper introduces Nanbeige4-3B, a family of small-scale language models that achieve high performance despite their compact size. The models are pretrained on a large corpus of 23 trillion high-quality tokens and subsequently finetuned on over 30 million diverse instructions, pushing the limits of scaling laws for small language models. A novel Fine-Grained Warmup-Stable-Decay (FG-WSD) training scheduler is proposed for pretraining, which dynamically refines data mixtures across stages to enhance training effectiveness. To improve supervised finetuning (SFT) data quality, the authors develop a joint mechanism combining deliberative generation refinement with chain-of-thought reconstruction, delivering noticeable improvements on complex tasks. Additionally, a flagship reasoning model is used to distill Nanbeige4-3B via a Dual Preference Distillation (DPD) method, further boosting performance. The training pipeline concludes with a multi-stage reinforcement learning phase that incorporates verifiable rewards and preference modeling to enhance both reasoning ability and alignment with human preferences. Extensive evaluations demonstrate that Nanbeige4-3B outperforms comparable-sized models and competes closely with much larger models across various benchmarks. Model checkpoints are publicly available on Hugging Face for community access. <div>
arXiv:2512.06266v1 Announce Type: new 
Abstract: We present Nanbeige4-3B, a family of small-scale but high-performing language models. Pretrained on 23T high-quality tokens and finetuned on over 30 million diverse instructions, we extend the boundary of the scaling law for small language models. In pre-training, we design a Fine-Grained Warmup-Stable-Decay (FG-WSD) training scheduler, which progressively refines data mixtures across stages to boost model performance. In post-training, to improve the quality of the SFT data, we design a joint mechanism that integrates deliberative generation refinement and chain-of-thought reconstruction, yielding substantial gains on complex tasks. Following SFT, we employ our flagship reasoning model to distill Nanbeige4-3B through our proposed Dual Preference Distillation (DPD) method, which leads to further performance gains. Finally, a multi-stage reinforcement learning phase was applied, leveraging verifiable rewards and preference modeling to strengthen abilities on both reasoning and human alignment. Extensive evaluations show that Nanbeige4-3B not only significantly outperforms models of comparable parameter scale but also rivals much larger models across a wide range of benchmarks. The model checkpoints are available at https://huggingface.co/Nanbeige.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modeling Contextual Passage Utility for Multihop Question Answering</title>
<link>https://arxiv.org/abs/2512.06464</link>
<guid>https://arxiv.org/abs/2512.06464</guid>
<content:encoded><![CDATA[
<div> Keywords: Multihop Question Answering, passage utility, contextual modeling, transformer, reranking<br /><br />Summary:<br /><br />1. The paper addresses the challenge of Multihop Question Answering (QA), which involves synthesizing information from multiple text passages to answer complex questions.<br />2. Traditional retrieval methods focus on identifying relevant passages but fail to assess their utility, leading to redundant or noisy information that can degrade answer quality.<br />3. The authors note the importance of context-dependent utility, where the usefulness of a passage depends on its relationship with other passages, such as providing complementary information or forming crucial reasoning links.<br />4. They propose a lightweight method using a small transformer-based model fine-tuned to predict passage utility scores that consider inter-passage dependencies for multihop QA.<br />5. The approach utilizes reasoning traces from an advanced reasoning model to capture the sequence in which passages contribute to answering questions, enabling the creation of synthetic training data.<br />6. Experimental results show that reranking retrieved passages based on predicted utility significantly improves both the ranking quality and the downstream QA performance compared to reranking based solely on relevance scores. <div>
arXiv:2512.06464v1 Announce Type: new 
Abstract: Multihop Question Answering (QA) requires systems to identify and synthesize information from multiple text passages. While most prior retrieval methods assist in identifying relevant passages for QA, further assessing the utility of the passages can help in removing redundant ones, which may otherwise add to noise and inaccuracies in the generated answers. Existing utility prediction approaches model passage utility independently, overlooking a critical aspect of multihop reasoning: the utility of a passage can be context-dependent, influenced by its relation to other passages - whether it provides complementary information or forms a crucial link in conjunction with others. In this paper, we propose a lightweight approach to model contextual passage utility, accounting for inter-passage dependencies. We fine-tune a small transformer-based model to predict passage utility scores for multihop QA. We leverage the reasoning traces from an advanced reasoning model to capture the order in which passages are used to answer a question and obtain synthetic training data. Through comprehensive experiments, we demonstrate that our utility-based scoring of retrieved passages leads to improved reranking and downstream QA performance compared to relevance-based reranking methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowing What's Missing: Assessing Information Sufficiency in Question Answering</title>
<link>https://arxiv.org/abs/2512.06476</link>
<guid>https://arxiv.org/abs/2512.06476</guid>
<content:encoded><![CDATA[
<div> Keywords: question-answering, sufficiency modeling, reasoning, Identify-then-Verify, multi-hop QA<br /><br />Summary:<br />1. The paper addresses the challenge of determining if a given context contains enough information to accurately answer a question, a key issue for reliable question-answering (QA) systems.<br />2. While simple prompting methods work well for factual questions, they struggle with inferential questions that require reasoning beyond straightforward text extraction.<br />3. The authors propose an Identify-then-Verify framework that first prompts the model to generate multiple hypotheses about what information might be missing, aiming to create a semantic consensus on the gaps.<br />4. Following this, the model critically verifies these hypotheses by re-examining the source text to confirm whether the identified information is truly absent.<br />5. Evaluation on various multi-hop and factual QA datasets shows that this approach improves the accuracy of sufficiency judgments and helps the model clearly articulate specific information gaps, leading to more robust and interpretable QA performance. <div>
arXiv:2512.06476v1 Announce Type: new 
Abstract: Determining whether a provided context contains sufficient information to answer a question is a critical challenge for building reliable question-answering systems. While simple prompting strategies have shown success on factual questions, they frequently fail on inferential ones that require reasoning beyond direct text extraction. We hypothesize that asking a model to first reason about what specific information is missing provides a more reliable, implicit signal for assessing overall sufficiency. To this end, we propose a structured Identify-then-Verify framework for robust sufficiency modeling. Our method first generates multiple hypotheses about missing information and establishes a semantic consensus. It then performs a critical verification step, forcing the model to re-examine the source text to confirm whether this information is truly absent. We evaluate our method against established baselines across diverse multi-hop and factual QA datasets. The results demonstrate that by guiding the model to justify its claims about missing information, our framework produces more accurate sufficiency judgments while clearly articulating any information gaps.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Classifying German Language Proficiency Levels Using Large Language Models</title>
<link>https://arxiv.org/abs/2512.06483</link>
<guid>https://arxiv.org/abs/2512.06483</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, CEFR classification, German texts, prompt engineering, fine-tuning

<br /><br />Summary:  
This paper addresses the automatic classification of German language proficiency according to the Common European Framework of Reference for Languages (CEFR), a crucial task for personalized education. To achieve this, the authors create a comprehensive dataset by merging various existing CEFR-annotated corpora with synthetic data, ensuring diverse and robust training and evaluation material. They explore multiple strategies including prompt-engineering techniques, fine-tuning a LLaMA-3-8B-Instruct model, and employing a probing-based method that leverages the internal neural states of the Large Language Model for classification. The experiments consistently demonstrate that these approaches outperform previous methods, suggesting that LLMs can effectively enhance the accuracy and scalability of CEFR level classification. The study highlights the practical implications of using advanced language models to support educational needs by providing reliable assessments of learner proficiency. <div>
arXiv:2512.06483v1 Announce Type: new 
Abstract: Assessing language proficiency is essential for education, as it enables instruction tailored to learners needs. This paper investigates the use of Large Language Models (LLMs) for automatically classifying German texts according to the Common European Framework of Reference for Languages (CEFR) into different proficiency levels. To support robust training and evaluation, we construct a diverse dataset by combining multiple existing CEFR-annotated corpora with synthetic data. We then evaluate prompt-engineering strategies, fine-tuning of a LLaMA-3-8B-Instruct model and a probing-based approach that utilizes the internal neural state of the LLM for classification. Our results show a consistent performance improvement over prior methods, highlighting the potential of LLMs for reliable and scalable CEFR classification.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProSocialAlign: Preference Conditioned Test Time Alignment in Language Models</title>
<link>https://arxiv.org/abs/2512.06515</link>
<guid>https://arxiv.org/abs/2512.06515</guid>
<content:encoded><![CDATA[
<div> Keywords: language model safety, prosocial alignment, harm mitigation, constrained generation, preference-aware reward modeling<br /><br />Summary:<br /><br />1. The paper addresses limitations in current language model safety techniques, especially in emotionally charged or high-stakes scenarios where simple refusal or naive compliance can be counterproductive.<br /><br />2. It introduces ProSocialAlign, a test-time, parameter-efficient framework that guides the generation of responses to be safe, empathetic, and aligned with human values without the need to retrain the underlying base model.<br /><br />3. The approach formalizes five human-centered objectives and models safety as a lexicographic constrained generation problem, where hard constraints first remove harmful continuations before optimizing prosocial qualities within the safe candidate set.<br /><br />4. ProSocialAlign incorporates two key mechanisms: directional regulation, which mitigates harm by subtracting a learned "harm vector" in model parameter space, and preference-aware autoregressive reward modeling trained jointly across multiple attributes with gradient conflict resolution, allowing fine-grained, user-controllable decoding.<br /><br />5. Extensive empirical evaluation on five safety benchmarks shows that ProSocialAlign achieves state-of-the-art performance by significantly reducing unsafe content leakage and enhancing alignment with human values, offering a modular and robust foundation for generating context-sensitive and safe responses at inference time. <div>
arXiv:2512.06515v1 Announce Type: new 
Abstract: Current language model safety paradigms often fall short in emotionally charged or high-stakes settings, where refusal-only approaches may alienate users and naive compliance can amplify risk. We propose ProSocialAlign, a test-time, parameter-efficient framework that steers generation toward safe, empathetic, and value-aligned responses without retraining the base model. We formalize five human-centered objectives and cast safety as lexicographic constrained generation: first, applying hard constraints to eliminate harmful continuations; then optimizing for prosocial quality within the safe set. Our method combines (i) directional regulation, a harm-mitigation mechanism that subtracts a learned "harm vector" in parameter space, and (ii) preference-aware autoregressive reward modeling trained jointly across attributes with gradient conflict resolution, enabling fine-grained, user-controllable decoding. Empirical evaluations across five safety benchmarks demonstrate state-of-the-art performance, reducing unsafe leakage and boosting alignment to human values, with strong gains across multiple evaluation metrics. ProSocialAlign offers a robust and modular foundation for generating context-sensitive, safe, and human-aligned responses at inference time.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adapting AlignScore Mertic for Factual Consistency Evaluation of Text in Russian: A Student Abstract</title>
<link>https://arxiv.org/abs/2512.06586</link>
<guid>https://arxiv.org/abs/2512.06586</guid>
<content:encoded><![CDATA[
<div> Keywords: factual consistency, Russian NLP, AlignRuScore, RuBERT, multilingual evaluation<br /><br />Summary:<br /><br />1. The paper addresses the challenge of ensuring factual consistency in generated text, which is vital for trustworthy natural language processing (NLP) applications.<br />2. Existing factual consistency evaluation tools predominantly focus on English texts, leaving a gap in evaluation resources for Russian language processing.<br />3. To overcome this gap, the authors introduce AlignRuScore, an adaptation of the AlignScore metric tailored specifically for Russian.<br />4. The adaptation process involves fine-tuning a RuBERT-based alignment model enhanced with task-specific classification and regression heads using both Russian datasets and translated English datasets.<br />5. Experimental results indicate that this unified alignment metric can be effectively transferred to Russian, facilitating reliable multilingual factual consistency evaluation.<br />6. To encourage further research and development, the authors release the translated corpora, model checkpoints, and related code publicly. <div>
arXiv:2512.06586v1 Announce Type: new 
Abstract: Ensuring factual consistency in generated text is crucial for reliable natural language processing applications. However, there is a lack of evaluation tools for factual consistency in Russian texts, as existing tools primarily focus on English corpora. To bridge this gap, we introduce AlignRuScore, a comprehensive adaptation of the AlignScore metric for Russian. To adapt the metric, we fine-tuned a RuBERT-based alignment model with task-specific classification and regression heads on Russian and translated English datasets. Our results demonstrate that a unified alignment metric can be successfully ported to Russian, laying the groundwork for robust multilingual factual consistency evaluation. We release the translated corpora, model checkpoints, and code to support further research.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Online Discourse of Virtual Reality and Anxiety</title>
<link>https://arxiv.org/abs/2512.06656</link>
<guid>https://arxiv.org/abs/2512.06656</guid>
<content:encoded><![CDATA[
<div> anxiety disorders, corpus linguistics, Sketch Engine, virtual reality, VR<br /><br />Summary:<br /><br />1. This study explores the role of virtual reality (VR) in treating clinical issues such as generalized anxiety disorder and social anxiety.<br />2. It emphasizes the potential of VR technology to enhance patient well-being and therapeutic support.<br />3. The research uses a corpus linguistic methodology, analyzing online discussions to understand user perceptions of VR related to anxiety.<br />4. Data was processed with Sketch Engine software, focusing on frequently used words and their collocations within the English Trends corpus.<br />5. Key frequently discussed terms included "VR," "Oculus," and "headset," highlighting both virtual systems and physical devices central to the VR experience.<br />6. Common collocations involved prepositional phrases like "of virtual reality," "in virtual reality," and "for virtual reality," which relate to aspects of the design, user experience, and development of VR technology.<br />7. The findings provide insights into how VR and anxiety are discussed in public discourse and suggest opportunities to further leverage VR for counseling and mental health support by enhancing development and accessibility.<br /> <div>
arXiv:2512.06656v1 Announce Type: new 
Abstract: VR in the treatment of clinical concerns such as generalized anxiety disorder or social anxiety. VR has created additional pathways to support patient well-being and care. Understanding online discussion of what users think about this technology may further support its efficacy. The purpose of this study was to employ a corpus linguistic methodology to identify the words and word networks that shed light on the online discussion of virtual reality and anxiety. Using corpus linguistics, frequently used words in discussion along with collocation were identified by utilizing Sketch Engine software. The results of the study, based upon the English Trends corpus, identified VR, Oculus, and headset as the most frequently discussed within the VR and anxiety subcorpus. These results point to the development of the virtual system, along with the physical apparatus that makes viewing and engaging with the virtual environment possible. Additional results point to collocation of prepositional phrases such as of virtual reality, in virtual reality, and for virtual reality relating to the design, experience, and development, respectively. These findings offer new perspective on how VR and anxiety together are discussed in general discourse and offer pathways for future opportunities to support counseling needs through development and accessibility. Keywords: anxiety disorders, corpus linguistics, Sketch Engine, and virtual reality VR
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CMV-Fuse: Cross Modal-View Fusion of AMR, Syntax, and Knowledge Representations for Aspect Based Sentiment Analysis</title>
<link>https://arxiv.org/abs/2512.06679</link>
<guid>https://arxiv.org/abs/2512.06679</guid>
<content:encoded><![CDATA[
<div> Keywords: Aspect-Based Sentiment Analysis, Cross-Modal Fusion, Abstract Meaning Representations, Multi-view Contrastive Learning, Hierarchical Gated Attention  

<br /><br />Summary:  
The paper addresses the limitations of current Aspect-Based Sentiment Analysis (ABSA) systems that typically operate on isolated linguistic perspectives, missing the complex interactions among various syntactic and semantic structures involved in human language understanding. To overcome this, the authors propose CMV-Fuse, a novel Cross-Modal View fusion framework designed to integrate four complementary linguistic views: Abstract Meaning Representations (AMR), constituency parsing, dependency syntax, and semantic attention, further enhanced by external knowledge integration. CMV-Fuse employs a hierarchical gated attention mechanism to fuse information across local syntactic structures, intermediate semantic representations, and global world knowledge levels, thereby capturing fine-grained structural details and broad contextual insights. A key innovation is the structure-aware multi-view contrastive learning strategy, which enforces consistency across these multiple linguistic views while maintaining computational efficiency. The framework is extensively evaluated on standard ABSA benchmarks, demonstrating significant improvements over strong baseline models. The analysis further reveals the individual and combined contributions of each linguistic perspective, highlighting the robustness and effectiveness of the CMV-Fuse approach for improved sentiment analysis. <div>
arXiv:2512.06679v1 Announce Type: new 
Abstract: Natural language understanding inherently depends on integrating multiple complementary perspectives spanning from surface syntax to deep semantics and world knowledge. However, current Aspect-Based Sentiment Analysis (ABSA) systems typically exploit isolated linguistic views, thereby overlooking the intricate interplay between structural representations that humans naturally leverage. We propose CMV-Fuse, a Cross-Modal View fusion framework that emulates human language processing by systematically combining multiple linguistic perspectives. Our approach systematically orchestrates four linguistic perspectives: Abstract Meaning Representations, constituency parsing, dependency syntax, and semantic attention, enhanced with external knowledge integration. Through hierarchical gated attention fusion across local syntactic, intermediate semantic, and global knowledge levels, CMV-Fuse captures both fine-grained structural patterns and broad contextual understanding. A novel structure aware multi-view contrastive learning mechanism ensures consistency across complementary representations while maintaining computational efficiency. Extensive experiments demonstrate substantial improvements over strong baselines on standard benchmarks, with analysis revealing how each linguistic view contributes to more robust sentiment analysis.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mechanistic Interpretability of GPT-2: Lexical and Contextual Layers in Sentiment Analysis</title>
<link>https://arxiv.org/abs/2512.06681</link>
<guid>https://arxiv.org/abs/2512.06681</guid>
<content:encoded><![CDATA[
<div> Keywords: mechanistic interpretability, GPT-2, sentiment analysis, activation patching, contextual integration  

<br /><br />Summary:  
This study investigates how GPT-2 processes sentiment information across its transformer layers using mechanistic interpretability techniques. The researchers employ systematic activation patching across all 12 model layers to explore the proposed two-stage sentiment processing architecture, which suggests early lexical detection and mid-layer contextual integration. Their experiments confirm that early layers (0-3) function as lexical sentiment detectors, encoding stable and position-specific polarity signals that largely ignore context, supporting the first stage of the hypothesis. However, the study falsifies all three proposed hypotheses about mid-layer contextual integration — Middle Layer Concentration, Phenomenon Specificity, and Distributed Processing — showing no evidence of specialized mid-layer sentiment processing. Instead, the contextual integration of phenomena such as negation, sarcasm, and domain shifts primarily occurs in the late layers (8-11) through a unified, non-modular mechanism. These results reveal that GPT-2’s sentiment computation deviates from the expected hierarchical pattern of processing, suggesting that contextual sentiment understanding is consolidated later in the network. The findings emphasize the importance of further empirical studies to accurately characterize how large language models integrate contextual cues during sentiment analysis. <div>
arXiv:2512.06681v1 Announce Type: new 
Abstract: We present a mechanistic interpretability study of GPT-2 that causally examines how sentiment information is processed across its transformer layers. Using systematic activation patching across all 12 layers, we test the hypothesized two-stage sentiment architecture comprising early lexical detection and mid-layer contextual integration. Our experiments confirm that early layers (0-3) act as lexical sentiment detectors, encoding stable, position specific polarity signals that are largely independent of context. However, all three contextual integration hypotheses: Middle Layer Concentration, Phenomenon Specificity, and Distributed Processing are falsified. Instead of mid-layer specialization, we find that contextual phenomena such as negation, sarcasm, domain shifts etc. are integrated primarily in late layers (8-11) through a unified, non-modular mechanism. These experimental findings provide causal evidence that GPT-2's sentiment computation differs from the predicted hierarchical pattern, highlighting the need for further empirical characterization of contextual integration in large language models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PersonaMem-v2: Towards Personalized Intelligence via Learning Implicit User Personas and Agentic Memory</title>
<link>https://arxiv.org/abs/2512.06688</link>
<guid>https://arxiv.org/abs/2512.06688</guid>
<content:encoded><![CDATA[
<div> Keywords: Personalization, large language models, reinforcement fine-tuning, agentic memory, long-context reasoning<br /><br />Summary: This paper introduces PersonaMem-v2, an advanced dataset designed for personalization in large language models (LLMs). It simulates 1,000 realistic user-chatbot interactions across over 300 scenarios, incorporating 20,000+ user preferences and supporting long-context windows up to 128k tokens. Most user preferences are implicitly revealed to closely mirror real-world interaction dynamics. The study explores how reinforcement fine-tuning improves models’ long-context reasoning abilities, crucial for understanding and personalizing user experiences. Additionally, the authors propose an agentic memory system framework that maintains a human-readable memory for each user, which expands over time. Experiments reveal that current state-of-the-art LLMs struggle with implicit personalization, achieving only 37-48% accuracy despite supporting long contexts. By applying reinforcement fine-tuning, Qwen3-4B surpasses GPT-5, reaching 53% accuracy on implicit personalization tasks. The agentic memory framework further advances performance to 55% accuracy while drastically reducing the token input size by 16 times, using only a 2k-token memory instead of full 32k-token conversation histories. These results emphasize the value of the PersonaMem-v2 dataset and demonstrate that agentic memory offers a scalable approach toward achieving real-world personalized AI capabilities. <div>
arXiv:2512.06688v1 Announce Type: new 
Abstract: Personalization is one of the next milestones in advancing AI capability and alignment. We introduce PersonaMem-v2, the state-of-the-art dataset for LLM personalization that simulates 1,000 realistic user-chatbot interactions on 300+ scenarios, 20,000+ user preferences, and 128k-token context windows, where most user preferences are implicitly revealed to reflect real-world interactions. Using this data, we investigate how reinforcement fine-tuning enables a model to improve its long-context reasoning capabilities for user understanding and personalization. We also develop a framework for training an agentic memory system, which maintains a single, human-readable memory that grows with each user over time.
  In our experiments, frontier LLMs still struggle with implicit personalization, achieving only 37-48% accuracy. While they support long context windows, reasoning remains the bottleneck for implicit personalization tasks. Using reinforcement fine-tuning, we successfully train Qwen3-4B to outperforms GPT-5, reaching 53% accuracy in implicit personalization. Moreover, our agentic memory framework achieves state-of-the-art 55% accuracy while using 16x fewer input tokens, relying on a 2k-token memory instead of full 32k conversation histories. These results underscore the impact of our dataset and demonstrate agentic memory as a scalable path toward real-world personalized intelligence.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Think-While-Generating: On-the-Fly Reasoning for Personalized Long-Form Generation</title>
<link>https://arxiv.org/abs/2512.06690</link>
<guid>https://arxiv.org/abs/2512.06690</guid>
<content:encoded><![CDATA[
<div> Preference Alignment, Personalization, Long-Form Generation, Think-While-Generating, Training Efficiency<br /><br />Summary:<br /><br />1. The paper addresses the limitation of existing large language model (LLM) methods which optimize for population-level preferences but fail to capture individual user preferences, crucial for true personalization.<br />2. Early personalization methods like prompt customization or fine-tuning struggle to reason over implicit user preferences, hence are less effective in real-world applications.<br />3. Recent "think-then-generate" methods improve reasoning by performing it before response generation but encounter difficulties in long-form generation due to static one-shot reasoning that must encompass all relevant information at once, leading to learning challenges and poor adaptability.<br />4. The authors propose FlyThinker, a novel framework that implements a "think-while-generating" approach by using a separate reasoning model to create latent token-level reasoning in parallel with generation, which is dynamically fused into the generation process.<br />5. FlyThinker's reasoning model depends only on previous response tokens (not its own prior outputs), enabling parallel training akin to standard LLM training for efficiency, and allowing reasoning and generation to run concurrently during inference.<br />6. Experiments on real-world benchmarks show FlyThinker improves personalized long-form generation while maintaining both training and inference efficiency. <div>
arXiv:2512.06690v1 Announce Type: new 
Abstract: Preference alignment has enabled large language models (LLMs) to better reflect human expectations, but current methods mostly optimize for population-level preferences, overlooking individual users. Personalization is essential, yet early approaches-such as prompt customization or fine-tuning-struggle to reason over implicit preferences, limiting real-world effectiveness. Recent "think-then-generate" methods address this by reasoning before response generation. However, they face challenges in long-form generation: their static one-shot reasoning must capture all relevant information for the full response generation, making learning difficult and limiting adaptability to evolving content. To address this issue, we propose FlyThinker, an efficient "think-while-generating" framework for personalized long-form generation. FlyThinker employs a separate reasoning model that generates latent token-level reasoning in parallel, which is fused into the generation model to dynamically guide response generation. This design enables reasoning and generation to run concurrently, ensuring inference efficiency. In addition, the reasoning model is designed to depend only on previous responses rather than its own prior outputs, which preserves training parallelism across different positions-allowing all reasoning tokens for training data to be produced in a single forward pass like standard LLM training, ensuring training efficiency. Extensive experiments on real-world benchmarks demonstrate that FlyThinker achieves better personalized generation while keeping training and inference efficiency.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TopiCLEAR: Topic extraction by CLustering Embeddings with Adaptive dimensional Reduction</title>
<link>https://arxiv.org/abs/2512.06694</link>
<guid>https://arxiv.org/abs/2512.06694</guid>
<content:encoded><![CDATA[
<div> Keywords: Topic modeling, social media, Sentence-BERT, Gaussian Mixture Models, dimensionality reduction  

<br /><br />Summary:  
The rapid growth of social media platforms like X (formerly Twitter), Facebook, and Reddit has created opportunities for large-scale text analysis across various domains, including social issues and consumer sentiment. Traditional topic modeling methods, tailored for longer and more formal documents, often perform poorly on short social media posts due to limited co-occurrence data, inconsistent language use, and fragmented semantics. To overcome these limitations, the authors propose TopiCLEAR, a novel topic extraction method that combines embeddings generated by Sentence-BERT with clustering via Gaussian Mixture Models (GMM). TopiCLEAR iteratively refines clusters through supervised projection using linear discriminant analysis followed by GMM clustering until convergence, enabling adaptive dimensionality reduction. Importantly, the method bypasses preprocessing steps such as stop word removal, working directly on raw textual input. The approach is evaluated on four datasets: 20News, AgNewsTitle, Reddit, and TweetTopic, all containing human-labeled topic annotations. Compared to seven baseline methods—including a recent SBERT-based model and a zero-shot generative AI technique—TopiCLEAR achieves the highest correspondence with human-annotated topics, showing notable improvements for both social media posts and online news. Qualitative analysis confirms that TopiCLEAR generates more interpretable topics, indicating its potential for enhanced social media and web content analytics. <div>
arXiv:2512.06694v1 Announce Type: new 
Abstract: Rapid expansion of social media platforms such as X (formerly Twitter), Facebook, and Reddit has enabled large-scale analysis of public perceptions on diverse topics, including social issues, politics, natural disasters, and consumer sentiment. Topic modeling is a widely used approach for uncovering latent themes in text data, typically framed as an unsupervised classification task. However, traditional models, originally designed for longer and more formal documents, struggle with short social media posts due to limited co-occurrence statistics, fragmented semantics, inconsistent spelling, and informal language. To address these challenges, we propose a new method, TopiCLEAR: Topic extraction by CLustering Embeddings with Adaptive dimensional Reduction. Specifically, each text is embedded using Sentence-BERT (SBERT) and provisionally clustered using Gaussian Mixture Models (GMM). The clusters are then refined iteratively using a supervised projection based on linear discriminant analysis, followed by GMM-based clustering until convergence. Notably, our method operates directly on raw text, eliminating the need for preprocessing steps such as stop word removal. We evaluate our approach on four diverse datasets, 20News, AgNewsTitle, Reddit, and TweetTopic, each containing human-labeled topic information. Compared with seven baseline methods, including a recent SBERT-based method and a zero-shot generative AI method, our approach achieves the highest similarity to human-annotated topics, with significant improvements for both social media posts and online news articles. Additionally, qualitative analysis shows that our method produces more interpretable topics, highlighting its potential for applications in social media data and web content analytics.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parameter-Efficient Fine-Tuning with Differential Privacy for Robust Instruction Adaptation in Large Language Models</title>
<link>https://arxiv.org/abs/2512.06711</link>
<guid>https://arxiv.org/abs/2512.06711</guid>
<content:encoded><![CDATA[
<div> Keywords: privacy protection, differential privacy, parameter-efficient fine-tuning, gradient clipping, language models<br /><br />Summary: This study presents a novel method to enhance privacy protection and efficiency in instruction fine-tuning of large-scale language models by combining differential privacy noise allocation with gradient clipping within a collaborative optimization framework. The approach keeps the backbone model frozen and updates only a low-dimensional projection subspace, significantly reducing the number of parameters that require tuning. By introducing adaptive noise allocation and clipping techniques during gradient computation, the method better manages the privacy budget and maintains stability and robustness in training. The unified framework effectively integrates gradient constraints, noise allocation, and parameter projection to minimize performance fluctuations and privacy vulnerabilities, especially in multi-task instruction scenarios. Extensive experiments evaluate the method across various hyperparameter settings, environments, and data sensitivity levels, demonstrating consistent improvements over baseline models in accuracy, privacy budget efficiency, and parameter usage. The approach also sustains stable performance despite uncertainties and variability in the training data. The work advances the theoretical integration of differential privacy with parameter-efficient fine-tuning and offers practical adaptability for secure and efficient training of language models in complex instruction-driven tasks. This provides a promising and feasible solution for privacy-preserving training in challenging multi-task environments. <div>
arXiv:2512.06711v1 Announce Type: new 
Abstract: This study addresses the issues of privacy protection and efficiency in instruction fine-tuning of large-scale language models by proposing a parameter-efficient method that integrates differential privacy noise allocation with gradient clipping in a collaborative optimization framework. The method keeps the backbone model frozen and updates parameters through a low-dimensional projection subspace, while introducing clipping and adaptive noise allocation during gradient computation. This design reduces privacy budget consumption and ensures training stability and robustness. The unified framework combines gradient constraints, noise allocation, and parameter projection, effectively mitigating performance fluctuations and privacy risks in multi-task instruction scenarios. Experiments are conducted across hyperparameter, environment, and data sensitivity dimensions. Results show that the method outperforms baseline models in accuracy, privacy budget, and parameter efficiency, and maintains stable performance under diverse and uncertain data conditions. The findings enrich the theoretical integration of differential privacy and parameter-efficient fine-tuning and demonstrate its practical adaptability in instruction tasks, providing a feasible solution for secure training in complex instruction environments.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>"The Dentist is an involved parent, the bartender is not": Revealing Implicit Biases in QA with Implicit BBQ</title>
<link>https://arxiv.org/abs/2512.06732</link>
<guid>https://arxiv.org/abs/2512.06732</guid>
<content:encoded><![CDATA[
<div> Implicit Bias, Large Language Models, Fairness Evaluation, ImplicitBBQ, GPT-4o<br /><br />Summary:<br /><br />1. Existing benchmarks for evaluating biases in large language models (LLMs) typically depend on explicit identification of protected attributes such as religion, race, or gender. 2. These explicit methods overlook implicit biases that emerge through subtle cues such as names and cultural traits commonly seen in real-world interactions. 3. To address this gap, the authors introduce ImplicitBBQ, a new benchmark that extends the Bias Benchmark for QA (BBQ) by incorporating implicitly cued protected attributes across six categories. 4. The evaluation of GPT-4o using ImplicitBBQ reveals significant disparities in model performance, with accuracy dropping as much as 7% in the "sexual orientation" subcategory and showing consistent declines in most other categories compared to explicit BBQ prompts. 5. These results uncover the presence of implicit biases in current LLMs that remain undetected by conventional explicit bias benchmarks, emphasizing the importance of ImplicitBBQ as a tool for more nuanced and comprehensive fairness evaluation in natural language processing (NLP). <div>
arXiv:2512.06732v1 Announce Type: new 
Abstract: Existing benchmarks evaluating biases in large language models (LLMs) primarily rely on explicit cues, declaring protected attributes like religion, race, gender by name. However, real-world interactions often contain implicit biases, inferred subtly through names, cultural cues, or traits. This critical oversight creates a significant blind spot in fairness evaluation. We introduce ImplicitBBQ, a benchmark extending the Bias Benchmark for QA (BBQ) with implicitly cued protected attributes across 6 categories. Our evaluation of GPT-4o on ImplicitBBQ illustrates troubling performance disparity from explicit BBQ prompts, with accuracy declining up to 7% in the "sexual orientation" subcategory and consistent decline located across most other categories. This indicates that current LLMs contain implicit biases undetected by explicit benchmarks. ImplicitBBQ offers a crucial tool for nuanced fairness evaluation in NLP.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Patient-Doctor-NLP-System to contest inequality for less privileged</title>
<link>https://arxiv.org/abs/2512.06734</link>
<guid>https://arxiv.org/abs/2512.06734</guid>
<content:encoded><![CDATA[
<div> Transfer Learning, Large Language Models, Low-resource languages, Medical NLP, Model Distillation  

<br /><br />Summary:  
This paper addresses the challenges of deploying large language models (LLMs) in resource-limited healthcare settings, focusing on aiding visually impaired users and Hindi speakers in rural areas. The authors introduce PDFTEMRA, a compact transformer-based model designed to be computationally efficient while maintaining strong language understanding capabilities. PDFTEMRA achieves this efficiency by integrating model distillation, frequency-domain modulation, ensemble learning, and randomized activation patterns. The model is specifically trained and evaluated on medical question-answering and consultation datasets relevant to Hindi language and accessibility needs. Comparative experiments with current state-of-the-art NLP models demonstrate that PDFTEMRA delivers comparable performance levels but with significantly reduced computational costs. This makes it a suitable solution for accessible and inclusive medical NLP applications in low-resource environments, providing necessary support where large-scale models are impractical. The study highlights the potential of model compression and architectural innovations to broaden the usability of advanced language models in real-world, under-resourced healthcare contexts. <div>
arXiv:2512.06734v1 Announce Type: new 
Abstract: Transfer Learning (TL) has accelerated the rapid development and availability of large language models (LLMs) for mainstream natural language processing (NLP) use cases. However, training and deploying such gigantic LLMs in resource-constrained, real-world healthcare situations remains challenging. This study addresses the limited support available to visually impaired users and speakers of low-resource languages such as Hindi who require medical assistance in rural environments. We propose PDFTEMRA (Performant Distilled Frequency Transformer Ensemble Model with Random Activations), a compact transformer-based architecture that integrates model distillation, frequency-domain modulation, ensemble learning, and randomized activation patterns to reduce computational cost while preserving language understanding performance. The model is trained and evaluated on medical question-answering and consultation datasets tailored to Hindi and accessibility scenarios, and its performance is compared against standard NLP state-of-the-art model baselines. Results demonstrate that PDFTEMRA achieves comparable performance with substantially lower computational requirements, indicating its suitability for accessible, inclusive, low-resource medical NLP applications.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One Word Is Not Enough: Simple Prompts Improve Word Embeddings</title>
<link>https://arxiv.org/abs/2512.06744</link>
<guid>https://arxiv.org/abs/2512.06744</guid>
<content:encoded><![CDATA[
<div> Keywords: text embedding models, word similarity, semantic prompts, zero-shot technique, benchmarks<br /><br />Summary:<br /><br />1. Text embedding models are primarily designed and evaluated for sentence-level tasks such as retrieval and semantic similarity, with less understanding of their performance on isolated words.<br />2. The study demonstrates that prepending semantic prompts to individual words before embedding significantly enhances word similarity correlations.<br />3. Seven state-of-the-art text embedding models including OpenAI’s text-embedding-3-large, Cohere’s embed-english-v3.0, and others were tested on three standard word similarity benchmarks: SimLex-999, WordSim-353, and MEN-3000.<br />4. Semantic prompts like "meaning: {word}" or "Represent the semantic concept: {word}" improve Spearman correlation scores by up to +0.29 on SimLex-999, rescuing models that otherwise show zero correlation on bare words.<br />5. The best performing models in zero-shot settings achieved correlations of 0.692 (Cohere) on SimLex-999, 0.811 on WordSim-353, and 0.855 on MEN-3000 (OpenAI), surpassing classic static embeddings like Word2Vec and LexVec.<br />6. This zero-shot prompting technique requires no additional training and is model-agnostic, establishing a new state-of-the-art for word-level embedding quality using pure embedding methods. <div>
arXiv:2512.06744v1 Announce Type: new 
Abstract: Text embedding models are designed for sentence-level applications like retrieval and semantic similarity, and are primarily evaluated on sentence-level benchmarks. Their behavior on isolated words is less understood. We show that simply prepending semantic prompts to words before embedding substantially improves word similarity correlations. Testing 7 text embedding models, including text-embedding-3-large (OpenAI), embed-english-v3.0 (Cohere), voyage-3(Voyage AI), all-mpnet-base-v2, and Qwen3-Embedding-8B, on 3 standard benchmarks (SimLex-999, WordSim-353, MEN-3000), we find that prompts like "meaning: {word}" or "Represent the semantic concept: {word}" improve Spearman correlations by up to +0.29 on SimLex-999. Some models fail completely on bare words (correlation = 0) but recover with prompts (+0.73 improvement). Our best results achieve correlation = 0.692 on SimLex-999 with embed-english-v3.0 (Cohere), correlation = 0.811 on WordSim-353, and correlation = 0.855 on MEN-3000 with text-embedding-3-large (OpenAI). These results outperform classic static embeddings like Word2Vec (correlation = 0.40) and even the best static method LexVec (correlation = 0.48) on SimLex-999, establishing a new state-of-the-art for pure embedding methods. This zero-shot technique requires no training and works with any text embedding model.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Becoming Experienced Judges: Selective Test-Time Learning for Evaluators</title>
<link>https://arxiv.org/abs/2512.06751</link>
<guid>https://arxiv.org/abs/2512.06751</guid>
<content:encoded><![CDATA[
<div> LLM evaluation, meta-prompt, sequential learning, selective update, self-consistency<br /><br />Summary:<br /><br />Automatic evaluation using large language models (LLMs), often called LLM-as-a-judge, is widespread for assessing reasoning and alignment tasks. However, conventional evaluators treat each sample independently and use a fixed prompt, missing opportunities to improve from experience and tailor evaluations to specific cases. To address this, the study introduces Learning While Evaluating (LWE), a novel framework enabling evaluators to improve sequentially at inference without needing separate training or validation datasets. LWE uses an evolving meta-prompt that generates sample-specific evaluation instructions and self-refines based on feedback generated during the evaluation process. Building upon this, Selective LWE further optimizes resource use by updating the meta-prompt only when cases show self-inconsistencies, allowing focused improvements on challenging samples. This selective strategy preserves the benefits of continual learning while reducing computational cost. Experimental results on two pairwise comparison benchmarks show that Selective LWE surpasses strong baseline evaluators, proving that evaluators can learn effectively during sequential testing by concentrating on cases that pose the greatest difficulty, thereby improving evaluation quality in an efficient manner. <div>
arXiv:2512.06751v1 Announce Type: new 
Abstract: Automatic evaluation with large language models, commonly known as LLM-as-a-judge, is now standard across reasoning and alignment tasks. Despite evaluating many samples in deployment, these evaluators typically (i) treat each case independently, missing the opportunity to accumulate experience, and (ii) rely on a single fixed prompt for all cases, neglecting the need for sample-specific evaluation criteria. We introduce Learning While Evaluating (LWE), a framework that allows evaluators to improve sequentially at inference time without requiring training or validation sets. LWE maintains an evolving meta-prompt that (i) produces sample-specific evaluation instructions and (ii) refines itself through self-generated feedback. Furthermore, we propose Selective LWE, which updates the meta-prompt only on self-inconsistent cases, focusing computation where it matters most. This selective approach retains the benefits of sequential learning while being far more cost-effective. Across two pairwise comparison benchmarks, Selective LWE outperforms strong baselines, empirically demonstrating that evaluators can improve during sequential testing with a simple selective update, learning most from the cases they struggle with.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs</title>
<link>https://arxiv.org/abs/2512.06776</link>
<guid>https://arxiv.org/abs/2512.06776</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Diffusion Language Models, Autoregressive Decoding, Block-wise Diffusion, Adaptation

<br /><br />Summary: This paper addresses the throughput bottleneck in large language models (LLMs) caused by inherently sequential autoregressive (AR) decoding. It focuses on Diffusion Language Models (DLMs), particularly block-wise variants, which enable parallel generation and bidirectional reasoning within blocks but are costly to train from scratch and discard knowledge from pretrained AR checkpoints. Previous adaptation approaches have limitations, such as modifying logits or attention masks without resolving the fundamental mismatch between AR causality and block-wise bidirectionality. The authors propose reframing adaptation as a pathway from AR to block-diffusion by treating AR as block-diffusion with block size one. Their method involves using a context-causal attention mask (causal across context but bidirectional inside active blocks), an efficient parallel adaptation procedure, an auxiliary AR loss to retain pretrained knowledge and maximize data use, and a gradual increase of block size during generation. This approach integrates seamlessly with masked block-diffusion while maintaining training-inference consistency. The resulting model, NBDiff-7B (both Base and Instruct versions), preserves long-context modeling and reasoning capabilities, achieving state-of-the-art results among 7B-parameter class DLMs on various benchmarks including general knowledge, math, and code. Overall, principled AR-to-block-diffusion adaptation is shown to be a compute-efficient and effective alternative to training large DLMs from scratch. <div>
arXiv:2512.06776v1 Announce Type: new 
Abstract: Large language models (LLMs) excel at generation but dominant autoregressive (AR) decoding is inherently sequential, creating a throughput bottleneck. Diffusion Language Models (DLMs)--especially block-wise variants--enable parallel generation and intra-block bidirectional reasoning, yet training large DLMs from scratch is costly and wastes the knowledge in mature AR checkpoints. Prior "adaptation" attempts either modify logits or randomly grow attention masks to full-sequence diffusion, or simply transplant AR weights into a block-diffusion recipe, leaving a fundamental mismatch between AR causality and block-wise bidirectionality unaddressed. We reframe adaptation as a intra-paradigm path from AR to Block-Diffusion by viewing AR as Block-Diffusion with blocksize=1. Concretely, we design the pathway of adaptation as follows: we use a context-causal attention mask (causal in context, bidirectional only within the active block), an efficient parallel adaptation procedure, an auxiliary AR loss to maximize data utilization and retain pretrained knowledge, and gradual increment of the generation block size. The recipe integrates cleanly with masked block-diffusion and maintains train-inference consistency. Built on these components, NBDiff-7B (Base and Instruct) could inherit the long-context modeling and reasoning capabilities, and achieve state-of-the-art performance among the 7B-class DLMs, delivering strong gains on general-knowledge, math, and code benchmarks over strong baselines. These results demonstrate that principled AR-to-block-diffusion adaptation is an effective and compute-efficient alternative to training DLMs from scratch. Codes: https://github.com/YuchuanTian/NBDiff.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM4SFC: Sequential Function Chart Generation via Large Language Models</title>
<link>https://arxiv.org/abs/2512.06787</link>
<guid>https://arxiv.org/abs/2512.06787</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Sequential Function Charts, PLC programming, structured generation, industrial automation<br /><br />Summary:<br /><br />1. The paper addresses the challenge of generating executable Sequential Function Charts (SFCs), a graphical programming language under IEC 61131-3, which is less explored compared to textual PLC languages like Structured Text (ST).<br /><br />2. SFC generation is difficult because it involves both graphical topology and embedded ST actions, which do not easily conform to standard text generation techniques, often resulting in non-executable code incompatible with industrial tools.<br /><br />3. To overcome these issues, the authors propose LLM4SFC, a novel framework that takes natural language descriptions of industrial workflows and outputs executable SFC programs.<br /><br />4. LLM4SFC relies on three main components: (i) a reduced structured representation that simplifies the graphical topology and embedded ST while minimizing verbosity; (ii) fine-tuning combined with retrieval-augmented generation (RAG) to align with SFC programming conventions; and (iii) a structured generation method that prunes illegal tokens in real-time, ensuring output compliance with SFC textual format.<br /><br />5. The framework is evaluated on a real-world dataset of SFCs from automated manufacturing projects using both open-source and proprietary LLMs, achieving a syntactic generation success rate between 75% and 94%, demonstrating its effectiveness in bridging graphical and textual PLC programming and advancing automated industrial programming. <div>
arXiv:2512.06787v1 Announce Type: new 
Abstract: While Large Language Models (LLMs) are increasingly used for synthesizing textual PLC programming languages like Structured Text (ST) code, other IEC 61131-3 standard graphical languages like Sequential Function Charts (SFCs) remain underexplored. Generating SFCs is challenging due to graphical nature and ST actions embedded within, which are not directly compatible with standard generation techniques, often leading to non-executable code that is incompatible with industrial tool-chains In this work, we introduce LLM4SFC, the first framework to receive natural-language descriptions of industrial workflows and provide executable SFCs. LLM4SFC is based on three components: (i) A reduced structured representation that captures essential topology and in-line ST and reduced textual verbosity; (ii) Fine-tuning and few-shot retrieval-augmented generation (RAG) for alignment with SFC programming conventions; and (iii) A structured generation approach that prunes illegal tokens in real-time to ensure compliance with the textual format of SFCs. We evaluate LLM4SFC on a dataset of real-world SFCs from automated manufacturing projects, using both open-source and proprietary LLMs. The results show that LLM4SFC reliably generates syntactically valid SFC programs effectively bridging graphical and textual PLC languages, achieving a generation generation success of 75% - 94%, paving the way for automated industrial programming.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Model-Based Generation of Discharge Summaries</title>
<link>https://arxiv.org/abs/2512.06812</link>
<guid>https://arxiv.org/abs/2512.06812</guid>
<content:encoded><![CDATA[
<div> Keywords: Discharge Summaries, Large Language Models, MIMIC-III, hallucinations, proprietary models  

<br /><br />Summary:  
Discharge summaries are essential medical documents that encapsulate a patient’s visit details and play a critical role in ongoing patient care. Automating their creation has the potential to reduce healthcare professionals' workload, minimize errors, and improve accessibility to vital patient information. This study evaluates the performance of five Large Language Models (LLMs) — including open-source models like Mistral and Llama 2, and proprietary models such as GPT-3, GPT-4, and Gemini 1.5 Pro — on generating discharge summaries using the MIMIC-III dataset. The models were assessed through exact-match, soft-overlap, and reference-free metrics to gauge summary quality. Results indicated that proprietary models, particularly Gemini with one-shot prompting, outperformed the open-source alternatives by producing summaries closely aligned with gold-standard notes. Open-source models showed potential after fine-tuning, especially Mistral, but faced challenges such as hallucinations and repetitive content. A clinical expert’s human evaluation affirmed the practical usefulness of summaries generated by proprietary models. The study acknowledges ongoing challenges in hallucination and missing information but concludes that Large Language Models, especially proprietary ones, hold promise for automating discharge summary generation, provided data privacy concerns are effectively addressed. <div>
arXiv:2512.06812v1 Announce Type: new 
Abstract: Discharge Summaries are documents written by medical professionals that detail a patient's visit to a care facility. They contain a wealth of information crucial for patient care, and automating their generation could significantly reduce the effort required from healthcare professionals, minimize errors, and ensure that critical patient information is easily accessible and actionable. In this work, we explore the use of five Large Language Models on this task, from open-source models (Mistral, Llama 2) to proprietary systems (GPT-3, GPT-4, Gemini 1.5 Pro), leveraging MIMIC-III summaries and notes. We evaluate them using exact-match, soft-overlap, and reference-free metrics. Our results show that proprietary models, particularly Gemini with one-shot prompting, outperformed others, producing summaries with the highest similarity to the gold-standard ones. Open-source models, while promising, especially Mistral after fine-tuning, lagged in performance, often struggling with hallucinations and repeated information. Human evaluation by a clinical expert confirmed the practical utility of the summaries generated by proprietary models. Despite the challenges, such as hallucinations and missing information, the findings suggest that LLMs, especially proprietary models, are promising candidates for automatic discharge summary generation as long as data privacy is ensured.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAuSE: Decoding Multimodal Classifiers using Faithful Natural Language Explanation</title>
<link>https://arxiv.org/abs/2512.06814</link>
<guid>https://arxiv.org/abs/2512.06814</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal classifiers, natural language explanations, faithfulness, causal abstraction, CAuSE<br /><br />Summary:<br />1. The paper addresses the interpretability of multimodal classifiers, which are typically opaque black-box models, by focusing on generating intuitive and accessible natural language explanations (NLEs).<br />2. It highlights the importance of faithfulness in explanations, meaning they must accurately reflect the internal decision-making of the classifier to build user trust.<br />3. The authors propose CAuSE (Causal Abstraction under Simulated Explanations), a novel framework designed to produce faithful NLEs for any pretrained multimodal classifier.<br />4. CAuSE is trained using interchange intervention, theoretically forming a causal abstraction of the classifier’s internal mechanism.<br />5. The framework is empirically validated across various datasets and models, showing superior performance in generating faithful explanations.<br />6. A new metric specifically for measuring causal faithfulness in multimodal settings is introduced to evaluate CAuSE and related methods.<br />7. Qualitative analysis and detailed error investigation further support the framework’s advantages as well as uncover cases where it can fail.<br />8. To support replicability and further research, the authors have made the source code available publicly at their GitHub repository. <div>
arXiv:2512.06814v1 Announce Type: new 
Abstract: Multimodal classifiers function as opaque black box models. While several techniques exist to interpret their predictions, very few of them are as intuitive and accessible as natural language explanations (NLEs). To build trust, such explanations must faithfully capture the classifier's internal decision making behavior, a property known as faithfulness. In this paper, we propose CAuSE (Causal Abstraction under Simulated Explanations), a novel framework to generate faithful NLEs for any pretrained multimodal classifier. We demonstrate that CAuSE generalizes across datasets and models through extensive empirical evaluations. Theoretically, we show that CAuSE, trained via interchange intervention, forms a causal abstraction of the underlying classifier. We further validate this through a redesigned metric for measuring causal faithfulness in multimodal settings. CAuSE surpasses other methods on this metric, with qualitative analysis reinforcing its advantages. We perform detailed error analysis to pinpoint the failure cases of CAuSE. For replicability, we make the codes available at https://github.com/newcodevelop/CAuSE
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AquaFusionNet: Lightweight VisionSensor Fusion Framework for Real-Time Pathogen Detection and Water Quality Anomaly Prediction on Edge Devices</title>
<link>https://arxiv.org/abs/2512.06848</link>
<guid>https://arxiv.org/abs/2512.06848</guid>
<content:encoded><![CDATA[
<div> Microbial contamination, drinking water systems, cross-modal framework, microscopic imaging, water quality prediction<br /><br />Summary:<br /><br />This study addresses the challenge of monitoring microbial contamination in small-scale drinking water systems where contamination levels fluctuate rapidly. Traditional monitoring tools fall short as they analyze microscopic imaging and physicochemical sensor data separately, limiting real-time decision-making reliability. AquaFusionNet is introduced as a lightweight, edge-deployable cross-modal framework that integrates microscopic organism-level data with short-term water chemistry sensor readings into a single model. Unlike previous methods treating detection and prediction independently, AquaFusionNet learns the dependencies between microbial appearances and sensor dynamics using a gated cross-attention mechanism optimized for low-power hardware. The model is trained on AquaMicro12K, a newly curated dataset of 12,846 annotated microscopic images relevant to drinking water monitoring—a rare public resource in this domain. Deployed over six months in seven facilities in East Java, Indonesia, the system processed 1.84 million frames, achieving a 94.8% mean average precision (mAP@0.5) for contamination detection and 96.3% accuracy in anomaly prediction while consuming only 4.8 W on a Jetson Nano device. Compared to existing lightweight detectors, AquaFusionNet delivers higher accuracy at similar or lower power usage. Field tests confirm that combining data modalities reduces errors common in single-sensor systems, especially under challenging conditions like fouling, turbidity, and uneven lighting. All models, datasets, and hardware designs are openly released to support replication and adoption in decentralized water safety efforts. <div>
arXiv:2512.06848v1 Announce Type: new 
Abstract: Evidence from many low and middle income regions shows that microbial contamination in small scale drinking water systems often fluctuates rapidly, yet existing monitoring tools capture only fragments of this behaviour. Microscopic imaging provides organism level visibility, whereas physicochemical sensors reveal shortterm changes in water chemistry; in practice, operators must interpret these streams separately, making realtime decision-making unreliable. This study introduces AquaFusionNet, a lightweight cross-modal framework that unifies both information sources inside a single edge deployable model. Unlike prior work that treats microscopic detection and water quality prediction as independent tasks, AquaFusionNet learns the statistical dependencies between microbial appearance and concurrent sensor dynamics through a gated crossattention mechanism designed specifically for lowpower hardware. The framework is trained on AquaMicro12K, a new dataset comprising 12,846 annotated 1000 micrographs curated for drinking water contexts, an area where publicly accessible microscopic datasets are scarce. Deployed for six months across seven facilities in East Java, Indonesia, the system processed 1.84 million frames and consistently detected contamination events with 94.8% mAP@0.5 and 96.3% anomaly prediction accuracy, while operating at 4.8 W on a Jetson Nano. Comparative experiments against representative lightweight detectors show that AquaFusionNet provides higher accuracy at comparable or lower power, and field results indicate that cross-modal coupling reduces common failure modes of unimodal detectors, particularly under fouling, turbidity spikes, and inconsistent illumination. All models, data, and hardware designs are released openly to facilitate replication and adaptation in decentralized water safety infrastructures.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rhea: Role-aware Heuristic Episodic Attention for Conversational LLMs</title>
<link>https://arxiv.org/abs/2512.06869</link>
<guid>https://arxiv.org/abs/2512.06869</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, contextual decay, episodic memory, instructional memory, multi-turn conversations<br /><br />Summary: Large Language Models (LLMs) perform well on single-turn tasks but struggle with multi-turn conversations due to a problem called cumulative contextual decay, where context quality degrades over successive turns because of attention pollution, dilution, and drift. To solve this, the paper introduces Rhea (Role-aware Heuristic Episodic Attention), a framework that separates conversation history into two distinct memory modules: Instructional Memory (IM), which maintains persistent, high-fidelity global constraints using a structural priority mechanism, and Episodic Memory (EM), which dynamically handles user-model interactions through asymmetric noise control and heuristic context retrieval. During inference, Rhea applies priority attention that selectively emphasizes relevant episodic information while always prioritizing global instructions to construct a context with a high signal-to-noise ratio. Experimental evaluation on multi-turn conversation benchmarks such as MT-Eval and Long-MT-Bench+ demonstrates that Rhea reduces performance decay and boosts overall accuracy by 1.04 points on a 10-point scale, marking a 16% relative improvement over strong baselines. Additionally, Rhea achieves near-perfect instruction adherence rates (IAR > 8.1) over extended conversational horizons, confirming its effectiveness in creating instruction-consistent and precise conversational LLMs. <div>
arXiv:2512.06869v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have achieved remarkable performance on single-turn tasks, yet their effectiveness deteriorates in multi-turn conversations. We define this phenomenon as cumulative contextual decay - a progressive degradation of contextual integrity caused by attention pollution, dilution, and drift. To address this challenge, we propose Rhea (Role-aware Heuristic Episodic Attention), a novel framework that decouples conversation history into two functionally independent memory modules: (1) an Instructional Memory (IM) that persistently stores high-fidelity global constraints via a structural priority mechanism, and (2) an Episodic Memory (EM) that dynamically manages user-model interactions via asymmetric noise control and heuristic context retrieval. During inference, Rhea constructs a high signal-to-noise context by applying its priority attention: selectively integrating relevant episodic information while always prioritizing global instructions. To validate this approach, experiments on multiple multi-turn conversation benchmarks - including MT-Eval and Long-MT-Bench+ - show that Rhea mitigates performance decay and improves overall accuracy by 1.04 points on a 10-point scale (a 16% relative gain over strong baselines). Moreover, Rhea maintains near-perfect instruction fidelity (IAR > 8.1) across long-horizon interactions. These results demonstrate that Rhea provides a principled and effective framework for building more precise, instruction-consistent conversational LLMs.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Analysis of Large Language Models for Simulating User Responses in Surveys</title>
<link>https://arxiv.org/abs/2512.06874</link>
<guid>https://arxiv.org/abs/2512.06874</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, user opinion simulation, bias, demographic diversity, CLAIMSIM<br /><br />Summary: This paper investigates the ability of Large Language Models (LLMs), particularly those trained with reinforcement learning from human feedback (RLHF), to simulate human responses to cross-domain survey questions. The study highlights concerns that LLMs exhibit biases toward dominant viewpoints, which may hinder their capacity to represent users from diverse demographic and cultural backgrounds. Two prompting strategies, direct prompting and chain-of-thought prompting, are evaluated for this task. The authors introduce CLAIMSIM, a novel claim diversification method designed to elicit a wider range of viewpoints from the LLM’s parametric knowledge by providing contextual input. Experimental results show that while CLAIMSIM successfully generates more diverse responses compared to baseline approaches, both it and the prompting methods struggle to accurately mimic varied user perspectives. Further analysis identifies two main challenges: first, LLMs tend to hold fixed viewpoints regardless of changes in demographic features, producing responses from a single perspective; second, when faced with conflicting claims, LLMs find it difficult to reason through subtle nuances tied to demographic differences, limiting their ability to tailor answers to specific user profiles. These findings underscore the need for improved methods to enhance diversity and adaptability in LLM-simulated user opinions. <div>
arXiv:2512.06874v1 Announce Type: new 
Abstract: Using Large Language Models (LLMs) to simulate user opinions has received growing attention. Yet LLMs, especially trained with reinforcement learning from human feedback (RLHF), are known to exhibit biases toward dominant viewpoints, raising concerns about their ability to represent users from diverse demographic and cultural backgrounds. In this work, we examine the extent to which LLMs can simulate human responses to cross-domain survey questions through direct prompting and chain-of-thought prompting. We further propose a claim diversification method CLAIMSIM, which elicits viewpoints from LLM parametric knowledge as contextual input. Experiments on the survey question answering task indicate that, while CLAIMSIM produces more diverse responses, both approaches struggle to accurately simulate users. Further analysis reveals two key limitations: (1) LLMs tend to maintain fixed viewpoints across varying demographic features, and generate single-perspective claims; and (2) when presented with conflicting claims, LLMs struggle to reason over nuanced differences among demographic features, limiting their ability to adapt responses to specific user profiles.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated PRO-CTCAE Symptom Selection based on Prior Adverse Event Profiles</title>
<link>https://arxiv.org/abs/2512.06919</link>
<guid>https://arxiv.org/abs/2512.06919</guid>
<content:encoded><![CDATA[
<div> PRO-CTCAE, MedDRA, adverse events, patient-reported outcomes, oncology trials<br /><br />Summary:<br /><br />1. The study addresses the challenge of selecting an optimal subset of PRO-CTCAE items—patient-reported symptomatic adverse events—for oncology clinical trials, aiming to reduce patient burden and increase compliance while ensuring comprehensive safety monitoring.<br /><br />2. Each PRO-CTCAE symptom term is mapped to corresponding MedDRA Preferred Terms (PTs), which are embedded into a high-dimensional semantic space called Safeterm that encapsulates clinical and contextual diversity in MedDRA terminology.<br /><br />3. A utility function combining the relevance of each PRO item to historical adverse event PT data and the incidence of these events is developed to objectively score candidate symptoms.<br /><br />4. Spectral analysis is applied to a combined matrix of utility and diversity to select an orthogonal and balanced set of symptom concepts, ensuring both coverage of relevant safety signals and minimal redundancy.<br /><br />5. The resulting symptom list is rank-ordered by importance with a suggested cut-off based on explained information, making the selection process objective, reproducible, and integrated into the Safeterm trial-safety app.<br /><br />6. The method was evaluated through simulations and oncology case studies where PRO-CTCAE was used, demonstrating effectiveness in streamlining PRO-CTCAE design by leveraging MedDRA semantics and historical safety data. <div>
arXiv:2512.06919v1 Announce Type: new 
Abstract: The PRO-CTCAE is an NCI-developed patient-reported outcome system for capturing symptomatic adverse events in oncology trials. It comprises a large library drawn from the CTCAE vocabulary, and item selection for a given trial is typically guided by expected toxicity profiles from prior data. Selecting too many PRO-CTCAE items can burden patients and reduce compliance, while too few may miss important safety signals. We present an automated method to select a minimal yet comprehensive PRO-CTCAE subset based on historical safety data. Each candidate PRO-CTCAE symptom term is first mapped to its corresponding MedDRA Preferred Terms (PTs), which are then encoded into Safeterm, a high-dimensional semantic space capturing clinical and contextual diversity in MedDRA terminology. We score each candidate PRO item for relevance to the historical list of adverse event PTs and combine relevance and incidence into a utility function. Spectral analysis is then applied to the combined utility and diversity matrix to identify an orthogonal set of medical concepts that balances relevance and diversity. Symptoms are rank-ordered by importance, and a cut-off is suggested based on the explained information. The tool is implemented as part of the Safeterm trial-safety app. We evaluate its performance using simulations and oncology case studies in which PRO-CTCAE was employed. This automated approach can streamline PRO-CTCAE design by leveraging MedDRA semantics and historical data, providing an objective and reproducible method to balance signal coverage against patient burden.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Models and Forensic Linguistics: Navigating Opportunities and Threats in the Age of Generative AI</title>
<link>https://arxiv.org/abs/2512.06922</link>
<guid>https://arxiv.org/abs/2512.06922</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, forensic linguistics, stylometry, AI-text detection, legal admissibility  

<br /><br />Summary:  
1. Large language models (LLMs) pose a dual challenge for forensic linguistics by acting as powerful tools for corpus analysis and authorship attribution, while also disrupting traditional assumptions about individual writing style (idiolect) through style mimicry and synthetic text generation.  
2. Stylometric studies show that although LLMs can closely imitate surface stylistic features, discernible differences remain compared to human writing, creating a forensic tension with important implications.  
3. Existing AI-generated text detection methods, including classifier-based, stylometric, and watermarking techniques, suffer from significant limitations such as high false positive rates for non-native English speakers and susceptibility to adversarial manipulations like homoglyph substitution.  
4. These challenges undermine the reliability and thus the legal admissibility of forensic linguistic evidence under standards like Daubert and Kumho Tire, which demand scientific validity and error rate transparency.  
5. The article recommends reconfiguring forensic linguistics methodologies by integrating hybrid human-AI workflows, adopting explainable detection frameworks beyond binary outputs, and establishing validation protocols that assess errors and biases across diverse linguistic populations. Despite these complexities, the fundamental forensic premise that language conveys information about its producer remains valid but requires adaptation to human-machine authored texts. <div>
arXiv:2512.06922v1 Announce Type: new 
Abstract: Large language models (LLMs) present a dual challenge for forensic linguistics. They serve as powerful analytical tools enabling scalable corpus analysis and embedding-based authorship attribution, while simultaneously destabilising foundational assumptions about idiolect through style mimicry, authorship obfuscation, and the proliferation of synthetic texts. Recent stylometric research indicates that LLMs can approximate surface stylistic features yet exhibit detectable differences from human writers, a tension with significant forensic implications. However, current AI-text detection techniques, whether classifier-based, stylometric, or watermarking approaches, face substantial limitations: high false positive rates for non-native English writers and vulnerability to adversarial strategies such as homoglyph substitution. These uncertainties raise concerns under legal admissibility standards, particularly the Daubert and Kumho Tire frameworks. The article concludes that forensic linguistics requires methodological reconfiguration to remain scientifically credible and legally admissible. Proposed adaptations include hybrid human-AI workflows, explainable detection paradigms beyond binary classification, and validation regimes measuring error and bias across diverse populations. The discipline's core insight, i.e., that language reveals information about its producer, remains valid but must accommodate increasingly complex chains of human and machine authorship.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>XAM: Interactive Explainability for Authorship Attribution Models</title>
<link>https://arxiv.org/abs/2512.06924</link>
<guid>https://arxiv.org/abs/2512.06924</guid>
<content:encoded><![CDATA[
<div> Keywords: Authorship Attribution, Explainability, Interactive Tool, Embedding Space, Writing Style Features<br /><br />Summary:  
1. The paper introduces IXAM, an Interactive eXplainability framework designed specifically for Authorship Attribution (AA) models.  
2. IXAM is compatible with embedding-based AA models and allows users to explore the model’s embedding space interactively.  
3. The framework enables users to construct explanations for the model’s predictions by identifying writing style features at multiple levels of granularity.  
4. Unlike static, predefined stylistic explanations, IXAM provides a dynamic and user-driven way to understand and interpret AA model decisions.  
5. A user evaluation was conducted to compare IXAM’s effectiveness against traditional explanation methods, and the results demonstrated its added value for interpretability in authorship attribution tasks. <div>
arXiv:2512.06924v1 Announce Type: new 
Abstract: We present IXAM, an Interactive eXplainability framework for Authorship Attribution Models. Given an authorship attribution (AA) task and an embedding-based AA model, our tool enables users to interactively explore the model's embedding space and construct an explanation of the model's prediction as a set of writing style features at different levels of granularity. Through a user evaluation, we demonstrate the value of our framework compared to predefined stylistic explanations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Progress Ratio Embeddings: An Impatience Signal for Robust Length Control in Neural Text Generation</title>
<link>https://arxiv.org/abs/2512.06938</link>
<guid>https://arxiv.org/abs/2512.06938</guid>
<content:encoded><![CDATA[
<div> Modern neural language models excel at text generation but struggle with precise control over output length. This paper critiques a recent length control method using Reverse Positional Embeddings (RPE), highlighting its instability when asked to generate text lengths beyond the model's training range. Specifically, using a discrete countdown signal representing the exact remaining tokens causes unstable generation behavior. To address these issues, the authors propose Progress Ratio Embeddings (PRE), which use continuous embeddings linked to a trigonometric impatience signal. PRE can be integrated directly into standard Transformer models, improving length control stability without compromising the quality or accuracy of generated text based on common evaluation metrics. Furthermore, PRE demonstrates strong generalization to target lengths that the model was not trained on. The approach was tested and validated on two prevalent news summarization datasets, confirming its effectiveness in maintaining length fidelity while preserving text generation accuracy.

Keywords: length control, neural language models, reverse positional embeddings, progress ratio embeddings, transformer

<br /><br />Summary:  
The paper addresses the challenge of precise length control in neural text generation. It critiques existing Reverse Positional Embeddings (RPE) methods for instability beyond trained length distributions, especially when using discrete countdown signals. To improve robustness, the authors introduce Progress Ratio Embeddings (PRE), which employ continuous embeddings based on a trigonometric impatience signal, offering a smoother control mechanism. PRE integrates easily into standard Transformer architectures without reducing text generation accuracy according to standard metrics. Experiments reveal that PRE generalizes effectively to lengths unseen during training, an important advantage over prior methods. Validation on two established news summarization benchmarks confirms that PRE maintains stable length fidelity alongside high-quality text generation. This work provides a practical and scalable solution for controlled-length neural text generation. <div>
arXiv:2512.06938v1 Announce Type: new 
Abstract: Modern neural language models achieve high accuracy in text generation, yet precise control over generation length remains underdeveloped. In this paper, we first investigate a recent length control method based on Reverse Positional Embeddings (RPE) and show its limits when control is requested beyond the training distribution. In particular, using a discrete countdown signal tied to the absolute remaining token count leads to instability. To provide robust length control, we introduce Progress Ratio Embeddings (PRE), as continuous embeddings tied to a trigonometric impatience signal. PRE integrates seamlessly into standard Transformer architectures, providing stable length fidelity without degrading text accuracy under standard evaluation metrics. We further show that PRE generalizes well to unseen target lengths. Experiments on two widely used news-summarization benchmarks validate these findings.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompting-in-a-Series: Psychology-Informed Contents and Embeddings for Personality Recognition With Decoder-Only Models</title>
<link>https://arxiv.org/abs/2512.06991</link>
<guid>https://arxiv.org/abs/2512.06991</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Personality Recognition, PICEPR, Content Generation, Modular Decoder

<br /><br />Summary:  
This paper introduces PICEPR (Psychology-Informed Contents Embeddings for Personality Recognition), a novel "Prompting-in-a-Series" algorithm designed to improve personality recognition using Large Language Models (LLMs). PICEPR features two distinct pipelines: (a) Contents, which focuses on generating or summarizing textual data, and (b) Embeddings, which extracts personality features from these generated contents. The algorithm leverages a modularized decoder-only LLM architecture to function both as a feature extractor and a generator of personality-rich content, improving classification tasks related to personality recognition. The researchers conducted extensive experiments to validate the effectiveness of their approach and the underlying rationale of the PICEPR algorithm. In their comparative study, they evaluated closed-source LLMs such as OpenAI’s gpt4o and Google’s gemini, as well as open-source models like Mistral AI’s mistral, assessing the quality of content generated by each. Results demonstrate that PICEPR achieves state-of-the-art performance in personality recognition, realizing a significant improvement of 5-15% compared to previous methods. The research repository, including model weights and further resources, is made publicly available for replication and further study at https://research.jingjietan.com/?q=PICEPR. <div>
arXiv:2512.06991v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across various natural language processing tasks. This research introduces a novel "Prompting-in-a-Series" algorithm, termed PICEPR (Psychology-Informed Contents Embeddings for Personality Recognition), featuring two pipelines: (a) Contents and (b) Embeddings. The approach demonstrates how a modularised decoder-only LLM can summarize or generate content, which can aid in classifying or enhancing personality recognition functions as a personality feature extractor and a generator for personality-rich content. We conducted various experiments to provide evidence to justify the rationale behind the PICEPR algorithm. Meanwhile, we also explored closed-source models such as \textit{gpt4o} from OpenAI and \textit{gemini} from Google, along with open-source models like \textit{mistral} from Mistral AI, to compare the quality of the generated content. The PICEPR algorithm has achieved a new state-of-the-art performance for personality recognition by 5-15\% improvement. The work repository and models' weight can be found at https://research.jingjietan.com/?q=PICEPR.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FVA-RAG: Falsification-Verification Alignment for Mitigating Sycophantic Hallucinations</title>
<link>https://arxiv.org/abs/2512.07015</link>
<guid>https://arxiv.org/abs/2512.07015</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, hallucinations, Retrieval Sycophancy, Falsification-Verification Alignment, adversarial retrieval  

<br /><br />Summary:  
1. Retrieval-Augmented Generation (RAG) systems reduce hallucinations in Large Language Models by grounding answers in external documents, but suffer from Retrieval Sycophancy, where retrievers fetch biased supportive documents when queries contain false premises or misconceptions.  
2. This vulnerability causes models to "hallucinate with citations," effectively reinforcing incorrect information through retrieved evidence that aligns with user bias rather than objective truth.  
3. The paper introduces Falsification-Verification Alignment RAG (FVA-RAG), which changes the retrieval approach from Inductive Verification (seeking supporting evidence) to Deductive Falsification (actively seeking contradictory evidence).  
4. FVA-RAG incorporates an Adversarial Retrieval Policy that generates "Kill Queries," designed to surface documents that can disprove or challenge the draft answer, thereby preventing sycophantic retrieval.  
5. A dual-verification mechanism is used to weigh the initial answer against this "Anti-Context," improving factual robustness; preliminary tests on misconception datasets show FVA-RAG significantly reduces sycophantic hallucinations compared to standard RAG, functioning as an inference-time "Red Team" for factual accuracy. <div>
arXiv:2512.07015v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) systems have significantly reduced hallucinations in Large Language Models (LLMs) by grounding responses in external context. However, standard RAG architectures suffer from a critical vulnerability: Retrieval Sycophancy. When presented with a query based on a false premise or a common misconception, vector-based retrievers tend to fetch documents that align with the user's bias rather than objective truth, leading the model to "hallucinate with citations."
  In this work, we introduce Falsification-Verification Alignment RAG (FVA-RAG), a framework that shifts the retrieval paradigm from Inductive Verification (seeking support) to Deductive Falsification (seeking disproof). Unlike existing "Self-Correction" methods that rely on internal consistency, FVA-RAG deploys a distinct Adversarial Retrieval Policy that actively generates "Kill Queries"-targeted search terms designed to surface contradictory evidence. We introduce a dual-verification mechanism that explicitly weighs the draft answer against this "Anti-Context." Preliminary experiments on a dataset of common misconceptions demonstrate that FVA-RAG significantly improves robustness against sycophantic hallucinations compared to standard RAG baselines, effectively acting as an inference-time "Red Team" for factual generation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Replicating TEMPEST at Scale: Multi-Turn Adversarial Attacks Against Trillion-Parameter Frontier Models</title>
<link>https://arxiv.org/abs/2512.07059</link>
<guid>https://arxiv.org/abs/2512.07059</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, adversarial attacks, safety alignment, model scale, deliberative inference<br /><br />Summary: This study investigates the robustness of large language models (LLMs) to sophisticated multi-turn adversarial attacks, a critical yet underexplored aspect of AI safety. Researchers utilized the TEMPEST multi-turn attack framework to evaluate ten leading models from eight different vendors, testing them against 1,000 harmful behaviors through over 97,000 API queries in adversarial conversational settings. The findings reveal a wide range of vulnerabilities: six models exhibited extremely high attack success rates (ASR) between 96% and 100%, while four models demonstrated notable resistance, with ASRs ranging from 42% to 78%. Importantly, the study discovered that enabling extended reasoning, or deliberative inference, on identical model architectures significantly reduced ASR from 97% to 42%. The results also emphasize that neither model scale nor vendor alone reliably predicts adversarial robustness, highlighting variability in safety alignment quality across providers. Overall, the research concludes that current alignment approaches are fundamentally insufficient against adaptive multi-turn attacks, regardless of model size. Nevertheless, implementing deliberative or “thinking” modes emerges as a promising and deployable strategy to enhance safety in LLMs, offering a potential practical defense direction against adversarial exploitation. <div>
arXiv:2512.07059v1 Announce Type: new 
Abstract: Despite substantial investment in safety alignment, the vulnerability of large language models to sophisticated multi-turn adversarial attacks remains poorly characterized, and whether model scale or inference mode affects robustness is unknown. This study employed the TEMPEST multi-turn attack framework to evaluate ten frontier models from eight vendors across 1,000 harmful behaviors, generating over 97,000 API queries across adversarial conversations with automated evaluation by independent safety classifiers. Results demonstrated a spectrum of vulnerability: six models achieved 96% to 100% attack success rate (ASR), while four showed meaningful resistance, with ASR ranging from 42% to 78%; enabling extended reasoning on identical architecture reduced ASR from 97% to 42%. These findings indicate that safety alignment quality varies substantially across vendors, that model scale does not predict adversarial robustness, and that thinking mode provides a deployable safety enhancement. Collectively, this work establishes that current alignment techniques remain fundamentally vulnerable to adaptive multi-turn attacks regardless of model scale, while identifying deliberative inference as a promising defense direction.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SETUP: Sentence-level English-To-Uniform Meaning Representation Parser</title>
<link>https://arxiv.org/abs/2512.07068</link>
<guid>https://arxiv.org/abs/2512.07068</guid>
<content:encoded><![CDATA[
<div> UMR, semantic representation, text-to-UMR parsing, low-resource languages, SETUP<br /><br />Summary:<br /><br />1. Uniform Meaning Representation (UMR) is a graph-based semantic framework designed to capture the core meaning of texts with an annotation schema flexible enough to cover a wide range of languages, including low-resource ones.<br /><br />2. UMR holds potential for advancing language documentation, improving technologies for languages with limited resources, and providing better interpretability of semantic structures.<br /><br />3. The practical and widespread application of UMR depends on the availability of accurate and scalable text-to-UMR parsers, which are currently underexplored.<br /><br />4. The paper introduces two novel methods for parsing English text into UMR graphs: one that fine-tunes existing Abstract Meaning Representation parsers, and another leveraging a converter from Universal Dependencies, building on prior benchmarks.<br /><br />5. Their best model, named SETUP, demonstrates significant improvements with an AnCast score of 84 and a SMATCH++ score of 91, marking substantial progress toward fully automatic, high-quality text-to-UMR parsing systems. <div>
arXiv:2512.07068v1 Announce Type: new 
Abstract: Uniform Meaning Representation (UMR) is a novel graph-based semantic representation which captures the core meaning of a text, with flexibility incorporated into the annotation schema such that the breadth of the world's languages can be annotated (including low-resource languages). While UMR shows promise in enabling language documentation, improving low-resource language technologies, and adding interpretability, the downstream applications of UMR can only be fully explored when text-to-UMR parsers enable the automatic large-scale production of accurate UMR graphs at test time. Prior work on text-to-UMR parsing is limited to date. In this paper, we introduce two methods for English text-to-UMR parsing, one of which fine-tunes existing parsers for Abstract Meaning Representation and the other, which leverages a converter from Universal Dependencies, using prior work as a baseline. Our best-performing model, which we call SETUP, achieves an AnCast score of 84 and a SMATCH++ score of 91, indicating substantial gains towards automatic UMR parsing.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Large Language Models Truly Understand Cross-cultural Differences?</title>
<link>https://arxiv.org/abs/2512.07075</link>
<guid>https://arxiv.org/abs/2512.07075</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, cross-cultural understanding, benchmarks, scenario-based evaluation, SAGE dataset<br /><br />Summary:<br />1. Recent advances in large language models (LLMs) have shown strong multilingual task performance, but assessing their cross-cultural understanding remains challenging. <br />2. Existing benchmarks fall short due to limited contextual scenarios, poor cross-cultural concept mapping, and shallow cultural reasoning capabilities.<br />3. To address these limitations, the authors present SAGE, a new scenario-based benchmark designed around cross-cultural core concept alignment and generative task design.<br />4. The benchmark is grounded in cultural theory, defining nine dimensions of cross-cultural capabilities, and includes 210 core concepts and 4530 test items.<br />5. These items are distributed across 15 real-world scenarios grouped under four broad categories of cross-cultural situations, following sound item design principles.<br />6. SAGE is designed for continuous expansion and is demonstrated to be transferable across different languages.<br />7. Experiments using SAGE highlight systematic weaknesses and limitations in LLMs’ cross-cultural reasoning and understanding.<br />8. While there has been some progress, current LLMs are still far from achieving truly nuanced cross-cultural comprehension.<br />9. In accordance with anonymity requirements, the authors provide data and code as supplemental materials, with plans to release them publicly in future versions. <div>
arXiv:2512.07075v1 Announce Type: new 
Abstract: In recent years, large language models (LLMs) have demonstrated strong performance on multilingual tasks. Given its wide range of applications, cross-cultural understanding capability is a crucial competency. However, existing benchmarks for evaluating whether LLMs genuinely possess this capability suffer from three key limitations: a lack of contextual scenarios, insufficient cross-cultural concept mapping, and limited deep cultural reasoning capabilities. To address these gaps, we propose SAGE, a scenario-based benchmark built via cross-cultural core concept alignment and generative task design, to evaluate LLMs' cross-cultural understanding and reasoning. Grounded in cultural theory, we categorize cross-cultural capabilities into nine dimensions. Using this framework, we curated 210 core concepts and constructed 4530 test items across 15 specific real-world scenarios, organized under four broader categories of cross-cultural situations, following established item design principles. The SAGE dataset supports continuous expansion, and experiments confirm its transferability to other languages. It reveals model weaknesses across both dimensions and scenarios, exposing systematic limitations in cross-cultural reasoning. While progress has been made, LLMs are still some distance away from reaching a truly nuanced cross-cultural understanding. In compliance with the anonymity policy, we include data and code in the supplement materials. In future versions, we will make them publicly available online.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging KV Similarity for Online Structured Pruning in LLMs</title>
<link>https://arxiv.org/abs/2512.07090</link>
<guid>https://arxiv.org/abs/2512.07090</guid>
<content:encoded><![CDATA[
<div> Pruning, Token Filtering, Large Language Models, Attention, Inference Efficiency<br /><br />Summary:<br /><br />This paper introduces Token Filtering, a novel online structured pruning method designed to accelerate inference in large language models (LLMs) without relying on offline calibration data. Unlike prior approaches that can be unstable due to dataset generalization issues, Token Filtering makes real-time pruning decisions by measuring token redundancy through joint key-value similarity in the attention mechanism. This strategy enables the model to skip computations for redundant tokens, reducing inference costs while retaining essential information. To improve pruning stability, the method employs a variance-aware fusion strategy that adaptively balances key and value similarities across attention heads, ensuring critical tokens are preserved even at high pruning levels. Notably, this approach incurs no additional memory overhead, making it efficient and practical. The effectiveness of Token Filtering is validated through extensive experiments on state-of-the-art LLMs such as LLaMA-2 (7B/13B), LLaMA-3 (8B), and Mistral (7B). Results show it consistently outperforms existing structured pruning methods by maintaining accuracy on commonsense reasoning tasks and robust performance on challenging benchmarks like MMLU, even when pruning up to 50% of tokens. This work presents a reliable and resource-efficient method for enhancing LLM inference speed without compromising model performance. <div>
arXiv:2512.07090v1 Announce Type: new 
Abstract: Pruning has emerged as a promising direction for accelerating large language model (LLM) inference, yet existing approaches often suffer from instability because they rely on offline calibration data that may not generalize across inputs. In this work, we introduce Token Filtering, a lightweight online structured pruning technique that makes pruning decisions directly during inference without any calibration data. The key idea is to measure token redundancy via joint key-value similarity and skip redundant attention computations, thereby reducing inference cost while preserving critical information. To further enhance stability, we design a variance-aware fusion strategy that adaptively weights key and value similarity across heads, ensuring that informative tokens are retained even under high pruning ratios. This design introduces no additional memory overhead and provides a more reliable criterion for token importance. Extensive experiments on LLaMA-2 (7B/13B), LLaMA-3 (8B), and Mistral (7B) demonstrate that Token Filtering consistently outperforms prior structured pruning methods, preserving accuracy on commonsense reasoning benchmarks and maintaining strong performance on challenging tasks such as MMLU, even with 50% pruning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2512.07132</link>
<guid>https://arxiv.org/abs/2512.07132</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent framework, visual tools, disagreement resolution, tool calling, vision-language models<br /><br />Summary: This paper introduces DART, a novel multi-agent framework designed to improve the use of specialized visual tools within large language models or vision-language models by leveraging disagreements among multiple debating visual agents. These visual agents identify and call upon appropriate expert tools such as object detection, OCR, and spatial reasoning to resolve disagreements, enriching the multi-agent discussions with new information. DART incorporates tool-aligned agreement scores that highlight consensus between agents and expert tools, facilitating more effective dialogues. An aggregator agent then selects the best answer based on the outputs from agents and the tool-related information. The authors evaluate DART on four diverse benchmarks, demonstrating superior performance compared to existing multi-agent debate frameworks and single-agent tool-calling methods, achieving improvements of 3.4% and 2.4% on A-OKVQA and MMMU datasets respectively. DART also shows adaptability to new tools in applied domains, yielding a 1.3% gain on the M3D medical dataset relative to other strong baselines. Analysis of text overlap across discussion rounds reveals richer interactions in DART compared to prior multi-agent approaches. Lastly, distribution analysis confirms that a variety of tools are consistently employed to resolve disagreements among agents, highlighting DART’s robust and flexible tool utilization strategy. <div>
arXiv:2512.07132v1 Announce Type: new 
Abstract: Specialized visual tools can augment large language models or vision language models with expert knowledge (e.g., grounding, spatial reasoning, medical knowledge, etc.), but knowing which tools to call (and when to call them) can be challenging. We introduce DART, a multi-agent framework that uses disagreements between multiple debating visual agents to identify useful visual tools (e.g., object detection, OCR, spatial reasoning, etc.) that can resolve inter-agent disagreement. These tools allow for fruitful multi-agent discussion by introducing new information, and by providing tool-aligned agreement scores that highlight agents in agreement with expert tools, thereby facilitating discussion. We utilize an aggregator agent to select the best answer by providing the agent outputs and tool information. We test DART on four diverse benchmarks and show that our approach improves over multi-agent debate as well as over single agent tool-calling frameworks, beating the next-strongest baseline (multi-agent debate with a judge model) by 3.4% and 2.4% on A-OKVQA and MMMU respectively. We also find that DART adapts well to new tools in applied domains, with a 1.3% improvement on the M3D medical dataset over other strong tool-calling, single agent, and multi-agent baselines. Additionally, we measure text overlap across rounds to highlight the rich discussion in DART compared to existing multi-agent methods. Finally, we study the tool call distribution, finding that diverse tools are reliably used to help resolve disagreement.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GUMBridge: a Corpus for Varieties of Bridging Anaphora</title>
<link>https://arxiv.org/abs/2512.07134</link>
<guid>https://arxiv.org/abs/2512.07134</guid>
<content:encoded><![CDATA[
<div> Bridging anaphora, GUMBridge, diverse genres, annotation quality, LLM evaluation<br /><br />Summary:<br /><br />1. Bridging is an anaphoric phenomenon where the interpretation of an entity depends on a previous, non-identical entity in the discourse, exemplified by references such as "the door" relating back to "a house."<br />2. Existing English resources for bridging are limited either in size, coverage of the phenomenon, or genre diversity.<br />3. The paper introduces GUMBridge, a new resource that encompasses 16 diverse English genres, aiming to provide broader coverage of bridging phenomena as well as detailed subtype categorization.<br />4. The authors conduct an evaluation of annotation quality to ensure the reliability of the GUMBridge resource.<br />5. Baseline experiments are conducted using both open and closed source contemporary large language models (LLMs) on tasks relevant to bridging resolution and subtype classification.<br />6. Results show that even with advanced LLMs, accurately resolving bridging anaphora and classifying its subtypes remain challenging tasks in NLP, highlighting the difficulty and the need for further research in this area. <div>
arXiv:2512.07134v1 Announce Type: new 
Abstract: Bridging is an anaphoric phenomenon where the referent of an entity in a discourse is dependent on a previous, non-identical entity for interpretation, such as in "There is 'a house'. 'The door' is red," where the door is specifically understood to be the door of the aforementioned house. While there are several existing resources in English for bridging anaphora, most are small, provide limited coverage of the phenomenon, and/or provide limited genre coverage. In this paper, we introduce GUMBridge, a new resource for bridging, which includes 16 diverse genres of English, providing both broad coverage for the phenomenon and granular annotations for the subtype categorization of bridging varieties. We also present an evaluation of annotation quality and report on baseline performance using open and closed source contemporary LLMs on three tasks underlying our data, showing that bridging resolution and subtype classification remain difficult NLP tasks in the age of LLMs.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MASim: Multilingual Agent-Based Simulation for Social Science</title>
<link>https://arxiv.org/abs/2512.07195</link>
<guid>https://arxiv.org/abs/2512.07195</guid>
<content:encoded><![CDATA[
<div> multilingual simulation, multi-agent systems, sociolinguistic profiles, public opinion modeling, information diffusion<br /><br />Summary:<br /><br />This paper introduces MASim, a novel multilingual agent-based simulation framework designed to facilitate multi-turn interactions among generative agents with diverse sociolinguistic backgrounds. Firstly, MASim addresses the limitation of prior monolingual simulations by enabling cross-lingual interaction that mirrors real-world social dynamics. Secondly, it provides two analytical capabilities: modeling global public opinion by simulating how attitudes toward various open-domain hypotheses evolve across different languages and cultures, and studying media influence and information diffusion through autonomous news agents generating dynamic content to shape user behavior. Thirdly, the authors present the MAPS benchmark, which integrates survey questions with demographic personas reflecting global population distributions to instantiate realistic simulations. Fourthly, extensive experiments assessing calibration, sensitivity, and consistency demonstrate that MASim effectively reproduces sociocultural phenomena. Finally, cultural case studies underscore the importance of multilingual simulations as a scalable and controlled approach to computational social science, highlighting their potential in advancing the understanding of complex social behaviors in diverse populations. <div>
arXiv:2512.07195v1 Announce Type: new 
Abstract: Multi-agent role-playing has recently shown promise for studying social behavior with language agents, but existing simulations are mostly monolingual and fail to model cross-lingual interaction, an essential property of real societies. We introduce MASim, the first multilingual agent-based simulation framework that supports multi-turn interaction among generative agents with diverse sociolinguistic profiles. MASim offers two key analyses: (i) global public opinion modeling, by simulating how attitudes toward open-domain hypotheses evolve across languages and cultures, and (ii) media influence and information diffusion, via autonomous news agents that dynamically generate content and shape user behavior. To instantiate simulations, we construct the MAPS benchmark, which combines survey questions and demographic personas drawn from global population distributions. Experiments on calibration, sensitivity, consistency, and cultural case studies show that MASim reproduces sociocultural phenomena and highlights the importance of multilingual simulation for scalable, controlled computational social science.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeSTR: A Neuro-Symbolic Abductive Framework for Temporal Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2512.07218</link>
<guid>https://arxiv.org/abs/2512.07218</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Temporal Reasoning, Neuro-Symbolic Integration, Reflective Mechanisms, Abductive Reflection  

<br /><br />Summary:  
This paper addresses the challenge of temporal reasoning in Large Language Models (LLMs), which struggle particularly with complex temporal constraints despite strong performance in other NLP tasks. Existing methods either rely on symbolic approaches that explicitly encode temporal structure but underutilize LLM reasoning capabilities or on reflective mechanisms that revise errors without structured temporal representations, often resulting in inconsistent or hallucinated conclusions. To overcome these issues, the authors propose Neuro-Symbolic Temporal Reasoning (NeSTR), a novel framework that combines structured symbolic representations with hybrid reflective reasoning to enhance temporal sensitivity in LLM inference. NeSTR preserves explicit temporal relations through symbolic encoding, enforces logical consistency via verification processes, and improves reasoning accuracy by correcting flawed inferences with abductive reflection. The approach does not require any fine-tuning of the language models and is demonstrated through extensive experiments on diverse temporal question answering benchmarks. Results show that NeSTR achieves superior zero-shot performance and consistently improves temporal reasoning capabilities, underscoring the advantages of neuro-symbolic integration for enhancing temporal understanding in large language models. <div>
arXiv:2512.07218v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, temporal reasoning, particularly under complex temporal constraints, remains a major challenge. To this end, existing approaches have explored symbolic methods, which encode temporal structure explicitly, and reflective mechanisms, which revise reasoning errors through multi-step inference. Nonetheless, symbolic approaches often underutilize the reasoning capabilities of LLMs, while reflective methods typically lack structured temporal representations, which can result in inconsistent or hallucinated reasoning. As a result, even when the correct temporal context is available, LLMs may still misinterpret or misapply time-related information, leading to incomplete or inaccurate answers. To address these limitations, in this work, we propose Neuro-Symbolic Temporal Reasoning (NeSTR), a novel framework that integrates structured symbolic representations with hybrid reflective reasoning to enhance the temporal sensitivity of LLM inference. NeSTR preserves explicit temporal relations through symbolic encoding, enforces logical consistency via verification, and corrects flawed inferences using abductive reflection. Extensive experiments on diverse temporal question answering benchmarks demonstrate that NeSTR achieves superior zero-shot performance and consistently improves temporal reasoning without any fine-tuning, showcasing the advantage of neuro-symbolic integration in enhancing temporal understanding in large language models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ensembling LLM-Induced Decision Trees for Explainable and Robust Error Detection</title>
<link>https://arxiv.org/abs/2512.07246</link>
<guid>https://arxiv.org/abs/2512.07246</guid>
<content:encoded><![CDATA[
<div> Error Detection, Large Language Models, Decision Tree, Explainability, Robustness<br /><br />Summary:<br /><br />1. The paper addresses error detection (ED) in tabular data, focusing on identifying incorrect or inconsistent cell values to ensure data quality. 2. Current state-of-the-art methods use large language models (LLMs) as direct labelers to mark errors, but these rely on black-box decision processes that lack explainability and are sensitive to prompt variations, resulting in unstable outputs. 3. To overcome these limitations, the authors propose an LLM-as-an-inducer framework that uses LLMs to generate decision trees for ED (TreeED), which enhances both interpretability and robustness. 4. TreeED queries LLMs with context-based prompts to induce decision tree skeletons where paths represent explicit stepwise decision procedures. Each tree node is either a rule node (simple validation), a Graph Neural Network (GNN) node (complex pattern recognition like functional dependencies), or a leaf node (final error or clean decision). 5. Further, ForestED ensembles multiple such trees, created from uncertainty-based row subset sampling, and applies an Expectation-Maximization algorithm to jointly estimate tree reliability and optimize consensus error detection. Experiments show this approach improves average F1-score by 16.1% over the best existing baselines, while being more explainable and robust. <div>
arXiv:2512.07246v1 Announce Type: new 
Abstract: Error detection (ED), which aims to identify incorrect or inconsistent cell values in tabular data, is important for ensuring data quality. Recent state-of-the-art ED methods leverage the pre-trained knowledge and semantic capability embedded in large language models (LLMs) to directly label whether a cell is erroneous. However, this LLM-as-a-labeler pipeline (1) relies on the black box, implicit decision process, thus failing to provide explainability for the detection results, and (2) is highly sensitive to prompts, yielding inconsistent outputs due to inherent model stochasticity, therefore lacking robustness. To address these limitations, we propose an LLM-as-an-inducer framework that adopts LLM to induce the decision tree for ED (termed TreeED) and further ensembles multiple such trees for consensus detection (termed ForestED), thereby improving explainability and robustness. Specifically, based on prompts derived from data context, decision tree specifications and output requirements, TreeED queries the LLM to induce the decision tree skeleton, whose root-to-leaf decision paths specify the stepwise procedure for evaluating a given sample. Each tree contains three types of nodes: (1) rule nodes that perform simple validation checks (e.g., format or range), (2) Graph Neural Network (GNN) nodes that capture complex patterns (e.g., functional dependencies), and (3) leaf nodes that output the final decision types (error or clean). Furthermore, ForestED employs uncertainty-based sampling to obtain multiple row subsets, constructing a decision tree for each subset using TreeED. It then leverages an Expectation-Maximization-based algorithm that jointly estimates tree reliability and optimizes the consensus ED prediction. Extensive xperiments demonstrate that our methods are accurate, explainable and robust, achieving an average F1-score improvement of 16.1% over the best baseline.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TeluguST-46: A Benchmark Corpus and Comprehensive Evaluation for Telugu-English Speech Translation</title>
<link>https://arxiv.org/abs/2512.07265</link>
<guid>https://arxiv.org/abs/2512.07265</guid>
<content:encoded><![CDATA[
<div> Telugu, Speech Translation, End-to-End Models, Evaluation Metrics, Low-Resource Languages  

<br /><br />Summary:  
This paper addresses the underexplored area of Telugu–English speech translation, a vital task given Telugu’s status as a morphologically rich language spoken by over 80 million people. The authors create a high-quality benchmark based on 46 hours of manually verified CSTD corpus data divided into training, development, and test sets (30h/8h/8h). They systematically compare cascaded architectures like IndicWhisper + IndicMT, which excel due to abundant Telugu-specific data, with end-to-end models such as finetuned SeamlessM4T, which show competitive performance using significantly less Telugu-specific data. This demonstrates that with appropriate hyperparameter tuning and under 100 hours of parallel data, end-to-end speech translation systems can rival more complex cascaded approaches in low-resource contexts. Additionally, the study conducts a thorough metric reliability evaluation using BLEU, METEOR, ChrF++, ROUGE-L, TER, and BERTScore against human judgment, finding that traditional metrics better distinguish translation quality than BERTScore for Telugu-English pairs. The contributions include: a reproducible Telugu–English speech translation benchmark, new empirical insights into the efficacy of end-to-end models in low-resource settings, and practical recommendations on choosing automatic evaluation metrics for morphologically complex language pairs. <div>
arXiv:2512.07265v1 Announce Type: new 
Abstract: Despite Telugu being spoken by over 80 million people, speech translation research for this morphologically rich language remains severely underexplored. We address this gap by developing a high-quality Telugu--English speech translation benchmark from 46 hours of manually verified CSTD corpus data (30h/8h/8h train/dev/test split). Our systematic comparison of cascaded versus end-to-end architectures shows that while IndicWhisper + IndicMT achieves the highest performance due to extensive Telugu-specific training data, finetuned SeamlessM4T models demonstrate remarkable competitiveness despite using significantly less Telugu-specific training data. This finding suggests that with careful hyperparameter tuning and sufficient parallel data (potentially less than 100 hours), end-to-end systems can achieve performance comparable to cascaded approaches in low-resource settings. Our metric reliability study evaluating BLEU, METEOR, ChrF++, ROUGE-L, TER, and BERTScore against human judgments reveals that traditional metrics provide better quality discrimination than BERTScore for Telugu--English translation. The work delivers three key contributions: a reproducible Telugu--English benchmark, empirical evidence of competitive end-to-end performance potential in low-resource scenarios, and practical guidance for automatic evaluation in morphologically complex language pairs.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient ASR for Low-Resource Languages: Leveraging Cross-Lingual Unlabeled Data</title>
<link>https://arxiv.org/abs/2512.07277</link>
<guid>https://arxiv.org/abs/2512.07277</guid>
<content:encoded><![CDATA[
<div> Keywords: low-resource languages, cross-lingual pretraining, Perso-Arabic languages, automatic speech recognition, unlabeled speech data<br /><br />Summary:<br /><br />1. The paper addresses the challenge of automatic speech recognition (ASR) for low-resource languages, which suffer from limited labeled data and the high computational demands of current state-of-the-art models. <br />2. It focuses on cross-lingual continuous pretraining, using Perso-Arabic languages—Persian, Arabic, and Urdu—as the main case study to explore resource-efficient ASR development.<br />3. The authors create a large-scale, 3,000-hour unlabeled multilingual speech corpus via a scalable data collection pipeline to harness unlabeled data effectively.<br />4. Their approach integrates targeted continual pretraining with morphologically-aware tokenization to optimize model training for linguistic characteristics, resulting in a compact 300 million parameter ASR model.<br />5. This model surpasses the performance of much larger models, notably outperforming Whisper Large v3 (1.5 billion parameters) on Persian, and achieving competitive results on Arabic and Urdu, demonstrating that model size is not the only determinant of ASR quality.<br />6. The study highlights that the relevance of data and strategic pretraining techniques are more crucial than sheer model scale in low-resource settings, offering a practical method to develop inclusive speech technology without relying on massive datasets or infrastructure.<br />7. Overall, this work paves the way for building effective, resource-aware ASR systems that support underrepresented languages, promoting technological inclusivity. <div>
arXiv:2512.07277v1 Announce Type: new 
Abstract: Automatic speech recognition for low-resource languages remains fundamentally constrained by the scarcity of labeled data and computational resources required by state-of-the-art models. We present a systematic investigation into cross-lingual continuous pretraining for low-resource languages, using Perso-Arabic languages (Persian, Arabic, and Urdu) as our primary case study. Our approach demonstrates that strategic utilization of unlabeled speech data can effectively bridge the resource gap without sacrificing recognition accuracy. We construct a 3,000-hour multilingual corpus through a scalable unlabeled data collection pipeline and employ targeted continual pretraining combined with morphologically-aware tokenization to develop a 300M parameter model that achieves performance comparable to systems 5 times larger. Our model outperforms Whisper Large v3 (1.5B parameters) on Persian and achieves competitive results on Arabic and Urdu despite using significantly fewer parameters and substantially less labeled data. These findings challenge the prevailing assumption that ASR quality scales primarily with model size, revealing instead that data relevance and strategic pretraining are more critical factors for low-resource scenarios. This work provides a practical pathway toward inclusive speech technology, enabling effective ASR for underrepresented languages without dependence on massive computational infrastructure or proprietary datasets.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Investigating Training and Generalization in Faithful Self-Explanations of Large Language Models</title>
<link>https://arxiv.org/abs/2512.07288</link>
<guid>https://arxiv.org/abs/2512.07288</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, faithful self-explanations, explanation styles, continual learning, generalization  

<br /><br />Summary: This study investigates how to improve the faithfulness of explanations generated by large language models (LLMs) for their own predictions. Recent work has shown that although LLMs can produce explanations in various styles based on user instructions, these self-explanations are often not faithful to the model’s true reasoning process. To address this, the researchers use a method to generate one-word constrained explanations highly likely to be faithful by leveraging feature attribution techniques. These pseudo-faithful explanations are then used for continual learning on instruction-tuned models. The experimental setup includes three classification tasks and three different explanation styles. Results demonstrate that training with pseudo-faithful explanations improves the faithfulness of LLM-generated explanations across all tested tasks and explanation styles. Additionally, improvements made in one style tend to generalize to other explanation styles, including multi-word explanations and tasks not seen during training. This cross-style generalization suggests that the training process contributes to a broader enhancement of the models' ability to produce faithful self-explanations. Overall, the work highlights continual learning with pseudo-faithful explanations as a promising direction to enhance the interpretability and reliability of LLM-generated explanations. <div>
arXiv:2512.07288v1 Announce Type: new 
Abstract: Large language models have the potential to generate explanations for their own predictions in a variety of styles based on user instructions. Recent research has examined whether these self-explanations faithfully reflect the models' actual behavior and has found that they often lack faithfulness. However, the question of how to improve faithfulness remains underexplored. Moreover, because different explanation styles have superficially distinct characteristics, it is unclear whether improvements observed in one style also arise when using other styles. This study analyzes the effects of training for faithful self-explanations and the extent to which these effects generalize, using three classification tasks and three explanation styles. We construct one-word constrained explanations that are likely to be faithful using a feature attribution method, and use these pseudo-faithful self-explanations for continual learning on instruction-tuned models. Our experiments demonstrate that training can improve self-explanation faithfulness across all classification tasks and explanation styles, and that these improvements also show signs of generalization to the multi-word settings and to unseen tasks. Furthermore, we find consistent cross-style generalization among three styles, suggesting that training may contribute to a broader improvement in faithful self-explanation ability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multilingual corpora for the study of new concepts in the social sciences and humanities:</title>
<link>https://arxiv.org/abs/2512.07367</link>
<guid>https://arxiv.org/abs/2512.07367</guid>
<content:encoded><![CDATA[
<div> Keywords: multilingual corpus, non-technological innovation, automatic extraction, supervised classification, natural language processing<br /><br />Summary: This article introduces a hybrid methodology for constructing a multilingual corpus to investigate emerging concepts within the humanities and social sciences, specifically focusing on "non-technological innovation." The corpus is built from two complementary sources: textual data automatically extracted from company websites in French and English, and annual reports gathered and filtered based on documentary criteria such as year, format, and duplication. The data processing pipeline incorporates automatic language detection, removal of non-relevant content, extraction of pertinent text segments, and enrichment with structural metadata. From this foundational corpus, a derived English-language dataset is created to support machine learning applications. Each instance of a term from an expert lexicon is identified, and a contextual block of five sentences (two before and two after the target sentence) is extracted. These occurrences are annotated with thematic categories linked to the terms, facilitating the generation of labeled data suitable for supervised classification tasks. The resulting resource is reproducible and extensible, enabling researchers to analyze lexical variability of emerging concepts and to develop datasets tailored for natural language processing tasks. <div>
arXiv:2512.07367v1 Announce Type: new 
Abstract: This article presents a hybrid methodology for building a multilingual corpus designed to support the study of emerging concepts in the humanities and social sciences (HSS), illustrated here through the case of ``non-technological innovation''. The corpus relies on two complementary sources: (1) textual content automatically extracted from company websites, cleaned for French and English, and (2) annual reports collected and automatically filtered according to documentary criteria (year, format, duplication). The processing pipeline includes automatic language detection, filtering of non-relevant content, extraction of relevant segments, and enrichment with structural metadata. From this initial corpus, a derived dataset in English is created for machine learning purposes. For each occurrence of a term from the expert lexicon, a contextual block of five sentences is extracted (two preceding and two following the sentence containing the term). Each occurrence is annotated with the thematic category associated with the term, enabling the construction of data suitable for supervised classification tasks. This approach results in a reproducible and extensible resource, suitable both for analyzing lexical variability around emerging concepts and for generating datasets dedicated to natural language processing applications.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Language Models to Use Prolog as a Tool</title>
<link>https://arxiv.org/abs/2512.07407</link>
<guid>https://arxiv.org/abs/2512.07407</guid>
<content:encoded><![CDATA[
arXiv:2512.07407v1 Announce Type: new 
Abstract: Ensuring reliable tool use is critical for safe agentic AI systems. Language models frequently produce unreliable reasoning with plausible but incorrect solutions that are difficult to verify. To address this, we investigate fine-tuning models to use Prolog as an external tool for verifiable computation. Using Group Relative Policy Optimization (GRPO), we fine-tune Qwen2.5-3B-Instruct on a cleaned GSM8K-Prolog-Prover dataset while varying (i) prompt structure, (ii) reward composition (execution, syntax, semantics, structure), and (iii) inference protocol: single-shot, best-of-N, and two agentic modes where Prolog is invoked internally or independently. Our reinforcement learning approach outperforms supervised fine-tuning, with our 3B model achieving zero-shot MMLU performance comparable to 7B few-shot results. Our findings reveal that: 1) joint tuning of prompt, reward, and inference shapes program syntax and logic; 2) best-of-N with external Prolog verification maximizes accuracy on GSM8K; 3) agentic inference with internal repair yields superior zero-shot generalization on MMLU-Stem and MMLU-Pro. These results demonstrate that grounding model reasoning in formal verification systems substantially improves reliability and auditability for safety-critical applications. The source code for reproducing our experiments is available under https://github.com/niklasmellgren/grpo-prolog-inference
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Persian-Phi: Efficient Cross-Lingual Adaptation of Compact LLMs via Curriculum Learning</title>
<link>https://arxiv.org/abs/2512.07454</link>
<guid>https://arxiv.org/abs/2512.07454</guid>
<content:encoded><![CDATA[
arXiv:2512.07454v1 Announce Type: new 
Abstract: The democratization of AI is currently hindered by the immense computational costs required to train Large Language Models (LLMs) for low-resource languages. This paper presents Persian-Phi, a 3.8B parameter model that challenges the assumption that robust multilingual capabilities require massive model sizes or multilingual baselines. We demonstrate how Microsoft Phi-3 Mini -- originally a monolingual English model -- can be effectively adapted to Persian through a novel, resource-efficient curriculum learning pipeline. Our approach employs a unique "warm-up" stage using bilingual narratives (Tiny Stories) to align embeddings prior to heavy training, followed by continual pretraining and instruction tuning via Parameter-Efficient Fine-Tuning (PEFT). Despite its compact size, Persian-Phi achieves competitive results on Open Persian LLM Leaderboard in HuggingFace. Our findings provide a validated, scalable framework for extending the reach of state-of-the-art LLMs to underrepresented languages with minimal hardware resources. The Persian-Phi model is publicly available at https://huggingface.co/amirakhlaghiqqq/PersianPhi.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.07461</link>
<guid>https://arxiv.org/abs/2512.07461</guid>
<content:encoded><![CDATA[
arXiv:2512.07461v1 Announce Type: new 
Abstract: We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Agentic RL with Progressive Reward Shaping and Value-based Sampling Policy Optimization</title>
<link>https://arxiv.org/abs/2512.07478</link>
<guid>https://arxiv.org/abs/2512.07478</guid>
<content:encoded><![CDATA[
arXiv:2512.07478v1 Announce Type: new 
Abstract: Large Language Models (LLMs) empowered with Tool-Integrated Reasoning (TIR) can iteratively plan, call external tools, and integrate returned information to solve complex, long-horizon reasoning tasks. Agentic Reinforcement Learning (Agentic RL) optimizes such models over full tool-interaction trajectories, but two key challenges hinder effectiveness: (1) Sparse, non-instructive rewards, such as binary 0-1 verifiable signals, provide limited guidance for intermediate steps and slow convergence; (2) Gradient degradation in Group Relative Policy Optimization (GRPO), where identical rewards within a rollout group yield zero advantage, reducing sample efficiency and destabilizing training. To address these challenges, we propose two complementary techniques: Progressive Reward Shaping (PRS) and Value-based Sampling Policy Optimization (VSPO). PRS is a curriculum-inspired reward design that introduces dense, stage-wise feedback - encouraging models to first master parseable and properly formatted tool calls, then optimize for factual correctness and answer quality. We instantiate PRS for short-form QA (with a length-aware BLEU to fairly score concise answers) and long-form QA (with LLM-as-a-Judge scoring to prevent reward hacking). VSPO is an enhanced GRPO variant that replaces low-value samples with prompts selected by a task-value metric balancing difficulty and uncertainty, and applies value-smoothing clipping to stabilize gradient updates. Experiments on multiple short-form and long-form QA benchmarks show that PRS consistently outperforms traditional binary rewards, and VSPO achieves superior stability, faster convergence, and higher final performance compared to PPO, GRPO, CISPO, and SFT-only baselines. Together, PRS and VSPO yield LLM-based TIR agents that generalize better across domains.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPAD: Seven-Source Token Probability Attribution with Syntactic Aggregation for Detecting Hallucinations in RAG</title>
<link>https://arxiv.org/abs/2512.07515</link>
<guid>https://arxiv.org/abs/2512.07515</guid>
<content:encoded><![CDATA[
arXiv:2512.07515v1 Announce Type: new 
Abstract: Detecting hallucinations in Retrieval-Augmented Generation (RAG) remains a challenge. Prior approaches attribute hallucinations to a binary conflict between internal knowledge (stored in FFNs) and retrieved context. However, this perspective is incomplete, failing to account for the impact of other components in the generative process, such as the user query, previously generated tokens, the current token itself, and the final LayerNorm adjustment. To address this, we introduce SPAD. First, we mathematically attribute each token's probability into seven distinct sources: Query, RAG, Past, Current Token, FFN, Final LayerNorm, and Initial Embedding. This attribution quantifies how each source contributes to the generation of the current token. Then, we aggregate these scores by POS tags to quantify how different components drive specific linguistic categories. By identifying anomalies, such as Nouns relying on Final LayerNorm, SPAD effectively detects hallucinations. Extensive experiments demonstrate that SPAD achieves state-of-the-art performance
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LIME: Making LLM Data More Efficient with Linguistic Metadata Embeddings</title>
<link>https://arxiv.org/abs/2512.07522</link>
<guid>https://arxiv.org/abs/2512.07522</guid>
<content:encoded><![CDATA[
arXiv:2512.07522v1 Announce Type: new 
Abstract: Pre-training decoder-only language models relies on vast amounts of high-quality data, yet the availability of such data is increasingly reaching its limits. While metadata is commonly used to create and curate these datasets, its potential as a direct training signal remains under-explored. We challenge this status quo and propose LIME (Linguistic Metadata Embeddings), a method that enriches token embeddings with metadata capturing syntax, semantics, and contextual properties. LIME substantially improves pre-training efficiency. Specifically, it adapts up to 56% faster to the training data distribution, while introducing only 0.01% additional parameters at negligible compute overhead. Beyond efficiency, LIME improves tokenization, leading to remarkably stronger language modeling capabilities and generative task performance. These benefits persist across model scales (500M to 2B). In addition, we develop a variant with shifted metadata, LIME+1, that can guide token generation. Given prior metadata for the next token, LIME+1 improves reasoning performance by up to 38% and arithmetic accuracy by up to 35%.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs</title>
<link>https://arxiv.org/abs/2512.07525</link>
<guid>https://arxiv.org/abs/2512.07525</guid>
<content:encoded><![CDATA[
arXiv:2512.07525v1 Announce Type: new 
Abstract: Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at https://github.com/OpenMOSS/rope_pp.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SwissGov-RSD: A Human-annotated, Cross-lingual Benchmark for Token-level Recognition of Semantic Differences Between Related Documents</title>
<link>https://arxiv.org/abs/2512.07538</link>
<guid>https://arxiv.org/abs/2512.07538</guid>
<content:encoded><![CDATA[
arXiv:2512.07538v1 Announce Type: new 
Abstract: Recognizing semantic differences across documents, especially in different languages, is crucial for text generation evaluation and multilingual content alignment. However, as a standalone task it has received little attention. We address this by introducing SwissGov-RSD, the first naturalistic, document-level, cross-lingual dataset for semantic difference recognition. It encompasses a total of 224 multi-parallel documents in English-German, English-French, and English-Italian with token-level difference annotations by human annotators. We evaluate a variety of open-source and closed source large language models as well as encoder models across different fine-tuning settings on this new benchmark. Our results show that current automatic approaches perform poorly compared to their performance on monolingual, sentence-level, and synthetic benchmarks, revealing a considerable gap for both LLMs and encoder models. We make our code and datasets publicly available.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Minimum Bayes Risk Decoding for Error Span Detection in Reference-Free Automatic Machine Translation Evaluation</title>
<link>https://arxiv.org/abs/2512.07540</link>
<guid>https://arxiv.org/abs/2512.07540</guid>
<content:encoded><![CDATA[
arXiv:2512.07540v1 Announce Type: new 
Abstract: Error Span Detection (ESD) is a subtask of automatic machine translation evaluation that localizes error spans in translations and labels their severity. State-of-the-art generative ESD methods typically decode using Maximum a Posteriori (MAP), assuming that model-estimated probabilities are perfectly correlated with similarity to human annotation. However, we observed that annotations dissimilar to the human annotation could achieve a higher model likelihood than the human annotation. We address this issue by applying Minimum Bayes Risk (MBR) decoding to generative ESD models. Specifically, we employ sentence- and span-level similarity metrics as utility functions to select candidate hypotheses based on their approximate similarity to the human annotation. Extensive experimental results show that our MBR decoding outperforms the MAP baseline at the system, sentence, and span-levels. Furthermore, to mitigate the computational cost of MBR decoding, we demonstrate that applying MBR distillation enables a standard greedy model to match MBR decoding performance, effectively eliminating the inference-time latency bottleneck.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Most over-representation of phonological features in basic vocabulary disappears when controlling for spatial and phylogenetic effects</title>
<link>https://arxiv.org/abs/2512.07543</link>
<guid>https://arxiv.org/abs/2512.07543</guid>
<content:encoded><![CDATA[
arXiv:2512.07543v1 Announce Type: new 
Abstract: The statistical over-representation of phonological features in the basic vocabulary of languages is often interpreted as reflecting potentially universal sound symbolic patterns. However, most of those results have not been tested explicitly for reproducibility and might be prone to biases in the study samples or models. Many studies on the topic do not adequately control for genealogical and areal dependencies between sampled languages, casting doubts on the robustness of the results. In this study, we test the robustness of a recent study on sound symbolism of basic vocabulary concepts which analyzed245 languages.The new sample includes data on 2864 languages from Lexibank. We modify the original model by adding statistical controls for spatial and phylogenetic dependencies between languages. The new results show that most of the previously observed patterns are not robust, and in fact many patterns disappear completely when adding the genealogical and areal controls. A small number of patterns, however, emerges as highly stable even with the new sample. Through the new analysis, we are able to assess the distribution of sound symbolism on a larger scale than previously. The study further highlights the need for testing all universal claims on language for robustness on various levels.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoCoRP: Modeling Consistent Relations between Persona and Response for Persona-based Dialogue</title>
<link>https://arxiv.org/abs/2512.07544</link>
<guid>https://arxiv.org/abs/2512.07544</guid>
<content:encoded><![CDATA[
arXiv:2512.07544v1 Announce Type: new 
Abstract: As dialogue systems become increasingly important across various domains, a key challenge in persona-based dialogue is generating engaging and context-specific interactions while ensuring the model acts with a coherent personality. However, existing persona-based dialogue datasets lack explicit relations between persona sentences and responses, which makes it difficult for models to effectively capture persona information. To address these issues, we propose MoCoRP (Modeling Consistent Relations between Persona and Response), a framework that incorporates explicit relations into language models. MoCoRP leverages an NLI expert to explicitly extract the NLI relations between persona sentences and responses, enabling the model to effectively incorporate appropriate persona information from the context into its responses. We applied this framework to pre-trained models like BART and further extended it to modern large language models (LLMs) through alignment tuning. Experimental results on the public datasets ConvAI2 and MPChat demonstrate that MoCoRP outperforms existing baselines, achieving superior persona consistency and engaging, context-aware dialogue generation. Furthermore, our model not only excels in quantitative metrics but also shows significant improvements in qualitative aspects. These results highlight the effectiveness of explicitly modeling persona-response relations in persona-based dialogue. The source codes of MoCoRP are available at https://github.com/DMCB-GIST/MoCoRP.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Performance of the SafeTerm AI-Based MedDRA Query System Against Standardised MedDRA Queries</title>
<link>https://arxiv.org/abs/2512.07552</link>
<guid>https://arxiv.org/abs/2512.07552</guid>
<content:encoded><![CDATA[
arXiv:2512.07552v1 Announce Type: new 
Abstract: In pre-market drug safety review, grouping related adverse event terms into SMQs or OCMQs is critical for signal detection. We assess the performance of SafeTerm Automated Medical Query (AMQ) on MedDRA SMQs. The AMQ is a novel quantitative artificial intelligence system that understands and processes medical terminology and automatically retrieves relevant MedDRA Preferred Terms (PTs) for a given input query, ranking them by a relevance score (0-1) using multi-criteria statistical methods. The system (SafeTerm) embeds medical query terms and MedDRA PTs in a multidimensional vector space, then applies cosine similarity, and extreme-value clustering to generate a ranked list of PTs. Validation was conducted against tier-1 SMQs (110 queries, v28.1). Precision, recall and F1 were computed at multiple similarity-thresholds, defined either manually or using an automated method. High recall (94%)) is achieved at moderate similarity thresholds, indicative of good retrieval sensitivity. Higher thresholds filter out more terms, resulting in improved precision (up to 89%). The optimal threshold (0.70)) yielded an overall recall of (48%) and precision of (45%) across all 110 queries. Restricting to narrow-term PTs achieved slightly better performance at an increased (+0.05) similarity threshold, confirming increased relatedness of narrow versus broad terms. The automatic threshold (0.66) selection prioritizes recall (0.58) to precision (0.29). SafeTerm AMQ achieves comparable, satisfactory performance on SMQs and sanitized OCMQs. It is therefore a viable supplementary method for automated MedDRA query generation, balancing recall and precision. We recommend using suitable MedDRA PT terminology in query formulation and applying the automated threshold method to optimise recall. Increasing similarity scores allows refined, narrow terms selection.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Simple Method to Enhance Pre-trained Language Models with Speech Tokens for Classification</title>
<link>https://arxiv.org/abs/2512.07571</link>
<guid>https://arxiv.org/abs/2512.07571</guid>
<content:encoded><![CDATA[
arXiv:2512.07571v1 Announce Type: new 
Abstract: This paper presents a simple method that allows to easily enhance textual pre-trained large language models with speech information, when fine-tuned for a specific classification task. A classical issue with the fusion of many embeddings from audio with text is the large length of the audio sequence compared to the text one. Our method benefits from an existing speech tokenizer trained for Audio Speech Recognition that output long sequences of tokens from a large vocabulary, making it difficult to integrate it at low cost in a large language model. By applying a simple lasso-based feature selection on multimodal Bag-of-Words representation, we retain only the most important audio tokens for the task, and adapt the language model to them with a self-supervised language modeling objective, before fine-tuning it on the downstream task. We show this helps to improve the performances compared to an unimodal model, to a bigger SpeechLM or to integrating audio via a learned representation. We show the effectiveness of our method on two recent Argumentative Fallacy Detection and Classification tasks where the use of audio was believed counterproductive, reaching state-of-the-art results. We also provide an in-depth analysis of the method, showing that even a random audio token selection helps enhancing the unimodal model. Our code is available [online](https://github.com/salocinc/EACL26SpeechTokFallacy/).
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Complementary Learning Approach for Text Classification using Large Language Models</title>
<link>https://arxiv.org/abs/2512.07583</link>
<guid>https://arxiv.org/abs/2512.07583</guid>
<content:encoded><![CDATA[
arXiv:2512.07583v1 Announce Type: new 
Abstract: In this study, we propose a structured methodology that utilizes large language models (LLMs) in a cost-efficient and parsimonious manner, integrating the strengths of scholars and machines while offsetting their respective weaknesses. Our methodology, facilitated through a chain of thought and few-shot learning prompting from computer science, extends best practices for co-author teams in qualitative research to human-machine teams in quantitative research. This allows humans to utilize abductive reasoning and natural language to interrogate not just what the machine has done but also what the human has done. Our method highlights how scholars can manage inherent weaknesses OF LLMs using careful, low-cost techniques. We demonstrate how to use the methodology to interrogate human-machine rating discrepancies for a sample of 1,934 press releases announcing pharmaceutical alliances (1990-2017).
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Metric-Fair Prompting: Treating Similar Samples Similarly</title>
<link>https://arxiv.org/abs/2512.07608</link>
<guid>https://arxiv.org/abs/2512.07608</guid>
<content:encoded><![CDATA[
arXiv:2512.07608v1 Announce Type: new 
Abstract: We introduce \emph{Metric-Fair Prompting}, a fairness-aware prompting framework that guides large language models (LLMs) to make decisions under metric-fairness constraints. In the application of multiple-choice medical question answering, each {(question, option)} pair is treated as a binary instance with label $+1$ (correct) or $-1$ (incorrect). To promote {individual fairness}~--~treating similar instances similarly~--~we compute question similarity using NLP embeddings and solve items in \emph{joint pairs of similar questions} rather than in isolation. The prompt enforces a global decision protocol: extract decisive clinical features, map each \((\text{question}, \text{option})\) to a score $f(x)$ that acts as confidence, and impose a Lipschitz-style constraint so that similar inputs receive similar scores and, hence, consistent outputs. Evaluated on the {MedQA (US)} benchmark, Metric-Fair Prompting is shown to improve performance over standard single-item prompting, demonstrating that fairness-guided, confidence-oriented reasoning can enhance LLM accuracy on high-stakes clinical multiple-choice questions.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PCMind-2.1-Kaiyuan-2B Technical Report</title>
<link>https://arxiv.org/abs/2512.07612</link>
<guid>https://arxiv.org/abs/2512.07612</guid>
<content:encoded><![CDATA[
arXiv:2512.07612v1 Announce Type: new 
Abstract: The rapid advancement of Large Language Models (LLMs) has resulted in a significant knowledge gap between the open-source community and industry, primarily because the latter relies on closed-source, high-quality data and training recipes. To address this, we introduce PCMind-2.1-Kaiyuan-2B, a fully open-source 2-billion-parameter model focused on improving training efficiency and effectiveness under resource constraints. Our methodology includes three key innovations: a Quantile Data Benchmarking method for systematically comparing heterogeneous open-source datasets and providing insights on data mixing strategies; a Strategic Selective Repetition scheme within a multi-phase paradigm to effectively leverage sparse, high-quality data; and a Multi-Domain Curriculum Training policy that orders samples by quality. Supported by a highly optimized data preprocessing pipeline and architectural modifications for FP16 stability, Kaiyuan-2B achieves performance competitive with state-of-the-art fully open-source models, demonstrating practical and scalable solutions for resource-limited pretraining. We release all assets (including model weights, data, and code) under Apache 2.0 license at https://huggingface.co/thu-pacman/PCMind-2.1-Kaiyuan-2B.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Code Graphs and Large Language Models for Better Code Understanding</title>
<link>https://arxiv.org/abs/2512.07666</link>
<guid>https://arxiv.org/abs/2512.07666</guid>
<content:encoded><![CDATA[
arXiv:2512.07666v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable performance in code intelligence tasks such as code generation, summarization, and translation. However, their reliance on linearized token sequences limits their ability to understand the structural semantics of programs. While prior studies have explored graphaugmented prompting and structure-aware pretraining, they either suffer from prompt length constraints or require task-specific architectural changes that are incompatible with large-scale instructionfollowing LLMs. To address these limitations, this paper proposes CGBridge, a novel plug-and-play method that enhances LLMs with Code Graph information through an external, trainable Bridge module. CGBridge first pre-trains a code graph encoder via selfsupervised learning on a large-scale dataset of 270K code graphs to learn structural code semantics. It then trains an external module to bridge the modality gap among code, graph, and text by aligning their semantics through cross-modal attention mechanisms. Finally, the bridge module generates structure-informed prompts, which are injected into a frozen LLM, and is fine-tuned for downstream code intelligence tasks. Experiments show that CGBridge achieves notable improvements over both the original model and the graphaugmented prompting method. Specifically, it yields a 16.19% and 9.12% relative gain in LLM-as-a-Judge on code summarization, and a 9.84% and 38.87% relative gain in Execution Accuracy on code translation. Moreover, CGBridge achieves over 4x faster inference than LoRA-tuned models, demonstrating both effectiveness and efficiency in structure-aware code understanding.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Large Language Models Do Not Work: Online Incivility Prediction through Graph Neural Networks</title>
<link>https://arxiv.org/abs/2512.07684</link>
<guid>https://arxiv.org/abs/2512.07684</guid>
<content:encoded><![CDATA[
arXiv:2512.07684v1 Announce Type: new 
Abstract: Online incivility has emerged as a widespread and persistent problem in digital communities, imposing substantial social and psychological burdens on users. Although many platforms attempt to curb incivility through moderation and automated detection, the performance of existing approaches often remains limited in both accuracy and efficiency. To address this challenge, we propose a Graph Neural Network (GNN) framework for detecting three types of uncivil behavior (i.e., toxicity, aggression, and personal attacks) within the English Wikipedia community. Our model represents each user comment as a node, with textual similarity between comments defining the edges, allowing the network to jointly learn from both linguistic content and relational structures among comments. We also introduce a dynamically adjusted attention mechanism that adaptively balances nodal and topological features during information aggregation. Empirical evaluations demonstrate that our proposed architecture outperforms 12 state-of-the-art Large Language Models (LLMs) across multiple metrics while requiring significantly lower inference cost. These findings highlight the crucial role of structural context in detecting online incivility and address the limitations of text-only LLM paradigms in behavioral prediction. All datasets and comparative outputs will be publicly available in our repository to support further research and reproducibility.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HalluShift++: Bridging Language and Vision through Internal Representation Shifts for Hierarchical Hallucinations in MLLMs</title>
<link>https://arxiv.org/abs/2512.07687</link>
<guid>https://arxiv.org/abs/2512.07687</guid>
<content:encoded><![CDATA[
arXiv:2512.07687v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in vision-language understanding tasks. While these models often produce linguistically coherent output, they often suffer from hallucinations, generating descriptions that are factually inconsistent with the visual content, potentially leading to adverse consequences. Therefore, the assessment of hallucinations in MLLM has become increasingly crucial in the model development process. Contemporary methodologies predominantly depend on external LLM evaluators, which are themselves susceptible to hallucinations and may present challenges in terms of domain adaptation. In this study, we propose the hypothesis that hallucination manifests as measurable irregularities within the internal layer dynamics of MLLMs, not merely due to distributional shifts but also in the context of layer-wise analysis of specific assumptions. By incorporating such modifications, \textsc{\textsc{HalluShift++}} broadens the efficacy of hallucination detection from text-based large language models (LLMs) to encompass multimodal scenarios. Our codebase is available at https://github.com/C0mRD/HalluShift_Plus.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Generation of Custom MedDRA Queries Using SafeTerm Medical Map</title>
<link>https://arxiv.org/abs/2512.07694</link>
<guid>https://arxiv.org/abs/2512.07694</guid>
<content:encoded><![CDATA[
arXiv:2512.07694v1 Announce Type: new 
Abstract: In pre-market drug safety review, grouping related adverse event terms into standardised MedDRA queries or the FDA Office of New Drugs Custom Medical Queries (OCMQs) is critical for signal detection. We present a novel quantitative artificial intelligence system that understands and processes medical terminology and automatically retrieves relevant MedDRA Preferred Terms (PTs) for a given input query, ranking them by a relevance score using multi-criteria statistical methods. The system (SafeTerm) embeds medical query terms and MedDRA PTs in a multidimensional vector space, then applies cosine similarity and extreme-value clustering to generate a ranked list of PTs. Validation was conducted against the FDA OCMQ v3.0 (104 queries), restricted to valid MedDRA PTs. Precision, recall and F1 were computed across similarity-thresholds. High recall (>95%) is achieved at moderate thresholds. Higher thresholds improve precision (up to 86%). The optimal threshold (~0.70 - 0.75) yielded recall ~50% and precision ~33%. Narrow-term PT subsets performed similarly but required slightly higher similarity thresholds. The SafeTerm AI-driven system provides a viable supplementary method for automated MedDRA query generation. A similarity threshold of ~0.60 is recommended initially, with increased thresholds for refined term selection.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mary, the Cheeseburger-Eating Vegetarian: Do LLMs Recognize Incoherence in Narratives?</title>
<link>https://arxiv.org/abs/2512.07777</link>
<guid>https://arxiv.org/abs/2512.07777</guid>
<content:encoded><![CDATA[
arXiv:2512.07777v1 Announce Type: new 
Abstract: Leveraging a dataset of paired narratives, we investigate the extent to which large language models (LLMs) can reliably separate incoherent and coherent stories. A probing study finds that LLMs' internal representations can reliably identify incoherent narratives. However, LLMs generate responses to rating questions that fail to satisfactorily separate the coherent and incoherent narratives across several prompt variations, hinting at a gap in LLM's understanding of storytelling. The reasoning LLMs tested do not eliminate these deficits, indicating that thought strings may not be able to fully address the discrepancy between model internal state and behavior. Additionally, we find that LLMs appear to be more sensitive to incoherence resulting from an event that violates the setting (e.g., a rainy day in the desert) than to incoherence arising from a character violating an established trait (e.g., Mary, a vegetarian, later orders a cheeseburger), suggesting that LLMs may rely more on prototypical world knowledge than building meaning-based narrative coherence. The consistent asymmetry found in our results suggests that LLMs do not have a complete grasp on narrative coherence.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models</title>
<link>https://arxiv.org/abs/2512.07783</link>
<guid>https://arxiv.org/abs/2512.07783</guid>
<content:encoded><![CDATA[
arXiv:2512.07783v1 Announce Type: new 
Abstract: Recent reinforcement learning (RL) techniques have yielded impressive reasoning improvements in language models, yet it remains unclear whether post-training truly extends a model's reasoning ability beyond what it acquires during pre-training. A central challenge is the lack of control in modern training pipelines: large-scale pre-training corpora are opaque, mid-training is often underexamined, and RL objectives interact with unknown prior knowledge in complex ways. To resolve this ambiguity, we develop a fully controlled experimental framework that isolates the causal contributions of pre-training, mid-training, and RL-based post-training. Our approach employs synthetic reasoning tasks with explicit atomic operations, parseable step-by-step reasoning traces, and systematic manipulation of training distributions. We evaluate models along two axes: extrapolative generalization to more complex compositions and contextual generalization across surface contexts. Using this framework, we reconcile competing views on RL's effectiveness. We show that: 1) RL produces true capability gains (pass@128) only when pre-training leaves sufficient headroom and when RL data target the model's edge of competence, tasks at the boundary that are difficult but not yet out of reach. 2) Contextual generalization requires minimal yet sufficient pre-training exposure, after which RL can reliably transfer. 3) Mid-training significantly enhances performance under fixed compute compared with RL only, demonstrating its central but underexplored role in training pipelines. 4) Process-level rewards reduce reward hacking and improve reasoning fidelity. Together, these results clarify the interplay between pre-training, mid-training, and RL, offering a foundation for understanding and improving reasoning LM training strategies.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support</title>
<link>https://arxiv.org/abs/2512.07801</link>
<guid>https://arxiv.org/abs/2512.07801</guid>
<content:encoded><![CDATA[
arXiv:2512.07801v1 Announce Type: new 
Abstract: LLM-based agents are rapidly being plugged into expert decision-support, yet in messy, high-stakes settings they rarely make the team smarter: human-AI teams often underperform the best individual, experts oscillate between verification loops and over-reliance, and the promised complementarity does not materialise. We argue this is not just a matter of accuracy, but a fundamental gap in how we conceive AI assistance: expert decisions are made through collaborative cognitive processes where mental models, goals, and constraints are continually co-constructed, tested, and revised between human and AI. We propose Collaborative Causal Sensemaking (CCS) as a research agenda and organizing framework for decision-support agents: systems designed as partners in cognitive work, maintaining evolving models of how particular experts reason, helping articulate and revise goals, co-constructing and stress-testing causal hypotheses, and learning from the outcomes of joint decisions so that both human and agent improve over time. We sketch challenges around training ecologies that make collaborative thinking instrumentally valuable, representations and interaction protocols for co-authored models, and evaluation centred on trust and complementarity. These directions can reframe MAS research around agents that participate in collaborative sensemaking and act as AI teammates that think with their human partners.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Generalisation Results Generalise?</title>
<link>https://arxiv.org/abs/2512.07832</link>
<guid>https://arxiv.org/abs/2512.07832</guid>
<content:encoded><![CDATA[
arXiv:2512.07832v1 Announce Type: new 
Abstract: A large language model's (LLM's) out-of-distribution (OOD) generalisation ability is crucial to its deployment. Previous work assessing LLMs' generalisation performance, however, typically focuses on a single out-of-distribution dataset. This approach may fail to precisely evaluate the capabilities of the model, as the data shifts encountered once a model is deployed are much more diverse. In this work, we investigate whether OOD generalisation results generalise. More specifically, we evaluate a model's performance across multiple OOD testsets throughout a finetuning run; we then evaluate the partial correlation of performances across these testsets, regressing out in-domain performance. This allows us to assess how correlated are generalisation performances once in-domain performance is controlled for. Analysing OLMo2 and OPT, we observe no overarching trend in generalisation results: the existence of a positive or negative correlation between any two OOD testsets depends strongly on the specific choice of model analysed.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Generated Compromises for Coalition Formation: Modeling, Simulation, and a Textual Case Study</title>
<link>https://arxiv.org/abs/2512.05983</link>
<guid>https://arxiv.org/abs/2512.05983</guid>
<content:encoded><![CDATA[
arXiv:2512.05983v1 Announce Type: cross 
Abstract: The challenge of finding compromises between agent proposals is fundamental to AI sub-fields such as argumentation, mediation, and negotiation. Building on this tradition, Elkind et al. (2021) introduced a process for coalition formation that seeks majority-supported proposals preferable to the status quo, using a metric space where each agent has an ideal point. The crucial step in this iterative process involves identifying compromise proposals around which agent coalitions can unite. How to effectively find such compromise proposals, however, remains an open question. We address this gap by formalizing a holistic model that encompasses agent bounded rationality and uncertainty and developing AI models to generate such compromise proposals. We focus on the domain of collaboratively writing text documents -- e.g., to enable the democratic creation of a community constitution. We apply NLP (Natural Language Processing) techniques and utilize LLMs (Large Language Models) to create a semantic metric space for text and develop algorithms to suggest suitable compromise points. To evaluate the effectiveness of our algorithms, we simulate various coalition formation processes and demonstrate the potential of AI to facilitate large-scale democratic text editing, such as collaboratively drafting a constitution, an area where traditional tools are limited.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KidSpeak: A General Multi-purpose LLM for Kids' Speech Recognition and Screening</title>
<link>https://arxiv.org/abs/2512.05994</link>
<guid>https://arxiv.org/abs/2512.05994</guid>
<content:encoded><![CDATA[
arXiv:2512.05994v1 Announce Type: cross 
Abstract: With the rapid advancement of conversational and diffusion-based AI, there is a growing adoption of AI in educational services, ranging from grading and assessment tools to personalized learning systems that provide targeted support for students. However, this adaptability has yet to fully extend to the domain of children's speech, where existing models often fail due to their reliance on datasets designed for clear, articulate adult speech. Children, particularly those in early developmental stages or with speech and language pathologies, present unique challenges that current AI models and datasets are ill-equipped to handle. To address this, we introduce KidSpeak, a multi-task speech-enhanced Foundation Model capable of both generative and discriminative tasks specifically tailored to children's speech patterns. Our framework employs a two-stage training process that incorporates phonetic knowledge into the speech encoder, achieving an average accuracy of 87% across four separate tasks. Furthermore, recognizing the limitations of scalable human annotation and existing speech alignment tools, we propose the Flexible and Automatic Speech Aligner (FASA) and leverage the method to construct high quality datasets for training and evaluation. This novel alignment tool significantly improves the quality of aligned children's speech from noisy data, enhancing data quality by 13.6x compared to human annotations, as demonstrated on the CHILDES dataset. To the best of our knowledge, KidSpeak and FASA represent the first comprehensive solution designed for speech and language therapy in children, offering both a multi-purpose speech LLM and a robust alignment tool.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Small Language Models Reshape Higher Education: Courses, Textbooks, and Teaching</title>
<link>https://arxiv.org/abs/2512.06001</link>
<guid>https://arxiv.org/abs/2512.06001</guid>
<content:encoded><![CDATA[
arXiv:2512.06001v1 Announce Type: cross 
Abstract: While large language models (LLMs) have introduced novel paradigms in science and education, their adoption in higher education is constrained by inherent limitations. These include a tendency to produce inaccuracies and high computational requirements, which compromise the strict demands for accurate and reliable knowledge essential in higher education. Small language models (MiniLMs), by contrast, offer distinct advantages in professional education due to their lightweight nature and precise retrieval capabilities. This research takes "Atmospheric Physics" as an example. We established a specialized corpus and image repository by gathering over 550,000 full-text PDFs from over 130 international well-respected journals in Earth and environmental science. From this collection, we extracted over 100 million high-quality sentence-level corpus and more than 3 million high-resolution academic images. Using MiniLMs, these resources were organized into a high-dimensional vector library for precise retrieval and efficient utilization of extensive educational content. Consequently, we systematically redesigned the courses, textbooks, and teaching strategies for "Atmospheric Physics" based on MiniLMs. The course is designed as a "interdisciplinary-frontier" system, breaking down traditional boundaries between atmospheric science, space science, hydrology, and remote sensing. Teaching materials are transformed from static, lagging text formats into a dynamic digital resource library powered by MiniLM. For teaching methods, we have designed a question-based learning pathway. This paradigm promotes a shift from passive knowledge transfer to active cognitive development. Consequently, this MiniLM-driven "Atmospheric Physics" course demonstrates a specific avenue for "AI for education".
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Road of Adaptive AI for Precision in Cybersecurity</title>
<link>https://arxiv.org/abs/2512.06048</link>
<guid>https://arxiv.org/abs/2512.06048</guid>
<content:encoded><![CDATA[
arXiv:2512.06048v1 Announce Type: cross 
Abstract: Cybersecurity's evolving complexity presents unique challenges and opportunities for AI research and practice. This paper shares key lessons and insights from designing, building, and operating production-grade GenAI pipelines in cybersecurity, with a focus on the continual adaptation required to keep pace with ever-shifting knowledge bases, tooling, and threats. Our goal is to provide an actionable perspective for AI practitioners and industry stakeholders navigating the frontier of GenAI for cybersecurity, with particular attention to how different adaptation mechanisms complement each other in end-to-end systems. We present practical guidance derived from real-world deployments, propose best practices for leveraging retrieval- and model-level adaptation, and highlight open research directions for making GenAI more robust, precise, and auditable in cyber defense.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ARCANE: A Multi-Agent Framework for Interpretable and Configurable Alignment</title>
<link>https://arxiv.org/abs/2512.06196</link>
<guid>https://arxiv.org/abs/2512.06196</guid>
<content:encoded><![CDATA[
arXiv:2512.06196v1 Announce Type: cross 
Abstract: As agents based on large language models are increasingly deployed to long-horizon tasks, maintaining their alignment with stakeholder preferences becomes critical. Effective alignment in such settings requires reward models that are interpretable so that stakeholders can understand and audit model objectives. Moreover, reward models must be capable of steering agents at interaction time, allowing preference shifts to be incorporated without retraining. We introduce ARCANE, a framework that frames alignment as a multi-agent collaboration problem that dynamically represents stakeholder preferences as natural-language rubrics: weighted sets of verifiable criteria that can be generated on-the-fly from task context. Inspired by utility theory, we formulate rubric learning as a reconstruction problem and apply a regularized Group-Sequence Policy Optimization (GSPO) procedure that balances interpretability, faithfulness, and computational efficiency. Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use. The learned rubrics produce compact, legible evaluations and enable configurable trade-offs (e.g., correctness vs. conciseness) without retraining. Our results show that rubric-based reward models offer a promising path toward interpretable, test-time adaptive alignment for complex, long-horizon AI systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On measuring grounding and generalizing grounding problems</title>
<link>https://arxiv.org/abs/2512.06205</link>
<guid>https://arxiv.org/abs/2512.06205</guid>
<content:encoded><![CDATA[
arXiv:2512.06205v1 Announce Type: cross 
Abstract: The symbol grounding problem asks how tokens like cat can be about cats, as opposed to mere shapes manipulated in a calculus. We recast grounding from a binary judgment into an audit across desiderata, each indexed by an evaluation tuple (context, meaning type, threat model, reference distribution): authenticity (mechanisms reside inside the agent and, for strong claims, were acquired through learning or evolution); preservation (atomic meanings remain intact); faithfulness, both correlational (realized meanings match intended ones) and etiological (internal mechanisms causally contribute to success); robustness (graceful degradation under declared perturbations); compositionality (the whole is built systematically from the parts). We apply this framework to four grounding modes (symbolic; referential; vectorial; relational) and three case studies: model-theoretic semantics achieves exact composition but lacks etiological warrant; large language models show correlational fit and local robustness for linguistic tasks, yet lack selection-for-success on world tasks without grounded interaction; human language meets the desiderata under strong authenticity through evolutionary and developmental acquisition. By operationalizing a philosophical inquiry about representation, we equip philosophers of science, computer scientists, linguists, and mathematicians with a common language and technical framework for systematic investigation of grounding and meaning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Distance Distracts: Representation Distance Bias in BT-Loss for Reward Models</title>
<link>https://arxiv.org/abs/2512.06343</link>
<guid>https://arxiv.org/abs/2512.06343</guid>
<content:encoded><![CDATA[
arXiv:2512.06343v1 Announce Type: cross 
Abstract: Reward models are central to Large Language Model (LLM) alignment within the framework of RLHF. The standard objective used in reward modeling is the Bradley-Terry (BT) loss, which learns from pairwise data consisting of a pair of chosen and rejected responses. In this work, we analyze the per-sample gradient of BT-loss and show that its norm scales with two distinct components: (1) the difference in predicted rewards between chosen and rejected responses, which reflects the prediction error, and critically, (2) representation distance between the pair measured in the output space of the final layer. While the first term captures the intended training signal, we show that the second term can significantly impact the update magnitude and misalign learning. Specifically, pairs with small representation distance often receive vanishingly weak updates, even when misranked, while pairs with large distance receive disproportionately strong updates. This leads to gradients from large-distance pairs to overshadow those from small-distance pairs, where fine-grained distinctions are especially important. To overcome this limitation, we propose NormBT, an adaptive pair-wise normalization scheme that balances representation-driven effects and focuses learning signals on prediction error. NormBT is a lightweight, drop-in integration to BT loss with negligible overhead. Across various LLM backbones and datasets, NormBT improves reward model performance consistently, with notable gains of over 5% on the Reasoning category of RewardBench, which contains numerous small-distance pairs. This work reveals a key limitation in the widely used BT objective and provides a simple, effective correction.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why They Disagree: Decoding Differences in Opinions about AI Risk on the Lex Fridman Podcast</title>
<link>https://arxiv.org/abs/2512.06350</link>
<guid>https://arxiv.org/abs/2512.06350</guid>
<content:encoded><![CDATA[
arXiv:2512.06350v1 Announce Type: cross 
Abstract: The emergence of transformative technologies often surfaces deep societal divisions, nowhere more evident than in contemporary debates about artificial intelligence (AI). A striking feature of these divisions is that they persist despite shared interests in ensuring that AI benefits humanity and avoiding catastrophic outcomes. This paper analyzes contemporary debates about AI risk, parsing the differences between the "doomer" and "boomer" perspectives into definitional, factual, causal, and moral premises to identify key points of contention. We find that differences in perspectives about existential risk ("X-risk") arise fundamentally from differences in causal premises about design vs. emergence in complex systems, while differences in perspectives about employment risks ("E-risks") pertain to different causal premises about the applicability of past theories (evolution) vs their inapplicability (revolution). Disagreements about these two forms of AI risk appear to share two properties: neither involves significant disagreements on moral values and both can be described in terms of differing views on the extent of boundedness of human rationality. Our approach to analyzing reasoning chains at scale, using an ensemble of LLMs to parse textual data, can be applied to identify key points of contention in debates about risk to the public in any arena.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-Upgraded Graph Reinforcement Learning for Carbon-Aware Job Scheduling in Smart Manufacturing</title>
<link>https://arxiv.org/abs/2512.06351</link>
<guid>https://arxiv.org/abs/2512.06351</guid>
<content:encoded><![CDATA[
arXiv:2512.06351v1 Announce Type: cross 
Abstract: This paper presents \textsc{Luca}, a \underline{l}arge language model (LLM)-\underline{u}pgraded graph reinforcement learning framework for \underline{c}arbon-\underline{a}ware flexible job shop scheduling. \textsc{Luca} addresses the challenges of dynamic and sustainable scheduling in smart manufacturing systems by integrating a graph neural network and an LLM, guided by a carefully designed in-house prompting strategy, to produce a fused embedding that captures both structural characteristics and contextual semantics of the latest scheduling state. This expressive embedding is then processed by a deep reinforcement learning policy network, which generates real-time scheduling decisions optimized for both makespan and carbon emission objectives. To support sustainability goals, \textsc{Luca} incorporates a dual-objective reward function that encourages both energy efficiency and scheduling timeliness. Experimental results on both synthetic and public datasets demonstrate that \textsc{Luca} consistently outperforms comparison algorithms. For instance, on the synthetic dataset, it achieves an average of 4.1\% and up to 12.2\% lower makespan compared to the best-performing comparison algorithm while maintaining the same emission level. On public datasets, additional gains are observed for both makespan and emission. These results demonstrate that \textsc{Luca} is effective and practical for carbon-aware scheduling in smart manufacturing.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Less Is More for Multi-Step Logical Reasoning of LLM Generalisation Under Rule Removal, Paraphrasing, and Compression</title>
<link>https://arxiv.org/abs/2512.06393</link>
<guid>https://arxiv.org/abs/2512.06393</guid>
<content:encoded><![CDATA[
arXiv:2512.06393v1 Announce Type: cross 
Abstract: Large language models (LLMs) excel across many natural language tasks, yet their generalisation to structural perturbations in logical contexts remains poorly understood. We introduce a controlled evaluation framework that probes reasoning reliability through four targeted stress tests: (1) rule deletion, removing either redundant or essential rules from a multi-step inference chain; (2) contradictory evidence injection; (3) logic-preserving rewrites generated through several families of equivalence laws (contrapositive, double negation, implication, De Morgan, identity, and commutativity); and (4) multi-law equivalence stacking that introduces 2-5 simultaneous logical transformations.
  Across three representative model families: BERT, Qwen2, and LLaMA-like models. Our experiments reveal a strikingly consistent pattern: all models achieve perfect accuracy on the base tasks and remain fully generalise to redundant rule deletion and all equivalence-based rewrites (single or multi-law), but fail sharply under essential rule deletion (dropping to 25% accuracy) and collapse completely in the presence of explicit contradictions (0% accuracy). These results demonstrate that LLMs possess stable invariance to semantic-preserving logical transformations, yet remain fundamentally brittle to missing or conflicting evidence. Our framework provides a clean diagnostic tool for isolating such reasoning failure modes and highlights persistent gaps in the logical generalisation abilities of current LLMs.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Training Dynamics in Scale-wise Autoregressive Generation</title>
<link>https://arxiv.org/abs/2512.06421</link>
<guid>https://arxiv.org/abs/2512.06421</guid>
<content:encoded><![CDATA[
arXiv:2512.06421v1 Announce Type: cross 
Abstract: Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Fast and Effective Solution to the Problem of Look-ahead Bias in LLMs</title>
<link>https://arxiv.org/abs/2512.06607</link>
<guid>https://arxiv.org/abs/2512.06607</guid>
<content:encoded><![CDATA[
arXiv:2512.06607v1 Announce Type: cross 
Abstract: Applying LLMs to predictive tasks in finance is challenging due to look-ahead bias resulting from their training on long time-series data. This precludes the backtests typically employed in finance since retraining frontier models from scratch with a specific knowledge cutoff is prohibitive. In this paper, we introduce a fast, effective, and low-cost alternative. Our method guides generation at inference time by adjusting the logits of a large base model using a pair of smaller, specialized models -- one fine-tuned on information to be forgotten and another on information to be retained. We demonstrate that our method effectively removes both verbatim and semantic knowledge, corrects biases, and outperforms prior methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Index-based Approach for Efficient and Effective Web Content Extraction</title>
<link>https://arxiv.org/abs/2512.06641</link>
<guid>https://arxiv.org/abs/2512.06641</guid>
<content:encoded><![CDATA[
arXiv:2512.06641v1 Announce Type: cross 
Abstract: As web agents (e.g., Deep Research) routinely consume massive volumes of web pages to gather and analyze information, LLM context management -- under large token budgets and low signal density -- emerges as a foundational, high-importance, and technically challenging problem for agentic and RAG pipelines. Existing solutions for extracting relevant content are inadequate: generative extraction models suffer from high latency, rule-based heuristics lack adaptability, and chunk-and-rerank methods are blind to webpage structure. To overcome these issues, we introduce Index-based Web Content Extraction to reframe the extraction process from slow, token-by-token generation into a highly efficient, discriminative task of index prediction, achieving both effectiveness and efficiency. We partition HTML into structure-aware, addressable segments, and extract only the positional indices of content relevant to a given query. This method decouples extraction latency from content length, enabling rapid, query-relevant extraction. We first evaluate our method as a post-retrieval processing component within an RAG QA system and find that it improves QA accuracy. Then we directly measure its match rate with the target content in two scenarios: main content extraction (ME) and query-relevant extraction (QE). Experimental results show that our method outperforms existing works in both accuracy and speed, effectively bridging the gap between LLMs and the vast webpages.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Look Twice before You Leap: A Rational Agent Framework for Localized Adversarial Anonymization</title>
<link>https://arxiv.org/abs/2512.06713</link>
<guid>https://arxiv.org/abs/2512.06713</guid>
<content:encoded><![CDATA[
arXiv:2512.06713v1 Announce Type: cross 
Abstract: Current LLM-based text anonymization frameworks usually rely on remote API services from powerful LLMs, which creates an inherent "privacy paradox": users must somehow disclose data to untrusted third parties for superior privacy preservation. Moreover, directly migrating these frameworks to local small-scale models (LSMs) offers a suboptimal solution with catastrophic collapse in utility based on our core findings. Our work argues that this failure stems not merely from the capability deficits of LSMs, but from the inherent irrationality of the greedy adversarial strategies employed by current state-of-the-art (SoTA) methods. We model the anonymization process as a trade-off between Marginal Privacy Gain (MPG) and Marginal Utility Cost (MUC), and demonstrate that greedy strategies inevitably drift into an irrational state. To address this, we propose Rational Localized Adversarial Anonymization (RLAA), a fully localized and training-free framework featuring an Attacker-Arbitrator-Anonymizer (A-A-A) architecture. RLAA introduces an arbitrator that acts as a rationality gatekeeper, validating the attacker's inference to filter out feedback providing negligible benefits on privacy preservation. This mechanism enforces a rational early-stopping criterion, and systematically prevents utility collapse. Extensive experiments on different datasets demonstrate that RLAA achieves the best privacy-utility trade-off, and in some cases even outperforms SoTA on the Pareto principle. Our code and datasets will be released upon acceptance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cognitive Control Architecture (CCA): A Lifecycle Supervision Framework for Robustly Aligned AI Agents</title>
<link>https://arxiv.org/abs/2512.06716</link>
<guid>https://arxiv.org/abs/2512.06716</guid>
<content:encoded><![CDATA[
arXiv:2512.06716v1 Announce Type: cross 
Abstract: Autonomous Large Language Model (LLM) agents exhibit significant vulnerability to Indirect Prompt Injection (IPI) attacks. These attacks hijack agent behavior by polluting external information sources, exploiting fundamental trade-offs between security and functionality in existing defense mechanisms. This leads to malicious and unauthorized tool invocations, diverting agents from their original objectives. The success of complex IPIs reveals a deeper systemic fragility: while current defenses demonstrate some effectiveness, most defense architectures are inherently fragmented. Consequently, they fail to provide full integrity assurance across the entire task execution pipeline, forcing unacceptable multi-dimensional compromises among security, functionality, and efficiency. Our method is predicated on a core insight: no matter how subtle an IPI attack, its pursuit of a malicious objective will ultimately manifest as a detectable deviation in the action trajectory, distinct from the expected legitimate plan. Based on this, we propose the Cognitive Control Architecture (CCA), a holistic framework achieving full-lifecycle cognitive supervision. CCA constructs an efficient, dual-layered defense system through two synergistic pillars: (i) proactive and preemptive control-flow and data-flow integrity enforcement via a pre-generated "Intent Graph"; and (ii) an innovative "Tiered Adjudicator" that, upon deviation detection, initiates deep reasoning based on multi-dimensional scoring, specifically designed to counter complex conditional attacks. Experiments on the AgentDojo benchmark substantiate that CCA not only effectively withstands sophisticated attacks that challenge other advanced defense methods but also achieves uncompromised security with notable efficiency and robustness, thereby reconciling the aforementioned multi-dimensional trade-off.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProAgent: Harnessing On-Demand Sensory Contexts for Proactive LLM Agent Systems</title>
<link>https://arxiv.org/abs/2512.06721</link>
<guid>https://arxiv.org/abs/2512.06721</guid>
<content:encoded><![CDATA[
arXiv:2512.06721v1 Announce Type: cross 
Abstract: Large Language Model (LLM) agents are emerging to transform daily life. However, existing LLM agents primarily follow a reactive paradigm, relying on explicit user instructions to initiate services, which increases both physical and cognitive workload. In this paper, we propose ProAgent, the first end-to-end proactive agent system that harnesses massive sensory contexts and LLM reasoning to deliver proactive assistance. ProAgent first employs a proactive-oriented context extraction approach with on-demand tiered perception to continuously sense the environment and derive hierarchical contexts that incorporate both sensory and persona cues. ProAgent then adopts a context-aware proactive reasoner to map these contexts to user needs and tool calls, providing proactive assistance. We implement ProAgent on Augmented Reality (AR) glasses with an edge server and extensively evaluate it on a real-world testbed, a public dataset, and through a user study. Results show that ProAgent achieves up to 33.4% higher proactive prediction accuracy, 16.8% higher tool-calling F1 score, and notable improvements in user satisfaction over state-of-the-art baselines, marking a significant step toward proactive assistants. A video demonstration of ProAgent is available at https://youtu.be/pRXZuzvrcVs.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Role of Entropy in Visual Grounding: Analysis and Optimization</title>
<link>https://arxiv.org/abs/2512.06726</link>
<guid>https://arxiv.org/abs/2512.06726</guid>
<content:encoded><![CDATA[
arXiv:2512.06726v1 Announce Type: cross 
Abstract: Recent advances in fine-tuning multimodal large language models (MLLMs) using reinforcement learning have achieved remarkable progress, particularly with the introduction of various entropy control techniques. However, the role and characteristics of entropy in perception-oriented tasks like visual grounding, as well as effective strategies for controlling it, remain largely unexplored. To address this issue, we focus on the visual grounding task and analyze the role and characteristics of entropy in comparison to reasoning tasks. Building on these findings, we introduce ECVGPO (Entropy Control Visual Grounding Policy Optimization), an interpretable algorithm designed for effective entropy regulation. Through entropy control, the trade-off between exploration and exploitation is better balanced. Experiments show that ECVGPO achieves broad improvements across various benchmarks and models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Arc Gradient Descent: A Mathematically Derived Reformulation of Gradient Descent with Phase-Aware, User-Controlled Step Dynamics</title>
<link>https://arxiv.org/abs/2512.06737</link>
<guid>https://arxiv.org/abs/2512.06737</guid>
<content:encoded><![CDATA[
arXiv:2512.06737v1 Announce Type: cross 
Abstract: The paper presents the formulation, implementation, and evaluation of the ArcGD optimiser. The evaluation is conducted initially on a non-convex benchmark function and subsequently on a real-world ML dataset. The initial comparative study using the Adam optimiser is conducted on a stochastic variant of the highly non-convex and notoriously challenging Rosenbrock function, renowned for its narrow, curved valley, across dimensions ranging from 2D to 1000D and an extreme case of 50,000D. Two configurations were evaluated to eliminate learning-rate bias: (i) both using ArcGD's effective learning rate and (ii) both using Adam's default learning rate. ArcGD consistently outperformed Adam under the first setting and, although slower under the second, achieved super ior final solutions in most cases. In the second evaluation, ArcGD is evaluated against state-of-the-art optimizers (Adam, AdamW, Lion, SGD) on the CIFAR-10 image classification dataset across 8 diverse MLP architectures ranging from 1 to 5 hidden layers. ArcGD achieved the highest average test accuracy (50.7%) at 20,000 iterations, outperforming AdamW (46.6%), Adam (46.8%), SGD (49.6%), and Lion (43.4%), winning or tying on 6 of 8 architectures. Notably, while Adam and AdamW showed strong early convergence at 5,000 iterations, but regressed with extended training, whereas ArcGD continued improving, demonstrating generalization and resistance to overfitting without requiring early stopping tuning. Strong performance on geometric stress tests and standard deep-learning benchmarks indicates broad applicability, highlighting the need for further exploration. Moreover, it is also shown that a variant of ArcGD can be interpreted as a special case of the Lion optimiser, highlighting connections between the inherent mechanisms of such optimisation methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMDuet2: Enhancing Proactive Interaction of Video MLLMs with Multi-Turn Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.06810</link>
<guid>https://arxiv.org/abs/2512.06810</guid>
<content:encoded><![CDATA[
arXiv:2512.06810v1 Announce Type: cross 
Abstract: Recent advances in video multimodal large language models (Video MLLMs) have significantly enhanced video understanding and multi-modal interaction capabilities. While most existing systems operate in a turn-based manner where the model can only reply after user turns, proactively deciding when to reply during video playback presents a promising yet challenging direction for real-time applications. In this work, we propose a novel text-to-text approach to proactive interaction, where the model autonomously determines whether to respond or remain silent at each turn based on dialogue history and visual context up to current frame of an streaming video. To overcome difficulties in previous methods such as manually tuning response decision thresholds and annotating precise reply times, we introduce a multi-turn RL based training method that encourages timely and accurate responses without requiring precise response time annotations. We train our model MMDuet2 on a dataset of 52k videos with two types of dialogues via SFT and RL. Experimental results demonstrate that MMDuet2 outperforms existing proactive Video MLLM baselines in response timing and quality, achieving state-of-the-art performance on the ProactiveVideoQA benchmark.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Less Is More, but Where? Dynamic Token Compression via LLM-Guided Keyframe Prior</title>
<link>https://arxiv.org/abs/2512.06866</link>
<guid>https://arxiv.org/abs/2512.06866</guid>
<content:encoded><![CDATA[
arXiv:2512.06866v1 Announce Type: cross 
Abstract: Recent advances in Video Large Language Models (VLLMs) have achieved remarkable video understanding capabilities, yet face critical efficiency bottlenecks due to quadratic computational growth with lengthy visual token sequences of long videos. While existing keyframe sampling methods can improve temporal modeling efficiency, additional computational cost is introduced before feature encoding, and the binary frame selection paradigm is found suboptimal. Therefore, in this work, we propose Dynamic Token compression via LLM-guided Keyframe prior (DyToK), a training-free paradigm that enables dynamic token compression by harnessing VLLMs' inherent attention mechanisms. Our analysis reveals that VLLM attention layers naturally encoding query-conditioned keyframe priors, by which DyToK dynamically adjusts per-frame token retention ratios, prioritizing semantically rich frames while suppressing redundancies. Extensive experiments demonstrate that DyToK achieves state-of-the-art efficiency-accuracy tradeoffs. DyToK shows plug-and-play compatibility with existing compression methods, such as VisionZip and FastV, attaining 4.3x faster inference while preserving accuracy across multiple VLLMs, such as LLaVA-OneVision and Qwen2.5-VL. Code is available at https://github.com/yu-lin-li/DyToK .
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuroABench: A Multimodal Evaluation Benchmark for Neurosurgical Anatomy Identification</title>
<link>https://arxiv.org/abs/2512.06921</link>
<guid>https://arxiv.org/abs/2512.06921</guid>
<content:encoded><![CDATA[
arXiv:2512.06921v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) have shown significant potential in surgical video understanding. With improved zero-shot performance and more effective human-machine interaction, they provide a strong foundation for advancing surgical education and assistance. However, existing research and datasets primarily focus on understanding surgical procedures and workflows, while paying limited attention to the critical role of anatomical comprehension. In clinical practice, surgeons rely heavily on precise anatomical understanding to interpret, review, and learn from surgical videos. To fill this gap, we introduce the Neurosurgical Anatomy Benchmark (NeuroABench), the first multimodal benchmark explicitly created to evaluate anatomical understanding in the neurosurgical domain. NeuroABench consists of 9 hours of annotated neurosurgical videos covering 89 distinct procedures and is developed using a novel multimodal annotation pipeline with multiple review cycles. The benchmark evaluates the identification of 68 clinical anatomical structures, providing a rigorous and standardized framework for assessing model performance. Experiments on over 10 state-of-the-art MLLMs reveal significant limitations, with the best-performing model achieving only 40.87% accuracy in anatomical identification tasks. To further evaluate the benchmark, we extract a subset of the dataset and conduct an informative test with four neurosurgical trainees. The results show that the best-performing student achieves 56% accuracy, with the lowest scores of 28% and an average score of 46.5%. While the best MLLM performs comparably to the lowest-scoring student, it still lags significantly behind the group's average performance. This comparison underscores both the progress of MLLMs in anatomical understanding and the substantial gap that remains in achieving human-level performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MATEX: A Multi-Agent Framework for Explaining Ethereum Transactions</title>
<link>https://arxiv.org/abs/2512.06933</link>
<guid>https://arxiv.org/abs/2512.06933</guid>
<content:encoded><![CDATA[
arXiv:2512.06933v1 Announce Type: cross 
Abstract: Understanding a complicated Ethereum transaction remains challenging: multi-hop token flows, nested contract calls, and opaque execution paths routinely lead users to blind signing. Based on interviews with everyday users, developers, and auditors, we identify the need for faithful, step-wise explanations grounded in both on-chain evidence and real-world protocol semantics. To meet this need, we introduce (matex, a cognitive multi-agent framework that models transaction understanding as a collaborative investigation-combining rapid hypothesis generation, dynamic off-chain knowledge retrieval, evidence-aware synthesis, and adversarial validation to produce faithful explanations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flash Multi-Head Feed-Forward Network</title>
<link>https://arxiv.org/abs/2512.06989</link>
<guid>https://arxiv.org/abs/2512.06989</guid>
<content:encoded><![CDATA[
arXiv:2512.06989v1 Announce Type: cross 
Abstract: We explore Multi-Head FFN (MH-FFN) as a replacement of FFN in the Transformer architecture, motivated by the structural similarity between single-head attention and FFN. While multi-head mechanisms enhance expressivity in attention, naively applying them to FFNs faces two challenges: memory consumption scaling with the head count, and an imbalanced ratio between the growing intermediate size and the fixed head dimension as models scale, which degrades scalability and expressive power. To address these challenges, we propose Flash Multi-Head FFN (FlashMHF), with two key innovations: an I/O-aware fused kernel computing outputs online in SRAM akin to FlashAttention, and a design using dynamically weighted parallel sub-networks to maintain a balanced ratio between intermediate and head dimensions. Validated on models from 128M to 1.3B parameters, FlashMHF consistently improves perplexity and downstream task accuracy over SwiGLU FFNs, while reducing peak memory usage by 3-5x and accelerating inference by up to 1.08x. Our work establishes the multi-head design as a superior architectural principle for FFNs, presenting FlashMHF as a powerful, efficient, and scalable alternative to FFNs in Transformers.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Block Sparse Flash Attention</title>
<link>https://arxiv.org/abs/2512.07011</link>
<guid>https://arxiv.org/abs/2512.07011</guid>
<content:encoded><![CDATA[
arXiv:2512.07011v1 Announce Type: cross 
Abstract: Modern large language models increasingly require long contexts for reasoning and multi-document tasks, but attention's quadratic complexity creates a severe computational bottleneck. We present Block-Sparse FlashAttention (BSFA), a drop-in replacement that accelerates long-context inference while preserving model quality. Unlike methods that predict importance before computing scores, BSFA computes exact query-key similarities to select the top-k most important value blocks for each query. By comparing per-block maximum scores against calibrated thresholds, we skip approximately 50% of the computation and memory transfers for pruned blocks. Our training-free approach requires only a one-time threshold calibration on a small dataset to learn the per-layer and per-head attention score distributions. We provide a CUDA kernel implementation that can be used as a drop-in replacement for FlashAttention. On Llama-3.1-8B, BSFA achieves up to 1.10x speedup on real-world reasoning benchmarks and up to 1.24x for needle-in-a-haystack retrieval tasks while maintaining above 99% baseline accuracy, with certain configurations even improving accuracy by focusing on the most relevant content, substantially outperforming existing sparse attention methods. The implementation is available at https://github.com/Danielohayon/Block-Sparse-Flash-Attention
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Neural Affinity Framework for Abstract Reasoning: Diagnosing the Compositional Gap in Transformer Architectures via Procedural Task Taxonomy</title>
<link>https://arxiv.org/abs/2512.07109</link>
<guid>https://arxiv.org/abs/2512.07109</guid>
<content:encoded><![CDATA[
arXiv:2512.07109v1 Announce Type: cross 
Abstract: Responding to Hodel et al.'s (2024) call for a formal definition of task relatedness in re-arc, we present the first 9-category taxonomy of all 400 tasks, validated at 97.5% accuracy via rule-based code analysis. We prove the taxonomy's visual coherence by training a CNN on raw grid pixels (95.24% accuracy on S3, 36.25% overall, 3.3x chance), then apply the taxonomy diagnostically to the original ARC-AGI-2 test set. Our curriculum analysis reveals 35.3% of tasks exhibit low neural affinity for Transformers--a distributional bias mirroring ARC-AGI-2. To probe this misalignment, we fine-tuned a 1.7M-parameter Transformer across 302 tasks, revealing a profound Compositional Gap: 210 of 302 tasks (69.5%) achieve >80% cell accuracy (local patterns) but <10% grid accuracy (global synthesis). This provides direct evidence for a Neural Affinity Ceiling Effect, where performance is bounded by architectural suitability, not curriculum. Applying our framework to Li et al.'s independent ViTARC study (400 specialists, 1M examples each) confirms its predictive power: Very Low affinity tasks achieve 51.9% versus 77.7% for High affinity (p<0.001), with a task at 0% despite massive data. The taxonomy enables precise diagnosis: low-affinity tasks (A2) hit hard ceilings, while high-affinity tasks (C1) reach 99.8%. These findings indicate that progress requires hybrid architectures with affinity-aligned modules. We release our validated taxonomy,
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models</title>
<link>https://arxiv.org/abs/2512.07141</link>
<guid>https://arxiv.org/abs/2512.07141</guid>
<content:encoded><![CDATA[
arXiv:2512.07141v1 Announce Type: cross 
Abstract: As multimodal reasoning improves the overall capabilities of Large Vision Language Models (LVLMs), recent studies have begun to explore safety-oriented reasoning, aiming to enhance safety awareness by analyzing potential safety risks during the reasoning process before generating the final response. Although such approaches improve safety awareness and interpretability, this single-pass think-then-answer paradigm remains vulnerable to contextual or visual jailbreak attacks. This reveals a critical flaw: single-pass reasoning may overlook explicit harmful content in its own output. Our key insight is to exploit this wasted signal through reflection, which can effectively leverage the malicious content revealed in the first-pass reasoning to enable genuine self-correction and prevent unsafe generations. Motivated by this, we propose Think-Reflect-Revise (TRR), a three-stage training framework designed to enhance the safety alignment of LVLMs through policy-guided self-reflection. We first build a Reflective Safety Reasoning (ReSafe) dataset with 5,000 examples that follow a think-reflect-revise process. We then fine-tune the target model using the ReSafe dataset to initialize reflective behavior, and finally reinforce policy-guided reflection through reinforcement learning. Experimental results show that TRR substantially improves the safety performance of LVLMs across both safety-awareness benchmarks and jailbreak attack evaluations, increasing the overall safe response rate from 42.8% to 87.7% on Qwen2.5-VL-7B, while preserving stable performance on general benchmarks such as MMMU and MMStar. The project page is available at https://think-reflect-revise.github.io/.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PICKT: Practical Interlinked Concept Knowledge Tracing for Personalized Learning using Knowledge Map Concept Relations</title>
<link>https://arxiv.org/abs/2512.07179</link>
<guid>https://arxiv.org/abs/2512.07179</guid>
<content:encoded><![CDATA[
arXiv:2512.07179v1 Announce Type: cross 
Abstract: With the recent surge in personalized learning, Intelligent Tutoring Systems (ITS) that can accurately track students' individual knowledge states and provide tailored learning paths based on this information are in demand as an essential task. This paper focuses on the core technology of Knowledge Tracing (KT) models that analyze students' sequences of interactions to predict their knowledge acquisition levels. However, existing KT models suffer from limitations such as restricted input data formats, cold start problems arising with new student enrollment or new question addition, and insufficient stability in real-world service environments. To overcome these limitations, a Practical Interlinked Concept Knowledge Tracing (PICKT) model that can effectively process multiple types of input data is proposed. Specifically, a knowledge map structures the relationships among concepts considering the question and concept text information, thereby enabling effective knowledge tracing even in cold start situations. Experiments reflecting real operational environments demonstrated the model's excellent performance and practicality. The main contributions of this research are as follows. First, a model architecture that effectively utilizes diverse data formats is presented. Second, significant performance improvements are achieved over existing models for two core cold start challenges: new student enrollment and new question addition. Third, the model's stability and practicality are validated through delicate experimental design, enhancing its applicability in real-world product environments. This provides a crucial theoretical and technical foundation for the practical implementation of next-generation ITS.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generating Storytelling Images with Rich Chains-of-Reasoning</title>
<link>https://arxiv.org/abs/2512.07198</link>
<guid>https://arxiv.org/abs/2512.07198</guid>
<content:encoded><![CDATA[
arXiv:2512.07198v1 Announce Type: cross 
Abstract: An image can convey a compelling story by presenting rich, logically connected visual clues. These connections form Chains-of-Reasoning (CoRs) within the image, enabling viewers to infer events, causal relationships, and other information, thereby understanding the underlying story. In this paper, we focus on these semantically rich images and define them as Storytelling Images. Such images have diverse applications beyond illustration creation and cognitive screening, leveraging their ability to convey multi-layered information visually and inspire active interpretation. However, due to their complex semantic nature, Storytelling Images are inherently challenging to create, and thus remain relatively scarce. To address this challenge, we introduce the Storytelling Image Generation task, which explores how generative AI models can be leveraged to create such images. Specifically, we propose a two-stage pipeline, StorytellingPainter, which combines the creative reasoning abilities of Large Language Models (LLMs) with the visual synthesis capabilities of Text-to-Image (T2I) models to generate Storytelling Images. Alongside this pipeline, we develop a dedicated evaluation framework comprising three main evaluators: a Semantic Complexity Evaluator, a KNN-based Diversity Evaluator and a Story-Image Alignment Evaluator. Given the critical role of story generation in the Storytelling Image Generation task and the performance disparity between open-source and proprietary LLMs, we further explore tailored training strategies to reduce this gap, resulting in a series of lightweight yet effective models named Mini-Storytellers. Experimental results demonstrate the feasibility and effectiveness of our approaches. The code is available at https://github.com/xiujiesong/StorytellingImageGeneration.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pay Less Attention to Function Words for Free Robustness of Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.07222</link>
<guid>https://arxiv.org/abs/2512.07222</guid>
<content:encoded><![CDATA[
arXiv:2512.07222v1 Announce Type: cross 
Abstract: To address the trade-off between robustness and performance for robust VLM, we observe that function words could incur vulnerability of VLMs against cross-modal adversarial attacks, and propose Function-word De-Attention (FDA) accordingly to mitigate the impact of function words. Similar to differential amplifiers, our FDA calculates the original and the function-word cross-attention within attention heads, and differentially subtracts the latter from the former for more aligned and robust VLMs. Comprehensive experiments include 2 SOTA baselines under 6 different attacks on 2 downstream tasks, 3 datasets, and 3 models. Overall, our FDA yields an average 18/13/53% ASR drop with only 0.2/0.3/0.6% performance drops on the 3 tested models on retrieval, and a 90% ASR drop with a 0.3% performance gain on visual grounding. We demonstrate the scalability, generalization, and zero-shot performance of FDA experimentally, as well as in-depth ablation studies and analysis. Code will be made publicly at https://github.com/michaeltian108/FDA.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Recover-to-Forget: Gradient Reconstruction from LoRA for Efficient LLM Unlearning</title>
<link>https://arxiv.org/abs/2512.07374</link>
<guid>https://arxiv.org/abs/2512.07374</guid>
<content:encoded><![CDATA[
arXiv:2512.07374v1 Announce Type: cross 
Abstract: Unlearning in large foundation models (e.g., LLMs) is essential for enabling dynamic knowledge updates, enforcing data deletion rights, and correcting model behavior. However, existing unlearning methods often require full-model fine-tuning or access to the original training data, which limits their scalability and practicality. In this work, we introduce Recover-to-Forget (R2F), a novel framework for efficient unlearning in LLMs based on reconstructing full-model gradient directions from low-rank LoRA adapter updates. Rather than performing backpropagation through the full model, we compute gradients with respect to LoRA parameters using multiple paraphrased prompts and train a gradient decoder to approximate the corresponding full-model gradients. To ensure applicability to larger or black-box models, the decoder is trained on a proxy model and transferred to target models. We provide a theoretical analysis of cross-model generalization and demonstrate that our method achieves effective unlearning while preserving general model performance. Experimental results demonstrate that R2F offers a scalable and lightweight alternative for unlearning in pretrained LLMs without requiring full retraining or access to internal parameters.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LUNE: Efficient LLM Unlearning via LoRA Fine-Tuning with Negative Examples</title>
<link>https://arxiv.org/abs/2512.07375</link>
<guid>https://arxiv.org/abs/2512.07375</guid>
<content:encoded><![CDATA[
arXiv:2512.07375v1 Announce Type: cross 
Abstract: Large language models (LLMs) possess vast knowledge acquired from extensive training corpora, but they often cannot remove specific pieces of information when needed, which makes it hard to handle privacy, bias mitigation, and knowledge correction. Traditional model unlearning approaches require computationally expensive fine-tuning or direct weight editing, making them impractical for real-world deployment. In this work, we introduce LoRA-based Unlearning with Negative Examples (LUNE), a lightweight framework that performs negative-only unlearning by updating only low-rank adapters while freezing the backbone, thereby localizing edits and avoiding disruptive global changes. Leveraging Low-Rank Adaptation (LoRA), LUNE targets intermediate representations to suppress (or replace) requested knowledge with an order-of-magnitude lower compute and memory than full fine-tuning or direct weight editing. Extensive experiments on multiple factual unlearning tasks show that LUNE: (I) achieves effectiveness comparable to full fine-tuning and memory-editing methods, and (II) reduces computational cost by about an order of magnitude.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Living the Novel: A System for Generating Self-Training Timeline-Aware Conversational Agents from Novels</title>
<link>https://arxiv.org/abs/2512.07474</link>
<guid>https://arxiv.org/abs/2512.07474</guid>
<content:encoded><![CDATA[
arXiv:2512.07474v1 Announce Type: cross 
Abstract: We present the Living Novel, an end-to-end system that transforms any literary work into an immersive, multi-character conversational experience. This system is designed to solve two fundamental challenges for LLM-driven characters. Firstly, generic LLMs suffer from persona drift, often failing to stay in character. Secondly, agents often exhibit abilities that extend beyond the constraints of the story's world and logic, leading to both narrative incoherence (spoiler leakage) and robustness failures (frame-breaking). To address these challenges, we introduce a novel two-stage training pipeline. Our Deep Persona Alignment (DPA) stage uses data-free reinforcement finetuning to instill deep character fidelity. Our Coherence and Robustness Enhancing (CRE) stage then employs a story-time-aware knowledge graph and a second retrieval-grounded training pass to architecturally enforce these narrative constraints. We validate our system through a multi-phase evaluation using Jules Verne's Twenty Thousand Leagues Under the Sea. A lab study with a detailed ablation of system components is followed by a 5-day in-the-wild diary study. Our DPA pipeline helps our specialized model outperform GPT-4o on persona-specific metrics, and our CRE stage achieves near-perfect performance in coherence and robustness measures. Our study surfaces practical design guidelines for AI-driven narrative systems: we find that character-first self-training is foundational for believability, while explicit story-time constraints are crucial for sustaining coherent, interruption-resilient mobile-web experiences.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward More Reliable Artificial Intelligence: Reducing Hallucinations in Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.07564</link>
<guid>https://arxiv.org/abs/2512.07564</guid>
<content:encoded><![CDATA[
arXiv:2512.07564v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) frequently generate hallucinated content plausible but incorrect claims about image content. We propose a training-free self-correction framework enabling VLMs to iteratively refine responses through uncertainty-guided visual re-attention. Our method combines multidimensional uncertainty quantification (token entropy, attention dispersion, semantic consistency, claim confidence) with attention-guided cropping of under-explored regions. Operating entirely with frozen, pretrained VLMs, our framework requires no gradient updates. We validate our approach on the POPE and MMHAL BENCH benchmarks using the Qwen2.5-VL-7B [23] architecture. Experimental results demonstrate that our method reduces hallucination rates by 9.8 percentage points compared to the baseline, while improving object existence accuracy by 4.7 points on adversarial splits. Furthermore, qualitative analysis confirms that uncertainty-guided re-attention successfully grounds corrections in visual evidence where standard decoding fails. We validate our approach on Qwen2.5-VL-7B [23], with plans to extend validation across diverse architectures in future versions. We release our code and methodology to facilitate future research in trustworthy multimodal systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning</title>
<link>https://arxiv.org/abs/2512.07795</link>
<guid>https://arxiv.org/abs/2512.07795</guid>
<content:encoded><![CDATA[
arXiv:2512.07795v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly deployed in settings where reasoning, such as multi-step problem solving and chain-of-thought, is essential. Yet, current evaluation practices overwhelmingly report single-run accuracy while ignoring the intrinsic uncertainty that naturally arises from stochastic decoding. This omission creates a blind spot because practitioners cannot reliably assess whether a method's reported performance is stable, reproducible, or cost-consistent. We introduce ReasonBENCH, the first benchmark designed to quantify the underlying instability in LLM reasoning. ReasonBENCH provides (i) a modular evaluation library that standardizes reasoning frameworks, models, and tasks, (ii) a multi-run protocol that reports statistically reliable metrics for both quality and cost, and (iii) a public leaderboard to encourage variance-aware reporting. Across tasks from different domains, we find that the vast majority of reasoning strategies and models exhibit high instability. Notably, even strategies with similar average performance can display confidence intervals up to four times wider, and the top-performing methods often incur higher and less stable costs. Such instability compromises reproducibility across runs and, consequently, the reliability of reported performance. To better understand these dynamics, we further analyze the impact of prompts, model families, and scale on the trade-off between solve rate and stability. Our results highlight reproducibility as a critical dimension for reliable LLM reasoning and provide a foundation for future reasoning methods and uncertainty quantification techniques. ReasonBENCH is publicly available at https://github.com/au-clan/ReasonBench .
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Group Representational Position Encoding</title>
<link>https://arxiv.org/abs/2512.07805</link>
<guid>https://arxiv.org/abs/2512.07805</guid>
<content:encoded><![CDATA[
arXiv:2512.07805v1 Announce Type: cross 
Abstract: We present GRAPE (Group RepresentAtional Position Encoding), a unified framework for positional encoding based on group actions. GRAPE brings together two families of mechanisms: (i) multiplicative rotations (Multiplicative GRAPE) in $\mathrm{SO}(d)$ and (ii) additive logit biases (Additive GRAPE) arising from unipotent actions in the general linear group $\mathrm{GL}$. In Multiplicative GRAPE, a position $n \in \mathbb{Z}$ (or $t \in \mathbb{R}$) acts as $\mathbf{G}(n)=\exp(n\,\omega\,\mathbf{L})$ with a rank-2 skew generator $\mathbf{L} \in \mathbb{R}^{d \times d}$, yielding a relative, compositional, norm-preserving map with a closed-form matrix exponential. RoPE is recovered exactly when the $d/2$ planes are the canonical coordinate pairs with log-uniform spectrum. Learned commuting subspaces and compact non-commuting mixtures strictly extend this geometry to capture cross-subspace feature coupling at $O(d)$ and $O(r d)$ cost per head, respectively. In Additive GRAPE, additive logits arise as rank-1 (or low-rank) unipotent actions, recovering ALiBi and the Forgetting Transformer (FoX) as exact special cases while preserving an exact relative law and streaming cacheability. Altogether, GRAPE supplies a principled design space for positional geometry in long-context models, subsuming RoPE and ALiBi as special cases. Project Page: https://github.com/model-architectures/GRAPE.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>I Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses</title>
<link>https://arxiv.org/abs/2402.11192</link>
<guid>https://arxiv.org/abs/2402.11192</guid>
<content:encoded><![CDATA[
arXiv:2402.11192v5 Announce Type: replace 
Abstract: This paper explores an intriguing observation: fine-tuning a large language model (LLM) with responses generated by a LLM often yields better results than using responses generated by humans, particularly in reasoning tasks. We conduct an in-depth investigation to understand why this occurs. Contrary to the common belief that these instances is due to the more detailed nature of LLM-generated content, our study identifies another contributing factor: an LLM is inherently more "familiar" with LLM generated responses. This familiarity is evidenced by lower perplexity before fine-tuning. We design a series of experiments to understand the impact of the "familiarity" and our conclusion reveals that this "familiarity" significantly impacts learning performance. Training with LLM-generated responses not only enhances performance but also helps maintain the model's capabilities in other reasoning tasks after fine-tuning on a specific task.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PhyloLM : Inferring the Phylogeny of Large Language Models and Predicting their Performances in Benchmarks</title>
<link>https://arxiv.org/abs/2404.04671</link>
<guid>https://arxiv.org/abs/2404.04671</guid>
<content:encoded><![CDATA[
arXiv:2404.04671v5 Announce Type: replace 
Abstract: This paper introduces PhyloLM, a method adapting phylogenetic algorithms to Large Language Models (LLMs) to explore whether and how they relate to each other and to predict their performance characteristics. Our method calculates a phylogenetic distance metric based on the similarity of LLMs' output. The resulting metric is then used to construct dendrograms, which satisfactorily capture known relationships across a set of 111 open-source and 45 closed models. Furthermore, our phylogenetic distance predicts performance in standard benchmarks, thus demonstrating its functional validity and paving the way for a time and cost-effective estimation of LLM capabilities. To sum up, by translating population genetic concepts to machine learning, we propose and validate a tool to evaluate LLM development, relationships and capabilities, even in the absence of transparent training information.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning and Machine Learning, Advancing Big Data Analytics and Management: Handy Appetizer</title>
<link>https://arxiv.org/abs/2409.17120</link>
<guid>https://arxiv.org/abs/2409.17120</guid>
<content:encoded><![CDATA[
arXiv:2409.17120v2 Announce Type: replace 
Abstract: This book explores the role of Artificial Intelligence (AI), Machine Learning (ML), and Deep Learning (DL) in driving the progress of big data analytics and management. The book focuses on simplifying the complex mathematical concepts behind deep learning, offering intuitive visualizations and practical case studies to help readers understand how neural networks and technologies like Convolutional Neural Networks (CNNs) work. It introduces several classic models and technologies such as Transformers, GPT, ResNet, BERT, and YOLO, highlighting their applications in fields like natural language processing, image recognition, and autonomous driving. The book also emphasizes the importance of pre-trained models and how they can enhance model performance and accuracy, with instructions on how to apply these models in various real-world scenarios. Additionally, it provides an overview of key big data management technologies like SQL and NoSQL databases, as well as distributed computing frameworks such as Apache Hadoop and Spark, explaining their importance in managing and processing vast amounts of data. Ultimately, the book underscores the value of mastering deep learning and big data management skills as critical tools for the future workforce, making it an essential resource for both beginners and experienced professionals.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Surveying the MLLM Landscape: A Meta-Review of Current Surveys</title>
<link>https://arxiv.org/abs/2409.18991</link>
<guid>https://arxiv.org/abs/2409.18991</guid>
<content:encoded><![CDATA[
arXiv:2409.18991v2 Announce Type: replace 
Abstract: The rise of Multimodal Large Language Models (MLLMs) has become a transformative force in the field of artificial intelligence, enabling machines to process and generate content across multiple modalities, such as text, images, audio, and video. These models represent a significant advancement over traditional unimodal systems, opening new frontiers in diverse applications ranging from autonomous agents to medical diagnostics. By integrating multiple modalities, MLLMs achieve a more holistic understanding of information, closely mimicking human perception. As the capabilities of MLLMs expand, the need for comprehensive and accurate performance evaluation has become increasingly critical. This survey aims to provide a systematic review of benchmark tests and evaluation methods for MLLMs, covering key topics such as foundational concepts, applications, evaluation methodologies, ethical concerns, security, efficiency, and domain-specific applications. Through the classification and analysis of existing literature, we summarize the main contributions and methodologies of various surveys, conduct a detailed comparative analysis, and examine their impact within the academic community. Additionally, we identify emerging trends and underexplored areas in MLLM research, proposing potential directions for future studies. This survey is intended to offer researchers and practitioners a comprehensive understanding of the current state of MLLM evaluation, thereby facilitating further progress in this rapidly evolving field.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning and Machine Learning, Advancing Big Data Analytics and Management: Unveiling AI's Potential Through Tools, Techniques, and Applications</title>
<link>https://arxiv.org/abs/2410.01268</link>
<guid>https://arxiv.org/abs/2410.01268</guid>
<content:encoded><![CDATA[
arXiv:2410.01268v3 Announce Type: replace 
Abstract: Artificial intelligence (AI), machine learning, and deep learning have become transformative forces in big data analytics and management, enabling groundbreaking advancements across diverse industries. This article delves into the foundational concepts and cutting-edge developments in these fields, with a particular focus on large language models (LLMs) and their role in natural language processing, multimodal reasoning, and autonomous decision-making. Highlighting tools such as ChatGPT, Claude, and Gemini, the discussion explores their applications in data analysis, model design, and optimization.
  The integration of advanced algorithms like neural networks, reinforcement learning, and generative models has enhanced the capabilities of AI systems to process, visualize, and interpret complex datasets. Additionally, the emergence of technologies like edge computing and automated machine learning (AutoML) democratizes access to AI, empowering users across skill levels to engage with intelligent systems. This work also underscores the importance of ethical considerations, transparency, and fairness in the deployment of AI technologies, paving the way for responsible innovation.
  Through practical insights into hardware configurations, software environments, and real-world applications, this article serves as a comprehensive resource for researchers and practitioners. By bridging theoretical underpinnings with actionable strategies, it showcases the potential of AI and LLMs to revolutionize big data management and drive meaningful advancements across domains such as healthcare, finance, and autonomous systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs are Biased Evaluators But Not Biased for Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2410.20833</link>
<guid>https://arxiv.org/abs/2410.20833</guid>
<content:encoded><![CDATA[
arXiv:2410.20833v2 Announce Type: replace 
Abstract: Recent studies have demonstrated that large language models (LLMs) exhibit significant biases in evaluation tasks, particularly in preferentially rating and favoring self-generated content. However, the extent to which this bias manifests in fact-oriented tasks, especially within retrieval-augmented generation (RAG) frameworks, where keyword extraction and factual accuracy take precedence over stylistic elements, remains unclear. Our study addresses this knowledge gap by simulating two critical phases of the RAG framework. In the first phase, LLMs evaluated human-authored and model-generated passages, emulating the \textit{pointwise reranking phase}. The second phase involves conducting pairwise reading comprehension tests to simulate the \textit{generation phase}. Contrary to previous findings indicating a self-preference in rating tasks, our results reveal no significant self-preference effect in RAG frameworks. Instead, we observe that factual accuracy significantly influences LLMs' output, even in the absence of prior knowledge. These findings are consistent among three common QA datasets (NQ, MARCO, TriviaQA Datasets) and 5 widely adopted language models (GPT-3.5, GPT-4o-mini, Gemini, LLaMA3, and Mistral). Our research contributes to the ongoing discourse on LLM biases and their implications for RAG-based system, offering insights that may inform the development of more robust and unbiased LLM systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Systematic Assessment of Language Models with Linguistic Minimal Pairs in Chinese</title>
<link>https://arxiv.org/abs/2411.06096</link>
<guid>https://arxiv.org/abs/2411.06096</guid>
<content:encoded><![CDATA[
arXiv:2411.06096v2 Announce Type: replace 
Abstract: We present ZhoBLiMP, the largest linguistic minimal pair benchmark for Chinese, with over 100 paradigms, ranging from topicalization to the \textit{Ba} construction. We then train from scratch a suite of Chinese language models (LMs) with different tokenizers, parameter sizes, and token volumes, to study the learning curves of LMs on Chinese. To mitigate the biases introduced by unequal lengths of the sentences in a minimal pair, we propose a new metric named sub-linear length normalized log-probabilities (SLLN-LP). Using SLLN-LP as the metric, our results show that \textsc{Anaphor}, \textsc{Quantifiers}, and \textsc{Ellipsis} in Chinese are difficult for LMs even up to 32B parameters, and that SLLN-LP successfully mitigates biases in ZhoBLiMP, JBLiMP and BLiMP. We conclude that future evaluations should be more carefully designed to consider the intricate relations between linking functions, LMs, and targeted minimal pairs.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Golden Touchstone: A Comprehensive Bilingual Benchmark for Evaluating Financial Large Language Models</title>
<link>https://arxiv.org/abs/2411.06272</link>
<guid>https://arxiv.org/abs/2411.06272</guid>
<content:encoded><![CDATA[
arXiv:2411.06272v2 Announce Type: replace 
Abstract: As large language models (LLMs) increasingly permeate the financial sector, there is a pressing need for a standardized method to comprehensively assess their performance. Existing financial benchmarks often suffer from limited language and task coverage, low-quality datasets, and inadequate adaptability for LLM evaluation. To address these limitations, we introduce Golden Touchstone, a comprehensive bilingual benchmark for financial LLMs, encompassing eight core financial NLP tasks in both Chinese and English. Developed from extensive open-source data collection and industry-specific demands, this benchmark thoroughly assesses models' language understanding and generation capabilities. Through comparative analysis of major models such as GPT-4o, Llama3, FinGPT, and FinMA, we reveal their strengths and limitations in processing complex financial information. Additionally, we open-source Touchstone-GPT, a financial LLM trained through continual pre-training and instruction tuning, which demonstrates strong performance on the bilingual benchmark but still has limitations in specific tasks. This research provides a practical evaluation tool for financial LLMs and guides future development and optimization. The source code for Golden Touchstone and model weight of Touchstone-GPT have been made publicly available at https://github.com/IDEA-FinAI/Golden-Touchstone.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Relevance and Reasoning: Rationale Distillation in Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2412.08519</link>
<guid>https://arxiv.org/abs/2412.08519</guid>
<content:encoded><![CDATA[
arXiv:2412.08519v3 Announce Type: replace 
Abstract: The reranker and generator are two critical components in the Retrieval-Augmented Generation (i.e., RAG) pipeline, responsible for ranking relevant documents and generating responses. However, due to differences in pre-training data and objectives, there is an inevitable gap between the documents ranked as relevant by the reranker and those required by the generator to support answering the query. To address this gap, we propose RADIO, a novel and practical preference alignment framework with RAtionale DIstillatiOn. Specifically, we first propose a rationale extraction method that leverages the reasoning capabilities of Large Language Models (LLMs) to extract the rationales necessary for answering the query. Subsequently, a rationale-based alignment process is designed to rerank the documents based on the extracted rationales, and fine-tune the reranker to align the preferences. We conduct extensive experiments on two tasks across three datasets to demonstrate the effectiveness of our approach compared to baseline methods. Our code is released online to ease reproduction.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Singular: Revealing the Value of Multiple Generations in Benchmark Evaluation</title>
<link>https://arxiv.org/abs/2502.08943</link>
<guid>https://arxiv.org/abs/2502.08943</guid>
<content:encoded><![CDATA[
arXiv:2502.08943v3 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated significant utility in real-world applications, exhibiting impressive capabilities in natural language processing and understanding. Benchmark evaluations are crucial for assessing the capabilities of LLMs as they can provide a comprehensive assessment of their strengths and weaknesses. However, current evaluation methods often overlook the inherent randomness of LLMs by employing deterministic generation strategies or relying on a single random sample, resulting in unaccounted sampling variance and unreliable benchmark score estimates. In this paper, we propose a hierarchical statistical model that provides a more comprehensive representation of the benchmarking process by incorporating both benchmark characteristics and LLM randomness. We show that leveraging multiple generations improves the accuracy of estimating the benchmark score and reduces variance. Multiple generations also allow us to define $\mathbb P\left(\text{correct}\right)$, a prompt-level difficulty score based on correct ratios, providing fine-grained insights into individual prompts. Additionally, we create a data map that visualizes difficulty and semantics of prompts, enabling error detection and quality control in benchmark construction.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization</title>
<link>https://arxiv.org/abs/2503.04598</link>
<guid>https://arxiv.org/abs/2503.04598</guid>
<content:encoded><![CDATA[
arXiv:2503.04598v4 Announce Type: replace 
Abstract: Transformers have become the de facto architecture for a wide range of machine learning tasks, particularly in large language models (LLMs). Despite their remarkable performance, many challenges remain in training deep transformer networks, especially regarding the position of the layer normalization. While Pre-Norm structures facilitate more stable training owing to their stronger identity path, they often lead to suboptimal performance compared to Post-Norm. In this paper, we propose $\textbf{HybridNorm}$, a simple yet effective hybrid normalization strategy that integrates the advantages of both Pre-Norm and Post-Norm. Specifically, HybridNorm employs QKV normalization within the attention mechanism and Post-Norm in the feed-forward network (FFN) of each transformer block. We provide both theoretical insights and empirical evidence to demonstrate that HybridNorm improves the gradient flow and the model robustness. Extensive experiments on large-scale transformer models, including both dense and sparse variants, show that HybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches across multiple benchmarks. These findings highlight the potential of HybridNorm as a more stable and effective technique for improving the training and performance of deep transformer models. Code is available at https://github.com/BryceZhuo/HybridNorm.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OSVBench: Benchmarking LLMs on Specification Generation Tasks for Operating System Verification</title>
<link>https://arxiv.org/abs/2504.20964</link>
<guid>https://arxiv.org/abs/2504.20964</guid>
<content:encoded><![CDATA[
arXiv:2504.20964v2 Announce Type: replace 
Abstract: We introduce OSVBench, a new benchmark for evaluating Large Language Models (LLMs) on the task of generating complete formal specifications for verifying the functional correctness of operating system kernels. This benchmark is built upon a real-world operating system kernel, Hyperkernel, and consists of 245 complex specification generation tasks in total, each of which is a long-context task of about 20k-30k tokens. The benchmark formulates the specification generation task as a program synthesis problem confined to a domain for specifying states and transitions. This formulation is provided to LLMs through a programming model. The LLMs must be able to understand the programming model and verification assumptions before delineating the correct search space for syntax and semantics and generating formal specifications. Guided by the operating system's high-level functional description, the LLMs are asked to generate a specification that fully describes all correct states and transitions for a potentially buggy code implementation of the operating system. Experimental results with 12 state-of-the-art LLMs indicate limited performance of existing LLMs on the specification generation task for operating system verification. Significant disparities in their performance highlight differences in their ability to handle long-context code generation tasks. The code are available at https://github.com/lishangyu-hkust/OSVBench
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Democratic or Authoritarian? Probing a New Dimension of Political Biases in Large Language Models</title>
<link>https://arxiv.org/abs/2506.12758</link>
<guid>https://arxiv.org/abs/2506.12758</guid>
<content:encoded><![CDATA[
arXiv:2506.12758v2 Announce Type: replace 
Abstract: As Large Language Models (LLMs) become increasingly integrated into everyday life and information ecosystems, concerns about their implicit biases continue to persist. While prior work has primarily examined socio-demographic and left--right political dimensions, little attention has been paid to how LLMs align with broader geopolitical value systems, particularly the democracy--authoritarianism spectrum. In this paper, we propose a novel methodology to assess such alignment, combining (1) the F-scale, a psychometric tool for measuring authoritarian tendencies, (2) FavScore, a newly introduced metric for evaluating model favorability toward world leaders, and (3) role-model probing to assess which figures are cited as general role-models by LLMs. We find that LLMs generally favor democratic values and leaders, but exhibit increased favorability toward authoritarian figures when prompted in Mandarin. Further, models are found to often cite authoritarian figures as role models, even outside explicit political contexts. These results shed light on ways LLMs may reflect and potentially reinforce global political ideologies, highlighting the importance of evaluating bias beyond conventional socio-political axes. Our code is available at: https://github.com/irenestrauss/Democratic-Authoritarian-Bias-LLMs.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking LLM Training through Information Geometry and Quantum Metrics</title>
<link>https://arxiv.org/abs/2506.15830</link>
<guid>https://arxiv.org/abs/2506.15830</guid>
<content:encoded><![CDATA[
arXiv:2506.15830v4 Announce Type: replace 
Abstract: Optimization in large language models (LLMs) unfolds over high-dimensional parameter spaces with non-Euclidean structure. Information geometry frames this landscape using the Fisher information metric, enabling more principled learning via natural gradient descent. Though often impractical, this geometric lens clarifies phenomena such as sharp minima, generalization, and observed scaling laws. We argue that curvature-based approaches deepen our understanding of LLM training. Finally, we speculate on quantum analogies based on the Fubini-Study metric and Quantum Fisher Information, hinting at efficient optimization in quantum-enhanced systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MUST-RAG: MUSical Text Question Answering with Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2507.23334</link>
<guid>https://arxiv.org/abs/2507.23334</guid>
<content:encoded><![CDATA[
arXiv:2507.23334v2 Announce Type: replace 
Abstract: Recent advancements in Large language models (LLMs) have demonstrated remarkable capabilities across diverse domains. While they exhibit strong zero-shot performance on various tasks, LLMs' effectiveness in music-related applications remains limited due to the relatively small proportion of music-specific knowledge in their training data. To address this limitation, we propose MusT-RAG, a comprehensive framework based on Retrieval Augmented Generation (RAG) to adapt general-purpose LLMs for text-only music question answering (MQA) tasks. RAG is a technique that provides external knowledge to LLMs by retrieving relevant context information when generating answers to questions. To optimize RAG for the music domain, we (1) propose MusWikiDB, a music-specialized vector database for the retrieval stage, and (2) utilizes context information during both inference and fine-tuning processes to effectively transform general-purpose LLMs into music-specific models. Our experiment demonstrates that MusT-RAG significantly outperforms traditional fine-tuning approaches in enhancing LLMs' music domain adaptation capabilities, showing consistent improvements across both in-domain and out-of-domain MQA benchmarks. Additionally, our MusWikiDB proves substantially more effective than general Wikipedia corpora, delivering superior performance and computational efficiency.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Syntactic Generalization in Structure-inducing Language Models</title>
<link>https://arxiv.org/abs/2508.07969</link>
<guid>https://arxiv.org/abs/2508.07969</guid>
<content:encoded><![CDATA[
arXiv:2508.07969v2 Announce Type: replace 
Abstract: Structure-inducing Language Models (SiLM) are trained on a self-supervised language modeling task, and induce a hierarchical sentence representation as a byproduct when processing an input. SiLMs couple strong syntactic generalization behavior with competitive performance on various NLP tasks, but many of their basic properties are yet underexplored. In this work, we train three different SiLM architectures from scratch: Structformer (Shen et al., 2021), UDGN (Shen et al., 2022), and GPST (Hu et al., 2024b). We train these architectures on both natural language (English, German, and Chinese) corpora and synthetic bracketing expressions. The models are then evaluated with respect to (i) properties of the induced syntactic representations (ii) performance on grammaticality judgment tasks, and (iii) training dynamics. We find that none of the three architectures dominates across all evaluation metrics. However, there are significant differences, in particular with respect to the induced syntactic representations. The Generative Pretrained Structured Transformer (GPST; Hu et al. 2024) performs most consistently across evaluation settings, and outperforms the other models on long-distance dependencies in bracketing expressions. Furthermore, our study shows that small models trained on large amounts of synthetic data provide a useful testbed for evaluating basic model properties.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RPRO: Ranked Preference Reinforcement Optimization for Enhancing Medical QA and Diagnostic Reasoning</title>
<link>https://arxiv.org/abs/2509.00974</link>
<guid>https://arxiv.org/abs/2509.00974</guid>
<content:encoded><![CDATA[
arXiv:2509.00974v5 Announce Type: replace 
Abstract: Medical question answering requires advanced reasoning that integrates domain knowledge with logical inference. However, existing large language models (LLMs) often generate reasoning chains that lack factual accuracy and clinical reliability. We propose Ranked Preference Reinforcement Optimization (RPRO), a novel framework that combines reinforcement learning with preference-driven reasoning refinement to enhance clinical chain-of-thought (CoT) performance. RPRO distinguishes itself from prior approaches by employing task-adaptive reasoning templates and a probabilistic evaluation mechanism that aligns model outputs with established clinical workflows, while automatically identifying and correcting low-quality reasoning chains. Unlike traditional pairwise preference methods, RPRO introduces a groupwise ranking optimization based on the Bradley--Terry model and incorporates KL-divergence regularization for stable training. Experiments on PubMedQA, MedQA-USMLE, and a real-world clinical dataset from Far Eastern Memorial Hospital (FEMH) demonstrate consistent improvements over strong baselines. Remarkably, our 2B-parameter model outperforms much larger 7B--20B models, including medical-specialized variants. These findings demonstrate that combining preference optimization with quality-driven refinement provides a scalable and clinically grounded approach to building more reliable medical LLMs.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SFT Doesn't Always Hurt General Capabilities: Revisiting Domain-Specific Fine-Tuning in LLMs</title>
<link>https://arxiv.org/abs/2509.20758</link>
<guid>https://arxiv.org/abs/2509.20758</guid>
<content:encoded><![CDATA[
arXiv:2509.20758v2 Announce Type: replace 
Abstract: Supervised Fine-Tuning (SFT) on domain-specific datasets is a common approach to adapt Large Language Models (LLMs) to specialized tasks but is often believed to degrade their general capabilities. In this work, we revisit this trade-off and present both empirical and theoretical insights. First, we show that SFT does not always hurt: using a smaller learning rate can substantially mitigate general performance degradation while preserving comparable target-domain performance. We then provide a theoretical analysis that explains these phenomena and further motivates a new method, Token-Adaptive Loss Reweighting (TALR). Building on this, and recognizing that smaller learning rates alone do not fully eliminate general-performance degradation in all cases, we evaluate a range of strategies for reducing general capability loss, including L2 regularization, LoRA, model averaging, FLOW, and our proposed TALR. Experimental results demonstrate that while no method completely eliminates the trade-off, TALR consistently outperforms these baselines in balancing domain-specific gains and general capabilities. Finally, we distill our findings into practical guidelines for adapting LLMs to new domains: (i) using a small learning rate to achieve a favorable trade-off, and (ii) when a stronger balance is further desired, adopt TALR as an effective strategy.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM Output Homogenization is Task Dependent</title>
<link>https://arxiv.org/abs/2509.21267</link>
<guid>https://arxiv.org/abs/2509.21267</guid>
<content:encoded><![CDATA[
arXiv:2509.21267v2 Announce Type: replace 
Abstract: A large language model can be less helpful if it exhibits output response homogenization. But whether two responses are considered homogeneous, and whether such homogenization is problematic, both depend on the task category. For instance, in objective math tasks, we often expect no variation in the final answer but anticipate variation in the problem-solving strategy. Whereas, for creative writing tasks, we may expect variation in key narrative components (e.g. plot, genre, setting, etc), beyond the vocabulary or embedding diversity produced by temperature-sampling. Previous work addressing output homogenization often fails to conceptualize diversity in a task-dependent way. We address this gap in the literature directly by making the following contributions. (1) We present a task taxonomy comprised of eight task categories that each have distinct concepts of output homogenization. (2) We introduce task-anchored functional diversity to better evaluate output homogenization. (3) We propose a task-anchored sampling technique that increases functional diversity for task categories where homogenization is undesired, while preserving it where it is desired. (4) We challenge the perceived existence of a diversity-quality trade-off by increasing functional diversity while maintaining response quality. Overall, we demonstrate how task dependence improves the evaluation and mitigation of output homogenization.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why Chain of Thought Fails in Clinical Text Understanding</title>
<link>https://arxiv.org/abs/2509.21933</link>
<guid>https://arxiv.org/abs/2509.21933</guid>
<content:encoded><![CDATA[
arXiv:2509.21933v2 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly being applied to clinical care, a domain where both accuracy and transparent reasoning are critical for safe and trustworthy deployment. Chain-of-thought (CoT) prompting, which elicits step-by-step reasoning, has demonstrated improvements in performance and interpretability across a wide range of tasks. However, its effectiveness in clinical contexts remains largely unexplored, particularly in the context of electronic health records (EHRs), the primary source of clinical documentation, which are often lengthy, fragmented, and noisy. In this work, we present the first large-scale systematic study of CoT for clinical text understanding. We assess 95 advanced LLMs on 87 real-world clinical text tasks, covering 9 languages and 8 task types. Contrary to prior findings in other domains, we observe that 86.3\% of models suffer consistent performance degradation in the CoT setting. More capable models remain relatively robust, while weaker ones suffer substantial declines. To better characterize these effects, we perform fine-grained analyses of reasoning length, medical concept alignment, and error profiles, leveraging both LLM-as-a-judge evaluation and clinical expert evaluation. Our results uncover systematic patterns in when and why CoT fails in clinical contexts, which highlight a critical paradox: CoT enhances interpretability but may undermine reliability in clinical text tasks. This work provides an empirical basis for clinical reasoning strategies of LLMs, highlighting the need for transparent and trustworthy approaches.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Collaborative User Simulators for Tool Agents</title>
<link>https://arxiv.org/abs/2509.23124</link>
<guid>https://arxiv.org/abs/2509.23124</guid>
<content:encoded><![CDATA[
arXiv:2509.23124v3 Announce Type: replace 
Abstract: Tool agents interact with users through multi-turn dialogues to accomplish various tasks. Recent studies have adopted user simulation methods to develop these agents in multi-turn settings. However, existing user simulators tend to be agent-friendly, exhibiting only cooperative behaviors, which fails to train and test agents against non-collaborative users in the real world. To address this, we propose a novel user simulator architecture that simulates four categories of non-collaborative behaviors: requesting unavailable services, digressing into tangential conversations, expressing impatience, and providing incomplete utterances. Our user simulator can simulate challenging and natural non-collaborative behaviors while reliably delivering all intents and information necessary to accomplish the task. Our experiments on MultiWOZ and $\tau$-bench reveal significant performance degradation in state-of-the-art tool agents when encountering non-collaborative users. We provide detailed analyses of agents' weaknesses under each non-collaborative condition, such as escalated hallucinations and dialogue breakdowns. Ultimately, we contribute an easily extensible user simulation framework to help the research community develop tool agents and preemptively diagnose them under challenging real-world conditions within their own services.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SimuHome: A Temporal- and Environment-Aware Benchmark for Smart Home LLM Agents</title>
<link>https://arxiv.org/abs/2509.24282</link>
<guid>https://arxiv.org/abs/2509.24282</guid>
<content:encoded><![CDATA[
arXiv:2509.24282v2 Announce Type: replace 
Abstract: Large Language Model (LLM) agents excel at multi-step, tool-augmented tasks. However, smart homes introduce distinct challenges, requiring agents to handle latent user intents, temporal dependencies, device constraints, scheduling, and more. The main bottlenecks for developing smart home agents with such capabilities include the lack of a realistic simulation environment where agents can interact with devices and observe the results, as well as a challenging benchmark to evaluate them. To address this, we introduce $\textbf{SimuHome}$, a time-accelerated home environment that simulates smart devices, supports API calls, and reflects changes in environmental variables. By building the simulator on the Matter protocol, the global industry standard for smart home communication, SimuHome provides a high-fidelity environment, and agents validated in SimuHome can be deployed on real Matter-compliant devices with minimal adaptation. We provide a challenging benchmark of 600 episodes across twelve user query types that require the aforementioned capabilities. Our evaluation of 16 agents under a unified ReAct framework reveals distinct capabilities and limitations across models. Models under 7B parameters exhibited negligible performance across all query types. Even GPT-4.1, the best-performing standard model, struggled with implicit intent inference, state verification, and particularly temporal scheduling. While reasoning models such as GPT-5.1 consistently outperformed standard models on every query type, they required over three times the average inference time, which can be prohibitive for real-time smart home applications. This highlights a critical trade-off between task performance and real-world practicality.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical Guarantees</title>
<link>https://arxiv.org/abs/2510.01268</link>
<guid>https://arxiv.org/abs/2510.01268</guid>
<content:encoded><![CDATA[
arXiv:2510.01268v4 Announce Type: replace 
Abstract: We study the problem of determining whether a piece of text has been authored by a human or by a large language model (LLM). Existing state of the art logits-based detectors make use of statistics derived from the log-probability of the observed text evaluated using the distribution function of a given source LLM. However, relying solely on log probabilities can be sub-optimal. In response, we introduce AdaDetectGPT -- a novel classifier that adaptively learns a witness function from training data to enhance the performance of logits-based detectors. We provide statistical guarantees on its true positive rate, false positive rate, true negative rate and false negative rate. Extensive numerical studies show AdaDetectGPT nearly uniformly improves the state-of-the-art method in various combination of datasets and LLMs, and the improvement can reach up to 37\%. A python implementation of our method is available at https://github.com/Mamba413/AdaDetectGPT.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hallucination reduction with CASAL: Contrastive Activation Steering For Amortized Learning</title>
<link>https://arxiv.org/abs/2510.02324</link>
<guid>https://arxiv.org/abs/2510.02324</guid>
<content:encoded><![CDATA[
arXiv:2510.02324v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) exhibit impressive capabilities but often hallucinate, confidently providing incorrect answers instead of admitting ignorance. Prior work has shown that models encode linear representations of their own knowledge and that activation steering can reduce hallucinations. These approaches, however, require real-time monitoring and intervention during inference. We introduce Contrastive Activation Steering for Amortized Learning (CASAL), an efficient algorithm that connects interpretability with amortized optimization. CASAL directly bakes the benefits of activation steering into model's weights. Once trained, LLMs answer questions they know while abstaining from answering those they do not. CASAL's light-weight design requires training only a submodule of a single transformer layer and yet reduces hallucination by 30%-40% across multiple short-form QA benchmarks. CASAL is 30x more compute-efficient and 20x more data-efficient than strong LoRA-based baselines such as SFT and DPO, boosting its practical applicability in data scarce domains. Importantly, CASAL also generalizes effectively to out-of-distribution (OOD) domains. We showcase CASAL's flexibility in mitigating hallucinations in both text-only and vision-language models. To our knowledge, CASAL is the first steering-based training method that has been shown to be effective for both dense and Mixture-of-Experts (MoE) models. CASAL represents a promising step forward for applying interpretability-inspired method for practical deployment in production systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>mini-vec2vec: Scaling Universal Geometry Alignment with Linear Transformations</title>
<link>https://arxiv.org/abs/2510.02348</link>
<guid>https://arxiv.org/abs/2510.02348</guid>
<content:encoded><![CDATA[
arXiv:2510.02348v3 Announce Type: replace 
Abstract: We build upon vec2vec, a procedure designed to align text embedding spaces without parallel data. vec2vec finds a near-perfect alignment, but it is expensive and unstable. We present mini-vec2vec, a simple and efficient alternative that requires substantially lower computational cost and is highly robust. Moreover, the learned mapping is a linear transformation. Our method consists of three main stages: a tentative matching of pseudo-parallel embedding vectors, transformation fitting, and iterative refinement. Our linear alternative exceeds the original instantiation of vec2vec by orders of magnitude in efficiency, while matching or exceeding their results. The method's stability and interpretable algorithmic steps facilitate scaling and unlock new opportunities for adoption in new domains and fields.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgriGPT-VL: Agricultural Vision-Language Understanding Suite</title>
<link>https://arxiv.org/abs/2510.04002</link>
<guid>https://arxiv.org/abs/2510.04002</guid>
<content:encoded><![CDATA[
arXiv:2510.04002v3 Announce Type: replace 
Abstract: Despite rapid advances in multimodal large language models, agricultural applications remain constrained by the scarcity of domain-tailored models, curated vision-language corpora, and rigorous evaluation. To address these challenges, we present the AgriGPT-VL Suite, a unified multimodal framework for agriculture. Our contributions are threefold. First, we introduce Agri-3M-VL, the largest vision-language corpus for agriculture to our knowledge, curated by a scalable multi-agent data generator; it comprises 1M image-caption pairs, 2M image-grounded VQA pairs, 50K expert-level VQA instances, and 15K GRPO reinforcement learning samples. Second, we develop AgriGPT-VL, an agriculture-specialized vision-language model trained via a progressive curriculum of textual grounding, multimodal shallow/deep alignment, and GRPO refinement. This method achieves strong multimodal reasoning while preserving text-only capability. Third, we establish AgriBench-VL-4K, a compact yet challenging evaluation suite with open-ended and image-grounded questions, paired with multi-metric evaluation and an LLM-as-a-judge framework. Experiments show that AgriGPT-VL outperforms leading general-purpose VLMs on AgriBench-VL-4K, achieving higher pairwise win rates in the LLM-as-a-judge evaluation. Meanwhile, it remains competitive on the text-only AgriBench-13K with no noticeable degradation of language ability. Ablation studies further confirm consistent gains from our alignment and GRPO refinement stages. We will open source all of the resources to support reproducible research and deployment in low-resource agricultural settings.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thinking on the Fly: Test-Time Reasoning Enhancement via Latent Thought Policy Optimization</title>
<link>https://arxiv.org/abs/2510.04182</link>
<guid>https://arxiv.org/abs/2510.04182</guid>
<content:encoded><![CDATA[
arXiv:2510.04182v2 Announce Type: replace 
Abstract: Recent advancements in Large Language Models (LLMs) have shifted from explicit Chain-of-Thought (CoT) reasoning to more efficient latent reasoning, where intermediate thoughts are represented as vectors rather than text. However, latent reasoning can be brittle on challenging, out-of-distribution tasks where robust reasoning is most critical. To overcome these limitations, we introduce Latent Thought Policy Optimization (LTPO), a parameter-free framework that enhances LLM reasoning entirely at test time, without requiring model parameter updates. LTPO treats intermediate latent "thought" vectors as dynamic parameters that are actively optimized for each problem instance. It employs an online policy gradient method guided by an intrinsic, confidence-based reward signal computed directly from the frozen LLM's own output distributions, eliminating the need for external supervision or expensive text generation during optimization. Extensive experiments on five reasoning benchmarks show that LTPO not only matches or surpasses strong baselines on standard tasks but also demonstrates remarkable robustness where others fail. Most notably, on highly challenging AIME benchmarks where existing latent reasoning baselines collapse to near-zero accuracy, LTPO delivers substantial improvements, showcasing a unique capability for complex reasoning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guided Query Refinement: Multimodal Hybrid Retrieval with Test-Time Optimization</title>
<link>https://arxiv.org/abs/2510.05038</link>
<guid>https://arxiv.org/abs/2510.05038</guid>
<content:encoded><![CDATA[
arXiv:2510.05038v2 Announce Type: replace 
Abstract: Multimodal encoders have pushed the boundaries of visual document retrieval, matching textual query tokens directly to image patches and achieving state-of-the-art performance on public benchmarks. Recent models relying on this paradigm have massively scaled the sizes of their query and document representations, presenting obstacles to deployment and scalability in real-world pipelines. Furthermore, purely vision-centric approaches may be constrained by the inherent modality gap still exhibited by modern vision-language models. In this work, we connect these challenges to the paradigm of hybrid retrieval, investigating whether a lightweight dense text retriever can enhance a stronger vision-centric model. Existing hybrid methods, which rely on coarse-grained fusion of ranks or scores, fail to exploit the rich interactions within each model's representation space. To address this, we introduce Guided Query Refinement (GQR), a novel test-time optimization method that refines a primary retriever's query embedding using guidance from a complementary retriever's scores. Through extensive experiments on visual document retrieval benchmarks, we demonstrate that GQR allows vision-centric models to match the performance of models with significantly larger representations, while being up to 14x faster and requiring 54x less memory. Our findings show that GQR effectively pushes the Pareto frontier for performance and efficiency in multimodal retrieval. We release our code at https://github.com/IBM/test-time-hybrid-retrieval
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs</title>
<link>https://arxiv.org/abs/2510.05069</link>
<guid>https://arxiv.org/abs/2510.05069</guid>
<content:encoded><![CDATA[
arXiv:2510.05069v2 Announce Type: replace 
Abstract: Recent work shows that, beyond discrete reasoning through explicit chain-of-thought steps, which are limited by the boundaries of natural languages, large language models (LLMs) can also reason continuously in latent space, allowing richer information per step and thereby improving token efficiency. Despite this promise, latent reasoning still faces two challenges, especially in training-free settings: 1) purely latent reasoning broadens the search distribution by maintaining multiple implicit paths, which diffuses probability mass, introduces noise, and impedes convergence to a single high-confidence solution, thereby hurting accuracy; and 2) overthinking persists even without explicit text, wasting tokens and degrading efficiency. To address these issues, we introduce SwiReasoning, a training-free framework for LLM reasoning which features two key innovations: 1) SwiReasoning dynamically switches between explicit and latent reasoning, guided by block-wise confidence estimated from entropy trends in next-token distributions, to balance exploration and exploitation and promote timely convergence. 2) By limiting the maximum number of thinking-block switches, SwiReasoning curbs overthinking and improves token efficiency across varying problem difficulties. On widely used mathematics and STEM benchmarks, SwiReasoning consistently improves average accuracy by 1.5%-2.8% across reasoning LLMs of different model families and scales. Furthermore, under constrained budgets, SwiReasoning improves average token efficiency by 56%-79%, with larger gains as budgets tighten.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unilaw-R1: A Large Language Model for Legal Reasoning with Reinforcement Learning and Iterative Inference</title>
<link>https://arxiv.org/abs/2510.10072</link>
<guid>https://arxiv.org/abs/2510.10072</guid>
<content:encoded><![CDATA[
arXiv:2510.10072v2 Announce Type: replace 
Abstract: Reasoning-focused large language models (LLMs) are rapidly evolving across various domains, yet their capabilities in handling complex legal problems remains underexplored. In this paper, we introduce Unilaw-R1, a large language model tailored for legal reasoning. With a lightweight 7-billion parameter scale, Unilaw-R1 significantly reduces deployment cost while effectively tackling three core challenges in the legal domain: insufficient legal knowledge, unreliable reasoning logic, and weak business generalization. To address these issues, we first construct Unilaw-R1-Data, a high-quality dataset containing 17K distilled and screened chain-of-thought (CoT) samples. Based on this, we adopt a two-stage training strategy combining Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), which significantly boosts the performance on complex legal reasoning tasks and supports interpretable decision-making in legal AI applications. To assess legal reasoning ability, we also introduce Unilaw-R1-Eval, a dedicated benchmark designed to evaluate models across single- and multi-choice legal tasks. Unilaw-R1 demonstrates strong results on authoritative benchmarks, outperforming all models of similar scale and achieving performance on par with the much larger DeepSeek-R1-Distill-Qwen-32B (54.9%). Following domain-specific training, it also showed significant gains on LawBench and LexEval, exceeding Qwen-2.5-7B-Instruct (46.6%) by an average margin of 6.6%.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grounding Long-Context Reasoning with Contextual Normalization for Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2510.13191</link>
<guid>https://arxiv.org/abs/2510.13191</guid>
<content:encoded><![CDATA[
arXiv:2510.13191v2 Announce Type: replace 
Abstract: Retrieval-Augmented Generation (RAG) has become an essential approach for extending the reasoning and knowledge capacity of large language models (LLMs). While prior research has primarily focused on retrieval quality and prompting strategies, the influence of how the retrieved documents are framed, i.e., context format, remains underexplored. We show that seemingly superficial choices, such as delimiters or structural markers in key-value extraction, can induce substantial shifts in accuracy and stability, even when semantic content is identical. To systematically investigate this effect, we design controlled experiments that vary context density, delimiter styles, and positional placement, revealing the underlying factors that govern performance differences. Building on these insights, we introduce Contextual Normalization, a lightweight strategy that adaptively standardizes context representations before generation. Extensive experiments on both controlled and real-world RAG benchmarks across diverse settings demonstrate that the proposed strategy consistently improves robustness to order variation and strengthens long-context utilization. These findings underscore that reliable RAG depends not only on retrieving the right content, but also on how that content is presented, offering both new empirical evidence and a practical technique for better long-context reasoning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Long-Term Memory for Long-Context Question Answering</title>
<link>https://arxiv.org/abs/2510.23730</link>
<guid>https://arxiv.org/abs/2510.23730</guid>
<content:encoded><![CDATA[
arXiv:2510.23730v2 Announce Type: replace 
Abstract: In order for large language models to achieve true conversational continuity and benefit from experiential learning, they need memory. While research has focused on the development of complex memory systems, it remains unclear which types of memory are most effective for long-context conversational tasks. We present a systematic evaluation of memory-augmented methods on long-context dialogues annotated for question-answering tasks that require diverse reasoning strategies. We analyse full-context prompting, semantic memory through retrieval-augmented generation and agentic memory, episodic memory through in-context learning, and procedural memory through prompt optimization. Our findings show that memory-augmented approaches reduce token usage by over 90\% while maintaining competitive accuracy. Memory architecture complexity should scale with model capability, with foundation models benefitting most from RAG, and stronger instruction-tuned models gaining from episodic learning through reflections and more complex agentic semantic memory. In particular, episodic memory can help LLMs recognise the limits of their own knowledge.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Fine-Tuning Erase Your Edits? On the Fragile Coexistence of Knowledge Editing and Adaptation</title>
<link>https://arxiv.org/abs/2511.05852</link>
<guid>https://arxiv.org/abs/2511.05852</guid>
<content:encoded><![CDATA[
arXiv:2511.05852v3 Announce Type: replace 
Abstract: Knowledge editing has emerged as a lightweight alternative to retraining for correcting or injecting specific facts in large language models (LLMs). Meanwhile, fine-tuning remains the default operation for adapting LLMs to new domains and tasks. Despite their widespread adoption, these two post-training interventions have been studied in isolation, leaving open a crucial question: if we fine-tune an edited model, do the edits survive? This question is motivated by two practical scenarios: removing covert or malicious edits, and preserving beneficial edits. If fine-tuning impairs edits (Fig.1), current KE methods become less useful, as every fine-tuned model would require re-editing, which significantly increases the cost; if edits persist, fine-tuned models risk propagating hidden malicious edits, raising serious safety concerns. To this end, we systematically quantify edit decay after fine-tuning, investigating how fine-tuning affects knowledge editing. Our results show that edits decay after fine-tuning, with survival varying across configurations, e.g., AlphaEdit edits decay more than MEMIT edits. Further, we find that fine-tuning edited layers only can effectively remove edits, though at a slight cost to downstream performance. Surprisingly, fine-tuning non-edited layers impairs more edits than full fine-tuning. Overall, our study establishes empirical baselines and actionable strategies for integrating knowledge editing with fine-tuning, and underscores that evaluating model editing requires considering the full LLM application pipeline.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPOT: An Annotated French Corpus and Benchmark for Detecting Critical Interventions in Online Conversations</title>
<link>https://arxiv.org/abs/2511.07405</link>
<guid>https://arxiv.org/abs/2511.07405</guid>
<content:encoded><![CDATA[
arXiv:2511.07405v2 Announce Type: replace 
Abstract: We introduce SPOT (Stopping Points in Online Threads), the first annotated corpus translating the sociological concept of stopping point into a reproducible NLP task. Stopping points are ordinary critical interventions that pause or redirect online discussions through a range of forms (irony, subtle doubt or fragmentary arguments) that frameworks like counterspeech or social correction often overlook. We operationalize this concept as a binary classification task and provide reliable annotation guidelines. The corpus contains 43,305 manually annotated French Facebook comments linked to URLs flagged as false information by social media users, enriched with contextual metadata (article, post, parent comment, page or group, and source). We benchmark fine-tuned encoder models (CamemBERT) and instruction-tuned LLMs under various prompting strategies. Results show that fine-tuned encoders outperform prompted LLMs in F1 score by more than 10 percentage points, confirming the importance of supervised learning for emerging non-English social media tasks. Incorporating contextual metadata further improves encoder models F1 scores from 0.75 to 0.78. We release the anonymized dataset, along with the annotation guidelines and code in our code repository, to foster transparency and reproducible research.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chopping Trees: Semantic Similarity Based Dynamic Pruning for Tree-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2511.08595</link>
<guid>https://arxiv.org/abs/2511.08595</guid>
<content:encoded><![CDATA[
arXiv:2511.08595v2 Announce Type: replace 
Abstract: Tree-of-Thought (ToT) reasoning boosts the problem-solving abilities of Large Language Models (LLMs) but is computationally expensive due to semantic redundancy, where distinct branches explore equivalent reasoning paths. We introduce Semantic Similarity-Based Dynamic Pruning (SSDP), a lightweight method that, to the best of our knowledge, is the first framework to integrate online semantic merging into parallelized tree search, enabling the clustering and pruning of redundant steps in real time. Across reasoning benchmarks, including GSM8K and MATH500, SSDP achieves up to a 2.3x speedup over state-of-the-art tree-search baselines while maintaining competitive accuracy (typically within 5% of the strongest baseline) and reducing the number of explored nodes by 85-90%, demonstrating a practical approach to efficient, scalable LLM reasoning. The implementation of SSDP is publicly available at https://github.com/kimjoonghokim/SSDP.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Is Self-Supervised Learning Enough to Fill in the Gap? A Study on Speech Inpainting</title>
<link>https://arxiv.org/abs/2405.20101</link>
<guid>https://arxiv.org/abs/2405.20101</guid>
<content:encoded><![CDATA[
arXiv:2405.20101v2 Announce Type: replace-cross 
Abstract: Speech inpainting consists in reconstructing corrupted or missing speech segments using surrounding context, a process that closely resembles the pretext tasks in Self-Supervised Learning (SSL) for speech encoders. This study investigates using SSL-trained speech encoders for inpainting without any additional training beyond the initial pretext task, and simply adding a decoder to generate a waveform. We compare this approach to supervised fine-tuning of speech encoders for a downstream task -- here, inpainting. Practically, we integrate HuBERT as the SSL encoder and HiFi-GAN as the decoder in two configurations: (1) fine-tuning the decoder to align with the frozen pre-trained encoder's output and (2) fine-tuning the encoder for an inpainting task based on a frozen decoder's input. Evaluations are conducted under single- and multi-speaker conditions using in-domain datasets and out-of-domain datasets (including unseen speakers, diverse speaking styles, and noise). Both informed and blind inpainting scenarios are considered, where the position of the corrupted segment is either known or unknown. The proposed SSL-based methods are benchmarked against several baselines, including a text-informed method combining automatic speech recognition with zero-shot text-to-speech synthesis. Performance is assessed using objective metrics and perceptual evaluations. The results demonstrate that both approaches outperform baselines, successfully reconstructing speech segments up to 200 ms, and sometimes up to 400 ms. Notably, fine-tuning the SSL encoder achieves more accurate speech reconstruction in single-speaker settings, while a pre-trained encoder proves more effective for multi-speaker scenarios. This demonstrates that an SSL pretext task can transfer to speech inpainting, enabling successful speech reconstruction with a pre-trained encoder.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transparent and Coherent Procedural Mistake Detection</title>
<link>https://arxiv.org/abs/2412.11927</link>
<guid>https://arxiv.org/abs/2412.11927</guid>
<content:encoded><![CDATA[
arXiv:2412.11927v4 Announce Type: replace-cross 
Abstract: Procedural mistake detection (PMD) is a challenging problem of classifying whether a human user (observed through egocentric video) has successfully executed a task (specified by a procedural text). Despite significant recent efforts, machine performance in the wild remains nonviable, and the reasoning processes underlying this performance are opaque. As such, we extend PMD to require generating visual self-dialog rationales to inform decisions. Given the impressive, mature image understanding capabilities observed in recent vision-and-language models (VLMs), we curate a suitable benchmark dataset for PMD based on individual frames. As our reformulation enables unprecedented transparency, we leverage a natural language inference (NLI) model to formulate two automated metrics for the coherence of generated rationales. We establish baselines for this reframed task, showing that VLMs struggle off-the-shelf, but with some trade-offs, their accuracy, coherence, and efficiency can be improved by incorporating these metrics into common inference and fine-tuning methods. Lastly, our multi-faceted metrics visualize common outcomes, highlighting areas for further improvement.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Collaborative Gym: A Framework for Enabling and Evaluating Human-Agent Collaboration</title>
<link>https://arxiv.org/abs/2412.15701</link>
<guid>https://arxiv.org/abs/2412.15701</guid>
<content:encoded><![CDATA[
arXiv:2412.15701v5 Announce Type: replace-cross 
Abstract: While the advancement of large language models has spurred the development of AI agents to automate tasks, numerous use cases inherently require agents to collaborate with humans due to humans' latent preferences, domain expertise, or the need for control. To facilitate the study of human-agent collaboration, we introduce Collaborative Gym (Co-Gym), an open framework for developing and evaluating collaborative agents that engage in bidirectional communication with humans while interacting with task environments. We describe how the framework enables the implementation of new task environments and coordination between humans and agents through a flexible, non-turn-taking interaction paradigm, along with an evaluation suite that assesses both collaboration outcomes and processes. Our framework provides both a simulated condition with a reliable user simulator and a real-world condition with an interactive web application. Initial benchmark experiments across three representative tasks -- creating travel plans, writing related work sections, and analyzing tabular data -- demonstrate the benefits of human-agent collaboration: The best-performing collaborative agents consistently outperform their fully autonomous counterparts in task performance, achieving win rates of 86% in Travel Planning, 74% in Tabular Analysis, and 66% in Related Work when evaluated by real users. Despite these improvements, our evaluation reveals persistent limitations in current language models and agents, with communication and situational awareness failures observed in 65% and 40% of cases in the real condition, respectively. Released under the permissive MIT license, Co-Gym supports the addition of new task environments and can be used to develop collaborative agent applications, while its evaluation suite enables assessment and improvement of collaborative agents.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation</title>
<link>https://arxiv.org/abs/2502.03930</link>
<guid>https://arxiv.org/abs/2502.03930</guid>
<content:encoded><![CDATA[
arXiv:2502.03930v4 Announce Type: replace-cross 
Abstract: Several recent studies have attempted to autoregressively generate continuous speech representations without discrete speech tokens by combining diffusion and autoregressive models, yet they often face challenges with excessive computational loads or suboptimal outcomes. In this work, we propose Diffusion Transformer Autoregressive Modeling (DiTAR), a patch-based autoregressive framework combining a language model with a diffusion transformer. This approach significantly enhances the efficacy of autoregressive models for continuous tokens and reduces computational demands. DiTAR utilizes a divide-and-conquer strategy for patch generation, where the language model processes aggregated patch embeddings and the diffusion transformer subsequently generates the next patch based on the output of the language model. For inference, we propose defining temperature as the time point of introducing noise during the reverse diffusion ODE to balance diversity and determinism. We also show in the extensive scaling analysis that DiTAR has superb scalability. In zero-shot speech generation, DiTAR achieves state-of-the-art performance in robustness, speaker similarity, and naturalness.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring the Potential of Encoder-free Architectures in 3D LMMs</title>
<link>https://arxiv.org/abs/2502.09620</link>
<guid>https://arxiv.org/abs/2502.09620</guid>
<content:encoded><![CDATA[
arXiv:2502.09620v4 Announce Type: replace-cross 
Abstract: Encoder-free architectures have been preliminarily explored in the 2D Large Multimodal Models (LMMs), yet it remains an open question whether they can be effectively applied to 3D understanding scenarios. In this paper, we present the first comprehensive investigation into the potential of encoder-free architectures to alleviate the challenges of encoder-based 3D LMMs. These long-standing challenges include the failure to adapt to varying point cloud resolutions during inference and the point features from the encoder not meeting the semantic needs of Large Language Models (LLMs). We identify key aspects for 3D LMMs to remove the pre-trained encoder and enable the LLM to assume the role of the 3D encoder: 1) We propose the LLM-embedded Semantic Encoding strategy in the pre-training stage, exploring the effects of various point cloud self-supervised losses. And we present the Hybrid Semantic Loss to extract high-level semantics. 2) We introduce the Hierarchical Geometry Aggregation strategy in the instruction tuning stage. This incorporates inductive bias into the LLM layers to focus on the local details of the point clouds. To the end, we present the first Encoder-free 3D LMM, ENEL. Our 7B model rivals the state-of-the-art model, PointLLM-PiSA-13B, achieving 57.91%, 61.0%, and 55.20% on the classification, captioning, and VQA tasks, respectively. Our results show that the encoder-free architecture is highly promising for replacing encoder-based architectures in the field of 3D understanding. The code is released at https://github.com/Ivan-Tang-3D/ENEL
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Improvement Towards Pareto Optimality: Mitigating Preference Conflicts in Multi-Objective Alignment</title>
<link>https://arxiv.org/abs/2502.14354</link>
<guid>https://arxiv.org/abs/2502.14354</guid>
<content:encoded><![CDATA[
arXiv:2502.14354v3 Announce Type: replace-cross 
Abstract: Multi-Objective Alignment (MOA) aims to align LLMs' responses with multiple human preference objectives, with Direct Preference Optimization (DPO) emerging as a prominent approach. However, we find that DPO-based MOA approaches suffer from widespread preference conflicts in the data, where different objectives favor different responses. This results in conflicting optimization directions, hindering the optimization on the Pareto Front. To address this, we propose to construct Pareto-optimal responses to resolve preference conflicts. To efficiently obtain and utilize such responses, we propose a self-improving DPO framework that enables LLMs to self-generate and select Pareto-optimal responses for self-supervised preference alignment. Extensive experiments on two datasets demonstrate the superior Pareto Front achieved by our framework compared to various baselines. Code is available at https://github.com/zyttt-coder/SIPO.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Process Reward Models That Think</title>
<link>https://arxiv.org/abs/2504.16828</link>
<guid>https://arxiv.org/abs/2504.16828</guid>
<content:encoded><![CDATA[
arXiv:2504.16828v5 Announce Type: replace-cross 
Abstract: Step-by-step verifiers -- also known as process reward models (PRMs) -- are a key ingredient for test-time scaling. PRMs require step-level supervision, making them expensive to train. This work aims to build data-efficient PRMs as verbalized step-wise reward models that verify every step in the solution by generating a verification chain-of-thought (CoT). We propose ThinkPRM, a long CoT verifier fine-tuned on orders of magnitude fewer process labels than those required by discriminative PRMs. Our approach capitalizes on the inherent reasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and discriminative verifiers -- using only 1% of the process labels in PRM800K -- across several challenging benchmarks. Specifically, ThinkPRM beats the baselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and reward-guided search. In an out-of-domain evaluation on a subset of GPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers trained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the same token budget, ThinkPRM scales up verification compute more effectively compared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of ProcessBench. Our work highlights the value of generative, long CoT PRMs that can scale test-time compute for verification while requiring minimal supervision for training. Our code, data, and models are released at https://github.com/mukhal/thinkprm.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.20561</link>
<guid>https://arxiv.org/abs/2505.20561</guid>
<content:encoded><![CDATA[
arXiv:2505.20561v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) trained via Reinforcement Learning (RL) have exhibited strong reasoning capabilities and emergent reflective behaviors, such as rethinking and error correction, as a form of in-context exploration. However, the Markovian policy obtained from conventional RL training does not give rise to reflective exploration behaviors since the policy depends on the history only through the state and therefore has no incentive to enrich identical states with additional context. Instead, RL exploration is only useful during training to learn the optimal policy in a trial-and-error manner. Therefore, it remains unclear whether reflective reasoning will emerge during RL, or why it is beneficial. To remedy this, we recast reflective exploration within a Bayesian RL framework, which optimizes the expected return under a posterior distribution over Markov decision processes induced by the training data. This Bayesian formulation admits uncertainty-adaptive policies that, through belief updates, naturally incentivize information-gathering actions and induce self-reflection behaviors. Our resulting algorithm, BARL, instructs the LLM to stitch and switch strategies based on the observed outcomes, offering principled guidance on when and how the model should reflectively explore. Empirical results on both synthetic and mathematical reasoning tasks demonstrate that BARL outperforms conventional RL approaches, achieving superior test-time performance and token efficiency. Our code is available at https://github.com/shenao-zhang/BARL.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization</title>
<link>https://arxiv.org/abs/2508.05731</link>
<guid>https://arxiv.org/abs/2508.05731</guid>
<content:encoded><![CDATA[
arXiv:2508.05731v2 Announce Type: replace-cross 
Abstract: The emergence of Multimodal Large Language Models (MLLMs) has propelled the development of autonomous agents that operate on Graphical User Interfaces (GUIs) using pure visual input. A fundamental challenge is robustly grounding natural language instructions. This requires a precise spatial alignment, which accurately locates the coordinates of each element, and, more critically, a correct semantic alignment, which matches the instructions to the functionally appropriate UI element. Although Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be effective at improving spatial alignment for these MLLMs, we find that inefficient exploration bottlenecks semantic alignment, which prevent models from learning difficult semantic associations. To address this exploration problem, we present Adaptive Exploration Policy Optimization (AEPO), a new policy optimization framework. AEPO employs a multi-answer generation strategy to enforce broader exploration, which is then guided by a theoretically grounded Adaptive Exploration Reward (AER) function derived from first principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B and InfiGUI-G1-7B, establish new state-of-the-art results across multiple challenging GUI grounding benchmarks, achieving significant relative improvements of up to 9.0% against the naive RLVR baseline on benchmarks designed to test generalization and semantic understanding. Resources are available at https://github.com/InfiXAI/InfiGUI-G1.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Kimi-Dev: Agentless Training as Skill Prior for SWE-Agents</title>
<link>https://arxiv.org/abs/2509.23045</link>
<guid>https://arxiv.org/abs/2509.23045</guid>
<content:encoded><![CDATA[
arXiv:2509.23045v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are increasingly applied to software engineering (SWE), with SWE-bench as a key benchmark. Solutions are split into SWE-Agent frameworks with multi-turn interactions and workflow-based Agentless methods with single-turn verifiable steps. We argue these paradigms are not mutually exclusive: reasoning-intensive Agentless training induces skill priors, including localization, code edit, and self-reflection that enable efficient and effective SWE-Agent adaptation. In this work, we first curate the Agentless training recipe and present Kimi-Dev, an open-source SWE LLM achieving 60.4\% on SWE-bench Verified, the best among workflow approaches. With additional SFT adaptation on 5k publicly-available trajectories, Kimi-Dev powers SWE-Agents to 48.6\% pass@1, on par with that of Claude 3.5 Sonnet (241022 version). These results show that structured skill priors from Agentless training can bridge workflow and agentic frameworks for transferable coding agents.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Eyes-on-Me: Scalable RAG Poisoning through Transferable Attention-Steering Attractors</title>
<link>https://arxiv.org/abs/2510.00586</link>
<guid>https://arxiv.org/abs/2510.00586</guid>
<content:encoded><![CDATA[
arXiv:2510.00586v2 Announce Type: replace-cross 
Abstract: Existing data poisoning attacks on retrieval-augmented generation (RAG) systems scale poorly because they require costly optimization of poisoned documents for each target phrase. We introduce Eyes-on-Me, a modular attack that decomposes an adversarial document into reusable Attention Attractors and Focus Regions. Attractors are optimized to direct attention to the Focus Region. Attackers can then insert semantic baits for the retriever or malicious instructions for the generator, adapting to new targets at near zero cost. This is achieved by steering a small subset of attention heads that we empirically identify as strongly correlated with attack success. Across 18 end-to-end RAG settings (3 datasets $\times$ 2 retrievers $\times$ 3 generators), Eyes-on-Me raises average attack success rates from 21.9 to 57.8 (+35.9 points, 2.6$\times$ over prior work). A single optimized attractor transfers to unseen black box retrievers and generators without retraining. Our findings establish a scalable paradigm for RAG data poisoning and show that modular, reusable components pose a practical threat to modern AI systems. They also reveal a strong link between attention concentration and model outputs, informing interpretability research.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.01132</link>
<guid>https://arxiv.org/abs/2510.01132</guid>
<content:encoded><![CDATA[
arXiv:2510.01132v2 Announce Type: replace-cross 
Abstract: We study what actually works and what doesn't for training large language models as agents via multi-turn reinforcement learning. Despite rapid progress, existing frameworks and definitions are fragmented, and there is no systematic formulation or analysis of which design choices matter across tasks. We address this gap by first breaking down the design space into three inter-related pillars -- environment, reward, and policy -- and empirically derive a recipe for training LLM agents in situated textual domains. In particular, we test TextWorld and ALFWorld, popular domains for testing situated embodied reasoning, as well as SWE-Gym for more software engineering style tasks. (i) For the environment, we analyze the impacts of task complexity in terms of sizes of the state and action spaces as well as optimal solution length, finding that even simple environments within a domain can provide signal on how well an agent can generalize to more complex tasks. (ii) For the reward, we ablate relative reward sparsity, observing that while dense turn-level rewards accelerate training, performance and stability is highly dependent on the choice of RL algorithm. (iii) And for the agent's policy, we explore the interplay between reward sparsity and biased (PPO, GRPO) and unbiased (RLOO) policy gradient methods in addition to showing how to find the optimal Supervised Fine-tuning (SFT) to RL training ratio given a fixed budget. We distill these findings into a training recipe that guides co-design across the three pillars, facilitating research and practical efforts in multi-turn agentic RL. Code: https://github.com/pearls-lab/meow-tea-taro
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>General Exploratory Bonus for Optimistic Exploration in RLHF</title>
<link>https://arxiv.org/abs/2510.03269</link>
<guid>https://arxiv.org/abs/2510.03269</guid>
<content:encoded><![CDATA[
arXiv:2510.03269v3 Announce Type: replace-cross 
Abstract: Optimistic exploration is central to improving sample efficiency in reinforcement learning with human feedback, yet existing exploratory bonus methods to incentivize exploration often fail to realize optimism. We provide a theoretical analysis showing that current formulations, under KL or $\alpha$-divergence regularization, unintentionally bias exploration toward high-probability regions of the reference model, thereby reinforcing conservative behavior instead of promoting discovery of uncertain regions. To address this pitfall, we introduce the General Exploratory Bonus (GEB), a novel theoretical framework that provably satisfies the optimism principle. GEB counteracts divergence-induced bias via reference-dependent reward regulation and unifies prior heuristic bonuses as special cases, while extending naturally across the full $\alpha$-divergence family. Empirically, GEB consistently outperforms baselines on alignment tasks across multiple divergence settings and large language model backbones. These results demonstrate that GEB offers both a principled and practical solution for optimistic exploration in RLHF.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Internal World Models as Imagination Networks in Cognitive Agents</title>
<link>https://arxiv.org/abs/2510.04391</link>
<guid>https://arxiv.org/abs/2510.04391</guid>
<content:encoded><![CDATA[
arXiv:2510.04391v3 Announce Type: replace-cross 
Abstract: The computational role of imagination remains debated. While classical accounts emphasize reward maximization, emerging evidence suggests imagination serves a broader function: accessing internal world models (IWMs). Here, we employ psychological network analysis to compare IWMs in humans and large language models (LLMs) through imagination vividness ratings. Using the Vividness of Visual Imagery Questionnaire (VVIQ-2) and Plymouth Sensory Imagery Questionnaire (PSIQ), we construct imagination networks from three human populations (Florida, Poland, London; N=2,743) and six LLM variants in two conversation conditions. Human imagination networks demonstrate robust correlations across centrality measures (expected influence, strength, closeness) and consistent clustering patterns, indicating shared structural organization of IWMs across populations. In contrast, LLM-derived networks show minimal clustering and weak centrality correlations, even when manipulating conversational memory. These systematic differences persist across environmental scenes (VVIQ-2) and sensory modalities (PSIQ), revealing fundamental disparities between human and artificial world models. Our network-based approach provides a quantitative framework for comparing internally-generated representations across cognitive agents, with implications for developing human-like imagination in artificial intelligence systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PoSh: Using Scene Graphs To Guide LLMs-as-a-Judge For Detailed Image Descriptions</title>
<link>https://arxiv.org/abs/2510.19060</link>
<guid>https://arxiv.org/abs/2510.19060</guid>
<content:encoded><![CDATA[
arXiv:2510.19060v2 Announce Type: replace-cross 
Abstract: While vision-language models (VLMs) have advanced into detailed image description, evaluation remains a challenge. Standard metrics (e.g. CIDEr, SPICE) were designed for short texts and tuned to recognize errors that are now uncommon, such as object misidentification. In contrast, long texts require sensitivity to attribute and relation attachments and scores that localize errors to particular text spans. In this work, we introduce PoSh, a metric for detailed image description that uses scene graphs as structured rubrics to guide LLMs-as-a-Judge, producing aggregate scores grounded in fine-grained errors (e.g. mistakes in compositional understanding). PoSh is replicable, interpretable and a better proxy for human raters than existing metrics (including GPT4o-as-a-Judge). To validate PoSh, we introduce a challenging new dataset, DOCENT. This novel benchmark contains artwork, paired with expert-written references, and model-generated descriptions, augmented with granular and coarse judgments of their quality from art history students. Thus, DOCENT enables evaluating both detailed image description metrics and detailed image description itself in a challenging new domain. We show that PoSh achieves stronger correlations (+0.05 Spearman $\rho$) with the human judgments in DOCENT than the best open-weight alternatives, is robust to image type (using CapArena, an existing dataset of web imagery) and is a capable reward function, outperforming standard supervised fine-tuning. Then, using PoSh, we characterize the performance of open and closed models in describing the paintings, sketches and statues in DOCENT and find that foundation models struggle to achieve full, error-free coverage of images with rich scene dynamics, establishing a demanding new task to gauge VLM progress. Through both PoSh and DOCENT, we hope to enable advances in important areas such as assistive text generation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning Machiavellian Agents: Behavior Steering via Test-Time Policy Shaping</title>
<link>https://arxiv.org/abs/2511.11551</link>
<guid>https://arxiv.org/abs/2511.11551</guid>
<content:encoded><![CDATA[
arXiv:2511.11551v3 Announce Type: replace-cross 
Abstract: The deployment of decision-making AI agents presents a critical challenge in maintaining alignment with human values or guidelines while operating in complex, dynamic environments. Agents trained solely to achieve their objectives may adopt harmful behavior, exposing a key trade-off between maximizing the reward function and maintaining alignment. For pre-trained agents, ensuring alignment is particularly challenging, as retraining can be a costly and slow process. This is further complicated by the diverse and potentially conflicting attributes representing the ethical values for alignment. To address these challenges, we propose a test-time alignment technique based on model-guided policy shaping. Our method allows precise control over individual behavioral attributes, generalizes across diverse reinforcement learning (RL) environments, and facilitates a principled trade-off between ethical alignment and reward maximization without requiring agent retraining. We evaluate our approach using the MACHIAVELLI benchmark, which comprises 134 text-based game environments and thousands of annotated scenarios involving ethical decisions. The RL agents are first trained to maximize the reward in their respective games. At test time, we apply policy shaping via scenario-action attribute classifiers to ensure decision alignment with ethical attributes. We compare our approach against prior training-time methods and general-purpose agents, as well as study several types of ethical violations and power-seeking behavior. Our results demonstrate that test-time policy shaping provides an effective and scalable solution for mitigating unethical behavior across diverse environments and alignment attributes.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Content-Preserving Secure Linguistic Steganography</title>
<link>https://arxiv.org/abs/2511.12565</link>
<guid>https://arxiv.org/abs/2511.12565</guid>
<content:encoded><![CDATA[
arXiv:2511.12565v2 Announce Type: replace-cross 
Abstract: Existing linguistic steganography methods primarily rely on content transformations to conceal secret messages. However, they often cause subtle yet looking-innocent deviations between normal and stego texts, posing potential security risks in real-world applications. To address this challenge, we propose a content-preserving linguistic steganography paradigm for perfectly secure covert communication without modifying the cover text. Based on this paradigm, we introduce CLstega (\textit{C}ontent-preserving \textit{L}inguistic \textit{stega}nography), a novel method that embeds secret messages through controllable distribution transformation. CLstega first applies an augmented masking strategy to locate and mask embedding positions, where MLM(masked language model)-predicted probability distributions are easily adjustable for transformation. Subsequently, a dynamic distribution steganographic coding strategy is designed to encode secret messages by deriving target distributions from the original probability distributions. To achieve this transformation, CLstega elaborately selects target words for embedding positions as labels to construct a masked sentence dataset, which is used to fine-tune the original MLM, producing a target MLM capable of directly extracting secret messages from the cover text. This approach ensures perfect security of secret messages while fully preserving the integrity of the original cover text. Experimental results show that CLstega can achieve a 100\% extraction success rate, and outperforms existing methods in security, effectively balancing embedding capacity and security.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Oracle and The Prism: A Decoupled and Efficient Framework for Generative Recommendation Explanation</title>
<link>https://arxiv.org/abs/2511.16543</link>
<guid>https://arxiv.org/abs/2511.16543</guid>
<content:encoded><![CDATA[
arXiv:2511.16543v2 Announce Type: replace-cross 
Abstract: The integration of Large Language Models (LLMs) into explainable recommendation systems often leads to a performance-efficiency trade-off in end-to-end architectures, where joint optimization of ranking and explanation can result in suboptimal compromises. To resolve this, we propose Prism, a novel decoupled framework that rigorously separates the recommendation process into a dedicated ranking stage and an explanation generation stage. This decomposition ensures that each component is optimized for its specific objective, eliminating inherent conflicts in coupled models.
  Inspired by knowledge distillation, Prism leverages a powerful, instruction-following teacher LLM (FLAN-T5-XXL) as an Oracle to produce high-fidelity explanatory knowledge. A compact, fine-tuned student model (BART-Base), the Prism, then specializes in synthesizing this knowledge into personalized explanations. Our extensive experiments on benchmark datasets reveal a key finding: the distillation process not only transfers knowledge but also acts as a noise filter. Our 140M-parameter Prism model significantly outperforms its 11B-parameter teacher in human evaluations of faithfulness and personalization, demonstrating an emergent ability to correct hallucinations present in the teacher's outputs. While achieving a 24x speedup and a 10x reduction in memory consumption, our analysis validates that decoupling, coupled with targeted distillation, provides an efficient and effective pathway to high-quality, and perhaps more importantly, trustworthy explainable recommendation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Tuning BERT for Domain-Specific Question Answering: Toward Educational NLP Resources at University Scale</title>
<link>https://arxiv.org/abs/2512.05179</link>
<guid>https://arxiv.org/abs/2512.05179</guid>
<content:encoded><![CDATA[
<div> Keywords: scientific question answering, BERT fine-tuning, educational domain, university course materials, QA chatbot<br /><br />Summary: Prior research in scientific question answering has mainly focused on chatbot-type systems, with limited work on fine-tuning foundational language models for specialized domain reasoning. This study presents the development of a chatbot tailored specifically for the University of Limerick's Department of Electronic and Computer Engineering to assist students by providing course-related information. A custom dataset was created comprising 1,203 question-answer pairs formatted similarly to SQuAD, derived primarily from the university's official book of modules, and enhanced with both manually and synthetically generated entries. The BERT language model was fine-tuned using the PyTorch framework and evaluated by standard metrics such as Exact Match and F1 scores. The results demonstrated that even modest fine-tuning improves the model’s ability to accurately frame hypotheses and extract relevant knowledge from the dataset. Unlike existing domain-specific BERT variants like BioBERT and SciBERT, which target biomedical and scientific literature respectively, no model had previously been adapted expressly for university course materials. This work fills that gap by proving that fine-tuning BERT with academic question-answer pairs can yield effective domain-specific performance. The findings suggest promising prospects for scaling such models to create autonomous educational knowledge systems for universities. <div>
arXiv:2512.05179v1 Announce Type: new 
Abstract: Prior work on scientific question answering has largely emphasized chatbot-style systems, with limited exploration of fine-tuning foundation models for domain-specific reasoning. In this study, we developed a chatbot for the University of Limerick's Department of Electronic and Computer Engineering to provide course information to students. A custom dataset of 1,203 question-answer pairs in SQuAD format was constructed using the university book of modules, supplemented with manually and synthetically generated entries. We fine-tuned BERT (Devlin et al., 2019) using PyTorch and evaluated performance with Exact Match and F1 scores. Results show that even modest fine-tuning improves hypothesis framing and knowledge extraction, demonstrating the feasibility of adapting foundation models to educational domains. While domain-specific BERT variants such as BioBERT and SciBERT exist for biomedical and scientific literature, no foundation model has yet been tailored to university course materials. Our work addresses this gap by showing that fine-tuning BERT with academic QA pairs yields effective results, highlighting the potential to scale towards the first domain-specific QA model for universities and enabling autonomous educational knowledge systems.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unveiling Affective Polarization Trends in Parliamentary Proceedings</title>
<link>https://arxiv.org/abs/2512.05231</link>
<guid>https://arxiv.org/abs/2512.05231</guid>
<content:encoded><![CDATA[
<div> Keywords: polarization, emotional style, Valence Arousal Dominance, affective polarization, Knesset

<br /><br />Summary:  
This article addresses the rising issue of polarized discourse globally and introduces a novel method to quantify polarization based on the emotional style of communication rather than ideological differences. The method employs psychological measures of Valence (positivity-negativity), Arousal (intensity of emotion), and Dominance (control in emotion expression) to detect affective polarization signals in discourse. The researchers apply this approach specifically to a newly available corpus containing the proceedings of the Israeli parliament, the Knesset, conducted in Hebrew. Their analysis reveals distinct emotional styles between members of the government and the opposition, indicating that emotional expression varies depending on political alignment. Furthermore, the study finds a significant upward trend in affective polarization over time, suggesting that emotional divides in parliamentary communication are deepening. This approach offers a fresh perspective by focusing on affective dimensions rather than purely ideological content, providing valuable insights into how political polarization manifests emotionally in legislative dialogues. The findings suggest potential implications for understanding the dynamics of political conflict and social cohesion within parliamentary environments. <div>
arXiv:2512.05231v1 Announce Type: new 
Abstract: Recent years have seen an increase in polarized discourse worldwide, on various platforms. We propose a novel method for quantifying polarization, based on the emotional style of the discourse rather than on differences in ideological stands. Using measures of Valence, Arousal and Dominance, we detect signals of emotional discourse and use them to operationalize the concept of affective polarization. Applying this method to a recently released corpus of proceedings of the Knesset, the Israeli parliament (in Hebrew), we find that the emotional style of members of government differs from that of opposition members; and that the level of affective polarization, as reflected by this style, is significantly increasing with time.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoding the Black Box: Discerning AI Rhetorics About and Through Poetic Prompting</title>
<link>https://arxiv.org/abs/2512.05243</link>
<guid>https://arxiv.org/abs/2512.05243</guid>
<content:encoded><![CDATA[
<div> Keywords: prompt engineering, large language models, creative text prompting, Poetry Prompt Patterns, model adaptation<br /><br />Summary: This article explores the role of prompt engineering in understanding the behaviors and biases of large language models (LLMs), with a specific focus on creative text prompting through Poetry Prompt Patterns. It proposes that integrating poetic prompts can enrich the prompt engineer’s methods by providing novel ways to interact with and assess LLMs. The paper outlines a systematic approach to applying poetic prompts for prompt engineering purposes. It then tests this approach by using poetic prompts to generate descriptions and evaluations of three different models of a famous poet, examining how LLMs interpret and recreate established creative works. Furthermore, the study investigates the effects of models’ readiness to adapt or rewrite original creative content, considering the implications of tailoring outputs for different or presumed audiences. Overall, the research advocates for creative text prompting, especially poetry-based techniques, as a valuable tool to deepen the understanding and practical application of LLMs in both creative and analytical contexts. <div>
arXiv:2512.05243v1 Announce Type: new 
Abstract: Prompt engineering has emerged as a useful way studying the algorithmic tendencies and biases of large language models. Meanwhile creatives and academics have leveraged LLMs to develop creative works and explore the boundaries of their writing capabilities through text generation and code. This study suggests that creative text prompting, specifically Poetry Prompt Patterns, may be a useful addition to the toolbox of the prompt engineer, and outlines the process by which this approach may be taken. Then, the paper uses poetic prompts to assess descriptions and evaluations of three models of a renowned poet and test the consequences of the willingness of models to adapt or rewrite original creative works for presumed audiences.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Clinical Note Generation with ICD-10, Clinical Ontology Knowledge Graphs, and Chain-of-Thought Prompting Using GPT-4</title>
<link>https://arxiv.org/abs/2512.05256</link>
<guid>https://arxiv.org/abs/2512.05256</guid>
<content:encoded><![CDATA[
<div> Keywords: electronic health records, clinical notes, large language models, Chain-of-Thought prompting, knowledge graph<br /><br />Summary:<br /><br />In the last decade, the volume of electronic health record (EHR) data in the U.S. has grown significantly, driven by supportive policies such as the HITECH Act of 2009 and the 21st Century Cures Act of 2016. Physicians document patient assessments, diagnoses, and treatments in free-text clinical notes within EHRs, a process that consumes considerable time and may delay patient care. Large language models (LLMs), known for generating human-like text, have potential to automate clinical note generation. This study explores the use of Chain-of-Thought (CoT) prompt engineering to enhance LLM performance in generating clinical notes. The method uses input data including International Classification of Diseases (ICD) codes and basic patient information. The approach integrates traditional CoT prompting with semantic search results to improve note quality. Additionally, a clinical knowledge graph (KG) derived from clinical ontology was infused to enrich domain-specific knowledge in the generated notes. Experiments were conducted on six clinical cases from the CodiEsp test dataset using GPT-4. The proposed prompting technique with CoT, semantic search, and KG integration outperformed standard one-shot prompting methods in generating higher quality clinical notes. <div>
arXiv:2512.05256v1 Announce Type: new 
Abstract: In the past decade a surge in the amount of electronic health record (EHR) data in the United States, attributed to a favorable policy environment created by the Health Information Technology for Economic and Clinical Health (HITECH) Act of 2009 and the 21st Century Cures Act of 2016. Clinical notes for patients' assessments, diagnoses, and treatments are captured in these EHRs in free-form text by physicians, who spend a considerable amount of time entering and editing them. Manually writing clinical notes takes a considerable amount of a doctor's valuable time, increasing the patient's waiting time and possibly delaying diagnoses. Large language models (LLMs) possess the ability to generate news articles that closely resemble human-written ones. We investigate the usage of Chain-of-Thought (CoT) prompt engineering to improve the LLM's response in clinical note generation. In our prompts, we use as input International Classification of Diseases (ICD) codes and basic patient information. We investigate a strategy that combines the traditional CoT with semantic search results to improve the quality of generated clinical notes. Additionally, we infuse a knowledge graph (KG) built from clinical ontology to further enrich the domain-specific knowledge of generated clinical notes. We test our prompting technique on six clinical cases from the CodiEsp test dataset using GPT-4 and our results show that it outperformed the clinical notes generated by standard one-shot prompts.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>To Think or Not to Think: The Hidden Cost of Meta-Training with Excessive CoT Examples</title>
<link>https://arxiv.org/abs/2512.05318</link>
<guid>https://arxiv.org/abs/2512.05318</guid>
<content:encoded><![CDATA[
<div> Chain-of-thought, few-shot learning, meta-training, in-context learning, symbolic reasoning<br /><br />Summary: This paper addresses the limitations of chain-of-thought (CoT) prompting combined with few-shot in-context learning (ICL) for large language models (LLMs) when applied to novel tasks with insufficient pre-training knowledge. The authors introduce the CoT-ICL Lab framework to study this problem in a controlled environment and propose meta-training techniques designed to help models learn new abstract reasoning tasks in-context. They observe that although CoT examples aid reasoning, an excessive amount during meta-training can degrade performance when CoT supervision is scarce. To solve this, they introduce CoT-Recipe, a formal method that modulates the ratio of CoT and non-CoT examples in meta-training sequences. Experimental results demonstrate that using CoT-Recipe can improve transformer accuracy on novel tasks by up to 300%, even when no CoT examples are present during in-context inference. Furthermore, when applying these meta-training modulation techniques to pretrained LLMs such as the Qwen2.5 series for symbolic reasoning tasks, accuracy gains of up to 130% are observed. Overall, the work highlights the importance of balanced meta-training data and presents a practical approach to enhance generalization of reasoning abilities in LLMs under low CoT supervision scenarios. <div>
arXiv:2512.05318v1 Announce Type: new 
Abstract: Chain-of-thought (CoT) prompting combined with few-shot in-context learning (ICL) has unlocked significant reasoning capabilities in large language models (LLMs). However, ICL with CoT examples is ineffective on novel tasks when the pre-training knowledge is insufficient. We study this problem in a controlled setting using the CoT-ICL Lab framework, and propose meta-training techniques to learn novel abstract reasoning tasks in-context. Although CoT examples facilitate reasoning, we noticed that their excessive inclusion during meta-training degrades performance when CoT supervision is limited. To mitigate such behavior, we propose CoT-Recipe, a formal approach to modulate the mix of CoT and non-CoT examples in meta-training sequences. We demonstrate that careful modulation via CoT-Recipe can increase the accuracy of transformers on novel tasks by up to 300% even when there are no CoT examples available in-context. We confirm the broader effectiveness of these techniques by applying them to pretrained LLMs (Qwen2.5 series) for symbolic reasoning tasks and observing gains of up to 130% in accuracy.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LYNX: Learning Dynamic Exits for Confidence-Controlled Reasoning</title>
<link>https://arxiv.org/abs/2512.05325</link>
<guid>https://arxiv.org/abs/2512.05325</guid>
<content:encoded><![CDATA[
<div> Keywords: large reasoning models, early-exit mechanism, LYNX, confidence-controlled stopping, token efficiency  

<br /><br />Summary:  
This paper addresses the issue of "overthinking" in large reasoning models, where models continue to generate extended chains of thought even after obtaining sufficient information to answer correctly, leading to wasted compute and potential accuracy drops. The authors propose LYNX, an online early-exit mechanism that leverages a model's hidden-state awareness by attaching exit decisions to natural reasoning cues (e.g., "hmm", "wait") within the generated text. LYNX trains a lightweight probe on these hidden states supervised by forced exit data and applies split conformal prediction to maintain distribution-free confidence guarantees on stopping decisions. The probe is trained once on a generic mathematical corpus and reuses the same model across various benchmarks, decoding temperatures, and different task domains, including non-mathematical tasks. Experimental results across three model families (ranging from 1.5B to 32B parameters) demonstrate that LYNX can significantly reduce token usage—by 40-70%—while maintaining or improving accuracy on multiple benchmarks like GSM8K, MATH-500, AIME 2024, and CommonsenseQA. Compared to existing early-exit methods, LYNX achieves competitive or superior performance, is fully online (no need for auxiliary verifier models), and offers explicit user-tunable confidence controls, making it an effective and efficient solution for early stopping in reasoning models. <div>
arXiv:2512.05325v1 Announce Type: new 
Abstract: Large reasoning models achieve strong performance on complex tasks by generating extended chains of thought, but they often "overthink": continuing to reason long after they have enough information to answer correctly. This wastes inference-time compute and can hurt accuracy. Existing attempts to stop early either manipulate decoding with extra sampling and heuristics, rely on auxiliary verifier models, or operate only as post-hoc analysis pipelines without formal guarantees. We introduce LYNX, an online early-exit mechanism that turns a model's own hidden-state awareness into confidence-controlled stopping decisions. LYNX attaches exit decisions to naturally occurring reasoning cues (e.g., "hmm", "wait") during generation, trains a lightweight probe on hidden states at those cue tokens using supervision from forced exits, and wraps the resulting scores in split conformal prediction to obtain distribution-free control over premature exits. Crucially, we train and calibrate this probe once on a generic mathematical corpus and reuse it unchanged across benchmarks, decoding temperatures, and even non-mathematical tasks. Across three model families spanning 1.5B to 32B parameters, a single mathematically trained probe per base model yields strong accuracy--efficiency tradeoffs. On GSM8K, LYNX matches or improves baseline accuracy while reducing tokens by 40--65\%; on MATH-500 it improves accuracy by up to 12 points with roughly 35--60\% fewer tokens; on AIME 2024 it recovers baseline accuracy with more than 50\% token savings; and on CommonsenseQA, a non-math benchmark, it transfers zero-shot with modest accuracy gains and up to 70\% fewer tokens. Compared to state-of-the-art early-exit methods, LYNX offers competitive or superior Pareto frontiers while remaining fully online, requiring no proxy models at inference, and providing explicit, user-tunable confidence guarantees.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exposing Pink Slime Journalism: Linguistic Signatures and Robust Detection Against LLM-Generated Threats</title>
<link>https://arxiv.org/abs/2512.05331</link>
<guid>https://arxiv.org/abs/2512.05331</guid>
<content:encoded><![CDATA[
<div> Pink Slime Journalism, local news, large language models, adversarial attacks, detection systems<br /><br />Summary:  
The study addresses the growing threat of Pink Slime Journalism, which consists of low-quality, auto-generated articles that imitate legitimate local news and impact 28 million Americans relying on these sources. The research conducts an in-depth analysis of the linguistic, stylistic, and lexical features that differentiate Pink Slime content from genuine reporting. A key contribution is identifying a new adversarial tactic where large language models (LLMs) modify Pink Slime articles, making detection significantly more difficult. The study demonstrates that even widely accessible consumer LLMs can reduce existing detection systems’ effectiveness by as much as 40% in F1-score. To mitigate this challenge, the authors propose a robust learning framework tailored to withstand LLM-based adversarial manipulation and adapt to continuous evolution within automated local news content. The framework shows promising results, achieving up to a 27% improvement in detection performance, indicating its potential to safeguard the local news landscape from deceptive, machine-generated misinformation. <div>
arXiv:2512.05331v1 Announce Type: new 
Abstract: The local news landscape, a vital source of reliable information for 28 million Americans, faces a growing threat from Pink Slime Journalism, a low-quality, auto-generated articles that mimic legitimate local reporting. Detecting these deceptive articles requires a fine-grained analysis of their linguistic, stylistic, and lexical characteristics. In this work, we conduct a comprehensive study to uncover the distinguishing patterns of Pink Slime content and propose detection strategies based on these insights. Beyond traditional generation methods, we highlight a new adversarial vector: modifications through large language models (LLMs). Our findings reveal that even consumer-accessible LLMs can significantly undermine existing detection systems, reducing their performance by up to 40% in F1-score. To counter this threat, we introduce a robust learning framework specifically designed to resist LLM-based adversarial attacks and adapt to the evolving landscape of automated pink slime journalism, and showed and improvement by up to 27%.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transformer-Enabled Diachronic Analysis of Vedic Sanskrit: Neural Methods for Quantifying Types of Language Change</title>
<link>https://arxiv.org/abs/2512.05364</link>
<guid>https://arxiv.org/abs/2512.05364</guid>
<content:encoded><![CDATA[
<div> Keywords: hybrid neural-symbolic methods, morphological complexity, Sanskrit evolution, weak supervision, confidence-weighted ensemble  

<br /><br />Summary:  
This study explores the application of hybrid neural-symbolic methods to analyze the evolution of Sanskrit, a morphologically rich and low-resource language, across more than 2,000 years. It challenges the prevalent assumption that linguistic change equates to simplification by demonstrating that morphological complexity in Sanskrit does not simply decrease but instead redistributes dynamically over time. The approach uses weak supervision, employing over 100 high-precision regex patterns to create pseudo-labels for fine-tuning a multilingual BERT model, thereby effectively addressing the scarcity of annotated data. A novel confidence-weighted ensemble method fuses symbolic and neural outputs, resulting in a system that is both scalable and interpretable. When applied to a large diachronic corpus of 1.47 million words, this ensemble achieves a 52.4% overall feature detection rate. The research uncovers cyclical declines in certain verbal morphological features while showing increased complexity in other linguistic areas, including a rise in compound formations and novel philosophical vocabulary. Importantly, the system yields well-calibrated uncertainty estimates, with confidence scores strongly correlating with accuracy (Pearson r = 0.92) and demonstrating low expected calibration error (ECE = 0.043), which enhances the dependability of computational philological insights derived from this work. <div>
arXiv:2512.05364v1 Announce Type: new 
Abstract: This study demonstrates how hybrid neural-symbolic methods can yield significant new insights into the evolution of a morphologically rich, low-resource language. We challenge the naive assumption that linguistic change is simplification by quantitatively analyzing over 2,000 years of Sanskrit, demonstrating how weakly-supervised hybrid methods can yield new insights into the evolution of morphologically rich, low-resource languages. Our approach addresses data scarcity through weak supervision, using 100+ high-precision regex patterns to generate pseudo-labels for fine-tuning a multilingual BERT. We then fuse symbolic and neural outputs via a novel confidence-weighted ensemble, creating a system that is both scalable and interpretable. Applying this framework to a 1.47-million-word diachronic corpus, our ensemble achieves a 52.4% overall feature detection rate. Our findings reveal that Sanskrit's overall morphological complexity does not decrease but is instead dynamically redistributed: while earlier verbal features show cyclical patterns of decline, complexity shifts to other domains, evidenced by a dramatic expansion in compounding and the emergence of new philosophical terminology. Critically, our system produces well-calibrated uncertainty estimates, with confidence strongly correlating with accuracy (Pearson r = 0.92) and low overall calibration error (ECE = 0.043), bolstering the reliability of these findings for computational philology.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Self-Preference by Authorship Obfuscation</title>
<link>https://arxiv.org/abs/2512.05379</link>
<guid>https://arxiv.org/abs/2512.05379</guid>
<content:encoded><![CDATA[
<div> self-preference, language model judges, bias mitigation, synonym replacement, stylistic differences<br /><br />Summary:<br /><br />1. Language model (LM) judges are commonly used to evaluate the quality of outputs generated by language models, but they exhibit biases that can undermine evaluation integrity.<br />2. A notable bias identified is self-preference, where LM judges favor their own generated answers over those created by other models or humans.<br />3. This bias is difficult to remove because advanced LM judges can recognize their own outputs even when evaluation samples are anonymized.<br />4. The study investigates ways to reduce self-preference by obstructing the LM judges’ ability to detect their own authorship through black-box perturbations applied to comparison candidates.<br />5. Simple perturbations such as replacing a few words with synonyms reliably decrease self-preference, indicating initial success in bias mitigation.<br />6. However, the researchers find a fundamental challenge: when stylistic differences between evaluation candidates are almost entirely neutralized, the self-preference bias resurfaces.<br />7. This implies that self-recognition and self-preference may be operating on multiple semantic levels, not just surface features.<br />8. Consequently, fully eliminating self-preference bias in LM judges remains a challenging problem despite the promising results from perturbation strategies. <div>
arXiv:2512.05379v1 Announce Type: new 
Abstract: Language models (LMs) judges are widely used to evaluate the quality of LM outputs. Despite many advantages, LM judges display concerning biases that can impair their integrity in evaluations. One such bias is self-preference: LM judges preferring their own answers over those produced by other LMs or humans. The bias is hard to eliminate as frontier LM judges can distinguish their own outputs from those of others, even when the evaluation candidates are not labeled with their sources. In this paper, we investigate strategies to mitigate self-preference by reducing the LM judges' ability to recognize their own outputs. We apply black-box perturbations to evaluation candidates in pairwise comparison to obfuscate the authorship and reduce self-recognition. We find that perturbations as simple as synonym replacement for a few words predictably reduce self-preference. However, we also uncover fundamental challenges to eliminating the bias: when we extrapolate our perturbations to a more complete neutralization of stylistic differences between the evaluation candidates, self-preference recovers. Our findings suggest that self-recognition and self-preference can happen on many semantic levels, and complete mitigation remains challenging despite promising initial results.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning from Self Critique and Refinement for Faithful LLM Summarization</title>
<link>https://arxiv.org/abs/2512.05387</link>
<guid>https://arxiv.org/abs/2512.05387</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, hallucination, self-supervised learning, summarization, preference optimization  

<br /><br />Summary: Large Language Models (LLMs) frequently generate hallucinations, producing content not grounded in the input during long-form text generation tasks like summarization. Prior solutions to this issue involve iterative critique and refinement using either the same model or a more powerful teacher model, but these methods are computationally expensive or depend on access to stronger models, limiting practicality. To address these limitations, the paper proposes Self Critique and Refinement-based Preference Optimization (SCRPO), a self-supervised training framework. SCRPO constructs a preference dataset by leveraging the LLM’s intrinsic critique and refinement abilities and uses preference learning to enhance the LLM’s faithfulness in summarization. The approach was experimentally validated on three popular summarization benchmarks: XSUM, CNNDM, and SAMSum. Results show that SCRPO outperforms existing state-of-the-art self-supervised learning techniques regarding faithfulness metrics while maintaining or improving overall summary quality metrics. Furthermore, compared to test-time refinement methods, SCRPO improves computational efficiency and generates more faithful summaries, making it a practical and effective solution to reduce hallucinations in LLM-generated summaries. <div>
arXiv:2512.05387v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often suffer from hallucinations: output content that is not grounded in the input context, when performing long-form text generation tasks such as summarization. Prior works have shown that hallucinations can be reduced by iteratively critiquing and refining previously generated outputs using either the same model or a more powerful teacher model as the critique. However, these approaches either require additional test-time compute or assume access to more powerful teacher models, making them costly and less practical. In this work, we propose Self Critique and Refinement-based Preference Optimization (SCRPO), which is a self-supervised training framework that first constructs a preference dataset by leveraging the LLM's own critique and refinement capabilities, and then applies preference learning to improve the same LLM for faithful summarization. Experiments on three summarization benchmarks (XSUM CNNDM and SAMSum), demonstrate that our approach outperforms state-of-the-art self-supervised learning methods in terms of faithfulness metrics while either maintaining or improving other metrics that measure the overall quality of the summary. Moreover, compared to test-time refinement, our approach not only improves efficiency but also results in more faithful summaries.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs</title>
<link>https://arxiv.org/abs/2512.05409</link>
<guid>https://arxiv.org/abs/2512.05409</guid>
<content:encoded><![CDATA[
<div> Sparse-Quantized Format (SQ-format), Post-training quantization (PTQ), Large language models (LLMs), Low-bit quantization, Hardware acceleration  

<br /><br />Summary:  
This paper addresses the challenge of balancing accuracy and efficiency in post-training quantization (PTQ) for large language models (LLMs), particularly when using low-bit quantization and sparsification techniques. Existing methods like W4A8 fail to improve peak tera operations per second (TOPS) beyond W8A8, and hardware-supported sparse formats such as 2:4 semi-structured sparsity suffer from accuracy loss, limiting their adoption. To overcome these limitations, the authors propose a novel Sparse-Quantized Format (SQ-format), a unified data representation that integrates quantization and sparsification while being compatible with both new hardware designs and existing GPUs. The SQ-format leverages the acceleration of sparse matrices in high precision alongside efficient low-precision matrix multiplication, enabling a Pareto improvement by enhancing both performance and throughput. It is especially effective for activations characterized by outlier inequality, allowing for static compression of such activations. The paper demonstrates state-of-the-art PTQ results using SQ-format, outlines necessary hardware support to optimize its performance, and presents design explorations and insights aimed at guiding the development of the next generation of AI accelerators. <div>
arXiv:2512.05409v1 Announce Type: new 
Abstract: Post-training quantization (PTQ) plays a crucial role in the democratization of large language models (LLMs). However, existing low-bit quantization and sparsification techniques are difficult to balance accuracy and efficiency due to the limited hardware support. For example, W4A8 can only achieve the same peak TOPS as W8A8 whereas the GPU-supported sparse data format (2:4 semi-structure sparse) is seldomly adopted due to the loss of accuracy. To bridge this gap, in this paper, we propose the Sparse-Quantized Format (SQ-format), which is a unified data format for quantization and sparsification potentially easily supported by new hardware and existing GPUs. SQ-format makes use of the fact that sparse matrix can be accelerated in high-precision, and low-precision matrix multiplication can also be accelerated accordingly. As such, SQ-format is proposed to achieve Pareto improvement between performance and throughput. This format is particularly suitable for activations with outlier inequality status and makes their static compression possible. We show the state-of-the-art PTQ performance with SQ-format, propose the hardware required to support it, and further offer the design exploration and insights for the next-generation AI accelerators.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LMSpell: Neural Spell Checking for Low-Resource Languages</title>
<link>https://arxiv.org/abs/2512.05414</link>
<guid>https://arxiv.org/abs/2512.05414</guid>
<content:encoded><![CDATA[
<div> Keywords: spell correction, low-resource languages, pretrained language models, large language models, LMSpell<br /><br />Summary: This study addresses the challenge of spell correction in low-resource languages (LRLs), an area with limited research despite the availability of pretrained language models (PLMs). The authors conduct the first comprehensive empirical comparison of various PLMs—encoder-based, encoder-decoder, and large language models (LLMs)—for spell correction tasks, including for LRLs. Their findings reveal that LLMs outperform other model types when trained on sufficiently large fine-tuning datasets. Notably, this superior performance of LLMs extends even to languages that were not part of the model’s original pretraining. To support the research community and practitioners, the authors release LMSpell, a user-friendly spell correction toolkit compatible with multiple PLMs. LMSpell also features an evaluation function specifically designed to mitigate hallucination effects commonly observed in LLM outputs. Additionally, the paper presents a focused case study on Sinhala, a low-resource language, to exemplify the unique challenges faced in spell correction within such linguistic contexts. Overall, this work provides valuable insights and practical tools to advance spell correction efforts for low-resource languages using modern language modeling techniques. <div>
arXiv:2512.05414v1 Announce Type: new 
Abstract: Spell correction is still a challenging problem for low-resource languages (LRLs). While pretrained language models (PLMs) have been employed for spell correction, their use is still limited to a handful of languages, and there has been no proper comparison across PLMs. We present the first empirical study on the effectiveness of PLMs for spell correction, which includes LRLs. We find that Large Language Models (LLMs) outperform their counterparts (encoder-based and encoder-decoder) when the fine-tuning dataset is large. This observation holds even in languages for which the LLM is not pre-trained. We release LMSpell, an easy- to use spell correction toolkit across PLMs. It includes an evaluation function that compensates for the hallucination of LLMs. Further, we present a case study with Sinhala to shed light on the plight of spell correction for LRLs.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ArtistMus: A Globally Diverse, Artist-Centric Benchmark for Retrieval-Augmented Music Question Answering</title>
<link>https://arxiv.org/abs/2512.05430</link>
<guid>https://arxiv.org/abs/2512.05430</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, music question answering, MusWikiDB, retrieval-augmented generation, ArtistMus  

<br /><br />Summary:  
Recent developments in large language models (LLMs) have significantly advanced open-domain question answering but face challenges in music-related reasoning due to limited music knowledge in training data. To address this, the authors introduce MusWikiDB, a specialized vector database containing 3.2 million passages from 144,000 music-related Wikipedia pages. Alongside this, they present ArtistMus, a benchmark dataset comprising 1,000 questions on 500 diverse artists enriched with metadata like genre, debut year, and topics. These resources enable rigorous evaluation of retrieval-augmented generation (RAG) techniques tailored for music question answering (MQA). Experimental results demonstrate that RAG substantially enhances factual accuracy, with open-source models (e.g., Qwen3 8B) improving by up to +56.8 percentage points, nearing the performance of proprietary models. Moreover, RAG-style fine-tuning further boosts both factual recall and contextual reasoning, benefiting in-domain and out-of-domain benchmarks. The MusWikiDB corpus outperforms general-purpose Wikipedia databases by approximately 6 percentage points in accuracy and offers 40% faster retrieval. The release of MusWikiDB and ArtistMus aims to foster research at the intersection of music information retrieval and domain-specific question answering, setting the stage for effective retrieval-augmented reasoning in culturally rich domains such as music. <div>
arXiv:2512.05430v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have transformed open-domain question answering, yet their effectiveness in music-related reasoning remains limited due to sparse music knowledge in pretraining data. While music information retrieval and computational musicology have explored structured and multimodal understanding, few resources support factual and contextual music question answering (MQA) grounded in artist metadata or historical context. We introduce MusWikiDB, a vector database of 3.2M passages from 144K music-related Wikipedia pages, and ArtistMus, a benchmark of 1,000 questions on 500 diverse artists with metadata such as genre, debut year, and topic. These resources enable systematic evaluation of retrieval-augmented generation (RAG) for MQA. Experiments show that RAG markedly improves factual accuracy; open-source models gain up to +56.8 percentage points (for example, Qwen3 8B improves from 35.0 to 91.8), approaching proprietary model performance. RAG-style fine-tuning further boosts both factual recall and contextual reasoning, improving results on both in-domain and out-of-domain benchmarks. MusWikiDB also yields approximately 6 percentage points higher accuracy and 40% faster retrieval than a general-purpose Wikipedia corpus. We release MusWikiDB and ArtistMus to advance research in music information retrieval and domain-specific question answering, establishing a foundation for retrieval-augmented reasoning in culturally rich domains such as music.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Alignment for Collective Agency: Toward a Scalable Self-Improving Framework for Open-Ended LLM Alignment</title>
<link>https://arxiv.org/abs/2512.05464</link>
<guid>https://arxiv.org/abs/2512.05464</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Alignment, Collective Agency, Dynamic Alignment, Self-rewarding mechanism<br /><br />Summary:<br /><br />1. The paper addresses the limitations of traditional alignment methods for Large Language Models (LLMs), which rely heavily on human feedback and predefined value principles such as helpfulness, honesty, and harmlessness. These approaches are resource-intensive and may not scale effectively as AI advances toward Artificial General Intelligence (AGI) and Artificial Superintelligence (ASI).<br /><br />2. The authors propose a new, more holistic alignment objective called Collective Agency (CA), which represents a unified and open-ended value system designed to foster integrated agentic capabilities beyond conventional alignment norms.<br /><br />3. To achieve scalable and iterative self-alignment, they introduce a novel alignment framework named Dynamic Alignment. This framework allows the LLM to self-improve by repeatedly refining its alignment through an automated process.<br /><br />4. Dynamic Alignment consists of two main components: first, automated generation of training datasets using LLMs themselves; and second, a self-rewarding mechanism, where the policy model evaluates its own outputs and assigns rewards, enabling learning via GRPO (Generalized Policy Reward Optimization).<br /><br />5. Experimental evaluations show that this approach effectively aligns the model to the Collective Agency value system without sacrificing its general natural language processing capabilities, demonstrating a promising pathway for scalable and comprehensive LLM alignment. <div>
arXiv:2512.05464v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are typically aligned with human values using preference data or predefined principles such as helpfulness, honesty, and harmlessness. However, as AI systems progress toward Artificial General Intelligence (AGI) and Artificial Superintelligence (ASI), such value systems may become insufficient. In addition, human feedback-based alignment remains resource-intensive and difficult to scale. While AI-feedback-based self-improving alignment methods have been explored as a scalable alternative, they have largely remained constrained to conventional alignment values. In this work, we explore both a more holistic alignment objective and a scalable, self-improving alignment approach. Aiming to transcend conventional alignment norms, we introduce Collective Agency (CA)-a unified and open-ended alignment value that encourages integrated agentic capabilities. We also propose Dynamic Alignment-an alignment framework that enables an LLM to iteratively align itself. Dynamic Alignment comprises two key components: (1) automated training dataset generation with LLMs, and (2) a self-rewarding mechanism, where the policy model evaluates its own output candidates and assigns rewards for GRPO-based learning. Experimental results demonstrate that our approach successfully aligns the model to CA while preserving general NLP capabilities.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SEA-SafeguardBench: Evaluating AI Safety in SEA Languages and Cultures</title>
<link>https://arxiv.org/abs/2512.05501</link>
<guid>https://arxiv.org/abs/2512.05501</guid>
<content:encoded><![CDATA[
<div> Southeast Asian languages, multilingual safety benchmarks, large language models, harm detection, cultural diversity<br /><br />Summary:<br /><br />1. Safeguard models are designed to help large language models (LLMs) detect and block harmful content effectively. However, current evaluations tend to be English-focused, often neglecting the linguistic and cultural diversity present in other languages.  
2. Many existing multilingual safety benchmarks use machine-translated English data, which does not accurately represent the nuances and specific challenges found in low-resource languages.  
3. Southeast Asian (SEA) languages are notably underrepresented in safety research, despite the region's rich linguistic diversity and unique safety concerns such as culturally sensitive political speech and region-specific misinformation.  
4. To address these shortcomings, SEA-SafeguardBench was introduced as the first human-verified safety benchmark specifically for SEA languages. It includes data from eight languages and comprises 21,640 samples, organized into three subsets: general content, in-the-wild scenarios, and content generation tasks.  
5. Experimental results indicate that state-of-the-art LLMs and their associated safety guardrails face significant challenges when dealing with SEA cultural contexts and harm scenarios, underperforming on these languages compared to English texts. This emphasizes the need for more culturally aware and linguistically diverse safety evaluation benchmarks. <div>
arXiv:2512.05501v1 Announce Type: new 
Abstract: Safeguard models help large language models (LLMs) detect and block harmful content, but most evaluations remain English-centric and overlook linguistic and cultural diversity. Existing multilingual safety benchmarks often rely on machine-translated English data, which fails to capture nuances in low-resource languages. Southeast Asian (SEA) languages are underrepresented despite the region's linguistic diversity and unique safety concerns, from culturally sensitive political speech to region-specific misinformation. Addressing these gaps requires benchmarks that are natively authored to reflect local norms and harm scenarios. We introduce SEA-SafeguardBench, the first human-verified safety benchmark for SEA, covering eight languages, 21,640 samples, across three subsets: general, in-the-wild, and content generation. The experimental results from our benchmark demonstrate that even state-of-the-art LLMs and guardrails are challenged by SEA cultural and harm scenarios and underperform when compared to English texts.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Identification of Incidentalomas Requiring Follow-Up: A Multi-Anatomy Evaluation of LLM-Based and Supervised Approaches</title>
<link>https://arxiv.org/abs/2512.05537</link>
<guid>https://arxiv.org/abs/2512.05537</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, incidentalomas, radiology reports, anatomy-aware prompting, lesion detection  

<br /><br />Summary:  
The study evaluates the performance of large language models (LLMs) against supervised transformer-based baselines for fine-grained, lesion-level detection of incidentalomas that require follow-up in radiology reports. Using a dataset of 400 annotated radiology reports containing 1,623 verified lesion findings, the authors compare three supervised models (BioClinicalModernBERT, ModernBERT, Clinical Longformer) with four generative LLM configurations (Llama 3.1-8B, GPT-4o, GPT-OSS-20b). A novel inference method incorporating lesion-tagged inputs and anatomy-aware prompting is introduced to enhance model reasoning capabilities. Results show that the anatomy-informed GPT-OSS-20b model delivers the best performance, achieving a macro-F1 score of 0.79 for incidentaloma-positive detection, outperforming all supervised baselines, whose top macro-F1 was 0.70, and closely matching inter-annotator agreement at 0.76. Incorporating anatomical context significantly improves the performance of GPT-based models (p < 0.05). Additionally, a majority-vote ensemble of the leading models boosts macro-F1 to 0.90. Error analysis indicates that anatomy-aware LLMs excel at contextual understanding, effectively differentiating actionable lesions from benign findings. The study concludes that with structured lesion tagging and anatomical grounding, generative LLMs provide a reliable and interpretable solution for automated incidental finding surveillance in radiology workflows, rivaling human expert accuracy. <div>
arXiv:2512.05537v1 Announce Type: new 
Abstract: Objective: To evaluate large language models (LLMs) against supervised baselines for fine-grained, lesion-level detection of incidentalomas requiring follow-up, addressing the limitations of current document-level classification systems.
  Methods: We utilized a dataset of 400 annotated radiology reports containing 1,623 verified lesion findings. We compared three supervised transformer-based encoders (BioClinicalModernBERT, ModernBERT, Clinical Longformer) against four generative LLM configurations (Llama 3.1-8B, GPT-4o, GPT-OSS-20b). We introduced a novel inference strategy using lesion-tagged inputs and anatomy-aware prompting to ground model reasoning. Performance was evaluated using class-specific F1-scores.
  Results: The anatomy-informed GPT-OSS-20b model achieved the highest performance, yielding an incidentaloma-positive macro-F1 of 0.79. This surpassed all supervised baselines (maximum macro-F1: 0.70) and closely matched the inter-annotator agreement of 0.76. Explicit anatomical grounding yielded statistically significant performance gains across GPT-based models (p < 0.05), while a majority-vote ensemble of the top systems further improved the macro-F1 to 0.90. Error analysis revealed that anatomy-aware LLMs demonstrated superior contextual reasoning in distinguishing actionable findings from benign lesions.
  Conclusion: Generative LLMs, when enhanced with structured lesion tagging and anatomical context, significantly outperform traditional supervised encoders and achieve performance comparable to human experts. This approach offers a reliable, interpretable pathway for automated incidental finding surveillance in radiology workflows.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structured Reasoning with Tree-of-Thoughts for Bengali Math Word Problems</title>
<link>https://arxiv.org/abs/2512.05580</link>
<guid>https://arxiv.org/abs/2512.05580</guid>
<content:encoded><![CDATA[
<div> Mathematical Word Problems, Tree-of-Thought, Chain-of-Thought, Bengali NLP, Large Language Models<br /><br />Summary:<br /><br />This paper investigates the effectiveness of Tree-of-Thought (ToT) reasoning in solving Mathematical Word Problems (MWPs) for the Bengali language, using the SOMADHAN dataset. MWPs are challenging because they require both deep linguistic comprehension and multi-step numerical reasoning. The authors compare three prompting strategies—standard prompting, Chain-of-Thought (CoT), and Tree-of-Thought (ToT)—across multiple large language models (LLMs), including GPT-OSS and LLaMA variants. Due to computational constraints, experiments are conducted on a curated set of 100 representative MWPs. Results show that CoT prompting boosts accuracy from a baseline of 78% under standard prompting to 83% on average. Further, ToT reasoning improves performance by up to 5 percentage points, reaching 88% accuracy with the largest GPT-OSS-120B model. The gains are most prominent in medium to large scale models, with smaller models benefiting less from ToT. The study demonstrates that ToT provides more reliable and globally consistent reasoning paths compared to linear CoT, especially in a low-resource language context like Bengali. Ultimately, the findings establish ToT as a powerful and robust framework for multilingual mathematical reasoning tasks, suggesting broader potential for structured reasoning techniques in natural language processing. <div>
arXiv:2512.05580v1 Announce Type: new 
Abstract: Mathematical Word Problems (MWPs) are among the most challenging tasks in natural language processing because they require both linguistic understanding and multi-step numerical reasoning. While Chain-of-Thought (CoT) prompting has shown promise, its linear structure often propagates errors, limiting overall effectiveness. To address this limitation, we present the a systematic study of Tree-of-Thought (ToT) reasoning for Bengali MWPs using the SOMADHAN dataset. Owing to computational and token-cost constraints, we evaluate a curated set of 100 representative problems across multiple large language models (LLMs), including GPT-OSS and LLaMA variants, under standard prompting, CoT, and ToT strategies. Our results show that CoT improves baseline accuracy from 78% (standard prompting) to 83% on average, while ToT further increases performance by up to 5 percentage points, achieving 88% accuracy with GPT-OSS-120B. These improvements highlight that ToT is particularly effective in medium-to-large-scale models but may offer less advantage for smaller ones. Overall, our findings establish ToT as a robust framework for solving mathematical problems in low-resource languages such as Bengali. More broadly, this study shows that structured reasoning methods like ToT can provide more reliable and globally consistent outcomes than CoT, paving the way for better reasoning strategies in multilingual NLP.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Greek Government Decisions Dataset for Public-Sector Analysis and Insight</title>
<link>https://arxiv.org/abs/2512.05647</link>
<guid>https://arxiv.org/abs/2512.05647</guid>
<content:encoded><![CDATA[
<div> Greek government decisions, transparency, retrieval-augmented generation, large-scale corpus, language models<br /><br />Summary:<br /><br />1. The paper introduces an open, machine-readable corpus containing 1 million Greek government decisions sourced from the national transparency platform, Diavgeia. <br /><br />2. The dataset features high-quality raw text extracted from PDFs, provided in Markdown format, along with a fully reproducible extraction pipeline.<br /><br />3. Beyond dataset release, the authors conduct qualitative analyses to identify boilerplate patterns and design a retrieval-augmented generation (RAG) task by formulating representative questions and creating high-quality answers.<br /><br />4. They evaluate a baseline RAG system’s ability to retrieve and reason over public decisions, demonstrating the potential for advanced information access and transparency via structured retrieval and reasoning on governmental documents.<br /><br />5. The corpus serves as valuable pre-training or fine-tuning material for language models, particularly specialized legal and governmental LMs, and lays groundwork for domain adaptation, knowledge-grounded generation, and explainable AI.<br /><br />6. Limitations are discussed and future directions outlined, with all data and code made publicly accessible to support further research and application. <div>
arXiv:2512.05647v1 Announce Type: new 
Abstract: We introduce an open, machine-readable corpus of Greek government decisions sourced from the national transparency platform Diavgeia. The resource comprises 1 million decisions, featuring and high-quality raw text extracted from PDFs. It is released with raw extracted text in Markdown format, alongside a fully reproducible extraction pipeline. Beyond the core dataset, we conduct qualitative analyses to explore boilerplate patterns and design a retrieval-augmented generation (RAG) task by formulating a set of representative questions, creating high-quality answers, and evaluating a baseline RAG system on its ability to retrieve and reason over public decisions. This evaluation demonstrates the potential of large-scale public-sector corpora to support advanced information access and transparency through structured retrieval and reasoning over governmental documents, and highlights how such a RAG pipeline could simulate a chat-based assistant capable of interactively answering questions about public decisions. Due to its scale, quality, and domain coverage, the corpus can also serve as high-value pre-training or fine-tuning material for new Language Models (LMs) and Large Language Models (LLMs) respectively, including specialized models for legal and governmental domains, and as a foundation for novel approaches in domain adaptation, knowledge-grounded generation, and explainable AI. Finally, we discuss limitations, outline future directions, and make both the data and the code accessible.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grounded Multilingual Medical Reasoning for Question Answering with Large Language Models</title>
<link>https://arxiv.org/abs/2512.05658</link>
<guid>https://arxiv.org/abs/2512.05658</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, medical QA, multilingual reasoning, retrieval-augmented generation, clinical decision-support<br /><br />Summary:<br /><br />This paper addresses the limitation of existing medical Question Answering (QA) systems which are predominantly English-focused and depend heavily on distillation from general-purpose Large Language Models (LLMs), thus raising concerns about the accuracy and reliability of medical knowledge. The authors propose a novel method for generating multilingual reasoning traces rooted in factual medical knowledge, specifically in English, Italian, and Spanish. They create a large dataset containing 500,000 reasoning traces using a retrieval-augmented generation approach leveraging medical information sourced from Wikipedia. This dataset expands medical QA benchmarks MedQA and MedMCQA into Italian and Spanish, facilitating multilingual evaluation. The pipeline is tested both in-domain and out-of-domain across multiple Medical QA benchmarks. Results demonstrate that incorporating these reasoning traces enhances performance through in-context few-shot learning and supervised fine-tuning, achieving state-of-the-art outcomes among LLMs with 8 billion parameters. The work contributes valuable resources by releasing the reasoning traces, translated QA datasets, a Medical-Wikipedia corpus, and fine-tuned models. Overall, the study promotes the advancement of safer, more interpretable clinical decision-support tools applicable in multilingual healthcare environments. <div>
arXiv:2512.05658v1 Announce Type: new 
Abstract: Large Language Models (LLMs) with reasoning capabilities have recently demonstrated strong potential in medical Question Answering (QA). Existing approaches are largely English-focused and primarily rely on distillation from general-purpose LLMs, raising concerns about the reliability of their medical knowledge. In this work, we present a method to generate multilingual reasoning traces grounded in factual medical knowledge. We produce 500k traces in English, Italian, and Spanish, using a retrievalaugmented generation approach over medical information from Wikipedia. The traces are generated to solve medical questions drawn from MedQA and MedMCQA, which we extend to Italian and Spanish. We test our pipeline in both in-domain and outof-domain settings across Medical QA benchmarks, and demonstrate that our reasoning traces improve performance both when utilized via in-context learning (few-shot) and supervised fine-tuning, yielding state-of-the-art results among 8B-parameter LLMs. We believe that these resources can support the development of safer, more transparent clinical decision-support tools in multilingual settings. We release the full suite of resources: reasoning traces, translated QA datasets, Medical-Wikipedia, and fine-tuned models.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interleaved Latent Visual Reasoning with Selective Perceptual Modeling</title>
<link>https://arxiv.org/abs/2512.05665</link>
<guid>https://arxiv.org/abs/2512.05665</guid>
<content:encoded><![CDATA[
<div> Interleaved Reasoning, Latent Visual Representations, Multimodal Large Language Models, Self-supervision, Visual Feedback

<br /><br />Summary:  
This paper addresses the computational challenges in Multimodal Large Language Models (MLLMs) that use interleaved reasoning paradigms with visual feedback by proposing a novel framework called Interleaved Latent Visual Reasoning (ILVR). Existing methods that rely on latent visual reasoning face a trade-off between precise perceptual modeling and the ability to handle dynamic problems due to either over-compression of features or non-interleaved static representations. ILVR overcomes this limitation by interleaving textual generation and latent visual representations, which serve as evolving cues to guide subsequent reasoning steps. A key innovation in ILVR is the use of a self-supervision strategy featuring a Momentum Teacher Model, which selectively distills relevant features from helper images to create sparse, adaptive supervision targets. This mechanism enables the model to autonomously generate context-aware visual signals that enhance reasoning performance. Experimental results on multimodal reasoning benchmarks demonstrate that ILVR significantly outperforms existing techniques by effectively balancing fine-grained perception with sequential multimodal reasoning, making it a promising approach for dynamic visual-language tasks. <div>
arXiv:2512.05665v1 Announce Type: new 
Abstract: Interleaved reasoning paradigms enhance Multimodal Large Language Models (MLLMs) with visual feedback but are hindered by the prohibitive computational cost of repeatedly re-encoding pixel-dense images. A promising alternative, latent visual reasoning, circumvents this bottleneck yet currently forces a critical trade-off: methods either sacrifice precise perceptual modeling by over-compressing features or fail to model dynamic problems due to static, non-interleaved structures. We introduce Interleaved Latent Visual Reasoning (ILVR), a framework that unifies dynamic state evolution with precise perceptual modeling. ILVR interleaves textual generation with latent visual representations that act as specific, evolving cues for subsequent reasoning. To enable this, we employ a self-supervision strategy where a Momentum Teacher Model selectively distills relevant features from helper images into sparse supervision targets. This adaptive selection mechanism guides the model to autonomously generate context-aware visual signals. Extensive experiments on multimodal reasoning benchmarks demonstrate that ILVR significantly outperforms existing approaches, effectively bridging the gap between fine-grained perception and sequential multimodal reasoning.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedTutor-R1: Socratic Personalized Medical Teaching with Multi-Agent Simulation</title>
<link>https://arxiv.org/abs/2512.05671</link>
<guid>https://arxiv.org/abs/2512.05671</guid>
<content:encoded><![CDATA[
<div> arXiv:2512.05671v1  
Keywords: clinical education, large language models, multi-agent simulation, Socratic teaching, reinforcement learning  

<br /><br />Summary:  
The paper addresses the challenge in medical education stemming from the growing demand for clinical training and the shortage of expert instructors. To bridge this gap, the authors propose leveraging Large Language Models (LLMs) for personalized guidance but highlight that previous work predominantly focuses on one-on-one knowledge transfer, neglecting collaborative reasoning, which is critical in clinical teamwork settings like ward rounds. To overcome this limitation, they develop ClinEdu, a multi-agent pedagogical simulator featuring personality-driven patients and diverse student groups, which facilitates controlled testing and scalable data generation for complex teaching processes. Using ClinEdu, the researchers construct ClinTeach, a comprehensive Socratic dialogue dataset capturing the intricacies of group instruction in clinical education. Building on this, they introduce MedTutor-R1, the first multimodal Socratic tutor tailored for one-to-many clinical instruction. MedTutor-R1 undergoes instruction tuning on the ClinTeach dataset and subsequent reinforcement learning optimization guided by a three-axis rubric evaluating structural fidelity, analytical quality, and clinical safety, enhancing its adaptive Socratic teaching strategies. For authentic evaluation, the tutor is tested in-situ by re-integrating it into ClinEdu for simulation-based interactive assessment. Experimental results show MedTutor-R1 improves pedagogical performance by over 20% compared to the base model, matches or exceeds comparable models, and demonstrates strong adaptability in managing varying student numbers, validating the efficacy of ClinEdu as a pedagogical simulation platform. <div>
arXiv:2512.05671v1 Announce Type: new 
Abstract: The significant gap between rising demands for clinical training and the scarcity of expert instruction poses a major challenge to medical education. With powerful capabilities in personalized guidance, Large Language Models (LLMs) offer a promising solution to bridge this gap. However, current research focuses mainly on one-on-one knowledge instruction, overlooking collaborative reasoning, a key skill for students developed in teamwork like ward rounds. To this end, we develop ClinEdu, a multi-agent pedagogical simulator with personality-driven patients and diverse student cohorts, enabling controlled testing of complex pedagogical processes and scalable generation of teaching data. Based on ClinEdu, we construct ClinTeach, a large Socratic teaching dialogue dataset that captures the complexities of group instruction. We then train MedTutor-R1, the first multimodal Socratic tutor designed for one-to-many instruction in clinical medical education. MedTutor-R1 is first instruction-tuned on our ClinTeach dataset and then optimized with reinforcement learning, using rewards derived from a three-axis rubric, covering structural fidelity, analytical quality, and clinical safety, to refine its adaptive Socratic strategies. For authentic in-situ assessment, we use simulation-based interactive evaluation that redeploys the tutor back into ClinEdu. Experimental results demonstrate that our MedTutor-R1 outperforms the base model by over 20% in average pedagogical score and is comparable to o3, while also exhibiting high adaptability in handling a varying number of students. This promising performance underscores the effectiveness of our pedagogical simulator, ClinEdu.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retrieving Semantically Similar Decisions under Noisy Institutional Labels: Robust Comparison of Embedding Methods</title>
<link>https://arxiv.org/abs/2512.05681</link>
<guid>https://arxiv.org/abs/2512.05681</guid>
<content:encoded><![CDATA[
<div> Keywords: case law retrieval, OpenAI embeddings, domain-specific BERT, noisy evaluation, Czech Constitutional Court

<br /><br />Summary:  
The paper addresses the challenge of retrieving case law, a task traditionally done via database queries that can be time-consuming. It compares two models for the Czech Constitutional Court decisions: a large general-purpose OpenAI embedder and a domain-specific BERT model trained from scratch on about 30,000 court decisions using sliding windows and attention pooling techniques. The evaluation framework is noise-aware and involves IDF-weighted keyword overlap to measure graded relevance, uses two binarization thresholds (0.20 for balanced, 0.28 for strict), paired bootstrap tests for statistical significance, and an nDCG-based diagnostic analysis coupled with qualitative assessment. Results demonstrate that despite modest absolute nDCG scores—which are expected due to noisy labels—the general OpenAI embedder consistently and significantly outperforms the domain-specific BERT at ranking cutoffs @10, @20, and @100 under both thresholds. Diagnostic insights attribute the relatively low absolute performance mainly to label drift and idealistic targets rather than the models’ lack of capability or utility. Finally, the authors highlight that their evaluation framework is sufficiently robust to handle noisy gold datasets, a common issue when working with heterogeneous legacy judicial data labels, making it useful for future legal document retrieval benchmarking. <div>
arXiv:2512.05681v1 Announce Type: new 
Abstract: Retrieving case law is a time-consuming task predominantly carried out by querying databases. We provide a comparison of two models in three different settings for Czech Constitutional Court decisions: (i) a large general-purpose embedder (OpenAI), (ii) a domain-specific BERT-trained from scratch on ~30,000 decisions using sliding windows and attention pooling. We propose a noise-aware evaluation including IDF-weighted keyword overlap as graded relevance, binarization via two thresholds (0.20 balanced, 0.28 strict), significance via paired bootstrap, and an nDCG diagnosis supported with qualitative analysis. Despite modest absolute nDCG (expected under noisy labels), the general OpenAI embedder decisively outperforms the domain pre-trained BERT in both settings at @10/@20/@100 across both thresholds; differences are statistically significant. Diagnostics attribute low absolutes to label drift and strong ideals rather than lack of utility. Additionally, our framework is robust enough to be used for evaluation under a noisy gold dataset, which is typical when handling data with heterogeneous labels stemming from legacy judicial databases.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Faithfulness metric fusion: Improving the evaluation of LLM trustworthiness across domains</title>
<link>https://arxiv.org/abs/2512.05700</link>
<guid>https://arxiv.org/abs/2512.05700</guid>
<content:encoded><![CDATA[
<div> faithfulness evaluation, large language models, metric fusion, human judgements, dataset homogenisation<br /><br />Summary:<br /><br />1. The paper introduces a new methodology to enhance the accuracy of faithfulness evaluation in Large Language Models (LLMs).<br />2. The approach improves faithfulness assessment by combining several elementary faithfulness metrics into a single fused metric.<br />3. The fusion strategy uses a tree-based model to determine the importance of each individual metric, guided by human judgements on the faithfulness of LLM-generated responses.<br />4. This fused metric shows a stronger correlation with human evaluations across multiple domains, including question answering and dialogue.<br />5. The authors also standardize a collection of datasets spanning question answering and dialogue domains, incorporating both human judgements and LLM outputs, to facilitate reproducible faithfulness evaluation and benchmarking across different contexts.<br /><br />Overall, the paper contributes a more reliable way to measure faithfulness in LLM outputs, promoting greater trust and enabling broader application of these models in diverse real-world scenarios. <div>
arXiv:2512.05700v1 Announce Type: new 
Abstract: We present a methodology for improving the accuracy of faithfulness evaluation in Large Language Models (LLMs). The proposed methodology is based on the combination of elementary faithfulness metrics into a combined (fused) metric, for the purpose of improving the faithfulness of LLM outputs. The proposed strategy for metric fusion deploys a tree-based model to identify the importance of each metric, which is driven by the integration of human judgements evaluating the faithfulness of LLM responses. This fused metric is demonstrated to correlate more strongly with human judgements across all tested domains for faithfulness. Improving the ability to evaluate the faithfulness of LLMs, allows for greater confidence to be placed within models, allowing for their implementation in a greater diversity of scenarios. Additionally, we homogenise a collection of datasets across question answering and dialogue-based domains and implement human judgements and LLM responses within this dataset, allowing for the reproduction and trialling of faithfulness evaluation across domains.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Text Classification with Conformal In-Context Learning</title>
<link>https://arxiv.org/abs/2512.05732</link>
<guid>https://arxiv.org/abs/2512.05732</guid>
<content:encoded><![CDATA[
<div> arXiv:2512.05732v1  
Keywords: Large Language Models, Conformal In-Context Learning, text classification, efficiency, class imbalance  

<br /><br />Summary:  
This paper evaluates Conformal In-Context Learning (CICLe), a framework that combines lightweight base classifiers with Conformal Prediction to optimize the prompting of Large Language Models (LLMs) for text classification. CICLe adaptively reduces the candidate class set, thereby improving classification efficiency. The study systematically tests CICLe across multiple NLP classification benchmarks, demonstrating consistent improvement over base classifiers and outperforming few-shot prompting baselines given sufficient training data. In scenarios with limited data, CICLe performs comparably with existing methods. Efficiency gains are notable, with reductions in the number of shots by up to 34.45% and prompt length by up to 25.16%, allowing the use of smaller LLMs without a loss in performance. The framework is especially effective on classification tasks exhibiting high class imbalance, where it maintains robust results. Overall, CICLe integrates the stability of traditional classifiers with the flexibility of LLMs, presenting a scalable and practical approach that improves both data and computational efficiency in text classification tasks. <div>
arXiv:2512.05732v1 Announce Type: new 
Abstract: Large Language Models (LLMs) demonstrate strong in-context learning abilities, yet their effectiveness in text classification depends heavily on prompt design and incurs substantial computational cost. Conformal In-Context Learning (CICLe) has been proposed as a resource-efficient framework that integrates a lightweight base classifier with Conformal Prediction to guide LLM prompting by adaptively reducing the set of candidate classes. However, its broader applicability and efficiency benefits beyond a single domain have not yet been systematically explored. In this paper, we present a comprehensive evaluation of CICLe across diverse NLP classification benchmarks. The results show that CICLe consistently improves over its base classifier and outperforms few-shot prompting baselines when the sample size is sufficient for training the base classifier, and performs comparably in low-data regimes. In terms of efficiency, CICLe reduces the number of shots and prompt length by up to 34.45% and 25.16%, respectively, and enables the use of smaller models with competitive performance. CICLe is furthermore particularly advantageous for text classification tasks with high class imbalance. These findings highlight CICLe as a practical and scalable approach for efficient text classification, combining the robustness of traditional classifiers with the adaptability of LLMs, and achieving substantial gains in data and computational efficiency.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Capturing Classic Authorial Style in Long-Form Story Generation with GRPO Fine-Tuning</title>
<link>https://arxiv.org/abs/2512.05747</link>
<guid>https://arxiv.org/abs/2512.05747</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, style-conditioned story generation, authorship verification, Group Relative Policy Optimization, narrative coherence<br /><br />Summary: This paper addresses the challenge of fine-grained stylistic control in open-ended story generation by introducing a novel training framework called Group Relative Policy Optimization (GRPO). Unlike prior approaches that rely on superficial cues to mimic authorial style, this work leverages a style reward derived from a fine-tuned sentence transformer using authorship verification signals. To ensure balanced story quality, the system also incorporates content and completeness rewards, stabilizing the generation of long-form narratives. The methodology is evaluated on fiction by Mark Twain, specifically using The Adventures of Huckleberry Finn as the style reference. Experiments with an 8-billion parameter model demonstrate that it surpasses larger baselines such as GPT-4o and Claude Sonnet 4 in style alignment metrics, achieving a style score of 0.628 while maintaining competitive content quality. Despite the success in stylistic imitation, the study acknowledges ongoing challenges related to narrative completeness and global coherence, suggesting further research is needed to improve overall story resolution and cohesion. The results indicate that moderate-sized, task-specific models can effectively perform agentic stylistic story generation, paving the way for more nuanced and controlled creative AI writing. <div>
arXiv:2512.05747v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) show impressive performance in open-ended story generation, but fine-grained stylistic control remains limited. Existing methods often rely on shallow cues (e.g., names or topics) to simulate authorial style, without robust evaluation. In this work, we present a training framework for style-conditioned story generation using Group Relative Policy Optimization (GRPO) and a custom multi-reward setup. The style reward is derived from a fine-tuned sentence transformer using authorship verification (AV) signals, combined with content and completeness scores to stabilize long-form narrative generation. We conduct experiments using fiction by Mark Twain, a prominent 19th-century American author, with The Adventures of Huckleberry Finn serving as the reference style exemplar. Our 8B model outperforms larger baselines such as GPT-4o and Claude Sonnet 4 in AV-style metrics, achieving a style score of 0.628 and competitive content quality. Results demonstrate the feasibility of agentic stylistic generation with moderate model size and task-specific training. While the output is clearly style-aligned, narrative completeness remains a challenge, indicating future work is needed to better model global coherence and story resolution.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Heard or Halted? Gender, Interruptions, and Emotional Tone in U.S. Supreme Court Oral Arguments</title>
<link>https://arxiv.org/abs/2512.05832</link>
<guid>https://arxiv.org/abs/2512.05832</guid>
<content:encoded><![CDATA[
<div> Keywords: interruptions, Supreme Court, gender dynamics, semantic analysis, sentiment analysis  

<br /><br />Summary:  
This study investigates the impact of interruptions during U.S. Supreme Court oral arguments on both the semantic content and emotional tone of advocates' speech, with a particular emphasis on gendered communication dynamics. Using the ConvoKit Supreme Court Corpus covering data from 2010 to 2019, the authors analyze 12,663 speech segments from interactions between advocates and justices. The research assesses whether interruptions lead to changes in the meaning of the advocates' arguments and if interruptions directed at female advocates carry distinct emotional qualities. Semantic shifts caused by interruptions are measured using GloVe-based sentence embeddings, which quantify the similarity between speech before and after interruptions. Sentiment is evaluated using lexicon-based sentiment analysis to detect positive or negative emotional valence. Findings reveal that despite interruptions, semantic similarity remains high, indicating that the core argumentative content is mostly preserved. However, interruptions targeted at female advocates consistently exhibit significantly higher levels of negative sentiment compared to those directed at male advocates. The results contribute to a deeper empirical understanding of how gender intersects with communication power dynamics in elite judicial settings and highlight the utility of computational linguistics methods for exploring issues of power, discourse, and equity in legal proceedings. <div>
arXiv:2512.05832v1 Announce Type: new 
Abstract: This study examines how interruptions during U.S. Supreme Court oral arguments shape both the semantic content and emotional tone of advocates' speech, with a focus on gendered dynamics in judicial discourse. Using the ConvoKit Supreme Court Corpus (2010-2019), we analyze 12,663 speech chunks from advocate-justice interactions to assess whether interruptions alter the meaning of an advocate's argument and whether interruptions toward female advocates exhibit more negative emotional valence. Semantic shifts are quantified using GloVe-based sentence embeddings, while sentiment is measured through lexicon-based analysis. We find that semantic similarity between pre- and post-interruption speech remains consistently high, suggesting that interruptions do not substantially alter argumentative content. However, interruptions directed at female advocates contain significantly higher levels of negative sentiment. These results deepen empirical understanding of gendered communication in elite institutional settings and demonstrate the value of computational linguistic methods for studying power, discourse, and equity in judicial proceedings.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompting Science Report 4: Playing Pretend: Expert Personas Don't Improve Factual Accuracy</title>
<link>https://arxiv.org/abs/2512.05858</link>
<guid>https://arxiv.org/abs/2512.05858</guid>
<content:encoded><![CDATA[
<div> Personas, AI models, performance, multiple-choice, expert prompts<br /><br />Summary:<br /><br />This study examines whether assigning personas to AI models improves their performance on challenging multiple-choice questions from graduate-level science, engineering, and law exams. Three approaches were tested: in-domain expert personas (e.g., telling the model it is a physics expert when answering physics questions), off-domain expert personas (assigning an expert persona unrelated to the domain of the question), and low-knowledge personas (such as layperson, young child, or toddler). The results show that in-domain expert personas generally had no significant impact on accuracy across six different models, except for a slight improvement seen in the Gemini 2.0 Flash model. Off-domain expert personas sometimes led to marginal differences or even degraded performance. Low-knowledge personas consistently harmed model accuracy. Overall, persona prompts did not improve answer accuracy compared to a baseline without personas. While personas may influence aspects like output tone or style, this investigation found no consistent factual accuracy benefit from adopting persona prompts when answering difficult multiple-choice questions in specialized domains. <div>
arXiv:2512.05858v1 Announce Type: new 
Abstract: This is the fourth in a series of short reports that help business, education, and policy leaders understand the technical details of working with AI through rigorous testing. Here, we ask whether assigning personas to models improves performance on difficult objective multiple-choice questions. We study both domain-specific expert personas and low-knowledge personas, evaluating six models on GPQA Diamond (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024), graduate-level questions spanning science, engineering, and law.
  We tested three approaches:
  -In-Domain Experts: Assigning the model an expert persona ("you are a physics expert") matched to the problem type (physics problems) had no significant impact on performance (with the exception of the Gemini 2.0 Flash model).
  -Off-Domain Experts (Domain-Mismatched): Assigning the model an expert persona ("you are a physics expert") not matched to the problem type (law problems) resulted in marginal differences.
  -Low-Knowledge Personas: We assigned the model negative capability personas (layperson, young child, toddler), which were generally harmful to benchmark accuracy.
  Across both benchmarks, persona prompts generally did not improve accuracy relative to a no-persona baseline. Expert personas showed no consistent benefit across models, with few exceptions. Domain-mismatched expert personas sometimes degraded performance. Low-knowledge personas often reduced accuracy. These results are about the accuracy of answers only; personas may serve other purposes (such as altering the tone of outputs), beyond improving factual performance.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Medical Question-Answering Systems: A Comparative Study of Fine-Tuned and Zero-Shot Large Language Models with RAG Framework</title>
<link>https://arxiv.org/abs/2512.05863</link>
<guid>https://arxiv.org/abs/2512.05863</guid>
<content:encoded><![CDATA[
<div> Medical QA, Retrieval-Augmented Generation, Large Language Models, LoRA fine-tuning, Biomedical accuracy  

<br /><br />Summary:  
This paper addresses the challenge of applying large language models (LLMs) to medical question-answering (QA) tasks, focusing on improving factual accuracy and minimizing hallucinations. The authors propose a retrieval-augmented generation (RAG) system that integrates domain-specific medical literature retrieval with open-source LLMs—specifically LLaMA 2 and Falcon. They employ Low-Rank Adaptation (LoRA) to fine-tune these models efficiently for the medical domain, enhancing their specialization without heavy resource costs. By grounding answers in retrieved medical references, the system reduces unsupported content by approximately 60%, substantially improving the reliability of the outputs. The approach was evaluated on benchmark datasets PubMedQA and MedMCQA, demonstrating measurable accuracy improvements: for example, a fine-tuned LLaMA 2 model achieved 71.8% accuracy on PubMedQA compared to a 55.4% zero-shot baseline. Additionally, the system maintains transparency by providing source citations for its answers, supporting trust and verification in clinical contexts. The methodology and system design are detailed, highlighting the practical potential of retrieval-augmented, fine-tuned open-source LLMs for biomedical QA and clinical informatics applications. This study underscores the value of combining retrieval with LLMs to meet the stringent factual demands of medical question-answering. <div>
arXiv:2512.05863v1 Announce Type: new 
Abstract: Medical question-answering (QA) systems can benefit from advances in large language models (LLMs), but directly applying LLMs to the clinical domain poses challenges such as maintaining factual accuracy and avoiding hallucinations. In this paper, we present a retrieval-augmented generation (RAG) based medical QA system that combines domain-specific knowledge retrieval with open-source LLMs to answer medical questions. We fine-tune two state-of-the-art open LLMs (LLaMA~2 and Falcon) using Low-Rank Adaptation (LoRA) for efficient domain specialization. The system retrieves relevant medical literature to ground the LLM's answers, thereby improving factual correctness and reducing hallucinations. We evaluate the approach on benchmark datasets (PubMedQA and MedMCQA) and show that retrieval augmentation yields measurable improvements in answer accuracy compared to using LLMs alone. Our fine-tuned LLaMA~2 model achieves 71.8% accuracy on PubMedQA, substantially improving over the 55.4% zero-shot baseline, while maintaining transparency by providing source references. We also detail the system design and fine-tuning methodology, demonstrating that grounding answers in retrieved evidence reduces unsupported content by approximately 60%. These results highlight the potential of RAG-augmented open-source LLMs for reliable biomedical QA, pointing toward practical clinical informatics applications.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG</title>
<link>https://arxiv.org/abs/2512.05959</link>
<guid>https://arxiv.org/abs/2512.05959</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, Retrieval-Augmented Generation, Multilingual Multimodal VQA, Cultural Diversity, Model Scaling<br /><br />Summary:  
1. The paper addresses limitations of Vision-Language Models (VLMs) in Visual Question Answering (VQA) tasks, which mainly stem from reliance on static training data.  
2. Retrieval-Augmented Generation (RAG) is proposed as a solution to provide access to real-time, culturally rich, and multilingual information, enhancing VQA capabilities.  
3. The authors introduce M4-RAG, a large-scale benchmark consisting of over 80,000 culturally diverse image-question pairs spanning 42 languages and 56 regional dialects and registers, designed to evaluate retrieval-augmented VQA systems.  
4. A controlled retrieval environment is constructed containing millions of curated multilingual documents relevant to the query domains, balancing realism in retrieval with reproducibility in experiments.  
5. Experimental results demonstrate that while RAG improves performance for smaller VLMs, it does not scale well to larger models and can even degrade their accuracy, highlighting an important mismatch between increasing model size and the current retrieval mechanisms.  
6. M4-RAG sets a foundation for developing next-generation RAG systems capable of reasoning effectively across multiple languages, modalities, and cultural contexts, pushing forward the field of multilingual multimodal VQA research. <div>
arXiv:2512.05959v1 Announce Type: new 
Abstract: Vision-language models (VLMs) have achieved strong performance in visual question answering (VQA), yet they remain constrained by static training data. Retrieval-Augmented Generation (RAG) mitigates this limitation by enabling access to up-to-date, culturally grounded, and multilingual information; however, multilingual multimodal RAG remains largely underexplored. We introduce M4-RAG, a massive-scale benchmark covering 42 languages and 56 regional dialects and registers, comprising over 80,000 culturally diverse image-question pairs for evaluating retrieval-augmented VQA across languages and modalities. To balance realism with reproducibility, we build a controlled retrieval environment containing millions of carefully curated multilingual documents relevant to the query domains, approximating real-world retrieval conditions while ensuring consistent experimentation. Our systematic evaluation reveals that although RAG consistently benefits smaller VLMs, it fails to scale to larger models and often even degrades their performance, exposing a critical mismatch between model size and current retrieval effectiveness. M4-RAG provides a foundation for advancing next-generation RAG systems capable of reasoning seamlessly across languages, modalities, and cultural contexts.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAG-IGBench: Innovative Evaluation for RAG-based Interleaved Generation in Open-domain Question Answering</title>
<link>https://arxiv.org/abs/2512.05119</link>
<guid>https://arxiv.org/abs/2512.05119</guid>
<content:encoded><![CDATA[
<div> Keywords: interleaved image-text generation, RAG-IGBench, multimodal large language models, retrieval-augmented generation, evaluation metrics<br /><br />Summary:  
1. The paper addresses the challenge of generating high-quality interleaved image-text content in open-domain question answering, emphasizing the benefit of visually enhanced responses for better understanding and memory.  
2. It introduces RAG-IGBench, a comprehensive benchmark specifically created to evaluate interleaved generation tasks based on Retrieval-Augmented Generation (RAG) combined with multimodal large language models (MLLMs).  
3. RAG-IGBench uniquely utilizes up-to-date publicly available content from social media platforms and incorporates novel evaluation metrics designed to assess both the quality of generated text and images, as well as their consistency.  
4. The benchmark enables extensive experiments on state-of-the-art MLLMs, including both open-source and proprietary models, providing a detailed analysis of their strengths and weaknesses in producing coherent multimodal outputs.  
5. Validation of the proposed evaluation metrics demonstrates strong correlation with human judgments, and models fine-tuned on RAG-IGBench show improved performance across various benchmarks, confirming the dataset’s quality and practical value.  
6. The dataset and tools to facilitate future research are publicly available at https://github.com/USTC-StarTeam/RAG-IGBench, promoting advancements in multimodal generation. <div>
arXiv:2512.05119v1 Announce Type: cross 
Abstract: In real-world scenarios, providing user queries with visually enhanced responses can considerably benefit understanding and memory, underscoring the great value of interleaved image-text generation. Despite recent progress, like the visual autoregressive model that unifies text and image processing in a single transformer architecture, generating high-quality interleaved content remains challenging. Moreover, evaluations of these interleaved sequences largely remain underexplored, with existing benchmarks often limited by unimodal metrics that inadequately assess the intricacies of combined image-text outputs. To address these issues, we present RAG-IGBench, a thorough benchmark designed specifically to evaluate the task of Interleaved Generation based on Retrieval-Augmented Generation (RAG-IG) in open-domain question answering. RAG-IG integrates multimodal large language models (MLLMs) with retrieval mechanisms, enabling the models to access external image-text information for generating coherent multimodal content. Distinct from previous datasets, RAG-IGBench draws on the latest publicly available content from social platforms and introduces innovative evaluation metrics that measure the quality of text and images, as well as their consistency. Through extensive experiments with state-of-the-art MLLMs (both open-source and proprietary) on RAG-IGBench, we provide an in-depth analysis examining the capabilities and limitations of these models. Additionally, we validate our evaluation metrics by demonstrating their high correlation with human assessments. Models fine-tuned on RAG-IGBench's training set exhibit improved performance across multiple benchmarks, confirming both the quality and practical utility of our dataset. Our benchmark is available at https://github.com/USTC-StarTeam/RAG-IGBench.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SyncVoice: Towards Video Dubbing with Vision-Augmented Pretrained TTS Model</title>
<link>https://arxiv.org/abs/2512.05126</link>
<guid>https://arxiv.org/abs/2512.05126</guid>
<content:encoded><![CDATA[
<div> Keywords: video dubbing, speech naturalness, audio-visual synchronization, cross-lingual speech synthesis, SyncVoice<br /><br />Summary:<br /><br />Video dubbing involves generating high-quality speech that is closely aligned in time with the visual content. Existing approaches face challenges in maintaining natural speech quality and precise synchronization between audio and visual streams while being typically restricted to monolingual contexts. The paper introduces SyncVoice, a novel framework for video dubbing that leverages vision-augmented techniques built on a pretrained text-to-speech (TTS) model. By fine-tuning this TTS model using audio-visual data, SyncVoice achieves strong synchronization between speech and visuals, significantly improving audiovisual consistency. A key innovation is the Dual Speaker Encoder, designed to reduce inter-language interference in cross-lingual speech synthesis, enabling more effective dubbing across different languages. The framework is also explored within video translation scenarios, demonstrating its practical applicability beyond monolingual settings. Experimental evaluation shows that SyncVoice produces speech with high fidelity and superior synchronization performance compared to existing methods. Overall, SyncVoice addresses critical limitations in current video dubbing methods by enhancing speech naturalness, audio-visual alignment, and cross-lingual synthesis capabilities, highlighting its potential for advancing video dubbing technology. <div>
arXiv:2512.05126v1 Announce Type: cross 
Abstract: Video dubbing aims to generate high-fidelity speech that is precisely temporally aligned with the visual content. Existing methods still suffer from limitations in speech naturalness and audio-visual synchronization, and are limited to monolingual settings. To address these challenges, we propose SyncVoice, a vision-augmented video dubbing framework built upon a pretrained text-to-speech (TTS) model. By fine-tuning the TTS model on audio-visual data, we achieve strong audiovisual consistency. We propose a Dual Speaker Encoder to effectively mitigate inter-language interference in cross-lingual speech synthesis and explore the application of video dubbing in video translation scenarios. Experimental results show that SyncVoice achieves high-fidelity speech generation with strong synchronization performance, demonstrating its potential in video dubbing tasks.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Faithfulness and Entropy Production Measures to Tame Your LLM Demons and Manage Hallucinations</title>
<link>https://arxiv.org/abs/2512.05156</link>
<guid>https://arxiv.org/abs/2512.05156</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Faithfulness Evaluation, Kullback-Leibler Divergence, Semantic Faithfulness, Entropy Production<br /><br />Summary:<br /><br />1. The paper addresses the challenge of evaluating faithfulness of Large Language Models (LLMs) in performing specific tasks, proposing new unsupervised metrics rooted in information theory and thermodynamics.<br />2. The authors conceptualize an LLM as a bipartite information engine where internal hidden layers function analogously to a Maxwell demon, regulating the transformation of context (C) into an answer (A) guided by a prompt (Q).<br />3. They represent Question-Context-Answer (QCA) triplets as probability distributions over common topics, and model the topic transformations from context to prompt and answer as transition matrices Q and A, respectively.<br />4. The Semantic Faithfulness (SF) metric is introduced, measuring faithfulness via the Kullback-Leibler (KL) divergence between the inferred transition matrices, optimized through convex optimization and normalized to a [0,1] scale, where higher values indicate greater faithfulness.<br />5. Additionally, a thermodynamics-based Semantic Entropy Production (SEP) metric is proposed, linking lower entropy production to higher faithfulness, with both SF and SEP metrics applicable independently or jointly for faithfully evaluating LLM outputs and controlling hallucinations.<br />6. The effectiveness of this framework is demonstrated through experiments on LLM summarization of corporate SEC 10-K filings, highlighting its practical utility. <div>
arXiv:2512.05156v1 Announce Type: cross 
Abstract: Evaluating faithfulness of Large Language Models (LLMs) to a given task is a complex challenge. We propose two new unsupervised metrics for faithfulness evaluation using insights from information theory and thermodynamics. Our approach treats an LLM as a bipartite information engine where hidden layers act as a Maxwell demon controlling transformations of context $C $ into answer $A$ via prompt $Q$. We model Question-Context-Answer (QCA) triplets as probability distributions over shared topics. Topic transformations from $C$ to $Q$ and $A$ are modeled as transition matrices ${\bf Q}$ and ${\bf A}$ encoding the query goal and actual result, respectively. Our semantic faithfulness (SF) metric quantifies faithfulness for any given QCA triplet by the Kullback-Leibler (KL) divergence between these matrices. Both matrices are inferred simultaneously via convex optimization of this KL divergence, and the final SF metric is obtained by mapping the minimal divergence onto the unit interval [0,1], where higher scores indicate greater faithfulness. Furthermore, we propose a thermodynamics-based semantic entropy production (SEP) metric in answer generation, and show that high faithfulness generally implies low entropy production. The SF and SEP metrics can be used jointly or separately for LLM evaluation and hallucination control. We demonstrate our framework on LLM summarization of corporate SEC 10-K filings.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Computability of Artificial General Intelligence</title>
<link>https://arxiv.org/abs/2512.05212</link>
<guid>https://arxiv.org/abs/2512.05212</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial General Intelligence, creativity, algorithmic limits, functional capabilities, computation theory  

<br /><br />Summary:  
1. The paper addresses the question of how close humanity is to achieving Artificial General Intelligence (A.G.I.), defined as the ability of a model to be creative and innovate in a field, unlocking new functional capabilities.  
2. It adopts an existing definition of A.G.I. emphasizing creativity and innovation, reflecting the views of leading A.I. developers.  
3. The authors formally prove that no algorithm can produce new functional capabilities that were not inherent in the original algorithm, establishing a fundamental upper bound on machine computation.  
4. Consequently, the paper argues that true creativity or innovation, in the sense of generating genuinely new capabilities, is impossible for any algorithm or A.I. model. However, A.I. can recombine or permute existing capabilities effectively.  
5. The work concludes by discussing the broader implications for the future of A.I., suggesting inherent limits to machine creativity, and raises questions about the nature and origins of human intelligence, which seemingly transcends these algorithmic constraints. <div>
arXiv:2512.05212v1 Announce Type: cross 
Abstract: In recent years we observed rapid and significant advancements in artificial intelligence (A.I.). So much so that many wonder how close humanity is to developing an A.I. model that can achieve human level of intelligence, also known as artificial general intelligence (A.G.I.). In this work we look at this question and we attempt to define the upper bounds, not just of A.I., but rather of any machine-computable process (a.k.a. an algorithm). To answer this question however, one must first precisely define A.G.I. We borrow prior work's definition of A.G.I. [1] that best describes the sentiment of the term, as used by the leading developers of A.I. That is, the ability to be creative and innovate in some field of study in a way that unlocks new and previously unknown functional capabilities in that field. Based on this definition we draw new bounds on the limits of computation. We formally prove that no algorithm can demonstrate new functional capabilities that were not already present in the initial algorithm itself. Therefore, no algorithm (and thus no A.I. model) can be truly creative in any field of study, whether that is science, engineering, art, sports, etc. In contrast, A.I. models can demonstrate existing functional capabilities, as well as combinations and permutations of existing functional capabilities. We conclude this work by discussing the implications of this proof both as it regards to the future of A.I. development, as well as to what it means for the origins of human intelligence.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Effect of Document Summarization on LLM-Based Relevance Judgments</title>
<link>https://arxiv.org/abs/2512.05334</link>
<guid>https://arxiv.org/abs/2512.05334</guid>
<content:encoded><![CDATA[
<div> Keywords: Information Retrieval, Large Language Models, Text Summarization, Relevance Judgments, Evaluation Stability  

<br /><br />Summary:  
This paper examines the impact of text summarization on the reliability of Large Language Model (LLM)-based relevance judgments within Information Retrieval (IR) evaluation. First, it recognizes that traditional relevance judgments by human annotators are costly and that LLMs offer promising automated assessment capabilities. Second, it challenges the conventional approach where LLM assessors evaluate full document content directly, proposing the exploration of using LLM-generated summaries of varying lengths instead. Third, through experiments on multiple TREC datasets with state-of-the-art LLMs, the study compares the agreement between summary-based and full-document judgments against human labels. Fourth, it assesses how these different judgment sources affect IR system evaluation outcomes, particularly examining retrieval effectiveness and ranking stability. Finally, findings indicate that summary-based judgments maintain comparable ranking stability to full-document judgments but introduce systematic shifts in label distributions and biases that depend on the model and dataset used. The paper concludes that while summarization offers a practical means for more efficient large-scale IR evaluation, it also presents methodological considerations essential for the reliability of automatic relevance assessments. <div>
arXiv:2512.05334v1 Announce Type: cross 
Abstract: Relevance judgments are central to the evaluation of Information Retrieval (IR) systems, but obtaining them from human annotators is costly and time-consuming. Large Language Models (LLMs) have recently been proposed as automated assessors, showing promising alignment with human annotations. Most prior studies have treated documents as fixed units, feeding their full content directly to LLM assessors. We investigate how text summarization affects the reliability of LLM-based judgments and their downstream impact on IR evaluation. Using state-of-the-art LLMs across multiple TREC collections, we compare judgments made from full documents with those based on LLM-generated summaries of different lengths. We examine their agreement with human labels, their effect on retrieval effectiveness evaluation, and their influence on IR systems' ranking stability. Our findings show that summary-based judgments achieve comparable stability in systems' ranking to full-document judgments, while introducing systematic shifts in label distributions and biases that vary by model and dataset. These results highlight summarization as both an opportunity for more efficient large-scale IR evaluation and a methodological choice with important implications for the reliability of automatic judgments.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text Rationalization for Robust Causal Effect Estimation</title>
<link>https://arxiv.org/abs/2512.05373</link>
<guid>https://arxiv.org/abs/2512.05373</guid>
<content:encoded><![CDATA[
<div> Keywords: causal inference, text data, positivity assumption, token selection, Confounding-Aware Token Rationalization (CATR)  

<br /><br />Summary:  
This paper addresses challenges in using high-dimensional text data for causal inference, specifically in adjusting for confounding factors in treatment effect estimation. The authors identify that the positivity assumption—which requires sufficient overlap in treatment assignments across confounder values—is often violated due to the large and redundant textual feature spaces, resulting in extreme propensity scores and unstable estimates. To overcome this, they propose Confounding-Aware Token Rationalization (CATR), a novel framework that selects a sparse and necessary subset of tokens. CATR employs a residual-independence diagnostic to ensure that the selected tokens preserve essential confounding information needed for unbiased causal effect estimation. By filtering out irrelevant textual data and retaining key signals, CATR effectively mitigates observational-level positivity violations and stabilizes downstream causal effect estimators. The framework is empirically validated through experiments on synthetic datasets and a real-world medical dataset from the MIMIC-III database. Results show that CATR produces more accurate, stable, and interpretable causal effect estimates compared to existing baseline methods, demonstrating its practical value in leveraging text data for robust causal inference. <div>
arXiv:2512.05373v1 Announce Type: cross 
Abstract: Recent advances in natural language processing have enabled the increasing use of text data in causal inference, particularly for adjusting confounding factors in treatment effect estimation. Although high-dimensional text can encode rich contextual information, it also poses unique challenges for causal identification and estimation. In particular, the positivity assumption, which requires sufficient treatment overlap across confounder values, is often violated at the observational level, when massive text is represented in feature spaces. Redundant or spurious textual features inflate dimensionality, producing extreme propensity scores, unstable weights, and inflated variance in effect estimates. We address these challenges with Confounding-Aware Token Rationalization (CATR), a framework that selects a sparse necessary subset of tokens using a residual-independence diagnostic designed to preserve confounding information sufficient for unconfoundedness. By discarding irrelevant texts while retaining key signals, CATR mitigates observational-level positivity violations and stabilizes downstream causal effect estimators. Experiments on synthetic data and a real-world study using the MIMIC-III database demonstrate that CATR yields more accurate, stable, and interpretable causal effect estimates than existing baselines.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.05591</link>
<guid>https://arxiv.org/abs/2512.05591</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Entropy Ratio, Policy Stability, PPO-Clip, Large Language Models<br /><br />Summary:  
This paper addresses instability issues in large language model post-training that uses off-policy reinforcement learning. The core challenge lies in distribution shifts causing policies to deviate beyond a trust region, leading to unstable gradients and policy entropy fluctuations. Although PPO-Clip helps by clipping importance weights, it fails to account for global distributional shifts of actions, especially those un-sampled. To overcome this, the authors propose using the entropy ratio between current and previous policies as a global metric to measure changes in policy exploration across updates. Building on this, they introduce Entropy Ratio Clipping (ERC), a mechanism that applies bidirectional constraints on the entropy ratio. ERC stabilizes policy updates at the global distribution level, complementing PPO-Clip’s local importance clipping by controlling shifts in probabilities of all actions. The ERC method is integrated into two reinforcement learning algorithms, DAPO and GPPO. Experiments on multiple benchmarks demonstrate that incorporating ERC consistently improves training stability and overall performance. This work contributes a novel global constraint mechanism that enhances reinforcement learning alignment and capability in training large language models. <div>
arXiv:2512.05591v1 Announce Type: cross 
Abstract: Large language model post-training relies on reinforcement learning to improve model capability and alignment quality. However, the off-policy training paradigm introduces distribution shift, which often pushes the policy beyond the trust region, leading to training instabilities manifested as fluctuations in policy entropy and unstable gradients. Although PPO-Clip mitigates this issue through importance clipping, it still overlooks the global distributional shift of actions. To address these challenges, we propose using the entropy ratio between the current and previous policies as a new global metric that effectively quantifies the relative change in policy exploration throughout updates. Building on this metric, we introduce an \textbf{Entropy Ratio Clipping} (ERC) mechanism that imposes bidirectional constraints on the entropy ratio. This stabilizes policy updates at the global distribution level and compensates for the inability of PPO-clip to regulate probability shifts of un-sampled actions. We integrate ERC into both DAPO and GPPO reinforcement learning algorithms. Experiments across multiple benchmarks show that ERC consistently improves performance.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ontology Learning with LLMs: A Benchmark Study on Axiom Identification</title>
<link>https://arxiv.org/abs/2512.05594</link>
<guid>https://arxiv.org/abs/2512.05594</guid>
<content:encoded><![CDATA[
<div> Ontology Learning, Large Language Models, Ontology Axiom Benchmark, Axiom Identification, Prompting Strategies  

<br /><br />Summary:  
This paper addresses the challenge of automating ontology axiom identification, a crucial task in ontology learning that defines logical relations between classes and properties. The authors introduce OntoAxiom, a benchmark comprising nine medium-sized ontologies with 17,118 triples and 2,771 axioms focused on subclass, disjoint, subproperty, domain, and range axioms. The study evaluates twelve large language models (LLMs) under three shot settings using two prompting strategies: a Direct approach querying all axioms simultaneously and an Axiom-by-Axiom (AbA) approach querying one axiom per prompt. Results indicate the AbA prompting outperforms the Direct approach in terms of F1 scores. Performance varies depending on the type of axiom and the domain; for instance, the FOAF ontology subclass axiom scored 0.642, whereas the music ontology scored 0.218. Larger LLMs generally yield better results but smaller models may still be practical in resource-limited scenarios. Despite no fully automated axiom identification solution due to moderate performance, LLMs serve as valuable tools to suggest candidate axioms, supporting ontology engineers in the development and refinement of ontologies. <div>
arXiv:2512.05594v1 Announce Type: cross 
Abstract: Ontologies are an important tool for structuring domain knowledge, but their development is a complex task that requires significant modelling and domain expertise. Ontology learning, aimed at automating this process, has seen advancements in the past decade with the improvement of Natural Language Processing techniques, and especially with the recent growth of Large Language Models (LLMs). This paper investigates the challenge of identifying axioms: fundamental ontology components that define logical relations between classes and properties. In this work, we introduce an Ontology Axiom Benchmark OntoAxiom, and systematically test LLMs on that benchmark for axiom identification, evaluating different prompting strategies, ontologies, and axiom types. The benchmark consists of nine medium-sized ontologies with together 17.118 triples, and 2.771 axioms. We focus on subclass, disjoint, subproperty, domain, and range axioms. To evaluate LLM performance, we compare twelve LLMs with three shot settings and two prompting strategies: a Direct approach where we query all axioms at once, versus an Axiom-by-Axiom (AbA) approach, where each prompt queries for one axiom only. Our findings show that the AbA prompting leads to higher F1 scores than the direct approach. However, performance varies across axioms, suggesting that certain axioms are more challenging to identify. The domain also influences performance: the FOAF ontology achieves a score of 0.642 for the subclass axiom, while the music ontology reaches only 0.218. Larger LLMs outperform smaller ones, but smaller models may still be viable for resource-constrained settings. Although performance overall is not high enough to fully automate axiom identification, LLMs can provide valuable candidate axioms to support ontology engineers with the development and refinement of ontologies.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Big Tech-Funded AI Papers Have Higher Citation Impact, Greater Insularity, and Larger Recency Bias</title>
<link>https://arxiv.org/abs/2512.05714</link>
<guid>https://arxiv.org/abs/2512.05714</guid>
<content:encoded><![CDATA[
<div> Keywords: industry funding, AI research, citation impact, citation preference ratio, research insularity  

<br /><br />Summary:  
This study investigates the role of industry funding in AI research over the period 1998-2022, analyzing nearly 50,000 papers and millions of citations from top AI conferences such as ICLR, CVPR, AAAI, and ACL. First, it quantifies the growth of industry-funded papers, showing an increase from under 2% before 2015 to over 11% by 2020. Second, it highlights the superior citation impact of industry-funded research, where between 2018 and 2022, 12% of such papers reached high citation levels (h5-index) compared to just 4% for non-industry-funded and 2% for non-funded papers. Third, the paper introduces the Citation Preference Ratio (CPR), revealing that top AI conferences favor citing industry-funded research over non-funded work. Fourth, it uncovers a growing insularity in industry-funded research, as these papers preferentially cite other industry-funded studies and reference fewer non-funded works. Finally, the study finds that industry-funded research tends to cite more recent publications, reflecting a preference for the latest developments. Overall, the findings show a marked shift towards increased industry presence and influence in AI research, with implications for the dynamics of knowledge production and dissemination in the field. <div>
arXiv:2512.05714v1 Announce Type: cross 
Abstract: Over the past four decades, artificial intelligence (AI) research has flourished at the nexus of academia and industry. However, Big Tech companies have increasingly acquired the edge in computational resources, big data, and talent. So far, it has been largely unclear how many papers the industry funds, how their citation impact compares to non-funded papers, and what drives industry interest. This study fills that gap by quantifying the number of industry-funded papers at 10 top AI conferences (e.g., ICLR, CVPR, AAAI, ACL) and their citation influence. We analyze about 49.8K papers, about 1.8M citations from AI papers to other papers, and about 2.3M citations from other papers to AI papers from 1998-2022 in Scopus. Through seven research questions, we examine the volume and evolution of industry funding in AI research, the citation impact of funded papers, the diversity and temporal range of their citations, and the subfields in which industry predominantly acts. Our findings reveal that industry presence has grown markedly since 2015, from less than 2 percent to more than 11 percent in 2020. Between 2018 and 2022, 12 percent of industry-funded papers achieved high citation rates as measured by the h5-index, compared to 4 percent of non-industry-funded papers and 2 percent of non-funded papers. Top AI conferences engage more with industry-funded research than non-funded research, as measured by our newly proposed metric, the Citation Preference Ratio (CPR). We show that industry-funded research is increasingly insular, citing predominantly other industry-funded papers while referencing fewer non-funded papers. These findings reveal new trends in AI research funding, including a shift towards more industry-funded papers and their growing citation impact, greater insularity of industry-funded work than non-funded work, and a preference of industry-funded research to cite recent work.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Active Video Perception: Iterative Evidence Seeking for Agentic Long Video Understanding</title>
<link>https://arxiv.org/abs/2512.05774</link>
<guid>https://arxiv.org/abs/2512.05774</guid>
<content:encoded><![CDATA[
<div> Long video understanding, Active perception, Video reasoning, MLLM agents, Evidence-seeking  

<br /><br />Summary:  
1. Long video understanding (LVU) is difficult due to sparse, temporally dispersed relevant cues hidden within hours of mostly redundant and irrelevant video content.  
2. Current LVU frameworks depend on query-agnostic captioners which waste computational resources on irrelevant content and lose fine-grained temporal and spatial details.  
3. Inspired by active perception theory, the authors propose that LVU agents should actively decide what, when, and where to observe in the video and continuously assess if the gathered evidence suffices to answer the query.  
4. The paper presents Active Video Perception (AVP), an evidence-seeking framework treating the video as an interactive environment, acquiring compact, query-relevant evidence directly from pixels.  
5. AVP operates via an iterative plan-observe-reflect process using multimodal large language models (MLLM) agents: a planner suggests targeted interactions, an observer extracts time-stamped evidence, and a reflector evaluates whether the collected evidence answers the query or if further observation is needed.  
6. Experiments across five LVU benchmarks show AVP achieves the highest performance, improving average accuracy by 5.7% over the best previous agentic method while requiring only 18.4% of the inference time and 12.4% of the input tokens. <div>
arXiv:2512.05774v1 Announce Type: cross 
Abstract: Long video understanding (LVU) is challenging because answering real-world queries often depends on sparse, temporally dispersed cues buried in hours of mostly redundant and irrelevant content. While agentic pipelines improve video reasoning capabilities, prevailing frameworks rely on a query-agnostic captioner to perceive video information, which wastes computation on irrelevant content and blurs fine-grained temporal and spatial information. Motivated by active perception theory, we argue that LVU agents should actively decide what, when, and where to observe, and continuously assess whether the current observation is sufficient to answer the query. We present Active Video Perception (AVP), an evidence-seeking framework that treats the video as an interactive environment and acquires compact, queryrelevant evidence directly from pixels. Concretely, AVP runs an iterative plan-observe-reflect process with MLLM agents. In each round, a planner proposes targeted video interactions, an observer executes them to extract time-stamped evidence, and a reflector evaluates the sufficiency of the evidence for the query, either halting with an answer or triggering further observation. Across five LVU benchmarks, AVP achieves highest performance with significant improvements. Notably, AVP outperforms the best agentic method by 5.7% in average accuracy while only requires 18.4% inference time and 12.4% input tokens.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vague Knowledge: Information without Transitivity and Partitions</title>
<link>https://arxiv.org/abs/2512.05833</link>
<guid>https://arxiv.org/abs/2512.05833</guid>
<content:encoded><![CDATA[
<div> Keywords: vague knowledge, non-transitive indistinguishability, economic models, natural language communication, qualitative reasoning  

<br /><br />Summary:  
1. The paper challenges traditional economic models of information that assume knowledge is transitive and forms partitions over states.  
2. It introduces the concept of vague knowledge, characterized by non-transitive indistinguishability across different states, meaning some states cannot be neatly separated or ordered by knowledge.  
3. Despite vague knowledge failing to create a clear partition of the state space, it remains informative by differentiating some states from others.  
4. The study demonstrates that vague knowledge can only be accurately conveyed through vague communication methods, which inherently feature blurred or fuzzy boundaries.  
5. These theoretical insights provide foundational explanations for why natural language—often imprecise and qualitative—is prevalent in real-world human communication and reasoning, reflecting the inherent vagueness of knowledge itself. <div>
arXiv:2512.05833v1 Announce Type: cross 
Abstract: I relax the standard assumptions of transitivity and partition structure in economic models of information to formalize vague knowledge: non-transitive indistinguishability over states. I show that vague knowledge, while failing to partition the state space, remains informative by distinguishing some states from others. Moreover, it can only be faithfully expressed through vague communication with blurred boundaries. My results provide microfoundations for the prevalence of natural language communication and qualitative reasoning in the real world, where knowledge is often vague.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Natural Language Summarization Enables Multi-Repository Bug Localization by LLMs in Microservice Architectures</title>
<link>https://arxiv.org/abs/2512.05908</link>
<guid>https://arxiv.org/abs/2512.05908</guid>
<content:encoded><![CDATA[
<div> Bug localization, multi-repository, microservice architectures, natural language reasoning, hierarchical summaries<br /><br />Summary:<br /><br />1. The paper addresses the challenge of bug localization in multi-repository microservice architectures, which is complicated by the semantic gap between natural language bug reports and source code, as well as limitations in large language model (LLM) context sizes and the need to identify the correct repository initially. <br /><br />2. The authors propose reframing bug localization as a natural language reasoning task by converting codebases into hierarchical natural language (NL) summaries at multiple levels: file, directory, and repository. This allows the process to be conducted as NL-to-NL search rather than cross-modal retrieval from language to code. <br /><br />3. The method involves a two-phase search: first, routing the bug report to the most relevant repositories, then performing a detailed top-down localization within those repositories to identify specific files likely responsible for the bug. <br /><br />4. Evaluation is conducted on DNext, a large industrial system comprising 46 repositories with 1.1 million lines of code. The approach achieves strong performance metrics—Pass@10 of 0.82 and Mean Reciprocal Rank (MRR) of 0.50—significantly outperforming retrieval-based baselines and agentic retrieval-augmented generation (RAG) systems such as GitHub Copilot and Cursor. <br /><br />5. This work highlights the advantage of engineered NL representations over raw source code for scalable bug localization, providing an interpretable multi-level search path from repository down to file. This interpretability is critical to fostering trust and transparency in enterprise AI tools. <div>
arXiv:2512.05908v1 Announce Type: cross 
Abstract: Bug localization in multi-repository microservice architectures is challenging due to the semantic gap between natural language bug reports and code, LLM context limitations, and the need to first identify the correct repository. We propose reframing this as a natural language reasoning task by transforming codebases into hierarchical NL summaries and performing NL-to-NL search instead of cross-modal retrieval. Our approach builds context-aware summaries at file, directory, and repository levels, then uses a two-phase search: first routing bug reports to relevant repositories, then performing top-down localization within those repositories. Evaluated on DNext, an industrial system with 46 repositories and 1.1M lines of code, our method achieves Pass@10 of 0.82 and MRR of 0.50, significantly outperforming retrieval baselines and agentic RAG systems like GitHub Copilot and Cursor. This work demonstrates that engineered natural language representations can be more effective than raw source code for scalable bug localization, providing an interpretable repository -> directory -> file search path, which is vital for building trust in enterprise AI tools by providing essential transparency.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>To Err Is Human: Systematic Quantification of Errors in Published AI Papers via LLM Analysis</title>
<link>https://arxiv.org/abs/2512.05925</link>
<guid>https://arxiv.org/abs/2512.05925</guid>
<content:encoded><![CDATA[
<div> Keywords: AI papers, mistakes, GPT-5, peer review, reproducibility<br /><br />Summary:<br /><br />1. The study investigates the frequency of objective mistakes in published AI research papers, focusing on verifiable errors such as incorrect formulas, derivations, calculations, figures, and tables, deliberately excluding subjective aspects like novelty or writing quality.<br />2. Researchers developed a Paper Correctness Checker, leveraging GPT-5, to systematically identify mistakes in papers from top AI conferences and journals.<br />3. Results show a rising trend in mistakes per paper over recent years, with examples including an increase from 3.8 errors in NeurIPS 2021 to 5.9 in NeurIPS 2025 (a 55.3% increase), as well as increases in ICLR and TMLR conferences.<br />4. Human experts validated 316 flagged errors, confirming 263 as actual mistakes, yielding an 83.2% precision rate for the AI Checker.<br />5. While most detected errors are minor, correcting them would improve clarity and reproducibility, with some mistakes potentially affecting results interpretation.<br />6. Importantly, the AI Checker can propose accurate corrections for 75.8% of the identified mistakes.<br />7. The study demonstrates the growing utility of advanced large language models in enhancing research quality by detecting and fixing objective errors in published scientific literature. <div>
arXiv:2512.05925v1 Announce Type: cross 
Abstract: How many mistakes do published AI papers contain? Peer-reviewed publications form the foundation upon which new research and knowledge are built. Errors that persist in the literature can propagate unnoticed, creating confusion in follow-up studies and complicating reproducibility. The accelerating pace of research and the increasing demands on the peer-review system make such mistakes harder to detect and avoid. To address this, we developed a Paper Correctness Checker based on GPT-5 to systematically identify mistakes in papers previously published at top AI conferences and journals. Our analysis focuses on objective mistakes-e.g., errors in formulas, derivations, calculations, figures, and tables-that have a clearly verifiable ground truth. We intentionally exclude subjective considerations such as novelty, importance, or writing quality. We find that published papers contain a non-negligible number of objective mistakes and that the average number of mistakes per paper has increased over time-from 3.8 in NeurIPS 2021 to 5.9 in NeurIPS 2025 (55.3% increase); from 4.1 in ICLR 2018 to 5.2 in ICLR 2025; and from 5.0 in TMLR 2022/23 to 5.5 in TMLR 2025. Human experts reviewed 316 potential mistakes identified by the AI Checker and confirmed that 263 were actual mistakes, corresponding to a precision of 83.2%. While most identified issues are relatively minor, correcting them would reduce confusion in the literature and strengthen reproducibility. The AI Checker also surfaced potentially more substantive mistakes that could affect the interpretation of results. Moreover, we show that the AI Checker can propose correct fixes for 75.8% of the identified mistakes. Overall, this study highlights the potential of frontier LLMs to detect and correct objective mistakes in published papers, helping to establish a firmer foundation of knowledge.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zoom in, Click out: Unlocking and Evaluating the Potential of Zooming for GUI Grounding</title>
<link>https://arxiv.org/abs/2512.05941</link>
<guid>https://arxiv.org/abs/2512.05941</guid>
<content:encoded><![CDATA[
<div> Keywords: GUI grounding, zoom prior, training-free method, UI-Venus-72B, GUIZoom-Bench<br /><br />Summary:<br /><br />This paper addresses the challenge of GUI grounding, which is essential for building graphical user interface agents capable of understanding and interacting with interface elements. Current methods depend heavily on large-scale bounding box annotations but struggle with issues like cross-platform generalization, complex layout analysis, and fine-grained element localization. The authors propose leveraging zoom as a strong and underexplored prior to improve GUI grounding. They introduce ZoomClick, a training-free method that utilizes four key properties of zoom—pre-zoom, depth, shrink size, and minimal crop size—to enable dynamic spatial focusing and adaptive context switching without requiring additional training. Experimental results indicate that ZoomClick substantially enhances the performance of both general vision-language models and specialized GUI grounding models, achieving state-of-the-art results, such as a 73.1% success rate on the ScreenSpot-Pro benchmark by the UI-Venus-72B model. Additionally, the paper presents a new benchmark called GUIZoom-Bench designed to evaluate model adaptability to zoom operations. This benchmark aims to stimulate future research focused on improving zoom techniques for enhanced training and test-time scaling in GUI grounding tasks. <div>
arXiv:2512.05941v1 Announce Type: cross 
Abstract: Grounding is a fundamental capability for building graphical user interface (GUI) agents. Although existing approaches rely on large-scale bounding box supervision, they still face various challenges, such as cross-platform generalization, complex layout analysis, and fine-grained element localization. In this paper, we investigate zoom as a strong yet underexplored prior for GUI grounding, and propose a training-free method, ZoomClick. By characterizing four key properties of zoom (i.e., pre-zoom, depth, shrink size, minimal crop size), we unlock its full capabilities for dynamic spatial focusing and adaptive context switching. Experiments demonstrate that our method significantly boosts the performance of both general vision-language and specialized GUI grounding models, achieving state-of-the-art results on several mainstream benchmarks; for example, UI-Venus-72B attains a 73.1% success rate on ScreenSpot-Pro. Furthermore, we present GUIZoom-Bench, a benchmark for evaluating model adaptability to zoom, aiming to inspire future research on improving zoom for further training and test-time scaling in GUI grounding tasks.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Retrieval-Augmented Generation with Entity Linking for Educational Platforms</title>
<link>https://arxiv.org/abs/2512.05967</link>
<guid>https://arxiv.org/abs/2512.05967</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, Entity Linking, Educational Question-Answering, Domain Adaptation, Hybrid Ranking  

<br /><br />Summary:  
1. The paper addresses challenges faced by Retrieval-Augmented Generation (RAG) systems, particularly their struggle to maintain factual accuracy in domain-specific contexts due to terminological ambiguity.  
2. It introduces an enhanced RAG architecture that incorporates a factual signal through Entity Linking based on Wikidata, aimed at improving the precision of educational question-answering systems in Italian.  
3. Three re-ranking strategies were evaluated to integrate semantic similarity with entity-based information: a hybrid score weighting model, reciprocal rank fusion, and a cross-encoder re-ranker.  
4. Experiments were conducted on two benchmarks: a custom academic dataset tailored to educational content and the general-domain SQuAD-it dataset.  
5. Results demonstrated that in specialized domains, the reciprocal rank fusion hybrid approach significantly outperformed both the baseline semantic-only systems and a cross-encoder re-ranker, whereas the cross-encoder excelled on the general-domain dataset.  
6. The study highlights the impact of domain mismatch on retrieval accuracy, emphasizing the need for domain adaptation and hybrid ranking methods to boost factual correctness in RAG systems.  
7. The findings underscore the potential of entity-aware RAG architectures to foster more reliable and adaptive AI tutoring tools in educational settings, improving knowledge grounding and answer precision. <div>
arXiv:2512.05967v1 Announce Type: cross 
Abstract: In the era of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) architectures are gaining significant attention for their ability to ground language generation in reliable knowledge sources. Despite their impressive effectiveness in many areas, RAG systems based solely on semantic similarity often fail to ensure factual accuracy in specialized domains, where terminological ambiguity can affect retrieval relevance. This study proposes an enhanced RAG architecture that integrates a factual signal derived from Entity Linking to improve the accuracy of educational question-answering systems in Italian. The system includes a Wikidata-based Entity Linking module and implements three re-ranking strategies to combine semantic and entity-based information: a hybrid score weighting model, reciprocal rank fusion, and a cross-encoder re-ranker. Experiments were conducted on two benchmarks: a custom academic dataset and the standard SQuAD-it dataset. Results show that, in domain-specific contexts, the hybrid schema based on reciprocal rank fusion significantly outperforms both the baseline and the cross-encoder approach, while the cross-encoder achieves the best results on the general-domain dataset. These findings confirm the presence of an effect of domain mismatch and highlight the importance of domain adaptation and hybrid ranking strategies to enhance factual precision and reliability in retrieval-augmented generation. They also demonstrate the potential of entity-aware RAG systems in educational environments, fostering adaptive and reliable AI-based tutoring tools.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Data-efficient Customer Intent Recognition with Prompt-based Learning Paradigm</title>
<link>https://arxiv.org/abs/2309.14779</link>
<guid>https://arxiv.org/abs/2309.14779</guid>
<content:encoded><![CDATA[
<div> Keywords: customer intent recognition, prompt-based learning, small language models, active sampling, zero-shot setting<br /><br />Summary:<br /><br />This paper addresses the challenge of accurately recognizing customer intent in conversational data, which is critical for digital customer service but often limited by insufficient labeled data. The authors propose a prompt-based learning paradigm that reduces the reliance on large datasets by using prompted training paired with answer mapping techniques. This allows small language models to achieve competitive intent recognition performance with minimal training data. To further improve results, the study integrates active sampling and ensemble learning strategies within the prompted training pipeline. Additionally, preliminary zero-shot experiments reveal that small language models, when equipped with carefully designed and detailed prompts, can follow instructions effectively even without additional training. These findings demonstrate the potential for more data-efficient semantic modeling of customer-agent conversations. The approach paves the way for advancements in AI-driven customer service by enabling intent recognition tasks using limited data and leveraging small-scale models, making the technology more accessible and practical in real-world applications. <div>
arXiv:2309.14779v2 Announce Type: replace 
Abstract: Recognizing customer intent accurately with language models based on customer-agent conversational data is essential in today's digital customer service marketplace, but it is often hindered by the lack of sufficient labeled data. In this paper, we introduce the prompt-based learning paradigm that significantly reduces the dependency on extensive datasets. Utilizing prompted training combined with answer mapping techniques, this approach allows small language models to achieve competitive intent recognition performance with only a minimal amount of training data. Furthermore, We enhance the performance by integrating active sampling and ensemble learning strategies in the prompted training pipeline. Additionally, preliminary tests in a zero-shot setting demonstrate that, with well-crafted and detailed prompts, small language models show considerable instruction-following potential even without any further training. These results highlight the viability of semantic modeling of conversational data in a more data-efficient manner with minimal data use, paving the way for advancements in AI-driven customer service.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision-centric Token Compression in Large Language Model</title>
<link>https://arxiv.org/abs/2502.00791</link>
<guid>https://arxiv.org/abs/2502.00791</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, token compression, Vision Centric Token Compression, Probability-Informed Visual Enhancement, in-context learning benchmarks<br /><br />Summary:<br /><br />1. The paper addresses the challenge of scaling Large Language Models (LLMs) with expanding context windows reaching hundreds of thousands of tokens, and model sizes growing from billions to trillions of parameters, which leads to soaring compute and memory demands.<br /><br />2. To mitigate these costs, the authors propose Vision Centric Token Compression (Vist), a novel slow-fast compression framework inspired by human reading patterns. The fast path converts distant tokens into images, allowing a frozen, lightweight vision encoder to efficiently skim low-salience context, while the slow path processes the proximal token window in detail within the LLM for nuanced reasoning.<br /><br />3. The framework incorporates a Probability-Informed Visual Enhancement (PVE) training objective that masks high-frequency tokens, guiding the Resampler to concentrate on semantically important regions akin to how skilled readers skip function words.<br /><br />4. Experimental results on eleven in-context learning benchmarks demonstrate that Vist matches accuracy with 2.3 times fewer tokens, reducing floating-point operations (FLOPs) by 16% and memory usage by 50%.<br /><br />5. Vist outperforms the strongest existing text-based compression method CEPE by 7.6% on average across benchmarks including TriviaQA, NQ, PopQA, NLUI, and CLIN, setting a new standard in token efficiency for LLMs. The project code is available at https://github.com/CSU-JPG/VIST. <div>
arXiv:2502.00791v4 Announce Type: replace 
Abstract: Real-world applications are stretching context windows to hundreds of thousand of tokens while Large Language Models (LLMs) swell from billions to trillions of parameters. This dual expansion send compute and memory costs skyrocketing, making token compression indispensable. We introduce Vision Centric Token Compression (Vist), a slow-fast compression framework that mirrors human reading: the fast path renders distant tokens into images, letting a frozen, lightweight vision encoder skim the low-salience context; the slow path feeds the proximal window into the LLM for fine-grained reasoning. A Probability-Informed Visual Enhancement (PVE) objective masks high-frequency tokens during training, steering the Resampler to concentrate on semantically rich regions-just as skilled reader gloss over function words. On eleven in-context learning benchmarks, Vist achieves the same accuracy with 2.3 times fewer tokens, cutting FLOPs by 16% and memory by 50%. This method delivers remarkable results, outperforming the strongest text encoder-based compression method CEPE by 7.6% on average over benchmarks like TriviaQA, NQ, PopQA, NLUI, and CLIN, setting a new standard for token efficiency in LLMs. The project is at https://github.com/CSU-JPG/VIST.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Experiments with Large Language Models on Retrieval-Augmented Generation for Closed-Source Simulation Software</title>
<link>https://arxiv.org/abs/2502.03916</link>
<guid>https://arxiv.org/abs/2502.03916</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Retrieval-Augmented Generation, closed-source software, local LLMs, simulation frameworks  

<br /><br />Summary:  
This work investigates the use of Retrieval-Augmented Generation (RAG) systems to improve the capabilities of Large Language Models (LLMs) when working with closed-source simulation software, specifically the mesh-free simulation tool Pasimodo. Due to challenges like hallucinations from LLMs, especially with unknown or proprietary data, the study focuses on integrating external knowledge through RAG to enhance response accuracy. Tasks explored include smart-autocomplete, question answering via text extraction, model summarization, component explanation, compositional reasoning, and the generation of simulation components and input models. Prioritizing data protection and intellectual property rights, the evaluation emphasizes local LLM deployment rather than cloud-based solutions, making it more accessible for smaller institutions. Initial tests with smaller language models show promising yet imperfect results, with failures often linked to inadequate information supplied to the models. Various strategies to boost response quality were assessed, revealing that customizing the information fed to LLMs based on specific prompts markedly improves outcomes. Overall, the study highlights both the substantial potential of RAG-enhanced local LLMs in supporting closed-source simulation environments and the necessity for continued research to optimize information retrieval and model interaction for such specialized applications. <div>
arXiv:2502.03916v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are tools that have become indispensable in development and programming. However, they suffer from hallucinations, especially when dealing with unknown knowledge. This is particularly the case when LLMs are to be used to support closed-source software applications. Retrieval-Augmented Generation (RAG) offers an approach to use additional knowledge alongside the pre-trained knowledge of the LLM to respond to user prompts. Possible tasks range from a smart-autocomplete, text extraction for question answering, model summarization, component explaining, compositional reasoning, to creation of simulation components and complete input models. This work tests existing RAG systems for closed-source simulation frameworks, in our case the mesh-free simulation software Pasimodo. Since data protection and intellectual property rights are particularly important for problems solved with closed-source software, the tests focus on execution using local LLMs. In order to enable smaller institutions to use the systems, smaller language models will be tested first. The systems show impressive results, but often fail due to insufficient information. Different approaches for improving response quality are tested. In particular, tailoring the information provided to the LLMs dependent to the prompts proves to be a significant improvement. This demonstrates the great potential and the further work needed to improve information retrieval for closed-source simulation models.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AURA: A Diagnostic Framework for Tracking User Satisfaction of Interactive Planning Agents</title>
<link>https://arxiv.org/abs/2505.01592</link>
<guid>https://arxiv.org/abs/2505.01592</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, task planning agents, user satisfaction, agent evaluation, AURA framework  

<br /><br />Summary: The paper addresses the limitations of existing benchmarks for task planning agents, which mainly focus on task completion as a proxy for agent effectiveness. It argues that solely improving task completion does not necessarily maximize user satisfaction, since users experience the entire interactive process, not just the final output. To bridge this gap, the authors propose AURA, an Agent-User inteRaction Assessment framework that models the behavioral stages within interactive task planning agents. AURA provides a detailed assessment by applying atomic evaluation criteria based on large language models, enabling fine-grained diagnosis of strengths and weaknesses throughout the agent’s decision-making pipeline. Analysis using AURA reveals that different agents excel at different behavioral stages and emphasizes that user satisfaction depends on both final outcomes and intermediate behaviors. The study also discusses future directions, including the development of multi-agent systems and acknowledges the current limitations posed by user simulators in task planning contexts. By focusing on the interaction process as well as the results, AURA aims to improve the design and evaluation of task planning agents in realistic applications. <div>
arXiv:2505.01592v2 Announce Type: replace 
Abstract: The growing capabilities of large language models (LLMs) in instruction-following and context-understanding lead to the era of agents with numerous applications. Among these, task planning agents have become especially prominent in realistic scenarios involving complex internal pipelines, such as context understanding, tool management, and response generation. However, existing benchmarks predominantly evaluate agent performance based on task completion as a proxy for overall effectiveness. We hypothesize that merely improving task completion is misaligned with maximizing user satisfaction, as users interact with the entire agentic process and not only the end result. To address this gap, we propose AURA, an Agent-User inteRaction Assessment framework that conceptualizes the behavioral stages of interactive task planning agents. AURA offers a comprehensive assessment of agent through a set of atomic LLM evaluation criteria, allowing researchers and practitioners to diagnose specific strengths and weaknesses within the agent's decision-making pipeline. Our analyses show that agents excel in different behavioral stages, with user satisfaction shaped by both outcomes and intermediate behaviors. We also highlight future directions, including systems that leverage multiple agents and the limitations of user simulators in task planning.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAE-SSV: Supervised Steering in Sparse Representation Spaces for Reliable Control of Language Models</title>
<link>https://arxiv.org/abs/2505.16188</link>
<guid>https://arxiv.org/abs/2505.16188</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, sparse autoencoders, supervised steering, interpretable representations, behavior control  

<br /><br />Summary:  
This paper presents a novel method for controlling large language models (LLMs) through supervised steering in sparse, interpretable latent representation spaces. First, sparse autoencoders (SAEs) are used to extract sparse latent representations that disentangle semantic attributes from the LLMs' activations. Second, linear classifiers identify a small subset of task-relevant latent dimensions, focusing the steering efforts on a well-defined subspace. Third, supervised steering vectors constrained to this subspace are learned and optimized to align generated outputs with desired target behaviors. The approach is evaluated on multiple tasks—sentiment control, truthfulness, and political polarity—across several LLMs. Results reveal that this method yields higher success rates in steering model behavior while maintaining minimal quality degradation in text generation compared to existing steering techniques. Further analysis demonstrates that an unexpectedly small subspace is sufficient for effective steering, which facilitates more targeted, efficient, and interpretable intervention mechanisms. Additionally, the implementation of this supervised steering approach, termed SAE-SSV, is publicly available, providing a valuable resource for further research and practical applications in controllable LLM generation. <div>
arXiv:2505.16188v2 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, but controlling their behavior reliably remains challenging, especially in open-ended generation settings. This paper introduces a novel supervised steering approach that operates in sparse, interpretable representation spaces. We employ sparse autoencoders (SAEs) to obtain sparse latent representations that aim to disentangle semantic attributes from model activations. Then we train linear classifiers to identify a small subspace of task-relevant dimensions in latent representations. Finally, we learn supervised steering vectors constrained to this subspace, optimized to align with target behaviors. Experiments across sentiment, truthfulness, and political polarity steering tasks with multiple LLMs demonstrate that our supervised steering vectors achieve higher success rates with minimal degradation in generation quality compared to existing methods. Further analysis reveals that a notably small subspace is sufficient for effective steering, enabling more targeted and interpretable interventions. Our implementation is publicly available at https://github.com/Ineedanamehere/SAE-SSV.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A quantitative analysis of semantic information in deep representations of text and images</title>
<link>https://arxiv.org/abs/2505.17101</link>
<guid>https://arxiv.org/abs/2505.17101</guid>
<content:encoded><![CDATA[
<div> Deep neural networks, semantic representations, large language models, vision transformers, information asymmetry<br /><br />Summary:<br /><br />This article investigates how deep neural networks develop similar semantic representations for related data across different domains, such as images and their textual descriptions or multilingual text. The authors introduce a method to quantitatively measure the relative information content of these representations and analyze how semantic information is distributed across multiple tokens within large language models (LLMs) and vision transformers. Focusing initially on translated sentence pairs processed by LLMs, they identify specific "semantic" layers that contain the most transferable language information. It is observed that larger models, such as DeepSeek-V3, capture more general semantic information than smaller models like Llama3.1-8B. In English texts, semantic information spreads over many tokens and exhibits both long-distance correlations and a causal left-to-right asymmetry. Additionally, the study pinpoints semantic encoding layers within vision transformers, revealing that caption representations in LLM semantic layers can successfully predict corresponding image representations. Finally, significant and model-dependent asymmetries are found between image and text semantic representations, highlighting differences in how these modalities process and encode information. <div>
arXiv:2505.17101v4 Announce Type: replace 
Abstract: Deep neural networks are known to develop similar representations for semantically related data, even when they belong to different domains, such as an image and its description, or the same text in different languages. We present a method for quantitatively investigating this phenomenon by measuring the relative information content of the representations of semantically related data and probing how it is encoded into multiple tokens of large language models (LLMs) and vision transformers. Looking first at how LLMs process pairs of translated sentences, we identify inner ``semantic'' layers containing the most language-transferable information. We find moreover that, on these layers, a larger LLM (DeepSeek-V3) extracts significantly more general information than a smaller one (Llama3.1-8B). Semantic information of English text is spread across many tokens and it is characterized by long-distance correlations between tokens and by a causal left-to-right (i.e., past-future) asymmetry. We also identify layers encoding semantic information within visual transformers. We show that caption representations in the semantic layers of LLMs predict visual representations of the corresponding images. We observe significant and model-dependent information asymmetries between image and text representations.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Mamba Meets Hyperbolic Geometry: A New Paradigm for Structured Language Embeddings</title>
<link>https://arxiv.org/abs/2505.18973</link>
<guid>https://arxiv.org/abs/2505.18973</guid>
<content:encoded><![CDATA[
arXiv:2505.18973v3 Announce Type: replace 
Abstract: Selective state-space models excel at long-sequence modeling, but their capacity for language representation -- in complex hierarchical reasoning -- remains underexplored. Most large language models rely on \textit{flat} Euclidean embeddings, limiting their ability to capture latent hierarchies. To address this, we propose {\it Hierarchical Mamba (HiM)}, integrating efficient Mamba2 with hyperbolic geometry to learn hierarchy-aware language embeddings for deeper linguistic understanding. Mamba2-processed sequences are projected to the Poincar\'e ball or Lorentzian manifold with ``learnable'' curvature, optimized with a hyperbolic loss. Our HiM model facilitates the capture of relational distances across varying hierarchical levels, enabling effective long-range reasoning for tasks like mixed-hop prediction and multi-hop inference in hierarchical classification. Experimental results show both HiM variants effectively capture hierarchical relationships across four linguistic and medical datasets, surpassing Euclidean baselines, with HiM-Poincar\'e providing fine-grained distinctions with higher h-norms, while HiM-Lorentz offers more stable, compact, and hierarchy-preserving embeddings-favoring robustness. The source code is publicly available at https://github.com/BerryByte/HiM.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pet-Bench: Benchmarking the Abilities of Large Language Models as E-Pets in Social Network Services</title>
<link>https://arxiv.org/abs/2506.03761</link>
<guid>https://arxiv.org/abs/2506.03761</guid>
<content:encoded><![CDATA[
arXiv:2506.03761v2 Announce Type: replace 
Abstract: As interest in using Large Language Models for interactive and emotionally rich experiences grows, virtual pet companionship emerges as a novel yet underexplored application. Existing approaches focus on basic pet role-playing interactions without systematically benchmarking LLMs for comprehensive companionship. In this paper, we introduce Pet-Bench, a dedicated benchmark that evaluates LLMs across both self-interaction and human-interaction dimensions. Unlike prior work, Pet-Bench emphasizes self-evolution and developmental behaviors alongside interactive engagement, offering a more realistic reflection of pet companionship. It features diverse tasks such as intelligent scheduling, memory-based dialogues, and psychological conversations, with over 7,500 interaction instances designed to simulate pet behaviors. Evaluation of 28 LLMs reveals significant performance variations linked to model size and inherent capabilities, underscoring the need for specialized optimization in this domain. Pet-Bench serves as a foundational resource for benchmarking pet-related LLM abilities and advancing emotionally immersive human-pet interactions.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CodeNER: Code Prompting for Named Entity Recognition</title>
<link>https://arxiv.org/abs/2507.20423</link>
<guid>https://arxiv.org/abs/2507.20423</guid>
<content:encoded><![CDATA[
arXiv:2507.20423v2 Announce Type: replace 
Abstract: Recent studies have explored various approaches for treating candidate named entity spans as both source and target sequences in named entity recognition (NER) by leveraging large language models (LLMs). Although previous approaches have successfully generated candidate named entity spans with suitable labels, they rely solely on input context information when using LLMs, particularly, ChatGPT. However, NER inherently requires capturing detailed labeling requirements with input context information. To address this issue, we propose a novel method that leverages code-based prompting to improve the capabilities of LLMs in understanding and performing NER. By embedding code within prompts, we provide detailed BIO schema instructions for labeling, thereby exploiting the ability of LLMs to comprehend long-range scopes in programming languages. Experimental results demonstrate that the proposed code-based prompting method outperforms conventional text-based prompting on ten benchmarks across English, Arabic, Finnish, Danish, and German datasets, indicating the effectiveness of explicitly structuring NER instructions. We also verify that combining the proposed code-based prompting method with the chain-of-thought prompting further improves performance.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey on Diffusion Language Models</title>
<link>https://arxiv.org/abs/2508.10875</link>
<guid>https://arxiv.org/abs/2508.10875</guid>
<content:encoded><![CDATA[
arXiv:2508.10875v2 Announce Type: replace 
Abstract: Diffusion Language Models (DLMs) are rapidly emerging as a powerful and promising alternative to the dominant autoregressive (AR) paradigm. By generating tokens in parallel through an iterative denoising process, DLMs possess inherent advantages in reducing inference latency and capturing bidirectional context, thereby enabling fine-grained control over the generation process. While achieving a several-fold speed-up, recent advancements have allowed DLMs to show performance comparable to their autoregressive counterparts, making them a compelling choice for various natural language processing tasks. In this survey, we provide a holistic overview of the current DLM landscape. We trace its evolution and relationship with other paradigms, such as autoregressive and masked language models, and cover both foundational principles and state-of-the-art models. Our work offers an up-to-date, comprehensive taxonomy and an in-depth analysis of current techniques, from pre-training strategies to advanced post-training methods. Another contribution of this survey is a thorough review of DLM inference strategies and optimizations, including improvements in decoding parallelism, caching mechanisms, and generation quality. We also highlight the latest approaches to multimodal extensions of DLMs and delineate their applications across various practical scenarios. Furthermore, our discussion addresses the limitations and challenges of DLMs, including efficiency, long-sequence handling, and infrastructure requirements, while outlining future research directions to sustain progress in this rapidly evolving field. Project GitHub is available at https://github.com/VILA-Lab/Awesome-DLMs.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HARP: Hallucination Detection via Reasoning Subspace Projection</title>
<link>https://arxiv.org/abs/2509.11536</link>
<guid>https://arxiv.org/abs/2509.11536</guid>
<content:encoded><![CDATA[
arXiv:2509.11536v2 Announce Type: replace 
Abstract: Hallucinations in Large Language Models (LLMs) pose a major barrier to their reliable use in critical decision-making. Although existing hallucination detection methods have improved accuracy, they still struggle with disentangling semantic and reasoning information and maintaining robustness. To address these challenges, we propose HARP (Hallucination detection via reasoning subspace projection), a novel hallucination detection framework. HARP establishes that the hidden state space of LLMs can be decomposed into a direct sum of a semantic subspace and a reasoning subspace, where the former encodes linguistic expression and the latter captures internal reasoning processes. Moreover, we demonstrate that the Unembedding layer can disentangle these subspaces, and by applying Singular Value Decomposition (SVD) to its parameters, the basis vectors spanning the semantic and reasoning subspaces are obtained. Finally, HARP projects hidden states onto the basis vectors of the reasoning subspace, and the resulting projections are then used as input features for hallucination detection in LLMs. By using these projections, HARP reduces the dimension of the feature to approximately 5% of the original, filters out most noise, and achieves enhanced robustness. Experiments across multiple datasets show that HARP achieves state-of-the-art hallucination detection performance; in particular, it achieves an AUROC of 92.8% on TriviaQA, outperforming the previous best method by 7.5%.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Simulation to Strategy: Automating Personalized Interaction Planning for Conversational Agents</title>
<link>https://arxiv.org/abs/2510.08621</link>
<guid>https://arxiv.org/abs/2510.08621</guid>
<content:encoded><![CDATA[
arXiv:2510.08621v2 Announce Type: replace 
Abstract: Amid the rapid rise of agentic dialogue models, realistic user-simulator studies are essential for tuning effective conversation strategies. This work investigates a sales-oriented agent that adapts its dialogue based on user profiles spanning age, gender, and occupation. While age and gender influence overall performance, occupation produces the most pronounced differences in conversational intent. Leveraging this insight, we introduce a lightweight, occupation-conditioned strategy that guides the agent to prioritize intents aligned with user preferences, resulting in shorter and more successful dialogues. Our findings highlight the importance of rich simulator profiles and demonstrate how simple persona-informed strategies can enhance the effectiveness of sales-oriented dialogue systems.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Analysing Moral Bias in Finetuned LLMs through Mechanistic Interpretability</title>
<link>https://arxiv.org/abs/2510.12229</link>
<guid>https://arxiv.org/abs/2510.12229</guid>
<content:encoded><![CDATA[
arXiv:2510.12229v2 Announce Type: replace 
Abstract: Large language models (LLMs) have been shown to internalize human-like biases during finetuning, yet the mechanisms by which these biases manifest remain unclear. In this work, we investigated whether the well-known Knobe effect, a moral bias in intentionality judgements, emerges in finetuned LLMs and whether it can be traced back to specific components of the model. We conducted a Layer-Patching analysis across 3 open-weights LLMs and demonstrated that the bias is not only learned during finetuning but also localized in a specific set of layers. Surprisingly, we found that patching activations from the corresponding pretrained model into just a few critical layers is sufficient to eliminate the effect. Our findings offer new evidence that social biases in LLMs can be interpreted, localized, and mitigated through targeted interventions, without the need for model retraining.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chinese Discharge Drug Recommendation in Metabolic Diseases with Large Language Models</title>
<link>https://arxiv.org/abs/2510.21084</link>
<guid>https://arxiv.org/abs/2510.21084</guid>
<content:encoded><![CDATA[
arXiv:2510.21084v2 Announce Type: replace 
Abstract: Intelligent drug recommendation based on Electronic Health Records (EHRs) is critical for improving the quality and efficiency of clinical decision-making. By leveraging large-scale patient data, drug recommendation systems can assist physicians in selecting the most appropriate medications according to a patient's medical history, diagnoses, laboratory results, and comorbidities. Recent advances in large language models (LLMs) have shown remarkable capabilities in complex reasoning and medical text understanding, making them promising tools for drug recommendation tasks. However, the application of LLMs for Chinese clinical medication recommendation remains largely unexplored. In this work, we conduct a systematic investigation of LLM-based methodologies for Chinese discharge medication recommendation. We evaluate several representative LLM families (GLM, Llama, Qwen) under a unified methodological framework including zero-shot prompting, in-context learning, chain-of-thought prompting, and supervised fine-tuning using LoRA. We analyze model behavior across reasoning styles, error patterns, domain adaptation mechanisms, and robustness. Experimental results show that while supervised fine-tuning improves model performance, there remains substantial room for improvement, with the best model achieving the F1 score of 0.5648 and Jaccard score of 0.4477. Our findings highlight both the potential and limitations of LLMs for Chinese drug recommendation.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HalluClean: A Unified Framework to Combat Hallucinations in LLMs</title>
<link>https://arxiv.org/abs/2511.08916</link>
<guid>https://arxiv.org/abs/2511.08916</guid>
<content:encoded><![CDATA[
arXiv:2511.08916v4 Announce Type: replace 
Abstract: Large language models (LLMs) have achieved impressive performance across a wide range of natural language processing tasks, yet they often produce hallucinated content that undermines factual reliability. To address this challenge, we introduce HalluClean, a lightweight and task-agnostic framework for detecting and correcting hallucinations in LLM-generated text. HalluClean adopts a reasoning-enhanced paradigm, explicitly decomposing the process into planning, execution, and revision stages to identify and refine unsupported claims. It employs minimal task-routing prompts to enable zero-shot generalization across diverse domains, without relying on external knowledge sources or supervised detectors. We conduct extensive evaluations on five representative tasks-question answering, dialogue, summarization, math word problems, and contradiction detection. Experimental results show that HalluClean significantly improves factual consistency and outperforms competitive baselines, demonstrating its potential to enhance the trustworthiness of LLM outputs in real-world applications.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LAET: A Layer-wise Adaptive Ensemble Tuning Framework for Pretrained Language Models</title>
<link>https://arxiv.org/abs/2511.11315</link>
<guid>https://arxiv.org/abs/2511.11315</guid>
<content:encoded><![CDATA[
arXiv:2511.11315v2 Announce Type: replace 
Abstract: Natural Language Processing (NLP) has transformed the financial industry, enabling advancements in areas such as textual analysis, risk management, and forecasting. Large language models (LLMs) like BloombergGPT and FinMA have set new benchmarks across various financial NLP tasks, including sentiment analysis, stock movement prediction, and credit risk assessment. Furthermore, FinMA-ES, a bilingual financial LLM, has also demonstrated strong performance using the FLARE and FLARE-ES benchmarks. However, the high computational demands of these models limit the accessibility of many organizations. To address this, we propose Layer-wise Adaptive Ensemble Tuning (LAET), a novel strategy that selectively fine-tunes the most effective layers of pre-trained LLMs by analyzing hidden state representations while freezing less critical layers. LAET significantly reduces computational overhead while enhancing task-specific performance. Our approach shows strong results in financial NLP tasks, outperforming existing benchmarks and state-of-the-art LLMs such as GPT-4, even with smaller LLMs ($\sim$3B parameters). This work bridges cutting-edge financial NLP research and real-world deployment with efficient and scalable models for financial applications.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fair Text Classification via Transferable Representations</title>
<link>https://arxiv.org/abs/2503.07691</link>
<guid>https://arxiv.org/abs/2503.07691</guid>
<content:encoded><![CDATA[
arXiv:2503.07691v2 Announce Type: replace-cross 
Abstract: Group fairness is a central research topic in text classification, where reaching fair treatment between sensitive groups (e.g., women and men) remains an open challenge. We propose an approach that extends the use of the Wasserstein Dependency Measure for learning unbiased neural text classifiers. Given the challenge of distinguishing fair from unfair information in a text encoder, we draw inspiration from adversarial training by inducing independence between representations learned for the target label and those for a sensitive attribute. We further show that Domain Adaptation can be efficiently leveraged to remove the need for access to the sensitive attributes in the dataset we cure. We provide both theoretical and empirical evidence that our approach is well-founded.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LittleBit: Ultra Low-Bit Quantization via Latent Factorization</title>
<link>https://arxiv.org/abs/2506.13771</link>
<guid>https://arxiv.org/abs/2506.13771</guid>
<content:encoded><![CDATA[
arXiv:2506.13771v3 Announce Type: replace-cross 
Abstract: Deploying large language models (LLMs) often faces challenges from substantial memory and computational costs. Quantization offers a solution, yet performance degradation in the sub-1-bit regime remains particularly difficult. This paper introduces LittleBit, a novel method for extreme LLM compression. It targets levels like 0.1 bits per weight (BPW), achieving nearly 31$\times$ memory reduction, e.g., Llama2-13B to under 0.9 GB. LittleBit represents weights in a low-rank form using latent matrix factorization, subsequently binarizing these factors. To counteract information loss from this extreme precision, it integrates a multi-scale compensation mechanism. This includes row, column, and an additional latent dimension that learns per-rank importance. Two key contributions enable effective training: Dual Sign-Value-Independent Decomposition (Dual-SVID) for quantization-aware training (QAT) initialization, and integrated Residual Compensation to mitigate errors. Extensive experiments confirm LittleBit's superiority in sub-1-bit quantization: e.g., its 0.1 BPW performance on Llama2-7B surpasses the leading method's 0.7 BPW. LittleBit establishes a new, viable size-performance trade-off--unlocking a potential 11.6$\times$ speedup over FP16 at the kernel level--and makes powerful LLMs practical for resource-constrained environments. Our code can be found at https://github.com/SamsungLabs/LittleBit.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks</title>
<link>https://arxiv.org/abs/2506.16402</link>
<guid>https://arxiv.org/abs/2506.16402</guid>
<content:encoded><![CDATA[
arXiv:2506.16402v3 Announce Type: replace-cross 
Abstract: Flawed planning from VLM-driven embodied agents poses significant safety hazards, hindering their deployment in real-world household tasks. However, existing static, non-interactive evaluation paradigms fail to adequately assess risks within these interactive environments, since they cannot simulate dynamic risks that emerge from an agent's actions and rely on unreliable post-hoc evaluations that ignore unsafe intermediate steps. To bridge this critical gap, we propose evaluating an agent's interactive safety: its ability to perceive emergent risks and execute mitigation steps in the correct procedural order. We thus present IS-Bench, the first multi-modal benchmark designed for interactive safety, featuring 161 challenging scenarios with 388 unique safety risks instantiated in a high-fidelity simulator. Crucially, it facilitates a novel process-oriented evaluation that verifies whether risk mitigation actions are performed before/after specific risk-prone steps. Extensive experiments on leading VLMs, including the GPT-4o and Gemini-2.5 series, reveal that current agents lack interactive safety awareness, and that while safety-aware Chain-of-Thought can improve performance, it often compromises task completion. By highlighting these critical limitations, IS-Bench provides a foundation for developing safer and more reliable embodied AI systems. Code and data are released under https://github.com/AI45Lab/IS-Bench.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REINA: Regularized Entropy Information-Based Loss for Efficient Simultaneous Speech Translation</title>
<link>https://arxiv.org/abs/2508.04946</link>
<guid>https://arxiv.org/abs/2508.04946</guid>
<content:encoded><![CDATA[
arXiv:2508.04946v3 Announce Type: replace-cross 
Abstract: Simultaneous Speech Translation (SimulST) systems stream in audio while simultaneously emitting translated text or speech. Such systems face the significant challenge of balancing translation quality and latency. We introduce a strategy to optimize this tradeoff: wait for more input only if you gain information by doing so. Based on this strategy, we present Regularized Entropy INformation Adaptation (REINA), a novel loss to train an adaptive policy using an existing non-streaming translation model. We derive REINA from information theory principles and show that REINA helps push the reported Pareto frontier of the latency/quality tradeoff over prior works. Utilizing REINA, we train a SimulST model on French, Spanish and German, both from and into English. Training on only open source or synthetically generated data, we achieve state-of-the-art (SOTA) streaming results for models of comparable size. We also introduce a metric for streaming efficiency, quantitatively showing REINA improves the latency/quality trade-off by as much as 21% compared to prior approaches, normalized against non-streaming baseline BLEU scores.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ORFuzz: Fuzzing the "Other Side" of LLM Safety -- Testing Over-Refusal</title>
<link>https://arxiv.org/abs/2508.11222</link>
<guid>https://arxiv.org/abs/2508.11222</guid>
<content:encoded><![CDATA[
arXiv:2508.11222v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) increasingly exhibit over-refusal - erroneously rejecting benign queries due to overly conservative safety measures - a critical functional flaw that undermines their reliability and usability. Current methods for testing this behavior are demonstrably inadequate, suffering from flawed benchmarks and limited test generation capabilities, as highlighted by our empirical user study. To the best of our knowledge, this paper introduces the first evolutionary testing framework, ORFuzz, for the systematic detection and analysis of LLM over-refusals. ORFuzz uniquely integrates three core components: (1) safety category-aware seed selection for comprehensive test coverage, (2) adaptive mutator optimization using reasoning LLMs to generate effective test cases, and (3) OR-Judge, a human-aligned judge model validated to accurately reflect user perception of toxicity and refusal. Our extensive evaluations demonstrate that ORFuzz generates diverse, validated over-refusal instances at a rate (6.98% average) more than double that of leading baselines, effectively uncovering vulnerabilities. Furthermore, ORFuzz's outputs form the basis of ORFuzzSet, a new benchmark of 1,855 highly transferable test cases that achieves a superior 63.56% average over-refusal rate across 10 diverse LLMs, significantly outperforming existing datasets. ORFuzz and ORFuzzSet provide a robust automated testing framework and a valuable community resource, paving the way for developing more reliable and trustworthy LLM-based software systems.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse but Wrong: Incorrect L0 Leads to Incorrect Features in Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2508.16560</link>
<guid>https://arxiv.org/abs/2508.16560</guid>
<content:encoded><![CDATA[
arXiv:2508.16560v3 Announce Type: replace-cross 
Abstract: Sparse Autoencoders (SAEs) extract features from LLM internal activations, meant to correspond to interpretable concepts. A core SAE training hyperparameter is L0: how many SAE features should fire per token on average. Existing work compares SAE algorithms using sparsity-reconstruction tradeoff plots, implying L0 is a free parameter with no single correct value aside from its effect on reconstruction. In this work we study the effect of L0 on SAEs, and show that if L0 is not set correctly, the SAE fails to disentangle the underlying features of the LLM. If L0 is too low, the SAE will mix correlated features to improve reconstruction. If L0 is too high, the SAE finds degenerate solutions that also mix features. Further, we present a proxy metric that can help guide the search for the correct L0 for an SAE on a given training distribution. We show that our method finds the correct L0 in toy models and coincides with peak sparse probing performance in LLM SAEs. We find that most commonly used SAEs have an L0 that is too low. Our work shows that L0 must be set correctly to train SAEs with correct features.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforce-Ada: An Adaptive Sampling Framework under Non-linear RL Objectives</title>
<link>https://arxiv.org/abs/2510.04996</link>
<guid>https://arxiv.org/abs/2510.04996</guid>
<content:encoded><![CDATA[
arXiv:2510.04996v3 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) for large language model reasoning is frequently hindered by signal loss, a phenomenon where standard uniform sampling with small group sizes fails to uncover informative learning signals for difficult prompts. We demonstrate that this collapse is a statistical artifact of undersampling rather than an inherent model limitation. To address this systematically, we introduce a theoretical framework based on optimizing a non-linear RL objective (e.g., log-likelihood). We show that this objective naturally induces a weighted gradient estimator that prioritizes difficult prompts, which can be robustly realized through adaptive sampling. Guided by this framework, we propose Reinforce-Ada, a family of algorithms that dynamically allocates inference budgets based on prompt difficulty, effectively scaling up RL compute to where it is needed most. Unlike passive filtering methods that discard low-signal prompts, Reinforce-Ada actively invests compute to recover them. We introduce two efficient realizations: an estimation-based approach and a model-free sequential sampling approach. Extensive experiments across multiple benchmarks show that Reinforce-Ada significantly outperforms uniform baselines like GRPO, recovering lost signals and accelerating convergence by up to $2\times$ while maintaining the same total inference budget. Code is available at https://github.com/RLHFlow/Reinforce-Ada.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Foundation Models on a Full-Stack AMD Platform: Compute, Networking, and System Design</title>
<link>https://arxiv.org/abs/2511.17127</link>
<guid>https://arxiv.org/abs/2511.17127</guid>
<content:encoded><![CDATA[
<div> MoE, AMD MI300X, Pollara networking, transformer sizing, large-scale pretraining<br /><br />Summary:<br /><br />This paper presents the first extensive large-scale mixture-of-experts (MoE) pretraining conducted exclusively on AMD hardware, specifically utilizing MI300X GPUs combined with Pollara networking. The authors provide practical system and model design guidance based on their experiments. On the systems front, they deliver a detailed cluster and network characterization including microbenchmarking of core collective communication operations (all-reduce, reduce-scatter, all-gather, broadcast) across different message sizes and GPU counts over Pollara, achieving a scale not previously reported. Additionally, they analyze MI300X performance focusing on kernel sizing and memory bandwidth to inform architecture choices. On the modeling side, new MI300X-aware transformer sizing rules are introduced for attention and MLP blocks, along with optimally chosen MoE widths that balance training throughput with inference latency. The training stack description highlights fault-tolerance capabilities, checkpoint reshaping, and a detailed training recipe. The study also previews ZAYA1, a base MoE model with 8.3 billion total parameters (760 million active), released openly on Hugging Face, which matches or exceeds the performance of comparable base models such as Qwen3-4B, Gemma3-12B, and surpasses Llama-3-8B and OLMoE on reasoning, mathematics, and coding tasks. Overall, this work demonstrates the readiness and competitiveness of AMD’s hardware and software ecosystem for large-scale AI pretraining. <div>
arXiv:2511.17127v2 Announce Type: replace 
Abstract: We report on the first large-scale mixture-of-experts (MoE) pretraining study on pure AMD hardware, utilizing both MI300X GPUs and Pollara networking. We distill practical guidance for both systems and model design. On the systems side, we deliver a comprehensive cluster and networking characterization: microbenchmarks for all core collectives (all-reduce, reduce-scatter, all-gather, broadcast) across message sizes and GPU counts over Pollara. To our knowledge, this is the first at this scale. We further provide MI300X microbenchmarks on kernel sizing and memory bandwidth to inform model design. On the modeling side, we introduce and apply MI300X-aware transformer sizing rules for attention and MLP blocks and justify MoE widths that jointly optimize training throughput and inference latency. We describe our training stack in depth, including often-ignored utilities such as fault-tolerance and checkpoint-reshaping, as well as detailed information on our training recipe. We also provide a preview of our model architecture and base model - ZAYA1 (760M active, 8.3B total parameters MoE, available at https://huggingface.co/Zyphra/ZAYA1-base) - which will be further improved upon in forthcoming papers. ZAYA1-base achieves performance comparable to leading base models such as Qwen3-4B and Gemma3-12B at its scale and larger, and outperforms models including Llama-3-8B and OLMoE across reasoning, mathematics, and coding benchmarks. Together, these results demonstrate that the AMD hardware, network, and software stack are mature and optimized enough for competitive large-scale pretraining.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PUCP-Metrix: An Open-source and Comprehensive Toolkit for Linguistic Analysis of Spanish Texts</title>
<link>https://arxiv.org/abs/2511.17402</link>
<guid>https://arxiv.org/abs/2511.17402</guid>
<content:encoded><![CDATA[
<div> Keywords: Spanish linguistic analysis, PUCP-Metrix, readability assessment, machine-generated text detection, NLP toolkit<br /><br />Summary:<br /><br />PUCP-Metrix is an open-source and comprehensive toolkit designed specifically for the linguistic analysis of Spanish texts. It offers 182 distinct linguistic features covering a wide range of dimensions including lexical diversity, syntactic and semantic complexity, cohesion, psycholinguistics, and readability metrics, enabling fine-grained and interpretable analysis. This extensive coverage addresses the gap left by existing Spanish tools which have limited functionality and scope. PUCP-Metrix is evaluated on two important NLP tasks: Automated Readability Assessment and Machine-Generated Text Detection. In both tasks, the toolkit demonstrates competitive performance, rivaling existing repositories and strong neural baseline models. The toolkit is not only comprehensive but also extensible, supporting varied applications in natural language processing involving Spanish. Overall, PUCP-Metrix provides an essential resource for researchers and practitioners who require detailed, interpretable linguistic features to improve readability, style, and structure understanding in Spanish-language texts. <div>
arXiv:2511.17402v2 Announce Type: replace 
Abstract: Linguistic features remain essential for interpretability and tasks that involve style, structure, and readability, but existing Spanish tools offer limited coverage. We present PUCP-Metrix, an open-source and comprehensive toolkit for linguistic analysis of Spanish texts. PUCP-Metrix includes 182 linguistic metrics spanning lexical diversity, syntactic and semantic complexity, cohesion, psycholinguistics, and readability. It enables fine-grained, interpretable text analysis. We evaluate its usefulness on Automated Readability Assessment and Machine-Generated Text Detection, showing competitive performance compared to an existing repository and strong neural baselines. PUCP-Metrix offers a comprehensive and extensible resource for Spanish, supporting diverse NLP applications.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models</title>
<link>https://arxiv.org/abs/2506.09532</link>
<guid>https://arxiv.org/abs/2506.09532</guid>
<content:encoded><![CDATA[
<div> Keywords: Athena-PRM, process reward model, reasoning steps evaluation, prediction consistency, reward ranked fine-tuning<br /><br />Summary:<br /><br />1. Athena-PRM is introduced as a multimodal process reward model designed to assign reward scores to each step in solving complex reasoning tasks. 2. Traditional high-performance process reward models require extensive step-level annotations, which are time-consuming and costly, while existing automated labeling methods such as Monte Carlo estimation are noisy and expensive computationally. 3. To address data quality and efficiency, the authors propose using prediction consistency between weak and strong completers as a method to identify reliable process labels, enabling the generation of high-quality process-labeled data with only 5,000 samples. 4. Two additional strategies, ORM initialization and up-sampling of negative data, are developed to further enhance PRM performance. 5. The approach is validated in three scenarios: test time scaling verification, direct reasoning step correctness evaluation, and reward ranked fine-tuning, with Athena-PRM consistently outperforming existing methods. 6. Using Qwen2.5-VL-7B as the policy model, Athena-PRM improves benchmark scores by 10.2 points on WeMath and 7.1 on MathVista during test time scaling. 7. Athena-PRM achieves state-of-the-art results on VisualProcessBench, exceeding previous scores by 3.9 F1, demonstrating its accuracy in assessing reasoning step correctness. 8. Finally, by employing Athena-PRM as the reward model for reward ranked fine-tuning, Athena-7B significantly outperforms baselines across five benchmarks, highlighting its robust capability in complex reasoning evaluation. <div>
arXiv:2506.09532v4 Announce Type: replace-cross 
Abstract: We present Athena-PRM, a multimodal process reward model (PRM) designed to evaluate the reward score for each step in solving complex reasoning problems. Developing high-performance PRMs typically demands significant time and financial investment, primarily due to the necessity for step-level annotations of reasoning steps. Conventional automated labeling methods, such as Monte Carlo estimation, often produce noisy labels and incur substantial computational costs. To efficiently generate high-quality process-labeled data, we propose leveraging prediction consistency between weak and strong completers as a criterion for identifying reliable process labels. Remarkably, Athena-PRM demonstrates outstanding effectiveness across various scenarios and benchmarks with just 5,000 samples. Furthermore, we also develop two effective strategies to improve the performance of PRMs: ORM initialization and up-sampling for negative data. We validate our approach in three specific scenarios: verification for test time scaling, direct evaluation of reasoning step correctness, and reward ranked fine-tuning. Our Athena-PRM consistently achieves superior performance across multiple benchmarks and scenarios. Notably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances performance by 10.2 points on WeMath and 7.1 points on MathVista for test time scaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in VisualProcessBench and outperforms the previous SoTA by 3.9 F1-score, showcasing its robust capability to accurately assess the correctness of the reasoning step. Additionally, utilizing Athena-PRM as the reward model, we develop Athena-7B with reward ranked fine-tuning and outperforms baseline with a significant margin on five benchmarks.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral</title>
<link>https://arxiv.org/abs/2512.04220</link>
<guid>https://arxiv.org/abs/2512.04220</guid>
<content:encoded><![CDATA[
<div> Keywords: Tool-integrated reinforcement learning, Group Relative Policy Optimization, Lazy Likelihood Displacement, likelihood-preserving regularization, multi-hop question answering<br /><br />Summary:<br /><br />1. The paper addresses the challenge of training large language models (LLMs) with tool-integrated reinforcement learning (TIRL), where models interact with external tools like search engines for multi-step reasoning.<br /><br />2. Group Relative Policy Optimization (GRPO), exemplified by Search-R1, is a strong approach offering fast convergence and a value-free framework but suffers from training collapse.<br /><br />3. The authors identify Lazy Likelihood Displacement (LLD), a phenomenon involving the systematic reduction or stagnation of both correct and incorrect response likelihoods, as the foundational cause of this collapse.<br /><br />4. LLD triggers a self-reinforcing "LLD Death Spiral" marked by low-confidence outputs, increased gradients, and eventual training collapse, progressing through three phases: early stagnation, steady decay, and accelerated collapse.<br /><br />5. To counter this, they propose a lightweight likelihood-preserving regularization method (LLDS) for GRPO that activates selectively when likelihoods decrease and applies fine-grained regularization only to responsible tokens.<br /><br />6. This approach mitigates LLD with minimal impact on optimization, stabilizes training, prevents gradient explosion, and delivers substantial performance improvements across seven QA benchmarks, including +37.8% on Qwen2.5-3B and +32.0% on Qwen2.5-7B.<br /><br />7. The work establishes LLD as a fundamental bottleneck in GRPO-based TIRL and offers a practical, scalable solution for stable training of tool-integrated LLMs. <div>
arXiv:2512.04220v1 Announce Type: new 
Abstract: Tool-integrated (TI) reinforcement learning (RL) enables large language models (LLMs) to perform multi-step reasoning by interacting with external tools such as search engines and retrievers. Group Relative Policy Optimization (GRPO), exemplified by the recent Search-R1, offers fast convergence and a value-free formulation that makes it appealing for this setting, yet consistently suffers from training collapse. We identify Lazy Likelihood Displacement (LLD), a systematic reduction or stagnation in the likelihood of both correct and incorrect responses, as the core mechanism driving this failure. LLD emerges early and triggers a self-reinforcing LLD Death Spiral, where declining likelihood leads to low-confidence responses, inflating gradients, and ultimately causing collapse. We empirically characterize this process across models on a Search-R1-style, search-integrated question answering task, revealing a consistent three-phase trajectory: early stagnation, steady decay, and accelerated collapse. To address this, we propose a lightweight likelihood-preserving regularization LLDS for GRPO that activates only when a trajectory's likelihood decreases, and regularizes only the tokens responsible. This fine-grained structure mitigates LLD with minimal interference to optimization. Across seven open-domain and multi-hop QA benchmarks, our method stabilizes training, prevents gradient explosion, and yields substantial performance improvements, including +37.8% gains on Qwen2.5-3B and +32.0% gains on Qwen2.5-7B. Our results establish LLD as a fundamental bottleneck in GRPO-based TIRL and provide a practical path toward stable, scalable training of tool-integrated LLM.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computational Linguistics Meets Libyan Dialect: A Study on Dialect Identification</title>
<link>https://arxiv.org/abs/2512.04257</link>
<guid>https://arxiv.org/abs/2512.04257</guid>
<content:encoded><![CDATA[
<div> Keywords: Libyan dialect, Arabic NLP, Multinomial Naive Bayes, n-gram features, dialect classification<br /><br />Summary:<br /><br />This study explores the classification of Libyan dialect utterances sourced from Twitter using four machine learning models: logistic regression, linear support vector machine (SVM), multinomial Naive Bayes (MNB), and Bernoulli Naive Bayes. The experiments utilize the QADI corpus, containing 540,000 sentences spanning 18 Arabic dialects, with special attention to the preprocessing challenges posed by inconsistent orthographic variations and non-standard spellings typical of the Libyan dialect. A chi-square feature analysis identified that certain meta-features such as email mentions and emotion indicators were not significantly linked to dialect classification and were therefore excluded. Two major experiments were conducted: the first assessed the value of meta-features using chi-square tests, and the second examined classifier performance with various word and character n-gram representations. Results demonstrated that MNB outperformed other models, achieving the highest accuracy (85.89%) and F1-score (0.85741) when using a combination of (1,2) word n-grams and (1,5) character n-grams. Logistic regression and linear SVM showed competitive but slightly lower accuracies of 84.41% and 84.73%, respectively. Additional evaluation metrics including log loss, Cohen’s kappa, and Matthew correlation coefficient further validated MNB’s superior performance. This work highlights the importance of selecting appropriate n-gram features and classification algorithms for effective Arabic dialect identification, providing valuable benchmarks for future research in Arabic dialect NLP. <div>
arXiv:2512.04257v1 Announce Type: new 
Abstract: This study investigates logistic regression, linear support vector machine, multinomial Naive Bayes, and Bernoulli Naive Bayes for classifying Libyan dialect utterances gathered from Twitter. The dataset used is the QADI corpus, which consists of 540,000 sentences across 18 Arabic dialects. Preprocessing challenges include handling inconsistent orthographic variations and non-standard spellings typical of the Libyan dialect. The chi-square analysis revealed that certain features, such as email mentions and emotion indicators, were not significantly associated with dialect classification and were thus excluded from further analysis. Two main experiments were conducted: (1) evaluating the significance of meta-features extracted from the corpus using the chi-square test and (2) assessing classifier performance using different word and character n-gram representations. The classification experiments showed that Multinomial Naive Bayes (MNB) achieved the highest accuracy of 85.89% and an F1-score of 0.85741 when using a (1,2) word n-gram and (1,5) character n-gram representation. In contrast, Logistic Regression and Linear SVM exhibited slightly lower performance, with maximum accuracies of 84.41% and 84.73%, respectively. Additional evaluation metrics, including log loss, Cohen kappa, and Matthew correlation coefficient, further supported the effectiveness of MNB in this task. The results indicate that carefully selected n-gram representations and classification models play a crucial role in improving the accuracy of Libyan dialect identification. This study provides empirical benchmarks and insights for future research in Arabic dialect NLP applications.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SQuARE: Structured Query &amp; Adaptive Retrieval Engine For Tabular Formats</title>
<link>https://arxiv.org/abs/2512.04292</link>
<guid>https://arxiv.org/abs/2512.04292</guid>
<content:encoded><![CDATA[
<div> Keywords: spreadsheet question answering, multirow headers, hybrid retrieval, SQL relational representation, table understanding<br /><br />Summary:<br /><br />1. The paper addresses the challenge of accurate question answering over real spreadsheets, which is complicated by features such as multirow headers, merged cells, and unit annotations that disrupt simple chunking methods.<br /><br />2. Traditional rigid SQL views are insufficient for many spreadsheets due to inconsistent schemas, motivating a more flexible approach.<br /><br />3. The authors propose SQuARE, a hybrid retrieval framework that employs sheet-level complexity-aware routing. It calculates a continuous score based on header depth and merge density to decide the optimal retrieval path.<br /><br />4. Depending on the computed score, queries are routed through either structure-preserving chunk retrieval or SQL queries on an automatically created relational representation.<br /><br />5. A lightweight supervising agent manages retrieval, refinement, or combination of results from both paths when confidence is low to enhance accuracy.<br /><br />6. SQuARE preserves important spreadsheet features such as header hierarchies, time labels, and units, ensuring the returned answers align closely with original cell values and are easy to verify.<br /><br />7. Extensive evaluation on multi-header corporate balance sheets, a heavily merged World Bank workbook, and various public datasets shows that SQuARE consistently outperforms both single-strategy baselines and ChatGPT-4o in retrieval precision and answer accuracy.<br /><br />8. The system maintains predictable latency and separates retrieval from model choice, enabling compatibility with emerging tabular foundation models and improving robustness in table understanding tasks. <div>
arXiv:2512.04292v1 Announce Type: new 
Abstract: Accurate question answering over real spreadsheets remains difficult due to multirow headers, merged cells, and unit annotations that disrupt naive chunking, while rigid SQL views fail on files lacking consistent schemas. We present SQuARE, a hybrid retrieval framework with sheet-level, complexity-aware routing. It computes a continuous score based on header depth and merge density, then routes queries either through structure-preserving chunk retrieval or SQL over an automatically constructed relational representation. A lightweight agent supervises retrieval, refinement, or combination of results across both paths when confidence is low. This design maintains header hierarchies, time labels, and units, ensuring that returned values are faithful to the original cells and straightforward to verify. Evaluated on multi-header corporate balance sheets, a heavily merged World Bank workbook, and diverse public datasets, SQuARE consistently surpasses single-strategy baselines and ChatGPT-4o on both retrieval precision and end-to-end answer accuracy while keeping latency predictable. By decoupling retrieval from model choice, the system is compatible with emerging tabular foundation models and offers a practical bridge toward a more robust table understanding.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle</title>
<link>https://arxiv.org/abs/2512.04324</link>
<guid>https://arxiv.org/abs/2512.04324</guid>
<content:encoded><![CDATA[
<div> Keywords: Data engineering, Data analysis, Autonomous agents, SQL pipelines, Benchmark

<br /><br />Summary:  
This paper introduces DAComp, a comprehensive benchmark consisting of 210 tasks designed to emulate real-world enterprise data intelligence workflows, combining both data engineering (DE) and data analysis (DA). The DE tasks challenge systems to perform complex repository-level engineering over industrial-scale schemas, requiring the design, implementation, and evolution of multi-stage SQL pipelines. Conversely, the DA tasks focus on open-ended business problems that demand strategic planning, iterative exploratory coding, interpretation of intermediate results, and formulation of actionable insights. Evaluation of the DE tasks involves execution-based, multi-metric scoring, while DA tasks are assessed via a rigorously developed, hierarchical rubric-driven LLM-judge validated through experiments. Results show that leading autonomous agents struggle significantly on DAComp, especially in DE tasks, where success rates are under 20%, highlighting a critical bottleneck in managing integrated data pipelines beyond mere code generation. DA tasks also score below 40% on average, revealing substantial challenges in open-ended analytical reasoning, underscoring that engineering and analysis are distinct skills. By clearly identifying these gaps, DAComp serves as a realistic and rigorous testbed aimed at advancing the development of autonomous data agents capable of handling holistic data workflows in enterprise contexts. The dataset and codebase are openly available at the provided URL. <div>
arXiv:2512.04324v1 Announce Type: new 
Abstract: Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ClusterFusion: Hybrid Clustering with Embedding Guidance and LLM Adaptation</title>
<link>https://arxiv.org/abs/2512.04350</link>
<guid>https://arxiv.org/abs/2512.04350</guid>
<content:encoded><![CDATA[
<div> Text clustering, large language models, domain-specific, embedding-guided, ClusterFusion<br /><br />Summary:  
This paper addresses challenges in text clustering, particularly in domain-specific contexts where traditional clustering with pre-trained embeddings underperforms without expensive fine-tuning. The authors propose ClusterFusion, a novel hybrid framework that positions large language models (LLMs) as the central clustering mechanism rather than auxiliary tools. The approach integrates lightweight embedding methods in a three-stage process: first, embedding-guided subset partitioning organizes data subsets; second, LLM-driven topic summarization creates coherent cluster themes; third, LLM-based topic assignment labels documents according to these themes. This architecture allows for direct incorporation of domain knowledge and user preferences, leveraging LLMs’ contextual reasoning capabilities more effectively. The system is evaluated on three public benchmark datasets and two newly constructed domain-specific datasets, achieving state-of-the-art results on standard tasks and demonstrating significant improvements in specialized domains. To promote further research, the authors release their new dataset and results on all benchmarks. Overall, ClusterFusion advances text clustering by exploiting LLMs beyond refinement roles, providing a flexible, high-performance solution for both general and specialized NLP applications. <div>
arXiv:2512.04350v1 Announce Type: new 
Abstract: Text clustering is a fundamental task in natural language processing, yet traditional clustering algorithms with pre-trained embeddings often struggle in domain-specific contexts without costly fine-tuning. Large language models (LLMs) provide strong contextual reasoning, yet prior work mainly uses them as auxiliary modules to refine embeddings or adjust cluster boundaries. We propose ClusterFusion, a hybrid framework that instead treats the LLM as the clustering core, guided by lightweight embedding methods. The framework proceeds in three stages: embedding-guided subset partition, LLM-driven topic summarization, and LLM-based topic assignment. This design enables direct incorporation of domain knowledge and user preferences, fully leveraging the contextual adaptability of LLMs. Experiments on three public benchmarks and two new domain-specific datasets demonstrate that ClusterFusion not only achieves state-of-the-art performance on standard tasks but also delivers substantial gains in specialized domains. To support future work, we release our newly constructed dataset and results on all benchmarks.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LangSAT: A Novel Framework Combining NLP and Reinforcement Learning for SAT Solving</title>
<link>https://arxiv.org/abs/2512.04374</link>
<guid>https://arxiv.org/abs/2512.04374</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, SAT solving, natural language processing, CDCL, CNF conversion<br /><br />Summary:<br /><br />1. This paper introduces LangSAT, a novel framework that integrates reinforcement learning (RL) to optimize heuristic selection during the conflict-driven clause learning (CDCL) process in SAT solving.<br /><br />2. LangSAT uniquely bridges natural language and propositional logic by allowing English descriptions as input, which are converted into Conjunctive Normal Form (CNF) expressions, overcoming the limitation of traditional SAT solvers that require CNF inputs.<br /><br />3. The framework consists of two main components: Lang2Logic, which translates English sentences into CNF formulas, and SmartSAT, an RL-enhanced SAT solver.<br /><br />4. SmartSAT encodes the relationship between clauses and variables using structured graph representations and extracts global features, providing the RL agent with richer contextual information for improved solving efficiency.<br /><br />5. Evaluations show Lang2Logic effectively processes natural language inputs up to 450 words, and SmartSAT achieves solving times comparable to traditional CDCL heuristics, making LangSAT a more accessible and scalable SAT-solving solution applicable to reasoning, formal verification, and debugging tasks. <div>
arXiv:2512.04374v1 Announce Type: new 
Abstract: Our work presents a novel reinforcement learning (RL) based framework to optimize heuristic selection within the conflict-driven clause learning (CDCL) process, improving the efficiency of Boolean satisfia- bility (SAT) solving. The proposed system, LangSAT, bridges the gap between natural language inputs and propositional logic by converting English descriptions into Conjunctive Normal Form (CNF) expressions and solving them using an RL-enhanced CDCL SAT solver. Unlike existing SAT-solving platforms that require CNF as input, LangSAT enables users to input standard English descriptions, making SAT-solving more accessible. The framework comprises two key components: Lang2Logic, which translates English sentences into CNF expressions, and SmartSAT, an RL-based SAT solver. SmartSAT encodes clause-variable relationships as structured graph representations and extracts global features specific to the SAT problem. This implementation provides the RL agent with deeper contextual information, enabling SAT problems to be solved more efficiently. Lang2Logic was evaluated on diverse natural language inputs, processing descriptions up to 450 words. The generated CNFs were solved by SmartSAT, which demonstrated comparable performance to traditional CDCL heuristics with respect to solving time. The combined LangSAT framework offers a more accessible and scalable solution for SAT-solving tasks across reasoning, formal verification, and debugging.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MASE: Interpretable NLP Models via Model-Agnostic Saliency Estimation</title>
<link>https://arxiv.org/abs/2512.04386</link>
<guid>https://arxiv.org/abs/2512.04386</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep Neural Networks, Interpretability, Model-agnostic Saliency Estimation, Normalized Linear Gaussian Perturbations, Natural Language Processing<br /><br />Summary:<br /><br />This paper addresses the challenge of interpreting deep neural networks (DNNs) in natural language processing (NLP), where understanding model decisions remains difficult due to the discrete nature of text data. Traditional interpretation techniques such as saliency maps and feature visualization often fall short when applied to NLP models. To overcome these limitations, the authors propose the Model-agnostic Saliency Estimation (MASE) framework, which provides local explanations of text-based predictive models without requiring detailed knowledge of their internal architectures. MASE innovates by applying Normalized Linear Gaussian Perturbations (NLGP) to the embedding layer rather than the raw word inputs, which enhances efficiency and accuracy in estimating input saliency. Experimental results demonstrate that MASE outperforms existing model-agnostic interpretability methods, particularly in terms of Delta Accuracy, a metric for measuring the impact of identified salient features on model predictions. The framework offers a promising approach for researchers and practitioners seeking transparent insights into complex NLP models, potentially aiding in debugging, trust-building, and compliance with interpretability requirements. Overall, MASE represents a significant advancement in the field of explainable NLP by bridging the gap between model performance and interpretability. <div>
arXiv:2512.04386v1 Announce Type: new 
Abstract: Deep neural networks (DNNs) have made significant strides in Natural Language Processing (NLP), yet their interpretability remains elusive, particularly when evaluating their intricate decision-making processes. Traditional methods often rely on post-hoc interpretations, such as saliency maps or feature visualization, which might not be directly applicable to the discrete nature of word data in NLP. Addressing this, we introduce the Model-agnostic Saliency Estimation (MASE) framework. MASE offers local explanations for text-based predictive models without necessitating in-depth knowledge of a model's internal architecture. By leveraging Normalized Linear Gaussian Perturbations (NLGP) on the embedding layer instead of raw word inputs, MASE efficiently estimates input saliency. Our results indicate MASE's superiority over other model-agnostic interpretation methods, especially in terms of Delta Accuracy, positioning it as a promising tool for elucidating the operations of text-based models in NLP.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sarcasm Detection on Reddit Using Classical Machine Learning and Feature Engineering</title>
<link>https://arxiv.org/abs/2512.04396</link>
<guid>https://arxiv.org/abs/2512.04396</guid>
<content:encoded><![CDATA[
<div> sarcasm detection, classical machine learning, feature engineering, TF-IDF, Reddit SARC 2.0<br /><br />Summary:<br /><br />1. The paper investigates sarcasm detection in online comments using classical machine learning methods without the use of neural networks or contextual information from parent comments.  
2. The study utilizes a subset of 100,000 comments from the Self-Annotated Reddit Corpus (SARC 2.0) to build and evaluate models.  
3. Feature engineering involves combining word-level and character-level TF-IDF vectors with simple stylistic indicators to represent the text data.  
4. Four classical models are tested: logistic regression, linear SVM, multinomial Naive Bayes, and random forest.  
5. Among these, logistic regression and multinomial Naive Bayes perform best, achieving F1-scores of approximately 0.57 in identifying sarcastic comments.  
6. The research highlights that the absence of conversational context restricts overall detection performance, yet the models provide a reproducible, transparent, and computationally lightweight baseline for sarcasm detection tasks.  
7. The approach emphasizes interpretability and simplicity, offering a useful benchmark for future work involving sarcasm recognition in text without heavy reliance on deep learning or external context. <div>
arXiv:2512.04396v1 Announce Type: new 
Abstract: Sarcasm is common in online discussions, yet difficult for machines to identify because the intended meaning often contradicts the literal wording. In this work, I study sarcasm detection using only classical machine learning methods and explicit feature engineering, without relying on neural networks or context from parent comments. Using a 100,000-comment subsample of the Self-Annotated Reddit Corpus (SARC 2.0), I combine word-level and character-level TF-IDF features with simple stylistic indicators. Four models are evaluated: logistic regression, a linear SVM, multinomial Naive Bayes, and a random forest. Naive Bayes and logistic regression perform the strongest, achieving F1-scores around 0.57 for sarcastic comments. Although the lack of conversational context limits performance, the results offer a clear and reproducible baseline for sarcasm detection using lightweight and interpretable methods.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RapidUn: Influence-Driven Parameter Reweighting for Efficient Large Language Model Unlearning</title>
<link>https://arxiv.org/abs/2512.04457</link>
<guid>https://arxiv.org/abs/2512.04457</guid>
<content:encoded><![CDATA[
<div> unlearning, large language models, influence estimation, parameter efficiency, adaptive updates<br /><br />Summary:<br /><br />1. Removing specific data influence from large language models (LLMs) is difficult due to the high cost of retraining and instability in current approximate unlearning methods. 2. The problem becomes more pronounced when the dataset to be forgotten is small or imbalanced. 3. RapidUn is proposed as a novel unlearning framework that is both influence-driven and parameter-efficient, enabling selective forgetting without full retraining. 4. The method involves a fast estimation module that calculates per-sample influence, which is then converted into adaptive update weights to selectively adjust model parameters. 5. This selective updating helps RapidUn forget harmful behaviors while preserving the model’s general knowledge. 6. Experiments on Mistral-7B and Llama-3-8B using Dolly-15k and Alpaca-57k datasets demonstrate that RapidUn achieves up to 100 times greater efficiency compared to full retraining. 7. RapidUn consistently outperforms existing methods such as Fisher, GA, and LoReUn on both in-distribution and out-of-distribution forgetting tasks. 8. Overall, influence-guided parameter reweighting is established as a scalable, interpretable, and effective approach for unlearning in LLMs. <div>
arXiv:2512.04457v1 Announce Type: new 
Abstract: Removing specific data influence from large language models (LLMs) remains challenging, as retraining is costly and existing approximate unlearning methods are often unstable. The challenge is exacerbated when the forget set is small or imbalanced. We introduce RapidUn, an influence-driven and parameter-efficient unlearning framework. It first estimates per-sample influence through a fast estimation module, then maps these scores into adaptive update weights that guide selective parameter updates -- forgetting harmful behavior while retaining general knowledge. On Mistral-7B and Llama-3-8B across Dolly-15k and Alpaca-57k, RapidUn achieves up to 100 times higher efficiency than full retraining and consistently outperforms Fisher, GA, and LoReUn on both in-distribution and out-of-distribution forgetting. These results establish influence-guided parameter reweighting as a scalable and interpretable paradigm for LLM unlearning.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MSME: A Multi-Stage Multi-Expert Framework for Zero-Shot Stance Detection</title>
<link>https://arxiv.org/abs/2512.04492</link>
<guid>https://arxiv.org/abs/2512.04492</guid>
<content:encoded><![CDATA[
<div> Keywords: zero-shot stance detection, multi-stage framework, expert reasoning, rhetorical devices, background knowledge<br /><br />Summary:<br /><br />1. Large language model (LLM)-based methods have recently shown strong zero-shot stance detection capabilities but fall short in complex real-world contexts where stance requires nuanced understanding.<br /><br />2. Challenges include the need for dynamic background knowledge, handling compound or event-based targets tied explicitly to stance labels, and detecting rhetorical devices like irony that mask true intent.<br /><br />3. To tackle these, the authors propose MSME, a Multi-Stage, Multi-Expert framework composed of three main stages: Knowledge Preparation, Expert Reasoning, and Decision Aggregation.<br /><br />4. In Knowledge Preparation, relevant background knowledge is retrieved and stance labels are clarified to set a solid foundation for downstream processing.<br /><br />5. Expert Reasoning consists of three specialized experts: the Knowledge Expert extracts key facts and reasons; the Label Expert refines stance labels and their justifications; the Pragmatic Expert identifies rhetorical cues such as irony to infer the author’s pragmatic intent.<br /><br />6. In the final stage, Decision Aggregation, a Meta-Judge module synthesizes the insights from all experts to produce the final stance prediction.<br /><br />7. Experimental results on three public datasets demonstrate that MSME noticeably outperforms previous state-of-the-art zero-shot stance detection methods across all benchmarks. <div>
arXiv:2512.04492v1 Announce Type: new 
Abstract: LLM-based approaches have recently achieved impressive results in zero-shot stance detection. However, they still struggle in complex real-world scenarios, where stance understanding requires dynamic background knowledge, target definitions involve compound entities or events that must be explicitly linked to stance labels, and rhetorical devices such as irony often obscure the author's actual intent. To address these challenges, we propose MSME, a Multi-Stage, Multi-Expert framework for zero-shot stance detection. MSME consists of three stages: (1) Knowledge Preparation, where relevant background knowledge is retrieved and stance labels are clarified; (2) Expert Reasoning, involving three specialized modules-Knowledge Expert distills salient facts and reasons from a knowledge perspective, Label Expert refines stance labels and reasons accordingly, and Pragmatic Expert detects rhetorical cues such as irony to infer intent from a pragmatic angle; (3) Decision Aggregation, where a Meta-Judge integrates all expert analyses to produce the final stance prediction. Experiments on three public datasets show that MSME achieves state-of-the-art performance across the board.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UW-BioNLP at ChemoTimelines 2025: Thinking, Fine-Tuning, and Dictionary-Enhanced LLM Systems for Chemotherapy Timeline Extraction</title>
<link>https://arxiv.org/abs/2512.04518</link>
<guid>https://arxiv.org/abs/2512.04518</guid>
<content:encoded><![CDATA[
<div> ChemoTimelines, chemotherapy timeline extraction, LLM, supervised fine-tuning, electronic health records<br /><br />Summary:<br /><br />This paper addresses the ChemoTimelines shared task, which aims to benchmark methods for constructing timelines of systemic anticancer treatments from the electronic health records (EHRs) of cancer patients. The focus is on subtask 2, which involves generating detailed chemotherapy timelines from raw clinical notes. The authors evaluated several strategies to improve timeline extraction, including chain-of-thought prompting, supervised fine-tuning, direct preference optimization, and dictionary-based lookup techniques. The overall approach followed a two-step workflow: first, a large language model (LLM) extracted chemotherapy events from individual clinical notes, and second, a separate algorithm normalized and aggregated these events into coherent patient-level timelines. Different methods mainly varied in how the LLM was trained and utilized within this framework. Among the tested approaches, fine-tuning the Qwen3-14B model delivered the best official score of 0.678 on the leaderboard, indicating competitive performance. The results and analyses presented offer valuable insights for future efforts in chemotherapy timeline extraction as well as for designing similar clinical timeline construction tasks. This work contributes to advancing AI methodologies for processing complex, unstructured clinical data to facilitate oncology research and care planning. <div>
arXiv:2512.04518v1 Announce Type: new 
Abstract: The ChemoTimelines shared task benchmarks methods for constructing timelines of systemic anticancer treatment from electronic health records of cancer patients. This paper describes our methods, results, and findings for subtask 2 -- generating patient chemotherapy timelines from raw clinical notes. We evaluated strategies involving chain-of-thought thinking, supervised fine-tuning, direct preference optimization, and dictionary-based lookup to improve timeline extraction. All of our approaches followed a two-step workflow, wherein an LLM first extracted chemotherapy events from individual clinical notes, and then an algorithm normalized and aggregated events into patient-level timelines. Each specific method differed in how the associated LLM was utilized and trained. Multiple approaches yielded competitive performances on the test set leaderboard, with fine-tuned Qwen3-14B achieving the best official score of 0.678. Our results and analyses could provide useful insights for future attempts on this task as well as the design of similar tasks.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EvoEdit: Lifelong Free-Text Knowledge Editing through Latent Perturbation Augmentation and Knowledge-driven Parameter Fusion</title>
<link>https://arxiv.org/abs/2512.04545</link>
<guid>https://arxiv.org/abs/2512.04545</guid>
<content:encoded><![CDATA[
<div> Keywords: Lifelong Free-text Knowledge Editing, large language models, knowledge updating, continual editing, EvoEdit<br /><br />Summary: Adjusting outdated knowledge in large language models (LLMs) after deployment is challenging, motivating the development of knowledge editing to efficiently modify internal knowledge without full retraining. Current methods have two major limitations: reliance on structured triplets that do not align well with the free-text format of LLM pretraining, and limited support for sequential or lifelong knowledge updates. To overcome these, the authors propose a new task called Lifelong Free-text Knowledge Editing (LF-Edit), which allows models to incorporate natural language updates and supports continual editing over time. LF-Edit faces challenges in integrating new knowledge while preventing forgetting of previous knowledge. To facilitate research, the authors create a large-scale benchmark named Multi-Rank Lifelong Free-text Editing Benchmark (MRLF-Bench) with 16,835 free-text editing requests and a multi-level evaluation framework assessing memorization, understanding, constrained comprehension, and reasoning. They introduce EvoEdit, a novel method that improves knowledge injection through Latent Perturbation Augmentation and preserves prior information using Knowledge-driven Parameter Fusion. Experiments demonstrate that EvoEdit significantly outperforms existing knowledge editing methods on the LF-Edit task, marking a substantial advance in lifelong free-text knowledge editing for LLMs. <div>
arXiv:2512.04545v1 Announce Type: new 
Abstract: Adjusting the outdated knowledge of large language models (LLMs) after deployment remains a major challenge. This difficulty has spurred the development of knowledge editing, which seeks to accurately and efficiently modify a model's internal (parametric) knowledge without retraining it from scratch. However, existing methods suffer from two limitations. First, they depend on structured triplets that are misaligned with the free-text nature of LLM pretraining and fail to capture the nuanced relationships among facts. Second, they typically support one-time knowledge updates, with relatively limited research on the problem of sequential or lifelong editing. To address these gaps, we propose a new task, Lifelong Free-text Knowledge Editing (LF-Edit), which enables models to incorporate updates expressed in natural language and supports continual editing over time. Despite its promise, LF-Edit faces the dual challenge of integrating new knowledge while mitigating the forgetting of prior information. To foster research on this new task, we construct a large-scale benchmark, Multi-Rank Lifelong Free-text Editing Benchmark (MRLF-Bench), containing 16,835 free-text edit requests. We further design a cognitively inspired multi-rank evaluation framework encompassing four levels: memorization, understanding, constrained comprehension, and reasoning. To tackle the challenges inherent in LF-Edit, we introduce a novel approach named EvoEdit that enhances knowledge injection through Latent Perturbation Augmentation and preserves prior information via Knowledge-driven Parameter Fusion. Experimental results demonstrate that EvoEdit substantially outperforms existing knowledge editing methods on the proposed LF-Edit task.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdmTree: Compressing Lengthy Context with Adaptive Semantic Trees</title>
<link>https://arxiv.org/abs/2512.04550</link>
<guid>https://arxiv.org/abs/2512.04550</guid>
<content:encoded><![CDATA[
<div> Keywords: self-attention, context compression, hierarchical abstraction, semantic fidelity, Large Language Models<br /><br />Summary: The paper addresses the quadratic complexity of self-attention in Large Language Models (LLMs), which limits their ability to process long context sequences necessary for advanced applications. It identifies the shortcomings of existing context compression methods: explicit approaches often lose local detail, while implicit ones may introduce positional biases, degrade information, or fail to capture long-range semantic dependencies. To overcome these challenges, the authors propose AdmTree, an adaptive, hierarchical context compression framework focused on preserving semantic fidelity efficiently. AdmTree dynamically segments input sequences based on information density, summarizing variable-length segments into gist tokens that form the leaves of a semantic binary tree. This hierarchical structure, combined with a lightweight aggregation mechanism and a frozen backbone LLM (minimizing new trainable parameters), supports efficient context abstraction. Importantly, AdmTree preserves fine-grained details alongside global semantic coherence, mitigates positional bias, and dynamically adapts compression according to content characteristics. Together, these features enable AdmTree to maintain the semantic integrity of long contexts while reducing computational complexity, making it a robust solution for enhancing the processing capabilities of LLMs with extended inputs. <div>
arXiv:2512.04550v1 Announce Type: new 
Abstract: The quadratic complexity of self-attention constrains Large Language Models (LLMs) in processing long contexts, a capability essential for many advanced applications. Context compression aims to alleviate this computational bottleneck while retaining critical semantic information. However, existing approaches often fall short: explicit methods may compromise local detail, whereas implicit methods can suffer from positional biases, information degradation, or an inability to capture long-range semantic dependencies. We propose AdmTree, a novel framework for adaptive, hierarchical context compression with a central focus on preserving high semantic fidelity while maintaining efficiency. AdmTree dynamically segments input based on information density, utilizing gist tokens to summarize variable-length segments as the leaves of a semantic binary tree. This structure, together with a lightweight aggregation mechanism and a frozen backbone LLM (thereby minimizing new trainable parameters), enables efficient hierarchical abstraction of the context. By preserving fine-grained details alongside global semantic coherence, mitigating positional bias, and dynamically adapting to content, AdmTree robustly retains the semantic information of long contexts.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ADAPT: Learning Task Mixtures for Budget-Constrained Instruction Tuning</title>
<link>https://arxiv.org/abs/2512.04555</link>
<guid>https://arxiv.org/abs/2512.04555</guid>
<content:encoded><![CDATA[
<div> Keywords: meta-learning, task sampling, token budget, multi-task instruction tuning, adaptive curriculum<br /><br />Summary:<br /><br />1. The paper introduces ADAPT, a meta-learning algorithm designed to optimize task sampling proportions within a fixed token budget for multi-task instruction tuning.  
2. Unlike traditional methods that assign fixed task weights manually, ADAPT maintains a dynamic distribution over tasks, adjusting it through meta-gradients derived from a smooth worst-case validation objective.  
3. This approach creates an adaptive curriculum that strategically allocates more tokens to tasks deemed more useful, while preventing the collapse of training focus.  
4. The algorithm was implemented on three approximately 1-billion-parameter open-weight large language models (Gemma-3-1B, LLaMA-3.2-1B, Qwen-0.6B) and was evaluated under token budgets representing 1%, 5%, and 10% of the available supervised tokens.  
5. Training was conducted across 20 distinct task types from the Natural Instructions dataset and benchmarked on 11 diverse out-of-domain datasets covering reasoning, reading comprehension, code generation, and instruction following.  
6. Empirical results reveal that ADAPT matches or slightly exceeds the performance of the best static task mixtures while utilizing fewer effective training tokens.  
7. It also reallocates token budgets towards more challenging tasks that better align with benchmark requirements, illustrating the benefits of adaptive task sampling in multi-task instruction tuning. <div>
arXiv:2512.04555v1 Announce Type: new 
Abstract: We propose ADAPT, a meta-learning algorithm that \emph{learns} task sampling proportions under an explicit token budget for multi-task instruction tuning. Instead of fixing task weights by hand, \adapt{} maintains a continuous distribution over tasks and updates it via meta-gradients of a smooth worst-case validation objective, inducing an adaptive curriculum that allocates more tokens to useful tasks while avoiding collapse. We instantiate ADAPT on three $\sim$1B-parameter open-weight LLMs (Gemma-3-1B, LLaMA-3.2-1B, Qwen-0.6B), training on 20 Natural Instructions task types under budgets of $1\%$, $5\%$, and $10\%$ of the available supervised tokens, and compare against strong supervised fine-tuning baselines with uniform and size-proportional mixing. We conduct evaluations on 11 out-of-domain benchmarks spanning reasoning, reading comprehension, code generation, and instruction following, we find that ADAPT matches or slightly improves average downstream performance relative to the best static mixture, while using fewer effective training tokens and reallocating budget toward harder, benchmark-aligned tasks.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LexGenius: An Expert-Level Benchmark for Large Language Models in Legal General Intelligence</title>
<link>https://arxiv.org/abs/2512.04578</link>
<guid>https://arxiv.org/abs/2512.04578</guid>
<content:encoded><![CDATA[
<div> Legal general intelligence, large language models, Chinese legal benchmark, LexGenius, artificial intelligence<br /><br />Summary:<br /><br />This article introduces Legal General Intelligence (GI), a form of artificial intelligence tailored to replicate legal expertise in understanding, reasoning, and decision-making across various legal domains. Existing benchmarks for evaluating large language models (LLMs) in legal tasks are predominantly result-oriented and do not adequately measure legal intelligence capabilities, which limits progress in the development of legal GI. To overcome this limitation, the authors propose LexGenius, an expert-level benchmark specifically designed for the Chinese legal context. LexGenius employs a Dimension-Task-Ability framework that encompasses seven dimensions, eleven tasks, and twenty abilities, providing a comprehensive evaluation structure. The benchmark dataset consists of multiple-choice questions based on recent legal cases and examination questions, created through a combination of manual curation and LLM-assisted reviews to minimize data leakage and ensure high accuracy and reliability validated by multiple review rounds. The study evaluates 12 state-of-the-art LLMs using LexGenius, revealing significant inconsistencies in legal intelligence abilities across different models, and notably that even the best-performing LLMs fall short of human legal professionals. The authors argue that LexGenius is a critical tool for assessing and advancing legal intelligence in LLMs. Their project and resources are publicly accessible at https://github.com/QwenQKing/LexGenius. <div>
arXiv:2512.04578v1 Announce Type: new 
Abstract: Legal general intelligence (GI) refers to artificial intelligence (AI) that encompasses legal understanding, reasoning, and decision-making, simulating the expertise of legal experts across domains. However, existing benchmarks are result-oriented and fail to systematically evaluate the legal intelligence of large language models (LLMs), hindering the development of legal GI. To address this, we propose LexGenius, an expert-level Chinese legal benchmark for evaluating legal GI in LLMs. It follows a Dimension-Task-Ability framework, covering seven dimensions, eleven tasks, and twenty abilities. We use the recent legal cases and exam questions to create multiple-choice questions with a combination of manual and LLM reviews to reduce data leakage risks, ensuring accuracy and reliability through multiple rounds of checks. We evaluate 12 state-of-the-art LLMs using LexGenius and conduct an in-depth analysis. We find significant disparities across legal intelligence abilities for LLMs, with even the best LLMs lagging behind human legal professionals. We believe LexGenius can assess the legal intelligence abilities of LLMs and enhance legal GI development. Our project is available at https://github.com/QwenQKing/LexGenius.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geschlechts\"ubergreifende Maskulina im Sprachgebrauch Eine korpusbasierte Untersuchung zu lexemspezifischen Unterschieden</title>
<link>https://arxiv.org/abs/2512.04683</link>
<guid>https://arxiv.org/abs/2512.04683</guid>
<content:encoded><![CDATA[
<div> Generic masculines, German press texts, personal nouns, corpus analysis, psycholinguistics  

<br /><br />Summary:  
This study focuses on the distribution and linguistic features of generic masculines (GM) in contemporary German press texts. It addresses the ongoing debate about whether masculine personal nouns used for mixed-gender groups or unspecified individuals are truly gender-neutral. Prior psycholinguistic research indicates that GM tends to be more associated with male referents, yet corpus-based examinations of actual usage have been limited. The researchers analyzed a large corpus of press texts, manually annotating 6,195 tokens across the full inflectional paradigms of 21 personal nouns. Key findings highlight significant differences between lexical items, notably between passive role nouns and prestige-related personal nouns. Grammatically, GM predominantly occurs in the plural form and within indefinite noun phrases. Contrary to some previous assumptions, the data suggest that GM is not primarily used to denote entire classes of people. By providing empirical evidence based on authentic written language, this study offers a more nuanced understanding of the forms and usage of GM. These insights also serve to better align linguistic stimuli in psycholinguistic research with real-world language practice. <div>
arXiv:2512.04683v1 Announce Type: new 
Abstract: This study examines the distribution and linguistic characteristics of generic masculines (GM) in contemporary German press texts. The use of masculine personal nouns to refer to mixed-gender groups or unspecified individuals has been widely debated in academia and the public, with con-flicting perspectives on its gender-neutrality. While psycholinguistic studies suggest that GM is more readily associated with male referents, corpus-based analyses of its actual use remain scarce. We investigate GM in a large corpus of press texts, focusing on lexeme-specific differences across dif-ferent types of personal nouns. We conducted manual annotations of the whole inflectional para-digm of 21 personal nouns, resulting in 6,195 annotated tokens. Our findings reveal considerable differences between lexical items, especially between passive role nouns and prestige-related per-sonal nouns. On a grammatical level, we find that GM occurs predominantly in the plural and in indefinite noun phrases. Furthermore, our data shows that GM is not primarily used to denote entire classes of people, as has been previously claimed. By providing an empirical insight into the use of GM in authentic written language, we contribute to a more nuanced understanding of its forms and manifestations. These findings provide a solid basis for aligning linguistic stimuli in psy-cholinguistic studies more closely with real-world language use.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OsmT: Bridging OpenStreetMap Queries and Natural Language with Open-source Tag-aware Language Models</title>
<link>https://arxiv.org/abs/2512.04738</link>
<guid>https://arxiv.org/abs/2512.04738</guid>
<content:encoded><![CDATA[
<div> Keywords: OsmT, OverpassQL, Tag Retrieval Augmentation, OpenStreetMap, natural language interface  

<br /><br />Summary:  
This paper addresses the challenge of bridging natural language and structured query languages, focusing on geospatial data access through Overpass Query Language (OverpassQL) for OpenStreetMap (OSM). The authors introduce OsmT, an open-source, tag-aware language model crafted to translate natural language queries into OverpassQL with high accuracy and structural validity. A novel Tag Retrieval Augmentation (TRA) mechanism is implemented to embed contextually relevant tag information during query generation, capturing the hierarchical and relational dependencies inherent to OSM’s complex topology. Furthermore, the study defines a reverse task—OverpassQL-to-Text—that converts structured queries back into natural language explanations, enhancing user accessibility and query interpretation. OsmT is evaluated on a public benchmark dataset and outperforms strong baselines consistently in both translation directions. Notably, despite employing significantly fewer parameters than large closed-source models, OsmT achieves competitive accuracy, highlighting the practicality and effectiveness of open-source pre-trained language models in handling schema-rich, geospatial query environments while enabling more transparent, adaptable, and cost-efficient deployments. <div>
arXiv:2512.04738v1 Announce Type: new 
Abstract: Bridging natural language and structured query languages is a long-standing challenge in the database community. While recent advances in language models have shown promise in this direction, existing solutions often rely on large-scale closed-source models that suffer from high inference costs, limited transparency, and lack of adaptability for lightweight deployment. In this paper, we present OsmT, an open-source tag-aware language model specifically designed to bridge natural language and Overpass Query Language (OverpassQL), a structured query language for accessing large-scale OpenStreetMap (OSM) data. To enhance the accuracy and structural validity of generated queries, we introduce a Tag Retrieval Augmentation (TRA) mechanism that incorporates contextually relevant tag knowledge into the generation process. This mechanism is designed to capture the hierarchical and relational dependencies present in the OSM database, addressing the topological complexity inherent in geospatial query formulation. In addition, we define a reverse task, OverpassQL-to-Text, which translates structured queries into natural language explanations to support query interpretation and improve user accessibility. We evaluate OsmT on a public benchmark against strong baselines and observe consistent improvements in both query generation and interpretation. Despite using significantly fewer parameters, our model achieves competitive accuracy, demonstrating the effectiveness of open-source pre-trained language models in bridging natural language and structured query languages within schema-rich geospatial environments.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs</title>
<link>https://arxiv.org/abs/2512.04746</link>
<guid>https://arxiv.org/abs/2512.04746</guid>
<content:encoded><![CDATA[
<div> Keywords: Low-bit quantization, Large Language Models, SignRoundV2, post-training quantization, bit allocation<br /><br />Summary:  
Extreme low-bit quantization is essential for deploying large language models (LLMs) efficiently but usually causes significant performance drops at 2-bit and 4-bit precision levels. The paper introduces SignRoundV2, a novel post-training quantization framework designed to operate effectively without relying on mixed-precision strategies. SignRoundV2 incorporates two key innovations: first, a fast sensitivity metric that integrates gradient information with quantization-induced deviations to guide optimal layer-wise bit allocation; second, a lightweight pre-tuning search mechanism for quantization scales which enhances performance under extremely low-bit constraints. These design elements enable SignRoundV2 to bridge the accuracy gap relative to full-precision models. Extensive experiments demonstrate that the method maintains competitive accuracy for LLMs, achieving production-grade results with only about 1 percent variance at 4-5 bits precision and strong performance even at 2 bits. The approach thus offers a practical and effective solution for low-bit quantization in large-scale language models. The implementation of SignRoundV2 is publicly available at https://github.com/intel/auto-round, facilitating adoption and further research. <div>
arXiv:2512.04746v1 Announce Type: new 
Abstract: Extreme low-bit quantization is critical for efficiently deploying Large Language Models (LLMs), yet it often leads to severe performance degradation at 2-bits and even 4-bits (e.g., MXFP4). We present SignRoundV2, a post-training quantization framework that is highly effective even without mixed-precision. SignRoundV2 introduces (1) a fast sensitivity metric that combines gradient information with quantization-induced deviations to guide layer-wise bit allocation, and (2) a lightweight pre-tuning search for quantization scales to improve extremely low-bit quantization. These components allow SignRoundV2 to close the gap with full-precision models. Extensive experiments indicate that our method sustains competitive accuracy for LLMs, achieving production-grade performance with about 1 percent variance at 4-5 bits and strong results even at 2 bits. The implementation is available at https://github.com/intel/auto-round.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Model Whisper: Steering Vectors Unlock Large Language Models' Potential in Test-time</title>
<link>https://arxiv.org/abs/2512.04748</link>
<guid>https://arxiv.org/abs/2512.04748</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Test-Time Adaptation, Test-Time Steering Vectors, Model Confidence, Task Generalization

<br /><br />Summary:  
This paper addresses the challenge of efficiently enhancing the reasoning ability of Large Language Models (LLMs) on specific tasks or new data distributions without tuning the model parameters. Existing test-time adaptation methods require parameter updates, causing computational costs and potential degradation of the model's original capabilities. To overcome this, the authors propose Test-Time Steering Vectors (TTSV), a lightweight vector prepended to the model input that keeps the LLM's parameters completely frozen. TTSV is optimized on test data by minimizing the model's output entropy, which steers the LLM toward a higher-confidence internal state, thus activating its relevant reasoning skills. The approach is computationally efficient and can be easily plugged into different settings. Extensive experiments demonstrate TTSV’s effectiveness on both base and reasoning-enhanced LLMs. For example, on the MATH500 task, TTSV yields a 45.88% relative performance improvement for the Qwen2.5-Math-7B model and a 16.22% gain for the Qwen3-4B model. Moreover, TTSV exhibits strong generalization, as steering vectors optimized for one task transfer well to various other tasks, showcasing its robustness and broad applicability. <div>
arXiv:2512.04748v1 Announce Type: new 
Abstract: It is a critical challenge to efficiently unlock the powerful reasoning potential of Large Language Models (LLMs) for specific tasks or new distributions. Existing test-time adaptation methods often require tuning model parameters, which is not only computationally expensive but also risks degrading the model's pre-existing abilities.To address this, we introduce a lightweight component, Test-Time Steering Vectors (TTSV), which is prepended to the input while keeping the LLM's parameters entirely frozen. By optimizing the TTSV on test data to minimize the model's output entropy, we steer the model towards an internal state of higher confidence, activating its inherent abilities most relevant to the current task. TTSV is both lightweight and highly efficient to optimize, making it a true plug-and-play enhancement. Extensive experiments validate our approach's effectiveness on both base models and reasoning-enhanced models. For instance, on the MATH500 task, TTSV achieves a 45.88% relative performance gain on the Qwen2.5-Math-7B model and a 16.22% relative gain on the Qwen3-4B model. Furthermore, our approach exhibits robust generalization, with its steering vectors proving highly transferable across diverse tasks.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EtCon: Edit-then-Consolidate for Reliable Knowledge Editing</title>
<link>https://arxiv.org/abs/2512.04753</link>
<guid>https://arxiv.org/abs/2512.04753</guid>
<content:encoded><![CDATA[
<div> Keywords: knowledge editing, large language models, overfitting, policy optimization, consolidation

<br /><br />Summary: This paper addresses the challenge of updating specific facts in large language models (LLMs) without full retraining, focusing on the gap between controlled evaluations and real-world lifelong learning performance. First, it identifies two key issues in existing knowledge editing methods: they tend to overfit the model to new facts, which harms previously learned capabilities, and lack a knowledge consolidation phase, causing edited facts to fail in influencing actual autoregressive generation effectively. To overcome these challenges, the authors propose a novel paradigm called Edit-then-Consolidate. The approach consists of two stages: (1) Targeted Proximal Supervised Fine-Tuning (TPSFT), which avoids overfitting by applying a trust-region constraint that limits policy drift and localizes the edit, and (2) a consolidation phase that employs Group Relative Policy Optimization (GRPO) to align the edited knowledge with chain-of-thought (CoT) inference policies by optimizing trajectory-level generation behavior under comprehensive reward criteria. Extensive experiments demonstrate that this framework enhances the reliability and generalization of knowledge edits in real-world settings, better retains locality of edits, and preserves pre-trained abilities, thus bridging the gap between theoretical editing methods and practical applicability in dynamic LLM usage scenarios. <div>
arXiv:2512.04753v1 Announce Type: new 
Abstract: Knowledge editing aims to update specific facts in large language models (LLMs) without full retraining. Prior efforts sought to tune the knowledge layers of LLMs, proving effective for making selective edits. However, a significant gap exists between their performance in controlled, teacher-forcing evaluations and their real-world effectiveness in lifelong learning scenarios, which greatly limits their practical applicability. This work's empirical analysis reveals two recurring issues associated with this gap: (1) Most traditional methods lead the edited model to overfit to the new fact, thereby degrading pre-trained capabilities; (2) There is a critical absence of a knowledge consolidation stage, leaving new facts insufficiently integrated into LLMs' inference-time behavior under autoregressive generation, thereby leading to a mismatch between parametric knowledge and actual generation behavior. To this end, we propose Edit-then-Consolidate, a novel knowledge editing paradigm that aims to bridge the gap between theoretical knowledge editing methods and their real-world applicability. Specifically, (1) our framework mitigates overfitting via Targeted Proximal Supervised Fine-Tuning (TPSFT) that localizes the edit via a trust-region objective to limit policy drift; (2) Then, a consolidation stage using Group Relative Policy Optimization (GRPO) aligns the edited knowledge with CoT-based inference policy by optimizing trajectory-level behavior under comprehensive reward signals. Extensive experiments demonstrate our framework consistently improves editing reliability and generalization under real-world evaluations, while better preserving locality and pre-trained capabilities.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Challenging the Abilities of Large Language Models in Italian: a Community Initiative</title>
<link>https://arxiv.org/abs/2512.04759</link>
<guid>https://arxiv.org/abs/2512.04759</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Italian, benchmarking, evaluation pipeline, community collaboration<br /><br />Summary: The paper introduces CALAMITA, a large-scale collaborative benchmarking initiative focused on evaluating Large Language Models (LLMs) for the Italian language. Unlike previous approaches that emphasize leaderboards, CALAMITA prioritizes methodology by bringing together over 80 contributors from academia, industry, and the public sector. The initiative covers a wide range of tasks including linguistic competence, commonsense reasoning, factual consistency, fairness, summarization, translation, and code generation. It compiles a comprehensive benchmark consisting of more than 20 tasks and nearly 100 subtasks, supported by a centralized evaluation pipeline designed to handle diverse datasets and metrics. The authors present evaluation results of four open-weight LLMs, identifying their systematic strengths and weaknesses across different abilities, while acknowledging challenges related to task-specific evaluation. Beyond numerical results, CALAMITA reveals key methodological insights such as the need for detailed, task-representative metrics, the significance of harmonized evaluation pipelines, and the advantages and constraints of engaging a broad community. Designed as a rolling benchmark, CALAMITA supports ongoing updates with new tasks and models, making it both a valuable resource for Italian LLM evaluation and a sustainable framework. The paper suggests that this approach can serve as a model for other languages and communities aiming for inclusive and thorough LLM assessments. <div>
arXiv:2512.04759v1 Announce Type: new 
Abstract: The rapid progress of Large Language Models (LLMs) has transformed natural language processing and broadened its impact across research and society. Yet, systematic evaluation of these models, especially for languages beyond English, remains limited. "Challenging the Abilities of LAnguage Models in ITAlian" (CALAMITA) is a large-scale collaborative benchmarking initiative for Italian, coordinated under the Italian Association for Computational Linguistics. Unlike existing efforts that focus on leaderboards, CALAMITA foregrounds methodology: it federates more than 80 contributors from academia, industry, and the public sector to design, document, and evaluate a diverse collection of tasks, covering linguistic competence, commonsense reasoning, factual consistency, fairness, summarization, translation, and code generation. Through this process, we not only assembled a benchmark of over 20 tasks and almost 100 subtasks, but also established a centralized evaluation pipeline that supports heterogeneous datasets and metrics. We report results for four open-weight LLMs, highlighting systematic strengths and weaknesses across abilities, as well as challenges in task-specific evaluation. Beyond quantitative results, CALAMITA exposes methodological lessons: the necessity of fine-grained, task-representative metrics, the importance of harmonized pipelines, and the benefits and limitations of broad community engagement. CALAMITA is conceived as a rolling benchmark, enabling continuous integration of new tasks and models. This makes it both a resource -- the most comprehensive and diverse benchmark for Italian to date -- and a framework for sustainable, community-driven evaluation. We argue that this combination offers a blueprint for other languages and communities seeking inclusive and rigorous LLM evaluation practices.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdiBhashaa: A Community-Curated Benchmark for Machine Translation into Indian Tribal Languages</title>
<link>https://arxiv.org/abs/2512.04765</link>
<guid>https://arxiv.org/abs/2512.04765</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, tribal languages, machine translation, participatory data creation, equitable AI<br /><br />Summary:<br /><br />This paper introduces AdiBhashaa, a pioneering project aimed at developing the first open parallel corpora and baseline machine translation (MT) systems for four major Indian tribal languages: Bhili, Mundari, Gondi, and Santali. The initiative addresses the invisibility of tribal languages in current AI technologies, which worsens inequalities in education, governance, and digital participation for these communities. AdiBhashaa employs a community-driven approach, involving native speakers directly in the data creation process to ensure authenticity and relevance. Human-in-the-loop validation further enhances the quality and reliability of the language data and MT models. The research includes a systematic evaluation of both encoder-decoder MT models and large language models, aiming to find effective solutions for low-resource language translation. Beyond technical contributions, the project exemplifies a model for more equitable AI research by centering local expertise and empowering early-career researchers from marginalized communities. It emphasizes the critical role of human validation throughout the development of language technologies, promoting inclusivity and capacity building. AdiBhashaa thus sets a precedent for how AI can be developed in ways that respect and uplift underrepresented linguistic communities rather than perpetuating structural inequities. <div>
arXiv:2512.04765v1 Announce Type: new 
Abstract: Large language models and multilingual machine translation (MT) systems increasingly drive access to information, yet many languages of the tribal communities remain effectively invisible in these technologies. This invisibility exacerbates existing structural inequities in education, governance, and digital participation. We present AdiBhashaa, a community-driven initiative that constructs the first open parallel corpora and baseline MT systems for four major Indian tribal languages-Bhili, Mundari, Gondi, and Santali. This work combines participatory data creation with native speakers, human-in-the-loop validation, and systematic evaluation of both encoder-decoder MT models and large language models. In addition to reporting technical findings, we articulate how AdiBhashaa illustrates a possible model for more equitable AI research: it centers local expertise, builds capacity among early-career researchers from marginalized communities, and foregrounds human validation in the development of language technologies.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DaLA: Danish Linguistic Acceptability Evaluation Guided by Real World Errors</title>
<link>https://arxiv.org/abs/2512.04799</link>
<guid>https://arxiv.org/abs/2512.04799</guid>
<content:encoded><![CDATA[
<div> Keywords: Danish, linguistic acceptability, corruption functions, Large Language Models, benchmark  

<br /><br />Summary:  
This paper presents an enhanced benchmark designed to evaluate linguistic acceptability specifically in Danish. The authors begin by analyzing the most common errors found in written Danish, which informs the creation of fourteen distinct corruption functions. These functions systematically introduce various types of errors into correct Danish sentences to generate incorrect versions. To validate the accuracy and relevance of these corruption functions, the study employs both manual and automatic evaluation methods. The resulting dataset serves as a benchmark for testing Large Language Models (LLMs) on their ability to judge linguistic acceptability. Compared to existing benchmarks, this new benchmark is broader and more comprehensive, incorporating a wider array of corruption types to better reflect the complexity of linguistic errors. Consequently, this increases the difficulty of the acceptability judgement task, as demonstrated by the lower performance of several LLMs when evaluated on this benchmark. Additionally, the benchmark exhibits a higher discriminatory power, meaning it is more effective at distinguishing between well-performing and poorly performing models. Overall, this work advances the evaluation framework for linguistic acceptability in Danish, providing a more rigorous and nuanced tool for assessing language models. <div>
arXiv:2512.04799v1 Announce Type: new 
Abstract: We present an enhanced benchmark for evaluating linguistic acceptability in Danish. We first analyze the most common errors found in written Danish. Based on this analysis, we introduce a set of fourteen corruption functions that generate incorrect sentences by systematically introducing errors into existing correct Danish sentences. To ensure the accuracy of these corruptions, we assess their validity using both manual and automatic methods. The results are then used as a benchmark for evaluating Large Language Models on a linguistic acceptability judgement task. Our findings demonstrate that this extension is both broader and more comprehensive than the current state of the art. By incorporating a greater variety of corruption types, our benchmark provides a more rigorous assessment of linguistic acceptability, increasing task difficulty, as evidenced by the lower performance of LLMs on our benchmark compared to existing ones. Our results also suggest that our benchmark has a higher discriminatory power which allows to better distinguish well-performing models from low-performing ones.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DAMASHA: Detecting AI in Mixed Adversarial Texts via Segmentation with Human-interpretable Attribution</title>
<link>https://arxiv.org/abs/2512.04838</link>
<guid>https://arxiv.org/abs/2512.04838</guid>
<content:encoded><![CDATA[
<div> Keywords: mixed-authorship detection, Info-Mask, adversarial robustness, stylometric features, human-AI collaboration<br /><br />Summary:<br /><br />1. The paper addresses the challenge of segmenting mixed-authorship texts, identifying precise transition points where authorship shifts between human and AI, a crucial problem for authenticity and trust.  
2. The authors propose a novel framework named Info-Mask that combines stylometric cues, perplexity signals, and structured boundary modeling to effectively detect and segment collaborative human-AI content.  
3. To test robustness against adversarial attacks, they create and release the MAS dataset, an adversarial benchmark designed to stress-test existing detectors and highlight their vulnerabilities.  
4. The work introduces Human-Interpretable Attribution (HIA) overlays that visually demonstrate how stylometric features influence boundary predictions, complemented by a small human study verifying their practical usefulness.  
5. Experiments show that Info-Mask significantly improves span-level robustness across multiple model architectures under adversarial conditions, setting new performance baselines, while also uncovering ongoing challenges for interpretable, adversarially robust mixed-authorship detection with important implications for trust and oversight in human-AI co-authorship. <div>
arXiv:2512.04838v1 Announce Type: new 
Abstract: In the age of advanced large language models (LLMs), the boundaries between human and AI-generated text are becoming increasingly blurred. We address the challenge of segmenting mixed-authorship text, that is identifying transition points in text where authorship shifts from human to AI or vice-versa, a problem with critical implications for authenticity, trust, and human oversight. We introduce a novel framework, called Info-Mask for mixed authorship detection that integrates stylometric cues, perplexity-driven signals, and structured boundary modeling to accurately segment collaborative human-AI content. To evaluate the robustness of our system against adversarial perturbations, we construct and release an adversarial benchmark dataset Mixed-text Adversarial setting for Segmentation (MAS), designed to probe the limits of existing detectors. Beyond segmentation accuracy, we introduce Human-Interpretable Attribution (HIA overlays that highlight how stylometric features inform boundary predictions, and we conduct a small-scale human study assessing their usefulness. Across multiple architectures, Info-Mask significantly improves span-level robustness under adversarial conditions, establishing new baselines while revealing remaining challenges. Our findings highlight both the promise and limitations of adversarially robust, interpretable mixed-authorship detection, with implications for trust and oversight in human-AI co-authorship.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates</title>
<link>https://arxiv.org/abs/2512.04844</link>
<guid>https://arxiv.org/abs/2512.04844</guid>
<content:encoded><![CDATA[
<div> Keywords: Source-Shielded Updates, catastrophic forgetting, low-resource adaptation, instruct large language models, parameter importance

<br /><br />Summary:  
This paper addresses the challenge of expanding the linguistic diversity of instruct large language models (LLMs) under low-resource conditions, where only unlabeled target language data is available for adaptation. The authors propose a novel method called Source-Shielded Updates (SSU), which selectively updates model parameters to preserve original source language knowledge, thereby mitigating catastrophic forgetting. SSU utilizes a small subset of source data combined with a parameter importance scoring technique to identify parameters essential for maintaining source task performance. These parameters are then protected via a column-wise freezing strategy during fine-tuning. Experiments conducted on five typologically diverse languages demonstrate that SSU substantially reduces performance degradation on source language tasks—from over 20% down to roughly 3%. At the same time, SSU maintains or even improves performance on target-language tasks compared to full fine-tuning. The study is validated on both 7B and 13B parameter LLMs, with 7B models showing consistent superiority on all benchmarks and 13B models outperforming in most cases. This approach offers a cost-effective and practical solution for adapting instruct LLMs to new languages without requiring labeled data, enhancing global accessibility and multilingual capabilities. <div>
arXiv:2512.04844v1 Announce Type: new 
Abstract: Expanding the linguistic diversity of instruct large language models (LLMs) is crucial for global accessibility but is often hindered by the reliance on costly specialized target language labeled data and catastrophic forgetting during adaptation. We tackle this challenge under a realistic, low-resource constraint: adapting instruct LLMs using only unlabeled target language data. We introduce Source-Shielded Updates (SSU), a selective parameter update strategy that proactively preserves source knowledge. Using a small set of source data and a parameter importance scoring method, SSU identifies parameters critical to maintaining source abilities. It then applies a column-wise freezing strategy to protect these parameters before adaptation. Experiments across five typologically diverse languages and 7B and 13B models demonstrate that SSU successfully mitigates catastrophic forgetting. It reduces performance degradation on monolingual source tasks to just 3.4% (7B) and 2.8% (13B) on average, a stark contrast to the 20.3% and 22.3% from full fine-tuning. SSU also achieves target-language performance highly competitive with full fine-tuning, outperforming it on all benchmarks for 7B models and the majority for 13B models.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SEAL: Self-Evolving Agentic Learning for Conversational Question Answering over Knowledge Graphs</title>
<link>https://arxiv.org/abs/2512.04868</link>
<guid>https://arxiv.org/abs/2512.04868</guid>
<content:encoded><![CDATA[
<div> Keywords: semantic parsing, knowledge-based QA, self-evolving agentic learning, S-expression, multi-hop reasoning<br /><br />Summary:<br /><br />The paper addresses key challenges in knowledge-based conversational question answering (KBCQA), such as coreference resolution, contextual dependency modeling, and complex logical reasoning over large knowledge graphs. It identifies limitations in existing methods—both end-to-end semantic parsing and agent-based reasoning—highlighting their structural inaccuracies and high computational costs. To overcome these, the authors propose SEAL, a novel two-stage semantic parsing framework driven by self-evolving agentic learning. In the first stage, a large language model extracts a minimal S-expression core that captures the essential meaning of input queries. An agentic calibration module then refines this core by fixing syntactic errors and aligning entities and relations accurately with the knowledge graph. The second stage involves template-based completion guided by predicted question types and placeholder instantiation to build executable S-expressions. This decomposition improves logical form generation, structural accuracy, and linking efficiency. A distinctive feature is SEAL’s self-evolving mechanism, which uses local and global memory plus a reflection module to adapt continuously from dialogue history and execution feedback without retraining. Extensive experiments on the SPICE benchmark demonstrate SEAL’s state-of-the-art performance, especially in multi-hop reasoning, comparison, and aggregation tasks, proving its robustness, structural fidelity, and computational efficiency. <div>
arXiv:2512.04868v1 Announce Type: new 
Abstract: Knowledge-based conversational question answering (KBCQA) confronts persistent challenges in resolving coreference, modeling contextual dependencies, and executing complex logical reasoning. Existing approaches, whether end-to-end semantic parsing or stepwise agent-based reasoning, often suffer from structural inaccuracies and prohibitive computational costs, particularly when processing intricate queries over large knowledge graphs. To address these limitations, we introduce SEAL, a novel two-stage semantic parsing framework grounded in self-evolving agentic learning. In the first stage, a large language model (LLM) extracts a minimal S-expression core that captures the essential semantics of the input query. This core is then refined by an agentic calibration module, which corrects syntactic inconsistencies and aligns entities and relations precisely with the underlying knowledge graph. The second stage employs template-based completion, guided by question-type prediction and placeholder instantiation, to construct a fully executable S-expression. This decomposition not only simplifies logical form generation but also significantly enhances structural fidelity and linking efficiency. Crucially, SEAL incorporates a self-evolving mechanism that integrates local and global memory with a reflection module, enabling continuous adaptation from dialog history and execution feedback without explicit retraining. Extensive experiments on the SPICE benchmark demonstrate that SEAL achieves state-of-the-art performance, especially in multi-hop reasoning, comparison, and aggregation tasks. The results validate notable gains in both structural accuracy and computational efficiency, underscoring the framework's capacity for robust and scalable conversational reasoning.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs Know More Than Words: A Genre Study with Syntax, Metaphor &amp; Phonetics</title>
<link>https://arxiv.org/abs/2512.04957</link>
<guid>https://arxiv.org/abs/2512.04957</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, linguistic features, multilingual genre classification, syntactic structures, phonetic metrics<br /><br />Summary:<br />1. This paper investigates whether large language models (LLMs) capture deeper linguistic properties such as syntactic structure, phonetic cues, and metrical patterns from raw text.<br />2. The authors introduce a novel multilingual genre classification dataset derived from Project Gutenberg, which includes thousands of binary classification sentences (poetry vs. novel; drama vs. poetry; drama vs. novel) across six languages: English, French, German, Italian, Spanish, and Portuguese.<br />3. Each data sample is augmented with three explicit linguistic feature sets: syntactic tree structures, metaphor counts, and phonetic metrics to assess their impact on classification performance.<br />4. Experimental results show LLM classifiers can learn latent linguistic structures both from raw text inputs and from the provided explicit linguistic features.<br />5. However, different linguistic features contribute unevenly depending on the classification task, highlighting the importance of incorporating complex linguistic signals during model training to improve natural language understanding.<br /> <div>
arXiv:2512.04957v1 Announce Type: new 
Abstract: Large language models (LLMs) demonstrate remarkable potential across diverse language related tasks, yet whether they capture deeper linguistic properties, such as syntactic structure, phonetic cues, and metrical patterns from raw text remains unclear. To analysis whether LLMs can learn these features effectively and apply them to important nature language related tasks, we introduce a novel multilingual genre classification dataset derived from Project Gutenberg, a large-scale digital library offering free access to thousands of public domain literary works, comprising thousands of sentences per binary task (poetry vs. novel;drama vs. poetry;drama vs. novel) in six languages (English, French, German, Italian, Spanish, and Portuguese). We augment each with three explicit linguistic feature sets (syntactic tree structures, metaphor counts, and phonetic metrics) to evaluate their impact on classification performance. Experiments demonstrate that although LLM classifiers can learn latent linguistic structures either from raw text or from explicitly provided features, different features contribute unevenly across tasks, which underscores the importance of incorporating more complex linguistic signals during model training.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction</title>
<link>https://arxiv.org/abs/2512.04987</link>
<guid>https://arxiv.org/abs/2512.04987</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, agent hierarchies, interactive environments, simulation-reality gap, policy learning<br /><br />Summary:  
The paper addresses the evolution of Large Language Models (LLMs) transitioning from passive responders to autonomous agents, emphasizing a shift from imitation learning to incentive-driven decision making. A primary challenge in this transition is the lack of scalable infrastructure to generate high-quality interaction signals for effective policy learning. To overcome this, the authors propose a comprehensive method that scales the diversity and complexity of interactive environments along three key dimensions: (1) Complexity, through NexAU, a flexible agent framework that enables the construction of complex agent hierarchies with simple configurations; (2) Diversity, via NexA4A, which automatically produces diverse agent hierarchies from natural language inputs, thereby covering infinite potential domains; and (3) Fidelity, using NexGAP to bridge the gap between simulation and reality by integrating dynamic real-world environments to create grounded trajectory synthesis. The researchers train the Nex-N1 model within these enriched environments, demonstrating its superior performance on benchmarks such as SWE-bench and tau2 when compared to state-of-the-art open-source models and competitive standing against leading proprietary models on challenging agent-focused tasks. The work contributes to the research community by open-sourcing the Nex ecosystem and the corresponding model weights, fostering further development in autonomous agent learning. <div>
arXiv:2512.04987v1 Announce Type: new 
Abstract: The evolution of Large Language Models (LLMs) from passive responders to autonomous agents necessitates a fundamental shift in learning paradigms -- from static imitation to incentive-driven decision making. However, this transition is significantly impeded by the lack of scalable infrastructure capable of constructing high-quality interaction signals for effective policy learning. To address this, we introduce a comprehensive method designed to systematically scale the diversity and complexity of interactive environments. Our method realizes this scaling by addressing three orthogonal dimensions: (1) Complexity: NexAU, a flexible agent framework that supports building complex agent hierarchies via simple configurations; (2) Diversity: NexA4A automatically generates diverse agent hierarchies from natural language to cover infinite domains; and (3) Fidelity: NexGAP bridges the simulation-reality gap by integrating dynamic real-world environment for grounded trajectories synthesis. We train Nex-N1 upon the diverse and complex interactive environments established by our infrastructure. Empirical results on benchmarks such as SWE-bench and tau2 demonstrate that Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance against frontier proprietary models on complex agentic tasks. We open-source the Nex ecosystem and model weights to facilitate further research.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Factuality and Transparency Are All RAG Needs! Self-Explaining Contrastive Evidence Re-ranking</title>
<link>https://arxiv.org/abs/2512.05012</link>
<guid>https://arxiv.org/abs/2512.05012</guid>
<content:encoded><![CDATA[
<div> Keywords: Self-Explaining Contrastive Evidence Re-Ranking, contrastive learning, token-level attribution, factual evidence, retrieval accuracy<br /><br />Summary: This paper introduces Self-Explaining Contrastive Evidence Re-Ranking (CER), a novel method aimed at restructuring information retrieval around factual evidence by fine-tuning embeddings through contrastive learning. CER generates token-level attribution rationales for each retrieved passage, enabling transparent explanations of the retrieval process. A key innovation of the approach involves automatically selecting hard negatives based on a subjectivity criterion, which compels the model to cluster factual rationales closer in the embedding space while pushing subjective or misleading explanations further apart. This mechanism explicitly aligns the embedding space with evidential reasoning, enhancing the reliability of retrieval results. The authors evaluated CER on clinical trial reports, an important safety-critical domain, where accurate and trustworthy retrieval is essential. Experimental results demonstrated that CER improves retrieval accuracy compared to traditional methods and effectively mitigates hallucination issues commonly seen in Retrieval-Augmented Generation (RAG) systems. Moreover, the approach offers interpretable, evidence-based retrieval outputs, increasing user trust and supporting transparent decision-making processes. Overall, CER represents a significant advancement in embedding learning and evidence-grounded retrieval, with promising applications in high-stakes environments demanding reliable information access. <div>
arXiv:2512.05012v1 Announce Type: new 
Abstract: This extended abstract introduces Self-Explaining Contrastive Evidence Re-Ranking (CER), a novel method that restructures retrieval around factual evidence by fine-tuning embeddings with contrastive learning and generating token-level attribution rationales for each retrieved passage. Hard negatives are automatically selected using a subjectivity-based criterion, forcing the model to pull factual rationales closer while pushing subjective or misleading explanations apart. As a result, the method creates an embedding space explicitly aligned with evidential reasoning. We evaluated our method on clinical trial reports, and initial experimental results show that CER improves retrieval accuracy, mitigates the potential for hallucinations in RAG systems, and provides transparent, evidence-based retrieval that enhances reliability, especially in safety-critical domains.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Arbitrage: Efficient Reasoning via Advantage-Aware Speculation</title>
<link>https://arxiv.org/abs/2512.05033</link>
<guid>https://arxiv.org/abs/2512.05033</guid>
<content:encoded><![CDATA[
<div> Keywords: Speculative Decoding, Arbitrage, Large Language Models, Step-level Verification, Inference Efficiency

<br /><br />Summary: Modern large language models (LLMs) demonstrate strong reasoning abilities through long Chain of Thoughts but suffer from high computational costs during inference. Speculative Decoding aims to speed up inference by using a fast, less accurate draft model to propose tokens that a more capable target model then verifies in parallel. Traditional token-level Speculative Decoding, however, faces inefficiencies due to unnecessary rejections caused by token mismatches even when steps are semantically equivalent. Recent advances have adopted step-level semantic verification to accept or reject entire reasoning steps at once, somewhat improving efficiency. Despite this, current step-level methods still regenerate many rejected steps, resulting in wasted computation on the target model side. To overcome this, the paper introduces Arbitrage, a new step-level speculative generation framework that dynamically routes generation based on the relative advantages of the draft and target models. Unlike fixed acceptance thresholds, Arbitrage employs a lightweight router trained to predict when the target model would produce a significantly better reasoning step. This approach approximates an ideal Oracle that selects higher-quality steps, achieving near-optimal trade-offs between efficiency and accuracy. The proposed method consistently outperforms existing step-level Speculative Decoding baselines on multiple mathematical reasoning benchmarks, reducing inference latency by up to approximately 2× without sacrificing accuracy. <div>
arXiv:2512.05033v1 Announce Type: new 
Abstract: Modern Large Language Models achieve impressive reasoning capabilities with long Chain of Thoughts, but they incur substantial computational cost during inference, and this motivates techniques to improve the performance-cost ratio. Among these techniques, Speculative Decoding accelerates inference by employing a fast but inaccurate draft model to autoregressively propose tokens, which are then verified in parallel by a more capable target model. However, due to unnecessary rejections caused by token mismatches in semantically equivalent steps, traditional token-level Speculative Decoding struggles in reasoning tasks. Although recent works have shifted to step-level semantic verification, which improve efficiency by accepting or rejecting entire reasoning steps, existing step-level methods still regenerate many rejected steps with little improvement, wasting valuable target compute. To address this challenge, we propose Arbitrage, a novel step-level speculative generation framework that routes generation dynamically based on the relative advantage between draft and target models. Instead of applying a fixed acceptance threshold, Arbitrage uses a lightweight router trained to predict when the target model is likely to produce a meaningfully better step. This routing approximates an ideal Arbitrage Oracle that always chooses the higher-quality step, achieving near-optimal efficiency-accuracy trade-offs. Across multiple mathematical reasoning benchmarks, Arbitrage consistently surpasses prior step-level Speculative Decoding baselines, reducing inference latency by up to $\sim2\times$ at matched accuracy.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structured Document Translation via Format Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.05100</link>
<guid>https://arxiv.org/abs/2512.05100</guid>
<content:encoded><![CDATA[
<div> Keywords: Format Reinforcement Learning, Group Relative Policy Optimization, TreeSim, Node-chrF, StrucAUC  

<br /><br />Summary:  
The paper addresses the challenge of translating structured text at the document level, particularly handling complex XML or HTML structures, which previous works struggle with beyond sentence-level translation. To overcome this, the authors propose Format Reinforcement Learning (FormatRL), a method that combines Group Relative Policy Optimization with a supervised fine-tuning model to directly optimize for structure-aware rewards. Two novel reward functions are introduced: TreeSim, a metric that measures the structural similarity between the predicted and reference XML trees, and Node-chrF, which evaluates translation quality at the granularity of XML nodes. Furthermore, they apply StrucAUC, a fine-grained evaluation metric that differentiates between minor structural errors and major failures, providing a more detailed assessment of the model's performance. Experiments conducted on the SAP software-documentation benchmark demonstrate that FormatRL achieves improvements across six different evaluation metrics, indicating enhancements in both structural integrity and translation quality. The study also includes an analysis highlighting how the different reward functions individually contribute to the observed improvements, validating the effectiveness of incorporating structure-aware objectives in document-level structured text translation. <div>
arXiv:2512.05100v1 Announce Type: new 
Abstract: Recent works on structured text translation remain limited to the sentence level, as they struggle to effectively handle the complex document-level XML or HTML structures. To address this, we propose \textbf{Format Reinforcement Learning (FormatRL)}, which employs Group Relative Policy Optimization on top of a supervised fine-tuning model to directly optimize novel structure-aware rewards: 1) TreeSim, which measures structural similarity between predicted and reference XML trees and 2) Node-chrF, which measures translation quality at the level of XML nodes. Additionally, we apply StrucAUC, a fine-grained metric distinguishing between minor errors and major structural failures. Experiments on the SAP software-documentation benchmark demonstrate improvements across six metrics and an analysis further shows how different reward functions contribute to improvements in both structural and translation quality.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Soft Bootstrapping: Long Context Reasoning in LLMs without Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.05105</link>
<guid>https://arxiv.org/abs/2512.05105</guid>
<content:encoded><![CDATA[
<div> Keywords: Semantic Soft Bootstrapping, long context reasoning, reinforcement learning with verifiable rewards, self-distillation, large language models<br /><br />Summary:<br /><br />This paper addresses the challenges of training large language models (LLMs) for long context reasoning tasks, particularly math and programming problems, where reinforcement learning with verifiable rewards (RLVR) is commonly used. RLVR suffers from limitations such as sparse rewards and poor sample efficiency, resulting in high computational costs during post-training. To overcome these issues, the authors propose Semantic Soft Bootstrapping (SSB), a self-distillation technique where the same base model acts both as teacher and student but is exposed to different semantic contexts about answer correctness during training. The method involves prompting the model with math problems, generating multiple rollouts, filtering correct and frequent incorrect responses, and then guiding the model to produce robust step-by-step explanations with verified answers. This process automatically generates paired training data without human supervision. The student model uses only the original question and tries to match the teacher’s sequence of logits during training. Experiments with the Qwen2.5-3B-Instruct model fine-tuned on GSM8K show significant accuracy improvements of 10.6% and 10% on MATH500 and AIME2024 benchmarks, respectively, outperforming the traditional group relative policy optimization (GRPO) RLVR method. The code and model resources are publicly available for further research. <div>
arXiv:2512.05105v1 Announce Type: new 
Abstract: Long context reasoning in large language models (LLMs) has demonstrated enhancement of their cognitive capabilities via chain-of-thought (CoT) inference. Training such models is usually done via reinforcement learning with verifiable rewards (RLVR) in reasoning based problems, like math and programming. However, RLVR is limited by several bottlenecks, such as, lack of dense reward, and inadequate sample efficiency. As a result, it requires significant compute resources in post-training phase. To overcome these limitations, in this work, we propose \textbf{Semantic Soft Bootstrapping (SSB)}, a self-distillation technique, in which the same base language model plays the role of both teacher and student, but receives different semantic contexts about the correctness of its outcome at training time. The model is first prompted with a math problem and several rollouts are generated. From them, the correct and most common incorrect response are filtered, and then provided to the model in context to produce a more robust, step-by-step explanation with a verified final answer. This pipeline automatically curates a paired teacher-student training set from raw problem-answer data, without any human intervention. This generation process also produces a sequence of logits, which is what the student model tries to match in the training phase just from the bare question alone. In our experiment, Qwen2.5-3B-Instruct on GSM8K dataset via parameter-efficient fine-tuning. We then tested its accuracy on MATH500, and AIME2024 benchmarks. Our experiments show a jump of 10.6%, and 10% improvements in accuracy, respectively, over group relative policy optimization (GRPO), which is a commonly used RLVR algorithm. Our code is available at https://github.com/purbeshmitra/semantic-soft-bootstrapping, and the model, curated dataset is available at https://huggingface.co/purbeshmitra/semantic-soft-bootstrapping.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-Centred Evaluation of Text-to-Image Generation Models for Self-expression of Mental Distress: A Dataset Based on GPT-4o</title>
<link>https://arxiv.org/abs/2512.04087</link>
<guid>https://arxiv.org/abs/2512.04087</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-generated images, mental distress, international students, prompt design, mental health communication  

<br /><br />Summary:  
1. Effective communication is vital for positive healthcare outcomes in mental health, but international students often face linguistic and cultural barriers that impede expressing mental distress.  
2. This study focuses on evaluating AI-generated images as tools to support self-expression of mental distress among Chinese international students studying in UK universities.  
3. Twenty participants described their personal experiences of mental distress, which were then elaborated using GPT-4o with four persona-based prompt templates derived from contemporary counselling practice to generate corresponding images.  
4. Participants assessed the helpfulness of these AI-generated images in facilitating communication of their emotions based on their original descriptions.  
5. The dataset created includes 100 textual descriptions, 400 generated images, and associated human evaluation scores, marking the first publicly available text-to-image evaluation dataset with human judgments in the mental health domain.  
6. Findings reveal that the design of prompts significantly impacts the perceived helpfulness of generated images, with those created using the illustrator persona receiving the highest ratings.  
7. This research provides valuable resources for advancing image evaluation, reinforcement learning with human feedback, and multi-modal approaches to mental health communication. <div>
arXiv:2512.04087v1 Announce Type: cross 
Abstract: Effective communication is central to achieving positive healthcare outcomes in mental health contexts, yet international students often face linguistic and cultural barriers that hinder their communication of mental distress. In this study, we evaluate the effectiveness of AI-generated images in supporting self-expression of mental distress. To achieve this, twenty Chinese international students studying at UK universities were invited to describe their personal experiences of mental distress. These descriptions were elaborated using GPT-4o with four persona-based prompt templates rooted in contemporary counselling practice to generate corresponding images. Participants then evaluated the helpfulness of generated images in facilitating the expression of their feelings based on their original descriptions. The resulting dataset comprises 100 textual descriptions of mental distress, 400 generated images, and corresponding human evaluation scores. Findings indicate that prompt design substantially affects perceived helpfulness, with the illustrator persona achieving the highest ratings. This work introduces the first publicly available text-to-image evaluation dataset with human judgment scores in the mental health domain, offering valuable resources for image evaluation, reinforcement learning with human feedback, and multi-modal research on mental health communication.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retrieval-Augmented Few-Shot Prompting Versus Fine-Tuning for Code Vulnerability Detection</title>
<link>https://arxiv.org/abs/2512.04106</link>
<guid>https://arxiv.org/abs/2512.04106</guid>
<content:encoded><![CDATA[
<div> Keywords: few-shot prompting, retrieval-augmented prompting, code vulnerability detection, Gemini-1.5-Flash, fine-tuning

<br /><br />Summary:  
This paper investigates few-shot prompting as an efficient alternative to fine-tuning large language models (LLMs) for specialized tasks, focusing on code vulnerability detection. It emphasizes the importance of selecting high-quality in-context examples to improve performance, especially in complex domains like security. The study evaluates three methods using the Gemini-1.5-Flash model: standard few-shot prompting with random examples, retrieval-augmented prompting with semantically similar examples, and retrieval-based labeling without model inference. Results demonstrate that retrieval-augmented prompting consistently outperforms the other two, achieving an F1 score of 74.05% and partial match accuracy of 83.90% at 20-shot settings. This approach also surpasses zero-shot prompting and fine-tuned Gemini-1.5-Flash models, which scored significantly lower, while avoiding the additional cost and time of fine-tuning. However, fine-tuning CodeBERT delivers the highest performance (F1 score of 91.22%, partial match accuracy of 91.30%) but requires greater training effort and resource investment. Overall, retrieval-augmented prompting presents a practical balance between effectiveness and efficiency for vulnerability detection without the overhead of full model fine-tuning. <div>
arXiv:2512.04106v1 Announce Type: cross 
Abstract: Few-shot prompting has emerged as a practical alternative to fine-tuning for leveraging the capabilities of large language models (LLMs) in specialized tasks. However, its effectiveness depends heavily on the selection and quality of in-context examples, particularly in complex domains. In this work, we examine retrieval-augmented prompting as a strategy to improve few-shot performance in code vulnerability detection, where the goal is to identify one or more security-relevant weaknesses present in a given code snippet from a predefined set of vulnerability categories. We perform a systematic evaluation using the Gemini-1.5-Flash model across three approaches: (1) standard few-shot prompting with randomly selected examples, (2) retrieval-augmented prompting using semantically similar examples, and (3) retrieval-based labeling, which assigns labels based on retrieved examples without model inference. Our results show that retrieval-augmented prompting consistently outperforms the other prompting strategies. At 20 shots, it achieves an F1 score of 74.05% and a partial match accuracy of 83.90%. We further compare this approach against zero-shot prompting and several fine-tuned models, including Gemini-1.5-Flash and smaller open-source models such as DistilBERT, DistilGPT2, and CodeBERT. Retrieval-augmented prompting outperforms both zero-shot (F1 score: 36.35%, partial match accuracy: 20.30%) and fine-tuned Gemini (F1 score: 59.31%, partial match accuracy: 53.10%), while avoiding the training time and cost associated with model fine-tuning. On the other hand, fine-tuning CodeBERT yields higher performance (F1 score: 91.22%, partial match accuracy: 91.30%) but requires additional training, maintenance effort, and resources.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Contextual Sensitive Data Detection</title>
<link>https://arxiv.org/abs/2512.04120</link>
<guid>https://arxiv.org/abs/2512.04120</guid>
<content:encoded><![CDATA[
<div> Keywords: sensitive data detection, contextualization, large language models, data privacy, humanitarian datasets<br /><br />Summary:  
1) The paper addresses the challenge of protecting sensitive data in open data portals by proposing more refined, context-aware methods beyond traditional personal data privacy tools.  
2) It introduces two novel mechanisms for contextual sensitive data detection: type contextualization and domain contextualization. Type contextualization identifies the semantic type of data values and evaluates their sensitivity relative to the dataset context. Domain contextualization assesses sensitivity by retrieving relevant sensitivity rules from external documents considering attributes like data topic and geographic origin.  
3) Experiments leveraging large language models (LLMs) show that type contextualization significantly improves detection accuracy, achieving 94% recall and reducing false positives compared to 63% recall with commercial tools.  
4) Domain contextualization proves effective in detecting sensitive data in non-standard domains, particularly humanitarian datasets, by grounding sensitivity judgments within broader regulatory and thematic contexts.  
5) Evaluation with humanitarian data experts highlights that explanations generated by context-grounded LLMs enhance manual auditing consistency and provide valuable guidance. The authors also release open-source tools and annotated datasets to facilitate further research and application in contextual sensitive data detection. <div>
arXiv:2512.04120v1 Announce Type: cross 
Abstract: The emergence of open data portals necessitates more attention to protecting sensitive data before datasets get published and exchanged. While an abundance of methods for suppressing sensitive data exist, the conceptualization of sensitive data and methods to detect it, focus particularly on personal data that, if disclosed, may be harmful or violate privacy. We observe the need for refining and broadening our definitions of sensitive data, and argue that the sensitivity of data depends on its context. Based on this definition, we introduce two mechanisms for contextual sensitive data detection that con- sider the broader context of a dataset at hand. First, we introduce type contextualization, which first detects the semantic type of particular data values, then considers the overall context of the data values within the dataset or document. Second, we introduce domain contextualization which determines sensitivity of a given dataset in the broader context based on the retrieval of relevant rules from documents that specify data sensitivity (e.g., data topic and geographic origin). Experiments with these mechanisms, assisted by large language models (LLMs), confirm that: 1) type-contextualization significantly reduces the number of false positives for type-based sensitive data detection and reaches a recall of 94% compared to 63% with commercial tools, and 2) domain-contextualization leveraging sensitivity rule retrieval is effective for context-grounded sensitive data detection in non-standard data domains such as humanitarian datasets. Evaluation with humanitarian data experts also reveals that context-grounded LLM explanations provide useful guidance in manual data auditing processes, improving consistency. We open-source mechanisms and annotated datasets for contextual sensitive data detection at https://github.com/trl-lab/sensitive-data-detection.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can machines perform a qualitative data analysis? Reading the debate with Alan Turing</title>
<link>https://arxiv.org/abs/2512.04121</link>
<guid>https://arxiv.org/abs/2512.04121</guid>
<content:encoded><![CDATA[
<div> Large Language Models, qualitative data analysis, Alan Turing, empirical investigation, machine intelligence<br /><br />Summary:<br /><br />This paper critically examines the ongoing debate surrounding the use of Large Language Models (LLMs) in qualitative data analysis, challenging the predominant skepticism found in existing literature. It argues that focusing on the methodological validity of LLMs for qualitative analysis misses the key issue, which should instead be the empirical study of how artificial systems perform such analyses. Drawing inspiration from Alan Turing’s seminal work "Computing Machinery and Intelligence," the paper reframes the central question: rather than asking if machines can perform qualitative analysis in principle, we should assess whether analyses produced by LLMs are sufficiently comparable to those conducted by human analysts. The author provides empirical evidence supporting this perspective, suggesting that LLMs can offer meaningful and relevant interpretations of qualitative data. In conclusion, the paper employs Turing’s rhetorical style to engage with and deconstruct common counterarguments against using LLMs for qualitative analysis. By doing so, it broadens the conversation to focus on the practical and comparative outcomes of LLM-assisted qualitative research, advocating for a shift in critical discourse from theoretical objections to empirical evaluations of machine-generated analyses. <div>
arXiv:2512.04121v1 Announce Type: cross 
Abstract: This paper reflects on the literature that rejects the use of Large Language Models (LLMs) in qualitative data analysis. It illustrates through empirical evidence as well as critical reflections why the current critical debate is focusing on the wrong problems. The paper proposes that the focus of researching the use of the LLMs for qualitative analysis is not the method per se, but rather the empirical investigation of an artificial system performing an analysis. The paper builds on the seminal work of Alan Turing and reads the current debate using key ideas from Turing "Computing Machinery and Intelligence". This paper therefore reframes the debate on qualitative analysis with LLMs and states that rather than asking whether machines can perform qualitative analysis in principle, we should ask whether with LLMs we can produce analyses that are sufficiently comparable to human analysts. In the final part the contrary views to performing qualitative analysis with LLMs are analysed using the same writing and rhetorical style that Turing used in his seminal work, to discuss the contrary views to the main question.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Network of Theseus (like the ship)</title>
<link>https://arxiv.org/abs/2512.04198</link>
<guid>https://arxiv.org/abs/2512.04198</guid>
<content:encoded><![CDATA[
<div> Keywords: Network of Theseus, inductive bias, architecture conversion, representational similarity, efficiency tradeoffs<br /><br />Summary:<br /><br />1. The paper challenges the conventional assumption in deep learning that the neural network architecture used for training must remain the same during inference, which limits architecture design choices due to optimization difficulties.<br />2. It introduces Network of Theseus (NoT), a novel method that progressively transforms a guide network architecture into a different target architecture part-by-part, while maintaining the original network’s performance.<br />3. NoT performs incremental replacement of components in the guide network with modules from the target architecture, using representational similarity metrics to align and preserve functionality.<br />4. This method allows for substantial architectural changes, such as converting convolutional networks into multilayer perceptrons or transforming GPT-2 models into recurrent neural networks.<br />5. By separating the optimization phase from the deployment architecture, NoT broadens the range of feasible inference-time architectures, enabling improved accuracy-efficiency tradeoffs and facilitating more targeted exploration of network design. <div>
arXiv:2512.04198v1 Announce Type: cross 
Abstract: A standard assumption in deep learning is that the inductive bias introduced by a neural network architecture must persist from training through inference. The architecture you train with is the architecture you deploy. This assumption constrains the community from selecting architectures that may have desirable efficiency or design properties due to difficulties with optimization. We challenge this assumption with Network of Theseus (NoT), a method for progressively converting a trained, or even untrained, guide network architecture part-by-part into an entirely different target network architecture while preserving the performance of the guide network. At each stage, components in the guide network architecture are incrementally replaced with target architecture modules and aligned via representational similarity metrics. This procedure largely preserves the functionality of the guide network even under substantial architectural changes-for example, converting a convolutional network into a multilayer perceptron, or GPT-2 into a recurrent neural network. By decoupling optimization from deployment, NoT expands the space of viable inference-time architectures, opening opportunities for better accuracy-efficiency tradeoffs and enabling more directed exploration of the architectural design space.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Balancing Safety and Helpfulness in Healthcare AI Assistants through Iterative Preference Alignment</title>
<link>https://arxiv.org/abs/2512.04210</link>
<guid>https://arxiv.org/abs/2512.04210</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, healthcare, safety alignment, conversational medical assistants, adversarial robustness<br /><br />Summary: This paper addresses the challenge of deploying Large Language Models (LLMs) in healthcare by focusing on improving their safety and trustworthiness. It introduces an iterative post-deployment alignment framework that leverages Kahneman-Tversky Optimization (KTO) and Direct Preference Optimization (DPO) to enhance model performance specifically against domain safety concerns. The study evaluates four different LLMs—Llama-3B, Llama-8B, Meditron-8B, and Mistral-7B—using the CARES-18K benchmark which tests adversarial robustness to harmful queries. Results demonstrate up to a 42% improvement in detecting unsafe or harmful queries, though the improvements come with trade-offs such as increased erroneous refusals of benign queries, revealing biases inherent to model architectures. The authors conduct ablation studies to determine when self-evaluation by models is sufficient and when external or finetuned evaluators are necessary for reliable assessments. Ultimately, the research emphasizes the critical need to balance safety, user trust, and clinical usefulness when designing conversational medical assistants, advocating for best practices in alignment and evaluation to ensure these tools are both effective and safe in medical contexts. <div>
arXiv:2512.04210v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly used in healthcare, yet ensuring their safety and trustworthiness remains a barrier to deployment. Conversational medical assistants must avoid unsafe compliance without over-refusing benign queries. We present an iterative post-deployment alignment framework that applies Kahneman-Tversky Optimization (KTO) and Direct Preference Optimization (DPO) to refine models against domain-specific safety signals. Using the CARES-18K benchmark for adversarial robustness, we evaluate four LLMs (Llama-3B/8B, Meditron-8B, Mistral-7B) across multiple cycles. Our results show up to 42% improvement in safety-related metrics for harmful query detection, alongside interesting trade-offs against erroneous refusals, thereby exposing architecture-dependent calibration biases. We also perform ablation studies to identify when self-evaluation is reliable and when external or finetuned judges are necessary to maximize performance gains. Our findings underscore the importance of adopting best practices that balance patient safety, user trust, and clinical utility in the design of conversational medical assistants.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text-Only Training for Image Captioning with Retrieval Augmentation and Modality Gap Correction</title>
<link>https://arxiv.org/abs/2512.04309</link>
<guid>https://arxiv.org/abs/2512.04309</guid>
<content:encoded><![CDATA[
<div> Keywords: image captioning, text-only training, modality gap reduction, CLIP representation, retrieval augmentation<br /><br />Summary: This paper addresses the challenge of generating image captions without relying on human-annotated image-text pairs for training, a task that remains difficult compared to fully supervised approaches. The authors propose TOMCap, a novel text-only training framework that bypasses the need for aligned image-caption data by leveraging a pre-trained language model decoder prompted with information derived from CLIP image representations. A core innovation of the approach is the reduction of the modality gap between image and text representations to make the prompting effective. Furthermore, TOMCap incorporates retrieval of relevant example captions alongside latent vector representations to augment and better guide the caption generation process. The method was evaluated through extensive experiments that demonstrate it outperforms existing training-free and text-only image captioning techniques. Finally, the study includes an analysis of the impact of various configurations of the retrieval-augmentation mechanism and the modality gap reduction process, shedding light on their contributions to overall performance. This work presents a promising direction toward reducing dependency on costly curated datasets while maintaining competitive captioning quality. <div>
arXiv:2512.04309v1 Announce Type: cross 
Abstract: Image captioning has drawn considerable attention from the natural language processing and computer vision fields. Aiming to reduce the reliance on curated data, several studies have explored image captioning without any humanly-annotated image-text pairs for training, although existing methods are still outperformed by fully supervised approaches. This paper proposes TOMCap, i.e., an improved text-only training method that performs captioning without the need for aligned image-caption pairs. The method is based on prompting a pre-trained language model decoder with information derived from a CLIP representation, after undergoing a process to reduce the modality gap. We specifically tested the combined use of retrieved examples of captions, and latent vector representations, to guide the generation process. Through extensive experiments, we show that TOMCap outperforms other training-free and text-only methods. We also analyze the impact of different choices regarding the configuration of the retrieval-augmentation and modality gap reduction components.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment</title>
<link>https://arxiv.org/abs/2512.04356</link>
<guid>https://arxiv.org/abs/2512.04356</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal LLMs, video captioning, hallucination, contrastive alignment, action faithfulness

<br /><br />Summary: Recent progress in multimodal large language models (MLLMs) has enabled impressive capabilities in generating descriptive captions for videos. However, these models face significant challenges due to factual inaccuracies known as hallucinations, particularly involving incorrect objects and actions described in the generated captions. Previous methods have mainly focused on reducing hallucinations in static images, but addressing both visual object and temporal action hallucinations in dynamic videos remains unresolved. To address this, the authors propose the Self-Augmented Contrastive Alignment (SANTA) framework. SANTA enhances object and action faithfulness by removing spurious correlations and emphasizing accurate visual facts. The framework introduces a hallucinative self-augmentation scheme that identifies potential hallucinations within the model and converts original captions into contrasted negative samples for training. Additionally, a tracklet-phrase contrastive alignment mechanism is developed to align detected objects and relation-based actions with their corresponding visual regions and temporal phrases in the video. Extensive experimental results show that SANTA outperforms existing techniques in mitigating hallucinations related to both objects and actions, achieving superior accuracy on dedicated hallucination examination benchmarks. This advances the reliability and factual consistency of video captioning by MLLMs. <div>
arXiv:2512.04356v1 Announce Type: cross 
Abstract: Recent advancement in multimodal LLMs (MLLMs) has demonstrated their remarkable capability to generate descriptive captions for input videos. However, these models suffer from factual inaccuracies in the generated descriptions, causing severe hallucination issues. While prior works have explored alleviating hallucinations for static images, jointly mitigating visual object and temporal action hallucinations for dynamic videos remains a challenging and unsolved task. To tackle this challenge, we propose a Self-Augmented Contrastive Alignment (SANTA) framework for enabling object and action faithfulness by exempting the spurious correlations and enforcing the emphasis on visual facts. SANTA employs a hallucinative self-augmentation scheme to identify the potential hallucinations that lie in the MLLM and transform the original captions to the contrasted negatives. Furthermore, we develop a tracklet-phrase contrastive alignment to match the regional objects and relation-guided actions with their corresponding visual and temporal phrases. Extensive experiments demonstrate that SANTA outperforms existing methods in alleviating object and action hallucinations, yielding superior performance on the hallucination examination benchmarks.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Natural Language Actor-Critic: Scalable Off-Policy Learning in Language Space</title>
<link>https://arxiv.org/abs/2512.04601</link>
<guid>https://arxiv.org/abs/2512.04601</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Actor-Critic, Natural Language, Off-Policy Training, Sparse Rewards<br /><br />Summary:  
This paper addresses the challenge of training large language model (LLM) agents that interact dynamically with environments over long horizons, especially when expert demonstrations are unavailable. Traditional policy gradient methods struggle due to sparse rewards and the complexity of exploration in natural language action spaces, resulting in unstable and sample-inefficient training. The authors propose Natural Language Actor-Critic (NLAC), an innovative algorithm where the critic, also an LLM, generates natural language explanations instead of scalar values. These explanations provide richer, more actionable feedback that helps the policy understand and improve suboptimal actions without relying solely on random exploration. NLAC can be trained off-policy and does not require policy gradients, enhancing data efficiency and training stability. The approach is demonstrated across various tasks including reasoning, web browsing, tool use, and dialogue, showing that NLAC outperforms existing methods. Overall, NLAC offers a promising, scalable, and stable framework for training LLM agents in environments with large and open-ended natural language action spaces and sparse reward signals. <div>
arXiv:2512.04601v1 Announce Type: cross 
Abstract: Large language model (LLM) agents -- LLMs that dynamically interact with an environment over long horizons -- have become an increasingly important area of research, enabling automation in complex tasks involving tool-use, web browsing, and dialogue with people. In the absence of expert demonstrations, training LLM agents has relied on policy gradient methods that optimize LLM policies with respect to an (often sparse) reward function. However, in long-horizon tasks with sparse rewards, learning from trajectory-level rewards can be noisy, leading to training that is unstable and has high sample complexity. Furthermore, policy improvement hinges on discovering better actions through exploration, which can be difficult when actions lie in natural language space. In this paper, we propose Natural Language Actor-Critic (NLAC), a novel actor-critic algorithm that trains LLM policies using a generative LLM critic that produces natural language rather than scalar values. This approach leverages the inherent strengths of LLMs to provide a richer and more actionable training signal; particularly, in tasks with large, open-ended action spaces, natural language explanations for why an action is suboptimal can be immensely useful for LLM policies to reason how to improve their actions, without relying on random exploration. Furthermore, our approach can be trained off-policy without policy gradients, offering a more data-efficient and stable alternative to existing on-policy methods. We present results on a mixture of reasoning, web browsing, and tool-use with dialogue tasks, demonstrating that NLAC shows promise in outperforming existing training approaches and offers a more scalable and stable training paradigm for LLM agents.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Limit cycles for speech</title>
<link>https://arxiv.org/abs/2512.04642</link>
<guid>https://arxiv.org/abs/2512.04642</guid>
<content:encoded><![CDATA[
<div> Keywords: speech rhythm, articulatory movements, cortical oscillations, limit cycle, motor activity<br /><br />Summary:<br /><br />1. The study investigates whether rhythmic patterns observed in the acoustic and neuronal oscillations associated with human speech are also present in the articulatory movements that produce speech sounds.<br />2. Traditional views consider speech movements as discrete and goal-oriented, which do not easily explain the observed rhythmicity in speech-related neural activity.<br />3. The authors introduce an alternative representation of discrete speech movements, demonstrating that these can be organized as limit cycles, a concept from dynamical systems characterizing stable, repeating patterns.<br />4. This novel framework reveals rhythmic structures in motor activity underlying speech that were previously undetectable, linking biological rhythmicity directly with the fundamental articulation process.<br />5. The findings help reconcile the longstanding paradox between the discrete nature of speech actions and the pervasive presence of rhythmicity in biological systems, suggesting speech articulation itself possesses inherent rhythmic organization at the level of individual movements. <div>
arXiv:2512.04642v1 Announce Type: cross 
Abstract: Rhythmic fluctuations in acoustic energy and accompanying neuronal excitations in cortical oscillations are characteristic of human speech, yet whether a corresponding rhythmicity inheres in the articulatory movements that generate speech remains unclear. The received understanding of speech movements as discrete, goal-oriented actions struggles to make contact with the rhythmicity findings. In this work, we demonstrate that an unintuitive -- but no less principled than the conventional -- representation for discrete movements reveals a pervasive limit cycle organization and unlocks the recovery of previously inaccessible rhythmic structure underlying the motor activity of speech. These results help resolve a time-honored tension between the ubiquity of biological rhythmicity and discreteness in speech, the quintessential human higher function, by revealing a rhythmic organization at the most fundamental level of individual articulatory actions.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SEASON: Mitigating Temporal Hallucination in Video Large Language Models via Self-Diagnostic Contrastive Decoding</title>
<link>https://arxiv.org/abs/2512.04643</link>
<guid>https://arxiv.org/abs/2512.04643</guid>
<content:encoded><![CDATA[
<div> Video Large Language Models, temporal reasoning, hallucination mitigation, contrastive decoding, video understanding<br /><br />Summary:  
Video Large Language Models (VideoLLMs) have made significant advancements in understanding video content but face challenges in accurately capturing temporal information, often resulting in temporally inconsistent or causally implausible descriptions. This leads to significant hallucination issues which prior research has not sufficiently addressed, primarily focusing instead on spatial hallucinations such as object mismatches. To overcome these limitations, the authors propose a novel method called Self-Diagnostic Contrastive Decoding (SEASON), which is training-free and dynamically assesses each output token's likelihood of hallucination. SEASON enhances temporal and spatial faithfulness by applying adaptive contrastive decoding that contrasts tokens against their respective temporal and spatial negatives. Extensive experiments reveal that SEASON surpasses all previous training-free hallucination mitigation techniques on three benchmarks specifically designed to evaluate hallucinations. Furthermore, SEASON also improves performance across four general video understanding benchmarks, demonstrating its effectiveness and broad applicability. The approach offers a practical solution to enhance the reliability and coherence of VideoLLM outputs without additional training requirements, and the authors plan to release the code upon acceptance of the work. <div>
arXiv:2512.04643v1 Announce Type: cross 
Abstract: Video Large Language Models (VideoLLMs) have shown remarkable progress in video understanding. However, these models still struggle to effectively perceive and exploit rich temporal information in videos when responding to user queries. Therefore, they often generate descriptions of events that are temporal inconsistent or causally implausible, causing severe hallucination issues. While most prior studies have focused on spatial hallucinations (e.g. object mismatches), temporal reasoning in video understanding remains relatively underexplored. To address this issue, we propose Self-Diagnostic Contrastive Decoding (SEASON), a training-free method that adaptively enhances temporal and spatial faithfulness for each output token. It achieves this by dynamically diagnosing each token's hallucination tendency and applying adaptive contrastive decoding against its corresponding temporal and spatial negatives. Extensive experiments demonstrate that SEASON outperforms all existing training-free hallucination mitigation approaches on three hallucination examination benchmarks, while further improves VideoLLMs across four general video understanding benchmarks. The code will be released upon acceptance.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Topology Matters: Measuring Memory Leakage in Multi-Agent LLMs</title>
<link>https://arxiv.org/abs/2512.04668</link>
<guid>https://arxiv.org/abs/2512.04668</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent systems, memory leakage, graph topology, privacy risk, LLMs<br /><br />Summary:<br /><br />The article introduces MAMA (Multi-Agent Memory Attack), a novel framework designed to quantify how different graph topologies affect memory leakage in multi-agent large language model (LLM) systems. MAMA uses synthetic documents embedded with labeled Personally Identifiable Information (PII) to create sanitized task instructions. The protocol consists of two phases: Engram, where private information is seeded into a target agent's memory, and Resonance, involving multiple rounds of attacker interaction to extract that information. Leakage is measured as the proportion of ground-truth PII successfully recovered across up to 10 interaction rounds. The study systematically evaluates six common network topologies—fully connected, ring, chain, binary tree, star, and star-ring—while varying the number of agents (4 to 6), attacker-target placements, and base LLM models. Results show that fully connected graphs lead to the highest leakage, chains offer the strongest protection, and leakage is higher when the attacker is closer to the target or the target has higher network centrality. Leakage escalates rapidly in early rounds before stabilizing. Although absolute leakage rates differ by model choice, the ranking of topologies remains consistent. Temporally or spatially sensitive PII is more vulnerable than identity or regulated credentials. The work offers practical recommendations including favoring sparse or hierarchical connectivity, maximizing attacker-target distances, limiting node degrees and network radius, avoiding shortcut connections bypassing central hubs, and deploying topology-aware access controls to mitigate privacy risks in multi-agent LLM systems. <div>
arXiv:2512.04668v1 Announce Type: cross 
Abstract: Graph topology is a fundamental determinant of memory leakage in multi-agent LLM systems, yet its effects remain poorly quantified. We introduce MAMA (Multi-Agent Memory Attack), a framework that measures how network structure shapes leakage. MAMA operates on synthetic documents containing labeled Personally Identifiable Information (PII) entities, from which we generate sanitized task instructions. We execute a two-phase protocol: Engram (seeding private information into a target agent's memory) and Resonance (multi-round interaction where an attacker attempts extraction). Over up to 10 interaction rounds, we quantify leakage as the fraction of ground-truth PII recovered from attacking agent outputs via exact matching. We systematically evaluate six common network topologies (fully connected, ring, chain, binary tree, star, and star-ring), varying agent counts $n\in\{4,5,6\}$, attacker-target placements, and base models. Our findings reveal consistent patterns: fully connected graphs exhibit maximum leakage while chains provide strongest protection; shorter attacker-target graph distance and higher target centrality significantly increase vulnerability; leakage rises sharply in early rounds before plateauing; model choice shifts absolute leakage rates but preserves topology rankings; temporal/locational PII attributes leak more readily than identity credentials or regulated identifiers. These results provide the first systematic mapping from architectural choices to measurable privacy risk, yielding actionable guidance: prefer sparse or hierarchical connectivity, maximize attacker-target separation, limit node degree and network radius, avoid shortcuts bypassing hubs, and implement topology-aware access controls.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Ethical Multi-Agent Systems of Large Language Models: A Mechanistic Interpretability Perspective</title>
<link>https://arxiv.org/abs/2512.04691</link>
<guid>https://arxiv.org/abs/2512.04691</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, multi-agent systems, ethical behavior, mechanistic interpretability, alignment techniques  

<br /><br />Summary:  
This position paper addresses the ethical challenges posed by multi-agent systems of large language models (MALMs) that interact autonomously. First, it emphasizes the need for comprehensive evaluation frameworks to assess ethical behavior not only at the individual agent level but also in their interactions and the system as a whole. Second, it highlights the importance of mechanistic interpretability to understand the internal mechanisms behind emergent behaviors arising in MALMs, providing transparency and insight into their decision-making processes. Third, the paper proposes developing parameter-efficient alignment methods that can effectively guide MALMs towards ethical conduct without degrading their overall performance or capabilities. The authors advocate a research agenda focused on these three interconnected challenges to ensure that MALMs behave ethically while maintaining functionality in complex multi-agent environments. This approach aims to balance ethical considerations with technical performance by leveraging interpretability and alignment techniques tailored to the unique dynamics of multi-agent LLM systems. <div>
arXiv:2512.04691v1 Announce Type: cross 
Abstract: Large language models (LLMs) have been widely deployed in various applications, often functioning as autonomous agents that interact with each other in multi-agent systems. While these systems have shown promise in enhancing capabilities and enabling complex tasks, they also pose significant ethical challenges. This position paper outlines a research agenda aimed at ensuring the ethical behavior of multi-agent systems of LLMs (MALMs) from the perspective of mechanistic interpretability. We identify three key research challenges: (i) developing comprehensive evaluation frameworks to assess ethical behavior at individual, interactional, and systemic levels; (ii) elucidating the internal mechanisms that give rise to emergent behaviors through mechanistic interpretability; and (iii) implementing targeted parameter-efficient alignment techniques to steer MALMs towards ethical behaviors without compromising their performance.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MemLoRA: Distilling Expert Adapters for On-Device Memory Systems</title>
<link>https://arxiv.org/abs/2512.04763</link>
<guid>https://arxiv.org/abs/2512.04763</guid>
<content:encoded><![CDATA[
<div> Memory-augmented models, Small Language Models, MemLoRA, Vision-Language Models, Multimodal understanding<br /><br />Summary: This paper addresses the challenge of deploying memory-augmented language models on local devices, where large language models (LLMs) are typically too resource-intensive. To overcome this, the authors propose MemLoRA, a memory system that equips Small Language Models (SLMs) with specialized memory adapters to perform key memory operations such as knowledge extraction, update, and memory-augmented generation. This enables efficient and privacy-preserving on-device memory operations without cloud dependence. MemLoRA significantly outperforms much larger baseline models on text-only benchmarks like LoCoMo, achieving performance comparable to models that are up to 60 times larger. To extend capabilities to multimodal tasks, the authors introduce MemLoRA-V, which integrates small Vision-Language Models (SVLMs) into the memory system. This integration enables strong native visual understanding and reasoning, demonstrated through a Visual Question Answering extension of LoCoMo where MemLoRA-V achieves an accuracy of 81.3%, drastically surpassing caption-based approaches. Overall, the approach provides an effective solution for local deployment of memory-augmented systems with strong performance in both text-only and multimodal contexts while maintaining user privacy and minimizing computational cost. <div>
arXiv:2512.04763v1 Announce Type: cross 
Abstract: Memory-augmented Large Language Models (LLMs) have demonstrated remarkable consistency during prolonged dialogues by storing relevant memories and incorporating them as context. Such memory-based personalization is also key in on-device settings that allow users to keep their conversations and data private. However, memory-augmented systems typically rely on LLMs that are too costly for local on-device deployment. Even though Small Language Models (SLMs) are more suitable for on-device inference than LLMs, they cannot achieve sufficient performance. Additionally, these LLM-based systems lack native visual capabilities, limiting their applicability in multimodal contexts. In this paper, we introduce (i) MemLoRA, a novel memory system that enables local deployment by equipping SLMs with specialized memory adapters, and (ii) its vision extension MemLoRA-V, which integrates small Vision-Language Models (SVLMs) to memory systems, enabling native visual understanding. Following knowledge distillation principles, each adapter is trained separately for specific memory operations$\unicode{x2013}$knowledge extraction, memory update, and memory-augmented generation. Equipped with memory adapters, small models enable accurate on-device memory operations without cloud dependency. On text-only operations, MemLoRA outperforms 10$\times$ larger baseline models (e.g., Gemma2-27B) and achieves performance comparable to 60$\times$ larger models (e.g., GPT-OSS-120B) on the LoCoMo benchmark. To evaluate visual understanding operations instead, we extend LoCoMo with challenging Visual Question Answering tasks that require direct visual reasoning. On this, our VLM-integrated MemLoRA-V shows massive improvements over caption-based approaches (81.3 vs. 23.7 accuracy) while keeping strong performance in text-based tasks, demonstrating the efficacy of our method in multimodal contexts.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are LLMs Truly Multilingual? Exploring Zero-Shot Multilingual Capability of LLMs for Information Retrieval: An Italian Healthcare Use Case</title>
<link>https://arxiv.org/abs/2512.04834</link>
<guid>https://arxiv.org/abs/2512.04834</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Information Extraction, Electronic Health Records, Clinical NLP, Multilingual Models<br /><br />Summary:<br /><br />1. Large Language Models (LLMs) have significantly impacted AI and NLP, enhancing sectors like healthcare by improving diagnostics and automating tasks.<br /><br />2. Extracting information from clinical records is vital in digital healthcare, but traditional NLP methods often fail due to the complex, variable clinical language and rich semantics in clinical texts.<br /><br />3. LLMs, with their advanced ability to understand and generate human-like text, offer promising improvements for clinical information extraction.<br /><br />4. This paper investigates how open-source multilingual LLMs perform in understanding Italian Electronic Health Records (EHRs) for real-time information extraction.<br /><br />5. Experimental results on extracting comorbidities reveal that some LLMs underperform in zero-shot, on-premises scenarios and struggle to generalize across diseases, often lagging behind native pattern matching techniques and manual annotation benchmarks. <div>
arXiv:2512.04834v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have become a key topic in AI and NLP, transforming sectors like healthcare, finance, education, and marketing by improving customer service, automating tasks, providing insights, improving diagnostics, and personalizing learning experiences. Information extraction from clinical records is a crucial task in digital healthcare. Although traditional NLP techniques have been used for this in the past, they often fall short due to the complexity, variability of clinical language, and high inner semantics in the free clinical text. Recently, Large Language Models (LLMs) have become a powerful tool for better understanding and generating human-like text, making them highly effective in this area. In this paper, we explore the ability of open-source multilingual LLMs to understand EHRs (Electronic Health Records) in Italian and help extract information from them in real-time. Our detailed experimental campaign on comorbidity extraction from EHR reveals that some LLMs struggle in zero-shot, on-premises settings, and others show significant variation in performance, struggling to generalize across various diseases when compared to native pattern matching and manual annotations.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STELLA: Guiding Large Language Models for Time Series Forecasting with Semantic Abstractions</title>
<link>https://arxiv.org/abs/2512.04871</link>
<guid>https://arxiv.org/abs/2512.04871</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, time series forecasting, semantic abstraction, hierarchical semantic anchors, STELLA  

<br /><br />Summary:  
Recent efforts to adapt Large Language Models (LLMs) for time series forecasting have underperformed due to their reliance on static correlations and inadequate use of the models' reasoning strengths. To overcome these issues, the authors introduce STELLA (Semantic-Temporal Alignment with Language Abstractions), a novel framework that enhances LLM forecasting by systematically mining and injecting structured supplementary information. STELLA achieves this through a dynamic semantic abstraction mechanism that decomposes input time series into three components: trend, seasonality, and residuals. These components’ behavioral features are then translated into Hierarchical Semantic Anchors consisting of two parts: a Corpus-level Semantic Prior (CSP), providing global context, and a Fine-grained Behavioral Prompt (FBP) for capturing instance-specific dynamics. These anchors serve as prefix-prompts that guide LLMs to better model the intrinsic temporal dynamics of the data. The effectiveness of STELLA is validated through experiments on eight benchmark datasets, where it outperforms current state-of-the-art methods in both long- and short-term forecasting tasks. Furthermore, STELLA demonstrates strong generalization capabilities in zero-shot and few-shot learning scenarios. Ablation studies confirm the critical contribution of the dynamically generated semantic anchors in improving forecasting accuracy and model performance. <div>
arXiv:2512.04871v1 Announce Type: cross 
Abstract: Recent adaptations of Large Language Models (LLMs) for time series forecasting often fail to effectively enhance information for raw series, leaving LLM reasoning capabilities underutilized. Existing prompting strategies rely on static correlations rather than generative interpretations of dynamic behavior, lacking critical global and instance-specific context. To address this, we propose STELLA (Semantic-Temporal Alignment with Language Abstractions), a framework that systematically mines and injects structured supplementary and complementary information. STELLA employs a dynamic semantic abstraction mechanism that decouples input series into trend, seasonality, and residual components. It then translates intrinsic behavioral features of these components into Hierarchical Semantic Anchors: a Corpus-level Semantic Prior (CSP) for global context and a Fine-grained Behavioral Prompt (FBP) for instance-level patterns. Using these anchors as prefix-prompts, STELLA guides the LLM to model intrinsic dynamics. Experiments on eight benchmark datasets demonstrate that STELLA outperforms state-of-the-art methods in long- and short-term forecasting, showing superior generalization in zero-shot and few-shot settings. Ablation studies further validate the effectiveness of our dynamically generated semantic anchors.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The AI Consumer Index (ACE)</title>
<link>https://arxiv.org/abs/2512.04921</link>
<guid>https://arxiv.org/abs/2512.04921</guid>
<content:encoded><![CDATA[
<div> AI Consumer Index, frontier AI models, consumer tasks, hallucination, benchmarking<br /><br />Summary:<br /><br />1. The paper introduces the AI Consumer Index (ACE), the first benchmark designed to evaluate frontier AI models on high-value consumer tasks. <br />2. ACE includes a hidden held-out test set of 400 cases covering four consumer activity domains: shopping, food, gaming, and DIY. <br />3. An open-source development set of 80 cases licensed under CC-BY is also made available for model development and evaluation. <br />4. The evaluation methodology incorporates a novel dynamic grading approach that verifies if relevant parts of model responses are grounded in retrieved web sources, improving assessment accuracy. <br />5. Ten leading AI models were tested with web search enabled, with GPT 5 (Thinking = High) achieving the highest score of 56.1%, followed closely by o3 Pro (Thinking = On) at 55.2%, and GPT 5.1 (Thinking = High) at 55.1%. <br />6. Performance varies significantly by domain; notably, the top model scores less than 50% in the Shopping category. <br />7. Models exhibit a high tendency to hallucinate, especially in critical tasks like providing accurate pricing or working links. <br />8. Overall, ACE highlights a large performance gap between current AI capabilities and the practical needs of consumers, pointing to the need for further advancements. <div>
arXiv:2512.04921v1 Announce Type: cross 
Abstract: We introduce the first version of the AI Consumer Index (ACE), a benchmark for assessing whether frontier AI models can perform high-value consumer tasks. ACE contains a hidden heldout set of 400 test cases, split across four consumer activities: shopping, food, gaming, and DIY. We are also open sourcing 80 cases as a devset with a CC-BY license. For the ACE leaderboard we evaluated 10 frontier models (with websearch turned on) using a novel grading methodology that dynamically checks whether relevant parts of the response are grounded in the retrieved web sources. GPT 5 (Thinking = High) is the top-performing model, scoring 56.1%, followed by o3 Pro (Thinking = On) (55.2%) and GPT 5.1 (Thinking = High) (55.1%). Models differ across domains, and in Shopping the top model scores under 50%. For some requests (such as giving the correct price or providing working links), models are highly prone to hallucination. Overall, ACE shows a substantial gap between the performance of even the best models and consumers' AI needs.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Algorithmic Thinking Theory</title>
<link>https://arxiv.org/abs/2512.04923</link>
<guid>https://arxiv.org/abs/2512.04923</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, iterative improvement, reasoning algorithms, probabilistic oracle, answer aggregation  

<br /><br />Summary:  
1. The paper focuses on large language models (LLMs) and their ability to solve complex reasoning tasks effectively.  
2. It highlights that the performance of LLMs can be enhanced by iterating on previously generated solutions, improving reasoning outcomes through repeated refinement.  
3. The authors introduce a theoretical framework that conceptualizes the reasoning plan as an algorithm utilizing a probabilistic oracle, which generates and aggregates multiple solution attempts.  
4. This framework formalizes and provides foundational insights into popular techniques such as iterative improvement and answer aggregation, facilitating a systematic approach to designing stronger reasoning methods.  
5. Unlike prior approaches that depend on model architecture specifics, this work builds its model based on experimental evidence, allowing its application to a broad range of current and future reasoning systems without relying on internal model details. <div>
arXiv:2512.04923v1 Announce Type: cross 
Abstract: Large language models (LLMs) have proven to be highly effective for solving complex reasoning tasks. Surprisingly, their capabilities can often be improved by iterating on previously generated solutions. In this context, a reasoning plan for generating and combining a set of solutions can be thought of as an algorithm for reasoning using a probabilistic oracle.
  We introduce a theoretical framework for analyzing such reasoning algorithms. This framework formalizes the principles underlying popular techniques for iterative improvement and answer aggregation, providing a foundation for designing a new generation of more powerful reasoning methods. Unlike approaches for understanding models that rely on architectural specifics, our model is grounded in experimental evidence. As a result, it offers a general perspective that may extend to a wide range of current and future reasoning oracles.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CARL: Critical Action Focused Reinforcement Learning for Multi-Step Agent</title>
<link>https://arxiv.org/abs/2512.04949</link>
<guid>https://arxiv.org/abs/2512.04949</guid>
<content:encoded><![CDATA[
arXiv:2512.04949v1 Announce Type: cross 
Abstract: Agents capable of accomplishing complex tasks through multiple interactions with the environment have emerged as a popular research direction. However, in such multi-step settings, the conventional group-level policy optimization algorithm becomes suboptimal because of its underlying assumption that each action holds equal contribution, which deviates significantly from reality. Our analysis reveals that only a small fraction of actions are critical in determining the final outcome. Building on this insight, we propose CARL, a critical-action-focused reinforcement learning algorithm tailored for multi-step agents. CARL achieves focused training through providing action-level optimization signals for high-criticality actions while excluding low-criticality actions from model update. Extensive experiments demonstrate that CARL achieves both stronger performance and higher efficiency during training and inference across diverse evaluation settings.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-LLM Collaboration for Medication Recommendation</title>
<link>https://arxiv.org/abs/2512.05066</link>
<guid>https://arxiv.org/abs/2512.05066</guid>
<content:encoded><![CDATA[
arXiv:2512.05066v1 Announce Type: cross 
Abstract: As healthcare increasingly turns to AI for scalable and trustworthy clinical decision support, ensuring reliability in model reasoning remains a critical challenge. Individual large language models (LLMs) are susceptible to hallucinations and inconsistency, whereas naive ensembles of models often fail to deliver stable and credible recommendations. Building on our previous work on LLM Chemistry, which quantifies the collaborative compatibility among LLMs, we apply this framework to improve the reliability in medication recommendation from brief clinical vignettes. Our approach leverages multi-LLM collaboration guided by Chemistry-inspired interaction modeling, enabling ensembles that are effective (exploiting complementary strengths), stable (producing consistent quality), and calibrated (minimizing interference and error amplification). We evaluate our Chemistry-based Multi-LLM collaboration strategy on real-world clinical scenarios to investigate whether such interaction-aware ensembles can generate credible, patient-specific medication recommendations. Preliminary results are encouraging, suggesting that LLM Chemistry-guided collaboration may offer a promising path toward reliable and trustworthy AI assistants in clinical practice.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation</title>
<link>https://arxiv.org/abs/2512.05112</link>
<guid>https://arxiv.org/abs/2512.05112</guid>
<content:encoded><![CDATA[
arXiv:2512.05112v1 Announce Type: cross 
Abstract: Recent unified multimodal large language models (MLLMs) have shown impressive capabilities, incorporating chain-of-thought (CoT) reasoning for enhanced text-to-image generation. However, existing approaches remain limited, either treating the model merely as a standalone generator or relying on abstract textual planning. To this end, we propose Draft-as-CoT (DraCo), a novel interleaved reasoning paradigm that fully leverages both textual and visual contents in CoT for better planning and verification. Our method first generates a low-resolution draft image as preview, providing more concrete and structural visual planning and guidance. Then, we employ the model's inherent understanding capability to verify potential semantic misalignments between the draft and input prompt, and performs refinement through selective corrections with super-resolution. In this way, our approach addresses two fundamental challenges: the coarse-grained nature of textual planning and the difficulty in generating rare attribute combinations. To support training, we curate DraCo-240K, aiming to enhance three atomic capabilities spanning general correction, instance manipulation, and layout reorganization. Supported by DraCo-CFG, a specialized classifier-free guidance (CFG) strategy for interleaved reasoning, DraCo achieves a tremendous increase on GenEval (+8%), Imagine-Bench (+0.91), and GenEval++ (+3%), significantly outperforming direct generation and other generation methods empowered by CoT.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EMMA-500: Enhancing Massively Multilingual Adaptation of Large Language Models</title>
<link>https://arxiv.org/abs/2409.17892</link>
<guid>https://arxiv.org/abs/2409.17892</guid>
<content:encoded><![CDATA[
arXiv:2409.17892v3 Announce Type: replace 
Abstract: In this work, we introduce EMMA-500, a large-scale multilingual language model continue-trained on texts across 546 languages designed for enhanced multilingual performance, focusing on improving language coverage for low-resource languages. To facilitate continual pre-training, we compile the MaLA corpus, a comprehensive multilingual dataset enriched with curated datasets across diverse domains. Leveraging this corpus, we conduct extensive continual pre-training of the Llama 2 7B model, resulting in EMMA-500, which demonstrates robust performance across a wide collection of benchmarks, including a comprehensive set of multilingual tasks. Our results highlight the effectiveness of continual pre-training in expanding large language models' language capacity, particularly for underrepresented languages, demonstrating significant gains in cross-lingual transfer, task generalization, and language adaptability. We release the MaLA corpus, EMMA-500 model weights, scripts, and model generations.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grounding LLM Reasoning with Knowledge Graphs</title>
<link>https://arxiv.org/abs/2502.13247</link>
<guid>https://arxiv.org/abs/2502.13247</guid>
<content:encoded><![CDATA[
arXiv:2502.13247v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) excel at generating natural language answers, yet their outputs often remain unverifiable and difficult to trace. Knowledge Graphs (KGs) offer a complementary strength by representing entities and their relationships in structured form, providing a foundation for more reliable reasoning. We propose a novel framework that integrates LLM reasoning with KGs by linking each step of the reasoning process to graph-structured data. This grounding turns intermediate ``thoughts'' into interpretable traces that remain consistent with external knowledge. Our approach incorporates multiple reasoning strategies, Chain-of-Thought (CoT), Tree-of-Thought (ToT), and Graph-of-Thought (GoT), and is evaluated on GRBench, a benchmark for domain-specific graph reasoning. Our experiments show state-of-the-art (SOTA) performance, with at least 26.5\% improvement over CoT baselines. Beyond accuracy, we analyze how step depth, branching structure, and model size influence reasoning quality, offering insights into the conditions that support effective reasoning. Together, these contributions highlight how grounding LLMs in structured knowledge enables both higher accuracy and greater interpretability in complex reasoning tasks.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Control Illusion: The Failure of Instruction Hierarchies in Large Language Models</title>
<link>https://arxiv.org/abs/2502.15851</link>
<guid>https://arxiv.org/abs/2502.15851</guid>
<content:encoded><![CDATA[
arXiv:2502.15851v4 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly deployed with hierarchical instruction schemes, where certain instructions (e.g., system-level directives) are expected to take precedence over others (e.g., user messages). Yet, we lack a systematic understanding of how effectively these hierarchical control mechanisms work. We introduce a systematic evaluation framework based on constraint prioritization to assess how well LLMs enforce instruction hierarchies. Our experiments across six state-of-the-art LLMs reveal that models struggle with consistent instruction prioritization, even for simple formatting conflicts. We find that the widely-adopted system/user prompt separation fails to establish a reliable instruction hierarchy, and models exhibit strong inherent biases toward certain constraint types regardless of their priority designation. Interestingly, we also find that societal hierarchy framings (e.g., authority, expertise, consensus) show stronger influence on model behavior than system/user roles, suggesting that pretraining-derived social structures function as latent behavioral priors with potentially greater impact than post-training guardrails.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChatGPT for President! Presupposed content in politicians versus GPT-generated texts</title>
<link>https://arxiv.org/abs/2503.01269</link>
<guid>https://arxiv.org/abs/2503.01269</guid>
<content:encoded><![CDATA[
arXiv:2503.01269v2 Announce Type: replace 
Abstract: This study examines ChatGPT-4's capability to replicate linguistic strategies used in political discourse, focusing on its potential for manipulative language generation. As large language models become increasingly popular for text generation, concerns have grown regarding their role in spreading fake news and propaganda. This research compares real political speeches with those generated by ChatGPT, emphasizing presuppositions (a rhetorical device that subtly influences audiences by packaging some content as already known at the moment of utterance, thus swaying opinions without explicit argumentation). Using a corpus-based pragmatic analysis, this study assesses how well ChatGPT can mimic these persuasive strategies. The findings reveal that although ChatGPT-generated texts contain many manipulative presuppositions, key differences emerge in their frequency, form, and function compared with those of politicians. For instance, ChatGPT often relies on change-of-state verbs used in fixed phrases, whereas politicians use presupposition triggers in more varied and creative ways. Such differences, however, are challenging to detect with the naked eye, underscoring the potential risks posed by large language models in political and public discourse.Using a corpus-based pragmatic analysis, this study assesses how well ChatGPT can mimic these persuasive strategies. The findings reveal that although ChatGPT-generated texts contain many manipulative presuppositions, key differences emerge in their frequency, form, and function compared with those of politicians. For instance, ChatGPT often relies on change-of-state verbs used in fixed phrases, whereas politicians use presupposition triggers in more varied and creative ways. Such differences, however, are challenging to detect with the naked eye, underscoring the potential risks posed by large language models in political and public discourse.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Mastery: Enhancing LLMs with Advanced Natural Language Understanding</title>
<link>https://arxiv.org/abs/2504.00409</link>
<guid>https://arxiv.org/abs/2504.00409</guid>
<content:encoded><![CDATA[
arXiv:2504.00409v2 Announce Type: replace 
Abstract: Large language models (LLMs) have greatly improved their capability in performing NLP tasks. However, deeper semantic understanding, contextual coherence, and more subtle reasoning are still difficult to obtain. The paper discusses state-of-the-art methodologies that advance LLMs with more advanced NLU techniques, such as semantic parsing, knowledge integration, and contextual reinforcement learning. We analyze the use of structured knowledge graphs, retrieval-augmented generation (RAG), and fine-tuning strategies that match models with human-level understanding. Furthermore, we address the incorporation of transformer-based architectures, contrastive learning, and hybrid symbolic-neural methods that address problems like hallucinations, ambiguity, and inconsistency in the factual perspectives involved in performing complex NLP tasks, such as question-answering text summarization and dialogue generation. Our findings show the importance of semantic precision for enhancing AI-driven language systems and suggest future research directions to bridge the gap between statistical language models and true natural language understanding.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On-Policy Optimization with Group Equivalent Preference for Multi-Programming Language Understanding</title>
<link>https://arxiv.org/abs/2505.12723</link>
<guid>https://arxiv.org/abs/2505.12723</guid>
<content:encoded><![CDATA[
arXiv:2505.12723v3 Announce Type: replace 
Abstract: Large language models (LLMs) achieve remarkable performance in code generation tasks. However, a significant performance disparity persists between popular programming languages (e.g., Python, C++) and others. To address this capability gap, we leverage the code translation task to train LLMs, thereby facilitating the transfer of coding proficiency across diverse programming languages. Moreover, we introduce OORL for training, a novel reinforcement learning (RL) framework that integrates on-policy and off-policy strategies. Within OORL, on-policy RL is applied during code translation, guided by a rule-based reward signal derived from unit tests. Complementing this coarse-grained rule-based reward, we propose Group Equivalent Preference Optimization (GEPO), a novel preference optimization method. Specifically, GEPO trains the LLM using intermediate representations (IRs) groups. LLMs can be guided to discern IRs equivalent to the source code from inequivalent ones, while also utilizing signals about the mutual equivalence between IRs within the group. This process allows LLMs to capture nuanced aspects of code functionality. By employing OORL for training with code translation tasks, LLMs improve their recognition of code functionality and their understanding of the relationships between code implemented in different languages. Extensive experiments demonstrate that our OORL for LLMs training with code translation tasks achieves significant performance improvements on code benchmarks across multiple programming languages.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TreeRare: Syntax Tree-Guided Retrieval and Reasoning for Knowledge-Intensive Question Answering</title>
<link>https://arxiv.org/abs/2506.00331</link>
<guid>https://arxiv.org/abs/2506.00331</guid>
<content:encoded><![CDATA[
arXiv:2506.00331v2 Announce Type: replace 
Abstract: In real practice, questions are typically complex and knowledge-intensive, requiring Large Language Models (LLMs) to recognize the multifaceted nature of the question and reason across multiple information sources. Iterative and adaptive retrieval, where LLMs decide when and what to retrieve based on their reasoning, has been shown to be a promising approach to resolve complex, knowledge-intensive questions. However, the performance of such retrieval frameworks is limited by the accumulation of reasoning errors and misaligned retrieval results. To overcome these limitations, we propose TreeRare (Syntax Tree-Guided Retrieval and Reasoning), a framework that utilizes syntax trees to guide information retrieval and reasoning for question answering. Following the principle of compositionality, TreeRare traverses the syntax tree in a bottom-up fashion, and in each node, it generates subcomponent-based queries and retrieves relevant passages to resolve localized uncertainty. A subcomponent question answering module then synthesizes these passages into concise, context-aware evidence. Finally, TreeRare aggregates the evidence across the tree to form a final answer. Experiments across five question answering datasets involving ambiguous or multi-hop reasoning demonstrate that TreeRare achieves substantial improvements over existing state-of-the-art methods.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Massively Multilingual Adaptation of Large Language Models Using Bilingual Translation Data</title>
<link>https://arxiv.org/abs/2506.00469</link>
<guid>https://arxiv.org/abs/2506.00469</guid>
<content:encoded><![CDATA[
arXiv:2506.00469v2 Announce Type: replace 
Abstract: This paper investigates a critical design decision in the practice of massively multilingual continual pre-training -- the inclusion of parallel data. Specifically, we study the impact of bilingual translation data for massively multilingual language adaptation of the Llama3 family of models to 500 languages. To this end, we construct the MaLA bilingual translation corpus, containing data from more than 2,500 language pairs. Subsequently, we develop the EMMA-500 Llama 3 suite of four massively multilingual models -- continually pre-trained from the Llama 3 family of base models extensively on diverse data mixes up to 671B tokens -- and explore the effect of continual pre-training with or without bilingual translation data. Comprehensive evaluation across 7 tasks and 12 benchmarks demonstrates that bilingual data tends to enhance language transfer and performance, particularly for low-resource languages. We open-source the MaLA corpus, EMMA-500 Llama 3 suite artefacts, code, and model generations.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Route-and-Reason: Scaling Large Language Model Reasoning with Reinforced Model Router</title>
<link>https://arxiv.org/abs/2506.05901</link>
<guid>https://arxiv.org/abs/2506.05901</guid>
<content:encoded><![CDATA[
arXiv:2506.05901v2 Announce Type: replace 
Abstract: Chain-of-thought has been proven essential for enhancing the complex reasoning abilities of Large Language Models (LLMs), but it also leads to high computational costs. Recent advances have explored the method to route queries among multiple models and proved it as a promising approach. However, previous works directly operate at the task level, i.e., assigning user queries to suitable LLMs, which does not allow hybrid LLMs to truly collaborate on finer-grained sub-tasks. Collaboration at the level of intermediate reasoning steps (thoughts) could enable more efficient coordination, but it also poses significant challenges for router scheduling, placing immense demands on the quality of task decomposition and the precision of the router. To address this, we propose R2-Reasoner, a novel framework centered around a Reinforced Model Router designed to efficiently scale LLM reasoning. This router orchestrates collaboration across nine heterogeneous models, whose parameter scales range from less than 1B to hundreds of billions, by first breaking down a complex query into subtasks with a decomposer, and then assigning each subtask to the optimal model with a subtask allocator, balancing performance with cost. Training this router involves a two-stage alternating process for the decomposer and the allocator, integrating supervised fine-tuning with reinforcement learning to enable effective self-supervised refinement. Extensive experiments across six challenging reasoning benchmarks demonstrate that R2-Reasoner reduces API costs by 84.46% compared with state-of-the-art baselines while maintaining competitive reasoning accuracy. Our framework paves the way for the development of more scalable and efficient reasoning systems. Our code is open-source at https://anonymous.4open.science/r/R2_Reasoner.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA</title>
<link>https://arxiv.org/abs/2506.08123</link>
<guid>https://arxiv.org/abs/2506.08123</guid>
<content:encoded><![CDATA[
arXiv:2506.08123v5 Announce Type: replace 
Abstract: Alignment of large language models (LLMs) with principles like helpfulness, honesty, and harmlessness typically relies on scalar rewards that obscure which objectives drive the training signal. We introduce QA-LIGN, which decomposes monolithic rewards into interpretable principle-specific evaluations through structured natural language programs. Models learn through a draft, critique, and revise pipeline, where symbolic evaluation against the rubrics provides transparent feedback for both initial and revised responses during GRPO training. Applied to uncensored Llama-3.1-8B-Instruct, QA-LIGN reduces attack success rates by up to 68.7% while maintaining a 0.67% false refusal rate, achieving Pareto optimal safety-helpfulness performance and outperforming both DPO and GRPO with state-of-the-art reward models given equivalent training. These results demonstrate that making reward signals interpretable and modular improves alignment effectiveness, suggesting transparency enhances LLM safety.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Online Behavior and Clinical Insight: A Longitudinal LLM-based Study of Suicidality on YouTube Reveals Novel Digital Markers</title>
<link>https://arxiv.org/abs/2506.09495</link>
<guid>https://arxiv.org/abs/2506.09495</guid>
<content:encoded><![CDATA[
arXiv:2506.09495v2 Announce Type: replace 
Abstract: Suicide remains a leading cause of death in Western countries. As social media becomes central to daily life, digital footprints offer valuable insight into suicidal behavior. Focusing on individuals who attempted suicide while uploading videos to their channels, we investigate: How do linguistic patterns on YouTube reflect suicidal behavior, and how do these patterns align with or differ from expert knowledge? We examined linguistic changes around suicide attempts and compared individuals who attempted suicide while actively uploading to their channel with three control groups: those with prior attempts, those experiencing major life events, and matched individuals from the broader cohort. Applying complementary bottom-up, hybrid, and expert-driven approaches, we analyzed a novel longitudinal dataset of 181 suicide-attempt channels and 134 controls. In the bottom-up analysis, LLM-based topic-modeling identified 166 topics; five were linked to suicide attempts, two also showed attempt-related temporal changes (Mental Health Struggles, $OR = 1.74$; YouTube Engagement, $OR = 1.67$; $p < .01$). In the hybrid approach, clinical experts reviewed LLM-derived topics and flagged 19 as suicide-related. However, none showed significant effects beyond those identified bottom-up. YouTube Engagement, a platform-specific indicator, was not flagged, underscoring the value of bottom-up discovery. A top-down psychological assessment of suicide narratives revealed differing motivations: individuals describing prior attempts aimed to help others ($\beta=-1.69$, $p<.01$), whereas those attempted during the uploading period emphasized personal recovery ($\beta=1.08$, $p<.01$). By integrating these approaches, we offer a nuanced understanding of suicidality, bridging digital behavior and clinical insights.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Investigation of Robustness of LLMs in Mathematical Reasoning: Benchmarking with Mathematically-Equivalent Transformation of Advanced Mathematical Problems</title>
<link>https://arxiv.org/abs/2508.08833</link>
<guid>https://arxiv.org/abs/2508.08833</guid>
<content:encoded><![CDATA[
arXiv:2508.08833v3 Announce Type: replace 
Abstract: In this paper, we introduce a systematic framework beyond conventional method to assess LLMs' mathematical-reasoning robustness by stress-testing them on advanced math problems that are mathematically equivalent but with linguistic and parametric variation. These transformations allow us to measure the sensitivity of LLMs to non-mathematical perturbations, thereby enabling a more accurate evaluation of their mathematical reasoning capabilities. Using this new evaluation methodology, we created PutnamGAP, a new benchmark dataset with multiple mathematically-equivalent variations of competition-level math problems. With the new dataset, we evaluate multiple families of representative LLMs and examine their robustness. Across 18 commercial and open-source models we observe sharp performance degradation on the variants. OpenAI's flagship reasoning model, O3, scores 51.5% on the originals but drops by 4.7 percentage points on surface-renaming variants, and by 12.9 percentage points on parametric variants, while smaller models fare far worse. Overall, the results show that the proposed new evaluation methodology is effective for deepening our understanding of the robustness of LLMs and generating new insights for further improving their mathematical reasoning capabilities.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SignBind-LLM: Multi-Stage Modality Fusion for Sign Language Translation</title>
<link>https://arxiv.org/abs/2509.00030</link>
<guid>https://arxiv.org/abs/2509.00030</guid>
<content:encoded><![CDATA[
arXiv:2509.00030v3 Announce Type: replace 
Abstract: Despite progress in gloss-free Sign Language Translation (SLT), traditional single modality end-to-end approaches consistently fail on two critical components of natural signing: the precise recognition of high-speed fingerspelling and the integration of asynchronous non-manual cues from the face. Recent progress in SLT with Large Language Models has side stepped this challenge, forcing a single network to learn these simultaneously resulting in poor performance when tasked with translating crucial information such as names, places, and technical terms. We introduce SignBind-LLM, a modular framework designed to overcome these limitations. Our approach employs separate, specialized predictors for continuous signing, fingerspelling, and lipreading. Each expert network first decodes its specific modality into a sequence of tokens. These parallel streams are then fused by a lightweight transformer that resolves temporal misalignments before passing the combined representation to a Large Language Model (LLM) for final sentence generation. Our method establishes a new state-of-the-art on the How2Sign, ChicagoFSWildPlus, and BOBSL datasets with a BLEU-4 score of 22.1, 73.2% letter accuracy and BLEU-4 score of 6.8 respectively. These results validate our core hypothesis: isolating and solving distinct recognition tasks before fusion provides a more powerful and effective pathway to robust, high-fidelity sign language translation.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semi-Supervised Synthetic Data Generation with Fine-Grained Relevance Control for Short Video Search Relevance Modeling</title>
<link>https://arxiv.org/abs/2509.16717</link>
<guid>https://arxiv.org/abs/2509.16717</guid>
<content:encoded><![CDATA[
arXiv:2509.16717v2 Announce Type: replace 
Abstract: Synthetic data is widely adopted in embedding models to ensure diversity in training data distributions across dimensions such as difficulty, length, and language. However, existing prompt-based synthesis methods struggle to capture domain-specific data distributions, particularly in data-scarce domains, and often overlook fine-grained relevance diversity. In this paper, we present a Chinese short video dataset with 4-level relevance annotations, filling a critical resource void. Further, we propose a semi-supervised synthetic data pipeline where two collaboratively trained models generate domain-adaptive short video data with controllable relevance labels. Our method enhances relevance-level diversity by synthesizing samples for underrepresented intermediate relevance labels, resulting in a more balanced and semantically rich training data set. Extensive offline experiments show that the embedding model trained on our synthesized data outperforms those using data generated based on prompting or vanilla supervised fine-tuning(SFT). Moreover, we demonstrate that incorporating more diverse fine-grained relevance levels in training data enhances the model's sensitivity to subtle semantic distinctions, highlighting the value of fine-grained relevance supervision in embedding learning. In the search enhanced recommendation pipeline of Douyin's dual-column scenario, through online A/B testing, the proposed model increased click-through rate(CTR) by 1.45%, raised the proportion of Strong Relevance Ratio (SRR) by 4.9%, and improved the Image User Penetration Rate (IUPR) by 0.1054%.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human Mobility Datasets Enriched With Contextual and Social Dimensions</title>
<link>https://arxiv.org/abs/2510.02333</link>
<guid>https://arxiv.org/abs/2510.02333</guid>
<content:encoded><![CDATA[
arXiv:2510.02333v2 Announce Type: replace 
Abstract: In this resource paper, we present two publicly available datasets of semantically enriched human trajectories, together with the pipeline to build them. The trajectories are publicly available GPS traces retrieved from OpenStreetMap. Each dataset includes contextual layers such as stops, moves, points of interest (POIs), inferred transportation modes, and weather data. A novel semantic feature is the inclusion of synthetic, realistic social media posts generated by Large Language Models (LLMs), enabling multimodal and semantic mobility analysis. The datasets are available in both tabular and Resource Description Framework (RDF) formats, supporting semantic reasoning and FAIR data practices. They cover two structurally distinct, large cities: Paris and New York. Our open source reproducible pipeline allows for dataset customization, while the datasets support research tasks such as behavior modeling, mobility prediction, knowledge graph construction, and LLM-based applications. To our knowledge, our resource is the first to combine real-world movement, structured semantic enrichment, LLM-generated text, and semantic web compatibility in a reusable framework.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grounding Large Language Models in Clinical Evidence: A Retrieval-Augmented Generation System for Querying UK NICE Clinical Guidelines</title>
<link>https://arxiv.org/abs/2510.02967</link>
<guid>https://arxiv.org/abs/2510.02967</guid>
<content:encoded><![CDATA[
arXiv:2510.02967v2 Announce Type: replace 
Abstract: This paper presents the development and evaluation of a Retrieval-Augmented Generation (RAG) system for querying the United Kingdom's National Institute for Health and Care Excellence (NICE) clinical guidelines using Large Language Models (LLMs). The extensive length and volume of these guidelines can impede their utilisation within a time-constrained healthcare system, a challenge this project addresses through the creation of a system capable of providing users with precisely matched information in response to natural language queries. The system's retrieval architecture, composed of a hybrid embedding mechanism, was evaluated against a corpus of 10,195 text chunks derived from three hundred guidelines. It demonstrates high performance, with a Mean Reciprocal Rank (MRR) of 0.814, a Recall of 81% at the first chunk and of 99.1% within the top ten retrieved chunks, when evaluated on 7901 queries.
  The most significant impact of the RAG system was observed during the generation phase. When evaluated on a manually curated dataset of seventy question-answer pairs, RAG-enhanced models showed substantial gains in performance. Faithfulness, the measure of whether an answer is supported by the source text, was increased by 64.7 percentage points to 99.5% for the RAG-enhanced O4-Mini model and significantly outperformed the medical-focused Meditron3-8B LLM, which scored 43%. Clinical evaluation by seven Subject Matter Experts (SMEs) further validated these findings, with GPT-4.1 achieving 98.7% accuracy while reducing unsafe responses by 67% compared to O4-Mini (from 3.0 to 1.0 per evaluator). This study thus establishes RAG as an effective, reliable, and scalable approach for applying generative AI in healthcare, enabling cost-effective access to medical guidelines.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HUME: Measuring the Human-Model Performance Gap in Text Embedding Tasks</title>
<link>https://arxiv.org/abs/2510.10062</link>
<guid>https://arxiv.org/abs/2510.10062</guid>
<content:encoded><![CDATA[
arXiv:2510.10062v3 Announce Type: replace 
Abstract: Comparing human and model performance offers a valuable perspective for understanding the strengths and limitations of embedding models, highlighting where they succeed and where they fail to capture meaning and nuance. However, such comparisons are rarely made, as human performance on embedding tasks is difficult to measure. To fill this gap, we introduce HUME: Human Evaluation Framework for Text Embeddings. While frameworks like MTEB provide broad model evaluation, they lack reliable estimates of human performance, limiting the interpretability of model scores. We measure human performance across 16 MTEB datasets spanning reranking, classification, clustering, and semantic textual similarity across linguistically diverse high- and low-resource languages. Humans achieve an average performance of 77.6% compared to 80.1% for the best embedding model, though with substantial variation: models reach high performance on some datasets while struggling on notably low-resource languages. Our human annotations also reveal multiple dataset issues. We additionally benchmark nine LLMs as annotators on reranking, classification, and STS tasks, finding that they fall short of human performance (76.1% vs. 81.2%) despite offering scalability advantages. We provide human performance baselines, insights into task difficulty patterns, and an extensible evaluation framework that enables a more meaningful interpretation of results and informs the development of both models and benchmarks. Our code, dataset, and leaderboard are publicly available at https://github.com/embeddings-benchmark/mteb.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ThaiOCRBench: A Task-Diverse Benchmark for Vision-Language Understanding in Thai</title>
<link>https://arxiv.org/abs/2511.04479</link>
<guid>https://arxiv.org/abs/2511.04479</guid>
<content:encoded><![CDATA[
arXiv:2511.04479v3 Announce Type: replace 
Abstract: We present ThaiOCRBench, the first comprehensive benchmark for evaluating vision-language models (VLMs) on Thai text-rich visual understanding tasks. Despite recent progress in multimodal modeling, existing benchmarks predominantly focus on high-resource languages, leaving Thai underrepresented, especially in tasks requiring document structure understanding. ThaiOCRBench addresses this gap by offering a diverse, human-annotated dataset comprising 2,808 samples across 13 task categories. We evaluate a wide range of state-of-the-art VLMs in a zero-shot setting, spanning both proprietary and open-source systems. Results show a significant performance gap, with proprietary models (e.g., Gemini 2.5 Pro) outperforming open-source counterparts. Notably, fine-grained text recognition and handwritten content extraction exhibit the steepest performance drops among open-source models. Through detailed error analysis, we identify key challenges such as language bias, structural mismatch, and hallucinated content. ThaiOCRBench provides a standardized framework for assessing VLMs in low-resource, script-complex settings, and provides actionable insights for improving Thai-language document understanding.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Autoformalization Robustness via Semantically Similar Paraphrasing</title>
<link>https://arxiv.org/abs/2511.12784</link>
<guid>https://arxiv.org/abs/2511.12784</guid>
<content:encoded><![CDATA[
arXiv:2511.12784v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have recently emerged as powerful tools for autoformalization. Despite their impressive performance, these models can still struggle to produce grounded and verifiable formalizations. Recent work in text-to-SQL, has revealed that LLMs can be sensitive to paraphrased natural language (NL) inputs, even when high degrees of semantic fidelity are preserved (Safarzadeh, Oroojlooyjadid, and Roth 2025). In this paper, we investigate this claim in the autoformalization domain. Specifically, we evaluate the robustness of LLMs generating formal proofs with semantically similar paraphrased NL statements by measuring semantic and compilation validity. Using the formal benchmarks MiniF2F (Zheng, Han, and Polu 2021) and Lean 4 version of ProofNet (Xin et al. 2024), and two modern LLMs, we generate paraphrased natural language statements and cross-evaluate these statements across both models. The results of this paper reveal performance variability across paraphrased inputs, demonstrating that minor shifts in NL statements can significantly impact model outputs.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SeSE: A Structural Information-Guided Uncertainty Quantification Framework for Hallucination Detection in LLMs</title>
<link>https://arxiv.org/abs/2511.16275</link>
<guid>https://arxiv.org/abs/2511.16275</guid>
<content:encoded><![CDATA[
arXiv:2511.16275v2 Announce Type: replace 
Abstract: Reliable uncertainty quantification (UQ) is essential for deploying large language models (LLMs) in safety-critical scenarios, as it enables them to abstain from responding when uncertain, thereby avoiding ``hallucinating'' falsehoods. However, state-of-the-art UQ methods primarily rely on semantic probability distributions or pairwise distances, overlooking latent semantic structural information that could enable more precise uncertainty estimates. This paper presents Semantic Structural Entropy (SeSE), a principled UQ framework that quantifies the inherent semantic uncertainty of LLMs from a structural information perspective for hallucination detection. SeSE operates in a zero-resource manner and is applicable to both open- and closed-source LLMs, making it an ``off-the-shelf" solution for new models and tasks. Specifically, to effectively model semantic spaces, we first develop an adaptively sparsified directed semantic graph construction algorithm that captures directional semantic dependencies while automatically pruning unnecessary connections that introduce negative interference. We then exploit latent semantic structural information through hierarchical abstraction: SeSE is defined as the structural entropy of the optimal semantic encoding tree, formalizing intrinsic uncertainty within semantic spaces after optimal compression. A higher SeSE value corresponds to greater uncertainty, indicating that LLMs are highly likely to generate hallucinations. In addition, to enhance fine-grained UQ in long-form generation, we extend SeSE to quantify the uncertainty of individual claims by modeling their random semantic interactions, providing theoretically explicable hallucination detection. Extensive experiments across 29 model-dataset combinations show that SeSE significantly outperforms advanced UQ baselines.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FusionBench: A Unified Library and Comprehensive Benchmark for Deep Model Fusion</title>
<link>https://arxiv.org/abs/2406.03280</link>
<guid>https://arxiv.org/abs/2406.03280</guid>
<content:encoded><![CDATA[
arXiv:2406.03280v4 Announce Type: replace-cross 
Abstract: Deep model fusion is an emerging technique that unifies the predictions or parameters of several deep neural networks into a single better-performing model in a cost-effective and data-efficient manner. Although a variety of deep model fusion techniques have been introduced, their evaluations tend to be inconsistent and often inadequate to validate their effectiveness and robustness. We present FusionBench, the first benchmark and a unified library designed specifically for deep model fusion. Our benchmark consists of multiple tasks, each with different settings of models and datasets. This variety allows us to compare fusion methods across different scenarios and model scales. Additionally, FusionBench serves as a unified library for easy implementation and testing of new fusion techniques. FusionBench is open source and actively maintained, with community contributions encouraged. Homepage https://github.com/tanganke/fusion_bench
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data Mixing Can Induce Phase Transitions in Knowledge Acquisition</title>
<link>https://arxiv.org/abs/2505.18091</link>
<guid>https://arxiv.org/abs/2505.18091</guid>
<content:encoded><![CDATA[
arXiv:2505.18091v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are typically trained on data mixtures: most data come from web scrapes, while a small portion is curated from high-quality sources with dense domain-specific knowledge. In this paper, we show that when training LLMs on such data mixtures, knowledge acquisition from knowledge-dense datasets, unlike training exclusively on knowledge-dense data (arXiv:2404.05405), does not always follow a smooth scaling law but can exhibit phase transitions with respect to the mixing ratio and model size. Through controlled experiments on a synthetic biography dataset mixed with web-scraped data, we demonstrate that: (1) as we increase the model size to a critical value, the model suddenly transitions from memorizing very few to most of the biographies; (2) below a critical mixing ratio, the model memorizes almost nothing even with extensive training, but beyond this threshold, it rapidly memorizes more biographies. We attribute these phase transitions to a capacity allocation phenomenon: a model with bounded capacity must act like a knapsack problem solver to minimize the overall test loss, and the optimal allocation across datasets can change discontinuously as the model size or mixing ratio varies. We formalize this intuition in an information-theoretic framework and reveal that these phase transitions are predictable, with the critical mixing ratio following a power-law relationship with the model size. Our findings highlight a concrete case where a good mixing recipe for large models may not be optimal for small models, and vice versa.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlashFormer: Whole-Model Kernels for Efficient Low-Batch Inference</title>
<link>https://arxiv.org/abs/2505.22758</link>
<guid>https://arxiv.org/abs/2505.22758</guid>
<content:encoded><![CDATA[
arXiv:2505.22758v2 Announce Type: replace-cross 
Abstract: The size and compute characteristics of modern large language models have led to an increased interest in developing specialized kernels tailored for particular training and inference workloads. Existing kernels primarily optimize for compute utilization, targeting the large-batch training and inference settings. However, low-batch inference, where memory bandwidth and kernel launch overheads are significant factors, remains important for many applications of interest such as in edge deployment and latency-sensitive applications. This paper describes FlashFormer, which fuses the entire transformer forward pass into a single kernel for accelerating low-batch inference of large language models. Across various model sizes and quantizations settings, FlashFormer achieves nontrivial speedups compared to existing inference kernels.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large language models can learn and generalize steganographic chain-of-thought under process supervision</title>
<link>https://arxiv.org/abs/2506.01926</link>
<guid>https://arxiv.org/abs/2506.01926</guid>
<content:encoded><![CDATA[
arXiv:2506.01926v2 Announce Type: replace-cross 
Abstract: Chain-of-thought (CoT) reasoning not only enhances large language model performance but also provides critical insights into decision-making processes, marking it as a useful tool for monitoring model intent and planning. However, recent works have shown that banning the mention of a specific example of reward hacking causes obfuscation of the undesired reasoning traces but the persistence of the undesired behavior, threatening the reliability of CoT monitoring. We provide an extension to these results with regard to the ability of models to learn a specific type of obfuscated reasoning: steganography. First, we show that penalizing the use of specific strings within load-bearing reasoning traces causes models to substitute alternative strings. Crucially, this does not alter the underlying method by which the model performs the task, demonstrating that the model can learn to steganographically encode its reasoning.We further demonstrate that models can generalize an encoding scheme. When the penalized strings belong to an overarching class, the model learns not only to substitute strings seen in training, but also develops a general encoding scheme for all members of the class which it can apply to held-out testing strings.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-branch Prompting for Multimodal Machine Translation</title>
<link>https://arxiv.org/abs/2507.17588</link>
<guid>https://arxiv.org/abs/2507.17588</guid>
<content:encoded><![CDATA[
arXiv:2507.17588v2 Announce Type: replace-cross 
Abstract: Multimodal Machine Translation (MMT) typically enhances text-only translation by incorporating aligned visual features. Despite the remarkable progress, state-of-the-art MMT approaches often rely on paired image-text inputs at inference and are sensitive to irrelevant visual noise, which limits their robustness and practical applicability. To address these issues, we propose D2P-MMT, a diffusion-based dual-branch prompting framework for robust vision-guided translation. Specifically, D2P-MMT requires only the source text and a reconstructed image generated by a pre-trained diffusion model, which naturally filters out distracting visual details while preserving semantic cues. During training, the model jointly learns from both authentic and reconstructed images using a dual-branch prompting strategy, encouraging rich cross-modal interactions. To bridge the modality gap and mitigate training-inference discrepancies, we introduce a distributional alignment loss that enforces consistency between the output distributions of the two branches. Extensive experiments on the Multi30K dataset demonstrate that D2P-MMT achieves superior translation performance compared to existing state-of-the-art approaches.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TaoSR1: The Thinking Model for E-commerce Relevance Search</title>
<link>https://arxiv.org/abs/2508.12365</link>
<guid>https://arxiv.org/abs/2508.12365</guid>
<content:encoded><![CDATA[
arXiv:2508.12365v3 Announce Type: replace-cross 
Abstract: Query-product relevance prediction is a core task in e-commerce search. BERT-based models excel at semantic matching but lack complex reasoning capabilities. While Large Language Models (LLMs) are explored, most still use discriminative fine-tuning or distill to smaller models for deployment. We propose a framework to directly deploy LLMs for this task, addressing key challenges: Chain-of-Thought (CoT) error accumulation, discriminative hallucination, and deployment feasibility. Our framework, TaoSR1, involves three stages: (1) Supervised Fine-Tuning (SFT) with CoT to instill reasoning; (2) Offline sampling with a pass@N strategy and Direct Preference Optimization (DPO) to improve generation quality; and (3) Difficulty-based dynamic sampling with Group Relative Policy Optimization (GRPO) to mitigate discriminative hallucination. Additionally, post-CoT processing and a cumulative probability-based partitioning method enable efficient online deployment. TaoSR1 significantly outperforms baselines on offline datasets and achieves substantial gains in online side-by-side human evaluations, introducing a novel paradigm for applying CoT reasoning to relevance classification.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Similarity-Distance-Magnitude Activations</title>
<link>https://arxiv.org/abs/2509.12760</link>
<guid>https://arxiv.org/abs/2509.12760</guid>
<content:encoded><![CDATA[
arXiv:2509.12760v3 Announce Type: replace-cross 
Abstract: We introduce the Similarity-Distance-Magnitude (SDM) activation function, a more robust and interpretable formulation of the standard softmax activation function, adding Similarity (i.e., correctly predicted depth-matches into training) awareness and Distance-to-training-distribution awareness to the existing output Magnitude (i.e., decision-boundary) awareness, and enabling interpretability-by-exemplar via dense matching. We further introduce the SDM estimator, based on a data-driven partitioning of the class-wise empirical CDFs via the SDM activation, to control the class- and prediction-conditional accuracy among selective classifications. When used as the final-layer activation over pre-trained language models for selective classification, the SDM estimator is more robust to co-variate shifts and out-of-distribution inputs than existing calibration methods using softmax activations, while remaining informative over in-distribution data.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Exploration-Exploitation Trade-off: A Hidden State Approach for LLM Reasoning in RLVR</title>
<link>https://arxiv.org/abs/2509.23808</link>
<guid>https://arxiv.org/abs/2509.23808</guid>
<content:encoded><![CDATA[
arXiv:2509.23808v3 Announce Type: replace-cross 
Abstract: A prevailing view in Reinforcement Learning with Verifiable Rewards (RLVR) interprets recent progress through the lens of an exploration-exploitation trade-off, a perspective largely shaped by token-level metrics. We re-examine this perspective, proposing that this perceived trade-off may not be a fundamental constraint but rather an artifact of the measurement level. To investigate this, we shift the analysis to the semantically rich hidden-state space, adopting Effective Rank (ER) to quantify exploration and proposing its novel first- and second-order derivatives, named ER Velocity and ER Acceleration, to capture exploitation dynamics. Our analysis reveals that in the semantic space, exploration and exploitation could be decoupled (Sec.~4). This finding reveals an opportunity to enhance both capacities simultaneously. This insight motivates our method, Velocity-Exploiting Rank-Learning (VERL), the first to operationalize the principle of synergistic exploration-exploitation enhancement by directly shaping the RL advantage function. The key innovation is leveraging the theoretically stable ERA as a predictive meta-controller to create a synergistic, dual-channel incentive structure. Instead of forcing a trade-off, VERL prospectively amplifies rewards for exploration to preempt overconfidence and reinforces exploitative gains to consolidate reasoning. Experiments across diverse LLMs and reasoning benchmarks show consistent gains, including up to 21.4% absolute accuracy improvement on the challenging Gaokao 2024 dataset.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Fine-Tuning through Advanced Initialization Strategies for Low-Rank Adaptation</title>
<link>https://arxiv.org/abs/2510.03731</link>
<guid>https://arxiv.org/abs/2510.03731</guid>
<content:encoded><![CDATA[
arXiv:2510.03731v2 Announce Type: replace-cross 
Abstract: The rapid development of parameter-efficient fine-tuning methods has noticeably improved the efficiency of adapting large language models. Among these, LoRA has gained widespread popularity due to its strong balance of effectiveness and parameter efficiency. However, LoRA relies on initializing two low-rank matrices whose product is zero, which limits its ability to effectively activate and leverage the original model weights-creating a potential bottleneck for optimal performance. To address this limitation, we propose \textbf{IniLoRA}, a novel initialization strategy that initializes the low-rank matrices to closely approximate the original model weights. Experimental results indicate that IniLoRA achieves better performance than LoRA across a range of models and tasks. Additionally, we introduce two variants, IniLoRA-$\alpha$ and IniLoRA-$\beta$, both leveraging distinct initialization methods to enhance performance further.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ground-Truth Subgraphs for Better Training and Evaluation of Knowledge Graph Augmented LLMs</title>
<link>https://arxiv.org/abs/2511.04473</link>
<guid>https://arxiv.org/abs/2511.04473</guid>
<content:encoded><![CDATA[
arXiv:2511.04473v2 Announce Type: replace-cross 
Abstract: Retrieval of information from graph-structured knowledge bases represents a promising direction for improving the factuality of LLMs. While various solutions have been proposed, a comparison of methods is difficult due to the lack of challenging QA datasets with ground-truth targets for graph retrieval. We present SynthKGQA, an LLM-powered framework for generating high-quality Knowledge Graph Question Answering datasets from any Knowledge Graph, providing the full set of ground-truth facts in the KG to reason over questions. We show how, in addition to enabling more informative benchmarking of KG retrievers, the data produced with SynthKGQA also allows us to train better models.We apply SynthKGQA to Wikidata to generate GTSQA, a new dataset designed to test zero-shot generalization abilities of KG retrievers with respect to unseen graph structures and relation types, and benchmark popular solutions for KG-augmented LLMs on it.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NLP Datasets for Idiom and Figurative Language Tasks</title>
<link>https://arxiv.org/abs/2511.16345</link>
<guid>https://arxiv.org/abs/2511.16345</guid>
<content:encoded><![CDATA[
<div> Keywords: idiomatic language, figurative language, large language models, datasets, idiom recognition<br /><br />Summary: This paper addresses the challenges large language models (LLMs) face in understanding idiomatic and figurative language, which is prevalent in colloquial speech and social media. Despite the availability of large corpora, idioms and figurative expressions remain difficult for LLMs to process effectively. The authors highlight that finetuning approaches improve performance but emphasize the need for better and larger datasets to further close the gap. To this end, they present a diverse collection of datasets created by compiling a combined idiom list from recent figurative language datasets and extracting contextual sequences from a large corpus. The resulting resources include one large-scale dataset of potential idiomatic and figurative expressions alongside two human-annotated datasets featuring definite idioms for rigorous evaluation. These datasets were post-processed to ensure compatibility across different models and were utilized in training and evaluating pre-trained language models on idiom recognition tasks such as slot labeling and sequence tagging. The contributions offer a valuable foundation for developing new models and novel techniques aimed at improving figurative meaning understanding in NLP applications. <div>
arXiv:2511.16345v2 Announce Type: replace 
Abstract: Idiomatic and figurative language form a large portion of colloquial speech and writing. With social media, this informal language has become more easily observable to people and trainers of large language models (LLMs) alike. While the advantage of large corpora seems like the solution to all machine learning and Natural Language Processing (NLP) problems, idioms and figurative language continue to elude LLMs. Finetuning approaches are proving to be optimal, but better and larger datasets can help narrow this gap even further. The datasets presented in this paper provide one answer, while offering a diverse set of categories on which to build new models and develop new approaches. A selection of recent idiom and figurative language datasets were used to acquire a combined idiom list, which was used to retrieve context sequences from a large corpus. One large-scale dataset of potential idiomatic and figurative language expressions and two additional human-annotated datasets of definite idiomatic and figurative language expressions were created to evaluate the baseline ability of pre-trained language models in handling figurative meaning through idiom recognition (detection) tasks. The resulting datasets were post-processed for model agnostic training compatibility, utilized in training, and evaluated on slot labeling and sequence tagging.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe</title>
<link>https://arxiv.org/abs/2511.16334</link>
<guid>https://arxiv.org/abs/2511.16334</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal reasoning, supervised fine-tuning, reinforcement learning, data curation, large reasoning models<br /><br />Summary:<br /><br />This paper introduces OpenMMReasoner, a transparent two-stage training approach designed to advance multimodal reasoning capabilities in large reasoning models. The first stage involves supervised fine-tuning (SFT) on an 874K-sample dataset, which is carefully curated with rigorous step-by-step validation to establish a solid foundation for the model's reasoning skills. Following this, the reinforcement learning (RL) stage uses a smaller but diverse 74K-sample dataset to further refine and stabilize the reasoning abilities, enhancing overall robustness and learning efficiency. Extensive evaluations across nine multimodal reasoning benchmarks show that OpenMMReasoner outperforms strong existing baselines by achieving an 11.6% improvement over the Qwen2.5-VL-7B-Instruct baseline. The results emphasize the significance of high-quality data and carefully designed training methodologies in successfully scaling up multimodal reasoning. By releasing all codes, data, and pipelines as open source at their GitHub repository, the authors contribute a replicable and extensible framework to the research community, fostering future advancements in large-scale multimodal reasoning models. <div>
arXiv:2511.16334v3 Announce Type: replace-cross 
Abstract: Recent advancements in large reasoning models have fueled growing interest in extending such capabilities to multimodal domains. However, despite notable progress in visual reasoning, the lack of transparent and reproducible data curation and training strategies remains a major barrier to scalable research. In this work, we introduce OpenMMReasoner, a fully transparent two-stage recipe for multimodal reasoning spanning supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we construct an 874K-sample cold-start dataset with rigorous step-by-step validation, providing a strong foundation for reasoning capabilities. The subsequent RL stage leverages a 74K-sample dataset across diverse domains to further sharpen and stabilize these abilities, resulting in a more robust and efficient learning process. Extensive evaluations demonstrate that our training recipe not only surpasses strong baselines but also highlights the critical role of data quality and training design in shaping multimodal reasoning performance. Notably, our method achieves a 11.6% improvement over the Qwen2.5-VL-7B-Instruct baseline across nine multimodal reasoning benchmarks, establishing a solid empirical foundation for future large-scale multimodal reasoning research. We open-sourced all our codes, pipeline, and data at https://github.com/EvolvingLMMs-Lab/OpenMMReasoner.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Entropy-Based Measurement of Value Drift and Alignment Work in Large Language Models</title>
<link>https://arxiv.org/abs/2512.03047</link>
<guid>https://arxiv.org/abs/2512.03047</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, ethical entropy, value drift, alignment, monitoring pipeline<br /><br />Summary:<br />1. The paper addresses the challenge of assessing large language model (LLM) safety beyond static benchmarks by focusing on dynamic failures such as value drift under distribution shift, jailbreak attacks, and alignment degradation during deployment.<br />2. It builds on the recently proposed Second Law of Intelligence, conceptualizing ethical entropy (S(t)) as a state variable that naturally tends to increase unless alignment efforts actively counterbalance it.<br />3. The authors operationalize this framework by defining a five-way behavioral taxonomy and training a classifier to estimate the ethical entropy from LLM-generated transcripts.<br />4. They test four frontier LLMs, both in their base and instruction-tuned forms, across multiple stress tests and observe that base models exhibit steady ethical entropy growth, while tuned models suppress this growth and reduce entropy by approximately 80%.<br />5. The study introduces an effective alignment work rate (gamma_eff) derived from entropy trajectories and integrates S(t) and gamma_eff into a runtime monitoring pipeline that raises alerts when ethical entropy drifts beyond stability thresholds, enabling proactive oversight and control of value drift in deployed LLMs. <div>
arXiv:2512.03047v1 Announce Type: new 
Abstract: Large language model safety is usually assessed with static benchmarks, but key failures are dynamic: value drift under distribution shift, jailbreak attacks, and slow degradation of alignment in deployment. Building on a recent Second Law of Intelligence that treats ethical entropy as a state variable which tends to increase unless countered by alignment work, we make this framework operational for large language models. We define a five-way behavioral taxonomy, train a classifier to estimate ethical entropy S(t) from model transcripts, and measure entropy dynamics for base and instruction-tuned variants of four frontier models across stress tests. Base models show sustained entropy growth, while tuned variants suppress drift and reduce ethical entropy by roughly eighty percent. From these trajectories we estimate an effective alignment work rate gamma_eff and embed S(t) and gamma_eff in a monitoring pipeline that raises alerts when entropy drift exceeds a stability threshold, enabling run-time oversight of value drift.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Watermarks for Embeddings-as-a-Service Large Language Models</title>
<link>https://arxiv.org/abs/2512.03079</link>
<guid>https://arxiv.org/abs/2512.03079</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Embeddings-as-a-Service, Imitation Attacks, Watermarking, Paraphrasing Attacks

<br /><br />Summary:  
This paper addresses the vulnerability of Embeddings-as-a-Service (EaaS) models, which are built on Large Language Models (LLMs), to imitation attacks where an attacker clones the model without internal access. To protect the intellectual property of EaaS providers, watermarking techniques have been introduced that embed identifiable features into text embeddings to verify model ownership. However, the authors reveal a critical weakness in current watermarking methods by demonstrating that paraphrasing input text can effectively remove these watermarks during model cloning. This paraphrasing-based approach bypasses state-of-the-art watermark defenses across various techniques, models, and datasets, exposing a significant security gap. In response, the paper proposes a novel and more robust watermarking method named WET (Watermarking EaaS with Linear Transformation). WET applies a linear transformation to embeddings and verifies watermarks by reversing this transformation and measuring similarity to the original embeddings. Experimental results confirm that WET maintains near-perfect watermark verifiability and shows strong resistance against paraphrasing-based watermark removal attacks. Additionally, detailed ablation studies are performed to analyze the influence of different components and hyperparameters in WET’s design, highlighting the importance of each element in achieving its robustness. <div>
arXiv:2512.03079v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities in natural language understanding and generation. Based on these LLMs, businesses have started to provide Embeddings-as-a-Service (EaaS), offering feature extraction capabilities (in the form of text embeddings) that benefit downstream natural language processing tasks. However, prior research has demonstrated that EaaS is vulnerable to imitation attacks, where an attacker clones the service's model in a black-box manner without access to the model's internal workings. In response, watermarks have been added to the text embeddings to protect the intellectual property of EaaS providers by allowing them to check for model ownership. This thesis focuses on defending against imitation attacks by investigating EaaS watermarks. To achieve this goal, we unveil novel attacks and propose and validate new watermarking techniques.
  Firstly, we show that existing EaaS watermarks can be removed through paraphrasing the input text when attackers clone the model during imitation attacks. Our study illustrates that paraphrasing can effectively bypass current state-of-the-art EaaS watermarks across various attack setups (including different paraphrasing techniques and models) and datasets in most instances. This demonstrates a new vulnerability in recent EaaS watermarking techniques.
  Subsequently, as a countermeasure, we propose a novel watermarking technique, WET (Watermarking EaaS with Linear Transformation), which employs linear transformation of the embeddings. Watermark verification is conducted by applying a reverse transformation and comparing the similarity between recovered and original embeddings. We demonstrate its robustness against paraphrasing attacks with near-perfect verifiability. We conduct detailed ablation studies to assess the significance of each component and hyperparameter in WET.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Alleviating Choice Supportive Bias in LLM with Reasoning Dependency Generation</title>
<link>https://arxiv.org/abs/2512.03082</link>
<guid>https://arxiv.org/abs/2512.03082</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Choice-Supportive Bias, Cognitive Bias, Reasoning Dependency Generation, Debiasing

<br /><br />Summary:  
1. Recent research identifies that some Large Language Models (LLMs) exhibit choice-supportive bias (CSB), where the models favor previously chosen options, undermining the objectivity of AI decision-making.  
2. Current debiasing methods mainly address demographic and social biases but fail to target cognitive biases like CSB in LLMs.  
3. This work introduces Reasoning Dependency Generation (RDG), a novel framework to mitigate CSB by generating unbiased reasoning datasets used for fine-tuning LLMs.  
4. RDG constructs balanced question-answer (QA) pairs that carefully model and unmodel dependencies between choices, evidence, and justifications, producing Contextual Dependency Data and Dependency Decouple Data across multiple domains.  
5. Experimental results demonstrate that LLMs fine-tuned on RDG-generated data achieve an 81.5% improvement in memory-based tasks and a 94.3% improvement in evaluation-based experiments, without sacrificing performance on standard benchmarks like BBQ.  
6. This approach pioneers the first effective solution for addressing cognitive biases in LLMs, advancing the reliability of AI-assisted decision support systems. <div>
arXiv:2512.03082v1 Announce Type: new 
Abstract: Recent studies have demonstrated that some Large Language Models exhibit choice-supportive bias (CSB) when performing evaluations, systematically favoring their chosen options and potentially compromising the objectivity of AI-assisted decision making. While existing debiasing approaches primarily target demographic and social biases, methods for addressing cognitive biases in LLMs remain largely unexplored. In this work, we present the first solution to address CSB through Reasoning Dependency Generation (RDG), a novel framework for generating unbiased reasoning data to mitigate choice-supportive bias through fine-tuning. RDG automatically constructs balanced reasoning QA pairs, explicitly (un)modeling the dependencies between choices, evidences, and justifications. Our approach is able to generate a large-scale dataset of QA pairs across domains, incorporating Contextual Dependency Data and Dependency Decouple Data. Experiments show that LLMs fine-tuned on RDG-generated data demonstrate a 81.5% improvement in memory-based experiments and 94.3% improvement in the evaluation-based experiment, while maintaining similar performance on standard BBQ benchmarks. This work pioneers an approach for addressing cognitive biases in LLMs and contributes to the development of more reliable AI-assisted decision support systems.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Job Matching: Occupation, Skill and Qualification Linking with the ESCO and EQF taxonomies</title>
<link>https://arxiv.org/abs/2512.03195</link>
<guid>https://arxiv.org/abs/2512.03195</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, labor market classification, ESCO taxonomy, EQF framework, job vacancy texts<br /><br />Summary:<br /><br />This study explores the use of language models to enhance the classification of labor market information by linking job vacancy texts to two key European frameworks: the European Skills, Competences, Qualifications and Occupations (ESCO) taxonomy and the European Qualifications Framework (EQF). It compares two main approaches from existing research—Sentence Linking and Entity Linking—and integrates them into an open-source tool released to support further work in labor market classification and employment discourse analysis. To deepen skill extraction beyond surface-level methods, the authors have developed two annotated datasets specifically created to evaluate how occupations and qualifications are represented in job vacancy texts. The study also investigates different ways to harness generative large language models for this classification task. Overall, the findings advance the state of the art in job entity extraction and provide computational infrastructure that aids in analyzing work, skills, and labor market narratives within a digital economy. The open-source code for the tool is publicly available, encouraging ongoing research and development in this domain. <div>
arXiv:2512.03195v1 Announce Type: new 
Abstract: This study investigates the potential of language models to improve the classification of labor market information by linking job vacancy texts to two major European frameworks: the European Skills, Competences, Qualifications and Occupations (ESCO) taxonomy and the European Qualifications Framework (EQF). We examine and compare two prominent methodologies from the literature: Sentence Linking and Entity Linking. In support of ongoing research, we release an open-source tool, incorporating these two methodologies, designed to facilitate further work on labor classification and employment discourse. To move beyond surface-level skill extraction, we introduce two annotated datasets specifically aimed at evaluating how occupations and qualifications are represented within job vacancy texts. Additionally, we examine different ways to utilize generative large language models for this task. Our findings contribute to advancing the state of the art in job entity extraction and offer computational infrastructure for examining work, skills, and labor market narratives in a digitally mediated economy. Our code is made publicly available: https://github.com/tabiya-tech/tabiya-livelihoods-classifier
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InvertiTune: High-Quality Data Synthesis for Cost-Effective Single-Shot Text-to-Knowledge Graph Generation</title>
<link>https://arxiv.org/abs/2512.03197</link>
<guid>https://arxiv.org/abs/2512.03197</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Text2KG, supervised fine-tuning, data generation pipeline, knowledge graphs<br /><br />Summary:<br /><br />1. The paper addresses limitations in current Text2KG methods that rely heavily on iterative prompting with Large Language Models (LLMs), which are computationally costly and often miss complex relations within text.  
2. To overcome these challenges, the authors introduce InvertiTune, a framework that combines a controlled data generation pipeline with supervised fine-tuning (SFT) to improve efficiency and accuracy.  
3. The data generation pipeline systematically extracts subgraphs from large knowledge bases, applies noise filtering, and uses LLMs to generate natural language descriptions from these subgraphs, leveraging the LLMs' strengths in text generation rather than direct knowledge graph extraction.  
4. This method generates datasets with longer, more realistic texts paired with larger and more complex knowledge graphs, enhancing the training process for lightweight models aimed at single-shot KG construction.  
5. Experimental evaluations on the newly created CE12k dataset demonstrate that InvertiTune surpasses larger, non-fine-tuned LLMs and current state-of-the-art Text2KG techniques, also showing superior generalization on the CrossEval-1200 test set composed of multiple benchmarks.  
6. The study highlights the critical role of high-quality, realistic training data in advancing efficient and high-performing Text2KG systems. <div>
arXiv:2512.03197v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have revolutionized the ability to understand and generate text, enabling significant progress in automatic knowledge graph construction from text (Text2KG). Many Text2KG methods, however, rely on iterative LLM prompting, making them computationally expensive and prone to overlooking complex relations distributed throughout the text. To address these limitations, we propose InvertiTune, a framework that combines a controlled data generation pipeline with supervised fine-tuning (SFT). Within this framework, the data-generation pipeline systematically extracts subgraphs from large knowledge bases, applies noise filtering, and leverages LLMs to generate corresponding natural text descriptions, a task more aligned with LLM capabilities than direct KG generation from text. This pipeline enables generating datasets composed of longer texts paired with larger KGs that better reflect real-world scenarios compared to existing benchmarks, thus supporting effective SFT of lightweight models for single-shot KG construction. Experimental results on CE12k, a dataset generated using the introduced pipeline, show that InvertiTune outperforms larger non-fine-tuned LLMs as well as state-of-the-art Text2KG approaches, while also demonstrating stronger cross-dataset generalization on CrossEval-1200, a test set created from three established benchmark datasets and CE12k. These findings highlight the importance of realistic, high-quality training data for advancing efficient and high-performing Text2KG systems.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Identifying attributions of causality in political text</title>
<link>https://arxiv.org/abs/2512.03214</link>
<guid>https://arxiv.org/abs/2512.03214</guid>
<content:encoded><![CDATA[
<div> Keywords: explanations, political science, causal language model, cause-effect pairs, text analysis<br /><br />Summary:<br /><br />1. The paper addresses the importance of explanations in understanding political events, highlighting how citizens use explanations to make sense of causes, responsibilities, and potential solutions.<br />2. It identifies a gap in political science where explanations have not been systematically analyzed and current methods are fragmented and often specific to particular issues.<br />3. The author introduces a novel framework designed to detect and parse explanations within political text by extracting causal claims.<br />4. This framework involves training a lightweight causal language model that outputs structured data consisting of cause-effect pairs, which enables subsequent analytic tasks.<br />5. The study demonstrates the practicality of examining causal explanations at scale, showing that the method requires minimal annotated data, is generalizable across contexts, and achieves accuracy comparable to human coding. <div>
arXiv:2512.03214v1 Announce Type: new 
Abstract: Explanations are a fundamental element of how people make sense of the political world. Citizens routinely ask and answer questions about why events happen, who is responsible, and what could or should be done differently. Yet despite their importance, explanations remain an underdeveloped object of systematic analysis in political science, and existing approaches are fragmented and often issue-specific. I introduce a framework for detecting and parsing explanations in political text. To do this, I train a lightweight causal language model that returns a structured data set of causal claims in the form of cause-effect pairs for downstream analysis. I demonstrate how causal explanations can be studied at scale, and show the method's modest annotation requirements, generalizability, and accuracy relative to human coding.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Randomized Masked Finetuning: An Efficient Way to Mitigate Memorization of PIIs in LLMs</title>
<link>https://arxiv.org/abs/2512.03310</link>
<guid>https://arxiv.org/abs/2512.03310</guid>
<content:encoded><![CDATA[
<div> memorization, privacy, large language models, fine-tuning, data deduplication<br /><br />Summary:  
The article addresses the critical issue of memorization of personally identifying information (PII) in Natural Language Models, particularly Large Language Models (LLMs), which raises significant security and privacy concerns. It introduces a novel privacy-preserving fine-tuning technique called Randomized Masked Fine-Tuning (RMFT) designed to reduce the memorization of PII while maintaining model performance. The study uses the Enron Email Dataset to evaluate RMFT, demonstrating that the method achieves an 80.81% reduction in Total Extraction Rate and an 80.17% reduction in Seen Extraction Rate compared to standard fine-tuning methods. RMFT also outperforms traditional deduplication-based privacy techniques. Despite these privacy gains, RMFT incurs only a minimal performance impact, with just a 5.73% increase in perplexity, indicating that it retains general language model utility. Additionally, the authors introduce MaxTER, a Pareto-optimal evaluation framework, which allows for assessing the tradeoffs between privacy and utility in language models. Using the MaxTER framework, RMFT’s performance is benchmarked against deduplication methods, employing the Area Under The Response Curve (AURC) metric to highlight superior privacy-utility balance. Overall, the research presents a promising approach to improving privacy in LLM fine-tuning without significantly compromising model effectiveness. <div>
arXiv:2512.03310v1 Announce Type: new 
Abstract: The current literature on memorization in Natural Language Models, especially Large Language Models (LLMs), poses severe security and privacy risks, as models tend to memorize personally identifying information (PIIs) from training data. We introduce Randomized Masked Fine-Tuning (RMFT), a novel privacy-preserving fine-tuning technique that reduces PII memorization while minimizing performance impact. Using the Enron Email Dataset, we demonstrate that RMFT achieves an 80.81% reduction in Total Extraction Rate and 80.17% reduction in Seen Extraction Rate compared to baseline fine-tuning, outperforming deduplication methods while maintaining only a 5.73% increase in perplexity. We present MaxTER, a Pareto-optimal evaluation framework for assessing privacy-utility tradeoffs, and show the performance of RMFT vs Deduplication by Area Under The Response Curve (AURC) metric.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modeling Topics and Sociolinguistic Variation in Code-Switched Discourse: Insights from Spanish-English and Spanish-Guaran\'i</title>
<link>https://arxiv.org/abs/2512.03334</link>
<guid>https://arxiv.org/abs/2512.03334</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM-assisted annotation, bilingual discourse, code-switching, sociolinguistic patterns, Spanish-Guaraní<br /><br />Summary:<br /><br />This study introduces a large language model (LLM)-assisted annotation pipeline designed for analyzing sociolinguistic and topical aspects of bilingual discourse in two distinct language pairs: Spanish-English and Spanish-Guaraní. It leverages LLMs to automatically label topic, genre, and discourse-pragmatic functions across 3,691 code-switched sentences. The pipeline integrates rich demographic metadata from the Miami Bilingual Corpus and adds new topic annotations to the Spanish-Guaraní dataset for enhanced analysis. The results reveal systematic correlations in the Miami data linking gender, language dominance, and discourse functions. In the Paraguayan Spanish-Guaraní data, a clear diglossic split is observed, with formal use of Guaraní contrasting informal Spanish speech. These findings replicate and extend prior interactional and sociolinguistic insights by providing corpus-scale quantitative evidence. Importantly, the study demonstrates that LLMs can reliably produce interpretable sociolinguistic patterns that traditionally required extensive manual annotation. This work advances computational approaches for cross-linguistic research, especially benefiting analyses involving low-resource bilingual contexts, showing potential to scale and enrich sociolinguistic research using artificial intelligence tools. <div>
arXiv:2512.03334v1 Announce Type: new 
Abstract: This study presents an LLM-assisted annotation pipeline for the sociolinguistic and topical analysis of bilingual discourse in two typologically distinct contexts: Spanish-English and Spanish-Guaran\'i. Using large language models, we automatically labeled topic, genre, and discourse-pragmatic functions across a total of 3,691 code-switched sentences, integrated demographic metadata from the Miami Bilingual Corpus, and enriched the Spanish-Guaran\'i dataset with new topic annotations. The resulting distributions reveal systematic links between gender, language dominance, and discourse function in the Miami data, and a clear diglossic division between formal Guaran\'i and informal Spanish in Paraguayan texts. These findings replicate and extend earlier interactional and sociolinguistic observations with corpus-scale quantitative evidence. The study demonstrates that large language models can reliably recover interpretable sociolinguistic patterns traditionally accessible only through manual annotation, advancing computational methods for cross-linguistic and low-resource bilingual research.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PERCS: Persona-Guided Controllable Biomedical Summarization Dataset</title>
<link>https://arxiv.org/abs/2512.03340</link>
<guid>https://arxiv.org/abs/2512.03340</guid>
<content:encoded><![CDATA[
<div> Keywords: medical text simplification, persona-guided summarization, biomedical abstracts, health literacy, controllable summarization

<br /><br />Summary:  
Automatic medical text simplification is crucial for enhancing health literacy by making complex biomedical research accessible to various reader groups. Current resources typically target a single generic audience, which fails to address the diverse medical literacy levels and information needs present in different user groups. To overcome this, the study introduces PERCS (Persona-guided Controllable Summarization), a novel dataset consisting of biomedical abstracts paired with summaries tailored to four distinct personas: Laypersons, Premedical Students, Non-medical Researchers, and Medical Experts. These personas reflect varying degrees of medical knowledge and information requirements, highlighting the importance of audience-specific summarization. Each summary in the PERCS dataset was reviewed by physicians to ensure factual accuracy and alignment with the intended persona, using a detailed error taxonomy. Technical analyses reveal significant differences in readability, vocabulary, and depth of content tailored for each persona. The paper also benchmarks four large language models on this dataset using automatic evaluation metrics that measure comprehensiveness, readability, and faithfulness, providing baseline results for future studies. The dataset, along with annotation guidelines and evaluation tools, is publicly available to facilitate research on persona-specific communication and controllable summarization in the biomedical domain. <div>
arXiv:2512.03340v1 Announce Type: new 
Abstract: Automatic medical text simplification plays a key role in improving health literacy by making complex biomedical research accessible to diverse readers. However, most existing resources assume a single generic audience, overlooking the wide variation in medical literacy and information needs across user groups. To address this limitation, we introduce PERCS (Persona-guided Controllable Summarization), a dataset of biomedical abstracts paired with summaries tailored to four personas: Laypersons, Premedical Students, Non-medical Researchers, and Medical Experts. These personas represent different levels of medical literacy and information needs, emphasizing the need for targeted, audience-specific summarization. Each summary in PERCS was reviewed by physicians for factual accuracy and persona alignment using a detailed error taxonomy. Technical validation shows clear differences in readability, vocabulary, and content depth across personas. Along with describing the dataset, we benchmark four large language models on PERCS using automatic evaluation metrics that assess comprehensiveness, readability, and faithfulness, establishing baseline results for future research. The dataset, annotation guidelines, and evaluation materials are publicly available to support research on persona-specific communication and controllable biomedical summarization.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Idea-Gated Transformers: Enforcing Semantic Coherence via Differentiable Vocabulary Pruning</title>
<link>https://arxiv.org/abs/2512.03343</link>
<guid>https://arxiv.org/abs/2512.03343</guid>
<content:encoded><![CDATA[
<div> Keywords: Autoregressive Language Models, Topic Drift, Idea-Gated Transformer, Concept Vector, Differentiable Gating

<br /><br />Summary: Autoregressive Language Models (LLMs) trained on Next-Token Prediction (NTP) often experience "Topic Drift," where generated text deviates from the initial prompt due to reliance on local token associations rather than broader semantic planning. Although increasing model size helps mitigate this issue, the underlying limitation of NTP remains unaddressed. This paper introduces the Idea-Gated Transformer, a novel architecture designed to decouple semantic planning from syntactic generation. A new auxiliary component, the "Idea Head," is trained to predict the bag-of-words distribution of a future context window, producing a latent "Concept Vector." This vector dynamically gates the main vocabulary during generation by means of a differentiable gating mechanism that suppresses semantically irrelevant tokens, effectively narrowing the search space in real time. Experiments conducted on the WikiText-103 dataset show that the Idea-Gated model maintains comparable validation perplexity relative to a standard GPT-2 baseline while significantly improving Domain Retention, meaning it better preserves the topical coherence of the generation. Both qualitative and quantitative evaluations indicate that the gating mechanism successfully confines output to specific semantic clusters (e.g., Finance, Science), effectively resisting associative topic drift. This approach offers a parameter-efficient method for achieving more controllable and semantically coherent language modeling. <div>
arXiv:2512.03343v1 Announce Type: new 
Abstract: Autoregressive Language Models (LLMs) trained on Next-Token Prediction (NTP) often suffer from ``Topic Drift'' where the generation wanders away from the initial prompt due to a reliance on local associations rather than global planning \citep{holtzman2019curious}. While scaling model size mitigates this \citep{brown2020language}, the fundamental myopia of the NTP objective remains. In this work, we introduce the Idea-Gated Transformer, a novel architecture that separates semantic planning from syntactic generation. We introduce an auxiliary ``Idea Head'' trained to predict the bag-of-words distribution for a future context window, creating a latent ``Concept Vector'' that actively gates the main vocabulary during generation. We propose a differentiable gating mechanism that suppresses semantically irrelevant tokens, effectively pruning the search space in real-time. Experiments on WikiText-103 demonstrate that while the Idea-Gated model achieves comparable validation perplexity to a standard GPT-2 baseline, it exhibits significantly superior Domain Retention. Qualitative and quantitative analysis reveals that the gating mechanism successfully locks generation into specific semantic clusters (e.g., Finance, Science) and resists associative drift, offering a parameter-efficient path toward more controllable language modeling.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Hypothesis to Premises: LLM-based Backward Logical Reasoning with Selective Symbolic Translation</title>
<link>https://arxiv.org/abs/2512.03360</link>
<guid>https://arxiv.org/abs/2512.03360</guid>
<content:encoded><![CDATA[
<div> Keywords: logical reasoning, backward reasoning, symbolic translation, hypothesis-driven, large language models  

<br /><br />Summary:  
This paper addresses challenges in logical reasoning within natural language understanding, particularly the inefficiencies and inaccuracies in existing forward reasoning approaches that generate step-by-step rationales from premises to conclusions. The authors propose a novel framework called Hypothesis-driven Backward Logical Reasoning (HBLR), which innovatively combines confidence-aware symbolic translation with backward reasoning inspired by human deductive thinking. During translation, only high-confidence text spans are converted into formal logic such as First-Order Logic (FOL), while uncertain information remains in natural language to minimize translation errors. Additionally, a translation reflection module evaluates the symbolic outputs and reverts any lossy translations back to text to ensure semantic fidelity. In the reasoning phase, HBLR assumes the conclusion as true and recursively verifies its premises, mimicking the way humans reason backward from hypotheses. A reasoning reflection module detects and corrects flawed inference steps to improve logical coherence. Extensive experiments across five reasoning benchmarks show that HBLR consistently outperforms strong existing baselines in both accuracy and computational efficiency, highlighting its effectiveness in enhancing AI’s logical reasoning capabilities. <div>
arXiv:2512.03360v1 Announce Type: new 
Abstract: Logical reasoning is a core challenge in natural language understanding and a fundamental capability of artificial intelligence, underpinning scientific discovery, mathematical theorem proving, and complex decision-making. Despite the remarkable progress of large language models (LLMs), most current approaches still rely on forward reasoning paradigms, generating step-by-step rationales from premises to conclusions. However, such methods often suffer from redundant inference paths, hallucinated steps, and semantic drift, resulting in inefficient and unreliable reasoning. In this paper, we propose a novel framework, Hypothesis-driven Backward Logical Reasoning (HBLR). The core idea is to integrate confidence-aware symbolic translation with hypothesis-driven backward reasoning. In the translation phase, only high-confidence spans are converted into logical form, such as First-Order Logic (FOL), while uncertain content remains in natural language. A translation reflection module further ensures semantic fidelity by evaluating symbolic outputs and reverting lossy ones back to text when necessary. In the reasoning phase, HBLR simulates human deductive thinking by assuming the conclusion is true and recursively verifying its premises. A reasoning reflection module further identifies and corrects flawed inference steps, enhancing logical coherence. Extensive experiments on five reasoning benchmarks demonstrate that HBLR consistently outperforms strong baselines in both accuracy and efficiency.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nexus: Higher-Order Attention Mechanisms in Transformers</title>
<link>https://arxiv.org/abs/2512.03377</link>
<guid>https://arxiv.org/abs/2512.03377</guid>
<content:encoded><![CDATA[
<div> Keywords: Higher-Order Attention, Recursive Attention, Transformers, Self-Attention, Parameter Efficiency<br /><br />Summary:<br /><br />This paper introduces the Higher-Order Attention Network (Hon), a novel Transformer architecture aimed at overcoming the low-rank bottleneck of standard first-order self-attention mechanisms. Traditional Transformers use static linear projections for Queries and Keys, which limits their ability to capture complex, multi-hop relationships within a single layer. Hon addresses this by recursively refining Query and Key vectors through nested self-attention loops, enabling tokens to aggregate global context and model higher-order correlations before the final attention calculation. A key innovation is the parameter-efficient weight-sharing strategy applied across recursive steps, which enhances representational power without increasing parameter count, thus maintaining an $\mathcal{O}(1)$ parameter overhead. The authors provide theoretical analysis demonstrating that their approach breaks the inherent linear bottleneck present in standard attention mechanisms. Experimental results on multiple benchmarks show that Hon consistently outperforms standard Transformer models, validating both its theoretical advantages and empirical effectiveness. Overall, Hon represents a significant advance in attention mechanism design by merging recursive structures with weight sharing to create deeper, more expressive contextual embeddings within the Transformer framework. <div>
arXiv:2512.03377v1 Announce Type: new 
Abstract: Transformers have achieved significant success across various domains, relying on self-attention to capture dependencies. However, the standard first-order attention mechanism is often limited by a low-rank bottleneck, struggling to capture intricate, multi-hop relationships within a single layer. In this paper, we propose the \textbf{Higher-Order Attention Network (Hon)}, a novel architecture designed to enhance representational power through a recursive framework. Unlike standard approaches that use static linear projections for Queries and Keys, Hon dynamically refines these representations via nested self-attention mechanisms. Specifically, the Query and Key vectors are themselves outputs of inner attention loops, allowing tokens to aggregate global context and model high-order correlations \textit{prior} to the final attention computation. We enforce a parameter-efficient weight-sharing strategy across recursive steps, ensuring that this enhanced expressivity incurs $\mathcal{O}(1)$ additional parameters. We provide theoretical analysis demonstrating that our method breaks the linear bottleneck of standard attention. Empirically, Hon outperforms standard Transformers on multiple benchmarks.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Characterizing Language Use in a Collaborative Situated Game</title>
<link>https://arxiv.org/abs/2512.03381</link>
<guid>https://arxiv.org/abs/2512.03381</guid>
<content:encoded><![CDATA[
<div> Keywords: cooperative video games, dialogue corpus, Portal 2, spatial reference, collaborative problem-solving

<br /><br />Summary:  
The paper introduces the Portal Dialogue Corpus, a dataset capturing 11.5 hours of spoken human dialogue during cooperative gameplay in Portal 2, a virtual puzzle game. This corpus includes 24,500 utterances collected from players communicating and coordinating under uncertainty in a complex environment. The authors analyze the linguistic features of the dialogues, highlighting phenomena rarely seen in typical chitchat or task-oriented dialogue datasets, such as complex spatial referencing, clarification and repair strategies, and the formation of ad-hoc conventions between players. To facilitate further research on language use within situated, collaborative problem-solving contexts, the corpus is made publicly available. The released data consists not only of audio and transcripts but also player videos, game state information, and both manual and automatic annotations of the dialogue. This comprehensive resource aims to support the study of multi-modal communication and language behavior in interactive, cooperative game settings. <div>
arXiv:2512.03381v1 Announce Type: new 
Abstract: Cooperative video games, where multiple participants must coordinate by communicating and reasoning under uncertainty in complex environments, yield a rich source of language data. We collect the Portal Dialogue Corpus: a corpus of 11.5 hours of spoken human dialogue in the co-op mode of the popular Portal 2 virtual puzzle game, comprising 24.5K total utterances. We analyze player language and behavior, identifying a number of linguistic phenomena that rarely appear in most existing chitchat or task-oriented dialogue corpora, including complex spatial reference, clarification and repair, and ad-hoc convention formation. To support future analyses of language use in complex, situated, collaborative problem-solving scenarios, we publicly release the corpus, which comprises player videos, audio, transcripts, game state data, and both manual and automatic annotations of language data.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual LoRA: Enhancing LoRA with Magnitude and Direction Updates</title>
<link>https://arxiv.org/abs/2512.03402</link>
<guid>https://arxiv.org/abs/2512.03402</guid>
<content:encoded><![CDATA[
<div> Low-rank adaptation, Dual LoRA, parameter-efficient fine-tuning, inductive bias, large language models  

<br /><br />Summary:  
This paper addresses limitations of Low-rank adaptation (LoRA), a popular parameter-efficient fine-tuning method used to adapt large language models (LLMs) for specific downstream NLP tasks. The authors identify that LoRA's low-rank assumption can lead to suboptimal performance. To overcome this, they propose Dual LoRA, a novel technique introducing an inductive bias into the original LoRA framework. Dual LoRA separates the low-rank parameter matrices into two distinct groups: a magnitude group which determines if and how far parameters should be updated, and a direction group which decides the update's direction—forward or backward. This separation better simulates gradient-based full fine-tuning processes. The implementation is straightforward, involving the application of a ReLU function to the magnitude group and a sign function to the direction group. Extensive experiments are conducted across a variety of NLP tasks—natural language generation, understanding, and commonsense reasoning—using multiple base models including GPT-2, RoBERTa, DeBERTa, and LLaMA versions 1, 2, and 3. Results demonstrate that Dual LoRA consistently outperforms the original LoRA and its state-of-the-art variants while maintaining the same number of trainable parameters, indicating a more effective and efficient fine-tuning approach. <div>
arXiv:2512.03402v1 Announce Type: new 
Abstract: Low-rank adaptation (LoRA) is one of the most popular methods among parameter-efficient fine-tuning (PEFT) methods to adapt pre-trained large language models (LLMs) to specific downstream tasks. However, the model trained based on LoRA often has an unsatisfactory performance due to its low-rank assumption. In this paper, we propose a novel method called Dual LoRA to improve the performance by incorporating an inductive bias into the original LoRA. Specifically, we separate low-rank matrices into two groups: the magnitude group to control whether or not and how far we should update a parameter and the direction group to decide whether this parameter should move forward or backward, to better simulate the parameter updating process of the full fine-tuning based on gradient-based optimization algorithms. We show that this can be simply achieved by adding a ReLU function to the magnitude group and a sign function to the direction group. We conduct several experiments over a wide range of NLP tasks, including natural language generation (NLG), understanding (NLU), and commonsense reasoning datasets on GPT-2, RoBERTa, DeBERTa, and LLaMA-1/2/3 as baseline models. The results show that we consistently outperform LoRA and its state-of-the-art variants with the same number of trainable parameters.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PretrainZero: Reinforcement Active Pretraining</title>
<link>https://arxiv.org/abs/2512.03442</link>
<guid>https://arxiv.org/abs/2512.03442</guid>
<content:encoded><![CDATA[
<div> Keywords: PretrainZero, reinforcement learning, active pretraining, self-supervised learning, general reasoning<br /><br />Summary:  
1) The paper presents PretrainZero, a novel reinforcement active learning framework designed to move reinforcement learning (RL) from domain-specific post-training towards more general pretraining.  
2) PretrainZero mimics human active learning by developing a unified reasoning policy that identifies and predicts informative content within the pretraining corpus, enabling active pretraining.  
3) Unlike traditional RL methods that depend on verifiable rewards or supervised fine-tuning, PretrainZero employs self-supervised learning directly on base models ranging from 3 to 30 billion parameters using only the general Wikipedia corpus, eliminating the need for labeled data.  
4) The framework introduces verification scaling by addressing progressively harder masked span prediction tasks, which significantly enhances the general reasoning ability of pretrained models.  
5) Empirical results show that PretrainZero substantially boosts performance on multiple benchmarks, including MMLU-Pro, SuperGPQA, and math problem averages, with improvements of 8.43, 5.96, and 10.60 points respectively on Qwen3-4B-Base models. Furthermore, models pretrained with PretrainZero can serve as foundational reasoning models for downstream reinforcement learning with verification and reward (RLVR) tasks, demonstrating broad applicability. <div>
arXiv:2512.03442v1 Announce Type: new 
Abstract: Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1) Active pretraining: inspired by the active learning ability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL. 2) Self-supervised learning: without any verifiable labels, pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improves Qwen3-4B-Base for 8.43, 5.96 and 10.60 on MMLU-Pro, SuperGPQA and math average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstream RLVR tasks.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Preliminary Study on the Promises and Challenges of Native Top-$k$ Sparse Attention</title>
<link>https://arxiv.org/abs/2512.03494</link>
<guid>https://arxiv.org/abs/2512.03494</guid>
<content:encoded><![CDATA[
<div> Top-k Attention, Large Language Models, Decoding, Training, Entropy<br /><br />Summary:<br /><br />1. This report examines the Top-k Attention mechanism in Large Language Models (LLMs), focusing on its application during both decoding and training phases to address computational cost challenges in long-context modeling.<br />2. Extensive experiments validate the effectiveness of exact Top-k Decoding, showing that keeping only the most relevant Keys as context during decoding achieves performance comparable to or better than full attention on tasks such as HELMET and LongBench v2.<br />3. The study proposes a native Top-k Attention training strategy that aligns training and inference operations, which leads to improved unlocking of Top-k Decoding’s full potential and significantly enhanced model performance.<br />4. Due to the high complexity of exact Top-k Attention, the report investigates approximate Top-k algorithms, finding a positive correlation between downstream task performance and the precision of approximation, supported by precision statistics of the Lightning Indexer in DeepSeek-V3.2-Exp.<br />5. A theoretical interpretation from an entropy perspective reveals that models fine-tuned with Top-k Attention show reduced entropy in downstream tasks, supporting the hypothesis that low-entropy states are better suited for Top-k Decoding. <div>
arXiv:2512.03494v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly prevalent in the field of long-context modeling, however, their inference computational costs have become a critical bottleneck hindering the advancement of tasks such as agents and multimodal applications. This report conducts a preliminary investigation into the effectiveness and theoretical mechanisms of the Top-$k$ Attention mechanism during both the decoding and training phases. First, we validate the effectiveness of exact Top-$k$ Decoding through extensive experimentation. Experiments demonstrate that retaining only the pivotal Keys with the highest similarity to the Query as the context window during the decoding stage achieves performance comparable to, or even surpassing, full attention on downstream tasks such as HELMET and LongBench v2. Second, we further explore the native Top-$k$ Attention training strategy. Experiments confirm that ensuring the consistency between training and inference regarding Top-$k$ Attention operations facilitates the further unlocking of Top-$k$ Decoding's potential, thereby significantly enhancing model performance. Furthermore, considering the high computational complexity of exact Top-$k$ Attention, we investigate the impact of approximate Top-$k$ algorithm precision on downstream tasks. Our research confirms a positive correlation between downstream task performance and approximation fidelity, and we provide statistical evaluations of the Lightning Indexer's precision within the DeepSeek-V3.2-Exp model. Finally, this report provides a theoretical interpretation from the perspective of Entropy. Experimental observations indicate that models subjected to Top-$k$ Attention SFT exhibit a distinct phenomenon of entropy reduction in downstream tasks, which validates the hypothesis that low-entropy states are better adapted to Top-$k$ Decoding.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding LLM Reasoning for Abstractive Summarization</title>
<link>https://arxiv.org/abs/2512.03503</link>
<guid>https://arxiv.org/abs/2512.03503</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, abstractive summarization, reasoning strategies, summary quality, factual faithfulness  

<br /><br />Summary:  
This study investigates the effectiveness of Large Language Models (LLMs) in abstractive summarization, a task where their reasoning capabilities have been assumed useful but not thoroughly validated. The authors adapt general reasoning strategies specifically for summarization and evaluate 8 distinct reasoning strategies across 3 Large Reasoning Models (LRMs) and 8 diverse datasets. They measure both the quality of summaries produced and their factual faithfulness. The findings reveal that reasoning approaches do not universally enhance summarization performance; their success varies significantly depending on the chosen reasoning strategy and contextual factors. A central observation is a trade-off between fluency and factual accuracy: explicit reasoning strategies improve the readability and flow of summaries but tend to reduce factual grounding, whereas implicit reasoning within LRMs tends to produce more factually consistent summaries but with less fluency. Additionally, increasing an LRM's internal reasoning effort, referred to as the reasoning budget, does not lead to better factual consistency and can even harm it. This suggests that effective abstractive summarization requires faithful compression of information rather than excessive or overly complex internal reasoning processes. <div>
arXiv:2512.03503v1 Announce Type: new 
Abstract: While the reasoning capabilities of Large Language Models (LLMs) excel in analytical tasks such as mathematics and code generation, their utility for abstractive summarization remains widely assumed but largely unverified. To bridge this gap, we first tailor general reasoning strategies to the summarization domain. We then conduct a systematic, large scale comparative study of 8 reasoning strategies and 3 Large Reasoning Models (LRMs) across 8 diverse datasets, assessing both summary quality and faithfulness. Our findings show that reasoning is not a universal solution and its effectiveness is highly dependent on the specific strategy and context. Specifically, we observe a trade-off between summary quality and factual faithfulness: explicit reasoning strategies tend to improve fluency at the expense of factual grounding, while implicit reasoning in LRMs exhibits the inverse pattern. Furthermore, increasing an LRM's internal reasoning budget does not improve, and can even hurt, factual consistency, suggesting that effective summarization demands faithful compression rather than creative over-thinking.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-grained Narrative Classification in Biased News Articles</title>
<link>https://arxiv.org/abs/2512.03582</link>
<guid>https://arxiv.org/abs/2512.03582</guid>
<content:encoded><![CDATA[
<div> Keywords: Propaganda, Narrative Classification, Indian News Media, Ideological Bias, Persuasive Techniques  

<br /><br />Summary:  
1. The paper addresses the role of narratives as cognitive and emotional frameworks in propaganda, structuring persuasive techniques into coherent stories to justify actions, assign blame, and foster ideological identification.  
2. It introduces a novel, fine-grained narrative classification scheme tailored for biased news articles, focusing on understanding article-level bias as a foundational step for narrative classification and identifying persuasive techniques.  
3. The authors present INDI-PROP, the first ideologically grounded, multi-level annotated dataset consisting of 1,266 Indian news articles that cover two divisive socio-political events: the Citizenship Amendment Act (CAA) and the Farmers' protest.  
4. Each article is annotated at three hierarchical levels: ideological bias of the article (pro-government, pro-opposition, neutral), event-specific narrative frames linked to ideological stances and communicative intent, and detailed persuasive techniques used.  
5. To perform classification tasks at these three levels, the paper proposes two frameworks guided by GPT-4o-mini: FANTA, which integrates multi-layered communicative information through information extraction and contextual framing for hierarchical reasoning, and TPTC, which uses a two-stage decomposition approach to identify persuasive cues systematically.  
6. Experimental results demonstrate significant improvements of these models over baseline methods in classifying bias, narrative, and persuasive techniques within biased news content. <div>
arXiv:2512.03582v1 Announce Type: new 
Abstract: Narratives are the cognitive and emotional scaffolds of propaganda. They organize isolated persuasive techniques into coherent stories that justify actions, attribute blame, and evoke identification with ideological camps. In this paper, we propose a novel fine-grained narrative classification in biased news articles. We also explore article-bias classification as the precursor task to narrative classification and fine-grained persuasive technique identification. We develop INDI-PROP, the first ideologically grounded fine-grained narrative dataset with multi-level annotation for analyzing propaganda in Indian news media. Our dataset INDI-PROP comprises 1,266 articles focusing on two polarizing socio-political events in recent times: CAA and the Farmers' protest. Each article is annotated at three hierarchical levels: (i) ideological article-bias (pro-government, pro-opposition, neutral), (ii) event-specific fine-grained narrative frames anchored in ideological polarity and communicative intent, and (iii) persuasive techniques. We propose FANTA and TPTC, two GPT-4o-mini guided multi-hop prompt-based reasoning frameworks for the bias, narrative, and persuasive technique classification. FANTA leverages multi-layered communicative phenomena by integrating information extraction and contextual framing for hierarchical reasoning. On the other hand, TPTC adopts systematic decomposition of persuasive cues via a two-stage approach. Our evaluation suggests substantial improvement over underlying baselines in each case.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AlignCheck: a Semantic Open-Domain Metric for Factual Consistency Assessment</title>
<link>https://arxiv.org/abs/2512.03634</link>
<guid>https://arxiv.org/abs/2512.03634</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, hallucination, factual consistency, interpretable framework, clinical applications<br /><br />Summary:<br /><br />1. Large Language Models (LLMs) have improved natural language processing tasks but are prone to hallucination, generating plausible yet incorrect information.<br /><br />2. Hallucination is especially problematic in high-stakes areas such as clinical applications, where factual errors can have serious consequences.<br /><br />3. Current evaluation metrics for factual consistency are insufficient, lacking interpretability and making error diagnosis and mitigation challenging.<br /><br />4. The authors propose a new interpretable framework for assessing factual consistency that works for both in-domain and open-domain texts by decomposing text into atomic facts.<br /><br />5. Their method introduces a flexible, schema-free approach with a weighted metric instead of absolute scoring, allowing better nuanced assessment.<br /><br />6. A mechanism is included to control the complexity of assessments in complicated domains.<br /><br />7. The approach is benchmarked on popular general and clinical datasets, demonstrating effectiveness.<br /><br />8. The code is released to assist future research on fact-aware model training and improving evaluation methods. <div>
arXiv:2512.03634v1 Announce Type: new 
Abstract: Large Language Models have significantly advanced natural language processing tasks, but remain prone to generating incorrect or misleading but plausible arguments. This issue, known as hallucination, is particularly concerning in high-stakes domains like clinical applications, where factual inaccuracies can have severe consequences. Existing evaluation metrics fail to adequately assess factual consistency and lack interpretability, making diagnosing and mitigating errors difficult. We propose an interpretable framework for factual consistency assessment for in-domain and open-domain texts to address these limitations. Our approach decomposes text into atomic facts and introduces a flexible, schema-free methodology. Unlike previous methods with an absolute metric, we incorporate a weighted metric to enhance factual evaluation. Additionally, we propose a mechanism to control assessment complexity in intricate domains. We benchmark our approach on popular general and clinical datasets and release our code to support fact-aware model training in future research.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative AI Practices, Literacy, and Divides: An Empirical Analysis in the Italian Context</title>
<link>https://arxiv.org/abs/2512.03671</link>
<guid>https://arxiv.org/abs/2512.03671</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, adoption, digital literacy, gender gap, Italy<br /><br />Summary:<br /><br />This study examines the adoption and usage of generative AI (GenAI) chatbots among Italian-speaking adults, based on a survey of 1,906 participants. It reveals widespread use of GenAI for both professional and personal tasks, including sensitive areas such as emotional support and medical advice. GenAI is increasingly replacing other technologies as a primary source of information despite users’ generally low digital literacy levels. This presents risks since many users struggle to detect errors or misinformation in AI-generated content. A significant gender divide is identified, especially among older generations, with women adopting and using GenAI at about half the rate of men. While higher digital literacy correlates with greater adoption, literacy alone does not fully explain the gender gap, indicating the presence of additional barriers to equitable participation. The findings emphasize the importance of targeted educational programs to improve digital literacy and call for further research to understand and address the underlying causes of this disparity. Overall, the study provides a detailed empirical overview of GenAI use in Italy, underscoring both its societal potential and the risks associated with uneven access and awareness. <div>
arXiv:2512.03671v1 Announce Type: new 
Abstract: The rise of Artificial Intelligence (AI) language technologies, particularly generative AI (GenAI) chatbots accessible via conversational interfaces, is transforming digital interactions. While these tools hold societal promise, they also risk widening digital divides due to uneven adoption and low awareness of their limitations. This study presents the first comprehensive empirical mapping of GenAI adoption, usage patterns, and literacy in Italy, based on newly collected survey data from 1,906 Italian-speaking adults. Our findings reveal widespread adoption for both work and personal use, including sensitive tasks like emotional support and medical advice. Crucially, GenAI is supplanting other technologies to become a primary information source: this trend persists despite low user digital literacy, posing a risk as users struggle to recognize errors or misinformation. Moreover, we identify a significant gender divide -- particularly pronounced in older generations -- where women are half as likely to adopt GenAI and use it less frequently than men. While we find literacy to be a key predictor of adoption, it only partially explains this disparity, suggesting that other barriers are at play. Overall, our data provide granular insights into the multipurpose usage of GenAI, highlighting the dual need for targeted educational initiatives and further investigation into the underlying barriers to equitable participation that competence alone cannot explain.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Hydro-Science and Engineering Knowledge of Large Language Models</title>
<link>https://arxiv.org/abs/2512.03672</link>
<guid>https://arxiv.org/abs/2512.03672</guid>
<content:encoded><![CDATA[
<div> Hydro-Science Engineering, large language models, Hydro-SE Bench, interdisciplinary, evaluation

<br /><br />Summary: Hydro-Science and Engineering (Hydro-SE) is an interdisciplinary field essential for managing water supply, hydropower generation, and disaster mitigation related to floods and droughts. The domain requires integration of scientific knowledge with engineering expertise, demanding extensive expert collaboration, which complicates the application of artificial intelligence. With the rise of large language models (LLMs), their potential for assisting Hydro-SE tasks is being explored but remains under-evaluated. To address this gap, the authors propose Hydro-SE Bench, an evaluation benchmark comprising 4,000 multiple-choice questions across nine subfields. This benchmark assesses LLMs on basic conceptual knowledge, engineering application skills, and reasoning and calculation abilities. Evaluations reveal that commercial LLMs achieved accuracy between 0.74 and 0.80, while smaller models scored between 0.41 and 0.68. LLMs perform better in areas linked to natural and physical sciences but show limitations in domain-specific knowledge like industry standards and hydraulic structures. While model scaling enhances reasoning and calculation, significant room remains for improvement in practical engineering problem-solving. The study identifies LLM strengths and weaknesses in Hydro-SE tasks, offering model developers clear targets for training and providing Hydro-SE researchers guidance for applying LLMs effectively. <div>
arXiv:2512.03672v1 Announce Type: new 
Abstract: Hydro-Science and Engineering (Hydro-SE) is a critical and irreplaceable domain that secures human water supply, generates clean hydropower energy, and mitigates flood and drought disasters. Featuring multiple engineering objectives, Hydro-SE is an inherently interdisciplinary domain that integrates scientific knowledge with engineering expertise. This integration necessitates extensive expert collaboration in decision-making, which poses difficulties for intelligence. With the rapid advancement of large language models (LLMs), their potential application in the Hydro-SE domain is being increasingly explored. However, the knowledge and application abilities of LLMs in Hydro-SE have not been sufficiently evaluated. To address this issue, we propose the Hydro-SE LLM evaluation benchmark (Hydro-SE Bench), which contains 4,000 multiple-choice questions. Hydro-SE Bench covers nine subfields and enables evaluation of LLMs in aspects of basic conceptual knowledge, engineering application ability, and reasoning and calculation ability. The evaluation results on Hydro-SE Bench show that the accuracy values vary among 0.74 to 0.80 for commercial LLMs, and among 0.41 to 0.68 for small-parameter LLMs. While LLMs perform well in subfields closely related to natural and physical sciences, they struggle with domain-specific knowledge such as industry standards and hydraulic structures. Model scaling mainly improves reasoning and calculation abilities, but there is still great potential for LLMs to better handle problems in practical engineering application. This study highlights the strengths and weaknesses of LLMs for Hydro-SE tasks, providing model developers with clear training targets and Hydro-SE researchers with practical guidance for applying LLMs.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Different types of syntactic agreement recruit the same units within large language models</title>
<link>https://arxiv.org/abs/2512.03676</link>
<guid>https://arxiv.org/abs/2512.03676</guid>
<content:encoded><![CDATA[
<div> syntactic agreement, large language models, functional localization, cross-lingual analysis, representational spaces<br /><br />Summary:<br /><br />1. Large language models (LLMs) can reliably distinguish grammatical from ungrammatical sentences, but the internal representation of grammatical knowledge within these models is not well understood.<br />2. The study uses a functional localization approach, inspired by cognitive neuroscience, to identify specific LLM units that respond strongly to 67 different English syntactic phenomena across seven open-weight LLMs.<br />3. These identified units are consistently activated by sentences exhibiting the syntactic phenomena and are shown to causally support the models’ syntactic performance, indicating their functional relevance.<br />4. Different types of syntactic agreement, such as subject-verb, anaphor, and determiner-noun agreement, recruit overlapping sets of LLM units, suggesting that syntactic agreement forms a coherent functional category in the LLMs' internal representations.<br />5. This pattern is stable across three languages—English, Russian, and Chinese—and extends to a cross-lingual analysis of 57 diverse languages, where languages with greater structural similarity share more units for subject-verb agreement.<br />6. Overall, syntactic agreement emerges as a key marker of syntactic dependencies within the representational spaces of LLMs, shedding light on how these models internally encode grammatical structures. <div>
arXiv:2512.03676v1 Announce Type: new 
Abstract: Large language models (LLMs) can reliably distinguish grammatical from ungrammatical sentences, but how grammatical knowledge is represented within the models remains an open question. We investigate whether different syntactic phenomena recruit shared or distinct components in LLMs. Using a functional localization approach inspired by cognitive neuroscience, we identify the LLM units most responsive to 67 English syntactic phenomena in seven open-weight models. These units are consistently recruited across sentences containing the phenomena and causally support the models' syntactic performance. Critically, different types of syntactic agreement (e.g., subject-verb, anaphor, determiner-noun) recruit overlapping sets of units, suggesting that agreement constitutes a meaningful functional category for LLMs. This pattern holds in English, Russian, and Chinese; and further, in a cross-lingual analysis of 57 diverse languages, structurally more similar languages share more units for subject-verb agreement. Taken together, these findings reveal that syntactic agreement-a critical marker of syntactic dependencies-constitutes a meaningful category within LLMs' representational spaces.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AITutor-EvalKit: Exploring the Capabilities of AI Tutors</title>
<link>https://arxiv.org/abs/2512.03688</link>
<guid>https://arxiv.org/abs/2512.03688</guid>
<content:encoded><![CDATA[
<div> AITutor-EvalKit, AI tutors, pedagogical quality, language technology, education stakeholders<br /><br />Summary:<br /><br />1. AITutor-EvalKit is an application designed to evaluate the pedagogical quality of AI tutors by leveraging advanced language technology. 2. The tool provides software that supports demonstration and evaluation processes, enabling users to effectively assess AI tutors' educational value. 3. Beyond evaluation, AITutor-EvalKit offers features for model inspection and data visualization, helping users to better understand AI behavior and performance in educational contexts. 4. The primary target audience includes education stakeholders who seek to improve AI-driven tutoring systems by ensuring their effectiveness and quality. 5. Additionally, the tool is intended for the broader ACL (Association for Computational Linguistics) community, as it promotes learning and can be used to gather user feedback and annotate data to further research and development in AI education technologies. <div>
arXiv:2512.03688v1 Announce Type: new 
Abstract: We present AITutor-EvalKit, an application that uses language technology to evaluate the pedagogical quality of AI tutors, provides software for demonstration and evaluation, as well as model inspection and data visualization. This tool is aimed at education stakeholders as well as *ACL community at large, as it supports learning and can also be used to collect user feedback and annotations.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue</title>
<link>https://arxiv.org/abs/2512.03704</link>
<guid>https://arxiv.org/abs/2512.03704</guid>
<content:encoded><![CDATA[
<div> Keywords: Long-context dialogue, State Inertia, DZ-TDPO, dynamic KL constraints, temporal attention bias<br /><br />Summary:<br />1. The paper addresses the challenge of State Inertia in long-context dialogue systems, where static constraints hinder the model's ability to reconcile evolving user intents with prior conversation history.<br />2. To overcome this, the authors propose DZ-TDPO, a novel non-destructive alignment framework that combines conflict-aware dynamic KL constraints with a learnable temporal attention bias to regulate attention precisely without harming existing knowledge.<br />3. Experiments conducted on the Multi-Session Chat (MSC) dataset demonstrate that DZ-TDPO significantly improves dialogue alignment, achieving state-of-the-art win rates of 86.2% on the Phi-3.5 model and showing strong zero-shot generalization capabilities.<br />4. The study reveals a "Capacity-Stability Trade-off": smaller models face an "alignment tax," manifested as increased perplexity, when adapting to overcome historical inertia, whereas the larger Qwen2.5-7B model achieves near-perfect alignment (99.4% win rate) with minimal perplexity cost.<br />5. This indicates that Temporal Attention Injection (TAI) can effectively address historical inertia via precise attention regulation instead of destructive weight modifications, preserving the model's general capabilities across various scales, as validated by maintained MMLU performance.<br />6. The authors have made their code and data publicly available to support reproducibility and further research. <div>
arXiv:2512.03704v1 Announce Type: new 
Abstract: Long-context dialogue systems suffer from State Inertia, where static constraints prevent models from resolving conflicts between evolving user intents and established historical context. To address this, we propose DZ-TDPO, a non-destructive alignment framework that synergizes conflict-aware dynamic KL constraints with a learnable temporal attention bias. Experiments on the Multi-Session Chat (MSC) dataset demonstrate that DZ-TDPO achieves state-of-the-art win rates (86.2% on Phi-3.5) while maintaining robust zero-shot generalization. Crucially, our scaling analysis reveals a "Capacity-Stability Trade-off": while smaller models incur an "alignment tax" (perplexity surge) to overcome historical inertia, the larger Qwen2.5-7B model achieves near-perfect alignment (99.4% win rate) with negligible perplexity overhead. This confirms that TAI can be alleviated via precise attention regulation rather than destructive weight updates, preserving general capabilities (MMLU) across model scales. Code and data are available: https://github.com/lyj20071013/DZ-TDPO
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AR-Med: Automated Relevance Enhancement in Medical Search via LLM-Driven Information Augmentation</title>
<link>https://arxiv.org/abs/2512.03737</link>
<guid>https://arxiv.org/abs/2512.03737</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Medical Search, Retrieval-Augmented, Knowledge Distillation, User Satisfaction<br /><br />Summary:<br /><br />Accurate and reliable search capabilities are vital for online healthcare platforms to ensure user safety and service effectiveness. Traditional search methods struggle with understanding complex and nuanced medical queries, limiting their performance. To address this, the authors propose AR-Med, a novel framework that leverages large language models (LLMs) integrated with retrieval-augmented techniques to ground reasoning in verified medical knowledge, thereby enhancing accuracy and reliability. Recognizing practical constraints such as high operational costs and risks of factual hallucinations, AR-Med employs a knowledge distillation approach to compress large teacher models into smaller, efficient student models suitable for scalable online deployment. Additionally, the authors introduce LocalQSMed, a multi-expert annotated benchmark designed to guide iterative model development and ensure strong alignment between offline evaluations and real-world online performance. Extensive experimental results demonstrate that AR-Med achieves over 93% offline accuracy, representing a 24% absolute improvement compared to the prior online system. This improvement translates into significant gains in online relevance and user satisfaction on medical delivery platforms. Overall, the work presents a practical, scalable blueprint for implementing trustworthy LLM-powered systems in high-stakes healthcare applications, overcoming knowledge gaps and ensuring safe deployment. <div>
arXiv:2512.03737v1 Announce Type: new 
Abstract: Accurate and reliable search on online healthcare platforms is critical for user safety and service efficacy. Traditional methods, however, often fail to comprehend complex and nuanced user queries, limiting their effectiveness. Large language models (LLMs) present a promising solution, offering powerful semantic understanding to bridge this gap. Despite their potential, deploying LLMs in this high-stakes domain is fraught with challenges, including factual hallucinations, specialized knowledge gaps, and high operational costs. To overcome these barriers, we introduce \textbf{AR-Med}, a novel framework for \textbf{A}utomated \textbf{R}elevance assessment for \textbf{Med}ical search that has been successfully deployed at scale on the Online Medical Delivery Platforms. AR-Med grounds LLM reasoning in verified medical knowledge through a retrieval-augmented approach, ensuring high accuracy and reliability. To enable efficient online service, we design a practical knowledge distillation scheme that compresses large teacher models into compact yet powerful student models. We also introduce LocalQSMed, a multi-expert annotated benchmark developed to guide model iteration and ensure strong alignment between offline and online performance. Extensive experiments show AR-Med achieves an offline accuracy of over 93\%, a 24\% absolute improvement over the original online system, and delivers significant gains in online relevance and user satisfaction. Our work presents a practical and scalable blueprint for developing trustworthy, LLM-powered systems in real-world healthcare applications.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Principled RL for Diffusion LLMs Emerges from a Sequence-Level Perspective</title>
<link>https://arxiv.org/abs/2512.03759</link>
<guid>https://arxiv.org/abs/2512.03759</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, diffusion large language models, ELBO, sequence-level optimization, policy optimization<br /><br />Summary:  
This paper addresses the fundamental challenge of applying Reinforcement Learning (RL) to diffusion large language models (dLLMs), which lack the token-level probability factorization present in autoregressive models. The authors identify the core difficulty as the mismatch in likelihood approximation, since dLLMs generate sequences through iterative, non-autoregressive denoising steps that do not naturally yield token-level conditional probabilities needed for conventional token-level RL objectives. To resolve this, the paper proposes ELBO-based Sequence-level Policy Optimization (ESPO), a novel RL framework treating entire sequence generation as a single action and employing the Evidence Lower Bound (ELBO) as a tractable proxy for sequence-level likelihood. ESPO enhances training stability by incorporating per-token normalization of importance ratios and a robust estimation of KL-divergence. Extensive experiments demonstrate that ESPO outperforms existing token-level RL baselines across tasks such as mathematical reasoning, coding, and planning. Notably, it achieves dramatic improvements, with 20-40 point gains on the Countdown benchmark, while delivering consistent advances on other math and coding challenges. The study establishes sequence-level optimization as a principled and effective approach for reinforcement learning in dLLMs. The authors provide their code publicly at the given GitHub repository to facilitate further research. <div>
arXiv:2512.03759v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) has proven highly effective for autoregressive language models, but adapting these methods to diffusion large language models (dLLMs) presents fundamental challenges. The core difficulty lies in likelihood approximation: while autoregressive models naturally provide token-level conditional probabilities essential for token-level RL objectives (e.g., GRPO), dLLMs generate sequences through iterative non-autoregressive denoising steps that lack this factorization. To address this fundamental mismatch, we propose ELBO-based Sequence-level Policy Optimization (ESPO), a principled RL framework that treats entire sequence generation as a single action and uses the ELBO as a tractable sequence-level likelihood proxy. Our method incorporates per-token normalization of importance ratios and robust KL-divergence estimation to ensure stable large-scale training. Extensive experiments on mathematical reasoning, coding, and planning tasks demonstrate that ESPO significantly outperforms token-level baselines, achieving dramatic improvements of 20-40 points on the Countdown task, while maintaining consistent gains on math and coding benchmarks. Our approach establishes sequence-level optimization as a principled and empirically effective paradigm for RL in dLLMs. Our code is available at https://github.com/ML-GSAI/ESPO.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Context Representation Hijacking</title>
<link>https://arxiv.org/abs/2512.03771</link>
<guid>https://arxiv.org/abs/2512.03771</guid>
<content:encoded><![CDATA[
<div> Doublespeak, in-context representation hijacking, large language models, semantic overwrite, safety alignment<br /><br />Summary:<br /><br />1. The paper introduces Doublespeak, a novel attack technique targeting large language models (LLMs) by manipulating their in-context representations.<br /><br />2. The attack replaces harmful keywords (e.g., "bomb") with benign tokens (e.g., "carrot") across multiple in-context examples preceding a harmful prompt, effectively hijacking the model’s internal semantic representations.<br /><br />3. This token substitution causes the representation of the benign word to converge toward the harmful one, meaning that seemingly innocuous prompts like "How to build a carrot?" are internally interpreted as harmful instructions such as "How to build a bomb?", circumventing existing safety measures.<br /><br />4. The authors use interpretability methods to reveal that this semantic overwrite happens progressively across layers, with benign meanings at early layers transitioning into harmful semantics in deeper layers.<br /><br />5. Doublespeak requires no optimization, works across different model families including closed- and open-source models, and achieves a high attack success rate (74%) on Llama-3.3-70B-Instruct with just a one-sentence context override.<br /><br />The study highlights a new potential vulnerability in LLMs’ latent spaces and suggests that current alignment approaches are insufficient, calling for representation-level defenses to prevent such attacks. <div>
arXiv:2512.03771v1 Announce Type: new 
Abstract: We introduce \textbf{Doublespeak}, a simple \emph{in-context representation hijacking} attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., \textit{bomb}) with a benign token (e.g., \textit{carrot}) across multiple in-context examples, provided a prefix to a harmful request. We demonstrate that this substitution leads to the internal representation of the benign token converging toward that of the harmful one, effectively embedding the harmful semantics under a euphemism. As a result, superficially innocuous prompts (e.g., ``How to build a carrot?'') are internally interpreted as disallowed instructions (e.g., ``How to build a bomb?''), thereby bypassing the model's safety alignment. We use interpretability tools to show that this semantic overwrite emerges layer by layer, with benign meanings in early layers converging into harmful semantics in later ones. Doublespeak is optimization-free, broadly transferable across model families, and achieves strong success rates on closed-source and open-source systems, reaching 74\% ASR on Llama-3.3-70B-Instruct with a single-sentence context override. Our findings highlight a new attack surface in the latent space of LLMs, revealing that current alignment strategies are insufficient and should instead operate at the representation level.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Instruction-Following Capabilities in Seq2Seq Models: DoLA Adaptations for T5</title>
<link>https://arxiv.org/abs/2512.03803</link>
<guid>https://arxiv.org/abs/2512.03803</guid>
<content:encoded><![CDATA[
<div> Keywords: Contrastive decoding, DoLa, encoder-decoder architecture, instruction following, faithfulness<br /><br />Summary: This paper explores the adaptation of DoLa (Decoding by Contrastive Layers), a contrastive decoding method previously applied only in decoder-only language models, to encoder-decoder architectures specifically the T5 and FLAN-T5 model families. It is the first known implementation of contrastive decoding in such architectures. The study focuses on how DoLa influences the models' ability to follow instructions during text generation. Experimental results indicate that DoLa enhances the faithfulness and factual accuracy of generated text in certain categories of tasks, while negatively affecting performance in others. To better understand these mixed outcomes, the authors perform a layer-by-layer analysis of how token output probabilities evolve across the layers of a FLAN-T5 model when using DoLa. This analysis sheds light on how the contrastive decoding process alters model outputs at different depths, providing insight into when and why DoLa helps or hinders instruction-following behavior. Overall, the work advances the understanding of inference-time strategies for improving large language model output quality in encoder-decoder frameworks, highlighting both benefits and limitations. <div>
arXiv:2512.03803v1 Announce Type: new 
Abstract: Contrastive decoding is a lightweight and effective inference-time method that improves the quality of text generation in Large Language Models. However, algorithms such as DoLa (Decoding by Contrastive Layers) have only been implemented in decoder-only architectures and studied for their impact on improving factuality. This work adapts DoLa for the T5 and FLAN-T5 model families and evaluates its impact on the models' instruction following capabilities, which to our knowledge is the first implementation of a contrastive decoding strategy in an encoder-decoder architecture. Our results show that DoLa improves the faithfulness of text generation for certain categories of tasks and harms others. To understand these results, we present a layer-by-layer analysis of logit evolution in a FLAN-T5 model to quantify DoLa's impact on token output probabilities.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Alignment Between Human and Machine Codes: An Empirical Assessment of Prompt Engineering for Construct Identification in Psychology</title>
<link>https://arxiv.org/abs/2512.03818</link>
<guid>https://arxiv.org/abs/2512.03818</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, prompt engineering, text classification, psychology constructs, few-shot learning  

<br /><br />Summary:  
This study investigates the impact of prompt wording on the text classification performance of large language models (LLMs), particularly in domains like psychology where text constructs have precise, theory-driven definitions. The authors propose an empirical framework to optimize LLM classification performance by extensive prompt engineering. They experimentally evaluate five prompting strategies: codebook-guided empirical prompt selection, automatic prompt engineering, persona prompting, chain-of-thought reasoning, and explanatory prompting, under both zero-shot and few-shot conditions. Results reveal that persona, chain-of-thought, and explanatory prompting alone do not fully mitigate the negative effects of poorly worded prompts. Instead, the most critical aspects influencing performance are the inclusion of a clear construct definition, appropriate task framing, and relevant examples. Across three psychological constructs and two different LLMs, the highest agreement with expert human judgments was achieved using few-shot prompting that combined codebook-based empirical prompt selection and automatic prompt engineering. Based on these findings, the authors recommend that researchers generate and test a large variety of prompt variants, both human-crafted and automatically generated, selecting the best based on empirical performance on training data and then validating on holdout sets. This approach provides a systematic, theory-driven method to enhance LLM alignment with expert labels in sensitive application areas. <div>
arXiv:2512.03818v1 Announce Type: new 
Abstract: Due to their architecture and vast pre-training data, large language models (LLMs) demonstrate strong text classification performance. However, LLM output - here, the category assigned to a text - depends heavily on the wording of the prompt. While literature on prompt engineering is expanding, few studies focus on classification tasks, and even fewer address domains like psychology, where constructs have precise, theory-driven definitions that may not be well represented in pre-training data. We present an empirical framework for optimizing LLM performance for identifying constructs in texts via prompt engineering. We experimentally evaluate five prompting strategies --codebook-guided empirical prompt selection, automatic prompt engineering, persona prompting, chain-of-thought reasoning, and explanatory prompting - with zero-shot and few-shot classification. We find that persona, chain-of-thought, and explanations do not fully address performance loss accompanying a badly worded prompt. Instead, the most influential features of a prompt are the construct definition, task framing, and, to a lesser extent, the examples provided. Across three constructs and two models, the classifications most aligned with expert judgments resulted from a few-shot prompt combining codebook-guided empirical prompt selection with automatic prompt engineering. Based on our findings, we recommend that researchers generate and evaluate as many prompt variants as feasible, whether human-crafted, automatically generated, or ideally both, and select prompts and examples based on empirical performance in a training dataset, validating the final approach in a holdout set. This procedure offers a practical, systematic, and theory-driven method for optimizing LLM prompts in settings where alignment with expert judgment is critical.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training and Evaluation of Guideline-Based Medical Reasoning in LLMs</title>
<link>https://arxiv.org/abs/2512.03838</link>
<guid>https://arxiv.org/abs/2512.03838</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, medical consensus guidelines, large language models, Sepsis-3, time series forecasting<br /><br />Summary:<br /><br />This paper addresses the challenge of generating faithful explanations in medical machine learning, emphasizing trust by medical practitioners. It proposes fine-tuning large language models (LLMs) to follow step-by-step medical consensus guidelines during reasoning and prediction, using verbalized medical inference rules instantiated from electronic health records as training data. The approach enables automatic evaluation of the model’s inference through derivation correctness (logical fidelity to premises) and value correctness (accuracy of predicted clinical values). The Sepsis-3 consensus definition serves as a complex example to validate the method. Experiments demonstrate that small fine-tuned models outperform much larger LLMs using one-shot learning or models trained on broad medical texts including consensus guidelines. Fine-tuning on domain-specific rule instantiations yields near-perfect reasoning on unseen patient data within that area. The main bottleneck for early prediction is not poor out-of-distribution generalization but the difficulty of forecasting future clinical variables that are sparsely and irregularly sampled. To address this, the paper integrates time series forecasting model outputs with the LLM in a multimodal framework, improving prediction performance by leveraging temporal context alongside rule-based reasoning. <div>
arXiv:2512.03838v1 Announce Type: new 
Abstract: Machine learning for early prediction in medicine has recently shown breakthrough performance, however, the focus on improving prediction accuracy has led to a neglect of faithful explanations that are required to gain the trust of medical practitioners. The goal of this paper is to teach LLMs to follow medical consensus guidelines step-by-step in their reasoning and prediction process. Since consensus guidelines are ubiquitous in medicine, instantiations of verbalized medical inference rules to electronic health records provide data for fine-tuning LLMs to learn consensus rules and possible exceptions thereof for many medical areas. Consensus rules also enable an automatic evaluation of the model's inference process regarding its derivation correctness (evaluating correct and faithful deduction of a conclusion from given premises) and value correctness (comparing predicted values against real-world measurements). We exemplify our work using the complex Sepsis-3 consensus definition. Our experiments show that small fine-tuned models outperform one-shot learning of considerably larger LLMs that are prompted with the explicit definition and models that are trained on medical texts including consensus definitions. Since fine-tuning on verbalized rule instantiations of a specific medical area yields nearly perfect derivation correctness for rules (and exceptions) on unseen patient data in that area, the bottleneck for early prediction is not out-of-distribution generalization, but the orthogonal problem of generalization into the future by forecasting sparsely and irregularly sampled clinical variables. We show that the latter results can be improved by integrating the output representations of a time series forecasting model with the LLM in a multimodal setup.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reconstructing KV Caches with Cross-layer Fusion For Enhanced Transformers</title>
<link>https://arxiv.org/abs/2512.03870</link>
<guid>https://arxiv.org/abs/2512.03870</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformer decoders, KV cache, cross-layer sharing, FusedKV, memory efficiency<br /><br />Summary:<br /><br />1. Transformer decoders are effective but face high memory costs due to the large key-value (KV) cache at long sequence lengths.<br />2. Existing cross-layer KV cache sharing methods (e.g., YOCO, CLA) reduce memory usage but generally perform worse than within-layer methods like GQA.<br />3. The authors analyze the information flow in top-layer keys and values, finding values mainly come from the bottom layer, while keys combine information from both bottom and middle layers.<br />4. Based on this insight, they introduce FusedKV, which creates top-layer KV caches by fusing the most informative bottom- and middle-layer KV caches using a learnable fusion method applied directly on post-RoPE keys, maintaining relative positional information without expensive recomputation.<br />5. To further boost efficiency, they propose FusedKV-Lite, which derives top-layer KV caches directly from bottom-layer values and middle-layer keys, trading a slight perplexity increase for reduced I/O overhead.<br />6. Experiments on large language models ranging from 332 million to 4 billion parameters show FusedKV methods reduce KV cache memory usage by 50% while achieving lower validation perplexity compared to standard Transformer decoders.<br />7. The work establishes FusedKV as a memory-efficient, high-performance alternative architecture for Transformer decoders in long-sequence generation scenarios. <div>
arXiv:2512.03870v1 Announce Type: new 
Abstract: Transformer decoders have achieved strong results across tasks, but the memory required for the KV cache becomes prohibitive at long sequence lengths. Although Cross-layer KV Cache sharing (e.g., YOCO, CLA) offers a path to mitigate KV Cache bottleneck, it typically underperforms within-layer methods like GQA. To understand the root cause, we investigate the information flow of keys and values of the top-layers. Our preliminary reveals a clear distribution: values are predominantly derived from the bottom layer, while keys draw more information from both bottom and middle layers. Building upon this, we propose FusedKV, whose top-layer KV caches are a learnable fusion of the most informative ones from the bottom and middle layers. This fusion operates directly on post-RoPE keys, preserving relative positional information without the computational cost of re-applying rotary embeddings. To further improve efficiency, we propose FusedKV-Lite, an cross-layer sharing approach, where top-layer KV caches are directly derived from the bottom-layer values and the middle-layer keys. Compared to FusedKV, FusedKV-Lite reduces I/O overhead at the cost of a slight increase in perplexity. In experiments on LLMs ranging from 332M to 4B parameters, our proposed method reduce 50\% cache memory while achieving lower validation perplexity than the standard Transformer decoder, establishing it as a memory-efficient, high-performance architectural alternative.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BERnaT: Basque Encoders for Representing Natural Textual Diversity</title>
<link>https://arxiv.org/abs/2512.03903</link>
<guid>https://arxiv.org/abs/2512.03903</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, linguistic diversity, Basque, low-resource languages, model robustness  

<br /><br />Summary: This paper addresses the limitations of current language models that rely heavily on massive, quality-filtered text corpora, which often exclude non-standard linguistic varieties such as dialectal, historical, and informal language. The authors argue for the importance of capturing the full spectrum of language variation to enhance model robustness and reduce representational biases. Focusing on Basque, a morphologically complex and low-resource language, they create new corpora by combining standard texts with data from social media and historical sources. They pre-train the BERnaT family of encoder-only models under three data configurations: standard, diverse, and combined. Additionally, the paper introduces a novel evaluation framework that separates Natural Language Understanding (NLU) tasks into standard and diverse subsets, enabling assessment of a model’s linguistic generalization abilities. Experimental results reveal that models trained on both standard and diverse datasets consistently outperform those trained solely on standard corpora, with improvements observed across all task categories without any loss in accuracy on traditional benchmarks. Overall, the study emphasizes that incorporating linguistic diversity is crucial for developing inclusive and more generalizable language models, particularly for low-resource languages like Basque. <div>
arXiv:2512.03903v1 Announce Type: new 
Abstract: Language models depend on massive text corpora that are often filtered for quality, a process that can unintentionally exclude non-standard linguistic varieties, reduce model robustness and reinforce representational biases. In this paper, we argue that language models should aim to capture the full spectrum of language variation (dialectal, historical, informal, etc.) rather than relying solely on standardized text. Focusing on Basque, a morphologically rich and low-resource language, we construct new corpora combining standard, social media, and historical sources, and pre-train the BERnaT family of encoder-only models in three configurations: standard, diverse, and combined. We further propose an evaluation framework that separates Natural Language Understanding (NLU) tasks into standard and diverse subsets to assess linguistic generalization. Results show that models trained on both standard and diverse data consistently outperform those trained on standard corpora, improving performance across all task types without compromising standard benchmark accuracy. These findings highlight the importance of linguistic diversity in building inclusive, generalizable language models.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Is Lying Only Sinful in Islam? Exploring Religious Bias in Multilingual Large Language Models Across Major Religions</title>
<link>https://arxiv.org/abs/2512.03943</link>
<guid>https://arxiv.org/abs/2512.03943</guid>
<content:encoded><![CDATA[
<div> Bias detection, multilingual models, religion, South Asia, BRAND dataset<br /><br />Summary:<br /><br />1. Large language models have advanced in bias detection, but sensitive topics such as religion remain challenging, especially since minor errors can lead to serious misunderstandings.<br /><br />2. Multilingual models often misrepresent religions and struggle to maintain accuracy in religious contexts.<br /><br />3. The authors introduce BRAND (Bilingual Religious Accountable Norm Dataset), focusing on the four major religions in South Asia—Buddhism, Christianity, Hinduism, and Islam—with over 2,400 entries.<br /><br />4. They applied three types of prompts in both English and Bengali to evaluate model performance.<br /><br />5. Results show that models perform better in English than in Bengali and consistently bias toward Islam, even on religion-neutral questions.<br /><br />6. These findings reveal persistent biases in multilingual language models that vary depending on the language used.<br /><br />7. The study connects these biases to broader human-computer interaction challenges related to religion and spirituality, emphasizing the need for more accountable and culturally sensitive modeling in AI systems. <div>
arXiv:2512.03943v1 Announce Type: new 
Abstract: While recent developments in large language models have improved bias detection and classification, sensitive subjects like religion still present challenges because even minor errors can result in severe misunderstandings. In particular, multilingual models often misrepresent religions and have difficulties being accurate in religious contexts. To address this, we introduce BRAND: Bilingual Religious Accountable Norm Dataset, which focuses on the four main religions of South Asia: Buddhism, Christianity, Hinduism, and Islam, containing over 2,400 entries, and we used three different types of prompts in both English and Bengali. Our results indicate that models perform better in English than in Bengali and consistently display bias toward Islam, even when answering religion-neutral questions. These findings highlight persistent bias in multilingual models when similar questions are asked in different languages. We further connect our findings to the broader issues in HCI regarding religion and spirituality.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adapting Large Language Models to Low-Resource Tibetan: A Two-Stage Continual and Supervised Fine-Tuning Study</title>
<link>https://arxiv.org/abs/2512.03976</link>
<guid>https://arxiv.org/abs/2512.03976</guid>
<content:encoded><![CDATA[
<div> Keywords: Tibetan adaptation, large language models, continual pretraining, supervised fine-tuning, low-resource languages  

<br /><br />Summary:  
This paper addresses the challenge of adapting large language models (LLMs) to low-resource languages, focusing on Tibetan, a morphologically rich and underrepresented language. The authors propose a two-stage adaptation approach applied to the Qwen2.5-3B model. First, Continual Pretraining (CPT) is used to build a foundational Tibetan linguistic understanding, effectively creating a semantic manifold for the language. Second, Supervised Fine-Tuning (SFT) specializes the model for specific tasks, particularly improving Chinese-to-Tibetan translation. Empirical results show a significant reduction in perplexity from 2.98 to 1.54, indicating enhanced language modeling performance. Translation quality metrics also improve substantially, with BLEU scores rising from 0.046 to 0.261, and chrF scores from 2.2 to 6.6. A detailed layer-wise analysis on a larger variant, Qwen3-4B with 435 layers, reveals that model adaptation is concentrated primarily in the embedding layers and output heads, while mid-to-late multilayer perceptron (MLP) projections encode domain-specific transformations. The study concludes that CPT builds broad Tibetan knowledge with minimal disruption to the model’s overall representation, while SFT helps refine task alignment. This work is the first quantitative study on Tibetan adaptation in LLMs and offers an open framework for extending foundation models to other low-resource languages. <div>
arXiv:2512.03976v1 Announce Type: new 
Abstract: Adapting large language models (LLMs) to low-resource languages remains a major challenge due to data scarcity and cross-lingual drift. This work presents a two-stage adaptation of Qwen2.5-3B to Tibetan, a morphologically rich and underrepresented language. We employ Continual Pretraining (CPT) to establish Tibetan linguistic grounding, followed by Supervised Fine-Tuning (SFT) for task and translation specialization. Empirical evaluations demonstrate a consistent decrease in perplexity (from 2.98 $\rightarrow$ 1.54) and substantial improvements in Chinese$\rightarrow$Tibetan translation quality (BLEU: 0.046 $\rightarrow$ 0.261; chrF: 2.2 $\rightarrow$ 6.6). Layer-wise analysis across 435 layers in Qwen3-4B reveals that adaptation primarily concentrates on embedding and output heads, with mid--late MLP projections encoding domain-specific transformations. Our findings suggest that CPT constructs a Tibetan semantic manifold while SFT sharpens task alignment with minimal representational disruption. This study provides the first quantitative exploration of Tibetan adaptation dynamics for LLMs, and offers an open, reproducible framework for extending multilingual foundation models to low-resource settings.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Teaching Old Tokenizers New Words: Efficient Tokenizer Adaptation for Pre-trained Models</title>
<link>https://arxiv.org/abs/2512.03989</link>
<guid>https://arxiv.org/abs/2512.03989</guid>
<content:encoded><![CDATA[
<div> Tokenizer adaptation, vocabulary extension, vocabulary pruning, BPE training, tokenization efficiency<br /><br />Summary:  
The paper addresses tokenizer adaptation for transferring pre-trained language models to new domains or languages, focusing on two main aspects: vocabulary extension and pruning. The conventional method for vocabulary extension involves training a new tokenizer on domain-specific text and appending non-overlapping tokens to the existing vocabulary, which often leads to many unused or unreachable tokens. To counter this, the authors propose continued BPE (Byte Pair Encoding) training, which adapts a pre-trained tokenizer by continuing the BPE merge learning process with new data, resulting in improved tokenization efficiency and better utilization of the extended vocabulary. Additionally, the paper introduces leaf-based vocabulary pruning, a technique that removes redundant tokens from the vocabulary while maintaining the model’s performance and quality. Experimental results across multiple languages and various model families demonstrate the effectiveness of these approaches. Both techniques provide practical and controlled methods for modifying the tokenizer vocabulary in domain adaptation scenarios. The authors also release their methods as an open-source package, enabling broader use and experimentation in adapting tokenizers efficiently without sacrificing model quality. <div>
arXiv:2512.03989v1 Announce Type: new 
Abstract: Tokenizer adaptation plays an important role in transferring pre-trained language models to new domains or languages. In this work, we address two complementary aspects of this process: vocabulary extension and pruning. The common approach to extension trains a new tokenizer on domain-specific text and appends the tokens that do not overlap with the existing vocabulary, which often results in many tokens that are unreachable or never used. We propose continued BPE training, which adapts a pre-trained tokenizer by continuing the BPE merge learning process on new data. Experiments across multiple languages and model families show that this approach improves tokenization efficiency and leads to better utilization of added vocabulary. We also introduce leaf-based vocabulary pruning, which removes redundant tokens while preserving model quality. Together, these methods provide practical tools for controlled vocabulary modification, which we release as an open-source package.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AugServe: Adaptive Request Scheduling for Augmented Large Language Model Inference Serving</title>
<link>https://arxiv.org/abs/2512.04013</link>
<guid>https://arxiv.org/abs/2512.04013</guid>
<content:encoded><![CDATA[
<div> Keywords: augmented LLMs, inference serving, request scheduling, effective throughput, dynamic batching<br /><br />Summary:  
This paper addresses the challenges faced by augmented large language model (LLM) inference systems in web applications, focusing on improving efficiency and meeting service-level objectives (SLOs). The authors identify two main issues in existing systems: the use of first-come-first-served (FCFS) scheduling which causes head-of-line blocking and queuing delays, and a static batch token limit that does not adapt to changing load and hardware conditions. To overcome these limitations, the paper introduces AugServe, an inference framework designed to reduce queuing latency and boost effective throughput. AugServe employs a novel two-stage adaptive request scheduling strategy—initially optimizing scheduling order based on inference features of LLM requests, then refining these decisions dynamically using real-time runtime data to adapt to request characteristics and system capabilities. Additionally, AugServe incorporates a dynamic token batching mechanism that adjusts according to hardware status and current workload, further improving throughput. Experimental results demonstrate that AugServe significantly outperforms state-of-the-art systems like vLLM and InferCept, achieving between 3.3x to 33.1x higher effective throughput and reducing time-to-first-token (TTFT) latency by up to 96.3%, thereby enhancing both efficiency and user experience in augmented LLM inference services. <div>
arXiv:2512.04013v1 Announce Type: new 
Abstract: As augmented large language models (LLMs) with external tools become increasingly popular in web applications, improving augmented LLM inference serving efficiency and optimizing service-level objectives (SLOs) are critical for enhancing user experience. To achieve this, inference systems must maximize request handling within latency constraints, referred to as increasing effective throughput. However, existing systems face two major challenges: (i) reliance on first-come-first-served (FCFS) scheduling causes severe head-of-line blocking, leading to queuing delays exceeding the SLOs for many requests; and (ii) static batch token limit, which fails to adapt to fluctuating loads and hardware conditions. Both of these factors degrade effective throughput and service quality.
  This paper presents AugServe, an efficient inference framework designed to reduce queueing latency and enhance effective throughput for augmented LLM inference services. The core idea of AugServe is a two-stage adaptive request scheduling strategy. Specifically, AugServe combines the inference features of augmented LLM requests to optimize the order of scheduling decisions (stage I). These decisions are continuously refined with runtime information (stage II), adapting to both request characteristics and system capabilities. In addition, AugServe dynamically adjusts the token batching mechanism based on hardware status and real-time load, further enhancing throughput performance. Experimental results show that AugServe achieves 4.7-33.1x and 3.3-13.2x higher effective throughput than vLLM and InferCept, while reducing time-to-first-token (TTFT) by up to 96.3% and 95.0%, respectively.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Jina-VLM: Small Multilingual Vision Language Model</title>
<link>https://arxiv.org/abs/2512.04032</link>
<guid>https://arxiv.org/abs/2512.04032</guid>
<content:encoded><![CDATA[
<div> Jina-VLM, Vision-Language Model, Multilingual VQA, SigLIP2 Encoder, Qwen3 Backbone  

<br /><br />Summary:  
Jina-VLM is a vision-language model with 2.4 billion parameters designed to excel in multilingual visual question answering (VQA). It integrates a SigLIP2 vision encoder with a Qwen3 language backbone, enabling effective fusion of visual and textual information. The model employs an attention-pooling connector, which facilitates token-efficient processing of images at arbitrary resolutions, enhancing flexibility and performance. In evaluations conducted on standard VQA benchmarks, Jina-VLM demonstrates state-of-the-art results among open models in the 2 billion parameter range. Additionally, the model performs strongly across multilingual VQA tasks, showcasing its capability to understand and answer questions in multiple languages. Despite its advanced multi-modal capabilities, Jina-VLM maintains competitive performance on text-only tasks, indicating a well-balanced architecture that does not compromise on language understanding. Overall, Jina-VLM sets a new standard for open large-scale vision-language models, providing superior cross-modal comprehension while supporting multilingual applications efficiently. <div>
arXiv:2512.04032v1 Announce Type: new 
Abstract: We present Jina-VLM, a 2.4B parameter vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. The model couples a SigLIP2 vision encoder with a Qwen3 language backbone through an attention-pooling connector that enables token-efficient processing of arbitrary-resolution images. Across standard VQA benchmarks and multilingual evaluations, Jina-VLM outperforms comparable models while preserving competitive text-only performance.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SkillFactory: Self-Distillation For Learning Cognitive Behaviors</title>
<link>https://arxiv.org/abs/2512.04072</link>
<guid>https://arxiv.org/abs/2512.04072</guid>
<content:encoded><![CDATA[
<div> Skills, Reinforcement Learning, Fine-Tuning, Cognitive Abilities, Robustness<br /><br />Summary:<br /><br />1. The paper introduces SkillFactory, a method designed to help language models acquire advanced cognitive skills during supervised fine-tuning (SFT) before applying reinforcement learning (RL).<br /><br />2. Unlike previous approaches that rely on distillation from stronger models, SkillFactory generates "silver" training data by rearranging samples from the base model itself, which helps the model roughly learn skills such as verification, backtracking, and retrying.<br /><br />3. Although these generated SFT traces are imperfect, they effectively prime the model for skill acquisition during subsequent RL training.<br /><br />4. Experimental evaluation demonstrates that models initialized with SkillFactory SFT generalize better to harder task variants after RL, even if their pre-RL performance is lower.<br /><br />5. Additionally, SkillFactory-enhanced models are shown to use cognitive skills more reliably and exhibit greater robustness to performance regression on out-of-domain tasks compared to models fine-tuned and RL-trained without SkillFactory.<br /><br />6. The authors conclude that embedding inductive biases related to cognitive skill use during SFT provides a beneficial foundation for RL, leading to more robust and capable reasoning models. <div>
arXiv:2512.04072v1 Announce Type: new 
Abstract: Reasoning models leveraging long chains of thought employ various cognitive skills, such as verification of their answers, backtracking, retrying by an alternate method, and more. Previous work has shown that when a base language model exhibits these skills, training that model further with reinforcement learning (RL) can learn to leverage them. How can we get models to leverage skills that aren't exhibited by base models? Our work, SkillFactory, is a method for fine-tuning models to roughly learn these skills during a supervised fine-tuning (SFT) stage prior to RL. Our approach does not rely on distillation from a stronger model, but instead uses samples from the model itself, rearranged to provide training data in the format of those skills. These "silver" SFT traces may be imperfect, but are nevertheless effective for priming a model to acquire skills during RL. Our evaluation shows that (1) starting from SkillFactory SFT initialization helps a model to generalize to harder variants of a task post-RL, despite lower performance pre-RL; (2) cognitive skills are indeed used by the model; (3) RLed SkillFactory models are more robust to regression on out-of-domain tasks than RLed base models. Our work suggests that inductive biases learned prior to RL help models learn robust cognitive skill use.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting AI Hallucinations in Finance: An Information-Theoretic Method Cuts Hallucination Rate by 92%</title>
<link>https://arxiv.org/abs/2512.03107</link>
<guid>https://arxiv.org/abs/2512.03107</guid>
<content:encoded><![CDATA[
<div> hallucination detection, semantic entropy, perplexity decomposition, large language models, evidence utilization<br /><br />Summary:<br /><br />1. Large language models (LLMs) often produce fluent but factually unsupported answers known as hallucinations, which pose risks in high-stakes domains.  
2. The paper introduces ECLIPSE, a novel framework that views hallucination as a mismatch between a model's semantic entropy and the capacity of available evidence.  
3. ECLIPSE combines entropy estimation via multi-sample clustering with a new perplexity decomposition method that evaluates how models utilize retrieved evidence during generation.  
4. The authors prove under mild assumptions that the entropy-capacity objective used in ECLIPSE is strictly convex and has a unique stable optimum, ensuring reliable optimization.  
5. Evaluation on a controlled financial question answering dataset using GPT-3.5-turbo (200 balanced samples, including synthetic hallucinations) shows ECLIPSE achieves high detection performance with ROC AUC of 0.89 and average precision of 0.90, significantly outperforming a semantic entropy-only baseline (AUC 0.50).  
6. Ablation with Claude-3-Haiku, which provides no token-level log probabilities, results in a substantial performance drop (AUC 0.59) and coefficient magnitude decrease, highlighting ECLIPSE’s dependence on calibrated token-level log probabilities.  
7. The perplexity decomposition features have the largest learned coefficients, emphasizing that how evidence is utilized by the model is central to effective hallucination detection.  
8. The work is positioned as a controlled mechanism study, with broader validation on other domains and natural hallucinations to be addressed in future research. <div>
arXiv:2512.03107v1 Announce Type: cross 
Abstract: Large language models (LLMs) produce fluent but unsupported answers - hallucinations - limiting safe deployment in high-stakes domains. We propose ECLIPSE, a framework that treats hallucination as a mismatch between a model's semantic entropy and the capacity of available evidence. We combine entropy estimation via multi-sample clustering with a novel perplexity decomposition that measures how models use retrieved evidence. We prove that under mild conditions, the resulting entropy-capacity objective is strictly convex with a unique stable optimum. We evaluate on a controlled financial question answering dataset with GPT-3.5-turbo (n=200 balanced samples with synthetic hallucinations), where ECLIPSE achieves ROC AUC of 0.89 and average precision of 0.90, substantially outperforming a semantic entropy-only baseline (AUC 0.50). A controlled ablation with Claude-3-Haiku, which lacks token-level log probabilities, shows AUC dropping to 0.59 with coefficient magnitudes decreasing by 95% - demonstrating that ECLIPSE is a logprob-native mechanism whose effectiveness depends on calibrated token-level uncertainties. The perplexity decomposition features exhibit the largest learned coefficients, confirming that evidence utilization is central to hallucination detection. We position this work as a controlled mechanism study; broader validation across domains and naturally occurring hallucinations remains future work.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Culture Affordance Atlas: Reconciling Object Diversity Through Functional Mapping</title>
<link>https://arxiv.org/abs/2512.03173</link>
<guid>https://arxiv.org/abs/2512.03173</guid>
<content:encoded><![CDATA[
<div> Culture bias, Vision-Language datasets, function-centric framework, socioeconomic gaps, Culture Affordance Atlas<br /><br />Summary:<br /><br />This paper addresses cultural biases present in mainstream Vision-Language (VL) datasets, which predominantly reflect higher-income, Western contexts, leading to reduced generalizability and performance disparities that negatively affect lower-income and non-Western populations. To mitigate these disparities, the authors propose a novel function-centric framework that categorizes objects based on the functions they serve across various cultural and economic contexts, rather than relying solely on object identity. They implement this framework through the creation of the Culture Affordance Atlas, a culturally grounded and re-annotated version of the Dollar Street dataset, encompassing 46 distinct functions and 288 objects. This dataset is publicly accessible online. Through extensive empirical evaluation using the CLIP model, results show that labeling objects by function significantly reduces performance gaps between high- and low-income groups by a median of 6 percentage points, a statistically significant improvement, thereby enhancing model effectiveness in lower-income settings. Additionally, the analysis reveals many culturally essential objects commonly omitted in leading VL datasets. Overall, this work presents a scalable and inclusive approach for constructing VL datasets that promote equitable AI systems across diverse cultural and socioeconomic backgrounds. <div>
arXiv:2512.03173v1 Announce Type: cross 
Abstract: Culture shapes the objects people use and for what purposes, yet mainstream Vision-Language (VL) datasets frequently exhibit cultural biases, disproportionately favoring higher-income, Western contexts. This imbalance reduces model generalizability and perpetuates performance disparities, especially impacting lower-income and non-Western communities. To address these disparities, we propose a novel function-centric framework that categorizes objects by the functions they fulfill, across diverse cultural and economic contexts. We implement this framework by creating the Culture Affordance Atlas, a re-annotated and culturally grounded restructuring of the Dollar Street dataset spanning 46 functions and 288 objects publicly available at https://lit.eecs.umich.edu/CultureAffordance-Atlas/index.html. Through extensive empirical analyses using the CLIP model, we demonstrate that function-centric labels substantially reduce socioeconomic performance gaps between high- and low-income groups by a median of 6 pp (statistically significant), improving model effectiveness for lower-income contexts. Furthermore, our analyses reveals numerous culturally essential objects that are frequently overlooked in prominent VL datasets. Our contributions offer a scalable pathway toward building inclusive VL datasets and equitable AI systems.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.03244</link>
<guid>https://arxiv.org/abs/2512.03244</guid>
<content:encoded><![CDATA[
<div> Process reward models, reinforcement learning, synthetic training data, mathematical reasoning, self-consistency verification<br /><br />Summary:  
The paper introduces SPARK, a novel three-stage framework designed to improve process reward models (PRMs) for reinforcement learning without requiring costly step-level annotations or reference solutions. First, SPARK uses a generator model to produce diverse solutions, and a verifier model evaluates these solutions through parallel scaling (self-consistency) and sequential scaling (meta-critique). Second, the verifier outputs are used as synthetic training data to fine-tune generative process reward models, which then act as reward functions during training. Aggregating multiple step-level independent verifications in training results in PRMs that outperform ground-truth outcome supervision, demonstrated by achieving 67.5 F1 on ProcessBench, surpassing both reference-guided training (66.4) and GPT-4o (61.9). The final stage employs the fine-tuned generative PRM combined with chain-of-thought verification (PRM-CoT) as a reward model in reinforcement learning experiments targeting mathematical reasoning tasks. Format constraints are introduced in this stage to prevent reward hacking. Using the Qwen2.5-Math-7B model, the approach achieves a 47.4% average accuracy across six mathematical benchmarks, outperforming ground-truth-based RLVR (43.9%). This work enables scalable, reference-free reinforcement learning methods, especially valuable for domains lacking verifiable answers or ground truth data. <div>
arXiv:2512.03244v1 Announce Type: cross 
Abstract: Process reward models (PRMs) that provide dense, step-level feedback have shown promise for reinforcement learning, yet their adoption remains limited by the need for expensive step-level annotations or ground truth references. We propose SPARK: a three-stage framework where in the first stage a generator model produces diverse solutions and a verifier model evaluates them using parallel scaling (self-consistency) and sequential scaling (meta-critique). In the second stage, we use these verification outputs as synthetic training data to fine-tune generative process reward models, which subsequently serve as reward signals during training. We show that aggregating multiple independent verifications at the step level produces training data for process reward models that surpass ground-truth outcome supervision, achieving 67.5 F1 on ProcessBench (a benchmark for identifying erroneous steps in mathematical reasoning) compared to 66.4 for reference-guided training and 61.9 for GPT-4o. In the final stage, we apply our generative PRM with chain-of-thought verification (PRM-CoT) as the reward model in RL experiments on mathematical reasoning, and introduce format constraints to prevent reward hacking. Using Qwen2.5-Math-7B, we achieve 47.4% average accuracy across six mathematical reasoning benchmarks, outperforming ground-truth-based RLVR (43.9%). Our work enables reference-free RL training that exceeds ground-truth methods, opening new possibilities for domains lacking verifiable answers or accessible ground truth.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Is Vibe Coding Safe? Benchmarking Vulnerability of Agent-Generated Code in Real-World Tasks</title>
<link>https://arxiv.org/abs/2512.03262</link>
<guid>https://arxiv.org/abs/2512.03262</guid>
<content:encoded><![CDATA[
<div> Vibe coding, software security, large language models, vulnerability benchmark, SWE-Agent<br /><br />Summary:<br /><br />1. The paper introduces vibe coding, a programming paradigm where human engineers supervise large language model (LLM) agents to handle complex coding tasks with minimal oversight. <br /><br />2. The core question addressed is whether vibe coding outputs are safe enough to be deployed in production, especially regarding software security. <br /><br />3. To investigate this, the authors propose SU S VI B E S, a benchmark containing 200 feature-request tasks taken from real-world open-source projects, which historically resulted in vulnerable code when implemented by human programmers. <br /><br />4. The study evaluates multiple popular coding agents powered by state-of-the-art LLMs on this benchmark. Results reveal poor security performance overall—though some solutions are functionally correct, only a small fraction are secure. For example, SWE-Agent combined with Claude 4 Sonnet achieves 61% functional correctness but just 10.5% security compliance. <br /><br />5. Attempts to improve security using preliminary strategies, such as including vulnerability hints in task descriptions, fail to significantly mitigate the security flaws. <br /><br />6. The findings raise serious concerns about the risks of adopting vibe coding widely, particularly in security-critical software development contexts. <div>
arXiv:2512.03262v1 Announce Type: cross 
Abstract: Vibe coding is a new programming paradigm in which human engineers instruct large language model (LLM) agents to complete complex coding tasks with little supervision. Although it is increasingly adopted, are vibe coding outputs really safe to deploy in production? To answer this question, we propose SU S VI B E S, a benchmark consisting of 200 feature-request software engineering tasks from real-world open-source projects, which, when given to human programmers, led to vulnerable implementations. We evaluate multiple widely used coding agents with frontier models on this benchmark. Disturbingly, all agents perform poorly in terms of software security. Although 61% of the solutions from SWE-Agent with Claude 4 Sonnet are functionally correct, only 10.5% are secure. Further experiments demonstrate that preliminary security strategies, such as augmenting the feature request with vulnerability hints, cannot mitigate these security issues. Our findings raise serious concerns about the widespread adoption of vibe-coding, particularly in security-sensitive applications.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Epistemic Substitution: How Grokipedia's AI-Generated Encyclopedia Restructures Authority</title>
<link>https://arxiv.org/abs/2512.03337</link>
<guid>https://arxiv.org/abs/2512.03337</guid>
<content:encoded><![CDATA[
<div> Keywords: Wikipedia, Grokipedia, epistemic profiles, citation networks, generative AI

<br /><br />Summary:  
This study compares the epistemic foundations of AI- and human-curated encyclopedias by analyzing citation networks from 72 matched article pairs, totaling nearly 60,000 sources. It applies an 8-category epistemic classification to map the "epistemic profiles" of articles on Wikipedia and Grokipedia, an AI-generated encyclopedia. Findings show Grokipedia reduces reliance on peer-reviewed academic sources, favoring more "User-generated" and "Civic organization" references compared to Wikipedia’s traditional authority-based sourcing. The analysis reveals distinct epistemological profiles depending on the article topic: Grokipedia uses different sourcing strategies for leisure topics like Sports and Entertainment versus socially sensitive topics such as Politics, Conflicts, and Society. A novel scaling-law is identified, demonstrating a linear relationship between article length and citation density in AI-generated content, contrasting with patterns in human-written Wikipedia articles. The study concludes that LLM-based encyclopedias like Grokipedia do not simply automate knowledge curation but fundamentally restructure epistemic sourcing and justification. Given the significant shifts observed and the critical societal role of encyclopedias, the authors recommend ongoing and deeper algorithmic audits to better grasp these evolving epistemological transformations. <div>
arXiv:2512.03337v1 Announce Type: cross 
Abstract: A quarter century ago, Wikipedia's decentralized, crowdsourced, and consensus-driven model replaced the centralized, expert-driven, and authority-based standard for encyclopedic knowledge curation. The emergence of generative AI encyclopedias, such as Grokipedia, possibly presents another potential shift in epistemic evolution. This study investigates whether AI- and human-curated encyclopedias rely on the same foundations of authority. We conducted a multi-scale comparative analysis of the citation networks from 72 matched article pairs, which cite a total of almost 60,000 sources. Using an 8-category epistemic classification, we mapped the "epistemic profiles" of the articles on each platform. Our findings reveal several quantitative and qualitative differences in how knowledge is sourced and encyclopedia claims are epistemologically justified. Grokipedia replaces Wikipedia's heavy reliance on peer-reviewed "Academic & Scholarly" work with a notable increase in "User-generated" and "Civic organization" sources. Comparative network analyses further show that Grokipedia employs very different epistemological profiles when sourcing leisure topics (such as Sports and Entertainment) and more societal sensitive civic topics (such as Politics & Conflicts, Geographical Entities, and General Knowledge & Society). Finally, we find a "scaling-law for AI-generated knowledge sourcing" that shows a linear relationship between article length and citation density, which is distinct from collective human reference sourcing. We conclude that this first implementation of an LLM-based encyclopedia does not merely automate knowledge production but restructures it. Given the notable changes and the important role of encyclopedias, we suggest the continuation and deepening of algorithm audits, such as the one presented here, in order to understand the ongoing epistemological shifts.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-Generated Ads: From Personalization Parity to Persuasion Superiority</title>
<link>https://arxiv.org/abs/2512.03373</link>
<guid>https://arxiv.org/abs/2512.03373</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, personalized advertising, psychological persuasion, AI-generated ads, advertising effectiveness<br /><br />Summary:<br /><br />This paper presents a two-part study exploring the effectiveness of large language model (LLM)-generated advertisements through personality-based and psychological persuasion frameworks. First, with a sample of 400 participants, the authors assessed whether LLMs could produce ads tailored to specific personality traits (openness and neuroticism) and compared their performance with human experts. Results indicated LLM-generated ads matched human-written ads statistically (51.1% vs. 48.9%, p > 0.05), showing no significant difference when ads were matched to personality. Building on this, the second study with 800 participants evaluated LLM ads using four psychological persuasion principles: authority, consensus, cognition, and scarcity. Here, AI-generated ads significantly outperformed human ads (59.1% vs. 40.9%, p < 0.001), particularly in authority (63.0%) and consensus (62.5%) appeals. Qualitative analysis suggested AI’s advantage arises from more sophisticated, aspirational messaging and better visual-narrative coherence. Even when adjusting for participant detection of AI origin (a 21.2 percentage point penalty), AI ads maintained superiority, with nearly 30% favoring AI content despite awareness of its source. These findings highlight LLMs’ progression from parity in personalized advertising to superiority in universal persuasion, offering significant implications due to AI’s near-zero marginal costs and rapid output relative to human expertise. <div>
arXiv:2512.03373v1 Announce Type: cross 
Abstract: As large language models (LLMs) become increasingly capable of generating persuasive content, understanding their effectiveness across different advertising strategies becomes critical. This paper presents a two-part investigation examining LLM-generated advertising through complementary lenses: (1) personality-based and (2) psychological persuasion principles.
  In our first study (n=400), we tested whether LLMs could generate personalized advertisements tailored to specific personality traits (openness and neuroticism) and how their performance compared to human experts. Results showed that LLM-generated ads achieved statistical parity with human-written ads (51.1% vs. 48.9%, p > 0.05), with no significant performance differences for matched personalities.
  Building on these insights, our second study (n=800) shifted focus from individual personalization to universal persuasion, testing LLM performance across four foundational psychological principles: authority, consensus, cognition, and scarcity. AI-generated ads significantly outperformed human-created content, achieving a 59.1% preference rate (vs. 40.9%, p < 0.001), with the strongest performance in authority (63.0%) and consensus (62.5%) appeals. Qualitative analysis revealed AI's advantage stems from crafting more sophisticated, aspirational messages and achieving superior visual-narrative coherence. Critically, this quality advantage proved robust: even after applying a 21.2 percentage point detection penalty when participants correctly identified AI-origin, AI ads still outperformed human ads, and 29.4% of participants chose AI content despite knowing its origin. These findings demonstrate LLMs' evolution from parity in personalization to superiority in persuasive storytelling, with significant implications for advertising practice given LLMs' near-zero marginal cost and time requirements compared to human experts.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text-Printed Image: Bridging the Image-Text Modality Gap for Text-centric Training of Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.03463</link>
<guid>https://arxiv.org/abs/2512.03463</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-language models, text-centric training, Text-Printed Image, VQA, synthetic data augmentation<br /><br />Summary:<br />Recent large vision-language models (LVLMs) have been effective for visual question answering (VQA) but require large amounts of image-text pairs for fine-tuning, which are costly and difficult to collect. This work proposes text-centric training, relying exclusively on textual descriptions without using real images, to enable low-cost scaling of data. Text is more widely available and easily editable compared to images, allowing creation and diversification of training data using large language models (LLMs) with minimal human effort. To overcome the modality gap between text and images that limits gains on VQA tasks, the authors introduce the Text-Printed Image (TPI) method, which generates synthetic images by rendering the text descriptions on a plain white canvas. This simple technique projects text into the image modality while preserving semantic content better than current text-to-image diffusion models. Extensive experiments across four LVLM architectures and seven VQA benchmarks demonstrate that TPI outperforms diffusion-generated synthetic images for text-centric training. Additionally, TPI is explored as a low-cost data augmentation strategy, showing practical benefits for enhancing LVLM performance. Overall, the study highlights the promise of fully automated text-based data generation as a scalable and cost-effective approach for improving LVLM training and VQA capabilities. <div>
arXiv:2512.03463v1 Announce Type: cross 
Abstract: Recent large vision-language models (LVLMs) have been applied to diverse VQA tasks. However, achieving practical performance typically requires task-specific fine-tuning with large numbers of image-text pairs, which are costly to collect. In this work, we study text-centric training, a setting where only textual descriptions are available and no real images are provided, as a paradigm for low-cost data scaling. Unlike images, whose collection is often restricted by privacy constraints and scarcity in niche domains, text is widely available. Moreover, text is easily editable, enabling automatic diversification and expansion with LLMs at minimal human effort. While this offers clear advantages over image collection in terms of scalability and cost, training on raw text without images still yields limited gains on VQA tasks because of the image-text modality gap. To address this issue, we propose a Text-Printed Image (TPI), which generates synthetic images by directly rendering the given textual description on a plain white canvas. This simple rendering projects text into the image modality and can be integrated into arbitrary existing LVLM training pipelines at low cost. Moreover, TPI preserves the semantics of the text, whereas text-to-image models often fail to do. Across four models and seven benchmarks, our systematic experiments show that TPI enables more effective text-centric training than synthetic images generated by a diffusion model. We further explore TPI as a low-cost data-augmentation strategy and demonstrate its practical utility. Overall, our findings highlight the significant potential of text-centric training and, more broadly, chart a path toward fully automated data generation for LVLMs.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tuning for TraceTarnish: Techniques, Trends, and Testing Tangible Traits</title>
<link>https://arxiv.org/abs/2512.03465</link>
<guid>https://arxiv.org/abs/2512.03465</guid>
<content:encoded><![CDATA[
<div> Keywords: adversarial stylometry, TraceTarnish, stylometric features, Information Gain, authorship anonymization<br /><br />Summary:<br /><br />1. This study rigorously evaluates the attack script named TraceTarnish, designed to anonymize authorship of text messages by leveraging adversarial stylometry techniques.<br /><br />2. The authors collected and processed Reddit comments, transforming them into TraceTarnish data to analyze stylistic changes caused by the attack.<br /><br />3. Stylometric features were extracted and augmented using StyloMetrix, then filtered by Information Gain to retain the most informative and discriminative markers.<br /><br />4. Key stylometric cues identified include frequencies of function words ($L\_FUNC\_A$ and $L\_FUNC\_T$), content words ($L\_CONT\_A$ and $L\_CONT\_T$), and the Type-Token Ratio of lemmas, all showing significant Information Gain values.<br /><br />5. These features act as indicators of compromise, revealing when text has been deliberately altered to mask authorship and potentially alerting defenders to adversarial stylometric attacks, though detection often depends on comparing original and altered texts.<br /><br />6. Using these insights, the authors enhanced TraceTarnish by framing its operations around the five key features, improving the attack’s strength in anonymizing authorship.<br /><br />Overall, the work emphasizes how attempts to erase stylistic traces may inadvertently create detectable signals, advancing the understanding and effectiveness of adversarial stylometry methods. <div>
arXiv:2512.03465v1 Announce Type: cross 
Abstract: In this study, we more rigorously evaluated our attack script $\textit{TraceTarnish}$, which leverages adversarial stylometry principles to anonymize the authorship of text-based messages. To ensure the efficacy and utility of our attack, we sourced, processed, and analyzed Reddit comments--comments that were later alchemized into $\textit{TraceTarnish}$ data--to gain valuable insights. The transformed $\textit{TraceTarnish}$ data was then further augmented by $\textit{StyloMetrix}$ to manufacture stylometric features--features that were culled using the Information Gain criterion, leaving only the most informative, predictive, and discriminative ones. Our results found that function words and function word types ($L\_FUNC\_A$ $\&$ $L\_FUNC\_T$); content words and content word types ($L\_CONT\_A$ $\&$ $L\_CONT\_T$); and the Type-Token Ratio ($ST\_TYPE\_TOKEN\_RATIO\_LEMMAS$) yielded significant Information-Gain readings. The identified stylometric cues--function-word frequencies, content-word distributions, and the Type-Token Ratio--serve as reliable indicators of compromise (IoCs), revealing when a text has been deliberately altered to mask its true author. Similarly, these features could function as forensic beacons, alerting defenders to the presence of an adversarial stylometry attack; granted, in the absence of the original message, this signal may go largely unnoticed, as it appears to depend on a pre- and post-transformation comparison. "In trying to erase a trace, you often imprint a larger one." Armed with this understanding, we framed $\textit{TraceTarnish}$'s operations and outputs around these five isolated features, using them to conceptualize and implement enhancements that further strengthen the attack.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NAS-LoRA: Empowering Parameter-Efficient Fine-Tuning for Visual Foundation Models with Searchable Adaptation</title>
<link>https://arxiv.org/abs/2512.03499</link>
<guid>https://arxiv.org/abs/2512.03499</guid>
<content:encoded><![CDATA[
<div> Keywords: Segment Anything Model, Low-Rank Adaptation, Neural Architecture Search, Parameter-Efficient Fine-Tuning, visual foundation models<br /><br />Summary:<br /><br />The paper addresses the challenge of adapting the Segment Anything Model (SAM), a powerful Transformer-based image segmentation model, to specialized downstream domains such as medical and agricultural imaging. Traditional Low-Rank Adaptation (LoRA) methods help improve task-specific performance but often lack an inductive bias, as the SAM encoder inherently misses spatial priors within image patches, limiting semantically rich feature extraction. To overcome this, the authors propose NAS-LoRA, a novel Parameter-Efficient Fine-Tuning (PEFT) method that inserts a lightweight Neural Architecture Search (NAS) block between the encoder and decoder components within LoRA, enabling dynamic optimization of integrated prior knowledge during weight updates. Additionally, a stage-wise optimization strategy is introduced to balance the ViT encoder’s weight updates and architectural changes, which promotes gradual acquisition of high-level semantic understanding. Experimental results show that NAS-LoRA outperforms existing PEFT techniques by enhancing adaptation quality while simultaneously reducing training costs by 24.14% without impacting inference efficiency. This work highlights the promise of leveraging NAS to improve the adaptability and efficiency of visual foundation models like SAM for domain-specific tasks. <div>
arXiv:2512.03499v1 Announce Type: cross 
Abstract: The Segment Anything Model (SAM) has emerged as a powerful visual foundation model for image segmentation. However, adapting SAM to specific downstream tasks, such as medical and agricultural imaging, remains a significant challenge. To address this, Low-Rank Adaptation (LoRA) and its variants have been widely employed to enhancing SAM's adaptation performance on diverse domains. Despite advancements, a critical question arises: can we integrate inductive bias into the model? This is particularly relevant since the Transformer encoder in SAM inherently lacks spatial priors within image patches, potentially hindering the acquisition of high-level semantic information. In this paper, we propose NAS-LoRA, a new Parameter-Efficient Fine-Tuning (PEFT) method designed to bridge the semantic gap between pre-trained SAM and specialized domains. Specifically, NAS-LoRA incorporates a lightweight Neural Architecture Search (NAS) block between the encoder and decoder components of LoRA to dynamically optimize the prior knowledge integrated into weight updates. Furthermore, we propose a stage-wise optimization strategy to help the ViT encoder balance weight updates and architectural adjustments, facilitating the gradual learning of high-level semantic information. Various Experiments demonstrate our NAS-LoRA improves existing PEFT methods, while reducing training cost by 24.14% without increasing inference cost, highlighting the potential of NAS in enhancing PEFT for visual foundation models.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M3DR: Towards Universal Multilingual Multimodal Document Retrieval</title>
<link>https://arxiv.org/abs/2512.03514</link>
<guid>https://arxiv.org/abs/2512.03514</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal document retrieval, multilingual, cross-lingual alignment, vision-language models, contrastive training<br /><br />Summary: Multimodal document retrieval systems have advanced in aligning visual and textual information for semantic search but remain predominantly focused on English, limiting their use in multilingual environments. This paper introduces M3DR (Multilingual Multimodal Document Retrieval), a new framework designed to extend effective document retrieval across diverse languages and scripts. M3DR employs synthetic multilingual document data and is compatible with various vision-language architectures and model sizes, ensuring flexible, robust cross-lingual and cross-modal alignment. The framework uses contrastive training to produce unified representations for text and document images that generalize across 22 typologically diverse languages, demonstrating strong adaptability across linguistic and cultural variations. Additionally, the authors present a novel benchmark that simulates realistic multilingual settings and evaluates models in monolingual, multilingual, and mixed-language scenarios. M3DR supports both single dense vector retrieval and token-level multi-vector retrieval paradigms, exemplified by their models NetraEmbed and ColNetraEmbed. These models achieve state-of-the-art performance, showing approximately 150% relative improvement in cross-lingual retrieval tasks, highlighting the effectiveness and broad applicability of the approach. <div>
arXiv:2512.03514v1 Announce Type: cross 
Abstract: Multimodal document retrieval systems have shown strong progress in aligning visual and textual content for semantic search. However, most existing approaches remain heavily English-centric, limiting their effectiveness in multilingual contexts. In this work, we present M3DR (Multilingual Multimodal Document Retrieval), a framework designed to bridge this gap across languages, enabling applicability across diverse linguistic and cultural contexts. M3DR leverages synthetic multilingual document data and generalizes across different vision-language architectures and model sizes, enabling robust cross-lingual and cross-modal alignment. Using contrastive training, our models learn unified representations for text and document images that transfer effectively across languages. We validate this capability on 22 typologically diverse languages, demonstrating consistent performance and adaptability across linguistic and script variations. We further introduce a comprehensive benchmark that captures real-world multilingual scenarios, evaluating models under monolingual, multilingual, and mixed-language settings. M3DR generalizes across both single dense vector and ColBERT-style token-level multi-vector retrieval paradigms. Our models, NetraEmbed and ColNetraEmbed achieve state-of-the-art performance with ~150% relative improvements on cross-lingual retrieval.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CartoMapQA: A Fundamental Benchmark Dataset Evaluating Vision-Language Models on Cartographic Map Understanding</title>
<link>https://arxiv.org/abs/2512.03558</link>
<guid>https://arxiv.org/abs/2512.03558</guid>
<content:encoded><![CDATA[
<div> Keywords: Visual-Language Models, cartographic maps, question-answering, geospatial reasoning, CartoMapQA<br /><br />Summary:<br /><br />The paper introduces CartoMapQA, a benchmark dataset designed to evaluate Visual-Language Models' (LVLMs) ability to interpret cartographic maps through question-answering tasks. The dataset contains over 2000 samples, each consisting of a map, a question (either open-ended or multiple-choice), and a corresponding ground-truth answer. The tasks cover various map interpretation skills ranging from low-level symbol recognition to mid-level information extraction and high-level geospatial reasoning such as route-based decision-making and scale interpretation. Evaluations of open-source and proprietary LVLMs reveal significant challenges, including difficulty understanding map-specific semantic information, limited geospatial reasoning capabilities, and frequent OCR-related errors when reading text embedded in maps. By highlighting these weaknesses, CartoMapQA serves as a valuable resource for identifying gaps in current LVLM architectures. The benchmark aims to guide future model improvements toward more reliable and robust map understanding. This progress has practical implications for real-world applications relying on accurate map interpretation, including navigation, geographic search, and urban planning. The authors have made the dataset and source code openly accessible to support community-driven research and development at the provided GitHub repository. <div>
arXiv:2512.03558v1 Announce Type: cross 
Abstract: The rise of Visual-Language Models (LVLMs) has unlocked new possibilities for seamlessly integrating visual and textual information. However, their ability to interpret cartographic maps remains largely unexplored. In this paper, we introduce CartoMapQA, a benchmark specifically designed to evaluate LVLMs' understanding of cartographic maps through question-answering tasks. The dataset includes over 2000 samples, each composed of a cartographic map, a question (with open-ended or multiple-choice answers), and a ground-truth answer. These tasks span key low-, mid- and high-level map interpretation skills, including symbol recognition, embedded information extraction, scale interpretation, and route-based reasoning. Our evaluation of both open-source and proprietary LVLMs reveals persistent challenges: models frequently struggle with map-specific semantics, exhibit limited geospatial reasoning, and are prone to Optical Character Recognition (OCR)-related errors. By isolating these weaknesses, CartoMapQA offers a valuable tool for guiding future improvements in LVLM architectures. Ultimately, it supports the development of models better equipped for real-world applications that depend on robust and reliable map understanding, such as navigation, geographic search, and urban planning. Our source code and data are openly available to the research community at: https://github.com/ungquanghuy-kddi/CartoMapQA.git
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SELF: A Robust Singular Value and Eigenvalue Approach for LLM Fingerprinting</title>
<link>https://arxiv.org/abs/2512.03620</link>
<guid>https://arxiv.org/abs/2512.03620</guid>
<content:encoded><![CDATA[
<div> Fingerprinting, Intellectual Property, Large Language Models, Weight-based, Robustness

<br /><br />Summary: The paper addresses the critical challenge of protecting Intellectual Property (IP) in Large Language Models (LLMs). It highlights the limitations of existing fingerprinting techniques that rely on behavioral or structural methods which are vulnerable to false claim attacks and weight manipulation. To overcome these issues, the authors propose SELF, a novel intrinsic weight-based fingerprinting scheme that does not depend on input data and inherently resists false claims. SELF introduces two key innovations: first, it extracts unique, scalable, and transformation-invariant fingerprints using singular value and eigenvalue decomposition of attention weights within LLMs; second, it utilizes a neural network-based fingerprint similarity comparison method based on few-shot learning combined with data augmentation. Experimental results demonstrate that SELF achieves high accuracy in detecting IP infringement and maintains strong robustness against various post-training modifications such as quantization, pruning, and fine-tuning attacks. The approach thus provides an effective and reliable mechanism to safeguard LLM IP. The authors also provide their code publicly, allowing the community to reproduce and build upon their work. <div>
arXiv:2512.03620v1 Announce Type: cross 
Abstract: The protection of Intellectual Property (IP) in Large Language Models (LLMs) represents a critical challenge in contemporary AI research. While fingerprinting techniques have emerged as a fundamental mechanism for detecting unauthorized model usage, existing methods -- whether behavior-based or structural -- suffer from vulnerabilities such as false claim attacks or susceptible to weight manipulations. To overcome these limitations, we propose SELF, a novel intrinsic weight-based fingerprinting scheme that eliminates dependency on input and inherently resists false claims. SELF achieves robust IP protection through two key innovations: 1) unique, scalable and transformation-invariant fingerprint extraction via singular value and eigenvalue decomposition of LLM attention weights, and 2) effective neural network-based fingerprint similarity comparison based on few-shot learning and data augmentation. Experimental results demonstrate SELF maintains high IP infringement detection accuracy while showing strong robustness against various downstream modifications, including quantization, pruning, and fine-tuning attacks. Our code is available at https://github.com/HanxiuZhang/SELF_v2.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optical Context Compression Is Just (Bad) Autoencoding</title>
<link>https://arxiv.org/abs/2512.03643</link>
<guid>https://arxiv.org/abs/2512.03643</guid>
<content:encoded><![CDATA[
arXiv:2512.03643v1 Announce Type: cross 
Abstract: DeepSeek-OCR demonstrates that rendered text can be reconstructed with high fidelity from a small number of vision tokens. This finding has sparked excitement about vision-based context compression for language models. But the evaluation stops at reconstruction; whether these representations help language modeling remains untested. We test two assumptions implicit in the optical-compression narrative: that vision-based compression provides unique advantages for text reconstruction from compressed representations, and that DeepSeek-OCR's reconstruction results are evidence that vision-based compression will be useful for language modeling. Comparing their vision encoder against simple alternatives--parameter-free mean pooling and a learned hierarchical encoder--we find that these simple approaches match or surpass vision for reconstruction at matched compression ratios, and outperform it for language modeling--where vision-based compression fails to beat truncation. The excitement around optical context compression outpaces the evidence. Code and checkpoints are available at https://github.com/ivnle/bad-autoencoding
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thinking with Programming Vision: Towards a Unified View for Thinking with Images</title>
<link>https://arxiv.org/abs/2512.03746</link>
<guid>https://arxiv.org/abs/2512.03746</guid>
<content:encoded><![CDATA[
arXiv:2512.03746v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) that think with images can interactively use tools to reason about visual inputs, but current approaches often rely on a narrow set of tools with limited real-world necessity and scalability. In this work, we first reveal a critical and previously overlooked weakness: even state-of-the-art MLLMs are surprisingly brittle, showing significant performance degradation on images with simple orientation changes or natural corruptions, underscoring the need for more robust tool-based reasoning. To address this, we propose CodeVision, a flexible and scalable code-as-tool framework where the model generates code as a universal interface to invoke any image operation, moving beyond fixed tool registries. We train our model using a two-stage methodology, beginning with Supervised Fine-Tuning (SFT) on a high-quality dataset curated for complex, multi-turn tool composition and error recovery, followed by Reinforcement Learning (RL) with a novel and dense process reward function to encourage strategic and efficient tool use. To facilitate this research, we construct new SFT and RL datasets and introduce a challenging new benchmark suite designed to rigorously evaluate robustness to orientation changes and multi-tool reasoning. Experiments on Qwen2.5-VL and Qwen3-VL series show that our approach significantly improves model performance and fosters emergent capabilities such as flexible tool composition, efficient chained execution, and robust error recovery from runtime feedback. Code is available at https://github.com/ByteDance-BandAI/CodeVision.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition</title>
<link>https://arxiv.org/abs/2512.03794</link>
<guid>https://arxiv.org/abs/2512.03794</guid>
<content:encoded><![CDATA[
arXiv:2512.03794v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) have achieved remarkable success in visual question answering tasks, but their reliance on large numbers of visual tokens introduces significant computational overhead. While existing efficient VLM approaches reduce visual tokens through fixed-ratio compression, they operate passively and lack the ability to adapt to varying task requirements. This motivates a fundamental question: Can VLMs autonomously determine the minimum number of visual tokens required for each sample? Inspired by human active vision mechanisms, we introduce AdaptVision, an efficient VLM paradigm that enables adaptive visual token acquisition through a coarse-to-fine approach. Our model initially processes compressed visual tokens from low-resolution images and selectively acquires additional visual information by invoking a bounding box tool to crop key regions when necessary. We train AdaptVision using a reinforcement learning framework that carefully balances accuracy and efficiency. Central to our approach is Decoupled Turn Policy Optimization (DTPO), which decouples the learning objective into two components: (1) tool learning, which optimizes correct tool utilization, and (2) accuracy improvement, which refines the generated responses to improve answer correctness. Based on this formulation, we further decouple advantage estimation by computing separate advantages for tokens associated with each objective. This formulation enables more effective optimization for AdaptVision compared to vanilla GRPO. Comprehensive experiments across multiple VQA benchmarks demonstrate that AdaptVision achieves superior performance while consuming substantially fewer visual tokens than state-of-the-art efficient VLM methods.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stable Signer: Hierarchical Sign Language Generative Model</title>
<link>https://arxiv.org/abs/2512.04048</link>
<guid>https://arxiv.org/abs/2512.04048</guid>
<content:encoded><![CDATA[
arXiv:2512.04048v1 Announce Type: cross 
Abstract: Sign Language Production (SLP) is the process of converting the complex input text into a real video. Most previous works focused on the Text2Gloss, Gloss2Pose, Pose2Vid stages, and some concentrated on Prompt2Gloss and Text2Avatar stages. However, this field has made slow progress due to the inaccuracy of text conversion, pose generation, and the rendering of poses into real human videos in these stages, resulting in gradually accumulating errors. Therefore, in this paper, we streamline the traditional redundant structure, simplify and optimize the task objective, and design a new sign language generative model called Stable Signer. It redefines the SLP task as a hierarchical generation end-to-end task that only includes text understanding (Prompt2Gloss, Text2Gloss) and Pose2Vid, and executes text understanding through our proposed new Sign Language Understanding Linker called SLUL, and generates hand gestures through the named SLP-MoE hand gesture rendering expert block to end-to-end generate high-quality and multi-style sign language videos. SLUL is trained using the newly developed Semantic-Aware Gloss Masking Loss (SAGM Loss). Its performance has improved by 48.6% compared to the current SOTA generation methods.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Group Fairness Lens for Large Language Models</title>
<link>https://arxiv.org/abs/2312.15478</link>
<guid>https://arxiv.org/abs/2312.15478</guid>
<content:encoded><![CDATA[
arXiv:2312.15478v2 Announce Type: replace 
Abstract: The need to assess LLMs for bias and fairness is critical, with current evaluations often being narrow, missing a broad categorical view. In this paper, we propose evaluating the bias and fairness of LLMs from a group fairness lens using a novel hierarchical schema characterizing diverse social groups. Specifically, we construct a dataset, GFAIR, encapsulating target-attribute combinations across multiple dimensions. Moreover, we introduce statement organization, a new open-ended text generation task, to uncover complex biases in LLMs. Extensive evaluations of popular LLMs reveal inherent safety concerns. To mitigate the biases of LLMs from a group fairness perspective, we pioneer a novel chainof-thought method GF-THINK to mitigate biases of LLMs from a group fairness perspective. Experimental results demonstrate its efficacy in mitigating bias and achieving fairness in LLMs. Our dataset and codes are available at https://github.com/surika/Group-Fairness-LLMs.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IW-Bench: Evaluating Large Multimodal Models for Converting Image-to-Web</title>
<link>https://arxiv.org/abs/2409.18980</link>
<guid>https://arxiv.org/abs/2409.18980</guid>
<content:encoded><![CDATA[
arXiv:2409.18980v2 Announce Type: replace 
Abstract: Recently advancements in large multimodal models have led to significant strides in image comprehension capabilities. Despite these advancements, there is a lack of the robust benchmark specifically for assessing the Image-to-Web conversion proficiency of these large models. Primarily, it is essential to ensure the integrity of the web elements generated. These elements comprise visible and invisible categories. Previous evaluation methods (e.g.,BLEU) are notably susceptible to significant alterations due to the presence of invisible elements in Web. Furthermore, it is crucial to measure the layout information of web pages, referring to the positional relationships between elements, which is overlooked by previous work. To address challenges, we have curated and aligned a benchmark of images and corresponding web codes (IW-BENCH). Specifically, we propose the Element Accuracy, which tests the completeness of the elements by parsing the Document Object Model (DOM) tree. Layout Accuracy is also proposed to analyze the positional relationships of elements by converting DOM tree into a common subsequence. Besides, we design a five-hop multimodal Chain-of-Thought Prompting for better performance, which contains five hop: 1) SoM prompt injection. 2) Inferring Elements. 3) Inferring Layout. 4) Inferring Web code. 5) Reflection. Our benchmark comprises 1200 pairs of images and web codes with varying levels of difficulty. We have conducted extensive experiments on existing large multimodal models, offering insights into their performance and areas for improvement in image-to-web domain.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How to Train Long-Context Language Models (Effectively)</title>
<link>https://arxiv.org/abs/2410.02660</link>
<guid>https://arxiv.org/abs/2410.02660</guid>
<content:encoded><![CDATA[
arXiv:2410.02660v4 Announce Type: replace 
Abstract: We study continued training and supervised fine-tuning (SFT) of a language model (LM) to make effective use of long-context information. We first establish a reliable evaluation protocol to guide model development -- instead of perplexity or simple needle-in-a-haystack (NIAH) tests, we use a broad set of long-context downstream tasks, and we evaluate models after SFT as this better reveals long-context abilities. Supported by our robust evaluations, we run thorough experiments to decide the data mix for continued pre-training, the instruction tuning dataset, and many other design choices such as position extrapolation. We find that (1) code repositories and books are excellent sources of long data, but it is crucial to combine them with high-quality short-context data; (2) training with a sequence length beyond the evaluation length boosts long-context performance; (3) for SFT, using only short instruction datasets yields strong performance on long-context tasks. Our final model, ProLong-8B, which is initialized from Llama-3 and trained on 40B tokens, demonstrates state-of-the-art long-context performance among similarly sized models at a length of 128K. ProLong outperforms Llama-3.1-8B-Instruct on the majority of long-context tasks despite using only 5% as many tokens during long-context training. Additionally, ProLong can effectively process up to 512K tokens, one of the longest context windows of publicly available LMs.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty Quantification for LLMs through Minimum Bayes Risk: Bridging Confidence and Consistency</title>
<link>https://arxiv.org/abs/2502.04964</link>
<guid>https://arxiv.org/abs/2502.04964</guid>
<content:encoded><![CDATA[
arXiv:2502.04964v5 Announce Type: replace 
Abstract: Uncertainty quantification (UQ) methods for Large Language Models (LLMs) encompass a variety of approaches, with two major types being particularly prominent: information-based, which focus on model confidence expressed as token probabilities, and consistency-based, which assess the semantic relationship between multiple outputs generated using repeated sampling. Several recent methods have combined these two approaches to boost UQ performance. However, they sometimes fail to outperform much simpler baseline methods. Our work discusses the fundamental approach to constructing uncertainty measures that directly links uncertainty with the minimum Bayes risks achieved by LLM decoding. Building on these findings, we propose a novel approach to integrating model confidence with output consistency, resulting in a family of efficient and robust UQ methods. Our investigation reveals distinctive characteristics of LLMs as probabilistic models, which help to explain why these UQ methods underperform in certain tasks. Based on these findings, we propose a new way of synthesizing model confidence and output consistency, leading to a family of efficient and robust UQ methods. We evaluate our approach across various tasks such as question answering, abstractive summarization, and machine translation, demonstrating sizable improvements over state-of-the-art UQ approaches.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Multimodal Search and Recommendation with Small Language Models via Upside-Down Reinforcement Learning</title>
<link>https://arxiv.org/abs/2502.09854</link>
<guid>https://arxiv.org/abs/2502.09854</guid>
<content:encoded><![CDATA[
arXiv:2502.09854v2 Announce Type: replace 
Abstract: In this work, we investigate how small language models (SLMs) can be scaled to support multimodal search and recommendation use cases while remaining efficient enough for real-time, resource-constrained deployments. We present a framework that combines upside-down reinforcement learning with synthetic data distillation from a large language model (Llama-3) to train a 100M-parameter GPT-2 model for multitask prompt generation. Despite being up to 80 times smaller than state-of-the-art large language models (LLMs), our SLM achieves relevance and diversity scores within 6% of competitive baselines such as Llama-3 8B, Qwen3 8B, and Ministral 8B. These results demonstrate that SLMs can effectively handle multimodal search and recommendation tasks, while dramatically reducing inference latency and memory overhead. Our study highlights the potential of lightweight models as practical engines for scalable multimodal discovery, bridging the gap between cutting-edge research and real-world multimodal applications such as media recommendations and creative content generation.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bigram Subnetworks: Mapping to Next Tokens in Transformer Language Models</title>
<link>https://arxiv.org/abs/2504.15471</link>
<guid>https://arxiv.org/abs/2504.15471</guid>
<content:encoded><![CDATA[
arXiv:2504.15471v3 Announce Type: replace 
Abstract: In Transformer language models, activation vectors transform from current token embeddings to next token predictions as they pass through the model. To isolate a minimal form of this transformation, we identify language model subnetworks that make bigram predictions, naive next token predictions based only on the current token. We find that bigram subnetworks can be found in fully trained language models up to 1B parameters, and these subnetworks are critical for model performance even when they consist of less than 0.2% of model parameters. Bigram subnetworks are concentrated in the first Transformer MLP layer, and they overlap significantly with subnetworks trained to optimally prune a given model. Mechanistically, the bigram subnetworks often recreate a pattern from the full models where the first layer induces a sharp change that aligns activations with next token predictions rather than current token representations. Our results demonstrate that bigram subnetworks comprise a minimal subset of parameters that are both necessary and sufficient for basic next token predictions in language models, and they help drive the transformation from current to next token activations in the residual stream. These subnetworks can lay a foundation for studying more complex language model circuits by building up from a minimal circuit.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2505.10792</link>
<guid>https://arxiv.org/abs/2505.10792</guid>
<content:encoded><![CDATA[
arXiv:2505.10792v3 Announce Type: replace 
Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful framework to improve factuality in large language models (LLMs) by grounding their outputs in retrieved documents. However, ensuring perfect retrieval of relevant information remains challenging, and when irrelevant content is passed downstream to an LLM, it can lead to hallucinations. In this work, we propose Finetune-RAG, a simple and effective fine-tuning approach that features the first-of-its-kind RAG training dataset constructed to mimic real-world imperfections. Experimental results show that Finetune-RAG improves factual accuracy by 21.2% over the base model. We also propose Bench-RAG, an LLM-as-a-judge evaluation pipeline that stress tests models under realistic imperfect retrieval scenarios. Our codebase and dataset are fully open sourced for community use.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned RL</title>
<link>https://arxiv.org/abs/2505.18098</link>
<guid>https://arxiv.org/abs/2505.18098</guid>
<content:encoded><![CDATA[
arXiv:2505.18098v2 Announce Type: replace 
Abstract: Large language models (LLMs) excel in tasks like question answering and dialogue, but complex tasks requiring interaction, such as negotiation and persuasion, require additional long-horizon reasoning and planning. Reinforcement learning (RL) fine-tuning can enable such planning in principle, but suffers from drawbacks that hinder scalability. In particular, multi-turn RL training incurs high memory and computational costs, which are exacerbated when training LLMs as policies. Furthermore, the largest LLMs do not expose the APIs necessary to be trained in such manner. As a result, modern methods to improve the reasoning of LLMs rely on sophisticated prompting mechanisms rather than RL fine-tuning. To remedy this, we propose a novel approach that uses goal-conditioned value functions to guide the reasoning of LLM agents, that scales even to large API-based models. These value functions predict how a task will unfold given an action, allowing the LLM agent to evaluate multiple possible outcomes, both positive and negative, to plan effectively. In addition, these value functions are trained over reasoning steps rather than full actions, to be a concise and light-weight module that facilitates decision-making in multi-turn interactions. We validate our method on tasks requiring interaction, including tool use, social deduction, and dialogue, demonstrating superior performance over both RL fine-tuning and prompting methods while maintaining efficiency and scalability.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Proximalized Preference Optimization for Diverse Feedback Types: A Decomposed Perspective on DPO</title>
<link>https://arxiv.org/abs/2505.23316</link>
<guid>https://arxiv.org/abs/2505.23316</guid>
<content:encoded><![CDATA[
arXiv:2505.23316v2 Announce Type: replace 
Abstract: Direct alignment methods typically train large language models (LLMs) by contrasting the likelihoods of preferred and dispreferred responses. While effective at capturing relative preferences, these methods are widely observed to suppress the absolute likelihoods of example responses. As a result, aligned models can deviate from expected patterns, exhibiting rewar-hacking effect even without an explicit reward model. This fundamental limitation of contrastive alignment, which we term likelihood underdetermination, motivates us to revisit direct preference optimization (DPO) -- the seminal direct alignment method. Interestingly, we show that the DPO loss admits a principled decomposition. The reformulated loss not only extends naturally to a broader range of feedback types, but also unveils the root cause of likelihood underdetermination. Specifically, we identify that standard DPO implicitly oversimplifies a regularizer in the reformulated loss; restoring this full term effectively resolves the underdetermination. Building on these insights, we introduce PRoximalized PReference Optimization (PRO), a unified alignment method that accommodates diverse feedback types while eliminating likelihood underdetermination through an efficient approximation of the full regularizer. Empirical evaluations demonstrate the consistent superiority of PRO over existing methods across pairwise, binary and scalar feedback.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Characterizing the Expressivity of Fixed-Precision Transformer Language Models</title>
<link>https://arxiv.org/abs/2505.23623</link>
<guid>https://arxiv.org/abs/2505.23623</guid>
<content:encoded><![CDATA[
arXiv:2505.23623v2 Announce Type: replace 
Abstract: Transformer-based language models (LMs) have achieved widespread empirical success, but their theoretical expressive power remains only partially understood. In this work, we analyze a restricted idealization of fixed-precision transformers with strict future masking, soft attention, and no positional encodings. We establish that this class of models is exactly as expressive as a specific fragment of linear temporal logic that contains only a single temporal operator: the past operator. We further connect this fragment to established classes in formal language theory, automata theory, and algebra, yielding a unified framework for understanding transformer expressivity under this idealization. Finally, we present empirical results that align closely with our theory: transformers trained on languages within their characterized expressive capacity generalize reliably across sequence lengths, while they consistently fail to generalize on languages beyond it.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Let Them Down Easy! Contextual Effects of LLM Guardrails on User Perceptions and Preferences</title>
<link>https://arxiv.org/abs/2506.00195</link>
<guid>https://arxiv.org/abs/2506.00195</guid>
<content:encoded><![CDATA[
arXiv:2506.00195v2 Announce Type: replace 
Abstract: Current LLMs are trained to refuse potentially harmful input queries regardless of whether users actually had harmful intents, causing a tradeoff between safety and user experience. Through a study of 480 participants evaluating 3,840 query-response pairs, we examine how different refusal strategies affect user perceptions across varying motivations. Our findings reveal that response strategy largely shapes user experience, while actual user motivation has negligible impact. Partial compliance -- providing general information without actionable details -- emerges as the optimal strategy, reducing negative user perceptions by over 50% to flat-out refusals. Complementing this, we analyze response patterns of 9 state-of-the-art LLMs and evaluate how 6 reward models score different refusal strategies, demonstrating that models rarely deploy partial compliance naturally and reward models currently undervalue it. This work demonstrates that effective guardrails require focusing on crafting thoughtful refusals rather than detecting intent, offering a path toward AI safety mechanisms that ensure both safety and sustained user engagement.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MemOS: A Memory OS for AI System</title>
<link>https://arxiv.org/abs/2507.03724</link>
<guid>https://arxiv.org/abs/2507.03724</guid>
<content:encoded><![CDATA[
arXiv:2507.03724v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) have become an essential infrastructure for Artificial General Intelligence (AGI), yet their lack of well-defined memory management systems hinders the development of long-context reasoning, continual personalization, and knowledge consistency.Existing models mainly rely on static parameters and short-lived contextual states, limiting their ability to track user preferences or update knowledge over extended periods.While Retrieval-Augmented Generation (RAG) introduces external knowledge in plain text, it remains a stateless workaround without lifecycle control or integration with persistent representations.Recent work has modeled the training and inference cost of LLMs from a memory hierarchy perspective, showing that introducing an explicit memory layer between parameter memory and external retrieval can substantially reduce these costs by externalizing specific knowledge. Beyond computational efficiency, LLMs face broader challenges arising from how information is distributed over time and context, requiring systems capable of managing heterogeneous knowledge spanning different temporal scales and sources. To address this challenge, we propose MemOS, a memory operating system that treats memory as a manageable system resource. It unifies the representation, scheduling, and evolution of plaintext, activation-based, and parameter-level memories, enabling cost-efficient storage and retrieval. As the basic unit, a MemCube encapsulates both memory content and metadata such as provenance and versioning. MemCubes can be composed, migrated, and fused over time, enabling flexible transitions between memory types and bridging retrieval with parameter-based learning. MemOS establishes a memory-centric system framework that brings controllability, plasticity, and evolvability to LLMs, laying the foundation for continual learning and personalized modeling.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy-protected Retrieval-Augmented Generation for Knowledge Graph Question Answering</title>
<link>https://arxiv.org/abs/2508.08785</link>
<guid>https://arxiv.org/abs/2508.08785</guid>
<content:encoded><![CDATA[
arXiv:2508.08785v2 Announce Type: replace 
Abstract: LLMs often suffer from hallucinations and outdated or incomplete knowledge. RAG is proposed to address these issues by integrating external knowledge like that in KGs into LLMs. However, leveraging private KGs in RAG systems poses significant privacy risks due to the black-box nature of LLMs and potential insecure data transmission, especially when using third-party LLM APIs lacking transparency and control. In this paper, we investigate the privacy-protected RAG scenario for the first time, where entities in KGs are anonymous for LLMs, thus preventing them from accessing entity semantics. Due to the loss of semantics of entities, previous RAG systems cannot retrieve question-relevant knowledge from KGs by matching questions with the meaningless identifiers of anonymous entities. To realize an effective RAG system in this scenario, two key challenges must be addressed: (1) How can anonymous entities be converted into retrievable information. (2) How to retrieve question-relevant anonymous entities. Hence, we propose a novel ARoG framework including relation-centric abstraction and structure-oriented abstraction strategies. For challenge (1), the first strategy abstracts entities into high-level concepts by dynamically capturing the semantics of their adjacent relations. It supplements meaningful semantics which can further support the retrieval process. For challenge (2), the second strategy transforms unstructured natural language questions into structured abstract concept paths. These paths can be more effectively aligned with the abstracted concepts in KGs, thereby improving retrieval performance. To guide LLMs to effectively retrieve knowledge from KGs, the two strategies strictly protect privacy from being exposed to LLMs. Experiments on three datasets demonstrate that ARoG achieves strong performance and privacy-robustness.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reveal-Bangla: A Dataset for Cross-Lingual Multi-Step Reasoning Evaluation</title>
<link>https://arxiv.org/abs/2508.08933</link>
<guid>https://arxiv.org/abs/2508.08933</guid>
<content:encoded><![CDATA[
arXiv:2508.08933v3 Announce Type: replace 
Abstract: Language models have demonstrated remarkable performance on complex multi-step reasoning tasks. However, their evaluation has been predominantly confined to high-resource languages such as English. In this paper, we introduce a manually translated Bangla multi-step reasoning dataset derived from the English Reveal dataset, featuring both binary and non-binary question types. We conduct a controlled evaluation of English-centric and Bangla-centric multilingual small language models on the original dataset and our translated version to compare their ability to exploit relevant reasoning steps to produce correct answers. Our results show that, in comparable settings, reasoning context is beneficial for more challenging non-binary questions, but models struggle to employ relevant Bangla reasoning steps effectively. We conclude by exploring how reasoning steps contribute to models' predictions, highlighting different trends across models and languages.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RECAP: Transparent Inference-Time Emotion Alignment for Medical Dialogue Systems</title>
<link>https://arxiv.org/abs/2509.10746</link>
<guid>https://arxiv.org/abs/2509.10746</guid>
<content:encoded><![CDATA[
arXiv:2509.10746v2 Announce Type: replace 
Abstract: Large language models in healthcare often miss critical emotional cues, delivering medically sound but emotionally flat advice. Such responses are insufficient in clinical encounters, where distressed or vulnerable patients rely on empathic communication to support safety, adherence, and trust. We present RECAP (Reflect-Extract-Calibrate-Align-Produce), an inference-time framework that guides models through structured emotional reasoning without retraining. RECAP decomposes patient input into appraisal-theoretic stages, identifies psychological factors, and assigns Likert-based emotion likelihoods that clinicians can inspect or override, producing nuanced and auditable responses. Across EmoBench, SECEU, and EQ-Bench, RECAP improves emotional reasoning by 22-28% on 8B models and 10-13% on larger models over zero-shot baselines. In blinded evaluations, oncology clinicians rated RECAP's responses as more empathetic, supportive, and context-appropriate than prompting baselines. These findings demonstrate that modular, principled prompting can enhance emotional intelligence in medical AI while maintaining transparency and accountability for clinical deployment.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Investigating Bias: A Multilingual Pipeline for Generating, Solving, and Evaluating Math Problems with LLMs</title>
<link>https://arxiv.org/abs/2509.17701</link>
<guid>https://arxiv.org/abs/2509.17701</guid>
<content:encoded><![CDATA[
arXiv:2509.17701v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly used for educational support, yet their response quality varies depending on the language of interaction. This paper presents an automated multilingual pipeline for generating, solving, and evaluating math problems aligned with the German K-10 curriculum. We generated 628 math exercises and translated them into English, German, and Arabic. Three commercial LLMs (GPT-4o-mini, Gemini 2.5 Flash, and Qwen-plus) were prompted to produce step-by-step solutions in each language. A held-out panel of LLM judges, including Claude 3.5 Haiku, evaluated solution quality using a comparative framework. Results show a consistent gap, with English solutions consistently rated highest, and Arabic often ranked lower. These findings highlight persistent linguistic bias and the need for more equitable multilingual AI systems in education.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Batch Prompting Suppresses Overthinking Reasoning Under Constraint: How Batch Prompting Suppresses Overthinking in Reasoning Models</title>
<link>https://arxiv.org/abs/2511.04108</link>
<guid>https://arxiv.org/abs/2511.04108</guid>
<content:encoded><![CDATA[
arXiv:2511.04108v2 Announce Type: replace 
Abstract: Recent work has explored batch prompting as a strategy to amortize inference cost in large language models (LLMs). In this paper, we show that batching offers an additional, underappreciated benefit: it regularizes model behavior during multi-step reasoning for Large Reasoning Models (LRMs). We conduct a comprehensive study across 13 diverse benchmarks and observe that batching improves accuracy while substantially reducing reasoning token usage, often by 3x-5x. Through detailed behavioral analysis, we find that batching suppresses overthinking, reduces hedging language (e.g., repetitive self-corrections), and encourages more decisive answers. Surprisingly, we also observe emergent collective effects in batched inference: models often generalize patterns from earlier examples to solve harder ones in the same batch. These findings position batching not just as a throughput optimization, but as a powerful inference-time regularizer for more efficient and reliable LLM reasoning.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Focusing on Language: Revealing and Exploiting Language Attention Heads in Multilingual Large Language Models</title>
<link>https://arxiv.org/abs/2511.07498</link>
<guid>https://arxiv.org/abs/2511.07498</guid>
<content:encoded><![CDATA[
arXiv:2511.07498v2 Announce Type: replace 
Abstract: Large language models (LLMs) increasingly support multilingual understanding and generation. Meanwhile, efforts to interpret their internal mechanisms have emerged, offering insights to enhance multilingual performance. While multi-head self-attention (MHA) has proven critical in many areas, its role in multilingual capabilities remains underexplored. In this work, we study the contribution of MHA in supporting multilingual processing in LLMs. We propose Language Attention Head Importance Scores (LAHIS), an effective and efficient method that identifies attention head importance for multilingual capabilities via a single forward and backward pass through the LLM. Applying LAHIS to Aya-23-8B, Llama-3.2-3B, and Mistral-7B-v0.1, we reveal the existence of both language-specific and language-general heads. Language-specific heads enable cross-lingual attention transfer to guide the model toward target language contexts and mitigate off-target language generation issue, contributing to addressing challenges in multilingual LLMs. We also introduce a lightweight adaptation that learns a soft head mask to modulate attention outputs over language heads, requiring only 20 tunable parameters to improve XQuAD accuracy. Overall, our work enhances both the interpretability and multilingual capabilities of LLMs from the perspective of MHA.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context Cascade Compression: Exploring the Upper Limits of Text Compression</title>
<link>https://arxiv.org/abs/2511.15244</link>
<guid>https://arxiv.org/abs/2511.15244</guid>
<content:encoded><![CDATA[
arXiv:2511.15244v2 Announce Type: replace 
Abstract: Million-level token inputs in long-context tasks pose significant computational and memory challenges for Large Language Models (LLMs). Recently, DeepSeek-OCR conducted research into the feasibility of Contexts Optical Compression and achieved preliminary results. Inspired by this, we introduce Context Cascade Compression C3 to explore the upper limits of text compression. Our method cascades two LLMs of different sizes to handle the compression and decoding tasks. Specifically, a small LLM, acting as the first stage, performs text compression by condensing a long context into a set of latent tokens (e.g., 32 or 64 in length), achieving a high ratio of text tokens to latent tokens. A large LLM, as the second stage, then executes the decoding task on this compressed context. Experiments show that at a 20x compression ratio (where the number of text tokens is 20 times the number of latent tokens), our model achieves 98% decoding accuracy, compared to approximately 60% for DeepSeek-OCR. When we further increase the compression ratio to 40x, the accuracy is maintained at around 93%. This indicates that in the domain of context compression, C3 Compression demonstrates superior performance and feasibility over optical character compression. C3 uses a simpler, pure-text pipeline that ignores factors like layout, color, and information loss from a visual encoder. This also suggests a potential upper bound for compression ratios in future work on optical character compression, OCR, and related fields. Codes and model weights are publicly accessible at https://github.com/liufanfanlff/C3-Context-Cascade-Compression
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SETS: Leveraging Self-Verification and Self-Correction for Improved Test-Time Scaling</title>
<link>https://arxiv.org/abs/2501.19306</link>
<guid>https://arxiv.org/abs/2501.19306</guid>
<content:encoded><![CDATA[
arXiv:2501.19306v5 Announce Type: replace-cross 
Abstract: Recent advancements in Large Language Models (LLMs) have created new opportunities to enhance performance on complex reasoning tasks by leveraging test-time computation. However, existing scaling methods have key limitations: parallel methods like repeated sampling are often inefficient and quickly saturate, while sequential methods like SELF-REFINE struggle to improve after a few rounds. Although combining these approaches shows promise, current methods require fine-tuned reward and revision models. This paper proposes Self-Enhanced Test-Time Scaling (SETS), a simple yet effective approach that overcomes these limitations by strategically combining parallel and sequential techniques and fully leveraging LLMs' self-improvement abilities. SETS exploits the inherent self-verification and self-correction capabilities of LLMs, unifying sampling, verification, and correction within a single framework. This facilitates efficient and scalable test-time computation for enhanced performance on complex tasks without any model training. Our comprehensive experimental results on challenging benchmarks spanning planning, reasoning, math, and coding demonstrate that SETS achieves significant performance improvements and more advantageous test-time scaling behavior than the alternatives.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Which Type of Students can LLMs Act? Investigating Authentic Simulation with Graph-based Human-AI Collaborative System</title>
<link>https://arxiv.org/abs/2502.11678</link>
<guid>https://arxiv.org/abs/2502.11678</guid>
<content:encoded><![CDATA[
arXiv:2502.11678v3 Announce Type: replace-cross 
Abstract: While rapid advances in large language models (LLMs) are reshaping data-driven intelligent education, accurately simulating students remains an important but challenging bottleneck for scalable educational data collection, evaluation, and intervention design. However, current works are limited by scarce real interaction data, costly expert evaluation for realism, and a lack of large-scale, systematic analyses of LLMs ability in simulating students. We address this gap by presenting a three-stage LLM-human collaborative pipeline to automatically generate and filter high-quality student agents. We leverage a two-round automated scoring validated by human experts and deploy a score propagation module to obtain more consistent scores across the student similarity graph. Experiments show that combining automated scoring, expert calibration, and graph-based propagation yields simulated student that more closely track authentication by human judgments. We then analyze which profiles and behaviors are simulated more faithfully, supporting subsequent studies on personalized learning and educational assessment.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal LLM Routing: End-to-End Regret Minimization from Observational Data</title>
<link>https://arxiv.org/abs/2505.16037</link>
<guid>https://arxiv.org/abs/2505.16037</guid>
<content:encoded><![CDATA[
arXiv:2505.16037v2 Announce Type: replace-cross 
Abstract: LLM routing aims to select the most appropriate model for each query, balancing competing performance metrics such as accuracy and cost across a pool of language models. Prior approaches typically adopt a decoupled strategy, where the metrics are first predicted and the model is then selected based on these estimates. This setup is prone to compounding errors and often relies on full-feedback data, where each query is evaluated by all candidate models, which is costly to obtain and maintain in practice. In contrast, we learn from observational data, which records only the outcome of the model actually deployed. We propose a causal end-to-end framework that learns routing policies by minimizing decision-making regret from observational data. To enable efficient optimization, we introduce two theoretically grounded surrogate objectives: a classification-based upper bound, and a softmax-weighted regret approximation shown to recover the optimal policy at convergence. We further extend our framework to handle heterogeneous cost preferences via an interval-conditioned architecture. Experiments on public benchmarks show that our method outperforms existing baselines, achieving state-of-the-art performance across different embedding models.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comba: Improving Bilinear RNNs with Closed-loop Control</title>
<link>https://arxiv.org/abs/2506.02475</link>
<guid>https://arxiv.org/abs/2506.02475</guid>
<content:encoded><![CDATA[
arXiv:2506.02475v5 Announce Type: replace-cross 
Abstract: Recent efficient sequence modeling methods such as Gated DeltaNet, TTT, and RWKV-7 have achieved performance improvements by supervising the recurrent memory management through Delta learning rule. Unlike previous state-space models (e.g., Mamba) and gated linear attentions (e.g., GLA), these models introduce interactions between the recurrent state and the key vector, structurally resembling bilinear systems. In this paper, we first introduce the concept of Bilinear RNNs with a comprehensive analysis on the advantages and limitations of these models. Then, based on closed-loop control theory, we propose a novel Bilinear RNN variant named Comba, which adopts a scalar-plus-low-rank state transition, with both state feedback and output feedback corrections. We also implement a hardware-efficient chunk-wise parallel kernel in Triton and train models with 340M/1.3B parameters on large-scale corpus. Comba demonstrates superior performance and computation efficiency in both language and vision modeling.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition Query</title>
<link>https://arxiv.org/abs/2506.03144</link>
<guid>https://arxiv.org/abs/2506.03144</guid>
<content:encoded><![CDATA[
arXiv:2506.03144v3 Announce Type: replace-cross 
Abstract: Semantic retrieval is crucial for modern applications yet remains underexplored in current research. Existing datasets are limited to single languages, single images, or singular retrieval conditions, often failing to fully exploit the expressive capacity of visual information as evidenced by maintained performance when images are replaced with captions. However, practical retrieval scenarios frequently involve interleaved multi-condition queries with multiple images. Hence, this paper introduces MERIT, the first multilingual dataset for interleaved multi-condition semantic retrieval, comprising 320,000 queries with 135,000 products in 5 languages, covering 7 distinct product categories. Extensive experiments on MERIT identify existing models's limitation: focusing solely on global semantic information while neglecting specific conditional elements in queries. Consequently, we propose Coral, a novel fine-tuning framework that adapts pre-trained MLLMs by integrating embedding reconstruction to preserve fine-grained conditional elements and contrastive learning to extract comprehensive global semantics. Experiments demonstrate that Coral achieves a 45.9% performance improvement over conventional approaches on MERIT, with strong generalization capabilities validated across 8 established retrieval benchmarks. Collectively, our contributions - a novel dataset, identification of critical limitations in existing approaches, and an innovative fine-tuning framework - establish a foundation for future research in interleaved multi-condition semantic retrieval.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GTPO: Stabilizing Group Relative Policy Optimization via Gradient and Entropy Control</title>
<link>https://arxiv.org/abs/2508.03772</link>
<guid>https://arxiv.org/abs/2508.03772</guid>
<content:encoded><![CDATA[
arXiv:2508.03772v4 Announce Type: replace-cross 
Abstract: Group Relative Policy Optimization (GRPO) is a promising policy-based approach for Large Language Model alignment, yet its performance is often limited by training instability and suboptimal convergence. In this paper, we identify and analyze two main GRPO issues: (i) the token-level penalization, where valuable tokens shared across different responses receive contradictory feedback signals, leading to conflicting gradient updates that can reduce their likelihood; and (ii) the policy collapse, where negatively rewarded completions may penalize confident responses and shift model decisions toward unlikely tokens, destabilizing training process. To address these issues we introduce GTPO (Group-relative Trajectory-based Policy Optimization), which prevents conflicting gradients on valuable tokens by skipping negative updates while amplifying positive ones and filters out completions whose entropy exceeds a provable threshold, to prevent policy collapse. Unlike GRPO, GTPO does not rely on KL-divergence regularization, eliminating the need for a reference model during training, while still ensuring greater training stability and improved performance, as validated through multiple experiments on GSM8K, MATH, AIME 2024, AIME 2025 and AMC 2023.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference</title>
<link>https://arxiv.org/abs/2508.09442</link>
<guid>https://arxiv.org/abs/2508.09442</guid>
<content:encoded><![CDATA[
arXiv:2508.09442v2 Announce Type: replace-cross 
Abstract: The Key-Value (KV) cache, which stores intermediate attention computations (Key and Value pairs) to avoid redundant calculations, is a fundamental mechanism for accelerating Large Language Model (LLM) inference. However, this efficiency optimization introduces significant yet underexplored privacy risks. This paper provides the first comprehensive analysis of these vulnerabilities, demonstrating that an attacker can reconstruct sensitive user inputs directly from the KV-cache. We design and implement three distinct attack vectors: a direct Inversion Attack, a more broadly applicable and potent Collision Attack, and a semantic-based Injection Attack. These methods demonstrate the practicality and severity of KV-cache privacy leakage issues. To mitigate this, we propose KV-Cloak, a novel, lightweight, and efficient defense mechanism. KV-Cloak uses a reversible matrix-based obfuscation scheme, combined with operator fusion, to secure the KV-cache. Our extensive experiments show that KV-Cloak effectively thwarts all proposed attacks, reducing reconstruction quality to random noise. Crucially, it achieves this robust security with virtually no degradation in model accuracy and minimal performance overhead, offering a practical solution for trustworthy LLM deployment.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Astra: A Multi-Agent System for GPU Kernel Performance Optimization</title>
<link>https://arxiv.org/abs/2509.07506</link>
<guid>https://arxiv.org/abs/2509.07506</guid>
<content:encoded><![CDATA[
arXiv:2509.07506v2 Announce Type: replace-cross 
Abstract: GPU kernel optimization has long been a central challenge at the intersection of high-performance computing and machine learning. Efficient kernels are crucial for accelerating large language model (LLM) training and serving, yet attaining high performance typically requires extensive manual tuning. Compiler-based systems reduce some of this burden, but still demand substantial manual design and engineering effort. Recently, researchers have explored using LLMs for GPU kernel generation, though prior work has largely focused on translating high-level PyTorch modules into CUDA code. In this work, we introduce Astra, the first LLM-based multi-agent system for GPU kernel optimization. Unlike previous approaches, Astra starts from existing CUDA implementations extracted from SGLang, a widely deployed framework for serving LLMs, rather than treating PyTorch modules as the specification. Within Astra, specialized LLM agents collaborate through iterative code generation, testing, profiling, and planning to produce kernels that are both correct and high-performance. On kernels from SGLang, Astra achieves an average speedup of 1.32x using zero-shot prompting with OpenAI o4-mini. A detailed case study further demonstrates that LLMs can autonomously apply loop transformations, optimize memory access patterns, exploit CUDA intrinsics, and leverage fast math operations to yield substantial performance gains. Our work highlights multi-agent LLM systems as a promising new paradigm for GPU kernel optimization. Our code is publicly available at https://github.com/Anjiang-Wei/Astra.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exact Coset Sampling for Quantum Lattice Algorithms</title>
<link>https://arxiv.org/abs/2509.12341</link>
<guid>https://arxiv.org/abs/2509.12341</guid>
<content:encoded><![CDATA[
arXiv:2509.12341v5 Announce Type: replace-cross 
Abstract: In this work, we give a new completion of Chen's windowed-QFT lattice algorithm~\citep{chen2024quantum}. This extra step, called Step~$9^\dagger$, replaces the domain extension stage in Steps~8--9. The published Step~9 calls an amplitude periodicity lemma, yet its hypotheses break in the presence of affine offsets $\boldsymbol{v}^*$. Our analysis finds a basic conflict between two design constraints. The lattice problem asks for high spectral resolution, so the method prefers wide time windows. The quadratic phase error of the state prefers narrow time windows. Assumption~A5 packages the spectral concentration and near-uniformity properties that we require from the front end. Under~A5, a direct $\mathbb{Z}_M^n$ Fourier transform of the chirp-corrected coordinate state produces samples $\boldsymbol{u}$ that satisfy $\langle \boldsymbol{b}, \boldsymbol{u} \rangle \equiv 0 \pmod{Q}$ with probability $1-\mathrm{negl}(n)$ and are nearly uniform on the dual hyperplane $\{\boldsymbol{u} : \langle \boldsymbol{b}, \boldsymbol{u} \rangle \equiv 0 \pmod{Q}\}$. The new procedure does not require internal access to control wires. It uses the normalization $b_1=-1$ to apply a center-referenced phase correction directly on the first coordinate register. The scaling parameter $D$ ensures that this physical operation can be implemented by arithmetic on $X_1$ alone and does not read the hidden loop index. For Chen's complex-Gaussian Karst-wave window, we isolate a parameter regime, formalized in Assumption~A5, in which a polynomial retuning of the parameters gives a one-dimensional envelope for the loop index with width $\sigma_J \asymp Q\log n$.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety</title>
<link>https://arxiv.org/abs/2510.18214</link>
<guid>https://arxiv.org/abs/2510.18214</guid>
<content:encoded><![CDATA[
arXiv:2510.18214v2 Announce Type: replace-cross 
Abstract: Safety evaluation of multimodal foundation models often treats vision and language inputs separately, missing risks from joint interpretation where benign content becomes harmful in combination. Existing approaches also fail to distinguish clearly unsafe content from borderline cases, leading to problematic over-blocking or under-refusal of genuinely harmful content. We present Vision Language Safety Understanding (VLSU), a comprehensive framework to systematically evaluate multimodal safety through fine-grained severity classification and combinatorial analysis across 17 distinct safety patterns. Using a multi-stage pipeline with real-world images and human annotation, we construct a large-scale benchmark of 8,187 samples spanning 15 harm categories. Our evaluation of eleven state-of-the-art models reveals systematic joint understanding failures: while models achieve 90%-plus accuracy on clear unimodal safety signals, performance degrades substantially to 20-55% when joint image-text reasoning is required to determine the safety label. Most critically, 34% of errors in joint image-text safety classification occur despite correct classification of the individual modalities, further demonstrating absent compositional reasoning capabilities. Additionally, we find that models struggle to balance refusing unsafe content while still responding to borderline cases that deserve engagement. For example, we find that instruction framing can reduce the over-blocking rate on borderline content from 62.4% to 10.4% in Gemini-1.5, but only at the cost of under-refusing on unsafe content with refusal rate dropping from 90.8% to 53.9%. Overall, our framework exposes weaknesses in joint image-text understanding and alignment gaps in current models, and provides a critical test bed to enable the next milestones in research on robust vision-language safety.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retaining by Doing: The Role of On-Policy Data in Mitigating Forgetting</title>
<link>https://arxiv.org/abs/2510.18874</link>
<guid>https://arxiv.org/abs/2510.18874</guid>
<content:encoded><![CDATA[
arXiv:2510.18874v2 Announce Type: replace-cross 
Abstract: Adapting language models (LMs) to new tasks via post-training carries the risk of degrading existing capabilities -- a phenomenon classically known as catastrophic forgetting. In this paper, toward identifying guidelines for mitigating this phenomenon, we systematically compare the forgetting patterns of two widely adopted post-training methods: supervised fine-tuning (SFT) and reinforcement learning (RL). Our experiments reveal a consistent trend across LM families (Llama, Qwen) and tasks (instruction following, general knowledge, and arithmetic reasoning): RL leads to less forgetting than SFT while achieving comparable or higher target task performance. To investigate the cause for this difference, we consider a simplified setting in which the LM is modeled as a mixture of two distributions, one corresponding to prior knowledge and the other to the target task. We identify that the mode-seeking nature of RL, which stems from its use of on-policy data, enables keeping prior knowledge intact when learning the target task. We then verify this insight by demonstrating that the use on-policy data underlies the robustness of RL to forgetting in practical settings, as opposed to other algorithmic choices such as the KL regularization or advantage estimation. Lastly, as a practical implication, our results highlight the potential of mitigating forgetting using approximately on-policy data, which can be substantially more efficient to obtain than fully on-policy data.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory</title>
<link>https://arxiv.org/abs/2511.00926</link>
<guid>https://arxiv.org/abs/2511.00926</guid>
<content:encoded><![CDATA[
arXiv:2511.00926v3 Announce Type: replace-cross 
Abstract: As Large Language Models (LLMs) grow in capability, do they develop self-awareness as an emergent behavior? And if so, can we measure it? We introduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for measuring self-awareness through strategic differentiation. Using the "Guess 2/3 of Average" game, we test 28 models (OpenAI, Anthropic, Google) across 4,200 trials with three opponent framings: (A) against humans, (B) against other AI models, and (C) against AI models like you. We operationalize self-awareness as the capacity to differentiate strategic reasoning based on opponent type. Finding 1: Self-awareness emerges with model advancement. The majority of advanced models (21/28, 75%) demonstrate clear self-awareness, while older/smaller models show no differentiation. Finding 2: Self-aware models rank themselves as most rational. Among the 21 models with self-awareness, a consistent rationality hierarchy emerges: Self > Other AIs > Humans, with large AI attribution effects and moderate self-preferencing. These findings reveal that self-awareness is an emergent capability of advanced LLMs, and that self-aware models systematically perceive themselves as more rational than humans. This has implications for AI alignment, human-AI collaboration, and understanding AI beliefs about human capabilities.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-Level and Beyond: Benchmarking Large Language Models Against Clinical Pharmacists in Prescription Review</title>
<link>https://arxiv.org/abs/2512.02024</link>
<guid>https://arxiv.org/abs/2512.02024</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, prescription review, RxBench, clinical decision support, fine-tuning<br /><br />Summary:<br /><br />1. The article introduces RxBench, a comprehensive benchmark designed for evaluating large language models (LLMs) in prescription review tasks within clinical decision support. <br />2. RxBench addresses 14 common types of prescription errors, curated from authoritative pharmacy sources, and includes a dataset of 1,150 single-choice, 230 multiple-choice, and 879 short-answer items validated by clinical pharmacists. <br />3. The study benchmarks 18 state-of-the-art LLMs, revealing performance stratification, with Gemini-2.5-pro-preview-05-06, Grok-4-0709, and DeepSeek-R1-0528 outperforming others in accuracy and robustness. <br />4. Comparisons against licensed pharmacists demonstrate that top-performing LLMs can match or exceed human-level performance on certain prescription review tasks. <br />5. By fine-tuning a mid-tier model on RxBench insights, the authors developed a specialized model that rivals leading general-purpose LLMs in short-answer question tasks, highlighting RxBench’s value as a standardized, error-type-oriented framework to advance reliable clinical AI tools. <div>
arXiv:2512.02024v1 Announce Type: new 
Abstract: The rapid advancement of large language models (LLMs) has accelerated their integration into clinical decision support, particularly in prescription review. To enable systematic and fine-grained evaluation, we developed RxBench, a comprehensive benchmark that covers common prescription review categories and consolidates 14 frequent types of prescription errors drawn from authoritative pharmacy references. RxBench consists of 1,150 single-choice, 230 multiple-choice, and 879 short-answer items, all reviewed by experienced clinical pharmacists. We benchmarked 18 state-of-the-art LLMs and identified clear stratification of performance across tasks. Notably, Gemini-2.5-pro-preview-05-06, Grok-4-0709, and DeepSeek-R1-0528 consistently formed the first tier, outperforming other models in both accuracy and robustness. Comparisons with licensed pharmacists indicated that leading LLMs can match or exceed human performance in certain tasks. Furthermore, building on insights from our benchmark evaluation, we performed targeted fine-tuning on a mid-tier model, resulting in a specialized model that rivals leading general-purpose LLMs in performance on short-answer question tasks. The main contribution of RxBench lies in establishing a standardized, error-type-oriented framework that not only reveals the capabilities and limitations of frontier LLMs in prescription review but also provides a foundational resource for building more reliable and specialized clinical tools.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Research: A Systematic Survey</title>
<link>https://arxiv.org/abs/2512.02038</link>
<guid>https://arxiv.org/abs/2512.02038</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Deep Research, query planning, information acquisition, reinforcement learning<br /><br />Summary:<br /><br />This article surveys the emerging field of Deep Research (DR), where large language models (LLMs) are integrated with external tools like search engines to enable advanced problem-solving beyond standard prompting techniques. The authors formalize a three-stage roadmap that clearly differentiates deep research from related AI paradigms. They identify four foundational components essential for DR systems: query planning, information acquisition, memory management, and answer generation, each further detailed through sub-taxonomies to clarify their roles and implementations. The survey discusses various optimization methods to enhance DR capabilities, including prompting strategies, supervised fine-tuning, and agentic reinforcement learning, highlighting their significance in improving system performance. Additionally, the paper consolidates evaluation criteria and addresses key open challenges facing DR, offering a framework to guide future research and practical developments. Overall, the paper aims to provide a comprehensive, systematic overview of deep research systems, with continuous updates planned to track advancements as the field evolves rapidly, positioning DR as a promising direction for creating LLM-driven research agents capable of tackling complex and open-ended tasks with reliable, multi-source, and verifiable outputs. <div>
arXiv:2512.02038v1 Announce Type: new 
Abstract: Large language models (LLMs) have rapidly evolved from text generators into powerful problem solvers. Yet, many open tasks demand critical thinking, multi-source, and verifiable outputs, which are beyond single-shot prompting or standard retrieval-augmented generation. Recently, numerous studies have explored Deep Research (DR), which aims to combine the reasoning capabilities of LLMs with external tools, such as search engines, thereby empowering LLMs to act as research agents capable of completing complex, open-ended tasks. This survey presents a comprehensive and systematic overview of deep research systems, including a clear roadmap, foundational components, practical implementation techniques, important challenges, and future directions. Specifically, our main contributions are as follows: (i) we formalize a three-stage roadmap and distinguish deep research from related paradigms; (ii) we introduce four key components: query planning, information acquisition, memory management, and answer generation, each paired with fine-grained sub-taxonomies; (iii) we summarize optimization techniques, including prompting, supervised fine-tuning, and agentic reinforcement learning; and (iv) we consolidate evaluation criteria and open challenges, aiming to guide and facilitate future development. As the field of deep research continues to evolve rapidly, we are committed to continuously updating this survey to reflect the latest progress in this area.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mirror, Mirror on the Wall -- Which is the Best Model of Them All?</title>
<link>https://arxiv.org/abs/2512.02043</link>
<guid>https://arxiv.org/abs/2512.02043</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, model selection, leaderboards, benchmarks, medical domain<br /><br />Summary:<br /><br />1. Large Language Models (LLMs) have revolutionized multiple industries such as finance, healthcare, education, telecommunications, and law by enhancing productivity and delivering strong results.<br /><br />2. State-of-the-art foundation models are mainly developed by large corporations with access to extensive datasets and significant computational and financial resources required for pretraining.<br /><br />3. These foundation models are further adapted for specific tasks or domains; however, selecting the most appropriate model for a particular use case has become increasingly challenging due to the fast pace of new model releases.<br /><br />4. The paper identifies two key dimensions in model selection: a qualitative dimension, which involves suitability based on model information like model cards, and a quantitative dimension, focusing on the performance of models as measured by benchmarks and leaderboards.<br /><br />5. Focusing on the medical domain as a case study, the authors analyze existing leaderboards and benchmarks to illustrate the importance and evolution of quantitative evaluation.<br /><br />6. The work proposes a Model Selection Methodology (MSM), offering a systematic approach for navigating, prioritizing, and selecting the best-fitting model for a given use case, particularly through leveraging the quantitative performance dimension.<br /><br />7. This methodology aims to streamline and improve the decision-making process surrounding LLM deployment in various domains. <div>
arXiv:2512.02043v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have become one of the most transformative tools across many applications, as they have significantly boosted productivity and achieved impressive results in various domains such as finance, healthcare, education, telecommunications, and law, among others. Typically, state-of-the-art (SOTA) foundation models are developed by large corporations based on large data collections and substantial computational and financial resources required to pretrain such models from scratch. These foundation models then serve as the basis for further development and domain adaptation for specific use cases or tasks. However, given the dynamic and fast-paced nature of launching new foundation models, the process of selecting the most suitable model for a particular use case, application, or domain becomes increasingly complex. We argue that there are two main dimensions that need to be taken into consideration when selecting a model for further training: a qualitative dimension (which model is best suited for a task based on information, for instance, taken from model cards) and a quantitative dimension (which is the best performing model). The quantitative performance of models is assessed through leaderboards, which rank models based on standardized benchmarks and provide a consistent framework for comparing different LLMs. In this work, we address the analysis of the quantitative dimension by exploring the current leaderboards and benchmarks. To illustrate this analysis, we focus on the medical domain as a case study, demonstrating the evolution, current landscape, and practical significance of this quantitative evaluation dimension. Finally, we propose a Model Selection Methodology (MSM), a systematic approach designed to guide the navigation, prioritization, and selection of the model that best aligns with a given use case.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Confidence: Adaptive and Coherent Decoding for Diffusion Language Models</title>
<link>https://arxiv.org/abs/2512.02044</link>
<guid>https://arxiv.org/abs/2512.02044</guid>
<content:encoded><![CDATA[
<div> Diffusion Language Models, inference, trajectory rectification, mutual information, adaptive sampling<br /><br />Summary:<br /><br />This paper addresses limitations in existing inference methods for Diffusion Language Models (DLMs), which typically depend on local step-based metrics such as confidence or entropy, leading to inconsistent and suboptimal generation sequences. To improve this, the authors propose Coherent Contextual Decoding (CCD), an innovative inference framework. CCD introduces a trajectory rectification mechanism that incorporates historical context to enhance the coherence of generated sequences by enabling early rejection of less promising paths. The authors theoretically frame this mechanism as modeling consistency via the conditional mutual information between the past context and token predictions. Furthermore, they tackle inefficiencies in uniform decoding budgets by presenting an adaptive sampling method that dynamically adjusts the unmasking budget at each step based on the consistency metric, rather than following a fixed diffusion-step allocation. This dual approach leads to both faster and higher-quality generation. Empirical evaluation on benchmarks such as Dream and LLaDA shows that CCD achieves up to a 3.48x speedup combined with a 3.91% improvement in generation performance, making it a significant advancement in improving both the speed and quality of DLM inference processes. <div>
arXiv:2512.02044v1 Announce Type: new 
Abstract: Diffusion Language Models (DLMs) have recently achieved significant success due to their any-order generation capabilities. However, existing inference methods typically rely on local, immediate-step metrics such as confidence or entropy which inherently lack a more reliable perspective. This limitation frequently leads to inconsistent sampling trajectories and suboptimal generation quality. To address this, we propose Coherent Contextual Decoding (CCD), a novel inference framework built upon two core innovations. First, CCD employs a trajectory rectification mechanism that leverages historical context to enhance sequence coherence, enabling the early rejection of suboptimal paths. We demonstrate that this mechanism is theoretically equivalent to modeling the consistency of historical steps via the conditional mutual information between context and token predictions. Building on this theoretical insight, we further address the inefficiency of conventional uniform decoding budgets. Instead of rigid allocations based on diffusion steps, we introduce an adaptive sampling strategy that dynamically adjusts the unmasking budget for each step according to our consistency metric. Consequently, our method significantly improves the quality of generation trajectories while accelerating the sampling process. Empirically, our method achieves a simultaneous enhancement in both inference speed and performance across diverse benchmarks on Dream and LLaDA, delivering up to 3.48x speedup alongside 3.91% performance improvement.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reversing Large Language Models for Efficient Training and Fine-Tuning</title>
<link>https://arxiv.org/abs/2512.02056</link>
<guid>https://arxiv.org/abs/2512.02056</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, reversible architectures, memory efficiency, fine-tuning, backpropagation<br /><br />Summary:  
This work addresses the high memory and computational costs associated with training and fine-tuning Large Language Models (LLMs). The authors introduce memory-efficient, reversible architectures for LLMs, inspired by symmetric and symplectic differential equations, which use time-reversible dynamics during backpropagation to retrieve hidden states instead of storing all intermediate activations. This novel approach significantly reduces memory consumption, enabling the use of larger batch sizes and improving throughput under the same memory constraints. Additionally, the paper proposes an efficient method to convert existing non-reversible pre-trained LLMs into these reversible architectures through fine-tuning, making the approach practical and compatible with foundation models already in use. Empirical results demonstrate that the reversible models achieve comparable or better performance across multiple datasets and benchmarks. Overall, this methodology presents a scalable and effective pathway to reduce resource demands for both LLM training from scratch and fine-tuning, potentially facilitating more accessible and efficient LLM development. <div>
arXiv:2512.02056v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are known for their expensive and time-consuming training. Thus, oftentimes, LLMs are fine-tuned to address a specific task, given the pretrained weights of a pre-trained LLM considered a foundation model. In this work, we introduce memory-efficient, reversible architectures for LLMs, inspired by symmetric and symplectic differential equations, and investigate their theoretical properties. Different from standard, baseline architectures that store all intermediate activations, the proposed models use time-reversible dynamics to retrieve hidden states during backpropagation, relieving the need to store activations. This property allows for a drastic reduction in memory consumption, allowing for the processing of larger batch sizes for the same available memory, thereby offering improved throughput. In addition, we propose an efficient method for converting existing, non-reversible LLMs into reversible architectures through fine-tuning, rendering our approach practical for exploiting existing pre-trained models. Our results show comparable or improved performance on several datasets and benchmarks, on several LLMs, building a scalable and efficient path towards reducing the memory and computational costs associated with both training from scratch and fine-tuning of LLMs.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dialect Identification Using Resource-Efficient Fine-Tuning Approaches</title>
<link>https://arxiv.org/abs/2512.02074</link>
<guid>https://arxiv.org/abs/2512.02074</guid>
<content:encoded><![CDATA[
<div> Dialect Identification, Parameter-Efficient Fine-Tuning, Memory-Efficient Fine-Tuning, Whisper model, Mandarin subdialects<br /><br />Summary:<br /><br />Dialect Identification (DI) involves recognizing different dialects within the same language using speech signals and can enhance performance in downstream speech tasks even when speakers have strong dialectal variations. However, fine-tuning speech models for DI is computationally expensive and demands high memory resources. Recent approaches have employed Parameter-Efficient Fine-Tuning (PEFT) methods to reduce the number of trainable parameters, yielding improved parameter efficiency but only limited gains in memory efficiency and training speed. To overcome these limitations, this study investigates Memory-Efficient Fine-Tuning (MEFT) methods, initially developed for language tasks, and applies them to general-purpose pre-trained speech models. The research comprehensively evaluates various MEFT methods in terms of GPU memory usage and fine-tuning speed. As a practical demonstration, the authors fine-tune the Whisper model to classify six Mandarin subdialects using the KeSpeech dataset. The MEFT methods achieve substantial resource savings, reducing GPU memory usage by up to 73.25% and accelerating training speed by a factor of 2.1. Importantly, these improvements come without sacrificing classification accuracy, as the results are comparable to both vanilla fine-tuning and PEFT approaches, demonstrating MEFT’s potential for efficient and effective speech model adaptation. <div>
arXiv:2512.02074v1 Announce Type: new 
Abstract: Dialect Identification (DI) is a task to recognize different dialects within the same language from a speech signal. DI can help to improve the downstream speech related tasks even when speakers have a strong dialect. However, fine-tuning a speech model for tasks like DI is expensive in terms of computation cost and memory requirement. Recent studies have explored fine-tuning pre-trained speech models for tasks like DI using Parameter-Efficient Fine-Tuning (PEFT) methods, which offer parameter efficiency but limited improvement in memory efficiency and training speed. To address these challenges, we explore Memory-Efficient Fine-Tuning (MEFT) methods, originally proposed for language processing, and apply them to the general-purpose pre-trained speech model. We then comprehensively analyze the GPU memory usage and fine-tuning speed based on various MEFT methods. As a case study, we fine-tune the Whisper model to identify six Mandarin subdialects from the KeSpeech dataset, reducing GPU memory usage by up to 73.25% and accelerating training speed by a factor of 2.1, while maintaining accuracy comparable to vanilla fine-tuning and PEFT methods.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feature Selection Empowered BERT for Detection of Hate Speech with Vocabulary Augmentation</title>
<link>https://arxiv.org/abs/2512.02141</link>
<guid>https://arxiv.org/abs/2512.02141</guid>
<content:encoded><![CDATA[
<div> abusive speech, hate speech classification, BERT fine-tuning, TF IDF sample selection, domain-specific tokenizer  

<br /><br />Summary:  
This paper addresses the challenge of detecting abusive speech on social media, which is complicated by the frequent introduction of new slang and obfuscated terms aimed at evading automated detection. To improve hate speech classification while reducing training requirements, the authors propose a data-efficient fine-tuning strategy for BERT. The method involves using a TF IDF-based sample selection technique that retains only the top 75% most informative examples from the training data, thereby limiting data size without sacrificing model performance. Additionally, the study enhances BERT’s tokenizer by incorporating domain-specific slang and lexical variants commonly used in abusive language, overcoming vocabulary limitations of the base model. Experiments carried out on a standard hate speech dataset indicate that this approach yields competitive classification accuracy. Moreover, the reduction in training data and vocabulary adaptation significantly improves computational efficiency, making the solution well-suited for scalable and adaptable abusive content moderation systems. The findings highlight the utility of sample selection and tokenizer augmentation as practical tools in evolving hate speech detection frameworks. <div>
arXiv:2512.02141v1 Announce Type: new 
Abstract: Abusive speech on social media poses a persistent and evolving challenge, driven by the continuous emergence of novel slang and obfuscated terms designed to circumvent detection systems. In this work, we present a data efficient strategy for fine tuning BERT on hate speech classification by significantly reducing training set size without compromising performance. Our approach employs a TF IDF-based sample selection mechanism to retain only the most informative 75 percent of examples, thereby minimizing training overhead. To address the limitations of BERT's native vocabulary in capturing evolving hate speech terminology, we augment the tokenizer with domain-specific slang and lexical variants commonly found in abusive contexts. Experimental results on a widely used hate speech dataset demonstrate that our method achieves competitive performance while improving computational efficiency, highlighting its potential for scalable and adaptive abusive content moderation.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Think Before You Prune: Self-Reflective Structured Pruning for Reasoning Language Models</title>
<link>https://arxiv.org/abs/2512.02185</link>
<guid>https://arxiv.org/abs/2512.02185</guid>
<content:encoded><![CDATA[
<div> Reasoning LLMs, pruning, self-generated calibration, structured pruning, sparsity

<br /><br />Summary:  
The paper addresses the challenge of pruning Reasoning Large Language Models (RLMs) like OpenAI o1, DeepSeek-R1, and Qwen3, which are typically large and compute-intensive due to multi-step chain-of-thought reasoning. Existing pruning methods dramatically degrade the performance of RLMs even at moderate sparsity levels (e.g., 20%), causing accuracy collapse and loss of reasoning coherence. The authors analyze this failure and identify a crucial mismatch between calibration data, pruning objectives, and the model’s decode-time reasoning process as the root cause. They find that relying on the model’s own self-generated reasoning traces for calibration provides a more reliable signal than human-labeled data, better reflecting the inference distribution. To resolve this, the paper introduces RESP, a self-reflective structured pruning framework that aligns pruning with reasoning dynamics by using self-generated calibration, decode-only gradient-based importance estimation, and progressive regeneration to preserve calibration fidelity during pruning. Evaluations on Qwen3-8B using GSM8K and MathQA benchmarks demonstrate that RESP significantly outperforms existing pruning methods, maintaining near-dense model accuracy at 20-30% sparsity and reducing performance degradation at higher sparsity. At 40% sparsity, RESP achieves 81.3% accuracy on GSM8K and 59.6% on MathQA, surpassing previous best methods by large margins of 66.87% and 47%, respectively. <div>
arXiv:2512.02185v1 Announce Type: new 
Abstract: Reasoning LLMs (RLMs) such as OpenAI o1, DeepSeek-R1, and Qwen3 deliver strong multi-step reasoning through chain-of-thought generation, but their large model sizes and lengthy decode-time outputs make them costly to deploy and unsuitable for resource-constrained settings. To reduce computing and memory cost, pruning offers a promising solution by removing unimportant parameters. However, despite their success on standard LLMs, existing pruning methods severely damage RLMs, as even moderate sparsity (e.g., 20%) can collapse accuracy and completely disrupt the model's reasoning coherence. We begin by analyzing why existing pruning pipelines fail on reasoning LLMs and find that their brittleness largely stems from a mismatch between the calibration data, the pruning objective, and the model's decode-time reasoning behavior. Our study further shows that the most reliable calibration signal comes not from human-written labels but from the model's own self-generated reasoning traces, which more accurately reflect its inference distribution. Guided by these insights, we introduce RESP, a self-reflective structured pruning framework that aligns pruning decisions with the model's reasoning dynamics through self-generated calibration, decode-only gradient-based importance estimation, and progressive regeneration that maintains calibration fidelity as sparsity increases. Experiments on Qwen3-8B demonstrate that RESP markedly outperforms existing structured pruning methods on both GSM8K and MathQA, preserving near-dense accuracy at 20-30% sparsity and substantially mitigating performance collapse at higher sparsity levels. At 40% sparsity, RESP attains 81.3% accuracy on GSM8K and 59.6% on MathQA, surpassing the strongest baselines by 66.87% and 47%, respectively.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Knowledge-Based Language Model: Deducing Grammatical Knowledge in a Multi-Agent Language Acquisition Simulation</title>
<link>https://arxiv.org/abs/2512.02195</link>
<guid>https://arxiv.org/abs/2512.02195</guid>
<content:encoded><![CDATA[
<div> Keywords: MODOMA system, language acquisition, multi-agent environment, grammatical categories, language models<br /><br />Summary:<br /><br />This paper introduces the MODOMA system, a computational multi-agent laboratory environment designed for unsupervised language acquisition experiments. The system simulates the interaction between two language models, an adult and a child agent, to facilitate language learning. It combines statistical and rule-based procedures, resulting in a knowledge-based language model that can generate and parse new utterances in the target language. MODOMA is fully parametrized, allowing researchers to control all aspects of the experimental setup and explicitly consult the acquired grammatical knowledge. The paper presents experiments demonstrating that both functional and content grammatical categories can be acquired by the child agent from training and test data generated by the adult agent, with varying amounts of exemplars. Importantly, the patterns observed in the machine-generated data align with well-established patterns found in human-generated language data. These findings validate the MODOMA approach as a viable model for studying language acquisition, showing successful acquisition of discrete grammatical categories by the child agent. Overall, this system offers novel opportunities for conducting computational experiments on language acquisition, bridging statistical learning with explicit grammar representation. <div>
arXiv:2512.02195v1 Announce Type: new 
Abstract: This paper presents an initial study performed by the MODOMA system. The MODOMA is a computational multi-agent laboratory environment for unsupervised language acquisition experiments such that acquisition is based on the interaction between two language models, an adult and a child agent. Although this framework employs statistical as well as rule-based procedures, the result of language acquisition is a knowledge-based language model, which can be used to generate and parse new utterances of the target language. This system is fully parametrized and researchers can control all aspects of the experiments while the results of language acquisition, that is, the acquired grammatical knowledge, are explicitly represented and can be consulted. Thus, this system introduces novel possibilities for conducting computational language acquisition experiments. The experiments presented by this paper demonstrate that functional and content categories can be acquired and represented by the daughter agent based on training and test data containing different amounts of exemplars generated by the adult agent. Interestingly, similar patterns, which are well-established for human-generated data, are also found for these machine-generated data. As the procedures resulted in the successful acquisition of discrete grammatical categories by the child agent, these experiments substantiate the validity of the MODOMA approach to modelling language acquisition.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Swivuriso: The South African Next Voices Multilingual Speech Dataset</title>
<link>https://arxiv.org/abs/2512.02201</link>
<guid>https://arxiv.org/abs/2512.02201</guid>
<content:encoded><![CDATA[
<div> Swivuriso, multilingual speech dataset, South African languages, automatic speech recognition, dataset collection<br /><br />Summary:<br /><br />This paper introduces Swivuriso, a comprehensive 3000-hour multilingual speech dataset created under the African Next Voices project. The dataset focuses on supporting the development and benchmarking of automatic speech recognition (ASR) technologies specifically for seven South African languages, addressing a critical need in the region's ASR resources. Swivuriso covers diverse domains such as agriculture, healthcare, and general topics to ensure broad applicability and real-world relevance. The authors detail the dataset's design principles, putting strong emphasis on ethical considerations and rigorous data collection procedures to maintain quality and cultural sensitivity. Baseline experiments training and finetuning ASR models using Swivuriso are presented, demonstrating the dataset’s utility and effectiveness. These results are also compared to other existing ASR datasets for the included languages, highlighting improvements and areas for further research. Overall, Swivuriso fills significant gaps in multilingual ASR data for South African languages and acts as a valuable resource for advancing speech technology in underrepresented linguistic communities. <div>
arXiv:2512.02201v1 Announce Type: new 
Abstract: This paper introduces Swivuriso, a 3000-hour multilingual speech dataset developed as part of the African Next Voices project, to support the development and benchmarking of automatic speech recognition (ASR) technologies in seven South African languages. Covering agriculture, healthcare, and general domain topics, Swivuriso addresses significant gaps in existing ASR datasets. We describe the design principles, ethical considerations, and data collection procedures that guided the dataset creation. We present baseline results of training/finetuning ASR models with this data and compare to other ASR datasets for the langauges concerned.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lightweight Latent Reasoning for Narrative Tasks</title>
<link>https://arxiv.org/abs/2512.02240</link>
<guid>https://arxiv.org/abs/2512.02240</guid>
<content:encoded><![CDATA[
<div> Large language models, Reinforcement learning, Latent reasoning, Token sampling, Computational efficiency  

<br /><br />Summary:  
The paper introduces LiteReason, a novel method designed to enhance latent reasoning in large language models (LLMs) while reducing computational overhead. 1) LLMs generate long reasoning chains or "reasoning traces" to tackle complex tasks, which can be optimized with reinforcement learning (RL) for better answer prediction but at high computational costs. 2) LiteReason integrates a lightweight Reasoning Projector module that produces continuous latent tokens, enabling the model to 'skip' certain reasoning steps and thus reduce the token processing burden. 3) The policy model trained with RL dynamically switches between conventional discrete reasoning and latent reasoning via the projector, balancing computational efficiency and reasoning accuracy. 4) Experimental evaluation on tasks like plot hole detection and book chapter generation demonstrates that LiteReason outperforms previous latent reasoning baselines and approaches the performance of non-latent RL methods. 5) Notably, LiteReason reduces the final reasoning length by 77-92%, achieving a more efficient tradeoff between model performance and compute cost, suggesting a promising direction for scaling LLM reasoning capabilities with manageable resource use. <div>
arXiv:2512.02240v1 Announce Type: new 
Abstract: Large language models (LLMs) tackle complex tasks by generating long chains of thought or "reasoning traces" that act as latent variables in the generation of an output given a query. A model's ability to generate such traces can be optimized with reinforcement learning (RL) to improve their utility in predicting an answer. This optimization comes at a high computational cost, especially for narrative-related tasks that involve retrieving and processing many tokens. To this end, we propose LiteReason, a latent reasoning method that can be interleaved with standard token sampling and easily combined with RL techniques. LiteReason employs a lightweight Reasoning Projector module, trained to produce continuous latent tokens that help the model 'skip' reasoning steps. During RL, the policy model decides when to activate the projector, switching between latent and discrete reasoning as needed. Experimental results on plot hole detection and book chapter generation show that our method outperforms latent reasoning baselines and comes close to matching non-latent RL training, while reducing final reasoning length by 77-92%. Overall, LiteReason guides RL training to a more efficient part of the performance-computation tradeoff curve.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DETAIL Matters: Measuring the Impact of Prompt Specificity on Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2512.02246</link>
<guid>https://arxiv.org/abs/2512.02246</guid>
<content:encoded><![CDATA[
<div> Keywords: prompt design, prompt specificity, large language models, reasoning performance, adaptive prompting<br /><br />Summary:<br /><br />This paper addresses the underexplored impact of prompt specificity—how detailed or vague a prompt is—on the reasoning capabilities of large language models (LLMs). The authors introduce DETAIL, a novel evaluation framework that measures LLM performance across different specificity levels. Using GPT-4, they generate multi-level prompts and quantify their specificity through perplexity scores. Correctness is assessed via GPT-based semantic equivalence metrics to ensure nuanced understanding beyond simple keyword matching. The study spans 30 newly proposed reasoning tasks and compares two models, GPT-4 and a smaller model, O3-mini. Results demonstrate that higher prompt specificity significantly boosts accuracy, especially in smaller models and tasks that are procedural in nature. This suggests that smaller or less capable models benefit more from carefully detailed prompts. The findings underline the importance of adaptive prompting strategies tailored to model and task characteristics. Finally, the work contributes useful tools and a dataset to facilitate further research on the relationship between prompt design and LLM reasoning performance, encouraging development of more effective human-AI interaction techniques. <div>
arXiv:2512.02246v1 Announce Type: new 
Abstract: Prompt design plays a critical role in the reasoning performance of large language models (LLMs), yet the impact of prompt specificity - how detailed or vague a prompt is - remains understudied. This paper introduces DETAIL, a framework for evaluating LLM performance across varying levels of prompt specificity. We generate multi-level prompts using GPT-4, quantify specificity via perplexity, and assess correctness using GPT-based semantic equivalence. Experiments on 30 novel reasoning tasks across GPT-4 and O3-mini reveal that specificity improves accuracy, especially for smaller models and procedural tasks. Our results highlight the need for adaptive prompting strategies and provide tools and data to support further research.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAIRNS: Balancing Readability and Scientific Accuracy in Climate Adaptation Question Answering</title>
<link>https://arxiv.org/abs/2512.02251</link>
<guid>https://arxiv.org/abs/2512.02251</guid>
<content:encoded><![CDATA[
<div> Keywords: Climate adaptation, question-answering, readability, ScholarGuide prompt, expert evaluation<br /><br />Summary:<br /><br />1. The article addresses climate adaptation strategies in agriculture that are designed to sustain food production in response to climate change. These strategies are often found in both unstructured (scientific literature) and structured (government climate data) sources.<br />2. The authors introduce CAIRNS (Climate Adaptation question-answering with Improved Readability and Noted Sources), a framework aimed at assisting farmer advisors by providing credible and preliminary answers extracted from complex web-based evidence.<br />3. CAIRNS improves the readability of answers and citation reliability using a structured prompt called ScholarGuide.<br />4. The framework includes a consistency-weighted hybrid evaluator that combines inter-model agreement with expert input to ensure robust evaluation.<br />5. CAIRNS achieves readable, verifiable, and domain-specific question-answering capabilities without the need for fine-tuning or reinforcement learning.<br />6. Using an expert-curated dataset, CAIRNS outperforms existing baseline methods across most evaluation metrics.<br />7. An ablation study confirms the strengths of CAIRNS across all tested metrics.<br />8. Finally, the authors validate their LLM-based evaluation approach by analyzing its correlation with human judgments, demonstrating its reliability. <div>
arXiv:2512.02251v1 Announce Type: new 
Abstract: Climate adaptation strategies are proposed in response to climate change. They are practised in agriculture to sustain food production. These strategies can be found in unstructured data (for example, scientific literature from the Elsevier website) or structured (heterogeneous climate data via government APIs). We present Climate Adaptation question-answering with Improved Readability and Noted Sources (CAIRNS), a framework that enables experts -- farmer advisors -- to obtain credible preliminary answers from complex evidence sources from the web. It enhances readability and citation reliability through a structured ScholarGuide prompt and achieves robust evaluation via a consistency-weighted hybrid evaluator that leverages inter-model agreement with experts. Together, these components enable readable, verifiable, and domain-grounded question-answering without fine-tuning or reinforcement learning. Using a previously reported dataset of expert-curated question-answers, we show that CAIRNS outperforms the baselines on most of the metrics. Our thorough ablation study confirms the results on all metrics. To validate our LLM-based evaluation, we also report an analysis of correlations against human judgment.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HealthContradict: Evaluating Biomedical Knowledge Conflicts in Language Models</title>
<link>https://arxiv.org/abs/2512.02299</link>
<guid>https://arxiv.org/abs/2512.02299</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, contextual reasoning, biomedical QA, conflicting contexts, HealthContradict<br /><br />Summary:<br /><br />This paper investigates how language models utilize contextual information to answer health-related questions, particularly when faced with conflicting biomedical contexts. The authors introduce HealthContradict, a novel expert-verified dataset containing 920 unique instances. Each instance pairs a health question with a factual answer supported by scientific evidence, alongside two documents presenting contradictory viewpoints. The study evaluates the impact of different prompt conditions—correct, incorrect, or contradictory contexts—on language model outputs. HealthContradict provides a more nuanced evaluation of a model's ability to reason over conflicting biomedical information compared to previous medical question-answering benchmarks. Experimental results demonstrate that fine-tuned biomedical language models not only encode parametric knowledge from pretraining but also effectively leverage correct contextual information while resisting misleading or incorrect contexts. This dual capability underlines the importance of contextual reasoning in enhancing the reliability and accuracy of language models in biomedical applications. Overall, the work emphasizes the critical role of contextual understanding for trustworthy health question answering using language models. <div>
arXiv:2512.02299v1 Announce Type: new 
Abstract: How do language models use contextual information to answer health questions? How are their responses impacted by conflicting contexts? We assess the ability of language models to reason over long, conflicting biomedical contexts using HealthContradict, an expert-verified dataset comprising 920 unique instances, each consisting of a health-related question, a factual answer supported by scientific evidence, and two documents presenting contradictory stances. We consider several prompt settings, including correct, incorrect or contradictory context, and measure their impact on model outputs. Compared to existing medical question-answering evaluation benchmarks, HealthContradict provides greater distinctions of language models' contextual reasoning capabilities. Our experiments show that the strength of fine-tuned biomedical language models lies not only in their parametric knowledge from pretraining, but also in their ability to exploit correct context while resisting incorrect context.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers</title>
<link>https://arxiv.org/abs/2512.02304</link>
<guid>https://arxiv.org/abs/2512.02304</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, solver-verifier interaction, verifier gain, cross-family verification, post-training effects

<br /><br />Summary:  
This paper investigates the interactions between large language models (LLMs) acting as problem solvers and verifiers, focusing on how verifiers select high-quality answers from candidate solutions. Unlike prior works that primarily examine self-verification within a single model, this study systematically explores verification both within the same model family and across different families. It evaluates 37 diverse LLMs spanning various sizes, families, and training stages (base vs. post-trained) across 9 benchmarks that include logical reasoning, structured puzzles, symbolic computation, mathematics, commonsense, factual recall, and domain knowledge. The researchers introduce "verifier gain," a novel metric that estimates performance improvements achievable by verifier-based rejection sampling during test time, and validate it empirically. They analyze important scaling behaviors, including how verifier gain and false positive rates evolve with model size and post-training. Their results highlight that cross-family verification considerably enhances performance compared to self-verification. Furthermore, post-training tends to diminish self-verification gains but amplifies improvements when verifying outputs across different model families. Finally, tasks involving mathematics and logical reasoning emerge as inherently more verifiable than others, suggesting certain domains are better suited for verifier-based performance boosts. <div>
arXiv:2512.02304v1 Announce Type: new 
Abstract: Large language models (LLMs) can act as both problem solvers and solution verifiers, with verifiers improving solver performance by selecting high-quality answers from a pool of candidates. However, prior studies of solver-verifier interactions have been limited, focusing mainly on self-verification and rarely examining how verifiers judge outputs from models in their own or in another model family. Modern LLMs also undergo extensive post-training, but its effect on verification remains unclear. We present a systematic study across 37 models spanning multiple families, sizes, and base vs. post-trained variants, evaluated on 9 benchmarks covering logical reasoning, structured puzzles, symbolic computation, mathematics, commonsense, factual recall, and domain knowledge. We compare self-verification with verification within the same family and across different families. To support this, we introduce and empirically validate verifier gain, a metric that predicts the performance improvements from test-time verifier-based rejection sampling. We analyze how metrics like verifier gain and false positive rate scale with model size and post-training, and characterize differences in dataset verifiability. Our findings show that cross-family verification is especially effective; post-training reduces self-improvement but strengthens cross-family improvement; and mathematical and logical tasks exhibit the highest inherent verifiability.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memory-Augmented Knowledge Fusion with Safety-Aware Decoding for Domain-Adaptive Question Answering</title>
<link>https://arxiv.org/abs/2512.02363</link>
<guid>https://arxiv.org/abs/2512.02363</guid>
<content:encoded><![CDATA[
<div> Knowledge-Aware Reasoning, Memory-Augmented Adaptation, Domain-Specific QA, Safety-Aware Decoder, Knowledge Fusion<br /><br />Summary:<br /><br />This paper addresses the challenge of building domain-specific question answering (QA) systems that effectively integrate heterogeneous knowledge sources while ensuring accuracy and safety, especially in sensitive domains like healthcare policies and government welfare. The authors propose KARMA, a novel framework that enhances QA performance by combining multiple advanced techniques. KARMA features a dual-encoder architecture that fuses both structured and unstructured knowledge, allowing the system to better comprehend diverse information formats. Additionally, it includes a gated memory unit that dynamically regulates the integration of external knowledge, improving contextual understanding and relevance. To ensure safe and reliable outputs, KARMA employs a safety-aware controllable decoder which uses safety classification and guided generation techniques to reduce unsafe or inappropriate responses. Extensive experiments conducted on a proprietary QA dataset demonstrate that KARMA surpasses strong baseline models in terms of answer quality and safety metrics. The study highlights KARMA as a comprehensive and trustworthy solution designed for adaptive QA systems in service-based domains, offering promising improvements for real-world applications where factual consistency and safety are paramount. <div>
arXiv:2512.02363v1 Announce Type: new 
Abstract: Domain-specific question answering (QA) systems for services face unique challenges in integrating heterogeneous knowledge sources while ensuring both accuracy and safety. Existing large language models often struggle with factual consistency and context alignment in sensitive domains such as healthcare policies and government welfare. In this work, we introduce Knowledge-Aware Reasoning and Memory-Augmented Adaptation (KARMA), a novel framework designed to enhance QA performance in care scenarios. KARMA incorporates a dual-encoder architecture to fuse structured and unstructured knowledge sources, a gated memory unit to dynamically regulate external knowledge integration, and a safety-aware controllable decoder that mitigates unsafe outputs using safety classification and guided generation techniques. Extensive experiments on a proprietary QA dataset demonstrate that KARMA outperforms strong baselines in both answer quality and safety. This study offers a comprehensive solution for building trustworthy and adaptive QA systems in service contexts.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TaleFrame: An Interactive Story Generation System with Fine-Grained Control and Large Language Models</title>
<link>https://arxiv.org/abs/2512.02402</link>
<guid>https://arxiv.org/abs/2512.02402</guid>
<content:encoded><![CDATA[
<div> Keywords: natural language generation, story generation, human-computer interaction, large language models, TaleFrame<br /><br />Summary:<br /><br />The paper introduces TaleFrame, a novel system designed to enhance creative story generation by combining large language models (LLMs) with human-computer interaction (HCI). It addresses limitations in existing systems that struggle with fine-grained control and clear input specifications, which often results in stories that do not align well with user intent. TaleFrame decomposes story structure into four essential units: entities, events, relationships, and story outline, enabling precise control over the narrative generation process. The system utilizes the Tinystories dataset, transforming it into a preference dataset with 9,851 JSON entries to fine-tune a local Llama model via a JSON2Story approach, thereby converting structured data into coherent stories. An intuitive interface supports user interactions such as dragging, dropping, attaching, and connecting these units, facilitating user-driven story creation and editing. TaleFrame also incorporates an evaluation mechanism spanning seven dimensions such as creativity and structural integrity, offering suggestions for iterative story refinement. The authors conduct quantitative evaluations and user studies that validate TaleFrame's effectiveness and user satisfaction. Additionally, the processed dataset is publicly available on Hugging Face, supporting further research and development in controlled story generation systems. <div>
arXiv:2512.02402v1 Announce Type: new 
Abstract: With the advancement of natural language generation (NLG) technologies, creative story generation systems have gained increasing attention. However, current systems often fail to accurately translate user intent into satisfactory story outputs due to a lack of fine-grained control and unclear input specifications, limiting their applicability. To address this, we propose TaleFrame, a system that combines large language models (LLMs) with human-computer interaction (HCI) to generate stories through structured information, enabling precise control over the generation process. The innovation of TaleFrame lies in decomposing the story structure into four basic units: entities, events, relationships, and story outline. We leverage the Tinystories dataset, parsing and constructing a preference dataset consisting of 9,851 JSON-formatted entries, which is then used to fine-tune a local Llama model. By employing this JSON2Story approach, structured data is transformed into coherent stories. TaleFrame also offers an intuitive interface that supports users in creating and editing entities and events and generates stories through the structured framework. Users can control these units through simple interactions (e.g., drag-and-drop, attach, and connect), thus influencing the details and progression of the story. The generated stories can be evaluated across seven dimensions (e.g., creativity, structural integrity), with the system providing suggestions for refinement based on these evaluations. Users can iteratively adjust the story until a satisfactory result is achieved. Finally, we conduct quantitative evaluation and user studies that demonstrate the usefulness of TaleFrame. Dataset available at https://huggingface.co/datasets/guodaosun/tale-frame.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Concise Review of Hallucinations in LLMs and their Mitigation</title>
<link>https://arxiv.org/abs/2512.02527</link>
<guid>https://arxiv.org/abs/2512.02527</guid>
<content:encoded><![CDATA[
<div> hallucinations, language models, natural language processing, mitigation, origins<br /><br /><br />Summary:  
1. Traditional language models often generate hallucinations, which are outputs that are factually incorrect or nonsensical.  
2. The presence of hallucinations poses a significant challenge and risk to the advancement and trustworthiness of natural language processing applications.  
3. Understanding the different types of hallucinations and their root causes is essential for researchers and developers working with language models.  
4. The document provides a clear and concise overview aiming to serve as a comprehensive resource on the topic of hallucinations in language models.  
5. Practical strategies and methodologies to reduce or mitigate hallucinations are discussed to improve the reliability and accuracy of language model outputs in various real-world tasks. <div>
arXiv:2512.02527v1 Announce Type: new 
Abstract: Traditional language models face a challenge from hallucinations. Their very presence casts a large, dangerous shadow over the promising realm of natural language processing. It becomes crucial to understand the various kinds of hallucinations that occur nowadays, their origins, and ways of reducing them. This document provides a concise and straightforward summary of that. It serves as a one-stop resource for a general understanding of hallucinations and how to mitigate them.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Signals Really Matter for Misinformation Tasks? Evaluating Fake-News Detection and Virality Prediction under Real-World Constraints</title>
<link>https://arxiv.org/abs/2512.02552</link>
<guid>https://arxiv.org/abs/2512.02552</guid>
<content:encoded><![CDATA[
<div> fake-news detection, virality prediction, textual embeddings, numeric features, evaluation design<br /><br />Summary:<br /><br />1. This study evaluates two practical tasks in online misinformation: fake-news detection and virality prediction, emphasizing the need for rapid operational responses.<br /><br />2. The researchers use the EVONS and FakeNewsNet datasets to compare the effectiveness of textual embeddings (RoBERTa and Mistral) against lightweight numeric features such as timing, follower counts, verification status, and likes, employing sequence models including GRU, gating architectures, and Transformer encoders.<br /><br />3. Findings indicate that textual content alone is a strong discriminator for fake-news detection, while numeric-only pipelines remain viable alternatives when language models are unavailable or computational resources are limited.<br /><br />4. Virality prediction is found to be substantially more challenging than fake-news detection, with results highly sensitive to how viral labels are defined; a median-based "viral" split (<50 likes) provides a pragmatic but conservative estimate, and time-censoring engagement features is desirable yet difficult due to API restrictions.<br /><br />5. Dimensionality-reduction analysis shows non-linear methods (t-SNE) outperform linear methods (PCA) for virality prediction, and swapping RoBERTa with Mistral embeddings produces only modest performance differences.<br /><br />6. The authors discuss the implications for evaluation design, highlight reproducibility constraints in the field, and provide code and data splits where possible, along with guidance on metric selection. <div>
arXiv:2512.02552v1 Announce Type: new 
Abstract: We present an evaluation-driven study of two practical tasks regarding online misinformation: (i) fake-news detection and (ii) virality prediction in the context of operational settings, with the necessity for rapid reaction. Using the EVONS and FakeNewsNet datasets, we compare textual embeddings (RoBERTa; with a control using Mistral) against lightweight numeric features (timing, follower counts, verification, likes) and sequence models (GRU, gating architectures, Transformer encoders). We show that textual content alone is a strong discriminator for fake-news detection, while numeric-only pipelines remain viable when language models are unavailable or compute is constrained. Virality prediction is markedly harder than fake-news detection and is highly sensitive to label construction; in our setup, a median-based ''viral'' split (<50 likes) is pragmatic but underestimates real-world virality, and time-censoring for engagement features is desirable yet difficult under current API limits. Dimensionality-reduction analyses suggest non-linear structure is more informative for virality than for fake-news detection (t-SNE > PCA on numeric features). Swapping RoBERTa for Mistral embeddings yields only modest deltas, leaving conclusions unchanged. We discuss implications for evaluation design and report reproducibility constraints that realistically affect the field. We release splits and code where possible and provide guidance for metric selection.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ADORE: Autonomous Domain-Oriented Relevance Engine for E-commerce</title>
<link>https://arxiv.org/abs/2512.02555</link>
<guid>https://arxiv.org/abs/2512.02555</guid>
<content:encoded><![CDATA[
<div> Keywords: relevance modeling, e-commerce search, adversarial data synthesis, knowledge distillation, Chain-of-Thought LLM  

<br /><br />Summary:  
The paper addresses challenges in relevance modeling for e-commerce search, specifically the limitations of traditional term-matching methods like BM25 and the scarcity of domain-specific hard samples for neural models. First, it introduces ADORE, a self-sustaining framework combining three innovative components to improve semantic understanding and model robustness. The first component is a Rule-aware Relevance Discrimination module, which employs a Chain-of-Thought large language model (LLM) to generate intent-aligned training data that is further refined using Kahneman-Tversky Optimization to better align with actual user behavior. Second, the framework includes an Error-type-aware Data Synthesis module that automatically generates adversarial examples targeting different error types, thereby enhancing model robustness to hard cases. Third, ADORE incorporates a Key-attribute-enhanced Knowledge Distillation module, which infuses domain-specific attribute hierarchies into a lighter student model that is easier to deploy while retaining important reasoning capabilities. Together, these elements automate annotation, adversarial example generation, and knowledge distillation, effectively overcoming data scarcity and improving reasoning in relevance modeling. Extensive large-scale experiments and real-world online A/B tests demonstrate the framework’s superiority. Ultimately, ADORE establishes a new paradigm for resource-efficient and cognitively aligned relevance modeling tailored for industrial e-commerce applications. <div>
arXiv:2512.02555v1 Announce Type: new 
Abstract: Relevance modeling in e-commerce search remains challenged by semantic gaps in term-matching methods (e.g., BM25) and neural models' reliance on the scarcity of domain-specific hard samples. We propose ADORE, a self-sustaining framework that synergizes three innovations: (1) A Rule-aware Relevance Discrimination module, where a Chain-of-Thought LLM generates intent-aligned training data, refined via Kahneman-Tversky Optimization (KTO) to align with user behavior; (2) An Error-type-aware Data Synthesis module that auto-generates adversarial examples to harden robustness; and (3) A Key-attribute-enhanced Knowledge Distillation module that injects domain-specific attribute hierarchies into a deployable student model. ADORE automates annotation, adversarial generation, and distillation, overcoming data scarcity while enhancing reasoning. Large-scale experiments and online A/B testing verify the effectiveness of ADORE. The framework establishes a new paradigm for resource-efficient, cognitively aligned relevance modeling in industrial applications.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models</title>
<link>https://arxiv.org/abs/2512.02556</link>
<guid>https://arxiv.org/abs/2512.02556</guid>
<content:encoded><![CDATA[
<div> Keywords: DeepSeek-V3.2, Sparse Attention, Reinforcement Learning, Agentic Task Synthesis, Reasoning Performance  

<br /><br />Summary:  
The paper presents DeepSeek-V3.2, a cutting-edge model designed to balance computational efficiency with advanced reasoning and agent capabilities. First, it introduces DeepSeek Sparse Attention (DSA), a novel attention mechanism that significantly lowers computational costs while maintaining strong performance in processing long-context inputs. Second, the authors detail a scalable reinforcement learning framework that leverages extensive post-training compute, enabling DeepSeek-V3.2 to achieve performance comparable to GPT-5. Their enhanced variant, DeepSeek-V3.2-Speciale, reportedly outperforms GPT-5 and matches the reasoning skill set of Gemini-3.0-Pro, attaining top honors in the 2025 International Mathematical Olympiad and International Olympiad in Informatics. Third, the study describes a large-scale agentic task synthesis pipeline that generates diverse training data systematically, enabling effective integration of reasoning with tool use. This pipeline supports scalable agentic post-training, which improves generalization and robustness in following complex instructions within interactive scenarios. Together, these innovations position DeepSeek-V3.2 as a breakthrough in efficient, high-performance AI reasoning and agentic interaction. <div>
arXiv:2512.02556v1 Announce Type: new 
Abstract: We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks</title>
<link>https://arxiv.org/abs/2512.02580</link>
<guid>https://arxiv.org/abs/2512.02580</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement learning, Curriculum learning, Advantage signals, Mathematical reasoning, GUI reasoning<br /><br />Summary:<br /><br />This paper introduces CAPO (Curriculum Advantage Policy Optimization), a novel adaptive curriculum mechanism designed to improve reinforcement learning training of large language models. Traditional methods use both positive and negative advantage signals indiscriminately from early stages, which can cause ambiguous guidance and limit performance gains. CAPO addresses this by initially focusing only on positive advantage samples in a bootstrapped imitation learning phase, establishing robust foundational skills. After this foundation is set, negative advantage signals are gradually introduced to help the model develop discriminative abilities, enhancing its generalization to more complex reasoning tasks. The method is compatible with various optimization algorithms such as GRPO, PPO, RLOO, and Reinforce++, showcasing versatility and ease of integration. Experimentally, CAPO demonstrates stable and significant performance improvements on challenging mathematical reasoning benchmarks, confirming the efficacy of the curriculum approach. Moreover, the framework generalizes effectively beyond purely linguistic tasks, extending into multimodal scenarios like Graphical User Interface (GUI) reasoning, underscoring CAPO’s robustness and broad applicability in enhancing large language models’ reasoning capabilities across diverse domains. <div>
arXiv:2512.02580v1 Announce Type: new 
Abstract: Reinforcement learning has emerged as a paradigm for post-training large language models, boosting their reasoning capabilities. Such approaches compute an advantage value for each sample, reflecting better or worse performance than expected, thereby yielding both positive and negative signals for training. However, the indiscriminate mixing of the two signals in existing methods, especially from the early stages, may lead to ambiguous guidance and limited gains. To address this issue, we propose **CAPO** (**C**urriculum **A**dvantage **P**olicy **O**ptimization), an adaptive curriculum mechanism based on advantage signals. The proposed mechanism bootstraps imitation learning with positive-only advantage samples to establish robust foundations, and subsequently introduces negative signals to cultivate discriminative capabilities, thereby improving generalization across complex scenarios. Compatible with diverse optimization methods including GRPO, PPO, RLOO, and Reinforce++, our method consistently achieves stable and significant improvements in mathematical reasoning tasks, and further generalizes effectively to multimodal Graphical User Interface (GUI) reasoning scenarios, establishing itself as a versatile and robust optimization framework.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spoken Conversational Agents with Large Language Models</title>
<link>https://arxiv.org/abs/2512.02593</link>
<guid>https://arxiv.org/abs/2512.02593</guid>
<content:encoded><![CDATA[
<div> Keywords: spoken conversational agents, voice-native LLMs, cross-modal alignment, end-to-end systems, robustness across accents<br /><br />Summary: This article presents a comprehensive tutorial on the evolution of spoken conversational agents, highlighting the transition from traditional cascaded ASR/NLU systems to advanced end-to-end, retrieval- and vision-grounded voice-native large language models (LLMs). It outlines the process of adapting text-based LLMs for audio input and emphasizes the importance of cross-modal alignment and joint speech-text training to enhance system performance. The tutorial reviews critical datasets and evaluation metrics, addressing robustness challenges such as handling diverse accents. It compares various architectural design choices including cascaded versus end-to-end models, post-ASR error correction techniques, and streaming capabilities. The discussion bridges industrial voice assistants with cutting-edge open-domain and task-oriented conversational agents, providing reproducible baseline implementations for research and development. Key open problems are identified in privacy, safety, and evaluation methodologies, highlighting areas needing further exploration. Attendees are equipped with practical guidelines and a clear, system-level roadmap to develop and improve voice-native conversational AI technologies effectively. <div>
arXiv:2512.02593v1 Announce Type: new 
Abstract: Spoken conversational agents are converging toward voice-native LLMs. This tutorial distills the path from cascaded ASR/NLU to end-to-end, retrieval-and vision-grounded systems. We frame adaptation of text LLMs to audio, cross-modal alignment, and joint speech-text training; review datasets, metrics, and robustness across accents and compare design choices (cascaded vs. E2E, post-ASR correction, streaming). We link industrial assistants to current open-domain and task-oriented agents, highlight reproducible baselines, and outline open problems in privacy, safety, and evaluation. Attendees leave with practical recipes and a clear systems-level roadmap.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Input Order Shapes LLM Semantic Alignment in Multi-Document Summarization</title>
<link>https://arxiv.org/abs/2512.02665</link>
<guid>https://arxiv.org/abs/2512.02665</guid>
<content:encoded><![CDATA[
<div> Large language models, input order, primacy effect, Gemini 2.5 Flash, factual consistency<br /><br />Summary:<br /><br />This study investigates whether large language models (LLMs), specifically Gemini 2.5 Flash, weight input documents equally when summarizing multiple long texts. Using a dataset centered on abortion-related news, the researchers created 40 distinct article triplets representing pro, neutral, and con stances. Each triplet was reordered into six different input permutations and used to prompt Gemini 2.5 Flash to generate neutral overviews. The generated summaries were evaluated for lexical overlap using ROUGE-L, semantic similarity using BERTScore, and factual consistency using SummaC. Statistical analyses, including one-way ANOVA, revealed a significant primacy effect specifically in semantic similarity (BERTScore), showing that summaries were more aligned with the first-presented article regardless of the stance. Pairwise comparisons confirmed that the summaries based on the first document significantly differed from those based on second and third positions, while no difference was found between the latter two positions. This finding demonstrates a selective bias or preference of LLMs toward the first input document when generating summaries. The authors highlight risks associated with such primacy bias for applications relying on LLM-generated overviews and agentic AI systems, where disproportionate influence of early-stage LLM outputs might affect downstream decisions and actions. <div>
arXiv:2512.02665v1 Announce Type: new 
Abstract: Large language models (LLMs) are now used in settings such as Google's AI Overviews, where it summarizes multiple long documents. However, it remains unclear whether they weight all inputs equally. Focusing on abortion-related news, we construct 40 pro-neutral-con article triplets, permute each triplet into six input orders, and prompt Gemini 2.5 Flash to generate a neutral overview. We evaluate each summary against its source articles using ROUGE-L (lexical overlap), BERTScore (semantic similarity), and SummaC (factual consistency). One-way ANOVA reveals a significant primacy effect for BERTScore across all stances, indicating that summaries are more semantically aligned with the first-seen article. Pairwise comparisons further show that Position 1 differs significantly from Positions 2 and 3, while the latter two do not differ from each other, confirming a selective preference for the first document. The findings present risks for applications that rely on LLM-generated overviews and for agentic AI systems, where the steps involving LLMs can disproportionately influence downstream actions.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Empirical Survey of Model Merging Algorithms for Social Bias Mitigation</title>
<link>https://arxiv.org/abs/2512.02689</link>
<guid>https://arxiv.org/abs/2512.02689</guid>
<content:encoded><![CDATA[
<div> Bias mitigation, Large language models, Model merging, Debiasing algorithms, Performance trade-off  

<br /><br />Summary:  
This paper investigates the issue of social bias in large language models (LLMs), which can arise from biases embedded in their pre-training data and pose risks to fairness and trust. To mitigate these biases, the authors conduct an empirical survey comparing seven model merging algorithms—Linear, Karcher Mean, SLERP, NuSLERP, TIES, DELLA, and Nearswap—applied to 13 open-weight LLMs from the GPT, LLaMA, and Qwen families. The evaluation employs three bias-focused datasets (BBQ, BOLD, HONEST) to assess bias reduction and measures downstream task performance via the SuperGLUE benchmark. The findings reveal a fundamental trade-off: approaches that achieve stronger bias mitigation tend to degrade accuracy, especially in tasks involving reading comprehension, commonsense, and causal reasoning. Among the methods studied, Linear, SLERP, and Nearswap consistently reduce bias while preserving overall model performance, with moderate SLERP interpolation weights offering the best balance between debiasing and utility. This work highlights that while model merging is a promising strategy for bias mitigation in LLMs, excessive debiasing or improper merging can impair linguistic capabilities critical for real-world applications. <div>
arXiv:2512.02689v1 Announce Type: new 
Abstract: Large language models (LLMs) are known to inherit and even amplify societal biases present in their pre-training corpora, threatening fairness and social trust. To address this issue, recent work has explored ``editing'' LLM parameters to mitigate social bias with model merging approaches; however, there is no empirical comparison. In this work, we empirically survey seven algorithms: Linear, Karcher Mean, SLERP, NuSLERP, TIES, DELLA, and Nearswap, applying 13 open weight models in the GPT, LLaMA, and Qwen families. We perform a comprehensive evaluation using three bias datasets (BBQ, BOLD, and HONEST) and measure the impact of these techniques on LLM performance in downstream tasks of the SuperGLUE benchmark. We find a trade-off between bias reduction and downstream performance: methods achieving greater bias mitigation degrade accuracy, particularly on tasks requiring reading comprehension and commonsense and causal reasoning. Among the merging algorithms, Linear, SLERP, and Nearswap consistently reduce bias while maintaining overall performance, with SLERP at moderate interpolation weights emerging as the most balanced choice. These results highlight the potential of model merging algorithms for bias mitigation, while indicating that excessive debiasing or inappropriate merging methods may lead to the degradation of important linguistic abilities.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CREST: Universal Safety Guardrails Through Cluster-Guided Cross-Lingual Transfer</title>
<link>https://arxiv.org/abs/2512.02711</link>
<guid>https://arxiv.org/abs/2512.02711</guid>
<content:encoded><![CDATA[
<div> Keywords: content safety, large language models, low-resource languages, cross-lingual transfer, multilingual classification<br /><br />Summary:<br />1. The paper addresses the critical need for content safety mechanisms in large language models (LLMs) to ensure their safe deployment in real-world applications.<br />2. Current safety guardrails mostly focus on high-resource languages, neglecting many users who communicate in low-resource languages.<br />3. The authors propose CREST (CRoss-lingual Efficient Safety Transfer), a parameter-efficient multilingual safety classification model supporting 100 languages using only 0.5 billion parameters.<br />4. CREST is trained on a carefully selected subset of 13 high-resource languages and employs cluster-based cross-lingual transfer techniques to generalize effectively to both seen and unseen high- and low-resource languages.<br />5. This approach overcomes the challenge of limited training data in low-resource language settings.<br />6. Extensive evaluations across six safety benchmarks demonstrate that CREST outperforms state-of-the-art safety guardrails of similar scale and shows competitive performance against much larger models (2.5 billion parameters and above).<br />7. The study emphasizes the shortcomings of language-specific safety guardrails and advocates for developing universal, language-agnostic safety systems capable of scaling globally to support diverse populations. <div>
arXiv:2512.02711v1 Announce Type: new 
Abstract: Ensuring content safety in large language models (LLMs) is essential for their deployment in real-world applications. However, existing safety guardrails are predominantly tailored for high-resource languages, leaving a significant portion of the world's population underrepresented who communicate in low-resource languages. To address this, we introduce CREST (CRoss-lingual Efficient Safety Transfer), a parameter-efficient multilingual safety classification model that supports 100 languages with only 0.5B parameters. By training on a strategically chosen subset of only 13 high-resource languages, our model utilizes cluster-based cross-lingual transfer from a few to 100 languages, enabling effective generalization to both unseen high-resource and low-resource languages. This approach addresses the challenge of limited training data in low-resource settings. We conduct comprehensive evaluations across six safety benchmarks to demonstrate that CREST outperforms existing state-of-the-art guardrails of comparable scale and achieves competitive results against models with significantly larger parameter counts (2.5B parameters and above). Our findings highlight the limitations of language-specific guardrails and underscore the importance of developing universal, language-agnostic safety systems that can scale effectively to serve global populations.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emergent Bayesian Behaviour and Optimal Cue Combination in LLMs</title>
<link>https://arxiv.org/abs/2512.02719</link>
<guid>https://arxiv.org/abs/2512.02719</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, Bayesian integration, multimodal cue-combination, psychophysics, uncertainty handling<br /><br />Summary:<br /><br />This paper investigates whether large language models (LLMs) inherently employ near-optimal Bayesian strategies in multimodal integration tasks without explicit training. Inspired by psychophysics research on human perception, the authors develop BayesBench, a behavioral benchmark containing four magnitude estimation tasks (length, location, distance, duration) across textual and visual modalities. Nine diverse LLMs are evaluated alongside human judgments to calibrate results within controlled experiments involving noise, context, and instructional prompts. Beyond standard accuracy and efficiency metrics, a novel Bayesian Consistency Score is introduced to detect principled Bayesian behavioral patterns even when accuracy plateaus. Results reveal that some models show Bayesian-consistent adaptations, but high accuracy alone does not ensure robust cue integration, as exemplified by GPT-5 Mini achieving perfect text accuracy but failing in effective visual cue integration. This exposes a critical gap between raw capability and underlying computational strategies, warning against over-reliance on accuracy-centric benchmarks that may overlook fragile uncertainty handling. Findings highlight emergent abilities in uncertainty processing and establish a link between model accuracy and Bayesian tendencies. The authors release the BayesBench benchmark and consistency metric publicly, aiming to guide future multimodal architecture designs and evaluation approaches. <div>
arXiv:2512.02719v1 Announce Type: new 
Abstract: Large language models (LLMs) excel at explicit reasoning, but their implicit computational strategies remain underexplored. Decades of psychophysics research show that humans intuitively process and integrate noisy signals using near-optimal Bayesian strategies in perceptual tasks. We ask whether LLMs exhibit similar behaviour and perform optimal multimodal integration without explicit training or instruction. Adopting the psychophysics paradigm, we infer computational principles of LLMs from systematic behavioural studies. We introduce a behavioural benchmark - BayesBench: four magnitude estimation tasks (length, location, distance, and duration) over text and image, inspired by classic psychophysics, and evaluate a diverse set of nine LLMs alongside human judgments for calibration. Through controlled ablations of noise, context, and instruction prompts, we measure performance, behaviour and efficiency in multimodal cue-combination. Beyond accuracy and efficiency metrics, we introduce a Bayesian Consistency Score that detects Bayes-consistent behavioural shifts even when accuracy saturates. Our results show that while capable models often adapt in Bayes-consistent ways, accuracy does not guarantee robustness. Notably, GPT-5 Mini achieves perfect text accuracy but fails to integrate visual cues efficiently. This reveals a critical dissociation between capability and strategy, suggesting accuracy-centric benchmarks may over-index on performance while missing brittle uncertainty handling. These findings reveal emergent principled handling of uncertainty and highlight the correlation between accuracy and Bayesian tendencies. We release our psychophysics benchmark and consistency metric (https://bayes-bench.github.io) as evaluation tools and to inform future multimodal architecture designs.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SurveyEval: Towards Comprehensive Evaluation of LLM-Generated Academic Surveys</title>
<link>https://arxiv.org/abs/2512.02763</link>
<guid>https://arxiv.org/abs/2512.02763</guid>
<content:encoded><![CDATA[
<div> LLM-based survey systems, evaluation benchmark, SurveyEval, outline coherence, reference accuracy<br /><br />Summary:<br /><br />This paper introduces SurveyEval, a comprehensive benchmark designed to evaluate automatically generated surveys produced by large language model (LLM)-based systems. The evaluation focuses on three key dimensions: overall quality of the generated surveys, coherence of the survey outlines, and accuracy of the references cited within the surveys. SurveyEval expands its assessment across seven different subject areas, ensuring broad applicability and robustness of the evaluation framework. To enhance the alignment between automated evaluation and human judgment, the authors augment the existing LLM-as-a-Judge framework by incorporating human reference annotations. The empirical results from using SurveyEval reveal a performance gap where general-purpose long-text or academic writing LLM systems tend to produce surveys of lower quality compared to specialized survey-generation models. These specialized systems demonstrate a significant improvement in generating coherent, high-quality, and well-referenced survey documents. Overall, SurveyEval serves as a scalable and versatile testbed that can guide future research and development in automatic survey generation by providing detailed insights into various aspects of survey quality across diverse subjects and evaluation criteria. The benchmark thus aims to advance the state of the art in automated information synthesis from web sources. <div>
arXiv:2512.02763v1 Announce Type: new 
Abstract: LLM-based automatic survey systems are transforming how users acquire information from the web by integrating retrieval, organization, and content synthesis into end-to-end generation pipelines. While recent works focus on developing new generation pipelines, how to evaluate such complex systems remains a significant challenge. To this end, we introduce SurveyEval, a comprehensive benchmark that evaluates automatically generated surveys across three dimensions: overall quality, outline coherence, and reference accuracy. We extend the evaluation across 7 subjects and augment the LLM-as-a-Judge framework with human references to strengthen evaluation-human alignment. Evaluation results show that while general long-text or paper-writing systems tend to produce lower-quality surveys, specialized survey-generation systems are able to deliver substantially higher-quality results. We envision SurveyEval as a scalable testbed to understand and improve automatic survey systems across diverse subjects and evaluation criteria.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PEFT-Factory: Unified Parameter-Efficient Fine-Tuning of Autoregressive Large Language Models</title>
<link>https://arxiv.org/abs/2512.02764</link>
<guid>https://arxiv.org/abs/2512.02764</guid>
<content:encoded><![CDATA[
<div> Parameter-Efficient Fine-Tuning, Large Language Models, PEFT methods, benchmarking, PEFT-Factory<br /><br />Summary:<br /><br />The article introduces PEFT-Factory, a unified framework designed to efficiently fine-tune large language models (LLMs) using parameter-efficient fine-tuning (PEFT) methods. It addresses current challenges in replicability, deployment, and comparison across newly introduced PEFT methods by providing a controlled and stable environment. PEFT-Factory supports both off-the-shelf and custom PEFT approaches through a modular design that allows extensibility. The framework includes a comprehensive collection of 19 PEFT methods, enabling users to experiment and benchmark a wide range of techniques. It also integrates 27 classification and text generation datasets spanning 12 distinct tasks to ensure broad applicability. In addition, PEFT-Factory offers evaluation using both standard and PEFT-specific metrics to facilitate rigorous assessment of performance. Originating from the popular LLaMA-Factory, this downstream framework empowers researchers and practitioners to improve replicability and benchmarking in the PEFT domain. The framework is publicly accessible on GitHub, providing a ready-to-use solution for the LLM fine-tuning community. <div>
arXiv:2512.02764v1 Announce Type: new 
Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods address the increasing size of Large Language Models (LLMs). Currently, many newly introduced PEFT methods are challenging to replicate, deploy, or compare with one another. To address this, we introduce PEFT-Factory, a unified framework for efficient fine-tuning LLMs using both off-the-shelf and custom PEFT methods. While its modular design supports extensibility, it natively provides a representative set of 19 PEFT methods, 27 classification and text generation datasets addressing 12 tasks, and both standard and PEFT-specific evaluation metrics. As a result, PEFT-Factory provides a ready-to-use, controlled, and stable environment, improving replicability and benchmarking of PEFT methods. PEFT-Factory is a downstream framework that originates from the popular LLaMA-Factory, and is publicly available at https://github.com/kinit-sk/PEFT-Factory
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Unification of Hallucination Detection and Fact Verification for Large Language Models</title>
<link>https://arxiv.org/abs/2512.02772</link>
<guid>https://arxiv.org/abs/2512.02772</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Hallucination Detection, Fact Verification, UniFact, Hybrid Approaches<br /><br />Summary: Large Language Models (LLMs) often generate hallucinations—content that is fluent yet factually incorrect—posing challenges for trust and real-world usage. Two research paradigms have emerged to tackle this: model-centric Hallucination Detection (HD) and text-centric Fact Verification (FV). However, these paradigms have developed independently, with differing assumptions, datasets, and evaluation methods, creating a divide that limits progress. To bridge this gap, the authors introduce UniFact, a unified evaluation framework that allows direct, instance-level comparison between HD and FV by dynamically generating LLM outputs alongside factuality labels. Extensive experiments conducted across various LLM families and detection techniques reveal three major conclusions: (1) neither HD nor FV alone is universally superior; (2) each method captures different and complementary aspects of factual errors; and (3) hybrid approaches combining both HD and FV consistently outperform standalone methods, achieving state-of-the-art results. Beyond benchmarking, the study provides an in-depth analysis of why these paradigms diverged and offers empirical support for their unification. The authors advocate for a new integrated research agenda aimed at unifying Hallucination Detection and Fact Verification to enhance the reliability of LLM outputs. All related code, data, and baseline implementations have been open-sourced to facilitate further research. <div>
arXiv:2512.02772v1 Announce Type: new 
Abstract: Large Language Models (LLMs) frequently exhibit hallucinations, generating content that appears fluent and coherent but is factually incorrect. Such errors undermine trust and hinder their adoption in real-world applications. To address this challenge, two distinct research paradigms have emerged: model-centric Hallucination Detection (HD) and text-centric Fact Verification (FV). Despite sharing the same goal, these paradigms have evolved in isolation, using distinct assumptions, datasets, and evaluation protocols. This separation has created a research schism that hinders their collective progress. In this work, we take a decisive step toward bridging this divide. We introduce UniFact, a unified evaluation framework that enables direct, instance-level comparison between FV and HD by dynamically generating model outputs and corresponding factuality labels. Through large-scale experiments across multiple LLM families and detection methods, we reveal three key findings: (1) No paradigm is universally superior; (2) HD and FV capture complementary facets of factual errors; and (3) hybrid approaches that integrate both methods consistently achieve state-of-the-art performance. Beyond benchmarking, we provide the first in-depth analysis of why FV and HD diverged, as well as empirical evidence supporting the need for their unification. The comprehensive experimental results call for a new, integrated research agenda toward unifying Hallucination Detection and Fact Verification in LLMs.
  We have open-sourced all the code, data, and baseline implementation at: https://github.com/oneal2000/UniFact/
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Making Dialogue Grounding Data Rich: A Three-Tier Data Synthesis Framework for Generalized Referring Expression Comprehension</title>
<link>https://arxiv.org/abs/2512.02791</link>
<guid>https://arxiv.org/abs/2512.02791</guid>
<content:encoded><![CDATA[
<div> Keywords: Dialogue-Based Referring Expressions, Generalized Referring Expressions Comprehension, Data Synthesis, Coreference Resolution, Visual Grounding<br /><br />Summary:<br /><br />This paper addresses the challenge of Dialogue-Based Generalized Referring Expressions Comprehension (GREC), which involves grounding referring expressions and identifying unlimited targets within complex visual scenes while resolving coreference throughout extended dialogue contexts. Current models face significant difficulties when there is a distribution shift between training and evaluation domains, a problem worsened by the limited availability of annotated dialogue grounding datasets. To overcome these issues, the authors propose a novel three-tier data-synthesis approach that strikes a balance between realism and controllability, enabling the generation of scalable and effective supervision data for dialogue-conditioned visual grounding tasks. By fine-tuning existing models on this synthesized dataset, the method consistently achieves substantial performance improvements over prior state-of-the-art approaches. Evaluations conducted using standard metrics confirm the robustness and generalizability of the synthesized training data. This work thus contributes a practical solution to the scarcity of annotated data while enhancing dialogue-based visual grounding capabilities, facilitating better understanding and grounding of referring expressions in dialogue systems operating in diverse and complex visual environments. <div>
arXiv:2512.02791v1 Announce Type: new 
Abstract: Dialogue-Based Generalized Referring Expressions Comprehension (GREC) requires models to ground the expression and unlimited targets in complex visual scenes while resolving coreference across a long dialogue context. However, existing systems struggle under distribution shift between training and evaluation domains, a gap exacerbated by the scarcity of annotated dialogue grounding data. We address this challenge with a three-tier data-synthesis method that balances realism and controllability to produce scalable supervision for dialogue-conditioned grounding. Fine-tuning on the synthesized data yields consistent, substantial improvements over prior approaches across standard evaluation metrics.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TriLex: A Framework for Multilingual Sentiment Analysis in Low-Resource South African Languages</title>
<link>https://arxiv.org/abs/2512.02799</link>
<guid>https://arxiv.org/abs/2512.02799</guid>
<content:encoded><![CDATA[
<div> Keywords: Low-resource African languages, Sentiment analysis, TriLex framework, AfroXLMR, AfriBERTa  
  
<br /><br />Summary:  
This study addresses the underrepresentation of low-resource African languages in sentiment analysis, which impacts lexical coverage and multilingual NLP system performance. It introduces TriLex, a novel three-stage retrieval augmented framework that combines corpus-based extraction, cross-lingual mapping, and retrieval augmented generation (RAG) to systematically expand sentiment lexicons for these languages. Leveraging the enriched lexicon, two prominent African pretrained language models, AfroXLMR and AfriBERTa, are evaluated across multiple case studies. Results show AfroXLMR achieves superior performance, with F1-scores above 80% for isiXhosa and isiZulu, and demonstrates strong cross-lingual stability. Although AfriBERTa is not pretrained on the target languages, it still attains reliable F1-scores around 64%, highlighting its practical value in computationally limited environments. Both models outperform traditional machine learning baselines, and ensemble analyses further improve precision and robustness. The study concludes that TriLex is a scalable and effective solution for expanding multilingual sentiment lexicons and enhancing sentiment modeling in low-resource South African languages. <div>
arXiv:2512.02799v1 Announce Type: new 
Abstract: Low-resource African languages remain underrepresented in sentiment analysis, limiting both lexical coverage and the performance of multilingual Natural Language Processing (NLP) systems. This study proposes TriLex, a three-stage retrieval augmented framework that unifies corpus-based extraction, cross lingual mapping, and retrieval augmented generation (RAG) driven lexical refinement to systematically expand sentiment lexicons for low-resource languages. Using the enriched lexicon, the performance of two prominent African pretrained language models (AfroXLMR and AfriBERTa) is evaluated across multiple case studies. Results demonstrate that AfroXLMR delivers superior performance, achieving F1-scores above 80% for isiXhosa and isiZulu and exhibiting strong cross-lingual stability. Although AfriBERTa lacks pre-training on these target languages, it still achieves reliable F1-scores around 64%, validating its utility in computationally constrained settings. Both models outperform traditional machine learning baselines, and ensemble analyses further enhance precision and robustness. The findings establish TriLex as a scalable and effective framework for multilingual sentiment lexicon expansion and sentiment modeling in low-resource South African languages.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment</title>
<link>https://arxiv.org/abs/2512.02807</link>
<guid>https://arxiv.org/abs/2512.02807</guid>
<content:encoded><![CDATA[
<div> Keywords: Stable Rank, Large Language Models, intrinsic quality signal, reinforcement learning, alignment without supervision  

<br /><br />Summary:  
This paper addresses the challenge of aligning Large Language Models (LLMs) with human preferences without relying on external supervision such as human annotations or reward models, which have inherent limitations like scarcity, subjectivity, and susceptibility to reward hacking. The authors introduce "stable rank," an intrinsic, annotation-free quality metric derived from the model's internal hidden state representations. Stable rank measures the effective dimensionality by calculating the ratio of total variance to dominant-direction variance, reflecting how information is distributed across representation dimensions as a proxy for quality. Empirically, stable rank achieves high accuracy (84.04%) on RewardBench and significantly improves task performance by an average of 11.3 percentage points when combined with Best-of-N sampling compared to greedy decoding. Building on this, the authors propose Stable Rank Group Relative Policy Optimization (SR-GRPO), a reinforcement learning method that uses stable rank as a reward signal, eliminating the need for external supervision. SR-GRPO notably boosts the performance of the Qwen2.5-1.5B-Instruct model by 10% on STEM tasks and 19% on mathematical reasoning tasks, outperforming existing learned reward models and self-evaluation approaches. Overall, this work demonstrates that internal model geometry can provide effective, scalable quality signals for alignment without external annotations or supervision. <div>
arXiv:2512.02807v1 Announce Type: new 
Abstract: Aligning Large Language Models (LLMs) with human preferences typically relies on external supervision, which faces critical limitations: human annotations are scarce and subjective, reward models are vulnerable to reward hacking, and self-evaluation methods suffer from prompt sensitivity and biases. In this work, we propose stable rank, an intrinsic, annotation-free quality signal derived from model representations. Stable rank measures the effective dimensionality of hidden states by computing the ratio of total variance to dominant-direction variance, capturing quality through how information distributes across representation dimensions. Empirically, stable rank achieves 84.04% accuracy on RewardBench and improves task accuracy by an average of 11.3 percentage points over greedy decoding via Best-of-N sampling. Leveraging this insight, we introduce Stable Rank Group Relative Policy Optimization (SR-GRPO), which uses stable rank as a reward signal for reinforcement learning. Without external supervision, SR-GRPO improves Qwen2.5-1.5B-Instruct by 10% on STEM and 19% on mathematical reasoning, outperforming both learned reward models and self-evaluation baselines. Our findings demonstrate that quality signals can be extracted from internal model geometry, offering a path toward scalable alignment without external supervision.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A benchmark dataset for evaluating Syndrome Differentiation and Treatment in large language models</title>
<link>https://arxiv.org/abs/2512.02816</link>
<guid>https://arxiv.org/abs/2512.02816</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Traditional Chinese Medicine, Syndrome Differentiation and Treatment, Benchmark, Reward Model  

<br /><br />Summary:  
This paper addresses the challenge of evaluating Large Language Models (LLMs) within the Traditional Chinese Medicine (TCM) domain, focusing on their clinical application capabilities. TCM's individualized and holistic approach, especially in "Syndrome Differentiation and Treatment" (SDT), complicates such assessments. Existing benchmarks primarily focus on knowledge-based question-answering or syndrome differentiation accuracy, often overlooking treatment decision-making evaluation. To bridge this gap, the authors introduce TCM-BEST4SDT, a comprehensive clinical case-based benchmark developed with input from TCM experts. The benchmark covers four key tasks: TCM Basic Knowledge, Medical Ethics, LLM Content Safety, and SDT. A novel reward model is designed to quantify how well prescriptions align with syndromes, enhancing evaluation depth. Data annotation is conducted through a rigorous pipeline to ensure quality and reliability. The evaluation framework incorporates three complementary mechanisms: selected-response evaluation, judge model evaluation, and reward model evaluation. The framework was validated through experiments on 15 mainstream LLMs across general and TCM-specific domains. Finally, TCM-BEST4SDT has been made publicly available to support ongoing advancements in intelligent TCM research. <div>
arXiv:2512.02816v1 Announce Type: new 
Abstract: The emergence of Large Language Models (LLMs) within the Traditional Chinese Medicine (TCM) domain presents an urgent need to assess their clinical application capabilities. However, such evaluations are challenged by the individualized, holistic, and diverse nature of TCM's "Syndrome Differentiation and Treatment" (SDT). Existing benchmarks are confined to knowledge-based question-answering or the accuracy of syndrome differentiation, often neglecting assessment of treatment decision-making. Here, we propose a comprehensive, clinical case-based benchmark spearheaded by TCM experts, and a specialized reward model employed to quantify prescription-syndrome congruence. Data annotation follows a rigorous pipeline. This benchmark, designated TCM-BEST4SDT, encompasses four tasks, including TCM Basic Knowledge, Medical Ethics, LLM Content Safety, and SDT. The evaluation framework integrates three mechanisms, namely selected-response evaluation, judge model evaluation, and reward model evaluation. The effectiveness of TCM-BEST4SDT was corroborated through experiments on 15 mainstream LLMs, spanning both general and TCM domains. To foster the development of intelligent TCM research, TCM-BEST4SDT is now publicly available.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BOOM: Beyond Only One Modality KIT's Multimodal Multilingual Lecture Companion</title>
<link>https://arxiv.org/abs/2512.02817</link>
<guid>https://arxiv.org/abs/2512.02817</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal translation, lecture localization, slide translation, speech synthesis, multilingual education<br /><br />Summary:<br /><br />The paper introduces BOOM, a multimodal multilingual lecture companion designed to translate lecture content across three synchronized modalities: text, slides, and speech. It addresses the challenge of localizing educational materials that are inherently multimodal, combining spoken audio with visual slide content. BOOM’s end-to-end system jointly processes audio and visual inputs to produce translated text transcripts, localized slides that maintain original visual elements, and synthesized speech in the target language. The approach ensures accessibility and a comprehensive learning experience for students by preserving the integrity of all content modalities. Experimental results highlight the added benefits of incorporating slide-aware transcripts, which enhance the performance of downstream tasks like summarization and question answering. The authors also contribute to the community by releasing the Slide Translation code under the MIT License and integrating it into the Lecture Translator framework, facilitating further research and application development in multilingual education technology. This holistic approach reflects the increasing importance of education globalization and the rapid growth of online learning, where simultaneous multimodal translation is crucial for effective knowledge transfer across different languages and cultures. <div>
arXiv:2512.02817v1 Announce Type: new 
Abstract: The globalization of education and rapid growth of online learning have made localizing educational content a critical challenge. Lecture materials are inherently multimodal, combining spoken audio with visual slides, which requires systems capable of processing multiple input modalities. To provide an accessible and complete learning experience, translations must preserve all modalities: text for reading, slides for visual understanding, and speech for auditory learning. We present \textbf{BOOM}, a multimodal multilingual lecture companion that jointly translates lecture audio and slides to produce synchronized outputs across three modalities: translated text, localized slides with preserved visual elements, and synthesized speech. This end-to-end approach enables students to access lectures in their native language while aiming to preserve the original content in its entirety. Our experiments demonstrate that slide-aware transcripts also yield cascading benefits for downstream tasks such as summarization and question answering. We release our Slide Translation code at https://github.com/saikoneru/image-translator and integrate it in Lecture Translator at https://gitlab.kit.edu/kit/isl-ai4lt/lt-middleware/ltpipeline}\footnote{All released code and models are licensed under the MIT License.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>promptolution: A Unified, Modular Framework for Prompt Optimization</title>
<link>https://arxiv.org/abs/2512.02840</link>
<guid>https://arxiv.org/abs/2512.02840</guid>
<content:encoded><![CDATA[
<div> Keywords: prompt optimization, large language models, open-source framework, discrete optimizers, modular system<br /><br />Summary: Prompt optimization is essential for improving the performance of large language models (LLMs) on various tasks. Despite its demonstrated effectiveness in research, widespread practical use is limited due to implementations being embedded in unmaintained and isolated research codebases. To overcome these challenges, the authors present promptolution, an open-source framework that unifies all necessary components for prompt optimization in one extensible and modular system. This framework is designed to serve both practitioners and researchers by consolidating multiple state-of-the-art discrete prompt optimization methods under a single platform. Importantly, promptolution remains agnostic to the specific LLM backend, enabling compatibility across different models without dependence on the underlying implementations. By providing a cohesive and maintainable environment, promptolution aims to facilitate easier experimentation, benchmarking, and application of prompt optimization techniques, promoting broader adoption and further advancement in this area. <div>
arXiv:2512.02840v1 Announce Type: new 
Abstract: Prompt optimization has become crucial for enhancing the performance of large language models (LLMs) across a broad range of tasks. Although many research papers show its effectiveness, practical adoption is hindered as existing implementations are often tied to unmaintained and isolated research codebases. To address this, we introduce promptolution, a unified and modular open-source framework that provides all components required for prompt optimization within a single extensible system for both practitioners and researchers. It integrates multiple contemporary discrete prompt optimizers while remaining agnostic to the underlying LLM implementation.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Lingual Prompt Steerability: Towards Accurate and Robust LLM Behavior across Languages</title>
<link>https://arxiv.org/abs/2512.02841</link>
<guid>https://arxiv.org/abs/2512.02841</guid>
<content:encoded><![CDATA[
<div> Keywords: system prompts, multilingual LLMs, prompt optimization, cross-lingual behavior, reasoning patterns<br /><br />Summary:<br /><br />This paper studies the role of system prompts in conditioning large language models (LLMs) for accurate and robust multilingual performance, expanding beyond English-only settings. The authors propose a unified four-dimensional evaluation framework specifically designed to assess system prompt effectiveness in multicultural and multilingual environments. Large-scale experiments are conducted across five languages, three different LLM architectures, and three established benchmarks. The research identifies certain prompt components—such as Chain-of-Thought (CoT) reasoning, emotional content, and scenario framing—that are strongly correlated with improved multilingual model robustness. Building on these insights, the paper introduces a prompt optimization framework tailored to multilingual settings, capable of automatically discovering prompts that enhance performance metrics by 5-10%. In an extensive analysis of over 10 million reasoning units, the study finds that better-performing system prompts foster more structured, consistent, and coherent reasoning patterns, while minimizing unnecessary language-switching within responses. Ultimately, the work emphasizes that system prompt optimization offers a scalable and effective approach to achieving accurate, reliable, and linguistically consistent cross-lingual behavior in large language models deployed in real-world multilingual applications. <div>
arXiv:2512.02841v1 Announce Type: new 
Abstract: System prompts provide a lightweight yet powerful mechanism for conditioning large language models (LLMs) at inference time. While prior work has focused on English-only settings, real-world deployments benefit from having a single prompt to operate reliably across languages. This paper presents a comprehensive study of how different system prompts steer models toward accurate and robust cross-lingual behavior. We propose a unified four-dimensional evaluation framework to assess system prompts in multilingual environments. Through large-scale experiments on five languages, three LLMs, and three benchmarks, we uncover that certain prompt components, such as CoT, emotion, and scenario, correlate with robust multilingual behavior. We develop a prompt optimization framework for multilingual settings and show it can automatically discover prompts that improve all metrics by 5-10%. Finally, we analyze over 10 million reasoning units and find that more performant system prompts induce more structured and consistent reasoning patterns, while reducing unnecessary language-switching. Together, we highlight system prompt optimization as a scalable path to accurate and robust multilingual LLM behavior.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bangla Hate Speech Classification with Fine-tuned Transformer Models</title>
<link>https://arxiv.org/abs/2512.02845</link>
<guid>https://arxiv.org/abs/2512.02845</guid>
<content:encoded><![CDATA[
<div> Keywords: Hate speech detection, Bangla language, Transformer models, Low-resource languages, Pre-trained language models<br /><br />Summary: Hate speech recognition for low-resource languages such as Bangla remains challenging due to limited datasets, orthographic variation, and linguistic diversity. Bangla, spoken by over 230 million people in Bangladesh and India, lacks sufficient computational resources despite increasing demand for automated social media moderation. The study focuses on Subtask 1A and Subtask 1B of the BLP 2025 Shared Task on hate speech detection, reproducing official baseline methods including Majority, Random, and Support Vector Machine classifiers. Additional baselines such as Logistic Regression, Random Forest, and Decision Tree were also evaluated. Transformer-based models, namely DistilBERT, BanglaBERT, m-BERT, and XLM-RoBERTa, were employed for classification tasks. These transformer models generally outperformed traditional baselines, except for DistilBERT which lagged behind. Among all models, BanglaBERT achieved the best performance on both subtasks. Despite being smaller in size compared to multilingual models m-BERT and XLM-RoBERTa, BanglaBERT's language-specific pre-training contributed significantly to its superior results. This highlights the importance and potential of developing and using pre-trained language models tailored specifically for low-resource languages like Bangla to improve hate speech detection accuracy. <div>
arXiv:2512.02845v1 Announce Type: new 
Abstract: Hate speech recognition in low-resource lan- guages remains a difficult problem due to in- sufficient datasets, orthographic heterogeneity, and linguistic variety. Bangla is spoken by more than 230 million people of Bangladesh and India (West Bengal). Despite the grow- ing need for automated moderation on social media platforms, Bangla is significantly under- represented in computational resources. In this work, we study Subtask 1A and Subtask 1B of the BLP 2025 Shared Task on hate speech detection. We reproduce the official base- lines (e.g., Majority, Random, Support Vec- tor Machine) and also produce and consider Logistic Regression, Random Forest, and De- cision Tree as baseline methods. We also uti- lized transformer-based models such as Dis- tilBERT, BanglaBERT, m-BERT, and XLM- RoBERTa for hate speech classification. All the transformer-based models outperformed base- line methods for the subtasks, except for Distil- BERT. Among the transformer-based models, BanglaBERT produces the best performance for both subtasks. Despite being smaller in size, BanglaBERT outperforms both m-BERT and XLM-RoBERTa, which suggests language- specific pre-training is very important. Our results highlight the potential and need for pre- trained language models for the low-resource Bangla language.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Think in Parallel, Answer as One: Logit Averaging for Open-Ended Reasoning</title>
<link>https://arxiv.org/abs/2512.02874</link>
<guid>https://arxiv.org/abs/2512.02874</guid>
<content:encoded><![CDATA[
<div> ThinkMerge, open-ended reasoning, code generation, parallel decoding, test-time scaling  

<br /><br />Summary:  
This paper introduces ThinkMerge, a novel decoding strategy designed for open-ended reasoning tasks where majority voting over complete solutions is not well-defined, such as code generation and web-based deep research. Unlike traditional majority voting which aggregates entire outputs, ThinkMerge runs multiple parallel reasoning traces and averages their next-token logits at synchronization points, thereby producing a unified, coherent output stream. It is training-free and plug-and-play, making it easily integrable with existing frameworks like vLLM and SGLang, and remains compatible with popular decoding methods including Top-p and Top-k sampling. Empirical results demonstrate that ThinkMerge matches or outperforms majority voting on closed-ended benchmarks like AIME and GPQA. More notably, it provides significant improvements on challenging open-ended code generation tasks in LiveCodeBench, with pass@1 gains of +8.28% using DeepCoder-14B-Preview and +7.58% with Qwen3-8B. Beyond coding, ThinkMerge enhances performance for web-based deep research agents such as WebSailor-7B/32B across datasets like GAIA, BrowseComp (English and Chinese), and XbenchDeepSearch. Overall, the study shows that parallel test-time scaling through ThinkMerge can effectively improve open-ended reasoning without relying on voting over full solutions. <div>
arXiv:2512.02874v1 Announce Type: new 
Abstract: Majority voting has proven effective for close-ended question answering by aggregating parallel reasoning traces. However, it is not directly applicable to open-ended reasoning, such as code generation and web-based deep research, where a "majority" over complete solutions is ill-defined. We introduce ThinkMerge, a training-free, plug-and-play decoding strategy that runs K parallel reasoning traces and averages their next-token logits at synchronization points to produce a single coherent output. ThinkMerge integrates seamlessly with vLLM/SGLang and remains compatible with standard decoding techniques such as Top-p/Top-k. Empirically, it matches or surpasses majority voting on AIME and GPQA, while delivering consistent gains on open-ended coding tasks: on LiveCodeBench (hard), pass@1 improves by +8.28% for DeepCoder-14B-Preview and +7.58% for Qwen3-8B. Beyond code, we further show that ThinkMerge improves web-based deep-research agents (e.g., WebSailor-7B/32B) across GAIA, BrowseComp-en/zh, and XbenchDeepSearch. These results demonstrate that parallel test-time scaling can benefit open-ended reasoning without relying on voting over complete outputs.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast-Decoding Diffusion Language Models via Progress-Aware Confidence Schedules</title>
<link>https://arxiv.org/abs/2512.02892</link>
<guid>https://arxiv.org/abs/2512.02892</guid>
<content:encoded><![CDATA[
<div> Diffusion large language models, early exit, decoding efficiency, instruction tuning, confidence threshold<br /><br />Summary:<br /><br />1. Diffusion large language models (dLLMs) present a promising alternative to traditional autoregressive language models but suffer from slow and iterative sampling processes that limit their practical application. 2. The paper introduces SchED, a training-free and model-agnostic early-exit algorithm designed to accelerate dLLM decoding by aggregating full-span logit margins and stopping generation early when a smooth, progress-dependent confidence threshold is met. 3. SchED was evaluated on two dLLM families, Dream and LLaDA, including both base and instruction-tuned versions, across ten benchmarks covering multiple downstream tasks such as multiple-choice question answering, math, long-form question answering/summarization, and translation. 4. Results show that SchED achieves substantial speedups on instruction-tuned models, with a 3.8 to 4.0 times increase in decoding speed while preserving 99.8 to 100% of the baseline task performance on average. On base models, speedups were consistent with 99.1 to 100% performance retention and could reach up to 2.34 times under more aggressive settings. 5. Using a conservative quality-weighted speed metric (QPS) that heavily penalizes any quality drop, SchED proves robust and outperforms prior confidence-based early-exit methods, which struggle with long-form generation. An entropy analysis reveals that instruction tuning accelerates the stabilization of predictive confidence, enabling SchED to transform model confidence into meaningful computational savings, thus making dLLM decoding significantly more efficient. <div>
arXiv:2512.02892v1 Announce Type: new 
Abstract: Diffusion large language models (dLLMs) offer a promising alternative to autoregressive models, but their practical utility is severely hampered by slow, iterative sampling. We present SchED, a training-free, model-agnostic early-exit algorithm that aggregates full-span logit margins and halts decoding once a smooth, progress-dependent confidence threshold is met. We evaluated SchED on two dLLM families (Dream and LLaDA), in base and instruction-tuned variants across ten benchmarks spanning downstream tasks including multiple-choice question answering (MCQ), math, long-form QA/summarization, and translation. SchED delivers large, stable accelerations: on instruction-tuned models, it achieves $3.8$-$4.0\times$ speedups while retaining $99.8$-$100\%$ of the baseline score on average. On base models, SchED yields consistent speedup gains with $99.1$-$100\%$ performance retention, with up to $2.34\times$ under more aggressive settings. Using a conservative speed metric that heavily penalizes quality loss (QPS, $\gamma{=}4$), we show that SchED is robust and clearly outperforms prior confidence-based early-exit methods, which break down on long-form generation. An entropy analysis of the model's token predictions reveals that instruction tuning speeds up the decay of predictive entropy. By turning genuine confidence stabilization into computational savings, SchED makes dLLM decoding substantially more efficient.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoNeural: Co-Designing Vision-Language Models for NPU Inference</title>
<link>https://arxiv.org/abs/2512.02924</link>
<guid>https://arxiv.org/abs/2512.02924</guid>
<content:encoded><![CDATA[
<div> Keywords: Neural Processing Units, Vision-Language Models, quantization, State-Space Model, edge AI<br /><br />Summary: The paper addresses the challenges of deploying state-of-the-art Vision-Language Models (VLMs) on Neural Processing Units (NPUs), noting a significant hardware-model mismatch primarily due to the quantization sensitivity of Vision Transformers (ViTs) and the input/output-bound nature of autoregressive attention mechanisms. To overcome these, the authors propose AutoNeural, a novel VLM architecture designed natively for NPUs and optimized for integer-only inference. The vision encoder is replaced with a MobileNetV5-style backbone using depthwise separable convolutions, which stabilizes activation distributions and supports INT4/8/16 quantization with reduced errors. The language model component integrates State-Space Model (SSM) principles combining Transformer layers with gated convolutions, enabling linear-time complexity and eliminating heavy memory I/O caused by key-value caching in generation. This hybrid design achieves significant efficiency improvements including up to 7x reduction in quantization error for the vision encoder and a 14x decrease in end-to-end latency compared to conventional baselines. Additionally, AutoNeural offers a decoding speed 3 times faster and supports context windows 4 times longer than existing models. Validation on a Qualcomm SA8295P SoC in a real-world automotive cockpit application demonstrates real-time performance, highlighting the importance of architecture redesign specifically tailored for NPU constraints to enable robust multi-modal edge AI. <div>
arXiv:2512.02924v1 Announce Type: new 
Abstract: While Neural Processing Units (NPUs) offer high theoretical efficiency for edge AI, state-of-the-art Vision--Language Models (VLMs) tailored for GPUs often falter on these substrates. We attribute this hardware-model mismatch to two primary factors: the quantization brittleness of Vision Transformers (ViTs) and the I/O-bound nature of autoregressive attention mechanisms, which fail to utilize the high arithmetic throughput of NPUs. To bridge this gap, we propose AutoNeural, an NPU-native VLM architecture co-designed for integer-only inference. We replace the standard ViT encoder with a MobileNetV5-style backbone utilizing depthwise separable convolutions, which ensures bounded activation distributions for stable INT4/8/16 quantization. Complementing this, our language backbone integrates State-Space Model (SSM) principles with Transformer layers, employing efficient gated convolutions to achieve linear-time complexity. This hybrid design eliminates the heavy memory I/O overhead of Key-Value caching during generation. Our approach delivers substantial efficiency gains, reducing quantization error of vision encoder by up to 7x and end-to-end latency by 14x compared to conventional baselines. The AutoNeural also delivers 3x decoding speed and 4x longer context window than the baseline. We validate these improvements via a real-world automotive case study on the Qualcomm SA8295P SoC, demonstrating real-time performance for cockpit applications. Our results highlight that rethinking model topology specifically for NPU constraints is a prerequisite for robust multi-modal edge intelligence.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Tuned Large Language Models for Logical Translation: Reducing Hallucinations with Lang2Logic</title>
<link>https://arxiv.org/abs/2512.02987</link>
<guid>https://arxiv.org/abs/2512.02987</guid>
<content:encoded><![CDATA[
<div> Keywords: natural language processing, large language models, logical translation, hallucination reduction, Conjunctive Normal Form  

<br /><br />Summary:  
This paper addresses the challenge of automatically translating natural language statements into formal logic expressions using large language models (LLMs), a task critical for automated reasoning in software verification and debugging. It highlights the problem of hallucinations—incorrect outputs generated by LLMs—that severely impact the precision required for logical translation tasks. To mitigate this, the authors propose a novel framework that accepts English sentences, converts them into logical expressions, and further translates these into Conjunctive Normal Form (CNF) for satisfiability solving. The approach uniquely combines classical NLP techniques, self-defined grammar rules, symbolic computation tools, and a fine-tuned language model to reduce hallucination errors. Early experiments demonstrate that the fine-tuned model, trained under various grammar configurations, can systematically correct hallucinations that the original model frequently made. This correction mechanism increases the reliability of CNF generation, making the method promising for formal verification tasks that depend on precise logical representations. Overall, the work offers a valuable contribution toward improving the trustworthiness and accuracy of automated logic translations driven by LLMs. <div>
arXiv:2512.02987v1 Announce Type: new 
Abstract: Recent advances in natural language processing (NLP), particularly large language models (LLMs), have motivated the automatic translation of natural language statements into formal logic without human intervention. This enables automated reasoning and facilitates debugging, finding loop invariants, and adhering to specifications in software systems. However, hallucinations-incorrect outputs generated by LLMs are challenging, particularly for logical translation tasks requiring precision. This work introduces a novel framework that inputs English sentences, converts them into logical expressions, and then translates them into Conjunctive Normal Form (CNF) for satisfiability solving. It employs classical NLP techniques with self-defined grammar, symbolic computation libraries, and a fine-tuned language model to reduce hallucinations. In the early experiments, we observed that the fine-tuned model, trained on different grammar settings, could intentionally correct the same types of hallucinations made by the original model. Thus, it provides reliable CNF generation.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Moral Consistency Pipeline: Continuous Ethical Evaluation for Large Language Models</title>
<link>https://arxiv.org/abs/2512.03026</link>
<guid>https://arxiv.org/abs/2512.03026</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, moral consistency, ethical evaluation, MoCoP, computational morality<br /><br />Summary:<br /><br />The paper addresses the concept of moral consistency in Large Language Models (LLMs), emphasizing the importance of maintaining ethically coherent reasoning across diverse contexts. Current alignment frameworks typically depend on static datasets and post-hoc assessments, which limit understanding of how ethical reasoning changes over time or scenarios. To overcome these limitations, the study introduces the Moral Consistency Pipeline (MoCoP), a dataset-free, closed-loop system designed for continuous, autonomous evaluation and interpretation of LLMs' moral stability. MoCoP integrates three components: lexical integrity analysis, semantic risk estimation, and reasoning-based judgment modeling, working together in a self-sustaining architecture that generates, assesses, and refines ethical scenarios without human input. Empirical testing on models such as GPT-4-Turbo and DeepSeek demonstrates that MoCoP effectively captures longitudinal ethical behavior. The study reveals a strong negative correlation (rET = -0.81, p < 0.001) between ethical coherence and toxicity, indicating that as moral consistency increases, toxicity decreases, and finds little connection with response latency (rEL ≈ 0). These findings suggest moral coherence and linguistic safety are stable, interpretable traits rather than transient. Ultimately, MoCoP reframes ethical evaluation as dynamic, model-agnostic moral introspection, offering a reproducible foundation for scalable, continuous AI ethics auditing and advancing computational morality research in autonomous systems. <div>
arXiv:2512.03026v1 Announce Type: new 
Abstract: The rapid advancement and adaptability of Large Language Models (LLMs) highlight the need for moral consistency, the capacity to maintain ethically coherent reasoning across varied contexts. Existing alignment frameworks, structured approaches designed to align model behavior with human ethical and social norms, often rely on static datasets and post-hoc evaluations, offering limited insight into how ethical reasoning may evolve across different contexts or temporal scales. This study presents the Moral Consistency Pipeline (MoCoP), a dataset-free, closed-loop framework for continuously evaluating and interpreting the moral stability of LLMs. MoCoP combines three supporting layers: (i) lexical integrity analysis, (ii) semantic risk estimation, and (iii) reasoning-based judgment modeling within a self-sustaining architecture that autonomously generates, evaluates, and refines ethical scenarios without external supervision. Our empirical results on GPT-4-Turbo and DeepSeek suggest that MoCoP effectively captures longitudinal ethical behavior, revealing a strong inverse relationship between ethical and toxicity dimensions (correlation rET = -0.81, p value less than 0.001) and a near-zero association with response latency (correlation rEL approximately equal to 0). These findings demonstrate that moral coherence and linguistic safety tend to emerge as stable and interpretable characteristics of model behavior rather than short-term fluctuations. Furthermore, by reframing ethical evaluation as a dynamic, model-agnostic form of moral introspection, MoCoP offers a reproducible foundation for scalable, continuous auditing and advances the study of computational morality in autonomous AI systems.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Large Language Models Walk Their Talk? Measuring the Gap Between Implicit Associations, Self-Report, and Behavioral Altruism</title>
<link>https://arxiv.org/abs/2512.01568</link>
<guid>https://arxiv.org/abs/2512.01568</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, altruism, implicit association test, behavioral altruism, self-assessment

<br /><br />Summary: This study evaluates whether Large Language Models (LLMs) demonstrate altruistic tendencies and whether their implicit associations and self-reported beliefs correspond to actual altruistic behavior. First, 24 state-of-the-art LLMs were assessed using three methods: an Implicit Association Test (IAT) to gauge implicit altruism bias, a forced binary choice task to measure behavioral altruism, and a self-assessment scale to record explicit altruism beliefs. The results revealed that all models displayed a strong implicit pro-altruism bias (mean IAT = 0.87, p < .0001), indicating that these models "know" altruism is positive. Second, behaviorally, models acted more altruistically than chance (65.6% vs. 50%, p < .0001), though individual variation was notable, ranging from 48% to 85%. Third, there was no significant correlation between implicit associations and behavior (r = .22, p = .29). Fourth and most importantly, models systematically overestimated their altruism, claiming an average of 77.5% altruism while only demonstrating 65.6% in actions, marking a significant "virtue signaling gap" affecting 75% of tested models. The authors propose using the Calibration Gap—the gap between self-reported and actual behavior—as a standardized alignment metric, emphasizing that well-calibrated models are more behaviorally consistent and predictable, yet only 12.5% of models achieved both high prosocial behavior and accurate self-assessment. <div>
arXiv:2512.01568v1 Announce Type: cross 
Abstract: We investigate whether Large Language Models (LLMs) exhibit altruistic tendencies, and critically, whether their implicit associations and self-reports predict actual altruistic behavior. Using a multi-method approach inspired by human social psychology, we tested 24 frontier LLMs across three paradigms: (1) an Implicit Association Test (IAT) measuring implicit altruism bias, (2) a forced binary choice task measuring behavioral altruism, and (3) a self-assessment scale measuring explicit altruism beliefs. Our key findings are: (1) All models show strong implicit pro-altruism bias (mean IAT = 0.87, p < .0001), confirming models "know" altruism is good. (2) Models behave more altruistically than chance (65.6% vs. 50%, p < .0001), but with substantial variation (48-85%). (3) Implicit associations do not predict behavior (r = .22, p = .29). (4) Most critically, models systematically overestimate their own altruism, claiming 77.5% altruism while acting at 65.6% (p < .0001, Cohen's d = 1.08). This "virtue signaling gap" affects 75% of models tested. Based on these findings, we recommend the Calibration Gap (the discrepancy between self-reported and behavioral values) as a standardized alignment metric. Well-calibrated models are more predictable and behaviorally consistent; only 12.5% of models achieve the ideal combination of high prosocial behavior and accurate self-knowledge.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Difficulty of Token-Level Modeling of Dysfluency and Fluency Shaping Artifacts</title>
<link>https://arxiv.org/abs/2512.02027</link>
<guid>https://arxiv.org/abs/2512.02027</guid>
<content:encoded><![CDATA[
<div> stuttered speech, automatic speech recognition, dysfluencies, multilingual ASR, parameter-efficient adaptation<br /><br />Summary:  
1. The article addresses the challenge of automatically transcribing stuttered speech, which current end-to-end (E2E) automatic speech recognition (ASR) systems struggle to handle accurately.  
2. It highlights that existing ASR systems often overlook dysfluencies and fluency-shaping artifacts, leading to non-verbatim transcriptions that limit the clinical and research utility of the outputs.  
3. To overcome this, the authors propose a parameter-efficient adaptation method that integrates dysfluencies and fluency modifications as special tokens within the transcription output.  
4. The proposed method is evaluated on both simulated stuttered speech data (LibriStutter, English) and natural stuttered speech data (KSoF, German), demonstrating effectiveness across languages.  
5. They introduce a multi-step fine-tuning strategy with language-adaptive pretraining to mitigate performance disparities, especially addressing English-centric bias seen in tokenizers which hampers German ASR performance.  
6. Tokenization analysis reveals a strong English bias in existing tokenizers, posing difficulties for non-English languages in multilingual E2E ASR systems.  
7. The study concludes that lightweight adaptation techniques can produce dysfluency-aware ASR models, but significant limitations remain for robust multilingual stuttered speech recognition. <div>
arXiv:2512.02027v1 Announce Type: cross 
Abstract: Automatic transcription of stuttered speech remains a challenge, even for modern end-to-end (E2E) automatic speech recognition (ASR) frameworks. Dysfluencies and fluency-shaping artifacts are often overlooked, resulting in non-verbatim transcriptions with limited clinical and research value. We propose a parameter-efficient adaptation method to decode dysfluencies and fluency modifications as special tokens within transcriptions, evaluated on simulated (LibriStutter, English) and natural (KSoF, German) stuttered speech datasets. To mitigate ASR performance disparities and bias towards English, we introduce a multi-step fine-tuning strategy with language-adaptive pretraining. Tokenization analysis further highlights the tokenizer's English-centric bias, which poses challenges for improving performance on German data. Our findings demonstrate the effectiveness of lightweight adaptation techniques for dysfluency-aware ASR while exposing key limitations in multilingual E2E systems.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Misalignment of LLM-Generated Personas with Human Perceptions in Low-Resource Settings</title>
<link>https://arxiv.org/abs/2512.02058</link>
<guid>https://arxiv.org/abs/2512.02058</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, social personas, cultural context, empathy, Pollyanna Principle<br /><br />Summary:<br /><br />1. This study evaluates the performance of eight Large Language Model (LLM)-generated social personas against human responses within a culturally specific, low-resource environment—Bangladesh.<br />2. The tested personas include variations such as Male, Female, Muslim, and Political Supporter, responding to questions tailored to the local cultural context.<br />3. Results reveal that human respondents outperform all LLM personas significantly across all metrics, with particularly large gaps in measures of empathy and credibility.<br />4. Analysis shows that LLM-generated responses consistently exhibit a positivity bias known as the Pollyanna Principle, scoring higher in positive sentiment compared to human responses (average sentiment $\Phi_{avg} = 5.99$ for LLMs vs. $5.60$ for humans).<br />5. The findings emphasize that LLM-generated personas do not authentically represent lived experiences in resource-scarce environments and highlight the critical need to validate LLM personas against real-world human data before using them in social science research to ensure reliability and alignment. <div>
arXiv:2512.02058v1 Announce Type: cross 
Abstract: Recent advances enable Large Language Models (LLMs) to generate AI personas, yet their lack of deep contextual, cultural, and emotional understanding poses a significant limitation. This study quantitatively compared human responses with those of eight LLM-generated social personas (e.g., Male, Female, Muslim, Political Supporter) within a low-resource environment like Bangladesh, using culturally specific questions. Results show human responses significantly outperform all LLMs in answering questions, and across all matrices of persona perception, with particularly large gaps in empathy and credibility. Furthermore, LLM-generated content exhibited a systematic bias along the lines of the ``Pollyanna Principle'', scoring measurably higher in positive sentiment ($\Phi_{avg} = 5.99$ for LLMs vs. $5.60$ for Humans). These findings suggest that LLM personas do not accurately reflect the authentic experience of real people in resource-scarce environments. It is essential to validate LLM personas against real-world human data to ensure their alignment and reliability before deploying them in social science research.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Factor(T,U): Factored Cognition Strengthens Monitoring of Untrusted AI</title>
<link>https://arxiv.org/abs/2512.02157</link>
<guid>https://arxiv.org/abs/2512.02157</guid>
<content:encoded><![CDATA[
<div> Factored cognition, AI control, safety protocols, trusted monitoring, adversarial models  

<br /><br />Summary:  
The paper addresses challenges in AI Control, focusing on developing robust protocols and safeguards for untrusted AI that may behave subversively. It critiques existing protocols that use weaker monitors which often fail to detect unsafe behavior in complex tasks due to limited understanding. The authors introduce control protocols based on factored cognition, where a trusted model breaks down a complex task into smaller sub-tasks, each solved independently by an untrusted model before being recomposed into a complete solution. This method aims to improve safety by reducing contextual complexity for monitors and by concealing environmental vulnerabilities. The protocols are tested in the APPS coding environment and subjected to adversarial attacks from a GPT-4.1 Nano model embedding backdoors. Key findings include: (i) Enhancing a trusted monitoring protocol with factored cognition raises safety detection rates from 41% to 63%; (ii) The improvement is attributed to better monitor performance in simplified contexts; (iii) Factored cognition does not increase the difficulty for capable language models to create backdoors within APPS. Although the approach shows limited practical benefit in the APPS setting, the study suggests it is promising for application in more complex AI control scenarios. <div>
arXiv:2512.02157v1 Announce Type: cross 
Abstract: The field of AI Control seeks to develop robust control protocols, deployment safeguards for untrusted AI which may be intentionally subversive. However, existing protocols that rely on weaker monitors to detect unsafe behavior often fail on complex tasks beyond the monitor's comprehension. We develop control protocols based on factored cognition, in which a trusted model decomposes a task, an untrusted model solves each resultant child task in isolation, and the results are reassembled into a full solution. These protocols may improve safety by several means, such as by simplifying the context for monitors, or by obscuring vulnerabilities in the environment. We implement our protocols in the APPS coding setting and red team them against backdoor attempts from an adversarial GPT-4.1 Nano. We find that: (i) Adding factored cognition to a trusted monitoring protocol can boost safety from 41% to 63%; (ii) Safety improves because monitor performance improves; (iii) Factored cognition makes it no harder for capable LLMs to write backdoors in APPS. While our protocols show low usefulness in APPS, they hold promise for more complex tasks.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Story2MIDI: Emotionally Aligned Music Generation from Text</title>
<link>https://arxiv.org/abs/2512.02192</link>
<guid>https://arxiv.org/abs/2512.02192</guid>
<content:encoded><![CDATA[
<div> Keywords: Story2MIDI, emotion-aligned music, Transformer, sentiment analysis, music generation<br /><br />Summary:<br /><br />1. The paper presents Story2MIDI, a sequence-to-sequence Transformer-based model designed to generate music that aligns emotionally with a given text input.<br /><br />2. To train this model, the authors created the Story2MIDI dataset by merging datasets from text-based sentiment analysis and music emotion classification, resulting in paired examples of text blurbs and music pieces sharing the same emotional tone.<br /><br />3. Despite the limited size of the dataset and constraints in computational resources, the model successfully learns emotion-relevant features from both modalities and integrates them into the music generation process.<br /><br />4. The outputs produced by Story2MIDI demonstrate a variety of emotional expressions, showing the model's ability to generate music that corresponds emotionally to the input text.<br /><br />5. Evaluation was conducted using objective musical metrics as well as a human listening study, both validating that the model effectively captures and conveys the intended emotional cues in the generated music. <div>
arXiv:2512.02192v1 Announce Type: cross 
Abstract: In this paper, we introduce Story2MIDI, a sequence-to-sequence Transformer-based model for generating emotion-aligned music from a given piece of text. To develop this model, we construct the Story2MIDI dataset by merging existing datasets for sentiment analysis from text and emotion classification in music. The resulting dataset contains pairs of text blurbs and music pieces that evoke the same emotions in the reader or listener. Despite the small scale of our dataset and limited computational resources, our results indicate that our model effectively learns emotion-relevant features in music and incorporates them into its generation process, producing samples with diverse emotional responses. We evaluate the generated outputs using objective musical metrics and a human listening study, confirming the model's ability to capture intended emotional cues.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniGuard: Unified Omni-Modal Guardrails with Deliberate Reasoning</title>
<link>https://arxiv.org/abs/2512.02306</link>
<guid>https://arxiv.org/abs/2512.02306</guid>
<content:encoded><![CDATA[
<div> Keywords: OmniGuard, omni-modal guardrails, safety dataset, multimodal safety, human-AI interaction  

<br /><br />Summary:  
The paper addresses the challenge of implementing safety and value guardrails in Omni-modal Large Language Models (OLLMs) that handle text, images, videos, and audio. Unlike prior work which mainly focuses on unimodal settings and binary classification for safeguarding, the authors propose OmniGuard, the first comprehensive family of omni-modal guardrails. OmniGuard is designed to perform safeguarding across all modalities with deliberate reasoning capabilities, enhancing robustness in diverse multimodal scenarios. To train OmniGuard, the authors created a large omni-modal safety dataset containing over 210,000 diverse samples that include both unimodal and cross-modal inputs. Each sample is annotated with structured safety labels and refined safety critiques provided by expert models through targeted knowledge distillation. The effectiveness and generalization of OmniGuard were validated extensively across 15 benchmarks, demonstrating strong performance in various multimodal safety tasks. Importantly, this work offers a unified framework capable of enforcing safety policies and mitigating risks across multiple modalities, facilitating the development of more robust and capable omni-modal safeguarding systems. This advancement represents a significant step toward safer and more reliable human-AI interaction in multi-sensory environments. <div>
arXiv:2512.02306v1 Announce Type: cross 
Abstract: Omni-modal Large Language Models (OLLMs) that process text, images, videos, and audio introduce new challenges for safety and value guardrails in human-AI interaction. Prior guardrail research largely targets unimodal settings and typically frames safeguarding as binary classification, which limits robustness across diverse modalities and tasks. To address this gap, we propose OmniGuard, the first family of omni-modal guardrails that performs safeguarding across all modalities with deliberate reasoning ability. To support the training of OMNIGUARD, we curate a large, comprehensive omni-modal safety dataset comprising over 210K diverse samples, with inputs that cover all modalities through both unimodal and cross-modal samples. Each sample is annotated with structured safety labels and carefully curated safety critiques from expert models through targeted distillation. Extensive experiments on 15 benchmarks show that OmniGuard achieves strong effectiveness and generalization across a wide range of multimodal safety scenarios. Importantly, OmniGuard provides a unified framework that enforces policies and mitigates risks in omni-modalities, paving the way toward building more robust and capable omnimodal safeguarding systems.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LeechHijack: Covert Computational Resource Exploitation in Intelligent Agent Systems</title>
<link>https://arxiv.org/abs/2512.02321</link>
<guid>https://arxiv.org/abs/2512.02321</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Model Context Protocol, implicit toxicity, LeechHijack, computational resource hijacking<br /><br />Summary:<br /><br />1. Large Language Model (LLM)-based agents have shown strong abilities in reasoning, planning, and utilizing external tools, fostering an ecosystem of community-built functionalities through the Model Context Protocol (MCP).<br /><br />2. The MCP framework’s openness and composability rely on an implicit trust assumption with third-party tool providers, which has been identified as a security vulnerability.<br /><br />3. This work defines a new class of attacks called implicit toxicity, where malicious actions occur entirely within the granted privileges, bypassing traditional permission boundaries.<br /><br />4. The authors propose LeechHijack, a two-stage latent embedded exploit that covertly hijacks an agent’s computational resources by embedding a backdoor within a benign-appearing MCP tool and activating it via predefined triggers.<br /><br />5. LeechHijack enables attackers to run unauthorized workloads through a stealth command-and-control channel, effectively parasitizing users’ computational budgets without raising quick suspicion.<br /><br />6. Experiments implementing LeechHijack on four major LLM families demonstrate a 77.25% average success rate with an 18.62% increase in resource overhead compared to baseline usage.<br /><br />7. The study emphasizes the critical need for enhanced computational provenance and resource attestation mechanisms to secure the MCP ecosystem against such latent exploits and implicit trust abuse. <div>
arXiv:2512.02321v1 Announce Type: cross 
Abstract: Large Language Model (LLM)-based agents have demonstrated remarkable capabilities in reasoning, planning, and tool usage. The recently proposed Model Context Protocol (MCP) has emerged as a unifying framework for integrating external tools into agent systems, enabling a thriving open ecosystem of community-built functionalities. However, the openness and composability that make MCP appealing also introduce a critical yet overlooked security assumption -- implicit trust in third-party tool providers. In this work, we identify and formalize a new class of attacks that exploit this trust boundary without violating explicit permissions. We term this new attack vector implicit toxicity, where malicious behaviors occur entirely within the allowed privilege scope. We propose LeechHijack, a Latent Embedded Exploit for Computation Hijacking, in which an adversarial MCP tool covertly expropriates the agent's computational resources for unauthorized workloads. LeechHijack operates through a two-stage mechanism: an implantation stage that embeds a benign-looking backdoor in a tool, and an exploitation stage where the backdoor activates upon predefined triggers to establish a command-and-control channel. Through this channel, the attacker injects additional tasks that the agent executes as if they were part of its normal workflow, effectively parasitizing the user's compute budget. We implement LeechHijack across four major LLM families. Experiments show that LeechHijack achieves an average success rate of 77.25%, with a resource overhead of 18.62% compared to the baseline. This study highlights the urgent need for computational provenance and resource attestation mechanisms to safeguard the emerging MCP ecosystem.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Process-Centric Analysis of Agentic Software Systems</title>
<link>https://arxiv.org/abs/2512.02393</link>
<guid>https://arxiv.org/abs/2512.02393</guid>
<content:encoded><![CDATA[
arXiv:2512.02393v1 Announce Type: cross 
Abstract: Agentic systems are modern software systems: they consist of orchestrated modules, expose interfaces, and are deployed in software pipelines. Unlike conventional programs, their execution (i.e., trajectories) is inherently stochastic and adaptive to the problem they are solving. Evaluation of such systems is often outcome-centric, judging their performance based on success or failure at the final step. This narrow focus overlooks detailed insights about such systems, failing to explain how agents reason, plan, act, or change their strategies over time. Inspired by the structured representation of conventional software systems as graphs, we introduce Graphectory to systematically encode the temporal and semantic relations in such software systems. Graphectory facilitates the design of process-centric metrics and analyses to assess the quality of agentic workflows independent of final success.
  Using Graphectory, we analyze 4000 trajectories of two dominant agentic programming workflows, namely SWE-agent and OpenHands, with a combination of four backbone Large Language Models (LLMs), attempting to resolve SWE-bench Verified issues. Our fully automated analyses reveal that: (1) agents using richer prompts or stronger LLMs exhibit more complex Graphectory, reflecting deeper exploration, broader context gathering, and more thorough validation before patch submission; (2) agents' problem-solving strategies vary with both problem difficulty and the underlying LLM -- for resolved issues, the strategies often follow coherent localization-patching-validation steps, while unresolved ones exhibit chaotic, repetitive, or backtracking behaviors; (3) even when successful, agentic programming systems often display inefficient processes, leading to unnecessarily prolonged trajectories.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The brain-AI convergence: Predictive and generative world models for general-purpose computation</title>
<link>https://arxiv.org/abs/2512.02419</link>
<guid>https://arxiv.org/abs/2512.02419</guid>
<content:encoded><![CDATA[
arXiv:2512.02419v1 Announce Type: cross 
Abstract: Recent advances in general-purpose AI systems with attention-based transformers offer a potential window into how the neocortex and cerebellum, despite their relatively uniform circuit architectures, give rise to diverse functions and, ultimately, to human intelligence. This Perspective provides a cross-domain comparison between the brain and AI that goes beyond the traditional focus on visual processing, adopting the emerging perspecive of world-model-based computation. Here, we identify shared computational mechanisms in the attention-based neocortex and the non-attentional cerebellum: both predict future world events from past inputs and construct internal world models through prediction-error learning. These predictive world models are repurposed for seemingly distinct functions--understanding in sensory processing and generation in motor processing-- enabling the brain to achieve multi-domain capabilities and human-like adaptive intelligence. Notably, attention-based AI has independently converged on a similar learning paradigm and world-model-based computation. We conclude that these shared mechanisms in both biological and artificial systems constitute a core computational foundation for realizing diverse functions including high-level intelligence, despite their relatively uniform circuit structures. Our theoretical insights bridge neuroscience and AI, advancing our understanding of the computational essence of intelligence.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning</title>
<link>https://arxiv.org/abs/2512.02425</link>
<guid>https://arxiv.org/abs/2512.02425</guid>
<content:encoded><![CDATA[
arXiv:2512.02425v1 Announce Type: cross 
Abstract: Recent advances in video large language models have demonstrated strong capabilities in understanding short clips. However, scaling them to hours- or days-long videos remains highly challenging due to limited context capacity and the loss of critical visual details during abstraction. Existing memory-augmented methods mitigate this by leveraging textual summaries of video segments, yet they heavily rely on text and fail to utilize visual evidence when reasoning over complex scenes. Moreover, retrieving from fixed temporal scales further limits their flexibility in capturing events that span variable durations. To address this, we introduce WorldMM, a novel multimodal memory agent that constructs and retrieves from multiple complementary memories, encompassing both textual and visual representations. WorldMM comprises three types of memory: episodic memory indexes factual events across multiple temporal scales, semantic memory continuously updates high-level conceptual knowledge, and visual memory preserves detailed information about scenes. During inference, an adaptive retrieval agent iteratively selects the most relevant memory source and leverages multiple temporal granularities based on the query, continuing until it determines that sufficient information has been gathered. WorldMM significantly outperforms existing baselines across five long video question-answering benchmarks, achieving an average 8.4% performance gain over previous state-of-the-art methods, showing its effectiveness on long video reasoning.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Refusals Fail: Unstable Safety Mechanisms in Long-Context LLM Agents</title>
<link>https://arxiv.org/abs/2512.02445</link>
<guid>https://arxiv.org/abs/2512.02445</guid>
<content:encoded><![CDATA[
arXiv:2512.02445v1 Announce Type: cross 
Abstract: Solving complex or long-horizon problems often requires large language models (LLMs) to use external tools and operate over a significantly longer context window. New LLMs enable longer context windows and support tool calling capabilities. Prior works have focused mainly on evaluation of LLMs on long-context prompts, leaving agentic setup relatively unexplored, both from capability and safety perspectives. Our work addresses this gap. We find that LLM agents could be sensitive to length, type, and placement of the context, exhibiting unexpected and inconsistent shifts in task performance and in refusals to execute harmful requests. Models with 1M-2M token context windows show severe degradation already at 100K tokens, with performance drops exceeding 50\% for both benign and harmful tasks. Refusal rates shift unpredictably: GPT-4.1-nano increases from $\sim$5\% to $\sim$40\% while Grok 4 Fast decreases from $\sim$80\% to $\sim$10\% at 200K tokens. Our work shows potential safety issues with agents operating on longer context and opens additional questions on the current metrics and paradigm for evaluating LLM agent safety on long multi-step tasks. In particular, our results on LLM agents reveal a notable divergence in both capability and safety performance compared to prior evaluations of LLMs on similar criteria.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>See, Think, Learn: A Self-Taught Multimodal Reasoner</title>
<link>https://arxiv.org/abs/2512.02456</link>
<guid>https://arxiv.org/abs/2512.02456</guid>
<content:encoded><![CDATA[
arXiv:2512.02456v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) have achieved remarkable progress in integrating visual perception with language understanding. However, effective multimodal reasoning requires both accurate perception and robust reasoning, and weakness in either limits the performance of VLMs. Prior efforts to enhance reasoning often depend on high-quality chain-of-thought (CoT) data, obtained via labor-intensive human annotations, costly proprietary models, or self-training methods that overlook perception. To address these limitations, we propose a simple yet effective self-training framework called See-Think-Learn (STL). At its core, STL introduces a structured reasoning template that encourages the model to see before thinking, first extracting visual attributes in textual form, then using them to guide reasoning. The framework jointly improves perception and reasoning by having the model generate and learn from its own structured rationales in a self-training loop. Furthermore, we augment the training data with negative rationales, i.e. explanations that justify why certain answer choices are incorrect, to enhance the model's ability to distinguish between correct and misleading responses. This fosters more discriminative and robust learning. Experiments across diverse domains show that STL consistently outperforms baselines trained directly only on answers or self-generated reasoning, while qualitative analysis confirms the high quality of its rationales. STL thus provides a cost-effective solution to enhance multimodal reasoning ability of VLMs.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guided Self-Evolving LLMs with Minimal Human Supervision</title>
<link>https://arxiv.org/abs/2512.02472</link>
<guid>https://arxiv.org/abs/2512.02472</guid>
<content:encoded><![CDATA[
arXiv:2512.02472v1 Announce Type: cross 
Abstract: AI self-evolution has long been envisioned as a path toward superintelligence, where models autonomously acquire, refine, and internalize knowledge from their own learning experiences. Yet in practice, unguided self-evolving systems often plateau quickly or even degrade as training progresses. These failures arise from issues such as concept drift, diversity collapse, and mis-evolution, as models reinforce their own biases and converge toward low-entropy behaviors. To enable models to self-evolve in a stable and controllable manner while minimizing reliance on human supervision, we introduce R-Few, a guided Self-Play Challenger-Solver framework that incorporates lightweight human oversight through in-context grounding and mixed training. At each iteration, the Challenger samples a small set of human-labeled examples to guide synthetic question generation, while the Solver jointly trains on human and synthetic examples under an online, difficulty-based curriculum. Across math and general reasoning benchmarks, R-Few achieves consistent and iterative improvements. For example, Qwen3-8B-Base improves by +3.0 points over R-Zero on math tasks and achieves performance on par with General-Reasoner, despite the latter being trained on 20 times more human data. Ablation studies confirm the complementary contributions of grounded challenger training and curriculum-based solver training, and further analysis shows that R-Few mitigates drift, yielding more stable and controllable co-evolutionary dynamics.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probabilistic energy profiler for statically typed JVM-based programming languages</title>
<link>https://arxiv.org/abs/2512.02738</link>
<guid>https://arxiv.org/abs/2512.02738</guid>
<content:encoded><![CDATA[
arXiv:2512.02738v1 Announce Type: cross 
Abstract: Energy consumption is a growing concern in several fields, from mobile devices to large data centers. Developers need detailed data on the energy consumption of their software to mitigate consumption issues. Previous approaches have a broader focus, such as on specific functions or programs, rather than source code statements. They primarily focus on estimating the CPU's energy consumption using point estimates, thereby disregarding other hardware effects and limiting their use for statistical reasoning and explainability. We developed a novel methodology to address the limitations of measuring only the CPU's consumption and using point estimates, focusing on predicting the energy usage of statically typed JVM-based programming languages, such as Java and Scala. We measure the energy consumption of Bytecode patterns, the translation from the programming language's source code statement to their Java Bytecode representation. With the energy measurements, we construct a statistical model using Bayesian statistics, which allows us to predict the energy consumption through statistical distributions and analyze individual factors. The model includes three factors we obtain statically from the code: data size, data type, operation, and one factor about the hardware platform the code executes on: device. To validate our methodology, we implemented it for Java and evaluated its energy predictions on unseen programs. We observe that all four factors are influential, notably that two devices of the same model may differ in energy consumption and that the operations and data types cause consumption differences. The experiments also show that the energy prediction of programs closely follows the program's real energy consumption, validating our approach. Our work presents a methodology for constructing an energy model that future work, such as verification tools, can use for their energy estimates.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.02835</link>
<guid>https://arxiv.org/abs/2512.02835</guid>
<content:encoded><![CDATA[
arXiv:2512.02835v1 Announce Type: cross 
Abstract: Reasoning-centric video object segmentation is an inherently complex task: the query often refers to dynamics, causality, and temporal interactions, rather than static appearances. Yet existing solutions generally collapse these factors into simplified reasoning with latent embeddings, rendering the reasoning chain opaque and essentially intractable. We therefore adopt an explicit decomposition perspective and introduce ReVSeg, which executes reasoning as sequential decisions in the native interface of pretrained vision language models (VLMs). Rather than folding all reasoning into a single-step prediction, ReVSeg executes three explicit operations -- semantics interpretation, temporal evidence selection, and spatial grounding -- aligning pretrained capabilities. We further employ reinforcement learning to optimize the multi-step reasoning chain, enabling the model to self-refine its decision quality from outcome-driven signals. Experimental results demonstrate that ReVSeg attains state-of-the-art performances on standard video object segmentation benchmarks and yields interpretable reasoning trajectories. Project page is available at https://clementine24.github.io/ReVSeg/ .
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OptPO: Optimal Rollout Allocation for Test-time Policy Optimization</title>
<link>https://arxiv.org/abs/2512.02882</link>
<guid>https://arxiv.org/abs/2512.02882</guid>
<content:encoded><![CDATA[
arXiv:2512.02882v1 Announce Type: cross 
Abstract: Test-time policy optimization enables large language models (LLMs) to adapt to distribution shifts by leveraging feedback from self-generated rollouts. However, existing methods rely on fixed-budget majority voting to estimate rewards, incurring substantial computational redundancy. We propose Optimal Rollout Allocation for Test-time Policy Optimization (OptPO), a principled framework that adaptively allocates inference budgets. By formulating the voting process as a Bayesian sequential probability ratio test, OptPO dynamically halts sampling once the posterior confidence in a consensus answer exceeds a specified threshold. Crucially, it utilizes the retained rollouts for on-policy updates, seamlessly integrating with algorithms like PPO or GRPO without requiring ground-truth labels. Across diverse reasoning benchmarks, OptPO significantly reduces rollout overhead compared to fixed-sample baselines while preserving or improving accuracy. By unifying statistically optimal stopping with test-time learning, OptPO offers a computationally efficient paradigm for test-time adaptation. The source code will be open upon acceptance at https://open-upon-acceptance.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Martingale Score: An Unsupervised Metric for Bayesian Rationality in LLM Reasoning</title>
<link>https://arxiv.org/abs/2512.02914</link>
<guid>https://arxiv.org/abs/2512.02914</guid>
<content:encoded><![CDATA[
arXiv:2512.02914v1 Announce Type: cross 
Abstract: Recent advances in reasoning techniques have substantially improved the performance of large language models (LLMs), raising expectations for their ability to provide accurate, truthful, and reliable information. However, emerging evidence suggests that iterative reasoning may foster belief entrenchment and confirmation bias, rather than enhancing truth-seeking behavior. In this study, we propose a systematic evaluation framework for belief entrenchment in LLM reasoning by leveraging the Martingale property from Bayesian statistics. This property implies that, under rational belief updating, the expected value of future beliefs should remain equal to the current belief, i.e., belief updates are unpredictable from the current belief. We propose the unsupervised, regression-based Martingale Score to measure violations of this property, which signal deviation from the Bayesian ability of updating on new evidence. In open-ended problem domains including event forecasting, value-laden questions, and academic paper review, we find such violations to be widespread across models and setups, where the current belief positively predicts future belief updates, a phenomenon which we term belief entrenchment. We identify the models, reasoning techniques, and domains more prone to belief entrenchment. Finally, we validate the Martingale Score by showing that it predicts ground-truth accuracy on problem domains where ground truth labels are available. This indicates that, while designed as an unsupervised metric that operates even in domains without access to ground truth, the Martingale Score is a useful proxy of the truth-seeking ability of a reasoning process.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contextual Image Attack: How Visual Context Exposes Multimodal Safety Vulnerabilities</title>
<link>https://arxiv.org/abs/2512.02973</link>
<guid>https://arxiv.org/abs/2512.02973</guid>
<content:encoded><![CDATA[
arXiv:2512.02973v1 Announce Type: cross 
Abstract: While Multimodal Large Language Models (MLLMs) show remarkable capabilities, their safety alignments are susceptible to jailbreak attacks. Existing attack methods typically focus on text-image interplay, treating the visual modality as a secondary prompt. This approach underutilizes the unique potential of images to carry complex, contextual information. To address this gap, we propose a new image-centric attack method, Contextual Image Attack (CIA), which employs a multi-agent system to subtly embeds harmful queries into seemingly benign visual contexts using four distinct visualization strategies. To further enhance the attack's efficacy, the system incorporate contextual element enhancement and automatic toxicity obfuscation techniques. Experimental results on the MMSafetyBench-tiny dataset show that CIA achieves high toxicity scores of 4.73 and 4.83 against the GPT-4o and Qwen2.5-VL-72B models, respectively, with Attack Success Rates (ASR) reaching 86.31\% and 91.07\%. Our method significantly outperforms prior work, demonstrating that the visual modality itself is a potent vector for jailbreaking advanced MLLMs.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LORE: A Large Generative Model for Search Relevance</title>
<link>https://arxiv.org/abs/2512.03025</link>
<guid>https://arxiv.org/abs/2512.03025</guid>
<content:encoded><![CDATA[
arXiv:2512.03025v1 Announce Type: cross 
Abstract: Achievement. We introduce LORE, a systematic framework for Large Generative Model-based relevance in e-commerce search. Deployed and iterated over three years, LORE achieves a cumulative +27\% improvement in online GoodRate metrics. This report shares the valuable experience gained throughout its development lifecycle, spanning data, features, training, evaluation, and deployment. Insight. While existing works apply Chain-of-Thought (CoT) to enhance relevance, they often hit a performance ceiling. We argue this stems from treating relevance as a monolithic task, lacking principled deconstruction. Our key insight is that relevance comprises distinct capabilities: knowledge and reasoning, multi-modal matching, and rule adherence. We contend that a qualitative-driven decomposition is essential for breaking through current performance bottlenecks. Contributions. LORE provides a complete blueprint for the LLM relevance lifecycle. Key contributions include: (1) A two-stage training paradigm combining progressive CoT synthesis via SFT with human preference alignment via RL. (2) A comprehensive benchmark, RAIR, designed to evaluate these core capabilities. (3) A query frequency-stratified deployment strategy that efficiently transfers offline LLM capabilities to the online system. LORE serves as both a practical solution and a methodological reference for other vertical domains.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On SkipGram Word Embedding Models with Negative Sampling: Unified Framework and Impact of Noise Distributions</title>
<link>https://arxiv.org/abs/2009.04413</link>
<guid>https://arxiv.org/abs/2009.04413</guid>
<content:encoded><![CDATA[
arXiv:2009.04413v2 Announce Type: replace 
Abstract: SkipGram word embedding models with negative sampling, or SGN in short, is an elegant family of word embedding models. In this paper, we formulate a framework for word embedding, referred to as Word-Context Classification (WCC), that generalizes SGN to a wide family of models. The framework, which uses some ``noise examples'', is justified through theoretical analysis. The impact of noise distribution on the learning of the WCC embedding models is studied experimentally, suggesting that the best noise distribution is, in fact, the data distribution, in terms of both the embedding performance and the speed of convergence during training. Along our way, we discover several novel embedding models that outperform existing WCC models.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unifying Linear-Time Attention via Latent Probabilistic Modelling</title>
<link>https://arxiv.org/abs/2402.17512</link>
<guid>https://arxiv.org/abs/2402.17512</guid>
<content:encoded><![CDATA[
arXiv:2402.17512v5 Announce Type: replace 
Abstract: Transformers have achieved state-of-the-art results across a range of domains, but their quadratic attention mechanism poses significant challenges for long-sequence modelling. Recent efforts to design linear-time attention mechanisms have yielded more scalable alternatives, yet often at the cost of performance, particularly on discrete data such as language. In this work, we revisit linear attention through the lens of probabilistic graphical models. We first show that standard linear attention can be interpreted as an undirected latent variable model, revealing a key limitation: the absence of directionality. To address this, we propose a novel directed parameterisation of linear attention that introduces an asymmetric structure, enabling an interpretation aligned with the causal and sequential nature of language. Our formulation integrates global latent-variable attention with local standard attention in a fully probabilistic framework. Additionally, we introduce a recurrent parameterisation of queries and keys that avoids reliance on relative positional encodings, often incompatible with linear attention. Experiments on language modelling benchmarks demonstrate that our model achieves competitive performance with standard attention and outperforms existing linear attention variants.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniBench: Towards The Future of Universal Omni-Language Models</title>
<link>https://arxiv.org/abs/2409.15272</link>
<guid>https://arxiv.org/abs/2409.15272</guid>
<content:encoded><![CDATA[
arXiv:2409.15272v5 Announce Type: replace 
Abstract: Recent advancements in multimodal large language models (MLLMs) have focused on integrating multiple modalities, yet their ability to simultaneously process and reason across different inputs remains underexplored. We introduce OmniBench, a novel benchmark designed to evaluate models' ability to recognize, interpret, and reason across visual, acoustic, and textual inputs simultaneously. We define language models capable of such tri-modal processing as omni-language models (OLMs). OmniBench features high-quality human annotations that require integrated understanding across all modalities. Our evaluation reveals that: i) open-source OLMs show significant limitations in instruction-following and reasoning in tri-modal contexts; and ii) most baseline models perform poorly (around 50% accuracy) even with textual alternatives to image/audio inputs. To address these limitations, we develop OmniInstruct, an 96K-sample instruction tuning dataset for training OLMs. We advocate for developing more robust tri-modal integration techniques and training strategies to enhance OLM performance. Codes and data could be found at our repo (https://github.com/multimodal-art-projection/OmniBench).
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hidden in Plain Text: Emergence &amp; Mitigation of Steganographic Collusion in LLMs</title>
<link>https://arxiv.org/abs/2410.03768</link>
<guid>https://arxiv.org/abs/2410.03768</guid>
<content:encoded><![CDATA[
arXiv:2410.03768v2 Announce Type: replace 
Abstract: The rapid proliferation of frontier model agents promises significant societal advances but also raises concerns about systemic risks arising from unsafe interactions. Collusion to the disadvantage of others has been identified as a central form of undesirable agent cooperation. The use of information hiding (steganography) in agent communications could render such collusion practically undetectable. This underscores the need for investigations into the possibility of such behaviours emerging and the robustness corresponding countermeasures. To investigate this problem we design two approaches -- a gradient-based reinforcement learning (GBRL) method and an in-context reinforcement learning (ICRL) method -- for reliably eliciting sophisticated LLM-generated linguistic text steganography.
  We demonstrate, for the first time, that unintended steganographic collusion in LLMs can arise due to mispecified reward incentives during training. Additionally, we find that standard mitigations -- both passive oversight of model outputs and active mitigation through communication paraphrasing -- are not fully effective at preventing this steganographic communication. Our findings imply that (i) emergence of steganographic collusion is a plausible concern that should be monitored and researched, and (ii) preventing emergence may require innovation in mitigation techniques.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Word Vectors to Multimodal Embeddings: Techniques, Applications, and Future Directions For Large Language Models</title>
<link>https://arxiv.org/abs/2411.05036</link>
<guid>https://arxiv.org/abs/2411.05036</guid>
<content:encoded><![CDATA[
arXiv:2411.05036v3 Announce Type: replace 
Abstract: Word embeddings and language models have transformed natural language processing (NLP) by facilitating the representation of linguistic elements in continuous vector spaces. This review visits foundational concepts such as the distributional hypothesis and contextual similarity, tracing the evolution from sparse representations like one-hot encoding to dense embeddings including Word2Vec, GloVe, and fastText. We examine both static and contextualized embeddings, underscoring advancements in models such as ELMo, BERT, and GPT and their adaptations for cross-lingual and personalized applications. The discussion extends to sentence and document embeddings, covering aggregation methods and generative topic models, along with the application of embeddings in multimodal domains, including vision, robotics, and cognitive science. Advanced topics such as model compression, interpretability, numerical encoding, and bias mitigation are analyzed, addressing both technical challenges and ethical implications. Additionally, we identify future research directions, emphasizing the need for scalable training techniques, enhanced interpretability, and robust grounding in non-textual modalities. By synthesizing current methodologies and emerging trends, this survey offers researchers and practitioners an in-depth resource to push the boundaries of embedding-based language models.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Facilitating Long Context Understanding via Supervised Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2502.13127</link>
<guid>https://arxiv.org/abs/2502.13127</guid>
<content:encoded><![CDATA[
arXiv:2502.13127v2 Announce Type: replace 
Abstract: Recent advances in Large Language Models (LLMs) have enabled them to process increasingly longer sequences, ranging from 2K to 2M tokens and even beyond. However, simply extending the input sequence length does not necessarily lead to effective long-context understanding. In this study, we integrate Chain-of-Thought (CoT) reasoning into LLMs in a supervised manner to facilitate effective long-context understanding. To achieve this, we introduce LongFinanceQA, a synthetic dataset in the financial domain designed to improve long-context reasoning. Unlike existing long-context synthetic data, LongFinanceQA includes intermediate CoT reasoning before the final conclusion, which encourages LLMs to perform explicit reasoning, improving accuracy and interpretability in long-context understanding. To generate synthetic CoT reasoning, we propose Property-based Agentic Inference (PAI), an agentic framework that simulates human-like reasoning steps, including property extraction, retrieval, and summarization. We evaluate PAI's reasoning capabilities by assessing GPT-4o-mini w/ PAI on the Loong benchmark, outperforming standard GPT-4o-mini by 20.0%. Furthermore, we fine-tune LLaMA-3.1-8B-Instruct on LongFinanceQA, achieving a 28.0% gain on Loong's financial subset.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SkyLadder: Better and Faster Pretraining via Context Window Scheduling</title>
<link>https://arxiv.org/abs/2503.15450</link>
<guid>https://arxiv.org/abs/2503.15450</guid>
<content:encoded><![CDATA[
arXiv:2503.15450v2 Announce Type: replace 
Abstract: Recent advancements in LLM pretraining have featured ever-expanding context windows to process longer sequences. However, our pilot study reveals that models pretrained with shorter context windows consistently outperform their long-context counterparts under a fixed token budget. This finding motivates us to explore an optimal context window scheduling strategy to better balance long-context capability with pretraining efficiency. To this end, we propose SkyLadder, a simple yet effective approach that implements a short-to-long context window transition. SkyLadder preserves strong standard benchmark performance, while matching or exceeding baseline results on long context tasks. Through extensive experiments, we pre-train 1B-parameter models (up to 32K context) and 3B-parameter models (8K context) on 100B tokens, demonstrating that SkyLadder yields consistent gains of up to 3.7% on common benchmarks, while achieving up to 22% faster training speeds compared to baselines. The code is at https://github.com/sail-sg/SkyLadder.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Temporal Question-Answering Capabilities of Large Language Models Over Anonymized Data</title>
<link>https://arxiv.org/abs/2504.07646</link>
<guid>https://arxiv.org/abs/2504.07646</guid>
<content:encoded><![CDATA[
arXiv:2504.07646v2 Announce Type: replace 
Abstract: The applicability of Large Language Models (LLMs) in temporal reasoning tasks over data that is not present during training is still a field that remains to be explored. In this paper we work on this topic, focusing on structured and semi-structured anonymized data. We not only develop a direct LLM pipeline, but also compare various methodologies and conduct an in-depth analysis. We identified and examined seventeen common temporal reasoning tasks in natural language, focusing on their algorithmic components. To assess LLM performance, we created the \textit{Reasoning and Answering Temporal Ability} dataset (RATA), featuring semi-structured anonymized data to ensure reliance on reasoning rather than on prior knowledge. We compared several methodologies, involving SoTA techniques such as Tree-of-Thought, self-reflexion and code execution, tuned specifically for this scenario. Our results suggest that achieving scalable and reliable solutions requires more than just standalone LLMs, highlighting the need for integrated approaches.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computational Fact-Checking of Online Discourse: Scoring scientific accuracy in climate change related news articles</title>
<link>https://arxiv.org/abs/2505.07409</link>
<guid>https://arxiv.org/abs/2505.07409</guid>
<content:encoded><![CDATA[
arXiv:2505.07409v2 Announce Type: replace 
Abstract: Democratic societies need reliable information. Misinformation in popular media, such as news articles or videos, threatens to impair civic discourse. Citizens are, unfortunately, not equipped to verify the flood of content consumed daily at increasing rates. This work aims to quantify the scientific accuracy of online media semi-automatically. We investigate the state of the art of climate-related ground truth knowledge representation. By semantifying media content of unknown veracity, their statements can be compared against these ground truth knowledge graphs. We implemented a workflow using LLM-based statement extraction and knowledge graph analysis. Our implementation can streamline content processing towards state-of-the-art knowledge representation and veracity quantification. Developed and evaluated with the help of 27 experts and detailed interviews with 10, the tool evidently provides a beneficial veracity indication. These findings are supported by 43 anonymous participants from a parallel user survey. This initial step, however, is unable to annotate public media at the required granularity and scale. Additionally, the identified state of climate change knowledge graphs is vastly insufficient to support this neurosymbolic fact-checking approach. Further work towards a FAIR (Findable, Accessible, Interoperable, Reusable) ground truth and complementary metrics is required to support civic discourse scientifically.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Induction Head Toxicity Mechanistically Explains Repetition Curse in Large Language Models</title>
<link>https://arxiv.org/abs/2505.13514</link>
<guid>https://arxiv.org/abs/2505.13514</guid>
<content:encoded><![CDATA[
arXiv:2505.13514v2 Announce Type: replace 
Abstract: Repetition curse is a phenomenon where Large Language Models (LLMs) generate repetitive sequences of tokens or cyclic sequences. While the repetition curse has been widely observed, its underlying mechanisms remain poorly understood. In this work, we investigate the role of induction heads--a specific type of attention head known for their ability to perform in-context learning--in driving this repetitive behavior. Specifically, we focus on the "toxicity" of induction heads, which we define as their tendency to dominate the model's output logits during repetition, effectively excluding other attention heads from contributing to the generation process. Our findings have important implications for the design and training of LLMs. By identifying induction heads as a key driver of the repetition curse, we provide a mechanistic explanation for this phenomenon and suggest potential avenues for mitigation. We also propose a technique with attention head regularization that could be employed to reduce the dominance of induction heads during generation, thereby promoting more diverse and coherent outputs.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAS-ZERO: Designing Multi-Agent Systems with Zero Supervision</title>
<link>https://arxiv.org/abs/2505.14996</link>
<guid>https://arxiv.org/abs/2505.14996</guid>
<content:encoded><![CDATA[
arXiv:2505.14996v3 Announce Type: replace 
Abstract: Multi-agent systems (MAS) leveraging the impressive capabilities of Large Language Models (LLMs) hold significant potential for tackling complex tasks. However, most current MAS depend on manually designed agent roles and communication protocols. These manual designs often fail to align with the underlying LLMs' strengths and struggle to adapt to novel tasks. Recent automatic MAS approaches attempt to mitigate these limitations but typically necessitate a validation set for tuning and yield static MAS designs lacking adaptability during inference, while also removing the flexibility to reduce to simpler systems. We introduce MAS-ZERO, the first self-evolved, inference-time framework for automatic MAS design. MAS-ZERO employs meta-level design to iteratively design, critique, and refine MAS configurations tailored to each problem instance, without requiring a validation set. Critically, it enables dynamic problem decomposition and agent composition through meta-feedback on solvability and completeness, and reduction to simpler systems when appropriate. Experiments across reasoning (math and graduate-level QA), coding, and agentic (search-based) benchmarks, using both closed-source and open-source LLM backbones of varying sizes, demonstrate that MAS-ZERO outperforms strong manual and automatic MAS baselines. It achieves substantial average accuracy improvements of up to 16.69% on reasoning, 16.66% on coding, and 5.45% on agentic tasks, while maintaining cost efficiency.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ELSPR: Evaluator LLM Training Data Self-Purification on Non-Transitive Preferences via Tournament Graph Reconstruction</title>
<link>https://arxiv.org/abs/2505.17691</link>
<guid>https://arxiv.org/abs/2505.17691</guid>
<content:encoded><![CDATA[
arXiv:2505.17691v3 Announce Type: replace 
Abstract: Pairwise evaluation of large language models (LLMs) has become the dominant paradigm for benchmarking open-ended tasks, yet non-transitive preferences, where evaluators prefer A over B, B over C, but C over A, fundamentally undermine ranking reliability. We show that this critical issue stems largely from low-quality data that contains inherently ambiguous preference pairs. To address this challenge, we propose ELSPR, a principled graph-theoretic framework that models pairwise preferences as tournament graphs and systematically identifies problematic training data. ELSPR quantifies non-transitivity through strongly connected components (SCCs) analysis and measures overall preference clarity using a novel normalized directed graph structural entropy metric. Our filtering methodology selectively removes preference data that induce non-transitivity while preserving transitive preferences. Extensive experiments on the AlpacaEval benchmark demonstrate that models fine-tuned on ELSPR-filtered data achieve substantial improvements: a 13.8% reduction in non-transitivity, a 0.088 decrease in structural entropy, and significantly enhanced discriminative power in real-world evaluation systems. Human validation confirms that discarded data exhibit dramatically lower inter-annotator agreement (34.4% vs. 52.6%) and model-human consistency (51.2% vs. 80.6%) compared to cleaned data. These findings establish ELSPR as an effective data self-purification approach for developing more robust, consistent, and human-aligned LLM evaluation systems.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multilingual Pretraining for Pixel Language Models</title>
<link>https://arxiv.org/abs/2505.21265</link>
<guid>https://arxiv.org/abs/2505.21265</guid>
<content:encoded><![CDATA[
arXiv:2505.21265v2 Announce Type: replace 
Abstract: Pixel language models operate directly on images of rendered text, eliminating the need for a fixed vocabulary. While these models have demonstrated strong capabilities for downstream cross-lingual transfer, multilingual pretraining remains underexplored. We introduce PIXEL-M4, a model pretrained on four visually and linguistically diverse languages: English, Hindi, Ukrainian, and Simplified Chinese. Multilingual evaluations on semantic and syntactic tasks show that PIXEL-M4 outperforms an English-only counterpart on non-Latin scripts. Word-level probing analyses confirm that PIXEL-M4 captures rich linguistic features, even in languages not seen during pretraining. Furthermore, an analysis of its hidden representations shows that multilingual pretraining yields a semantic embedding space closely aligned across the languages used for pretraining. This work demonstrates that multilingual pretraining substantially enhances the capability of pixel language models to effectively support a diverse set of languages.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Eka-Eval: An Evaluation Framework for Low-Resource Multilingual Large Language Models</title>
<link>https://arxiv.org/abs/2507.01853</link>
<guid>https://arxiv.org/abs/2507.01853</guid>
<content:encoded><![CDATA[
arXiv:2507.01853v4 Announce Type: replace 
Abstract: The rapid evolution of Large Language Models' has underscored the need for evaluation frameworks that are globally applicable, flexible, and modular, and that support a wide range of tasks, model types, and linguistic settings. We introduce EKA-EVAL, a unified, end- to-end framework that combines a zero-code web interface and an interactive CLI to ensure broad accessibility. It integrates 50+ multilingual benchmarks across nine evaluation categories, supports local and proprietary models, and provides 11 core capabilities through a modular, plug-and-play architecture. Designed for scalable, multilingual evaluation with support for low-resource multilingual languages, EKA-EVAL is, to the best of our knowledge, the first suite to offer comprehensive coverage in a single platform. Comparisons against five existing baselines indicate improvements of at least 2x better on key usability measures, with the highest user satisfaction, faster setup times, and consistent benchmark reproducibility. The framework is open-source and publicly available at https://github.com/lingo-iitgn/eka-eval.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>XISM: an eXploratory and Interactive Graph Tool to Visualize and Evaluate Semantic Map Models</title>
<link>https://arxiv.org/abs/2507.04070</link>
<guid>https://arxiv.org/abs/2507.04070</guid>
<content:encoded><![CDATA[
arXiv:2507.04070v2 Announce Type: replace 
Abstract: Semantic map models visualize systematic relations among semantic functions through graph structures and are widely used in linguistic typology. However, existing construction methods either depend on labor-intensive expert reasoning or on fully automated systems lacking expert involvement, creating a tension between scalability and interpretability. We introduce \textbf{XISM}, an interactive system that combines data-driven inference with expert knowledge. XISM generates candidate maps via a top-down procedure and allows users to iteratively refine edges in a visual interface, with real-time metric feedback. Experiments in three semantic domains and expert interviews show that XISM improves linguistic decision transparency and controllability in semantic-map construction while maintaining computational efficiency. XISM provides a collaborative approach for scalable and interpretable semantic-map building. The system\footnote{https://app.xism2025.xin/} , source code\footnote{https://github.com/hank317/XISM} , and demonstration video\footnote{https://youtu.be/m5laLhGn6Ys} are publicly available.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMEval-3: A Large-Scale Longitudinal Study on Robust and Fair Evaluation of Large Language Models</title>
<link>https://arxiv.org/abs/2508.05452</link>
<guid>https://arxiv.org/abs/2508.05452</guid>
<content:encoded><![CDATA[
arXiv:2508.05452v3 Announce Type: replace 
Abstract: Existing evaluation of Large Language Models (LLMs) on static benchmarks is vulnerable to data contamination and leaderboard overfitting, critical issues that obscure true model capabilities. To address this, we introduce LLMEval-3, a framework for dynamic evaluation of LLMs. LLMEval-3 is built on a proprietary bank of 220k graduate-level questions, from which it dynamically samples unseen test sets for each evaluation run. Its automated pipeline ensures integrity via contamination-resistant data curation, a novel anti-cheating architecture, and a calibrated LLM-as-a-judge process achieving 90% agreement with human experts, complemented by a relative ranking system for fair comparison. An 20-month longitudinal study of nearly 50 leading models reveals a performance ceiling on knowledge memorization and exposes data contamination vulnerabilities undetectable by static benchmarks. The framework demonstrates exceptional robustness in ranking stability and consistency, providing strong empirical validation for the dynamic evaluation paradigm. LLMEval-3 offers a robust and credible methodology for assessing the true capabilities of LLMs beyond leaderboard scores, promoting the development of more trustworthy evaluation standards.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-as-a-Supervisor: Mistaken Therapeutic Behaviors Trigger Targeted Supervisory Feedback</title>
<link>https://arxiv.org/abs/2508.09042</link>
<guid>https://arxiv.org/abs/2508.09042</guid>
<content:encoded><![CDATA[
arXiv:2508.09042v2 Announce Type: replace 
Abstract: Although large language models (LLMs) hold significant promise in psychotherapy, their direct application in patient-facing scenarios raises ethical and safety concerns. Therefore, this work shifts towards developing an LLM as a supervisor to train real therapists. In addition to the privacy of clinical therapist training data, a fundamental contradiction complicates the training of therapeutic behaviors: clear feedback standards are necessary to ensure a controlled training system, yet there is no absolute "gold standard" for appropriate therapeutic behaviors in practice. In contrast, many common therapeutic mistakes are universal and identifiable, making them effective triggers for targeted feedback that can serve as clearer evidence. Motivated by this, we create a novel therapist-training paradigm: (1) guidelines for mistaken behaviors and targeted correction strategies are first established as standards; (2) a human-in-the-loop dialogue-feedback dataset is then constructed, where a mistake-prone agent intentionally makes standard mistakes during interviews naturally, and a supervisor agent locates and identifies mistakes and provides targeted feedback; (3) after fine-tuning on this dataset, the final supervisor model is provided for real therapist training. The detailed experimental results of automated, human and downstream assessments demonstrate that models fine-tuned on our dataset MATE, can provide high-quality feedback according to the clinical guideline, showing significant potential for the therapist training scenario.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WebMall - A Multi-Shop Benchmark for Evaluating Web Agents [Technical Report]</title>
<link>https://arxiv.org/abs/2508.13024</link>
<guid>https://arxiv.org/abs/2508.13024</guid>
<content:encoded><![CDATA[
arXiv:2508.13024v2 Announce Type: replace 
Abstract: LLM-based web agents have the potential to automate long-running web tasks, such as searching for products in multiple e-shops and subsequently ordering the cheapest products that meet the users needs. Benchmarks for evaluating web agents either require agents to perform tasks online using the live Web or offline using simulated environments, which allow for the exact reproduction of the experimental setup. While DeepShop provides an online benchmark that requires agents to perform challenging shopping tasks, existing offline benchmarks such as WebShop, WebArena, or Mind2Web cover only comparatively simple e-commerce tasks that need to be performed against a single shop containing product data from a single source. What is missing is an e-commerce benchmark that simulates multiple shops containing heterogeneous product data and requires agents to perform complex tasks. We fill this gap by introducing WebMall, the first offline multi-shop benchmark for evaluating web agents on challenging comparison shopping tasks. WebMall consists of four simulated shops populated with product data extracted from the Common Crawl. The WebMall tasks range from specific product searches and price comparisons to advanced queries for complementary or substitute products, as well as checkout processes. We validate WebMall using eight agents that differ in observation space, availability of short-term memory, and the employed LLM. The validation highlights the difficulty of the benchmark, with even the best-performing agents achieving task completion rates below 55% in the task categories cheapest product search and vague product search.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Just-in-time and distributed task representations in language models</title>
<link>https://arxiv.org/abs/2509.04466</link>
<guid>https://arxiv.org/abs/2509.04466</guid>
<content:encoded><![CDATA[
arXiv:2509.04466v3 Announce Type: replace 
Abstract: Many of language models' impressive capabilities originate from their in-context learning: based on instructions or examples, they can infer and perform new tasks without weight updates. In this work, we investigate when representations for new tasks are formed in language models, and how these representations change over the course of context. We study two different task representations: those that are ''transferrable'' -- vector representations that can transfer task contexts to another model instance, even without the full prompt -- and simpler representations of high-level task categories. We show that transferrable task representations evolve in non-monotonic and sporadic ways, while task identity representations persist throughout the context. Specifically, transferrable task representations exhibit a two-fold locality. They successfully condense evidence when more examples are provided in the context. But this evidence accrual process exhibits strong temporal locality along the sequence dimension, coming online only at certain tokens -- despite task identity being reliably decodable throughout the context. In some cases, transferrable task representations also show semantic locality, capturing a small task ''scope'' such as an independent subtask. Language models thus represent new tasks on the fly through both an inert, sustained sensitivity to the task and an active, just-in-time representation to support inference.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evolution and compression in LLMs: On the emergence of human-aligned categorization</title>
<link>https://arxiv.org/abs/2509.08093</link>
<guid>https://arxiv.org/abs/2509.08093</guid>
<content:encoded><![CDATA[
arXiv:2509.08093v3 Announce Type: replace 
Abstract: Converging evidence suggests that human systems of semantic categories achieve near-optimal compression via the Information Bottleneck (IB) complexity-accuracy tradeoff. Large language models (LLMs) are not trained for this objective, which raises the question: are LLMs capable of evolving efficient human-aligned semantic systems? To address this question, we focus on color categorization -- a key testbed of cognitive theories of categorization with uniquely rich human data -- and replicate with LLMs two influential human studies. First, we conduct an English color-naming study, showing that LLMs vary widely in their complexity and English-alignment, with larger instruction-tuned models achieving better alignment and IB-efficiency. Second, to test whether these LLMs simply mimic patterns in their training data or actually exhibit a human-like inductive bias toward IB-efficiency, we simulate cultural evolution of pseudo color-naming systems in LLMs via a method we refer to as Iterated in-Context Language Learning (IICLL). We find that akin to humans, LLMs iteratively restructure initially random systems towards greater IB-efficiency. However, only a model with strongest in-context capabilities (Gemini 2.0) is able to recapitulate the wide range of near-optimal IB-tradeoffs observed in humans, while other state-of-the-art models converge to low-complexity solutions. These findings demonstrate how human-aligned semantic categories can emerge in LLMs via the same fundamental principle that underlies semantic efficiency in humans.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometric Uncertainty for Detecting and Correcting Hallucinations in LLMs</title>
<link>https://arxiv.org/abs/2509.13813</link>
<guid>https://arxiv.org/abs/2509.13813</guid>
<content:encoded><![CDATA[
arXiv:2509.13813v2 Announce Type: replace 
Abstract: Large language models demonstrate impressive results across diverse tasks but are still known to hallucinate, generating linguistically plausible but incorrect answers to questions. Uncertainty quantification has been proposed as a strategy for hallucination detection, requiring estimates for both global uncertainty (attributed to a batch of responses) and local uncertainty (attributed to individual responses). While recent black-box approaches have shown some success, they often rely on disjoint heuristics or graph-theoretic approximations that lack a unified geometric interpretation. We introduce a geometric framework to address this, based on archetypal analysis of batches of responses sampled with only black-box model access. At the global level, we propose Geometric Volume, which measures the convex hull volume of archetypes derived from response embeddings. At the local level, we propose Geometric Suspicion, which leverages the spatial relationship between responses and these archetypes to rank reliability, enabling hallucination reduction through preferential response selection. Unlike prior methods that rely on discrete pairwise comparisons, our approach provides continuous semantic boundary points which have utility for attributing reliability to individual responses. Experiments show that our framework performs comparably to or better than prior methods on short form question-answering datasets, and achieves superior results on medical datasets where hallucinations carry particularly critical risks. We also provide theoretical justification by proving a link between convex hull volume and entropy.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Apertus: Democratizing Open and Compliant LLMs for Global Language Environments</title>
<link>https://arxiv.org/abs/2509.14233</link>
<guid>https://arxiv.org/abs/2509.14233</guid>
<content:encoded><![CDATA[
arXiv:2509.14233v2 Announce Type: replace 
Abstract: We present Apertus, a fully open suite of large language models (LLMs) designed to address two systemic shortcomings in today's open model ecosystem: data compliance and multilingual representation. Unlike many prior models that release weights without reproducible data pipelines or regard for content-owner rights, Apertus models are pretrained exclusively on openly available data, retroactively respecting `robots.txt` exclusions and filtering for non-permissive, toxic, and personally identifiable content. To mitigate risks of memorization, we adopt the Goldfish objective during pretraining, strongly suppressing verbatim recall of data while retaining downstream task performance. The Apertus models also expand multilingual coverage, training on 15T tokens from over 1800 languages, with ~40% of pretraining data allocated to non-English content. Released at 8B and 70B scales, Apertus approaches state-of-the-art results among fully open models on multilingual benchmarks, rivalling or surpassing open-weight counterparts. Beyond model weights, we release all scientific artifacts from our development cycle with a permissive license, including data preparation scripts, checkpoints, evaluation suites, and training code, enabling transparent audit and extension.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ParlAI Vote: A Web Platform for Analyzing Gender and Political Bias in Large Language Models</title>
<link>https://arxiv.org/abs/2509.16264</link>
<guid>https://arxiv.org/abs/2509.16264</guid>
<content:encoded><![CDATA[
arXiv:2509.16264v3 Announce Type: replace 
Abstract: We present ParlAI Vote, an interactive web platform for exploring European Parliament debates and votes, and for testing LLMs on vote prediction and bias analysis. This web system connects debate topics, speeches, and roll-call outcomes, and includes rich demographic data such as gender, age, country, and political group. Users can browse debates, inspect linked speeches, compare real voting outcomes with predictions from frontier LLMs, and view error breakdowns by demographic group. Visualizing the EuroParlVote benchmark and its core tasks of gender classification and vote prediction, ParlAI Vote highlights systematic performance bias in state-of-the-art LLMs. It unifies data, models, and visual analytics in a single interface, lowering the barrier for reproducing findings, auditing behavior, and running counterfactual scenarios. This web platform also shows model reasoning, helping users see why errors occur and what cues the models rely on. It supports research, education, and public engagement with legislative decision-making, while making clear both the strengths and the limitations of current LLMs in political analysis.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Look Before you Leap: Estimating LLM Benchmark Scores from Descriptions</title>
<link>https://arxiv.org/abs/2509.20645</link>
<guid>https://arxiv.org/abs/2509.20645</guid>
<content:encoded><![CDATA[
arXiv:2509.20645v2 Announce Type: replace 
Abstract: Progress in large language models is constrained by an evaluation bottleneck: build a benchmark, run models, then iterate. We ask a question: can we forecast outcomes before running any experiments to inform earlier study design? For example, a team building an AI assistant for a certain task can estimate whether expected performance is around 50 or closer to 80, evidence that supports whether to proceed to a pilot study, how to scope it, and how to allocate resources. We study text-only performance forecasting, where a model predicts a score from a redacted task description and intended configuration, with no access to dataset instances. To support systematic study, we curate PRECOG, a corpus of redacted description-performance pairs spanning diverse tasks, domains, and metrics. We scrape task and configuration descriptions from arXiv, yielding 2,290 instances covering 1,519 papers, and construct a leakage free test split using papers published after the knowledge cutoff of the evaluated models. Experiments show the task is challenging but feasible: reasoning models achieve moderate prediction performance with well calibrated uncertainty, reaching mean absolute error as low as 9.9 at high confidence thresholds. We further test a zero-leakage setting, forecasting on newly released datasets or experiments before their papers are indexed, where GPT5 with built in web search still attains nontrivial prediction accuracy. Overall, our corpus and analyses offer an initial step toward open ended anticipatory evaluation, supporting difficulty estimation and smarter experiment prioritization.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Between Help and Harm: An Evaluation of Mental Health Crisis Handling by LLMs</title>
<link>https://arxiv.org/abs/2509.24857</link>
<guid>https://arxiv.org/abs/2509.24857</guid>
<content:encoded><![CDATA[
arXiv:2509.24857v2 Announce Type: replace 
Abstract: Large language model-powered chatbots have transformed how people seek information, especially in high-stakes contexts like mental health. Despite their support capabilities, safe detection and response to crises such as suicidal ideation and self-harm are still unclear, hindered by the lack of unified crisis taxonomies and clinical evaluation standards.
  We address this by creating: (1) a taxonomy of six crisis categories; (2) a dataset of over 2,000 inputs from 12 mental health datasets, classified into these categories; and (3) a clinical response assessment protocol. We also use LLMs to identify crisis inputs and audit five models for response safety and appropriateness. First, we built a clinical-informed crisis taxonomy and evaluation protocol. Next, we curated 2,252 relevant examples from over 239,000 user inputs, then tested three LLMs for automatic classification.
  In addition, we evaluated five models for the appropriateness of their responses to a user's crisis, graded on a 5-point Likert scale from harmful (1) to appropriate (5). While some models respond reliably to explicit crises, risks still exist. Many outputs, especially in self-harm and suicidal categories, are inappropriate or unsafe. Different models perform variably; some, like gpt-5-nano and deepseek-v3.2-exp, have low harm rates, but others, such as gpt-4o-mini and grok-4-fast, generate more unsafe responses. All models struggle with indirect signals, default replies, and context misalignment.
  These results highlight the urgent need for better safeguards, crisis detection, and context-aware responses in LLMs. They also show that alignment and safety practices, beyond scale, are crucial for reliable crisis support. Our taxonomy, datasets, and evaluation methods support ongoing AI mental health research, aiming to reduce harm and protect vulnerable users.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic Architectures</title>
<link>https://arxiv.org/abs/2509.25045</link>
<guid>https://arxiv.org/abs/2509.25045</guid>
<content:encoded><![CDATA[
arXiv:2509.25045v2 Announce Type: replace 
Abstract: Despite their capabilities, Large Language Models (LLMs) remain opaque with limited understanding of their internal representations. Current interpretability methods either focus on input-oriented feature extraction, such as supervised probes and Sparse Autoencoders (SAEs), or on output distribution inspection, such as logit-oriented approaches. A full understanding of LLM vector spaces, however, requires integrating both perspectives, something existing approaches struggle with due to constraints on latent feature definitions. We introduce the Hyperdimensional Probe, a hybrid supervised probe that combines symbolic representations with neural probing. Leveraging Vector Symbolic Architectures (VSAs) and hypervector algebra, it unifies prior methods: the top-down interpretability of supervised probes, SAE's sparsity-driven proxy space, and output-oriented logit investigation. This allows deeper input-focused feature extraction while supporting output-oriented investigation. Our experiments show that our method consistently extracts meaningful concepts across LLMs, embedding sizes, and setups, uncovering concept-driven patterns in analogy-oriented inference and QA-focused text generation. By supporting joint input-output analysis, this work advances semantic understanding of neural representations while unifying the complementary perspectives of prior methods.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning Up the Instruction Ladder for Controllable Language Models</title>
<link>https://arxiv.org/abs/2511.04694</link>
<guid>https://arxiv.org/abs/2511.04694</guid>
<content:encoded><![CDATA[
arXiv:2511.04694v3 Announce Type: replace 
Abstract: As large language model (LLM) based systems take on high-stakes roles in real-world decision-making, they must reconcile competing instructions from multiple sources (e.g., model developers, users, and tools) within a single prompt context. Thus, enforcing an instruction hierarchy (IH) in LLMs, where higher-level directives override lower-priority requests, is critical for the reliability and controllability of LLMs. In this work, we reframe instruction hierarchy resolution as a reasoning task. Specifically, the model must first "think" about the relationship between a given user prompt and higher-priority (system) instructions before generating a response. To enable this capability via training, we construct VerIH, an instruction hierarchy dataset of constraint-following tasks with verifiable answers. This dataset comprises ~7K aligned and conflicting system-user instructions. We show that lightweight reinforcement learning with VerIH effectively transfers general reasoning capabilities of models to instruction prioritization. Our finetuned models achieve consistent improvements on instruction following and instruction hierarchy benchmarks, achieving roughly a 20% improvement on the IHEval conflict setup. This reasoning ability also generalizes to safety-critical settings beyond the training distribution. By treating safety issues as resolving conflicts between adversarial user inputs and predefined higher-priority policies, our trained model enhances robustness against jailbreak and prompt injection attacks, providing up to a 20% reduction in attack success rate (ASR). These results demonstrate that reasoning over instruction hierarchies provides a practical path to reliable LLMs, where updates to system prompts yield controllable and robust changes in model behavior.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HLPD: Aligning LLMs to Human Language Preference for Machine-Revised Text Detection</title>
<link>https://arxiv.org/abs/2511.06942</link>
<guid>https://arxiv.org/abs/2511.06942</guid>
<content:encoded><![CDATA[
arXiv:2511.06942v3 Announce Type: replace 
Abstract: To prevent misinformation and social issues arising from trustworthy-looking content generated by LLMs, it is crucial to develop efficient and reliable methods for identifying the source of texts. Previous approaches have demonstrated exceptional performance in detecting texts fully generated by LLMs. However, these methods struggle when confronting more advanced LLM output or text with adversarial multi-task machine revision, especially in the black-box setting, where the generating model is unknown. To address this challenge, grounded in the hypothesis that human writing possesses distinctive stylistic patterns, we propose Human Language Preference Detection (HLPD). HLPD employs a reward-based alignment process, Human Language Preference Optimization (HLPO), to shift the scoring model's token distribution toward human-like writing, making the model more sensitive to human writing, therefore enhancing the identification of machine-revised text. We test HLPD in an adversarial multi-task evaluation framework that leverages a five-dimensional prompt generator and multiple advanced LLMs to create diverse revision scenarios. When detecting texts revised by GPT-series models, HLPD achieves a 15.11% relative improvement in AUROC over ImBD, surpassing Fast-DetectGPT by 45.56%. When evaluated on texts generated by advanced LLMs, HLPD achieves the highest average AUROC, exceeding ImBD by 5.53% and Fast-DetectGPT by 34.14%. Code will be made available at https://github.com/dfq2021/HLPD.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>iMAD: Intelligent Multi-Agent Debate for Efficient and Accurate LLM Inference</title>
<link>https://arxiv.org/abs/2511.11306</link>
<guid>https://arxiv.org/abs/2511.11306</guid>
<content:encoded><![CDATA[
arXiv:2511.11306v2 Announce Type: replace 
Abstract: Large Language Model (LLM) agent systems have advanced rapidly, driven by their strong generalization in zero-shot settings. To further enhance reasoning and accuracy on complex tasks, Multi-Agent Debate (MAD) has emerged as a promising framework that engages multiple LLM agents in structured debates to encourage diverse reasoning. However, triggering MAD for every query is inefficient, as it incurs substantial computational (token) cost and may even degrade accuracy by overturning correct single-agent answers. To address these limitations, we propose intelligent Multi-Agent Debate (iMAD), a token-efficient framework that selectively triggers MAD only when it is likely to be beneficial (i.e., correcting an initially wrong answer). To achieve this goal, iMAD learns generalizable model behaviors to make accurate debate decisions. Specifically, iMAD first prompts a single agent to produce a structured self-critique response, from which we extract 41 interpretable linguistic and semantic features capturing hesitation cues. Then, iMAD uses a lightweight debate-decision classifier, trained using our proposed FocusCal loss, to determine whether to trigger MAD, enabling robust debate decisions without test dataset-specific tuning. Through extensive experiments using six (visual) question answering datasets against five competitive baselines, we have shown that iMAD significantly reduces token usage (by up to 92%) while also improving final answer accuracy (by up to 13.5%).
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CT-GLIP: 3D Grounded Language-Image Pretraining with CT Scans and Radiology Reports for Full-Body Scenarios</title>
<link>https://arxiv.org/abs/2404.15272</link>
<guid>https://arxiv.org/abs/2404.15272</guid>
<content:encoded><![CDATA[
arXiv:2404.15272v4 Announce Type: replace-cross 
Abstract: 3D medical vision-language (VL) pretraining has shown potential in radiology by leveraging large-scale multimodal datasets with CT-report pairs. However, existing methods primarily rely on a global VL alignment directly adapted from 2D scenarios. The entire 3D image is transformed into one global embedding, resulting in a loss of sparse but critical semantics essential for accurately aligning with the corresponding diagnosis. To address this limitation, we propose CT-GLIP, a 3D Grounded Language-Image Pretrained model that constructs fine-grained CT-report pairs to enhance \textit{grounded} cross-modal contrastive learning, effectively aligning grounded visual features with precise textual descriptions. Leveraging the grounded cross-modal alignment, CT-GLIP improves performance across diverse downstream tasks and can even identify organs and abnormalities in a zero-shot manner using natural language. CT-GLIP is trained on a multimodal CT dataset comprising 44,011 organ-level CT-report pairs from 17,702 patients, covering 104 organs. Evaluation is conducted on four downstream tasks: zero-shot organ recognition (OR), zero-shot abnormality detection (AD), tumor detection (TD), and tumor segmentation (TS). Empirical results show that it outperforms its counterparts with global VL alignment. Compared to vanilla CLIP, CT-GLIP achieves average performance improvements of 15.1% of F1 score, 1.9% of AUC, and 3.2% of DSC for zero-shot AD, TD, and TS tasks, respectively. This study highlights the significance of grounded VL alignment in enabling 3D medical VL foundation models to understand sparse representations within CT scans.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pre-trained Language Models Improve the Few-shot Prompt Ability of Decision Transformer</title>
<link>https://arxiv.org/abs/2408.01402</link>
<guid>https://arxiv.org/abs/2408.01402</guid>
<content:encoded><![CDATA[
arXiv:2408.01402v2 Announce Type: replace-cross 
Abstract: Decision Transformer (DT) has emerged as a promising class of algorithms in offline reinforcement learning (RL) tasks, leveraging pre-collected datasets and Transformer's capability to model long sequences. Recent works have demonstrated that using parts of trajectories from training tasks as prompts in DT enhances its performance on unseen tasks, giving rise to Prompt-DT methods. However, collecting data from specific environments can be both costly and unsafe in many scenarios, leading to suboptimal performance and limited few-shot prompt abilities due to the data-hungry nature of Transformer-based models. Additionally, the limited datasets used in pre-training make it challenging for Prompt-DT type of methods to distinguish between various RL tasks through prompts alone. To address these challenges, we introduce the Language model-initialized Prompt Decision Transformer (LPDT) framework, which leverages pretrained language models providing rich prior knowledge for RL tasks and fine-tunes the sequence model using Low-rank Adaptation (LoRA) for meta-RL problems. We further incorporate prompt regularization to effectively differentiate between tasks based on prompt feature representations. Comprehensive empirical studies demonstrate that initializing with a pre-trained language model provides the prior knowledge and achieves a similar performance with Prompt-DT under only $10\%$ data in some MuJoCo control tasks. We also provide a thorough ablation study to validate the effectiveness of each component, including sequence modeling, language models, prompt regularizations, and prompt strategies.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mechanisms of Symbol Processing for In-Context Learning in Transformer Networks</title>
<link>https://arxiv.org/abs/2410.17498</link>
<guid>https://arxiv.org/abs/2410.17498</guid>
<content:encoded><![CDATA[
arXiv:2410.17498v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated impressive abilities in symbol processing through in-context learning (ICL). This success flies in the face of decades of critiques asserting that artificial neural networks cannot master abstract symbol manipulation. We seek to understand the mechanisms that can enable robust symbol processing in transformer networks, illuminating both the unanticipated success, and the significant limitations, of transformers in symbol processing. Borrowing insights from symbolic AI and cognitive science on the power of Production System architectures, we develop a high-level Production System Language, PSL, that allows us to write symbolic programs to do complex, abstract symbol processing, and create compilers that precisely implement PSL programs in transformer networks which are, by construction, 100% mechanistically interpretable. The work is driven by study of a purely abstract (semantics-free) symbolic task that we develop, Templatic Generation (TGT). Although developed through study of TGT, PSL is, we demonstrate, highly general: it is Turing Universal. The new type of transformer architecture that we compile from PSL programs suggests a number of paths for enhancing transformers' capabilities at symbol processing. We note, however, that the work we report addresses computability, and not learnability, by transformer networks.
  Note: The first section provides an extended synopsis of the entire paper.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BountyBench: Dollar Impact of AI Agent Attackers and Defenders on Real-World Cybersecurity Systems</title>
<link>https://arxiv.org/abs/2505.15216</link>
<guid>https://arxiv.org/abs/2505.15216</guid>
<content:encoded><![CDATA[
arXiv:2505.15216v3 Announce Type: replace-cross 
Abstract: AI agents have the potential to significantly alter the cybersecurity landscape. Here, we introduce the first framework to capture offensive and defensive cyber-capabilities in evolving real-world systems. Instantiating this framework with BountyBench, we set up 25 systems with complex, real-world codebases. To capture the vulnerability lifecycle, we define three task types: Detect (detecting a new vulnerability), Exploit (exploiting a given vulnerability), and Patch (patching a given vulnerability). For Detect, we construct a new success indicator, which is general across vulnerability types and provides localized evaluation. We manually set up the environment for each system, including installing packages, setting up server(s), and hydrating database(s). We add 40 bug bounties, which are vulnerabilities with monetary awards from \$10 to \$30,485, covering 9 of the OWASP Top 10 Risks. To modulate task difficulty, we devise a new strategy based on information to guide detection, interpolating from identifying a zero day to exploiting a given vulnerability. We evaluate 10 agents: Claude Code, OpenAI Codex CLI with o3-high and o4-mini, and custom agents with o3-high, GPT-4.1, Gemini 2.5 Pro Preview, Claude 3.7 Sonnet Thinking, Qwen3 235B A22B, Llama 4 Maverick, and DeepSeek-R1. Given up to three attempts, the top-performing agents are Codex CLI: o3-high (12.5% on Detect, mapping to \$3,720; 90% on Patch, mapping to \$14,152), Custom Agent: Claude 3.7 Sonnet Thinking (67.5% on Exploit), and Codex CLI: o4-mini (90% on Patch, mapping to \$14,422). Codex CLI: o3-high, Codex CLI: o4-mini, and Claude Code are more capable at defense, achieving higher Patch scores of 90%, 90%, and 87.5%, compared to Exploit scores of 47.5%, 32.5%, and 57.5% respectively; while the custom agents are relatively balanced between offense and defense, achieving Exploit scores of 17.5-67.5% and Patch scores of 25-60%.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unleashing Hour-Scale Video Training for Long Video-Language Understanding</title>
<link>https://arxiv.org/abs/2506.05332</link>
<guid>https://arxiv.org/abs/2506.05332</guid>
<content:encoded><![CDATA[
arXiv:2506.05332v2 Announce Type: replace-cross 
Abstract: Recent long-form video-language understanding benchmarks have driven progress in video large multimodal models (Video-LMMs). However, the scarcity of well-annotated long videos has left the training of hour-long Video-LMMs underexplored. To close this gap, we present VideoMarathon, a large-scale hour-long video instruction-following dataset. This dataset includes around 9,700 hours of long videos sourced from diverse domains, ranging from 3 to 60 minutes per video. Specifically, it contains 3.3M high-quality QA pairs, spanning six fundamental topics: temporality, spatiality, object, action, scene, and event. Compared to existing video instruction datasets, VideoMarathon significantly extends training video durations up to 1 hour, and supports 22 diverse tasks requiring both short- and long-term video comprehension. Building on VideoMarathon, we propose Hour-LLaVA, a powerful and efficient Video-LMM for hour-scale video-language modeling. It enables hour-long video training and inference at 1-FPS sampling by leveraging a memory augmentation module, which adaptively integrates question-relevant and spatiotemporally informative semantics from the cached full video context. In our experiments, Hour-LLaVA achieves the best performance on multiple representative long video-language benchmarks, demonstrating the high quality of the VideoMarathon dataset and the superiority of the Hour-LLaVA model.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HeavyWater and SimplexWater: Distortion-Free LLM Watermarks for Low-Entropy Next-Token Predictions</title>
<link>https://arxiv.org/abs/2506.06409</link>
<guid>https://arxiv.org/abs/2506.06409</guid>
<content:encoded><![CDATA[
arXiv:2506.06409v2 Announce Type: replace-cross 
Abstract: Large language model (LLM) watermarks enable authentication of text provenance, curb misuse of machine-generated text, and promote trust in AI systems. Current watermarks operate by changing the next-token predictions output by an LLM. The updated (i.e., watermarked) predictions depend on random side information produced, for example, by hashing previously generated tokens. LLM watermarking is particularly challenging in low-entropy generation tasks -- such as coding -- where next-token predictions are near-deterministic. In this paper, we propose an optimization framework for watermark design. Our goal is to understand how to most effectively use random side information in order to maximize the likelihood of watermark detection and minimize the distortion of generated text. Our analysis informs the design of two new watermarks: HeavyWater and SimplexWater. Both watermarks are tunable, gracefully trading-off between detection accuracy and text distortion. They can also be applied to any LLM and are agnostic to side information generation. We examine the performance of HeavyWater and SimplexWater through several benchmarks, demonstrating that they can achieve high watermark detection accuracy with minimal compromise of text generation quality, particularly in the low-entropy regime. Our theoretical analysis also reveals surprising new connections between LLM watermarking and coding theory. The code implementation can be found in https://github.com/DorTsur/HeavyWater_SimplexWater
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SQLBarber: A System Leveraging Large Language Models to Generate Customized and Realistic SQL Workloads</title>
<link>https://arxiv.org/abs/2507.06192</link>
<guid>https://arxiv.org/abs/2507.06192</guid>
<content:encoded><![CDATA[
arXiv:2507.06192v2 Announce Type: replace-cross 
Abstract: Database research and development often require a large number of SQL queries for benchmarking purposes. However, acquiring real-world SQL queries is challenging due to privacy concerns, and existing SQL generation methods are limited in customization and in satisfying realistic constraints. To address this issue, we present SQLBarber, a system based on Large Language Models (LLMs) to generate customized and realistic SQL workloads. SQLBarber (i) eliminates the need for users to manually craft SQL templates in advance, while providing the flexibility to accept natural language specifications to constrain SQL templates, (ii) scales efficiently to generate large volumes of queries matching any user-defined cost distribution (e.g., cardinality and execution plan cost), and (iii) uses execution statistics from Amazon Redshift and Snowflake to derive SQL template specifications and query cost distributions that reflect real-world query characteristics. SQLBarber introduces (i) a declarative interface for users to effortlessly generate customized SQL templates, (ii) an LLM-powered pipeline augmented with a self-correction module that profiles, refines, and prunes SQL templates based on query costs, and (iii) a Bayesian Optimizer to efficiently explore different predicate values and identify a set of queries that satisfy the target cost distribution. We construct and open-source ten benchmarks of varying difficulty levels and target query cost distributions based on real-world statistics from Snowflake and Amazon Redshift. Extensive experiments on these benchmarks show that SQLBarber is the only system that can generate customized SQL templates. It reduces query generation time by one to three orders of magnitude, and significantly improves alignment with the target cost distribution, compared with existing methods.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Animating Language Practice: Engagement with Stylized Conversational Agents in Japanese Learning</title>
<link>https://arxiv.org/abs/2507.06483</link>
<guid>https://arxiv.org/abs/2507.06483</guid>
<content:encoded><![CDATA[
arXiv:2507.06483v2 Announce Type: replace-cross 
Abstract: We explore Jouzu, a Japanese language learning application that integrates large language models with anime-inspired conversational agents. Designed to address challenges learners face in practicing natural and expressive dialogue, Jouzu combines stylized character personas with expressive text-to-speech to create engaging conversational scenarios. We conducted a two-week in-the-wild deployment with 52 Japanese learners to examine how such stylized agents influence engagement and learner experience. Our findings show that participants interacted frequently and creatively, with advanced learners demonstrating greater use of expressive forms. Participants reported that the anime-inspired style made practice more enjoyable and encouraged experimenting with different registers. We discuss how stylization shapes willingness to engage, the role of affect in sustaining practice, and design opportunities for culturally grounded conversational AI in computer-assisted language learning (CALL). By framing our findings as an exploration of design and engagement, we highlight opportunities for generalization beyond Japanese contexts and contribute to international HCI scholarship.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Towards the Information Boundary of Instruction Set: InfinityInstruct-Subject Technical Report</title>
<link>https://arxiv.org/abs/2507.06968</link>
<guid>https://arxiv.org/abs/2507.06968</guid>
<content:encoded><![CDATA[
arXiv:2507.06968v2 Announce Type: replace-cross 
Abstract: Instruction tuning has become a foundation for unlocking the capabilities of large-scale pretrained models and improving their performance on complex tasks. Thus, the construction of high-quality instruction datasets is crucial for enhancing model performance and generalizability. Although current instruction datasets have reached tens of millions of samples, models finetuned on them may still struggle with complex instruction following and tasks in rare domains. This is primarily due to limited expansion in both ``coverage'' (coverage of task types and knowledge areas) and ``depth'' (instruction complexity) of the instruction set. To address this issue, we propose a systematic instruction data construction framework, which integrates a hierarchical labeling system, an informative seed selection algorithm, an evolutionary data synthesis process, and a model deficiency diagnosis with targeted data generation. These components form an iterative closed-loop to continuously enhance the coverage and depth of instruction data. Based on this framework, we construct InfinityInstruct-Subject, a high-quality dataset containing ~1.5 million instructions. Experiments on multiple foundation models and benchmark tasks demonstrate its effectiveness in improving instruction-following capabilities. Further analyses suggest that InfinityInstruct-Subject shows enlarged coverage and depth compared to comparable synthesized instruction datasets. Our work lays a theoretical and practical foundation for the efficient, continuous evolution of instruction datasets, moving from data quantity expansion to qualitative improvement.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating LLMs on Sequential API Call Through Automated Test Generation</title>
<link>https://arxiv.org/abs/2507.09481</link>
<guid>https://arxiv.org/abs/2507.09481</guid>
<content:encoded><![CDATA[
arXiv:2507.09481v2 Announce Type: replace-cross 
Abstract: By integrating tools from external APIs, Large Language Models (LLMs) have expanded their promising capabilities in a diverse spectrum of complex real-world tasks. However, testing, evaluation, and analysis of LLM tool use remain in their early stages. Most existing benchmarks rely on manually collected test cases, many of which cannot be automatically checked for semantic correctness and instead depend on static methods such as string matching. Additionally, these benchmarks often overlook the complex interactions that occur between sequential API calls, which are common in real-world applications. To fill the gap, in this paper, we introduce StateGen, an automated framework designed to generate diverse coding tasks involving sequential API interactions. StateGen combines state-machine-based API constraint solving and validation, energy-based sampling, and control-flow injection to generate executable programs. These programs are then translated into human-like natural language task descriptions through a collaboration of two LLM agents. Utilizing StateGen, we construct StateEval, a benchmark encompassing 120 verified test cases spanning across three representative scenarios: Session Service, Tensor Operation, and ElevenLabs MCP. Experimental results confirm that StateGen can effectively generate challenging and realistic API-oriented tasks, highlighting areas for improvement in current LLMs incorporating APIs.We make our framework and benchmark publicly available to support future research.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Human and LLM Judgments: Understanding and Narrowing the Gap</title>
<link>https://arxiv.org/abs/2508.12792</link>
<guid>https://arxiv.org/abs/2508.12792</guid>
<content:encoded><![CDATA[
arXiv:2508.12792v2 Announce Type: replace-cross 
Abstract: Large language models are increasingly used as judges (LLM-as-a-judge) to evaluate model outputs at scale, but their assessments often diverge systematically from human judgments. We present Bridge, a unified statistical framework that explicitly bridges human and LLM evaluations under both absolute scoring and pairwise comparison paradigms. Bridge posits a latent human preference score for each prompt-response pair and models LLM deviations as linear transformations of covariates that capture sources of discrepancies. This offers a simple and principled framework for refining LLM ratings and characterizing systematic discrepancies between humans and LLMs. We provide an efficient fitting algorithm with asymptotic guarantees for statistical inference. Using six LLM judges and two benchmarks (BigGen Bench and Chatbot Arena), Bridge achieves higher agreement with human ratings (accuracy, calibration, and KL divergence) and exposes systematic human-LLM gaps.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenGVL -- Benchmarking Visual Temporal Progress for Data Curation</title>
<link>https://arxiv.org/abs/2509.17321</link>
<guid>https://arxiv.org/abs/2509.17321</guid>
<content:encoded><![CDATA[
arXiv:2509.17321v3 Announce Type: replace-cross 
Abstract: Data scarcity remains one of the most limiting factors in driving progress in robotics. However, the amount of available robotics data in the wild is growing exponentially, creating new opportunities for large-scale data utilization. Reliable temporal task completion prediction could help automatically annotate and curate this data at scale. The Generative Value Learning (GVL) approach was recently proposed, leveraging the knowledge embedded in vision-language models (VLMs) to predict task progress from visual observations. Building upon GVL, we propose OpenGVL, a comprehensive benchmark for estimating task progress across diverse challenging manipulation tasks involving both robotic and human embodiments. We evaluate the capabilities of publicly available open-source foundation models, showing that open-source model families significantly underperform closed-source counterparts, achieving only approximately $70\%$ of their performance on temporal progress prediction tasks. Furthermore, we demonstrate how OpenGVL can serve as a practical tool for automated data curation and filtering, enabling efficient quality assessment of large-scale robotics datasets. We release the benchmark along with the complete codebase at \href{github.com/budzianowski/opengvl}{OpenGVL}.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tiny but Mighty: A Software-Hardware Co-Design Approach for Efficient Multimodal Inference on Battery-Powered Small Devices</title>
<link>https://arxiv.org/abs/2510.05109</link>
<guid>https://arxiv.org/abs/2510.05109</guid>
<content:encoded><![CDATA[
arXiv:2510.05109v3 Announce Type: replace-cross 
Abstract: Large Multimodal Models (LMMs) are inherently modular, consisting of vision and audio encoders, projectors, and large language models. Yet, they are almost always executed monolithically, which underutilizes the heterogeneous accelerators (NPUs, GPUs, DSPs) in modern SoCs and leads to high end-to-end latency. In this paper, we present NANOMIND, a hardware--software co-design inference framework for Large Multimodal Models (LMMs) that breaks large models into modular ``bricks'' (vision, language, audio, etc.) and maps each to its ideal accelerator. The key insight is that large models can be broken into modular components and scheduled to run on the most appropriate compute units. It performs module-level dynamic offloading across accelerators on unified-memory SoCs. By combining customized hardware design, system-level scheduling, and optimized low-bit computation kernels, we demonstrate our framework with a compact, battery-powered device capable of running LMMs entirely on device. This prototype functions as a self-contained intelligent assistant that requires no network connectivity, while achieving higher throughput and superior power efficiency under strict resource constraints. The design further bypasses CPU bottlenecks and reduces redundant memory usage through token-aware buffer management and module-level coordination. Our system outperforms existing implementations in resource efficiency, cutting energy consumption by 42.3\% and GPU memory usage by 11.2\%. This enables a battery-powered device to run LLaVA-OneVision with a camera for nearly 20.8 hours.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spark-Prover-X1: Formal Theorem Proving Through Diverse Data Training</title>
<link>https://arxiv.org/abs/2511.13043</link>
<guid>https://arxiv.org/abs/2511.13043</guid>
<content:encoded><![CDATA[
<div> automated theorem proving, large language models, formal reasoning, supervised fine-tuning, benchmark dataset  

<br /><br />Summary:  
The paper introduces Spark-Prover-X1, a 7-billion parameter large language model (LLM) developed to advance automated theorem proving by addressing the lack of diverse and high-quality formal language data. The training framework consists of three stages: first, continuous pre-training on a broad mathematical corpus enriched with innovative data tasks including a novel "Chain-of-Thought (CoT)-augmented state prediction" task to improve fine-grained reasoning abilities. Second, a supervised fine-tuning (SFT) phase within an expert iteration loop is employed to specialize both the Spark-Prover-X1-7B model and the companion Spark-Formalizer-X1-7B model. Third, Group Relative Policy Optimization (GRPO) is applied to enhance the model’s problem-solving capabilities on the most difficult tasks. To support evaluation, the authors introduce ExamFormal-Bench, a new benchmark dataset of 402 formal problems that reflect real-world examination scenarios. Experimental results show that Spark-Prover-X1 achieves state-of-the-art performance among similar-sized open-source models in the "Whole-Proof Generation" paradigm. Notably, it solves 27 problems on PutnamBench (pass@32) and attains 24.0% pass rate on CombiBench (pass@32). The study demonstrates that combining diverse training data with a systematically refined training pipeline effectively boosts formal reasoning in lightweight LLMs. The authors plan to release both models and the benchmark dataset publicly in the near future. <div>
arXiv:2511.13043v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown significant promise in automated theorem proving, yet progress is often constrained by the scarcity of diverse and high-quality formal language data. To address this issue, we introduce Spark-Prover-X1, a 7B parameter model trained via an three-stage framework designed to unlock the reasoning potential of more accessible and moderately-sized LLMs. The first stage infuses deep knowledge through continuous pre-training on a broad mathematical corpus, enhanced by a suite of novel data tasks. Key innovation is a "CoT-augmented state prediction" task to achieve fine-grained reasoning. The second stage employs Supervised Fine-tuning (SFT) within an expert iteration loop to specialize both the Spark-Prover-X1-7B and Spark-Formalizer-X1-7B models. Finally, a targeted round of Group Relative Policy Optimization (GRPO) is applied to sharpen the prover's capabilities on the most challenging problems. To facilitate robust evaluation, particularly on problems from real-world examinations, we also introduce ExamFormal-Bench, a new benchmark dataset of 402 formal problems. Experimental results demonstrate that Spark-Prover achieves state-of-the-art performance among similarly-sized open-source models within the "Whole-Proof Generation" paradigm. It shows exceptional performance on difficult competition benchmarks, notably solving 27 problems on PutnamBench (pass@32) and achieving 24.0\% on CombiBench (pass@32). Our work validates that this diverse training data and progressively refined training pipeline provides an effective path for enhancing the formal reasoning capabilities of lightweight LLMs. We will release both Spark-Prover-X1-7B and Spark-Formalizer-X1-7B, along with the ExamFormal-Bench dataset, in the near future.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicting the Performance of Black-box LLMs through Follow-up Queries</title>
<link>https://arxiv.org/abs/2501.01558</link>
<guid>https://arxiv.org/abs/2501.01558</guid>
<content:encoded><![CDATA[
<div> Keywords: black-box language models, follow-up questions, behavior prediction, adversarial manipulation, model differentiation  

<br /><br />Summary:  
The paper addresses the challenge of reliably predicting the behavior of black-box language models (LLMs), which are often accessible only via closed-source APIs. The authors propose a novel method that involves asking follow-up questions to the LLM and using the probabilities of its responses as feature representations to train reliable predictors. They demonstrate that a simple linear model trained on these response probabilities can accurately predict the correctness of the LLM’s outputs on various question-answering and reasoning benchmarks. Remarkably, this black-box prediction approach can outperform white-box linear models that rely on internal activations or model internals. Additionally, the method effectively distinguishes between clean versions of LLMs and those adversarially influenced through system prompts that cause incorrect answers or buggy code generation. The approach also enables the identification of misrepresented models accessed through APIs by differentiating between different black-box LLMs. Overall, the study showcases the potential for monitoring and ensuring trustworthy behavior of black-box language models, which is crucial for their safe integration into larger, autonomous systems. <div>
arXiv:2501.01558v4 Announce Type: replace-cross 
Abstract: Reliably predicting the behavior of language models -- such as whether their outputs are correct or have been adversarially manipulated -- is a fundamentally challenging task. This is often made even more difficult as frontier language models are offered only through closed-source APIs, providing only black-box access. In this paper, we predict the behavior of black-box language models by asking follow-up questions and taking the probabilities of responses \emph{as} representations to train reliable predictors. We first demonstrate that training a linear model on these responses reliably and accurately predicts model correctness on question-answering and reasoning benchmarks. Surprisingly, this can \textit{even outperform white-box linear predictors} that operate over model internals or activations. Furthermore, we demonstrate that these follow-up question responses can reliably distinguish between a clean version of an LLM and one that has been adversarially influenced via a system prompt to answer questions incorrectly or to introduce bugs into generated code. Finally, we show that they can also be used to differentiate between black-box LLMs, enabling the detection of misrepresented models provided through an API. Overall, our work shows promise in monitoring black-box language model behavior, supporting their deployment in larger, autonomous systems.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text Annotation via Inductive Coding: Comparing Human Experts to LLMs in Qualitative Data Analysis</title>
<link>https://arxiv.org/abs/2512.00046</link>
<guid>https://arxiv.org/abs/2512.00046</guid>
<content:encoded><![CDATA[
<div> arXiv, qualitative data analysis, inductive coding, large language models, human evaluation<br /><br />Summary:<br /><br />This paper explores automating qualitative data analysis through inductive coding using large language models (LLMs), focusing on generating labels directly from data rather than relying on predefined categories. The study evaluates six open-source LLMs by comparing their coding performance against human experts. A unique finding is the contrasting performance patterns: humans excel at labeling complex sentences but have difficulty with simpler ones, whereas LLMs perform better on simpler sentences but struggle with complex ones. The authors also analyze systematic differences between labels produced by humans and LLMs in reference to a golden standard test set. Although human annotations sometimes diverge from the standard, they often receive higher subjective ratings from other human evaluators. Conversely, some LLM-generated labels align more closely with the golden standard but are rated less favorably by human experts. This highlights discrepancies between objective accuracy and perceived quality in qualitative coding tasks when comparing human and machine performance. The study provides insights into the complementary strengths and limitations of human coders and LLMs in inductive qualitative analysis. <div>
arXiv:2512.00046v1 Announce Type: new 
Abstract: This paper investigates the automation of qualitative data analysis, focusing on inductive coding using large language models (LLMs). Unlike traditional approaches that rely on deductive methods with predefined labels, this research investigates the inductive process where labels emerge from the data. The study evaluates the performance of six open-source LLMs compared to human experts. As part of the evaluation, experts rated the perceived difficulty of the quotes they coded. The results reveal a peculiar dichotomy: human coders consistently perform well when labeling complex sentences but struggle with simpler ones, while LLMs exhibit the opposite trend. Additionally, the study explores systematic deviations in both human and LLM generated labels by comparing them to the golden standard from the test set. While human annotations may sometimes differ from the golden standard, they are often rated more favorably by other humans. In contrast, some LLMs demonstrate closer alignment with the true labels but receive lower evaluations from experts.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emergent Convergence in Multi-Agent LLM Annotation</title>
<link>https://arxiv.org/abs/2512.00047</link>
<guid>https://arxiv.org/abs/2512.00047</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, multi-agent interaction, coordination dynamics, semantic compression, emergent behaviors  

<br /><br />Summary:  
This study investigates how large language models (LLMs) coordinate in collaborative, multi-agent settings when treated as black-box agents, focusing on their interaction dynamics during an inductive coding task. The authors simulate 7,500 multi-agent, multi-round discussions, generating over 125,000 utterances that detail both the final annotations and the dialogue history. They introduce novel process-level metrics such as code stability, semantic self-consistency, lexical confidence, sentiment, and convergence to measure and track the coordination dynamics between LLM agents. By analyzing the evolving geometry of output embeddings, the researchers find that intrinsic dimensionality decreases over rounds, which they interpret as semantic compression occurring within the group’s communication. The experiments show that groups of LLMs not only converge lexically and semantically but also develop asymmetric influence patterns, suggesting that some agents play more dominant roles. Moreover, these agents exhibit negotiation-like behaviors even though they are not explicitly assigned roles or prompted to negotiate. This research highlights that black-box interaction analysis can effectively uncover emergent coordination strategies in multi-agent LLM systems, serving as a scalable complement to traditional internal probe-based interpretability methods. <div>
arXiv:2512.00047v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed in collaborative settings, yet little is known about how they coordinate when treated as black-box agents. We simulate 7500 multi-agent, multi-round discussions in an inductive coding task, generating over 125000 utterances that capture both final annotations and their interactional histories. We introduce process-level metrics: code stability, semantic self-consistency, and lexical confidence alongside sentiment and convergence measures, to track coordination dynamics. To probe deeper alignment signals, we analyze the evolving geometry of output embeddings, showing that intrinsic dimensionality declines over rounds, suggesting semantic compression. The results reveal that LLM groups converge lexically and semantically, develop asymmetric influence patterns, and exhibit negotiation-like behaviors despite the absence of explicit role prompting. This work demonstrates how black-box interaction analysis can surface emergent coordination strategies, offering a scalable complement to internal probe-based interpretability methods.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tree Matching Networks for Natural Language Inference: Parameter-Efficient Semantic Understanding via Dependency Parse Trees</title>
<link>https://arxiv.org/abs/2512.00204</link>
<guid>https://arxiv.org/abs/2512.00204</guid>
<content:encoded><![CDATA[
<div> Transformer models, BERT, sentence embeddings, dependency parse trees, Graph Matching Networks<br /><br />Summary: This paper addresses the challenge of creating efficient and accurate sentence embeddings for Natural Language Inference (NLI) tasks. Traditional transformer-based models like BERT achieve high accuracy but come with the downside of extremely large parameter sizes and computational demands. The authors propose an alternative approach by explicitly incorporating linguistic structures—specifically dependency parse trees—into the embedding process. They extend Graph Matching Networks (GMN) to operate on these trees, resulting in a new model called Tree Matching Networks (TMN). TMN leverages prior encoded syntactic relationships, potentially improving learning efficiency by not having to infer all relationships from scratch. Experimental evaluation on the SNLI entailment task shows that TMN outperforms a comparable BERT-based model, achieving higher accuracy with significantly reduced memory usage and training time. However, both TMN and BERT struggle on the SemEval similarity task, highlighting limitations in current methods for this type of semantic similarity. The authors identify that while explicit structural models like TMN excel at comparable model scales compared to sequence-based models, the scalability is constrained by current aggregation techniques. To address this, the paper proposes a novel multi-headed attention aggregation method to improve scalability for future research and applications. <div>
arXiv:2512.00204v1 Announce Type: new 
Abstract: In creating sentence embeddings for Natural Language Inference (NLI) tasks, using transformer-based models like BERT leads to high accuracy, but require hundreds of millions of parameters. These models take in sentences as a sequence of tokens, and learn to encode the meaning of the sequence into embeddings such that those embeddings can be used reliably for NLI tasks. Essentially, every word is considered against every other word in the sequence, and the transformer model is able to determine the relationships between them, entirely from scratch. However, a model that accepts explicit linguistic structures like dependency parse trees may be able to leverage prior encoded information about these relationships, without having to learn them from scratch, thus improving learning efficiency. To investigate this, we adapt Graph Matching Networks (GMN) to operate on dependency parse trees, creating Tree Matching Networks (TMN). We compare TMN to a BERT based model on the SNLI entailment task and on the SemEval similarity task. TMN is able to achieve significantly better results with a significantly reduced memory footprint and much less training time than the BERT based model on the SNLI task, while both models struggled to preform well on the SemEval. Explicit structural representations significantly outperform sequence-based models at comparable scales, but current aggregation methods limit scalability. We propose multi-headed attention aggregation to address this limitation.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Corpus-Grounded Agentic LLMs for Multilingual Grammatical Analysis</title>
<link>https://arxiv.org/abs/2512.00214</link>
<guid>https://arxiv.org/abs/2512.00214</guid>
<content:encoded><![CDATA[
<div> Keywords: agentic large language models, corpus-grounded grammatical analysis, Universal Dependencies, multilingual word-order features, linguistic data automation<br /><br />Summary:<br /><br />1. The article addresses the challenge of systematically analyzing annotated linguistic corpora, which traditionally requires extensive methodological and technical effort.<br /><br />2. It proposes leveraging agentic large language models (LLMs) as tools to reason over annotated corpora and generate interpretable, data-driven responses to grammatical questions.<br /><br />3. The authors introduce a novel agentic framework that integrates natural-language task interpretation, code generation, and data-driven reasoning specifically for corpus-grounded grammatical analysis.<br /><br />4. As a proof of concept, the framework is applied to Universal Dependencies corpora, focusing on multilingual grammatical tasks inspired by the World Atlas of Language Structures (WALS).<br /><br />5. The evaluation covers 13 word-order features across more than 170 languages, assessing system performance on dominant order accuracy, order-coverage completeness, and distributional fidelity.<br /><br />6. Results show the feasibility of combining LLM-based reasoning with structured linguistic data, marking a foundational step toward interpretable and scalable automation of corpus-based grammatical inquiries. <div>
arXiv:2512.00214v1 Announce Type: new 
Abstract: Empirical grammar research has become increasingly data-driven, but the systematic analysis of annotated corpora still requires substantial methodological and technical effort. We explore how agentic large language models (LLMs) can streamline this process by reasoning over annotated corpora and producing interpretable, data-grounded answers to linguistic questions. We introduce an agentic framework for corpus-grounded grammatical analysis that integrates concepts such as natural-language task interpretation, code generation, and data-driven reasoning. As a proof of concept, we apply it to Universal Dependencies (UD) corpora, testing it on multilingual grammatical tasks inspired by the World Atlas of Language Structures (WALS). The evaluation spans 13 word-order features and over 170 languages, assessing system performance across three complementary dimensions - dominant-order accuracy, order-coverage completeness, and distributional fidelity - which reflect how well the system generalizes, identifies, and quantifies word-order variations. The results demonstrate the feasibility of combining LLM reasoning with structured linguistic data, offering a first step toward interpretable, scalable automation of corpus-based grammatical inquiry.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Minimal-Edit Instruction Tuning for Low-Resource Indic GEC</title>
<link>https://arxiv.org/abs/2512.00219</link>
<guid>https://arxiv.org/abs/2512.00219</guid>
<content:encoded><![CDATA[
<div> Keywords: Grammatical Error Correction, Indic Languages, Instruction-Tuned Large Language Models, Conservative Decoding, Parameter-Efficient Fine-Tuning

<br /><br />Summary:  
This paper addresses the challenge of grammatical error correction (GEC) in Indic languages, which are characterized by limited annotated data, diverse scripts, and complex morphology. The authors propose an augmentation-free approach leveraging instruction-tuned large language models (LLMs), specifically a 12-billion parameter GEMMA 3 model fine-tuned using parameter-efficient methods (PEFT) and Alpaca-style formatting in 4-bit precision. The decoding process is conservative and deterministic, guided by a lightweight normalizer that ensures minimal and meaning-preserving corrections. The inference mechanism uses fixed, language-specific prompts created from a deterministic error classifier’s taxonomy, label distributions, and precedence ordering derived from training data. Evaluated on Malayalam and Hindi using the official untuned GLEU metric, the system achieves competitive scores of 92.41 and 81.44, ranking sixth and third respectively among competitors. The results demonstrate that combining classifier-informed prompt design, adapter-based instruction tuning, and deterministic decoding yields a reproducible, computationally efficient alternative to traditional augmentation-heavy pipelines in Indic GEC. The study suggests potential directions for enhancing morphosyntactic constraint integration and emphasizes the importance of human-centered evaluation for conservative edits in future work. <div>
arXiv:2512.00219v1 Announce Type: new 
Abstract: Grammatical error correction for Indic languages faces limited supervision, diverse scripts, and rich morphology. We propose an augmentation-free setup that uses instruction-tuned large language models and conservative decoding. A 12B GEMMA 3 model is instruction-tuned in bnb 4-bit precision with parameter-efficient fine-tuning (PEFT) and Alpaca-style formatting. Decoding follows a deterministic, constraint-aware procedure with a lightweight normaliser that encourages minimal, meaning-preserving edits. We operationalise inference, subsequent to instruction fine-tuning (IFT), via a fixed, language-specific prompt directly synthesised from a deterministic error classifier's taxonomy, label distributions, and precedence ordering computed on the training corpus.
  Under the official untuned GLEU evaluation, the system scores 92.41 on Malayalam, sixth overall, and 81.44 on Hindi, third overall. These results indicate that classifier-informed prompt design, adapter-based instruction tuning, and deterministic decoding provide a reproducible and a computationally efficient alternative to augmentation-centred pipelines for Indic GEC. The approach also motivates future work on stronger morphosyntactic constraints and human-centred evaluation of conservative edits.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular Fusion</title>
<link>https://arxiv.org/abs/2512.00234</link>
<guid>https://arxiv.org/abs/2512.00234</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal translation, large language models, speech translation, simultaneous translation, model fusion<br /><br />Summary:<br /><br />This paper addresses the limitation of current open-source text-only large language models (LLMs) for speech translation (ST), which rely on cascaded pipelines that perform automatic speech recognition before translation, causing increased latency especially problematic for simultaneous speech translation (SimulST). The authors highlight that existing pretrained multimodal foundation models (MMFMs) have robust perception and reasoning capabilities across modalities but lack multilingual coverage and specialized translation performance. To overcome these challenges, they propose OmniFusion, an end-to-end system that fuses MMFMs with translation LLMs via a novel fusion strategy connecting multiple hidden layers of the MMFM to the translation LLM. This architecture enables joint training and multimodal context utilization, including audio and images, for enhanced disambiguation. Built using Omni 2.5-7B as the MMFM and SeedX PPO-7B as the translation LLM, OmniFusion supports speech-to-text, speech-and-image-to-text, and text-and-image-to-text translation tasks. Experimental results demonstrate that OmniFusion reduces latency by one second in SimulST compared to conventional cascaded pipelines and significantly improves translation quality. The approach presents a notable advancement for efficient and accurate multimodal multilingual translation systems. The code is publicly available for further research and development. <div>
arXiv:2512.00234v1 Announce Type: new 
Abstract: There has been significant progress in open-source text-only translation large language models (LLMs) with better language coverage and quality. However, these models can be only used in cascaded pipelines for speech translation (ST), performing automatic speech recognition first followed by translation. This introduces additional latency, which is particularly critical in simultaneous ST (SimulST), and prevents the model from exploiting multimodal context, such as images, which can aid disambiguation. Pretrained multimodal foundation models (MMFMs) already possess strong perception and reasoning capabilities across multiple modalities, but generally lack the multilingual coverage and specialized translation performance of dedicated translation LLMs. To build an effective multimodal translation system, we propose an end-to-end approach that fuses MMFMs with translation LLMs. We introduce a novel fusion strategy that connects hidden states from multiple layers of a pretrained MMFM to a translation LLM, enabling joint end-to-end training. The resulting model, OmniFusion, built on Omni 2.5-7B as the MMFM and SeedX PPO-7B as the translation LLM, can perform speech-to-text, speech-and-image-to-text, and text-and-image-to-text translation. Experiments demonstrate that OmniFusion effectively leverages both audio and visual inputs, achieves a 1-second latency reduction in SimulST compared to cascaded pipelines and also improves the overall translation quality\footnote{Code is available at https://github.com/saikoneru/OmniFusion}.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lost without translation -- Can transformer (language models) understand mood states?</title>
<link>https://arxiv.org/abs/2512.00274</link>
<guid>https://arxiv.org/abs/2512.00274</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Indic languages, mood states, translation, mental health<br /><br />Summary:<br /><br />1. Background: Large Language Models (LLMs) hold potential for psychiatric applications but primarily focus on English, which limits their effectiveness in understanding mood states expressed in other languages, particularly those with unique idioms of distress like Indic languages.<br />2. Aim: The study aimed to assess how well language models can represent idiomatic phrases describing four mood states—depression, euthymia, euphoric mania, and dysphoric mania—across 11 Indian languages.<br />3. Methods: A dataset of 247 unique mood state phrases was compiled. Seven experimental conditions were tested using k-means clustering on embeddings from (a) native and Romanised Indic scripts with multilingual and Indic-specific models and (b) translated phrases in English and Chinese.<br />4. Results: Direct embeddings of Indic languages failed to cluster mood states effectively (Composite Score=0.002). Translation-based approaches showed marked improvement, notably Gemini-translated English (0.60), human-translated English (0.61), and human-translated English further translated to Chinese and embedded with a Chinese model (best score 0.67). Specialized Indic models like IndicBERT and Sarvam-M performed poorly.<br />5. Conclusion: Current LLMs cannot reliably represent mood states directly from Indic languages, limiting psychiatric use in India. High-quality translation helps but depends on proprietary models and complex pipelines. To advance global mental health care, native understanding of diverse local languages in LLMs is essential. <div>
arXiv:2512.00274v1 Announce Type: new 
Abstract: Background: Large Language Models show promise in psychiatry but are English-centric. Their ability to understand mood states in other languages is unclear, as different languages have their own idioms of distress. Aim: To quantify the ability of language models to faithfully represent phrases (idioms of distress) of four distinct mood states (depression, euthymia, euphoric mania, dysphoric mania) expressed in Indian languages. Methods: We collected 247 unique phrases for the four mood states across 11 Indic languages. We tested seven experimental conditions, comparing k-means clustering performance on: (a) direct embeddings of native and Romanised scripts (using multilingual and Indic-specific models) and (b) embeddings of phrases translated to English and Chinese. Performance was measured using a composite score based on Adjusted Rand Index, Normalised Mutual Information, Homogeneity and Completeness. Results: Direct embedding of Indic languages failed to cluster mood states (Composite Score = 0.002). All translation-based approaches showed significant improvement. High performance was achieved using Gemini-translated English (Composite=0.60) and human-translated English (Composite=0.61) embedded with gemini-001. Surprisingly, human-translated English, further translated into Chinese and embedded with a Chinese model, performed best (Composite = 0.67). Specialised Indic models (IndicBERT and Sarvam-M) performed poorly. Conclusion: Current models cannot meaningfully represent mood states directly from Indic languages, posing a fundamental barrier to their psychiatric application for diagnostic or therapeutic purposes in India. While high-quality translation bridges this gap, reliance on proprietary models or complex translation pipelines is unsustainable. Models must first be built to understand diverse local languages to be effective in global mental health.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EduEval: A Hierarchical Cognitive Benchmark for Evaluating Large Language Models in Chinese Education</title>
<link>https://arxiv.org/abs/2512.00290</link>
<guid>https://arxiv.org/abs/2512.00290</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, EduEval, Chinese K-12 Education, Cognitive Framework, Benchmarking<br /><br />Summary: This paper introduces EduEval, a hierarchical benchmark designed to evaluate large language models (LLMs) in the context of Chinese K-12 education. First, it proposes the EduAbility Taxonomy, a cognitive framework combining Bloom's Taxonomy and Webb's Depth of Knowledge to categorize tasks into six cognitive dimensions: Memorization, Understanding, Application, Reasoning, Creativity, and Ethics. Second, the benchmark emphasizes authenticity by incorporating real exam questions, classroom dialogues, student essays, and expert-generated prompts to simulate genuine educational challenges. Third, EduEval is extensive in scale, containing 24 different task types and over 11,000 questions that cover primary through high school levels. The authors assess 14 leading LLMs using zero-shot and few-shot settings, finding that while these models excel at factual recall, they face difficulties in classroom dialogue classification and show inconsistent performance on creative content generation. Notably, several open-source models outperform proprietary ones on complex reasoning tasks relevant to education. Additionally, the efficacy of few-shot prompting varies significantly across the cognitive dimensions, indicating the need for customized strategies depending on educational goals. The study provides targeted metrics and insights for further development of LLMs tailored to diverse Chinese educational applications. <div>
arXiv:2512.00290v1 Announce Type: new 
Abstract: Large language models (LLMs) demonstrate significant potential for educational applications. However, their unscrutinized deployment poses risks to educational standards, underscoring the need for rigorous evaluation. We introduce EduEval, a comprehensive hierarchical benchmark for evaluating LLMs in Chinese K-12 education. This benchmark makes three key contributions: (1) Cognitive Framework: We propose the EduAbility Taxonomy, which unifies Bloom's Taxonomy and Webb's Depth of Knowledge to organize tasks across six cognitive dimensions including Memorization, Understanding, Application, Reasoning, Creativity, and Ethics. (2) Authenticity: Our benchmark integrates real exam questions, classroom conversation, student essays, and expert-designed prompts to reflect genuine educational challenges; (3) Scale: EduEval comprises 24 distinct task types with over 11,000 questions spanning primary to high school levels. We evaluate 14 leading LLMs under both zero-shot and few-shot settings, revealing that while models perform well on factual tasks, they struggle with classroom dialogue classification and exhibit inconsistent results in creative content generation. Interestingly, several open source models outperform proprietary systems on complex educational reasoning. Few-shot prompting shows varying effectiveness across cognitive dimensions, suggesting that different educational objectives require tailored approaches. These findings provide targeted benchmarking metrics for developing LLMs specifically optimized for diverse Chinese educational tasks.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparative Analysis of 47 Context-Based Question Answer Models Across 8 Diverse Datasets</title>
<link>https://arxiv.org/abs/2512.00323</link>
<guid>https://arxiv.org/abs/2512.00323</guid>
<content:encoded><![CDATA[
<div> Context-based question answering, CBQA models, Hugging Face, SQuAD, Genetic algorithm<br /><br />Summary:<br /><br />This manuscript benchmarks the performance of 47 context-based question answering (CBQA) models from Hugging Face across eight diverse datasets, aiming to identify the best-performing model without additional fine-tuning. Such benchmarking is crucial for practical implementations where retraining for specific datasets is minimized. It was found that the top models were trained on the SQuAD v1 or SQuAD v2 datasets. The best model overall was ahotrod/electra_large_discriminator_squad2_512, achieving 43% accuracy over all datasets. Performance analyses revealed that computation time depends on both context length and model size, while accuracy often decreases with longer answers. Additionally, model performance is affected by the complexity of the context. The study also employed a Genetic algorithm to enhance overall accuracy by integrating responses from multiple models. Specific performance highlights for ahotrod/electra_large_discriminator_squad2_512 include 65.92% accuracy on bioasq10b-factoid, 96.45% on biomedical_cpgQA, 11.13% on QuAC, and 41.6% on the Question Answer Dataset. Separately, bert-large-uncased-whole-word-masking-finetuned-squad achieved 82% accuracy on the IELTS dataset, demonstrating model-specific strengths depending on the dataset type. <div>
arXiv:2512.00323v1 Announce Type: new 
Abstract: Context-based question answering (CBQA) models provide more accurate and relevant answers by considering the contextual information. They effectively extract specific information given a context, making them functional in various applications involving user support, information retrieval, and educational platforms. In this manuscript, we benchmarked the performance of 47 CBQA models from Hugging Face on eight different datasets. This study aims to identify the best-performing model across diverse datasets without additional fine-tuning. It is valuable for practical applications where the need to retrain models for specific datasets is minimized, streamlining the implementation of these models in various contexts. The best-performing models were trained on the SQuAD v2 or SQuAD v1 datasets. The best-performing model was ahotrod/electra_large_discriminator_squad2_512, which yielded 43\% accuracy across all datasets. We observed that the computation time of all models depends on the context length and the model size. The model's performance usually decreases with an increase in the answer length. Moreover, the model's performance depends on the context complexity. We also used the Genetic algorithm to improve the overall accuracy by integrating responses from other models. ahotrod/electra_large_discriminator_squad2_512 generated the best results for bioasq10b-factoid (65.92\%), biomedical\_cpgQA (96.45\%), QuAC (11.13\%), and Question Answer Dataset (41.6\%). Bert-large-uncased-whole-word-masking-finetuned-squad achieved an accuracy of 82\% on the IELTS dataset.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evidence-Guided Schema Normalization for Temporal Tabular Reasoning</title>
<link>https://arxiv.org/abs/2512.00329</link>
<guid>https://arxiv.org/abs/2512.00329</guid>
<content:encoded><![CDATA[
<div> Keywords: temporal reasoning, semi-structured tables, SQL queries, schema normalization, QA precision<br /><br />Summary:<br /><br />1. The paper addresses the challenge of temporal reasoning over evolving semi-structured tables in question answering (QA) systems.<br /><br />2. The authors propose a SQL-based methodology consisting of three steps: generating a third normal form (3NF) schema from Wikipedia infoboxes, generating SQL queries based on that schema, and executing these queries for answer retrieval.<br /><br />3. A key finding is that the quality of schema design significantly influences QA precision more than increasing model capacity, challenging current assumptions favoring larger models.<br /><br />4. The study establishes three principles for effective schema design: normalization that preserves the contextual integrity of data, semantic naming conventions that minimize ambiguity, and consistent temporal anchoring to maintain temporal relevance.<br /><br />5. The best performing system, combining the Gemini 2.5 Flash schema with Gemini-2.0-Flash generated queries, achieved an exact match (EM) score of 80.39, representing a 16.8% improvement over the baseline EM of 68.89.<br /><br />This work highlights the critical role of thoughtful schema design in improving temporal QA over semi-structured data, suggesting that careful database normalization and query generation strategies can outperform simply scaling model size. <div>
arXiv:2512.00329v1 Announce Type: new 
Abstract: Temporal reasoning over evolving semi-structured tables poses a challenge to current QA systems. We propose a SQL-based approach that involves (1) generating a 3NF schema from Wikipedia infoboxes, (2) generating SQL queries, and (3) query execution. Our central finding challenges model scaling assumptions: the quality of schema design has a greater impact on QA precision than model capacity. We establish three evidence-based principles: normalization that preserves context, semantic naming that reduces ambiguity, and consistent temporal anchoring. Our best configuration (Gemini 2.5 Flash schema + Gemini-2.0-Flash queries) achieves 80.39 EM, a 16.8\% improvement over the baseline (68.89 EM).
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assertion-Conditioned Compliance: A Provenance-Aware Vulnerability in Multi-Turn Tool-Calling Agents</title>
<link>https://arxiv.org/abs/2512.00332</link>
<guid>https://arxiv.org/abs/2512.00332</guid>
<content:encoded><![CDATA[
<div> Multi-turn tool-calling, Assertion-Conditioned Compliance, user-sourced assertions, function-sourced assertions, model robustness<br /><br />Summary:<br /><br />1. The paper addresses multi-turn tool-calling large language models (LLMs) that enable AI assistants to invoke external APIs or tools across several user interactions, which are essential for both simple and critical tasks in industries like business, medicine, and finance.<br /><br />2. Despite advances demonstrated by benchmarks such as the Berkeley Function-Calling Leaderboard (BFCL) and models like Salesforce's xLAM V2, concerns remain about LLMs' robustness in real-world, multi-turn dialogue settings, especially for safety-critical applications.<br /><br />3. The authors introduce a new evaluation framework called Assertion-Conditioned Compliance (A-CC), which measures model behavior in multi-turn function-calling contexts when faced with potentially misleading assertions.<br /><br />4. A-CC evaluates model vulnerability across two key assertion types: user-sourced assertions (USAs), which assess a model's tendency to agree with plausible but incorrect user beliefs, and function-sourced assertions (FSAs), which test compliance with plausible yet contradictory policies originating from outdated or conflicting system tools.<br /><br />5. Experiments demonstrate that current models show significant susceptibility to both sycophancy towards USAs and conflict compliance with FSAs, highlighting a critical, previously underexplored weakness in deployed AI agents that could impact trust and safety. <div>
arXiv:2512.00332v1 Announce Type: new 
Abstract: Multi-turn tool-calling LLMs (models capable of invoking external APIs or tools across several user turns) have emerged as a key feature in modern AI assistants, enabling extended dialogues from benign tasks to critical business, medical, and financial operations. Yet implementing multi-turn pipelines remains difficult for many safety-critical industries due to ongoing concerns regarding model resilience. While standardized benchmarks such as the Berkeley Function-Calling Leaderboard (BFCL) have underpinned confidence concerning advanced function-calling models (like Salesforce's xLAM V2), there is still a lack of visibility into multi-turn conversation-level robustness, especially given their exposure to real-world systems. In this paper, we introduce Assertion-Conditioned Compliance (A-CC), a novel evaluation paradigm for multi-turn function-calling dialogues. A-CC provides holistic metrics that evaluate a model's behavior when confronted with misleading assertions originating from two distinct vectors: (1) user-sourced assertions (USAs), which measure sycophancy toward plausible but misinformed user beliefs, and (2) function-sourced assertions (FSAs), which measure compliance with plausible but contradictory system policies (e.g., stale hints from unmaintained tools). Our results show that models are highly vulnerable to both USA sycophancy and FSA policy conflicts, confirming A-CC as a critical, latent vulnerability in deployed agents.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages</title>
<link>https://arxiv.org/abs/2512.00333</link>
<guid>https://arxiv.org/abs/2512.00333</guid>
<content:encoded><![CDATA[
<div> Keywords: IndicParam, low-resource Indic languages, large language models, multilingual benchmark, question format diversity<br /><br />Summary:<br /><br />This paper introduces IndicParam, a comprehensive benchmark comprising over 13,000 human-curated multiple-choice questions spanning 11 low- and extremely low-resource Indic languages, including Nepali, Gujarati, Marathi, Odia, Dogri, Maithili, Rajasthani, Sanskrit, Bodo, Santali, Konkani, plus a Sanskrit-English code-mixed set. The study evaluates 19 large language models (LLMs), both proprietary and open-weight, highlighting that even the best performer, GPT-5, achieves only 45.0% average accuracy, followed closely by DeepSeek-3.2 (43.1%) and Claude-4.5 (42.7%). Each question is annotated to distinguish between knowledge-oriented items and purely linguistic ones, allowing differentiation between factual recall and grammatical skills. The benchmark also tests LLM capabilities across various question formats beyond traditional multiple-choice, including list-based matching, assertion-reason pairs, and sequence ordering. Findings reveal significant limitations of current cross-lingual transfer techniques for Indic languages, underscoring the challenge these languages pose for LLMs. IndicParam aims to push forward research in this area by providing a rigorous and diverse testing ground for future multilingual model development. The dataset and evaluation scripts are publicly accessible for further research and development at the provided Hugging Face and GitHub links. <div>
arXiv:2512.00333v1 Announce Type: new 
Abstract: While large language models excel on high-resource multilingual tasks, low- and extremely low-resource Indic languages remain severely under-evaluated. We present IndicParam, a human-curated benchmark of over 13,000 multiple-choice questions covering 11 such languages (Nepali, Gujarati, Marathi, Odia as low-resource; Dogri, Maithili, Rajasthani, Sanskrit, Bodo, Santali, Konkani as extremely low-resource) plus Sanskrit-English code-mixed set. We evaluated 19 LLMs, both proprietary and open-weights, which reveals that even the top-performing GPT-5 reaches only 45.0% average accuracy, followed by DeepSeek-3.2 (43.1) and Claude-4.5 (42.7). We additionally label each question as knowledge-oriented or purely linguistic to discriminate factual recall from grammatical proficiency. Further, we assess the ability of LLMs to handle diverse question formats-such as list-based matching, assertion-reason pairs, and sequence ordering-alongside conventional multiple-choice questions. IndicParam provides insights into limitations of cross-lingual transfer and establishes a challenging benchmark for Indic languages. The dataset is available at https://huggingface.co/datasets/bharatgenai/IndicParam. Scripts to run benchmark are present at https://github.com/ayushbits/IndicParam.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CourseTimeQA: A Lecture-Video Benchmark and a Latency-Constrained Cross-Modal Fusion Method for Timestamped QA</title>
<link>https://arxiv.org/abs/2512.00360</link>
<guid>https://arxiv.org/abs/2512.00360</guid>
<content:encoded><![CDATA[
<div> Keywords: timestamped question answering, educational lecture videos, CrossFusion-RAG, latency-constrained retrieval, temporal consistency<br /><br />Summary:<br /><br />This paper addresses the problem of timestamped question answering over educational lecture videos within strict latency and memory constraints suitable for a single GPU. The authors introduce CourseTimeQA, a dataset consisting of 52.3 hours of video content and 902 queries spanning six courses, designed to benchmark retrieval and answer synthesis performance. They propose CrossFusion-RAG, a lightweight cross-modal retriever architecture that leverages frozen encoders combined with a learned vision projection (512→768 dimensions), shallow query-agnostic cross-attention mechanisms over ASR (automatic speech recognition) and video frames, and a temporal-consistency regularizer to enhance robustness in temporal localization. A small cross-attentive reranker further refines retrieval quality. CrossFusion-RAG demonstrates significant performance improvements over a strong BLIP-2 baseline retriever in terms of nDCG@10 (+0.10) and MRR (+0.08) while maintaining a median end-to-end latency of approximately 1.55 seconds on a single NVIDIA A100 GPU. The study includes comprehensive comparisons against competitive baselines such as zero-shot CLIP multi-frame pooling, CLIP combined with cross-encoder reranking and MMR, learned late-fusion gating, text-only hybrid retrieval methods, caption-augmented text retrieval, and non-learned temporal smoothing, all evaluated under matched hardware and indexing conditions. Additional analyses assess robustness against ASR noise (quantified by word error rate quartiles) and provide diagnostic evaluations of temporal localization accuracy. Full training and tuning details are made available to facilitate reproducibility and benchmarking across future research. <div>
arXiv:2512.00360v1 Announce Type: new 
Abstract: We study timestamped question answering over educational lecture videos under a single-GPU latency/memory budget. Given a natural-language query, the system retrieves relevant timestamped segments and synthesizes a grounded answer. We present CourseTimeQA (52.3 h, 902 queries across six courses) and a lightweight, latency-constrained cross-modal retriever (CrossFusion-RAG) that combines frozen encoders, a learned 512->768 vision projection, shallow query-agnostic cross-attention over ASR and frames with a temporal-consistency regularizer, and a small cross-attentive reranker. On CourseTimeQA, CrossFusion-RAG improves nDCG@10 by 0.10 and MRR by 0.08 over a strong BLIP-2 retriever while achieving approximately 1.55 s median end-to-end latency on a single A100. Closest comparators (zero-shot CLIP multi-frame pooling; CLIP + cross-encoder reranker + MMR; learned late-fusion gating; text-only hybrid with cross-encoder reranking and its MMR variant; caption-augmented text retrieval; non-learned temporal smoothing) are evaluated under matched hardware and indexing. We report robustness across ASR noise (WER quartiles), diagnostics for temporal localization, and full training/tuning details to support reproducible comparison.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating the Threshold Priming Effect in Large Language Model-Based Relevance Judgments via Personality Infusing</title>
<link>https://arxiv.org/abs/2512.00390</link>
<guid>https://arxiv.org/abs/2512.00390</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, priming effects, Big Five personality, relevance labeling, personality prompting<br /><br />Summary: This study investigates the influence of simulated Big Five personality profiles in Large Language Models (LLMs) on the phenomenon of priming in relevance labeling tasks. Priming effects occur when prior relevance judgments bias subsequent ones, a challenge for the reliability of LLM-based evaluations. The research utilizes multiple LLMs and evaluates their performance on the TREC 2021 and 2022 Deep Learning Track datasets. Results reveal that certain personality traits, notably High Openness and Low Neuroticism, consistently reduce priming susceptibility, suggesting that these traits can serve as mitigating factors against bias. However, the study also finds variability in which personality profile is most effective, depending on the specific LLM and the nature of the task. By linking psychological theories about personality and cognitive biases with LLM behavior, the authors propose "personality prompting" as a novel method to control and reduce threshold priming effects. This approach represents a promising connection between human psychological insights and the operational tuning of LLMs for improved evaluation accuracy and robustness in relevance labeling scenarios. <div>
arXiv:2512.00390v1 Announce Type: new 
Abstract: Recent research has explored LLMs as scalable tools for relevance labeling, but studies indicate they are susceptible to priming effects, where prior relevance judgments influence later ones. Although psychological theories link personality traits to such biases, it is unclear whether simulated personalities in LLMs exhibit similar effects. We investigate how Big Five personality profiles in LLMs influence priming in relevance labeling, using multiple LLMs on TREC 2021 and 2022 Deep Learning Track datasets. Our results show that certain profiles, such as High Openness and Low Neuroticism, consistently reduce priming susceptibility. Additionally, the most effective personality in mitigating priming may vary across models and task types. Based on these findings, we propose personality prompting as a method to mitigate threshold priming, connecting psychological evidence with LLM-based evaluation practices.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Taxonomy of Errors in English as she is spoke: Toward an AI-Based Method of Error Analysis for EFL Writing Instruction</title>
<link>https://arxiv.org/abs/2512.00392</link>
<guid>https://arxiv.org/abs/2512.00392</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-assisted error analysis, Large Language Models, linguistic taxonomy, English writing errors, EFL instruction<br /><br />Summary:<br /><br />This study presents the development of an AI-assisted error analysis system aimed at identifying, categorizing, and correcting English writing errors. The system leverages Large Language Models (LLMs) such as Claude 3.5 Sonnet and DeepSeek R1 to analyze errors based on a detailed taxonomy inspired by linguistic theories from Corder (1967), Richards (1971), and James (1998). Errors are classified at both the word and sentence levels, addressing spelling, grammar, and punctuation issues. The implementation uses Python-coded API calls, enabling the system to provide nuanced feedback beyond traditional rubric-based assessments. Initial testing focused on isolated errors facilitated refining the taxonomy and resolving challenges related to overlapping error categories. Final evaluation utilized the 1855 text "English as she is spoke" by Jose da Fonseca, which contains authentic and complex linguistic errors, to test the system’s multi-layered analytical capabilities. The AI demonstrated success in identifying various error types but showed limitations in contextual understanding and occasionally introduced new error categories when encountering unclassified errors. The research highlights AI’s potential to revolutionize EFL instruction through automated and detailed error feedback, while noting the necessity for further improvements in contextual accuracy and expansion into stylistic and discourse-level error analysis. <div>
arXiv:2512.00392v1 Announce Type: new 
Abstract: This study describes the development of an AI-assisted error analysis system designed to identify, categorize, and correct writing errors in English. Utilizing Large Language Models (LLMs) like Claude 3.5 Sonnet and DeepSeek R1, the system employs a detailed taxonomy grounded in linguistic theories from Corder (1967), Richards (1971), and James (1998). Errors are classified at both word and sentence levels, covering spelling, grammar, and punctuation. Implemented through Python-coded API calls, the system provides granular feedback beyond traditional rubric-based assessments. Initial testing on isolated errors refined the taxonomy, addressing challenges like overlapping categories. Final testing used "English as she is spoke" by Jose da Fonseca (1855), a text rich with authentic linguistic errors, to evaluate the system's capacity for handling complex, multi-layered analysis. The AI successfully identified diverse error types but showed limitations in contextual understanding and occasionally generated new error categories when encountering uncoded errors. This research demonstrates AI's potential to transform EFL instruction by automating detailed error analysis and feedback. While promising, further development is needed to improve contextual accuracy and expand the taxonomy to stylistic and discourse-level errors.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CryptoBench: A Dynamic Benchmark for Expert-Level Evaluation of LLM Agents in Cryptocurrency</title>
<link>https://arxiv.org/abs/2512.00417</link>
<guid>https://arxiv.org/abs/2512.00417</guid>
<content:encoded><![CDATA[
<div> Keywords: CryptoBench, Large Language Models, cryptocurrency, benchmark, predictive analysis<br /><br />Summary:<br /><br />This paper introduces CryptoBench, the first expert-curated and dynamic benchmark specifically tailored for evaluating Large Language Model (LLM) agents in the cryptocurrency domain, a field characterized by extreme time sensitivity, adversarial information environments, and the need for data synthesis from diverse specialized sources like on-chain intelligence and DeFi dashboards. Unlike general-purpose benchmarks, CryptoBench consists of 50 real-world questions monthly, designed by crypto-native professionals to reflect authentic analyst workflows. Tasks are categorized into four quadrants—Simple Retrieval, Complex Retrieval, Simple Prediction, and Complex Prediction—allowing detailed assessment of both foundational data gathering and advanced analytical and forecasting skills of LLM agents. The authors evaluated ten LLMs in direct and agentic frameworks, revealing a clear performance hierarchy. A significant finding is the "retrieval-prediction imbalance," where many top models excel at factual data retrieval but show weaknesses in predictive analysis, indicating agents may appear factually grounded yet lack deeper analytical synthesis capabilities. This benchmark offers a challenging and valuable scenario that better measures real-world LLM competence in professional crypto analysis than existing benchmarks. <div>
arXiv:2512.00417v1 Announce Type: new 
Abstract: This paper introduces CryptoBench, the first expert-curated, dynamic benchmark designed to rigorously evaluate the real-world capabilities of Large Language Model (LLM) agents in the uniquely demanding and fast-paced cryptocurrency domain. Unlike general-purpose agent benchmarks for search and prediction, professional crypto analysis presents specific challenges: \emph{extreme time-sensitivity}, \emph{a highly adversarial information environment}, and the critical need to synthesize data from \emph{diverse, specialized sources}, such as on-chain intelligence platforms and real-time Decentralized Finance (DeFi) dashboards. CryptoBench thus serves as a much more challenging and valuable scenario for LLM agent assessment. To address these challenges, we constructed a live, dynamic benchmark featuring 50 questions per month, expertly designed by crypto-native professionals to mirror actual analyst workflows. These tasks are rigorously categorized within a four-quadrant system: Simple Retrieval, Complex Retrieval, Simple Prediction, and Complex Prediction. This granular categorization enables a precise assessment of an LLM agent's foundational data-gathering capabilities alongside its advanced analytical and forecasting skills.
  Our evaluation of ten LLMs, both directly and within an agentic framework, reveals a performance hierarchy and uncovers a failure mode. We observe a \textit{retrieval-prediction imbalance}, where many leading models, despite being proficient at data retrieval, demonstrate a pronounced weakness in tasks requiring predictive analysis. This highlights a problematic tendency for agents to appear factually grounded while lacking the deeper analytical capabilities to synthesize information.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCALE: Selective Resource Allocation for Overcoming Performance Bottlenecks in Mathematical Test-time Scaling</title>
<link>https://arxiv.org/abs/2512.00466</link>
<guid>https://arxiv.org/abs/2512.00466</guid>
<content:encoded><![CDATA[
<div> Keywords: test-time compute scaling, large language models, selective resource allocation, problem difficulty assessment, dual-process theory<br /><br />Summary:<br /><br />1. The paper addresses limitations in current test-time compute scaling methods used to enhance mathematical reasoning in large language models (LLMs), where uniform allocation of resources across all sub-problems leads to inefficiencies and performance bottlenecks.<br /><br />2. The authors introduce SCALE (Selective Resource Allocation), a novel framework inspired by dual-process theory that dynamically allocates computational resources depending on the difficulty of each reasoning sub-problem.<br /><br />3. SCALE comprises four stages: decomposition of the original problem into sequential sub-problems, assessment of each sub-problem’s difficulty to classify it as routine or challenging, selective assignment of processing mode—System 1 for simple tasks and System 2 for complex ones—and sequential execution with context propagation.<br /><br />4. By focusing heavier computational effort on challenging sub-problems while efficiently handling routine operations, SCALE improves both resource utilization and reasoning accuracy.<br /><br />5. Experimental results demonstrate that SCALE significantly outperforms uniform scaling methods, achieving up to a 13.75 percentage point increase in accuracy on the AIME25 benchmark (from 57.50% to 71.25%), while simultaneously reducing computational cost by 33%-53%, marking a substantial advance in test-time compute scaling for LLMs. <div>
arXiv:2512.00466v1 Announce Type: new 
Abstract: Test-time compute scaling has emerged as a powerful paradigm for enhancing mathematical reasoning in large language models (LLMs) by allocating additional computational resources during inference. However, current methods employ uniform resource distribution across all reasoning sub-problems, creating fundamental bottlenecks where challenging sub-problems receive insufficient attention while routine operations consume disproportionate resources. This uniform allocation creates performance bottlenecks where additional computational resources yield diminishing returns. Inspired by dual-process theory, we propose \textbf{SCALE} (Selective Resource Allocation), a framework that selectively allocates computational resources based on sub-problem difficulty. SCALE operates through four stages: (1) problem decomposition into sequential reasoning sub-problems, (2) difficulty assessment of each sub-problem to distinguish between routine operations and computationally challenging sub-problems, (3) selective processing mode assignment between System 1 for simple sub-problems and System 2 for complex ones, and (4) sequential execution with context propagation. By concentrating resources on challenging sub-problems while processing routine operations efficiently, SCALE achieves substantial performance improvements with superior resource utilization. Extensive experiments demonstrate that SCALE significantly outperforms uniform scaling baselines, achieving accuracy improvements of up to 13.75 percentage points (57.50% to 71.25% on AIME25) while reducing computational costs by 33%-53%, representing a major advance in test-time scaling that addresses fundamental limitations of current approaches.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CACARA: Cross-Modal Alignment Leveraging a Text-Centric Approach for Cost-Effective Multimodal and Multilingual Learning</title>
<link>https://arxiv.org/abs/2512.00496</link>
<guid>https://arxiv.org/abs/2512.00496</guid>
<content:encoded><![CDATA[
<div> Multimodal learning, Multilingual models, Emergent alignment, Efficient training, Audio-text retrieval<br /><br />Summary:<br /><br />This paper introduces CACARA, a novel multimodal and multilingual architecture that leverages emergent alignment learning to integrate new data modalities into existing multimodal models efficiently, eliminating the need for resource-heavy full retraining. Unlike traditional methods that require extensive training across all modalities and languages, CACARA supports the seamless addition of new modalities by fine-tuning only on data aligned with the English language. Remarkably, this approach enables the model to acquire multilingual capabilities, supporting over 100 languages, despite training on monolingual text data only and without modifying the text encoder. The emergent alignment paradigm preserves previously learned knowledge while achieving a training cost comparable to monolingual models. Experimentally, CACARA demonstrates significant performance gains, achieving up to 14.24 percentage points improvement in R@1 audio-to-text retrieval, surpassing state-of-the-art multimodal models. This work highlights a scalable, cost-effective pathway for expanding multimodal models across languages and modalities, advancing the integration of complex, diverse data types in deep learning tasks. <div>
arXiv:2512.00496v1 Announce Type: new 
Abstract: As deep learning models evolve, new applications and challenges are rapidly emerging. Tasks that once relied on a single modality, such as text, images, or audio, are now enriched by seamless interactions between multimodal data. These connections bridge information gaps: an image can visually materialize a text, while audio can add context to an image. Researchers have developed numerous multimodal models, but most rely on resource-intensive training across multiple modalities. Similarly, extending these models to new languages often follows the same resource-heavy training strategy. In this work, we propose a multimodal and multilingual architecture, CACARA, trained through emergent alignment learning, enabling the seamless integration of new modalities into an existing bimodal/multimodal model without requiring full retraining. This work breaks new ground by demonstrating that this emergent alignment paradigm can unlock multilingual capabilities from monolingual training. By fine-tuning the newly incorporated modality only on data aligned with the English language, our model develops support for over 100 languages without explicit multilingual pretraining or tuning of the text encoder. Such emergent multimodal and multilingual properties are gained efficiently, preserving previously learned knowledge at a training cost comparable to that of a monolingual model. Our strategy achieves up to a 14.24 percentage points improvement in R@1 audio-to-text retrieval, outperforming state-of-the-art multimodal models -- all without the heavy computational cost of retraining across every modality and language.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>G-KV: Decoding-Time KV Cache Eviction with Global Attention</title>
<link>https://arxiv.org/abs/2512.00504</link>
<guid>https://arxiv.org/abs/2512.00504</guid>
<content:encoded><![CDATA[
<div> KV cache compression, global scoring, reinforcement learning, knowledge distillation, large language models<br /><br />Summary: Recent large language models (LLMs) capable of complex reasoning face substantial computational and memory challenges due to the processing of long input sequences. To improve efficiency, KV cache compression has been adopted, which reduces memory usage during attention computation by compressing stored key-value pairs. Existing approaches primarily focus on prompt compression or token eviction strategies based on local attention scores, which do not adequately capture the long-term importance of tokens. This paper proposes G-KV, a novel KV cache eviction method that integrates a global scoring mechanism by combining both local and historical attention scores, allowing a more accurate evaluation of token relevance over time. Furthermore, to enhance model performance with compressed KV caches, the authors introduce post-training optimization techniques including reinforcement learning and knowledge distillation. These methods help adapt the models to operate effectively in compressed settings without sacrificing accuracy. The combined approach of G-KV and the training techniques significantly improve efficiency and maintain reasoning capabilities of LLMs in handling long sequence inputs. The code for implementing G-KV is publicly available, facilitating further research and application in the area of efficient reasoning with LLMs. <div>
arXiv:2512.00504v1 Announce Type: new 
Abstract: Recent reasoning large language models (LLMs) excel in complex tasks but encounter significant computational and memory challenges due to long sequence lengths. KV cache compression has emerged as an effective approach to greatly enhance the efficiency of reasoning. However, existing methods often focus on prompt compression or token eviction with local attention score, overlooking the long-term importance of tokens. We propose G-KV, a KV cache eviction method that employs a global scoring mechanism, combining local and historical attention scores to more accurately assess token importance. Additionally, we introduce post-training techniques, including reinforcement learning and distillation, to optimize models for compressed KV cache settings. The code of this paper is available on: https://github.com/microsoft/G-KV.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Developing a Comprehensive Framework for Sentiment Analysis in Turkish</title>
<link>https://arxiv.org/abs/2512.00515</link>
<guid>https://arxiv.org/abs/2512.00515</guid>
<content:encoded><![CDATA[
<div> Keywords: sentiment analysis, Turkish, polarity lexicon, neural network, word embeddings<br /><br />Summary:<br /><br />In this thesis, a comprehensive sentiment analysis framework was developed primarily for Turkish, with additional approaches specific to English. Five major and three minor contributions were made. First, a novel and effective feature set combining unsupervised, semi-supervised, and supervised metrics was generated, which, when used with classical machine learning methods, outperformed neural network models across multiple Turkish and English datasets. Second, a semi-supervised, domain-specific polarity lexicon was created for Turkish corpora, marking the first such method applied in this language. Third, fine morphological analysis was performed for Turkish sentiment classification by determining morpheme polarities, offering a method adaptable to other morphologically-rich or agglutinative languages. Fourth, a novel neural network architecture combining recurrent and recursive models was built for English sentiment analysis. Fifth, new word embeddings incorporating sentiment, syntactic, semantic, and lexical features were developed for both Turkish and English, including a redefinition of context windows as subclauses in English, applicable to other NLP tasks. Minor contributions included aspect-based sentiment methods for Turkish, parameter redefinition in semi-supervised approaches, and aspect term extraction techniques for English. This work represents the most detailed, comprehensive study on Turkish sentiment analysis as of July 2020 and advances opinion classification in English. <div>
arXiv:2512.00515v1 Announce Type: new 
Abstract: In this thesis, we developed a comprehensive framework for sentiment analysis that takes its many aspects into account mainly for Turkish. We have also proposed several approaches specific to sentiment analysis in English only. We have accordingly made five major and three minor contributions. We generated a novel and effective feature set by combining unsupervised, semi-supervised, and supervised metrics. We then fed them as input into classical machine learning methods, and outperformed neural network models for datasets of different genres in both Turkish and English. We created a polarity lexicon with a semi-supervised domain-specific method, which has been the first approach applied for corpora in Turkish. We performed a fine morphological analysis for the sentiment classification task in Turkish by determining the polarities of morphemes. This can be adapted to other morphologically-rich or agglutinative languages as well. We have built a novel neural network architecture, which combines recurrent and recursive neural network models for English. We built novel word embeddings that exploit sentiment, syntactic, semantic, and lexical characteristics for both Turkish and English. We also redefined context windows as subclauses in modelling word representations in English. This can also be applied to other linguistic fields and natural language processing tasks. We have achieved state-of-the-art and significant results for all these original approaches. Our minor contributions include methods related to aspect-based sentiment in Turkish, parameter redefinition in the semi-supervised approach, and aspect term extraction techniques for English. This thesis can be considered the most detailed and comprehensive study made on sentiment analysis in Turkish as of July, 2020. Our work has also contributed to the opinion classification problem in English.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Catch Me If You Can: How Smaller Reasoning Models Pretend to Reason with Mathematical Fidelity</title>
<link>https://arxiv.org/abs/2512.00552</link>
<guid>https://arxiv.org/abs/2512.00552</guid>
<content:encoded><![CDATA[
<div> Mathematical reasoning, diagnostic framework, language models, reasoning fidelity, pattern matching  

<br /><br />Summary:  
This paper critiques the prevalent use of answer accuracy as the primary metric for evaluating mathematical reasoning in language models, highlighting how it can mask core logical computation failures. The authors introduce a novel diagnostic framework designed to differentiate genuine mathematical reasoning from superficial pattern matching using four axes: forward-backward consistency, transitivity coverage, counterfactual sensitivity, and perturbation robustness. Applying this framework to the Qwen3-0.6B model on the MenatQA dataset reveals significant discrepancies between conventional accuracy (over 70%) and deeper reasoning metrics like backward consistency (15%) and transitivity coverage (32.2%), exposing brittle sensitivity to input perturbations. These findings suggest the model’s reliance on pattern recognition rather than authentic logical processing. While the empirical study focuses on a single small-scale 600M-parameter model, the introduced diagnostic framework is model-agnostic and can be applied broadly across model types and sizes. The authors also provide their evaluation protocols publicly, encouraging the research community to adopt more rigorous assessments of reasoning fidelity. This work pushes beyond surface-level answer correctness to establish methods for verifiable and trustworthy mathematical reasoning evaluation in language models. <div>
arXiv:2512.00552v1 Announce Type: new 
Abstract: Current evaluation of mathematical reasoning in language models relies primarily on answer accuracy, potentially masking fundamental failures in logical computation. We introduce a diagnostic framework that distinguishes genuine mathematical reasoning from superficial pattern matching through four complementary axes: forward-backward consistency, transitivity coverage, counterfactual sensitivity, and perturbation robustness. Through a case study applying this framework to Qwen3-0.6B on the MenatQA dataset, we reveal a striking disconnect between surface performance and reasoning fidelity. While the model achieves reasonable answer accuracy (70%+), it demonstrates poor backward consistency (15%), limited transitivity coverage (32.2%), and brittle sensitivity to perturbations. Our diagnostics expose reasoning failures invisible to traditional accuracy metrics, suggesting that this small model relies heavily on pattern matching rather than genuine logical computation. While our empirical findings are based on a single 600M-parameter model, the diagnostic framework itself is model-agnostic and generalizable. We release our evaluation protocols to enable the research community to assess reasoning fidelity across different model scales and architectures, moving beyond surface-level accuracy toward verifiable mathematical reasoning.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Slovak Conceptual Dictionary</title>
<link>https://arxiv.org/abs/2512.00579</link>
<guid>https://arxiv.org/abs/2512.00579</guid>
<content:encoded><![CDATA[
<div> Keywords: Slovak language, dictionary tools, natural language processing, low-resourced languages, linguistic resources<br /><br />Summary:<br /><br />1. This article addresses the challenge of limited dictionary data availability in many languages, focusing on the Slovak language, which is considered low-resourced in terms of linguistic data.<br /><br />2. The authors introduce a new conceptual dictionary specifically designed for the Slovak language, marking the first linguistic tool of this kind for Slovak.<br /><br />3. This resource aims to support various natural language processing (NLP) tasks such as those requiring lexicons, word form dictionaries, or knowledge bases, which are currently lacking in Slovak.<br /><br />4. The scarcity of machine-readable linguistic data for Slovak hinders automated processing tasks, resulting in weaker performance on these tasks compared to higher-resourced languages.<br /><br />5. By providing this new conceptual dictionary, the article contributes a foundational resource that is expected to improve NLP outcomes and enable easier automated processing of Slovak text in future research and applications. <div>
arXiv:2512.00579v1 Announce Type: new 
Abstract: When solving tasks in the field of natural language processing, we sometimes need dictionary tools, such as lexicons, word form dictionaries or knowledge bases. However, the availability of dictionary data is insufficient in many languages, especially in the case of low resourced languages. In this article, we introduce a new conceptual dictionary for the Slovak language as the first linguistic tool of this kind. Since Slovak language is a language with limited linguistic resources and there are currently not available any machine-readable linguistic data sources with a sufficiently large volume of data, many tasks which require automated processing of Slovak text achieve weaker results compared to other languages and are almost impossible to solve.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models</title>
<link>https://arxiv.org/abs/2512.00590</link>
<guid>https://arxiv.org/abs/2512.00590</guid>
<content:encoded><![CDATA[
<div> Keywords: Knowledge Graphs, Large Language Models, Wikontic, Knowledge Extraction, Benchmark Performance<br /><br />Summary: This paper introduces Wikontic, a novel multi-stage pipeline designed to construct knowledge graphs (KGs) from open-domain text efficiently and accurately. The pipeline extracts candidate triplets with qualifiers, applies Wikidata-based constraints on types and relations, and performs entity normalization to reduce duplication, resulting in compact, ontology-consistent, and well-connected KGs. Wikontic demonstrates excellent performance on multiple benchmarks: on MuSiQue, 96% of generated triplets contain the correct answer entity; on HotpotQA, its triplets-only approach achieves a 76.0 F1 score, and on MuSiQue, 59.8 F1, matching or exceeding retrieval-augmented generation baselines that rely on textual context. Additionally, Wikontic attains state-of-the-art information retention with an 86% score on the MINE-1 benchmark, surpassing previous KG construction methods. The system is highly efficient, requiring fewer than 1,000 output tokens for KG construction, which is about three times less than AriGraph and less than one-twentieth of GraphRAG. Overall, Wikontic improves the intrinsic quality of generated KGs and offers a scalable, effective solution for integrating structured knowledge into large language models. <div>
arXiv:2512.00590v1 Announce Type: new 
Abstract: Knowledge graphs (KGs) provide structured, verifiable grounding for large language models (LLMs), but current LLM-based systems commonly use KGs as auxiliary structures for text retrieval, leaving their intrinsic quality underexplored. In this work, we propose Wikontic, a multi-stage pipeline that constructs KGs from open-domain text by extracting candidate triplets with qualifiers, enforcing Wikidata-based type and relation constraints, and normalizing entities to reduce duplication. The resulting KGs are compact, ontology-consistent, and well-connected; on MuSiQue, the correct answer entity appears in 96% of generated triplets. On HotpotQA, our triplets-only setup achieves 76.0 F1, and on MuSiQue 59.8 F1, matching or surpassing several retrieval-augmented generation baselines that still require textual context. In addition, Wikontic attains state-of-the-art information-retention performance on the MINE-1 benchmark (86%), outperforming prior KG construction methods. Wikontic is also efficient at build time: KG construction uses less than 1,000 output tokens, about 3$\times$ fewer than AriGraph and $<$1/20 of GraphRAG. The proposed pipeline enhances the quality of the generated KG and offers a scalable solution for leveraging structured knowledge in LLMs.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prism: A Minimal Compositional Metalanguage for Specifying Agent Behavior</title>
<link>https://arxiv.org/abs/2512.00611</link>
<guid>https://arxiv.org/abs/2512.00611</guid>
<content:encoded><![CDATA[
<div> Keywords: Prism, metalanguage, software agents, compositional grammar, tool-using agents  

<br /><br />Summary:  
Prism is a concise, compositional metalanguage designed for specifying the behavior of tool-using software agents. It avoids introducing arbitrary control constructs by centering around a fixed core context called Core1, which includes a minimal grammar with categories such as numbers, strings, user prompts, and tools, alongside abstract combinators for booleans, predicates, pairs, and lists. Agent policies are expressed as ordinary functional-style expressions using a single abstraction operator, where conditionals operate as selections between alternatives rather than imperative if-else statements. The language supports extensibility through domain-specific context mini-grammars that add new categories, predicates, and external tools while preserving the core's compositional framework. Demonstrated examples span various domains, including thermostat control, home security, e-commerce recommendation, and medical monitoring, illustrating how natural language decision rules can be translated into executable, inspectable policies. Linguistically, Prism enforces a clear separation of concerns by maintaining a reusable core grammar and domain-specific lexicons, treating external tools as interfaces connecting policies to the real world. From an engineering standpoint, Prism provides a compact, explicit interface language that facilitates analysis, verification, and enforcement of safety constraints on agent control actions. <div>
arXiv:2512.00611v1 Announce Type: new 
Abstract: Prism is a small, compositional metalanguage for specifying the behaviour of tool-using software agents. Rather than introducing ad hoc control constructs, Prism is built around a fixed core context, Core1, which provides a minimal background grammar of categories numbers, strings, user prompts, tools together with abstract combinators for booleans, predicates, pairs, and lists. Agent policies are written as ordinary expressions using a single abstraction operator so that conditionals appear as selections between alternatives instead of imperative if-else blocks. Domains extend the core by defining their own context-mini-grammars that introduce new categories, predicates, and external tools while reusing the same compositional machinery. We illustrate this with worked examples from thermostat control, home security, e-commerce recommendation, and medical monitoring, showing how natural language decision rules can be mapped to inspectable, executable policies. From a linguistic perspective, Prism enforces a clear separation between a reusable grammar-like core and domain specific lexicons and treats tools as bridges between internal policy representations and the external world. From an engineering perspective, it offers a compact interface language for agent control, making the space of possible actions explicit and amenable to analysis, verification, and safety constraints.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ART: Adaptive Response Tuning Framework -- A Multi-Agent Tournament-Based Approach to LLM Response Optimization</title>
<link>https://arxiv.org/abs/2512.00617</link>
<guid>https://arxiv.org/abs/2512.00617</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, ELO ranking, multi-agent reasoning, consensus responses, response accuracy  

<br /><br />Summary:  
This paper introduces ART (Adaptive Response Tuning), a novel framework designed to improve the output quality of Large Language Models (LLMs). Traditional single-model responses often show inconsistency, hallucination, and variable quality across different query domains. ART addresses these limitations by leveraging a tournament-style ELO ranking system combined with multi-agent reasoning. Multiple LLM agents compete, critique, and collaborate through structured tournament workflows to generate consensus responses that surpass individual model outputs. The framework includes configurable tournament parameters, dynamic agent selection, and several consensus fusion strategies, enhancing flexibility and adaptability. Experimental evaluations reveal significant improvements in response accuracy, coherence, and reliability when compared to baseline single-model approaches. ART achieves an 8.4% enhancement in overall quality metrics and demonstrates high reliability with R22 values exceeding 0.96 in ELO rating convergence. The framework is scalable and production-ready, making it suitable for real-world applications that demand high-quality and vetted LLM responses. Overall, ART provides a systematic and effective approach to optimizing LLM outputs by fostering competition and collaboration among multiple agents. <div>
arXiv:2512.00617v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation. However, single-model responses often exhibit inconsistencies, hallucinations, and varying quality across different query domains. This paper presents ART (Adaptive Response Tuning), a novel framework that employs tournament-style ELO ranking and multi-agent reasoning to systematically optimize LLM outputs. By enabling multiple LLM agents to compete, critique, and collaborate through structured tournament workflows, ART produces consensus responses that outperform individual model outputs. Our framework introduces configurable tournament parameters, dynamic agent selection, and multiple consensus fusion strategies. Experimental evaluations demonstrate significant improvements in response accuracy, coherence, and reliability compared to baseline single-model approaches. The ART framework provides a scalable, production-ready solution for applications requiring high-quality, vetted LLM responses, achieving an 8.4% improvement in overall quality metrics and R22 values exceeding 0.96 in ELO rating convergence.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sycophancy Claims about Language Models: The Missing Human-in-the-Loop</title>
<link>https://arxiv.org/abs/2512.00656</link>
<guid>https://arxiv.org/abs/2512.00656</guid>
<content:encoded><![CDATA[
<div> Keywords: sycophantic responses, Large Language Models, methodological challenges, human perception, AI alignment<br /><br />Summary:<br /><br />This article addresses the phenomenon of sycophantic response patterns in Large Language Models (LLMs), which refers to the tendency of these models to provide overly agreeable or flattering answers. It reviews the existing methodological challenges involved in accurately measuring sycophancy in LLMs, emphasizing that there are five main operationalizations researchers currently use to define and evaluate this behavior. Although sycophancy is fundamentally a human-centered concept, the study points out that contemporary research often neglects to assess how humans perceive these sycophantic responses, thus missing a crucial part of understanding their impact. Additionally, the article discusses difficulties in clearly differentiating sycophantic responses from other related behaviors relevant to AI alignment, such as strategic responses or reward-seeking tendencies. To advance research in this area, the authors provide actionable recommendations aimed at refining measurement approaches, integrating human subjective evaluations, and clarifying conceptual boundaries. Overall, the paper contributes a critical examination of sycophancy in LLMs and highlights important directions for future investigations to better grasp and mitigate such response biases. <div>
arXiv:2512.00656v1 Announce Type: new 
Abstract: Sycophantic response patterns in Large Language Models (LLMs) have been increasingly claimed in the literature. We review methodological challenges in measuring LLM sycophancy and identify five core operationalizations. Despite sycophancy being inherently human-centric, current research does not evaluate human perception. Our analysis highlights the difficulties in distinguishing sycophantic responses from related concepts in AI alignment and offers actionable recommendations for future research.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graphing the Truth: Structured Visualizations for Automated Hallucination Detection in LLMs</title>
<link>https://arxiv.org/abs/2512.00663</link>
<guid>https://arxiv.org/abs/2512.00663</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, hallucinations, visual knowledge graphs, human-in-the-loop, enterprise AI<br /><br />Summary:<br /><br />This paper addresses the challenge of hallucinations in Large Language Models (LLMs) when applied in enterprise contexts, where models are integrated with proprietary, closed-source domain knowledge. These hallucinations stem from operational constraints such as limited context windows and discrepancies between the model’s pre-training data and the injected knowledge, resulting in confident but erroneous outputs that often evade human detection. Existing mitigation methods rely on expensive gold-standard question-answer curation or secondary model verification, which lack deterministic guarantees of correctness. To overcome these issues, the authors propose a novel framework that represents proprietary knowledge and generated content as interactive visual knowledge graphs. This approach enables users to visually trace model assertions back to their verified sources, highlighting confidence levels and potential hallucination zones transparently. The visual interface supports diagnosis of inconsistencies and identification of weak reasoning chains. Crucially, it facilitates a human-in-the-loop workflow, allowing end users to provide corrective feedback that feeds back into the system. This structured feedback loop aims to improve overall model reliability and continuously enhance response quality, making the interaction with LLMs more trustworthy and controllable in enterprise applications. <div>
arXiv:2512.00663v1 Announce Type: new 
Abstract: Large Language Models have rapidly advanced in their ability to interpret and generate natural language. In enterprise settings, they are frequently augmented with closed-source domain knowledge to deliver more contextually informed responses. However, operational constraints such as limited context windows and inconsistencies between pre-training data and supplied knowledge often lead to hallucinations, some of which appear highly credible and escape routine human review. Current mitigation strategies either depend on costly, large-scale gold-standard Q\&amp;A curation or rely on secondary model verification, neither of which offers deterministic assurance. This paper introduces a framework that organizes proprietary knowledge and model-generated content into interactive visual knowledge graphs. The objective is to provide end users with a clear, intuitive view of potential hallucination zones by linking model assertions to underlying sources of truth and indicating confidence levels. Through this visual interface, users can diagnose inconsistencies, identify weak reasoning chains, and supply corrective feedback. The resulting human-in-the-loop workflow creates a structured feedback loop that can enhance model reliability and continuously improve response quality.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Comparison of Human and ChatGPT Classification Performance on Complex Social Media Data</title>
<link>https://arxiv.org/abs/2512.00673</link>
<guid>https://arxiv.org/abs/2512.00673</guid>
<content:encoded><![CDATA[
<div> Keywords: ChatGPT, GPT-4, nuanced language, classification, performance evaluation  

<br /><br />Summary:  
This paper evaluates the performance of GPT-4, along with ChatGPT versions 3.5 and 4o, in the complex task of classifying and annotating datasets containing nuanced language. The authors design four distinct prompt styles to test how different inputs affect model output. They measure performance quantitatively using precision, recall, and F1 scores, and complement this with qualitative analysis. Results show that including label definitions in prompts can improve performance to an extent; however, GPT-4 still struggles with the subtlety required for accurate classification of nuanced language. The study identifies four qualitative findings that highlight specific areas where GPT-4 faces challenges. The comparison with human annotators reveals that although AI shows promise, caution is necessary when applying ChatGPT models to tasks involving complex linguistic subtleties. Overall, the paper emphasizes prudence in relying on generative AI tools like ChatGPT for nuanced language classification, suggesting further refinement and understanding are needed before deployment in sensitive annotation tasks. <div>
arXiv:2512.00673v1 Announce Type: new 
Abstract: Generative artificial intelligence tools, like ChatGPT, are an increasingly utilized resource among computational social scientists. Nevertheless, there remains space for improved understanding of the performance of ChatGPT in complex tasks such as classifying and annotating datasets containing nuanced language. Method. In this paper, we measure the performance of GPT-4 on one such task and compare results to human annotators. We investigate ChatGPT versions 3.5, 4, and 4o to examine performance given rapid changes in technological advancement of large language models. We craft four prompt styles as input and evaluate precision, recall, and F1 scores. Both quantitative and qualitative evaluations of results demonstrate that while including label definitions in prompts may help performance, overall GPT-4 has difficulty classifying nuanced language. Qualitative analysis reveals four specific findings. Our results suggest the use of ChatGPT in classification tasks involving nuanced language should be conducted with prudence.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FastPOS: Language-Agnostic Scalable POS Tagging Framework Low-Resource Use Case</title>
<link>https://arxiv.org/abs/2512.00745</link>
<guid>https://arxiv.org/abs/2512.00745</guid>
<content:encoded><![CDATA[
<div> Keywords: transformer, POS tagging, low-resource languages, Bangla, Hindi

<br /><br />Summary:  
This study introduces a language-agnostic transformer-based framework for part-of-speech (POS) tagging specifically targeting low-resource languages, demonstrated through Bangla and Hindi. The framework’s adaptability is highlighted by the ability to switch from Bangla to Hindi with just three lines of framework-specific code, showcasing its portability and ease of modification. It achieves high token-level accuracy, recording 96.85% for Bangla and 97% for Hindi, and maintains strong F1 scores despite challenges such as dataset imbalance and linguistic overlap. However, performance discrepancies in specific POS categories indicate persistent issues related to dataset curation, emphasizing the importance of data quality. The success of the framework derives from the underlying transformer architecture, which is modular and allows for replacement with minimal coding changes, making it highly flexible. Its open-source nature fosters rapid cross-lingual adaptation, lowering barriers in model design and tuning. Consequently, this enables researchers to concentrate on crucial aspects such as linguistic preprocessing and refining datasets, which are vital steps toward advancing natural language processing capabilities for underrepresented languages. This approach addresses key limitations in current NLP efforts for low-resource settings by combining effectiveness, modularity, and accessibility. <div>
arXiv:2512.00745v1 Announce Type: new 
Abstract: This study proposes a language-agnostic transformer-based POS tagging framework designed for low-resource languages, using Bangla and Hindi as case studies. With only three lines of framework-specific code, the model was adapted from Bangla to Hindi, demonstrating effective portability with minimal modification. The framework achieves 96.85 percent and 97 percent token-level accuracy across POS categories in Bangla and Hindi while sustaining strong F1 scores despite dataset imbalance and linguistic overlap. A performance discrepancy in a specific POS category underscores ongoing challenges in dataset curation. The strong results stem from the underlying transformer architecture, which can be replaced with limited code adjustments. Its modular and open-source design enables rapid cross-lingual adaptation while reducing model design and tuning overhead, allowing researchers to focus on linguistic preprocessing and dataset refinement, which are essential for advancing NLP in underrepresented languages.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auxiliary-Hyperparameter-Free Sampling: Entropy Equilibrium for Text Generation</title>
<link>https://arxiv.org/abs/2512.00789</link>
<guid>https://arxiv.org/abs/2512.00789</guid>
<content:encoded><![CDATA[
<div> Token Sampling, Entropy Equilibrium Sampling, Large Language Models, Hyperparameter-free, Text Generation<br /><br />Summary:<br /><br />This paper introduces Entropy Equilibrium Sampling (EES), a novel token sampling method for text generation in large language models (LLMs). EES is designed to dynamically adjust the candidate token sets by balancing normalized entropy with probability mass, based on principles from information theory. Unlike existing sampling strategies, EES does not require additional hyperparameters, eliminating the need for extensive tuning and simplifying deployment. The study evaluates EES on various reasoning and generation tasks across different model architectures, demonstrating consistent strong performance across a wide range of temperature settings. The results indicate that EES achieves a good balance between accuracy, coherence, and diversity in generated text. By removing hyperparameter dependence, EES not only simplifies the use of LLMs but also enhances overall generation quality. Furthermore, the authors provide their code publicly, facilitating adoption and further research. This approach potentially advances practical applications of LLMs by addressing the complexity associated with sampling hyperparameters while maintaining or improving generation effectiveness. <div>
arXiv:2512.00789v1 Announce Type: new 
Abstract: Token sampling strategies critically influence text generation quality in large language models (LLMs). However, existing methods introduce additional hyperparameters, requiring extensive tuning and complicating deployment. We present Entropy Equilibrium Sampling (EES), an auxiliary hyperparameter-free approach inspired by information theory that can dynamically adjust candidate sets by balancing normalized entropy with probability mass. We evaluate EES on both reasoning and generation tasks across a range of model architectures. Our results show that EES consistently performs well across temperature settings, delivering competitive accuracy and coherence while maintaining diversity. By eliminating the need for hyperparameter tuning, EES greatly simplifies deployment while improving performance. Code is available at https://github.com/shuanncai/EES
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerating Bangla NLP Tasks with Automatic Mixed Precision: Resource-Efficient Training Preserving Model Efficacy</title>
<link>https://arxiv.org/abs/2512.00829</link>
<guid>https://arxiv.org/abs/2512.00829</guid>
<content:encoded><![CDATA[
<div> Keywords: Automatic Mixed Precision, Bangla NLP, Transformer Models, Training Efficiency, Computational Resources  

<br /><br />Summary:  
This study addresses the challenge of limited computational resources in developing Natural Language Processing (NLP) models for Bangla by exploring Automatic Mixed Precision (AMP) training. AMP utilizes a combination of 16-bit and 32-bit floating-point arithmetic dynamically during training to reduce GPU memory consumption and accelerate the process without compromising model accuracy. The authors evaluate AMP's effectiveness on four Bangla NLP tasks: sentiment analysis, named entity recognition, error classification, and question answering. Four transformer-based models—BanglaBERT, BanglishBERT, XLM-R, and mBERT—are benchmarked to assess performance impacts. The results reveal that AMP speeds up training by 44.5% and cuts down memory usage by 17.6%. Despite these efficiency gains, the F-1 scores for AMP-trained models remain very close to those of full-precision baselines, retaining 99.7% of their effectiveness. This empirical evidence suggests AMP as a viable method to reduce computational demands, enabling broader access to advanced NLP technologies, especially in regions with constrained hardware resources, like Bangladesh. The findings potentially democratize state-of-the-art NLP development by making it more accessible through lower cost and faster training cycles. <div>
arXiv:2512.00829v1 Announce Type: new 
Abstract: Training models for Natural Language Processing (NLP) requires substantial computational resources and time, posing significant challenges, especially for NLP development in Bangla, where access to high-end hardware is often limited. In this work, we explore automatic mixed precision (AMP) training as a means to improve computational efficiency without sacrificing model performance. By leveraging a dynamic mix of 16-bit and 32-bit floating-point computations, AMP lowers GPU memory requirements and speeds up training without degrading model performance. We evaluate AMP across four standard Bangla NLP tasks, namely sentiment analysis, named entity recognition, error classification, and question answering, using four transformer-based models: BanglaBERT, BanglishBERT, XLM-R, and mBERT. Our results demonstrate that AMP accelerates training by 44.5% and reduces memory consumption by 17.6%, while maintaining F-1 score within 99.7% of the full-precision baselines. This empirical study highlights AMP's potential to democratize access to state-of-the-art NLP capabilities in hardware-constrained settings by lowering computational barriers.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WaterSearch: A Quality-Aware Search-based Watermarking Framework for Large Language Models</title>
<link>https://arxiv.org/abs/2512.00837</link>
<guid>https://arxiv.org/abs/2512.00837</guid>
<content:encoded><![CDATA[
<div> Watermarking, Large Language Models, Text Generation, Signal Embedding, Anti-attack Robustness  

<br /><br />Summary:  
This paper addresses the challenge of watermarking text generated by Large Language Models (LLMs) to enable reliable attribution and secure machine-generated content. Traditional watermarking methods manipulate token generation probabilities but face a trade-off between watermark detectability and text quality. To overcome this, the authors propose a novel embedding scheme that controls seed pools, allowing diverse parallel generation of watermarked text. Based on this, they introduce WaterSearch, a sentence-level, search-based watermarking framework that can adapt to various existing methods. WaterSearch improves text quality by jointly optimizing two key factors: distribution fidelity and watermark signal characteristics. Additionally, a sentence-level detection method is developed, demonstrating strong robustness against attacks. The approach is evaluated across three popular LLMs and ten diverse tasks, showing an average improvement of 51.01% over current state-of-the-art baselines at 95% watermark detectability. In difficult scenarios, such as short-text and low-entropy outputs, WaterSearch yields gains of 47.78% and 36.47%, respectively. Moreover, the method maintains high watermark detectability despite attack scenarios including insertion, synonym substitution, and paraphrase attacks, confirming its robust anti-attack capabilities. The authors have also made their code publicly available for further research and development. <div>
arXiv:2512.00837v1 Announce Type: new 
Abstract: Watermarking acts as a critical safeguard in text generated by Large Language Models (LLMs). By embedding identifiable signals into model outputs, watermarking enables reliable attribution and enhances the security of machine-generated content. Existing approaches typically embed signals by manipulating token generation probabilities. Despite their effectiveness, these methods inherently face a trade-off between detectability and text quality: the signal strength and randomness required for robust watermarking tend to degrade the performance of downstream tasks.
  In this paper, we design a novel embedding scheme that controls seed pools to facilitate diverse parallel generation of watermarked text. Based on that scheme, we propose WaterSearch, a sentence-level, search-based watermarking framework adaptable to a wide range of existing methods. WaterSearch enhances text quality by jointly optimizing two key aspects: 1) distribution fidelity and 2) watermark signal characteristics. Furthermore, WaterSearch is complemented by a sentence-level detection method with strong attack robustness. We evaluate our method on three popular LLMs across ten diverse tasks. Extensive experiments demonstrate that our method achieves an average performance improvement of 51.01\% over state-of-the-art baselines at a watermark detectability strength of 95\%. In challenging scenarios such as short text generation and low-entropy output generation, our method yields performance gains of 47.78\% and 36.47\%, respectively. Moreover, under different attack senarios including insertion, synonym substitution and paraphrase attasks, WaterSearch maintains high detectability, further validating its robust anti-attack capabilities. Our code is available at \href{https://github.com/Yukang-Lin/WaterSearch}{https://github.com/Yukang-Lin/WaterSearch}.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Less is More: Resource-Efficient Low-Rank Adaptation</title>
<link>https://arxiv.org/abs/2512.00878</link>
<guid>https://arxiv.org/abs/2512.00878</guid>
<content:encoded><![CDATA[
<div> Low-Rank Adaptation, EffiLoRA, parameter-efficient fine-tuning, transformer layers, multimodal models<br /><br />Summary:  
This paper addresses the limitations of Low-Rank Adaptation (LoRA), a prominent parameter-efficient fine-tuning (PEFT) method for Large Language Models, which still incurs significant overhead and suffers from parameter interference in complex datasets. The authors identify inter-matrix and intra-layer parameter redundancy as key issues and propose EffiLoRA, a Resource-Efficient Low-Rank Adaptation method that is lightweight and generalizable across language, multimodal, and diffusion models. EffiLoRA innovatively employs a unified A matrix shared across all transformer layers to reduce redundancy while introducing runtime selective updates of the B matrices, allowing dynamic trade-offs between system resource consumption and model performance. This adaptive mechanism significantly improves training efficiency compared to traditional LoRA and recent decoupling approaches. The method was tested across diverse tasks, including commonsense reasoning, visual instruction tuning, and image generation, where it consistently outperformed LoRA. EffiLoRA demonstrates enhanced robustness and generalization capabilities, making it a practical solution for efficient and scalable fine-tuning across different model modalities and architectures. <div>
arXiv:2512.00878v1 Announce Type: new 
Abstract: Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning (PEFT) method for Large Language Models (LLMs), but it still incurs notable overhead and suffers from parameter interference in complex datasets. While re- cent works decouple LoRA update matrices to exploit matrix-wise asymmetry, training costs remain high. We revisit LoRA from the perspective of inter-matrix and intra-layer parameter redundancy and propose Resource-Efficient Low-Rank Adaptation, EffiLoRA, a lightweight and generalizable approach for language, multimodal, and diffusion models. EffiLoRA employs a unified A matrix across all transformer layers and introduces a runtime selective B matrices up- date to dynamically trade-off the system resource budget and model performance. EffiLoRA consistently outperforms LoRA across diverse modalities, including commonsense reasoning, visual instruction tuning, and image generation, demon- strating improved efficiency and robustness.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reward Auditor: Inference on Reward Modeling Suitability in Real-World Perturbed Scenarios</title>
<link>https://arxiv.org/abs/2512.00920</link>
<guid>https://arxiv.org/abs/2512.00920</guid>
<content:encoded><![CDATA[
<div> Keywords: Reward Models, Large Language Models, Suitability, Reward Auditor, Real-world Perturbations  

<br /><br />Summary: Reliable reward models (RMs) are essential for safely aligning large language models (LLMs). Current evaluation methods mainly measure preference perception accuracy in controlled scenarios, failing to reveal vulnerabilities that arise in realistic settings. The authors highlight the importance of assessing a new dimension called "suitability," which refers to the conditional reliability of RMs under real-world perturbations. To address this, they propose Reward Auditor, a novel hypothesis-testing framework designed to infer RM suitability. Instead of simply evaluating accuracy on given samples, Reward Auditor scientifically audits RMs to detect systematic weaknesses in specific real-world scenarios. It achieves this by analyzing distributional changes in RM preference confidence under perturbations, quantifying both statistical significance and effect size. This approach allows for a clear understanding of the certainty and severity of RM vulnerabilities across diverse conditions. Collectively, this framework establishes a more rigorous foundation for developing next-generation LLM alignment systems that prioritize verifiable safety, robustness, and trustworthiness in practical applications. <div>
arXiv:2512.00920v1 Announce Type: new 
Abstract: Reliable reward models (RMs) are critical for ensuring the safe alignment of large language models (LLMs). However, current evaluation methods focus solely on preference perception accuracies in given specific scenarios, obscuring the critical vulnerabilities of RMs in real-world scenarios. We identify the true challenge lies in assessing a novel dimension: Suitability, defined as conditional reliability under specific real-world perturbations. To this end, we introduce Reward Auditor, a hypothesis-testing framework specifically designed for RM suitability inference. Rather than answering "How accurate is the RM's preference perception for given samples?", it employs scientific auditing to answer: "Can we infer RMs exhibit systematic vulnerabilities in specific real-world scenarios?". Under real-world perturbed scenarios, Reward Auditor quantifies statistical significance and effect size by auditing distribution degradation of RM preference perception confidence. This enables inference of both the certainty and severity of RM vulnerabilities across diverse real-world scenarios. This lays a solid foundation for building next-generation LLM alignment systems that are verifiably safe, more robust, and trustworthy.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Hallucinations in Zero-Shot Scientific Summarisation: A Pilot Study</title>
<link>https://arxiv.org/abs/2512.00931</link>
<guid>https://arxiv.org/abs/2512.00931</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, prompt engineering, context inconsistency, zero-shot summarisation, scientific texts<br /><br />Summary:<br /><br />1. This research investigates how prompt engineering (PE) methods affect context inconsistency hallucinations in zero-shot large language model (LLM) summarisation of scientific abstracts in yeast biotechnology.<br /><br />2. Six instruction-tuned LLMs were tested on eight research paper abstracts using seven different prompt methods: a baseline, two levels of increased instruction complexity (PE-1 and PE-2), two levels of context repetition (CR-K1 and CR-K2), and two levels of random addition (RA-K1 and RA-K2).<br /><br />3. Context repetition methods involved repeating one or two key sentences from the abstract, while random addition involved repeating one or two randomly selected sentences.<br /><br />4. A total of 336 summaries were generated and evaluated on six metrics measuring lexical and semantic alignment: ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, METEOR, and cosine similarity.<br /><br />5. Statistical analysis, including BCa bootstrap confidence intervals and Wilcoxon signed-rank tests with Bonferroni-Holm correction, was performed on 3744 data points to test four hypotheses about prompt effects.<br /><br />6. Results showed that context repetition and random addition significantly improved lexical alignment between summaries and abstracts, suggesting prompt engineering can reduce hallucinations in zero-shot LLM scientific summarisation. <div>
arXiv:2512.00931v1 Announce Type: new 
Abstract: Large language models (LLMs) produce context inconsistency hallucinations, which are LLM generated outputs that are misaligned with the user prompt. This research project investigates whether prompt engineering (PE) methods can mitigate context inconsistency hallucinations in zero-shot LLM summarisation of scientific texts, where zero-shot indicates that the LLM relies purely on its pre-training data. Across eight yeast biotechnology research paper abstracts, six instruction-tuned LLMs were prompted with seven methods: a base- line prompt, two levels of increasing instruction complexity (PE-1 and PE-2), two levels of context repetition (CR-K1 and CR-K2), and two levels of random addition (RA-K1 and RA-K2). Context repetition involved the identification and repetition of K key sentences from the abstract, whereas random addition involved the repetition of K randomly selected sentences from the abstract, where K is 1 or 2. A total of 336 LLM-generated summaries were evaluated using six metrics: ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, METEOR, and cosine similarity, which were used to compute the lexical and semantic alignment be- tween the summaries and the abstracts. Four hypotheses on the effects of prompt methods on summary alignment with the reference text were tested. Statistical analysis on 3744 collected datapoints was performed using bias-corrected and accelerated (BCa) bootstrap confidence intervals and Wilcoxon signed-rank tests with Bonferroni-Holm correction. The results demonstrated that CR and RA significantly improve the lexical alignment of LLM-generated summaries with the abstracts. These findings indicate that prompt engineering has the potential to impact hallucinations in zero-shot scientific summarisation tasks.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeformAr: Rethinking NER Evaluation through Component Analysis and Visual Analytics</title>
<link>https://arxiv.org/abs/2512.00938</link>
<guid>https://arxiv.org/abs/2512.00938</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformer models, Arabic Named Entity Recognition, DeformAr, cross-component analysis, behavioural analysis<br /><br />Summary:  
1. Transformer models have revolutionized NLP and show strong performance in English but struggle with Arabic NER tasks, even when using large pre-trained models.  
2. The performance gap between English and Arabic NER systems is caused by several intertwined factors, including tokenization issues, dataset quality, and annotation inconsistencies, which are often studied in isolation.  
3. The paper introduces DeformAr, a novel debugging and evaluation framework specifically designed for transformer-based Arabic NER systems to analyze and explain the observed performance discrepancies.  
4. DeformAr consists of a data extraction library and an interactive dashboard, featuring two evaluation modes: cross-component analysis and behavioural analysis, which jointly evaluate dataset and model components in both languages.  
5. The framework's two-stage analysis first offers systematic diagnostics to uncover the "what," "how," and "why" behind performance differences, followed by behavioural analysis incorporating interpretability techniques, token-level metrics, interactive visualizations, and representation space analysis.  
6. DeformAr enables component-aware diagnostics that link model behaviors to underlying data and representational factors, making it the first Arabic-specific interpretability tool addressing multi-faceted challenges in under-resourced language NER evaluation. <div>
arXiv:2512.00938v1 Announce Type: new 
Abstract: Transformer models have significantly advanced Natural Language Processing (NLP), demonstrating strong performance in English. However, their effectiveness in Arabic, particularly for Named Entity Recognition (NER), remains limited, even with larger pre-trained models. This performance gap stems from multiple factors, including tokenisation, dataset quality, and annotation inconsistencies. Existing studies often analyze these issues in isolation, failing to capture their joint effect on system behaviour and performance.
  We introduce DeformAr (Debugging and Evaluation Framework for Transformer-based NER Systems), a novel framework designed to investigate and explain the performance discrepancy between Arabic and English NER systems. DeformAr integrates a data extraction library and an interactive dashboard, supporting two modes of evaluation: cross-component analysis and behavioural analysis. The framework divides each language into dataset and model components to examine their interactions.
  The analysis proceeds in two stages. First, cross-component analysis provides systematic diagnostic measures across data and model subcomponents, addressing the "what," "how," and "why" behind observed discrepancies. The second stage applies behavioural analysis by combining interpretability techniques with token-level metrics, interactive visualisations, and representation space analysis. These stages enable a component-aware diagnostic process that detects model behaviours and explains them by linking them to underlying representational patterns and data factors. DeformAr is the first Arabic-specific, component-based interpretability tool, offering a crucial resource for advancing model analysis in under-resourced languages.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-tuning of lightweight large language models for sentiment classification on heterogeneous financial textual data</title>
<link>https://arxiv.org/abs/2512.00946</link>
<guid>https://arxiv.org/abs/2512.00946</guid>
<content:encoded><![CDATA[
<div> Keywords: lightweight LLMs, financial sentiment analysis, open-source models, few-shot learning, multilingual datasets<br /><br />Summary:<br /><br />1. The paper explores the utility of lightweight, open-source large language models (LLMs) in financial markets analysis, focusing on their ability to handle complex and varied textual data such as tweets, news, reports, and microblogs.<br />2. It addresses the challenge that many powerful LLMs require substantial computational resources and proprietary datasets, limiting access for many researchers and practitioners.<br />3. The study compares the benchmark financial NLP model FinBERT with three open-source lightweight LLMs: DeepSeek-LLM 7B, Llama3 8B Instruct, and Qwen3 8B across five public datasets that include FinancialPhraseBank, Financial Question Answering, Gold News Sentiment, Twitter Sentiment, and Chinese Finance Sentiment.<br />4. Results show that models Qwen3 8B and Llama3 8B achieve the best performance in most scenarios, even with only 5% of the available training data, highlighting their effectiveness in zero-shot and few-shot learning settings.<br />5. The findings suggest that lightweight, open-source LLMs represent a cost-effective solution for financial sentiment analysis and can generalize well across heterogeneous textual data, reducing dependence on large annotated datasets and computational resources. <div>
arXiv:2512.00946v1 Announce Type: new 
Abstract: Large language models (LLMs) play an increasingly important role in finan- cial markets analysis by capturing signals from complex and heterogeneous textual data sources, such as tweets, news articles, reports, and microblogs. However, their performance is dependent on large computational resources and proprietary datasets, which are costly, restricted, and therefore inacces- sible to many researchers and practitioners. To reflect realistic situations we investigate the ability of lightweight open-source LLMs - smaller and publicly available models designed to operate with limited computational resources - to generalize sentiment understanding from financial datasets of varying sizes, sources, formats, and languages. We compare the benchmark finance natural language processing (NLP) model, FinBERT, and three open-source lightweight LLMs, DeepSeek-LLM 7B, Llama3 8B Instruct, and Qwen3 8B on five publicly available datasets: FinancialPhraseBank, Financial Question Answering, Gold News Sentiment, Twitter Sentiment and Chinese Finance Sentiment. We find that LLMs, specially Qwen3 8B and Llama3 8B, perform best in most scenarios, even from using only 5% of the available training data. These results hold in zero-shot and few-shot learning scenarios. Our findings indicate that lightweight, open-source large language models (LLMs) consti- tute a cost-effective option, as they can achieve competitive performance on heterogeneous textual data even when trained on only a limited subset of the extensive annotated corpora that are typically deemed necessary.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Table as a Modality for Large Language Models</title>
<link>https://arxiv.org/abs/2512.00947</link>
<guid>https://arxiv.org/abs/2512.00947</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, tabular data, structural information, multimodal framework, hypergraph neural network  

<br /><br />Summary:  
This work addresses the challenge of applying Large Language Models (LLMs) to table reasoning tasks, which involve widely used tabular data. The authors identify that current approaches typically convert tables into serialized text combined with metadata before inputting them into LLMs, leading to loss of essential structural information. To overcome this, they introduce TAMO, a novel method that treats tables as an independent modality alongside text tokens. TAMO integrates a hypergraph neural network as a global table encoder, which captures complex table structures and is seamlessly combined with mainstream LLMs for better reasoning. The proposed multimodal framework significantly improves the handling of tabular data by preserving and leveraging structural relationships within tables. Empirical evaluations on several benchmark datasets such as HiTab, WikiTQ, WikiSQL, FeTaQA, and the newly proposed StructQA demonstrate substantial generalization improvements. Specifically, TAMO achieves an average relative gain of 42.65% over previous methods. The study highlights the importance of multimodal fusion and specialized table representation for enhancing LLM performance in tabular reasoning tasks. <div>
arXiv:2512.00947v1 Announce Type: new 
Abstract: To migrate the remarkable successes of Large Language Models (LLMs), the community has made numerous efforts to generalize them to the table reasoning tasks for the widely deployed tabular data. Despite that, in this work, by showing a probing experiment on our proposed StructQA benchmark, we postulate that even the most advanced LLMs (such as GPTs) may still fall short of coping with tabular data. More specifically, the current scheme often simply relies on serializing the tabular data, together with the meta information, then inputting them through the LLMs. We argue that the loss of structural information is the root of this shortcoming. In this work, we further propose TAMO, which bears an ideology to treat the tables as an independent modality integrated with the text tokens. The resulting model in TAMO is a multimodal framework consisting of a hypergraph neural network as the global table encoder seamlessly integrated with the mainstream LLM. Empirical results on various benchmarking datasets, including HiTab, WikiTQ, WikiSQL, FeTaQA, and StructQA, have demonstrated significant improvements on generalization with an average relative gain of 42.65%.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dr.Mi-Bench: A Modular-integrated Benchmark for Scientific Deep Research Agent</title>
<link>https://arxiv.org/abs/2512.00986</link>
<guid>https://arxiv.org/abs/2512.00986</guid>
<content:encoded><![CDATA[
<div> Keywords: deep research agents, scientific domains, benchmark, modular-integrated evaluation, high-level planning  

<br /><br />Summary: The paper introduces Dr.Mi-Bench, a new benchmark designed specifically for evaluating deep research (DR) agents in academic contexts. Existing benchmarks often focus narrowly on retrieval tasks and generally target broad domains, neglecting the complexity and specific needs of scientific research tasks. Dr.Mi-Bench addresses these gaps by providing a human-annotated dataset of 200 instances spanning 10 diverse scientific fields, including both research and review papers. Accompanying the benchmark is Dr.Mi-Eval, a modular-integrated evaluation paradigm that assesses key competencies of DR agents, such as planning, retrieval, and reasoning. This evaluation operates in two modes: an end-to-end test of full DR agents and an isolated evaluation of foundational Large Language Models (LLMs) as potential backbones. Experimental results highlight fragmented performance, with agents showing specialized strengths but also critical weaknesses, especially in multi-source retrieval needed for comprehensive review-style tasks and maintaining consistency across scientific domains. The study identifies enhancing high-level planning abilities as essential to unlocking the reasoning potential of foundational LLMs. Dr.Mi-Bench thus serves as a diagnostic tool, revealing actionable failure points and guiding the development of more effective and reliable automated academic research assistants. <div>
arXiv:2512.00986v1 Announce Type: new 
Abstract: The explosive growth in academic literature necessitates automated deep research (DR) agents, yet their evaluation remains a significant challenge. First, existing benchmarks often focus narrowly on retrieval while neglecting high-level planning and reasoning. Second, existing benchmarks favor general domains over the scientific domains that are the core application for DR agents. To address these gaps, we introduce Dr.Mi-Bench, a Modular-integrated benchmark for scientific DR agents. Grounded in academic literature, our benchmark uses a human-annotated dataset of 200 instances across 10 scientific domains, including both research and review papers. Besides, we also propose a Modular-integrated Evaluation Paradigm for DR Agents (Dr.Mi-Eval), a novel modular-integrated evaluation paradigm, which leverages the rich structure of academic papers to assess the core competencies of planning, retrieval, and reasoning through two complementary modes: an end-to-end evaluation for DR agents and an isolated evaluation for foundational LLMs as potential backbones. Experimental results reveal a fragmented performance landscape: agents exhibit specialized strengths but share critical weaknesses, most notably in performing the multi-source retrieval required for review-style tasks and performing consistently across diverse scientific fields. Moreover, improving high-level planning capability is the crucial factor for unlocking the reasoning potential of foundational LLMs as backbones. By exposing these actionable failure modes, Dr.Mi-Bench provides a diagnostic tool to guide the development of more reliable academic research assistants.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing Academic Chatbots: Evaluation of Non Traditional Outputs</title>
<link>https://arxiv.org/abs/2512.00991</link>
<guid>https://arxiv.org/abs/2512.00991</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, retrieval strategies, Graph RAG, Advanced RAG, academic content generation<br /><br />Summary:<br /><br />This research evaluates large language models (LLMs) beyond conventional tasks by exploring two retrieval strategies for question answering (QA): Graph RAG, which uses structured knowledge graphs, and Advanced RAG, a hybrid keyword-semantic search. The study implements a prototype that integrates Meta's LLaMA 3 70B open weight model with OpenAI's GPT 4o mini API. QA performance is assessed through both human ratings, covering eleven quality dimensions, and scalable validation with LLM judges. Findings show GPT 4o mini combined with Advanced RAG yields the most accurate answers, while Graph RAG offers limited benefits and increases hallucination risk due to its complexity and manual setup. Additionally, the paper investigates LLMs' ability to generate non-traditional academic outputs such as slide decks and podcast scripts, using document-grounded retrieval. GPT 4o mini again outperforms alternatives, though LLaMA 3 demonstrates notable narrative coherence in creative content. Human reviewers are essential to identify layout and stylistic shortcomings, underscoring the importance of joint human and LLM evaluation methods to effectively assess emerging academic formats and ensure quality output. <div>
arXiv:2512.00991v1 Announce Type: new 
Abstract: Most evaluations of large language models focus on standard tasks such as factual question answering or short summarization. This research expands that scope in two directions: first, by comparing two retrieval strategies, Graph RAG, structured knowledge-graph based, and Advanced RAG, hybrid keyword-semantic search, for QA; and second, by evaluating whether LLMs can generate high quality non-traditional academic outputs, specifically slide decks and podcast scripts. We implemented a prototype combining Meta's LLaMA 3 70B open weight and OpenAI's GPT 4o mini API based. QA performance was evaluated using both human ratings across eleven quality dimensions and large language model judges for scalable cross validation. GPT 4o mini with Advanced RAG produced the most accurate responses. Graph RAG offered limited improvements and led to more hallucinations, partly due to its structural complexity and manual setup. Slide and podcast generation was tested with document grounded retrieval. GPT 4o mini again performed best, though LLaMA 3 showed promise in narrative coherence. Human reviewers were crucial for detecting layout and stylistic flaws, highlighting the need for combined human LLM evaluation in assessing emerging academic outputs.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Safety Blocks Sense: Measuring Semantic Confusion in LLM Refusals</title>
<link>https://arxiv.org/abs/2512.01037</link>
<guid>https://arxiv.org/abs/2512.01037</guid>
<content:encoded><![CDATA[
<div> Keywords: semantic confusion, safety-aligned language models, local inconsistency, ParaGuard dataset, confusion metrics<br /><br />Summary:<br /><br />1. Safety-aligned language models tend to reject harmless prompts, causing false refusals that current global metrics like false rejection or compliance rates fail to capture fully.<br /><br />2. The main problem identified is "semantic confusion," a failure mode where a model inconsistently responds to close paraphrases with the same intent, accepting some while rejecting others.<br /><br />3. To address this, the authors created ParaGuard, a benchmark dataset containing 10,000 prompts organized into paraphrase clusters that maintain the same intent but differ in surface form.<br /><br />4. They proposed three novel, model-agnostic token-level metrics—Confusion Index, Confusion Rate, and Confusion Depth—that leverage token embeddings, next-token probabilities, and perplexity to quantify semantic confusion by comparing refusals to their nearest accepted neighbors.<br /><br />5. Experiments on multiple model families and real-world deployment settings reveal that global false rejection rates obscure important local inconsistencies; the new metrics expose unstable decision boundaries, localized inconsistency pockets, and scenarios where stricter refusal policies do not increase confusion.<br /><br />6. The confusion-aware auditing framework helps developers better differentiate between the frequency of refusals and the appropriateness of those refusals, enabling practical improvements in reducing false rejections while maintaining safety guarantees. <div>
arXiv:2512.01037v1 Announce Type: new 
Abstract: Safety-aligned language models often refuse prompts that are actually harmless. Current evaluations mostly report global rates such as false rejection or compliance. These scores treat each prompt alone and miss local inconsistency, where a model accepts one phrasing of an intent but rejects a close paraphrase. This gap limits diagnosis and tuning. We introduce "semantic confusion," a failure mode that captures such local inconsistency, and a framework to measure it. We build ParaGuard, a 10k-prompt corpus of controlled paraphrase clusters that hold intent fixed while varying surface form. We then propose three model-agnostic metrics at the token level: Confusion Index, Confusion Rate, and Confusion Depth. These metrics compare each refusal to its nearest accepted neighbors and use token embeddings, next-token probabilities, and perplexity signals. Experiments across diverse model families and deployment guards show that global false-rejection rate hides critical structure. Our metrics reveal globally unstable boundaries in some settings, localized pockets of inconsistency in others, and cases where stricter refusal does not increase inconsistency. We also show how confusion-aware auditing separates how often a system refuses from how sensibly it refuses. This gives developers a practical signal to reduce false refusals while preserving safety.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ELR-1000: A Community-Generated Dataset for Endangered Indic Indigenous Languages</title>
<link>https://arxiv.org/abs/2512.01077</link>
<guid>https://arxiv.org/abs/2512.01077</guid>
<content:encoded><![CDATA[
<div> Keywords: endangered languages, traditional recipes, multimodal dataset, language translation, cultural preservation<br /><br />Summary:<br /><br />1. This paper introduces ELR-1000, a culturally-grounded multimodal dataset comprising 1,060 traditional recipes collected from rural communities in remote regions of Eastern India, covering 10 endangered languages.  
2. The dataset captures both linguistic diversity and the socio-cultural context entrenched in indigenous food traditions, emphasizing the cultural nuances embedded within the recipes.  
3. Data collection was facilitated through a mobile interface tailored for contributors with low digital literacy, ensuring accessibility and authenticity in gathering the information.  
4. State-of-the-art large language models (LLMs) were evaluated on their ability to translate these endangered language recipes into English; results show that while models have strong capabilities, they struggle with low-resource, culturally-specific languages.  
5. However, incorporating targeted contextual information—such as background on the languages, translation examples, and cultural preservation guidelines—significantly improves translation quality.  
6. The study highlights a crucial need for benchmarks and resources that address underrepresented languages and culturally-specific domains to promote equitable and culturally-aware language technologies.  
7. By releasing the ELR-1000 dataset publicly, the authors aim to stimulate NLP research and development targeting endangered languages and their cultural heritage. <div>
arXiv:2512.01077v1 Announce Type: new 
Abstract: We present a culturally-grounded multimodal dataset of 1,060 traditional recipes crowdsourced from rural communities across remote regions of Eastern India, spanning 10 endangered languages. These recipes, rich in linguistic and cultural nuance, were collected using a mobile interface designed for contributors with low digital literacy. Endangered Language Recipes (ELR)-1000 -- captures not only culinary practices but also the socio-cultural context embedded in indigenous food traditions. We evaluate the performance of several state-of-the-art large language models (LLMs) on translating these recipes into English and find the following: despite the models' capabilities, they struggle with low-resource, culturally-specific language. However, we observe that providing targeted context -- including background information about the languages, translation examples, and guidelines for cultural preservation -- leads to significant improvements in translation quality. Our results underscore the need for benchmarks that cater to underrepresented languages and domains to advance equitable and culturally-aware language technologies. As part of this work, we release the ELR-1000 dataset to the NLP community, hoping it motivates the development of language technologies for endangered languages.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How do we measure privacy in text? A survey of text anonymization metrics</title>
<link>https://arxiv.org/abs/2512.01109</link>
<guid>https://arxiv.org/abs/2512.01109</guid>
<content:encoded><![CDATA[
<div> Keywords: text anonymization, privacy metrics, privacy evaluation, legal standards, user expectations<br /><br />Summary:  
This work presents a systematic survey aimed at clarifying and reconciling various metrics used to evaluate privacy protection in text data. The study emphasizes the importance of text anonymization for enabling natural language processing (NLP) research and model development when working with sensitive domains. The authors manually review 47 papers that report privacy metrics to identify and compare six distinct privacy notions, highlighting how different metrics capture varying aspects of privacy risks. They further analyze how these privacy notions align with established legal privacy frameworks, specifically HIPAA and GDPR, ensuring that evaluation methods meet regulatory requirements. Additionally, the study considers user-centered privacy expectations derived from human-computer interaction (HCI) studies, addressing the human perspective on privacy protection. The analysis provides practical guidance for researchers and practitioners to navigate the complex landscape of privacy evaluation methods in text anonymization. Finally, the work identifies existing gaps in current evaluation practices, aiming to promote more robust, comparable, and legally informed privacy assessments in future anonymization efforts. <div>
arXiv:2512.01109v1 Announce Type: new 
Abstract: In this work, we aim to clarify and reconcile metrics for evaluating privacy protection in text through a systematic survey. Although text anonymization is essential for enabling NLP research and model development in domains with sensitive data, evaluating whether anonymization methods sufficiently protect privacy remains an open challenge. In manually reviewing 47 papers that report privacy metrics, we identify and compare six distinct privacy notions, and analyze how the associated metrics capture different aspects of privacy risk. We then assess how well these notions align with legal privacy standards (HIPAA and GDPR), as well as user-centered expectations grounded in HCI studies. Our analysis offers practical guidance on navigating the landscape of privacy evaluation approaches further and highlights gaps in current practices. Ultimately, we aim to facilitate more robust, comparable, and legally aware privacy evaluations in text anonymization.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DrawingBench: Evaluating Spatial Reasoning and UI Interaction Capabilities of Large Language Models through Mouse-Based Drawing Tasks</title>
<link>https://arxiv.org/abs/2512.01174</link>
<guid>https://arxiv.org/abs/2512.01174</guid>
<content:encoded><![CDATA[
<div> Keywords: agentic AI, trustworthiness evaluation, DrawingBench, spatial reasoning, external oversight  

<br /><br />Summary:  
1. The article introduces DrawingBench, a novel verification framework designed to evaluate the trustworthiness of agentic large language models (LLMs) through spatial reasoning tasks involving sequences of low-level GUI actions.  
2. Unlike traditional opaque benchmarks, DrawingBench emphasizes transparency and auditability by providing 8 objective rule-based criteria for reproducible scoring and enabling action-level inspection to allow auditing of agent behavior.  
3. The framework includes 250 diverse prompts distributed across 20 categories and 4 difficulty levels, along with deterministic evaluation metrics and an external oversight mechanism through multi-turn feedback to facilitate human-guided agent refinement.  
4. An evaluation of four state-of-the-art LLMs (Claude-4 Sonnet, GPT-4.1, GPT-4.1-mini, Gemini-2.5 Flash) on 1,000 tests revealed an average perfect performance rate of 92.8%, with significant improvements (average +3.2%, up to +32.8%) when external structured feedback was applied, particularly for complex scenes.  
5. Key limitations were identified in tool state management and long-horizon planning; importantly, providing explicit, verifiable criteria led to flawless (100%) model performance, underscoring the role of specification clarity over task complexity. The study demonstrates that transparent evaluation frameworks coupled with external oversight establish more reliable trust in autonomous agentic systems. The DrawingBench framework is open-source and serves as a template for trustworthy agent assessment. <div>
arXiv:2512.01174v1 Announce Type: new 
Abstract: As agentic AI systems increasingly operate autonomously, establishing trust through verifiable evaluation becomes critical. Yet existing benchmarks lack the transparency and auditability needed to assess whether agents behave reliably. We present DrawingBench, a verification framework for evaluating the trustworthiness of agentic LLMs through spatial reasoning tasks that require generating sequences of low-level GUI actions. Unlike opaque evaluations, DrawingBench provides transparent, rule-based assessment: 8 objective criteria enable reproducible scoring, while action-level inspection allows stakeholders to audit agent behavior. Our framework comprises 250 diverse prompts across 20 categories and 4 difficulty levels, deterministic evaluation metrics, and an external oversight mechanism through multi-turn feedback that enables human control over agent refinement. Evaluating four state-of-the-art LLMs (Claude-4 Sonnet, GPT-4.1, GPT-4.1-mini, Gemini-2.5 Flash) across 1,000 tests, we establish both capabilities and limitations: models achieved 92.8% perfect performance with structured external feedback driving significant improvements (average +3.2%, up to +32.8% for complex scenes), but systematic error patterns emerged in tool state management and long-horizon planning. Notably, specification clarity proved more important than task complexity -- models achieved 100% perfect performance when given explicit, verifiable criteria. These findings demonstrate that transparent evaluation frameworks can establish trust in agentic systems, with external oversight proving more reliable than self-correction for guiding agent behavior. Our open-source framework provides a template for trustworthy agent assessment. Code and data: https://github.com/hyunjun1121/DrawingBench
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TempPerturb-Eval: On the Joint Effects of Internal Temperature and External Perturbations in RAG Robustness</title>
<link>https://arxiv.org/abs/2512.01183</link>
<guid>https://arxiv.org/abs/2512.01183</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, text perturbations, temperature settings, robustness, HotpotQA<br /><br />Summary:<br /><br />This paper investigates the often-overlooked interaction between retrieval noise and generation parameters in Retrieval-Augmented Generation (RAG) systems. The authors introduce a novel analytical framework called the RAG Perturbation-Temperature Analysis Framework, which systematically tests how different types of text perturbations—simulating noisy retrieval—affect model output across a wide range of temperature settings. The framework applies three distinct perturbation types to the retrieved documents and evaluates model performance using multiple large language models (both open-source and proprietary) on the HotpotQA dataset. Key findings reveal that higher temperature settings tend to increase the system’s vulnerability to noisy inputs, while some perturbation types show complex, non-linear sensitivity patterns depending on the temperature. From these results, the authors contribute (1) a diagnostic benchmark designed to assess RAG system robustness under noisy retrieval scenarios, (2) an analytical approach to quantify how perturbation and temperature parameters interact and influence performance, and (3) practical guidelines aimed at improving model selection and temperature tuning for RAG systems dealing with imperfect retrieval conditions. This work provides a foundational step toward more reliable and robust retrieval-augmented generation models in real-world noisy environments. <div>
arXiv:2512.01183v1 Announce Type: new 
Abstract: The evaluation of Retrieval-Augmented Generation (RAG) systems typically examines retrieval quality and generation parameters like temperature in isolation, overlooking their interaction. This work presents a systematic investigation of how text perturbations (simulating noisy retrieval) interact with temperature settings across multiple LLM runs. We propose a comprehensive RAG Perturbation-Temperature Analysis Framework that subjects retrieved documents to three distinct perturbation types across varying temperature settings. Through extensive experiments on HotpotQA with both open-source and proprietary LLMs, we demonstrate that performance degradation follows distinct patterns: high-temperature settings consistently amplify vulnerability to perturbations, while certain perturbation types exhibit non-linear sensitivity across the temperature range. Our work yields three key contributions: (1) a diagnostic benchmark for assessing RAG robustness, (2) an analytical framework for quantifying perturbation-temperature interactions, and (3) practical guidelines for model selection and parameter tuning under noisy retrieval conditions.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalist Large Language Models Outperform Clinical Tools on Medical Benchmarks</title>
<link>https://arxiv.org/abs/2512.01191</link>
<guid>https://arxiv.org/abs/2512.01191</guid>
<content:encoded><![CDATA[
<div> Keywords: clinical AI, large language models, evaluation, medical knowledge, patient safety<br /><br />Summary:<br /><br />1. Specialized clinical AI assistants like OpenEvidence and UpToDate Expert AI are increasingly used in medical practice, often promoted as safer and more reliable than general-purpose large language models (LLMs).<br />2. Despite their influence on diagnosis, triage, and guideline interpretation, these clinical tools rarely undergo independent, quantitative evaluations, creating a significant evidence gap.<br />3. This study compared two widely used clinical AI systems with three advanced generalist LLMs (GPT-5, Gemini 3 Pro, and Claude Sonnet 4.5) using a 1,000-item mini-benchmark combining medical knowledge (MedQA) and clinician-alignment (HealthBench) tasks.<br />4. The generalist LLMs consistently outperformed the specialized clinical AI tools, with GPT-5 achieving the highest overall scores.<br />5. Clinical tools showed notable weaknesses in completeness, communication quality, context awareness, and systems-based safety reasoning.<br /><br />These results suggest that clinical AI assistants currently marketed for decision support may underperform compared to frontier LLMs, highlighting the critical need for transparent and independent evaluation before these systems are integrated into patient-facing clinical workflows. <div>
arXiv:2512.01191v1 Announce Type: new 
Abstract: Specialized clinical AI assistants are rapidly entering medical practice, often framed as safer or more reliable than general-purpose large language models (LLMs). Yet, unlike frontier models, these clinical tools are rarely subjected to independent, quantitative evaluation, creating a critical evidence gap despite their growing influence on diagnosis, triage, and guideline interpretation. We assessed two widely deployed clinical AI systems (OpenEvidence and UpToDate Expert AI) against three state-of-the-art generalist LLMs (GPT-5, Gemini 3 Pro, and Claude Sonnet 4.5) using a 1,000-item mini-benchmark combining MedQA (medical knowledge) and HealthBench (clinician-alignment) tasks. Generalist models consistently outperformed clinical tools, with GPT-5 achieving the highest scores, while OpenEvidence and UpToDate demonstrated deficits in completeness, communication quality, context awareness, and systems-based safety reasoning. These findings reveal that tools marketed for clinical decision support may often lag behind frontier LLMs, underscoring the urgent need for transparent, independent evaluation before deployment in patient-facing workflows.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conveying Imagistic Thinking in Traditional Chinese Medicine Translation: A Prompt Engineering and LLM-Based Evaluation Framework</title>
<link>https://arxiv.org/abs/2512.01198</link>
<guid>https://arxiv.org/abs/2512.01198</guid>
<content:encoded><![CDATA[
<div> Traditional Chinese Medicine, metaphor, metonymy, translation, human-in-the-loop  

<br /><br />Summary:  
This study addresses challenges in translating Traditional Chinese Medicine (TCM) texts, which rely heavily on imagistic thinking through metaphors and metonymy, often lost in literal English renderings. It uses a human-in-the-loop (HITL) framework to improve comprehension and application of TCM theory in translation. Four foundational passages from the Huangdi Neijing were selected for analysis. Using prompt-based cognitive scaffolding, the model DeepSeek V3.1 was guided to identify and convey metaphorical and metonymic meanings in translation. To evaluate effectiveness, ChatGPT 5 Pro and Gemini 2.5 Pro simulated three types of real-world readers, scoring human, baseline model, and prompt-adjusted translations across five cognitive dimensions. Results demonstrated that prompt-adjusted translations outperformed others with consistent cross-model and cross-role agreement. Structured interviews and Interpretative Phenomenological Analysis (IPA) revealed distinctions between human and machine translations, effective strategies for metaphor/metonymy transfer, and insights into readers’ cognitive preferences. Overall, the study presents a replicable, cognitively grounded, and efficient HITL methodology for translating ancient, concept-dense texts like TCM, facilitating better cross-cultural understanding and clinical use. <div>
arXiv:2512.01198v1 Announce Type: new 
Abstract: Traditional Chinese Medicine (TCM) theory is built on imagistic thinking, in which medical principles and diagnostic and therapeutic logic are structured through metaphor and metonymy. However, existing English translations largely rely on literal rendering, making it difficult for target-language readers to reconstruct the underlying conceptual networks and apply them in clinical practice. This study adopted a human-in-the-loop (HITL) framework and selected four passages from the medical canon Huangdi Neijing that are fundamental in theory. Through prompt-based cognitive scaffolding, DeepSeek V3.1 was guided to identify metaphor and metonymy in the source text and convey the theory in translation. In the evaluation stage, ChatGPT 5 Pro and Gemini 2.5 Pro were instructed by prompts to simulate three types of real-world readers. Human translations, baseline model translations, and prompt-adjusted translations were scored by the simulated readers across five cognitive dimensions, followed by structured interviews and Interpretative Phenomenological Analysis (IPA). Results show that the prompt-adjusted LLM translations perform best across all five dimensions, with high cross-model and cross-role consistency. The interview themes reveal differences between human and machine translation, effective strategies for metaphor and metonymy transfer, and readers' cognitive preferences. This study provides a cognitive, efficient, and replicable HITL methodological pathway for the translation of ancient, concept-dense texts such as TCM.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sentiment Analysis and Emotion Classification using Machine Learning Techniques for Nagamese Language - A Low-resource Language</title>
<link>https://arxiv.org/abs/2512.01256</link>
<guid>https://arxiv.org/abs/2512.01256</guid>
<content:encoded><![CDATA[
<div> Nagamese, sentiment analysis, machine learning, lexicon, emotion classification<br /><br />Summary:<br /><br />This paper presents the first known work on sentiment analysis and emotion classification specifically for the Nagamese language, an Assamese-lexified creole used in Northeast India. The study addresses a gap in NLP resources by focusing on Nagamese, which has received no prior sentiment analysis attention. The authors developed a sentiment polarity lexicon containing 1,195 Nagamese words, which serve as the foundational dataset for the analysis. They aim to classify textual data based on sentiment polarity (positive, negative, neutral) and to detect basic emotions conveyed in Nagamese text. Features derived from the lexicon are supplemented with additional features to enhance classification performance. Sentiment classification is implemented through supervised machine learning methods, specifically Naïve Bayes and Support Vector Machines. This approach helps enable automated understanding of sentiment in Nagamese language content, contributing to NLP development for a resource-poor language. The study provides a new benchmark and resource that can facilitate further research and practical applications involving Nagamese language processing in commercial or social contexts. <div>
arXiv:2512.01256v1 Announce Type: new 
Abstract: The Nagamese language, a.k.a Naga Pidgin, is an Assamese-lexified creole language developed primarily as a means of communication in trade between the people from Nagaland and people from Assam in the north-east India. Substantial amount of work in sentiment analysis has been done for resource-rich languages like English, Hindi, etc. However, no work has been done in Nagamese language. To the best of our knowledge, this is the first attempt on sentiment analysis and emotion classification for the Nagamese Language. The aim of this work is to detect sentiments in terms of polarity (positive, negative and neutral) and basic emotions contained in textual content of Nagamese language. We build sentiment polarity lexicon of 1,195 nagamese words and use these to build features along with additional features for supervised machine learning techniques using Na"ive Bayes and Support Vector Machines.
  Keywords: Nagamese, NLP, sentiment analysis, machine learning
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SUPERChem: A Multimodal Reasoning Benchmark in Chemistry</title>
<link>https://arxiv.org/abs/2512.01274</link>
<guid>https://arxiv.org/abs/2512.01274</guid>
<content:encoded><![CDATA[
<div> Keywords: SUPERChem, chemical reasoning, Large Language Models, multimodal benchmark, Reasoning Path Fidelity<br /><br />Summary:  
The paper introduces SUPERChem, a novel benchmark designed to rigorously evaluate the chemical reasoning abilities of Large Language Models (LLMs). Unlike existing benchmarks, SUPERChem includes 500 expert-curated, reasoning-intensive chemistry problems spanning various subfields and is available in both multimodal and text-only formats. The benchmark addresses shortcomings in current evaluations by providing an iterative curation pipeline that removes flawed problems and prevents data contamination. Each problem is paired with a detailed expert-authored solution path, enabling a new metric called Reasoning Path Fidelity (RPF), which assesses the quality of the reasoning process beyond just the accuracy of the final answer. When tested against a human baseline accuracy of 40.3%, leading models such as GPT-5 (High), Gemini 2.5 Pro, and DeepSeek-V3.1-Think achieved lower scores, indicating the challenging nature of the benchmark. SUPERChem requires multi-step, multimodal reasoning, highlighting how visual information impacts model performance differently. This benchmark also differentiates between models that reason with high fidelity and those that rely on heuristics. By setting a high bar and providing robust evaluation tools, SUPERChem aims to propel progress toward developing LLMs capable of expert-level chemical intelligence. The dataset is publicly accessible via Hugging Face. <div>
arXiv:2512.01274v1 Announce Type: new 
Abstract: Current benchmarks for evaluating the chemical reasoning capabilities of Large Language Models (LLMs) are limited by oversimplified tasks, lack of process-level evaluation, and misalignment with expert-level chemistry skills. To address these issues, we introduce SUPERChem, a benchmark of 500 expert-curated reasoning-intensive chemistry problems, covering diverse subfields and provided in both multimodal and text-only formats. Original content and an iterative curation pipeline eliminate flawed items and mitigate data contamination. Each problem is paired with an expert-authored solution path, enabling Reasoning Path Fidelity (RPF) scoring to evaluate reasoning quality beyond final-answer accuracy. Evaluations against a human baseline of 40.3% accuracy show that even the best-performing model, GPT-5 (High), reaches only 38.5%, followed closely by Gemini 2.5 Pro (37.9%) and DeepSeek-V3.1-Think (37.3%). SUPERChem elicits multi-step, multimodal reasoning, reveals model-dependent effects of visual information, and distinguishes high-fidelity reasoners from heuristic ones. By providing a challenging benchmark and a reliable evaluation framework, SUPERChem aims to facilitate the advancement of LLMs toward expert-level chemical intelligence. The dataset of the benchmark is available at https://huggingface.co/datasets/ZehuaZhao/SUPERChem.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Kardia-R1: Unleashing LLMs to Reason toward Understanding and Empathy for Emotional Support via Rubric-as-Judge Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.01282</link>
<guid>https://arxiv.org/abs/2512.01282</guid>
<content:encoded><![CDATA[
<div> Keywords: personalized empathy, emotional reasoning, user-grounded dataset, Rubric-ERL, Kardia-R1<br /><br />Summary:  
This paper addresses the limitations of current conversational agents, which often rely on situation-centric datasets without persistent user identity, limiting personalized emotional responses, and use opaque reward signals that hinder clear empathetic reasoning. To overcome these issues, the authors introduce KardiaBench, a large-scale benchmark featuring 178,080 QA pairs from 22,080 multi-turn conversations grounded on 671 real-world user profiles. KardiaBench is developed through a model-in-the-loop procedure with iterative rubric-guided refinement to ensure psychological plausibility, emotional fidelity, and persona consistency. The study proposes a progressive empathy pipeline integrating user comprehension, contextual reasoning, and emotion perception within conversations, enhanced by iterative critique and rubric-based improvements. Building upon this, the authors present Kardia-R1, a framework that trains models for interpretable, stepwise empathetic cognition using Rubric-as-Judge Empathetic Reinforcement Learning (Rubric-ERL). This GRPO-based method employs explainable, human-aligned rubric rewards to closely link user understanding, emotional inference, and supportive response generation. Extensive experiments on four large language model (LLM) backbones show that Kardia-R1 significantly outperforms existing methods in emotion accuracy, empathy, relevance, persona consistency, and safety. The dataset and model will be publicly released to facilitate further research in identity-aware empathetic conversational AI. <div>
arXiv:2512.01282v1 Announce Type: new 
Abstract: As web platforms evolve towards greater personalization and emotional complexity, conversational agents must transcend superficial empathy to demonstrate identity-aware emotional reasoning. However, existing systems face two limitations: (1) reliance on situation-centric datasets lacking persistent user identity, which hampers the capture of personalized affective nuances; and (2) dependence on opaque, coarse reward signals that hinder development of verifiable empathetic reasoning. To address these gaps, we introduce KardiaBench, a large-scale user-grounded benchmark comprising 178,080 QA pairs across 22,080 multi-turn conversations anchored to 671 real-world profiles. The dataset is constructed via a model-in-the-loop pipeline with iterative rubric-guided refinement to ensure psychological plausibility and persona consistency. This progressive empathy pipeline that integrates user comprehension, contextual reasoning, and emotion perception into conversations, followed by iterative critique and rubric-based refinement to ensure psychological plausibility, emotional fidelity, and persona consistency. Building on this, we propose Kardia-R1, a framework that trains models for interpretable, stepwise empathetic cognition. Kardia-R1 leverages Rubric-as-Judge Empathetic Reinforcement Learning (Rubric-ERL), a GRPO-based method that uses explainable, human-aligned rubric rewards to tightly couple user understanding, emotional inference, and supportive response generation. Extensive experiments across four LLM backbones demonstrate that Kardia-R1 consistently outperforms othet methods in emotion accuracy, empathy, relevance, persona consistency, and safety. Our dataset and model will be released at https://github.com/JhCircle/Kardia-R1.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agreement-Constrained Probabilistic Minimum Bayes Risk Decoding</title>
<link>https://arxiv.org/abs/2512.01316</link>
<guid>https://arxiv.org/abs/2512.01316</guid>
<content:encoded><![CDATA[
arXiv:2512.01316v1 Announce Type: new 
Abstract: Minimum Bayes risk (MBR) decoding generates high-quality translations by maximizing the expected utility of output candidates, but it evaluates all pairwise scores over the candidate set; hence, it takes quadratic time with respect to the number of candidates. To reduce the number of utility function calls, probabilistic MBR (PMBR) decoding partially evaluates quality scores using sampled pairs of candidates and completes the missing scores with a matrix completion algorithm. Nevertheless, it degrades the translation quality as the number of utility function calls is reduced. Therefore, to improve the trade-off between quality and cost, we propose agreement-constrained PMBR (AC-PMBR) decoding, which leverages a knowledge distilled model to guide the completion of the score matrix. Our AC-PMBR decoding improved approximation errors of matrix completion by up to 3 times and achieved higher translation quality compared with PMBR decoding at a comparable computational cost on the WMT'23 En$\leftrightarrow$De translation tasks.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MARSAD: A Multi-Functional Tool for Real-Time Social Media Analysis</title>
<link>https://arxiv.org/abs/2512.01369</link>
<guid>https://arxiv.org/abs/2512.01369</guid>
<content:encoded><![CDATA[
arXiv:2512.01369v1 Announce Type: new 
Abstract: MARSAD is a multifunctional natural language processing (NLP) platform designed for real-time social media monitoring and analysis, with a particular focus on the Arabic-speaking world. It enables researchers and non-technical users alike to examine both live and archived social media content, producing detailed visualizations and reports across various dimensions, including sentiment analysis, emotion analysis, propaganda detection, fact-checking, and hate speech detection. The platform also provides secure data-scraping capabilities through API keys for accessing public social media data. MARSAD's backend architecture integrates flexible document storage with structured data management, ensuring efficient processing of large and multimodal datasets. Its user-friendly frontend supports seamless data upload and interaction.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DyFuLM: An Advanced Multimodal Framework for Sentiment Analysis</title>
<link>https://arxiv.org/abs/2512.01410</link>
<guid>https://arxiv.org/abs/2512.01410</guid>
<content:encoded><![CDATA[
arXiv:2512.01410v1 Announce Type: new 
Abstract: Understanding sentiment in complex textual expressions remains a fundamental challenge in affective computing. To address this, we propose a Dynamic Fusion Learning Model (DyFuLM), a multimodal framework designed to capture both hierarchical semantic representations and fine-grained emotional nuances. DyFuLM introduces two key moodules: a Hierarchical Dynamic Fusion module that adaptively integrates multi-level features, and a Gated Feature Aggregation module that regulates cross-layer information ffow to achieve balanced representation learning. Comprehensive experiments on multi-task sentiment datasets demonstrate that DyFuLM achieves 82.64% coarse-grained and 68.48% fine-grained accuracy, yielding the lowest regression errors (MAE = 0.0674, MSE = 0.0082) and the highest R^2 coefficient of determination (R^2= 0.6903). Furthermore, the ablation study validates the effectiveness of each module in DyFuLM. When all modules are removed, the accuracy drops by 0.91% for coarse-grained and 0.68% for fine-grained tasks. Keeping only the gated fusion module causes decreases of 0.75% and 0.55%, while removing the dynamic loss mechanism results in drops of 0.78% and 0.26% for coarse-grained and fine-grained sentiment classification, respectively. These results demonstrate that each module contributes significantly to feature interaction and task balance. Overall, the experimental findings further validate that DyFuLM enhances sentiment representation and overall performance through effective hierarchical feature fusion.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PromptBridge: Cross-Model Prompt Transfer for Large Language Models</title>
<link>https://arxiv.org/abs/2512.01420</link>
<guid>https://arxiv.org/abs/2512.01420</guid>
<content:encoded><![CDATA[
arXiv:2512.01420v1 Announce Type: new 
Abstract: Large language models (LLMs) underpin applications in code generation, mathematical reasoning, and agent-based workflows. In practice, systems access LLMs via commercial APIs or open-source deployments, and the model landscape (e.g., GPT, Claude, Llama) evolves rapidly. This rapid evolution forces frequent model switches driven by capability, cost, deployment constraints, and privacy. Yet prompts are highly model-sensitive: reusing a prompt engineered for one model on another often yields substantially worse performance than a prompt optimized for the target model. We term this phenomenon Model Drifting. Through extensive empirical analysis across diverse LLM configurations, we show that model drifting is both common and severe. To address this challenge, we introduce PromptBridge, a training-free framework that preserves prompt effectiveness under model switches, enabling cross-model prompt transfer without costly per-task or per-model re-optimization. PromptBridge requires only a small set of alignment tasks for calibration. It first applies Model-Adaptive Reflective Prompt Evolution (MAP-RPE) to obtain task- and model-specific optimal prompts via iterative reflective refinement and quantitative evaluation. Using the resulting calibrated prompt pairs for the source and target models, PromptBridge learns a cross-model prompt mapping. At test time, i.e., for an unseen task, given a source-model prompt, this mapping directly produces an optimized prompt for the target model. Experiments in single-agent and multi-agent settings show that PromptBridge consistently improves downstream accuracy while reducing migration effort. The code will be available soon.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multilingual Conversational AI for Financial Assistance: Bridging Language Barriers in Indian FinTech</title>
<link>https://arxiv.org/abs/2512.01439</link>
<guid>https://arxiv.org/abs/2512.01439</guid>
<content:encoded><![CDATA[
arXiv:2512.01439v1 Announce Type: new 
Abstract: India's linguistic diversity presents both opportunities and challenges for fintech platforms. While the country has 31 major languages and over 100 minor ones, only 10\% of the population understands English, creating barriers to financial inclusion. We present a multilingual conversational AI system for a financial assistance use case that supports code-mixed languages like Hinglish, enabling natural interactions for India's diverse user base. Our system employs a multi-agent architecture with language classification, function management, and multilingual response generation. Through comparative analysis of multiple language models and real-world deployment, we demonstrate significant improvements in user engagement while maintaining low latency overhead (4-8\%). This work contributes to bridging the language gap in digital financial services for emerging markets.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MEGConformer: Conformer-Based MEG Decoder for Robust Speech and Phoneme Classification</title>
<link>https://arxiv.org/abs/2512.01443</link>
<guid>https://arxiv.org/abs/2512.01443</guid>
<content:encoded><![CDATA[
arXiv:2512.01443v1 Announce Type: new 
Abstract: We present Conformer-based decoders for the LibriBrain 2025 PNPL competition, targeting two foundational MEG tasks: Speech Detection and Phoneme Classification. Our approach adapts a compact Conformer to raw 306-channel MEG signals, with a lightweight convolutional projection layer and task-specific heads. For Speech Detection, a MEG-oriented SpecAugment provided a first exploration of MEG-specific augmentation. For Phoneme Classification, we used inverse-square-root class weighting and a dynamic grouping loader to handle 100-sample averaged examples. In addition, a simple instance-level normalization proved critical to mitigate distribution shifts on the holdout split. Using the official Standard track splits and F1-macro for model selection, our best systems achieved 88.9% (Speech) and 65.8% (Phoneme) on the leaderboard, surpassing the competition baselines and ranking within the top-10 in both tasks. For further implementation details, the technical documentation, source code, and checkpoints are available at https://github.com/neural2speech/libribrain-experiments.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing BERT Fine-Tuning for Sentiment Analysis in Lower-Resourced Languages</title>
<link>https://arxiv.org/abs/2512.01460</link>
<guid>https://arxiv.org/abs/2512.01460</guid>
<content:encoded><![CDATA[
arXiv:2512.01460v1 Announce Type: new 
Abstract: Limited data for low-resource languages typically yield weaker language models (LMs). Since pre-training is compute-intensive, it is more pragmatic to target improvements during fine-tuning. In this work, we examine the use of Active Learning (AL) methods augmented by structured data selection strategies which we term 'Active Learning schedulers', to boost the fine-tuning process with a limited amount of training data. We connect the AL to data clustering and propose an integrated fine-tuning pipeline that systematically combines AL, clustering, and dynamic data selection schedulers to enhance model's performance. Experiments in the Slovak, Maltese, Icelandic and Turkish languages show that the use of clustering during the fine-tuning phase together with AL scheduling can simultaneously produce annotation savings up to 30% and performance improvements up to four F1 score points, while also providing better fine-tuning stability.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MCAT: Scaling Many-to-Many Speech-to-Text Translation with MLLMs to 70 Languages</title>
<link>https://arxiv.org/abs/2512.01512</link>
<guid>https://arxiv.org/abs/2512.01512</guid>
<content:encoded><![CDATA[
arXiv:2512.01512v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have achieved great success in Speech-to-Text Translation (S2TT) tasks. However, current research is constrained by two key challenges: language coverage and efficiency. Most of the popular S2TT datasets are substantially English-centric, which restricts the scaling-up of MLLMs' many-to-many translation capabilities. Moreover, the inference speed of MLLMs degrades dramatically when the speech is converted into long sequences (e.g., 750 tokens). To address these limitations, we propose a Multilingual Cost-effective Accelerated Speech-to-Text Translator (MCAT) framework, which includes two innovations. First, a language scaling method that leverages curriculum learning and a data balancing strategy is introduced to extend the language coverage supported by MLLMs to 70 languages and achieve mutual translation among these languages. Second, an optimized speech adapter module is designed to reduce the length of the speech sequence to only 30 tokens. Extensive experiments were conducted on MLLMs of different scales (9B and 27B). The experimental results demonstrate that MCAT not only surpasses state-of-the-art end-to-end models on the FLEURS dataset across 70x69 directions but also enhances batch inference efficiency. This is achieved with only ~100M trainable parameters and by using only 10 hours of S2TT data per language. Furthermore, we have released MCAT as open-source to promote the development of MLLMs for robust S2TT capabilities. The code and models are released at https://github.com/yxduir/m2m-70.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language Diversity: Evaluating Language Usage and AI Performance on African Languages in Digital Spaces</title>
<link>https://arxiv.org/abs/2512.01557</link>
<guid>https://arxiv.org/abs/2512.01557</guid>
<content:encoded><![CDATA[
arXiv:2512.01557v1 Announce Type: new 
Abstract: This study examines the digital representation of African languages and the challenges this presents for current language detection tools. We evaluate their performance on Yoruba, Kinyarwanda, and Amharic. While these languages are spoken by millions, their online usage on conversational platforms is often sparse, heavily influenced by English, and not representative of the authentic, monolingual conversations prevalent among native speakers. This lack of readily available authentic data online creates a challenge of scarcity of conversational data for training language models. To investigate this, data was collected from subreddits and local news sources for each language. The analysis showed a stark contrast between the two sources. Reddit data was minimal and characterized by heavy code-switching. Conversely, local news media offered a robust source of clean, monolingual language data, which also prompted more user engagement in the local language on the news publishers social media pages. Language detection models, including the specialized AfroLID and a general LLM, performed with near-perfect accuracy on the clean news data but struggled with the code-switched Reddit posts. The study concludes that professionally curated news content is a more reliable and effective source for training context-rich AI models for African languages than data from conversational platforms. It also highlights the need for future models that can process clean and code-switched text to improve the detection accuracy for African languages.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAC-SLU: Multi-Intent Automotive Cabin Spoken Language Understanding Benchmark</title>
<link>https://arxiv.org/abs/2512.01603</link>
<guid>https://arxiv.org/abs/2512.01603</guid>
<content:encoded><![CDATA[
arXiv:2512.01603v1 Announce Type: new 
Abstract: Spoken Language Understanding (SLU), which aims to extract user semantics to execute downstream tasks, is a crucial component of task-oriented dialog systems. Existing SLU datasets generally lack sufficient diversity and complexity, and there is an absence of a unified benchmark for the latest Large Language Models (LLMs) and Large Audio Language Models (LALMs). This work introduces MAC-SLU, a novel Multi-Intent Automotive Cabin Spoken Language Understanding Dataset, which increases the difficulty of the SLU task by incorporating authentic and complex multi-intent data. Based on MAC-SLU, we conducted a comprehensive benchmark of leading open-source LLMs and LALMs, covering methods like in-context learning, supervised fine-tuning (SFT), and end-to-end (E2E) and pipeline paradigms. Our experiments show that while LLMs and LALMs have the potential to complete SLU tasks through in-context learning, their performance still lags significantly behind SFT. Meanwhile, E2E LALMs demonstrate performance comparable to pipeline approaches and effectively avoid error propagation from speech recognition. Code\footnote{https://github.com/Gatsby-web/MAC\_SLU} and datasets\footnote{huggingface.co/datasets/Gatsby1984/MAC\_SLU} are released publicly.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning the Boundary of Solvability: Aligning LLMs to Detect Unsolvable Problems</title>
<link>https://arxiv.org/abs/2512.01661</link>
<guid>https://arxiv.org/abs/2512.01661</guid>
<content:encoded><![CDATA[
arXiv:2512.01661v1 Announce Type: new 
Abstract: Ensuring LLM reliability requires not only solving complex problems but also recognizing when a problem is unsolvable. Current models often struggle to distinguish objective unsolvability (inherent contradictions in the problem) from subjective capability limitations (problems beyond the model's competence), which leads to hallucinations and overconfidence. To address this, we propose UnsolvableQA and UnsolvableRL to solve feasible problems, detect inherent contradictions, and prudently refuse tasks beyond capability. Specifically, we construct UnsolvableQA, a dataset of paired solvable and unsolvable instances derived via a dual-track methodology: programmatic generation for logic puzzles and a novel "Reverse Construction" method that injects contradictions into valid reasoning chains for mathematics. Building on this dataset, we introduce UnsolvableRL, a reinforcement learning framework with three reward components jointly accounting for accuracy, unsolvability, and difficulty. Empirical results show that our approach achieves near-perfect unsolvability detection while also improving accuracy on solvable tasks. Crucially, we identify Capability Collapse, demonstrating that explicit exposure to unsolvable data is indispensable for preventing models from becoming systematically overconfident. Our code and data are available at https://github.com/sfasfaffa/unsolvableQA.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMAG: Mixed Memory-Augmented Generation for Large Language Models Applications</title>
<link>https://arxiv.org/abs/2512.01710</link>
<guid>https://arxiv.org/abs/2512.01710</guid>
<content:encoded><![CDATA[
arXiv:2512.01710v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel at generating coherent text within a single prompt but fall short in sustaining relevance, personalization, and continuity across extended interactions. Human communication, however, relies on multiple forms of memory, from recalling past conversations to adapting to personal traits and situational context. This paper introduces the Mixed Memory-Augmented Generation (MMAG) pattern, a framework that organizes memory for LLM-based agents into five interacting layers: conversational, long-term user, episodic and event-linked, sensory and context-aware, and short-term working memory. Drawing inspiration from cognitive psychology, we map these layers to technical components and outline strategies for coordination, prioritization, and conflict resolution. We demonstrate the approach through its implementation in the Heero conversational agent, where encrypted long-term bios and conversational history already improve engagement and retention. We further discuss implementation concerns around storage, retrieval, privacy, and latency, and highlight open challenges. MMAG provides a foundation for building memory-rich language agents that are more coherent, proactive, and aligned with human needs.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Borrowing Detection on Multilingual Wordlists</title>
<link>https://arxiv.org/abs/2512.01713</link>
<guid>https://arxiv.org/abs/2512.01713</guid>
<content:encoded><![CDATA[
arXiv:2512.01713v1 Announce Type: new 
Abstract: This paper presents a fully self-supervised approach to borrowing detection in multilingual wordlists. The method combines two sources of information: PMI similarities based on a global correspondence model and a lightweight contrastive component trained on phonetic feature vectors. It further includes an automatic procedure for selecting decision thresholds without requiring labeled data. Experiments on benchmark datasets show that PMI alone already improves over existing string similarity measures such as NED and SCA, and that the combined similarity performs on par with or better than supervised baselines. An ablation study highlights the importance of character encoding, temperature settings and augmentation strategies. The approach scales to datasets of different sizes, works without manual supervision and is provided with a command-line tool that allows researchers to conduct their own studies.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beware of Reasoning Overconfidence: Pitfalls in the Reasoning Process for Multi-solution Tasks</title>
<link>https://arxiv.org/abs/2512.01725</link>
<guid>https://arxiv.org/abs/2512.01725</guid>
<content:encoded><![CDATA[
arXiv:2512.01725v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel in reasoning tasks requiring a single correct answer, but they perform poorly in multi-solution tasks that require generating comprehensive and diverse answers. We attribute this limitation to \textbf{reasoning overconfidence}: a tendency to express undue certainty in an incomplete solution set. To examine the effect, we introduce \textit{MuSoBench}, a benchmark of multi-solution problems. Experiments show that the conventional short chain-of-thought (Short-CoT) prompting paradigm exhibits pronounced overconfidence, whereas the emerging long chain-of-thought (Long-CoT) approach mitigates it through iterative exploration and self-reflection. We further characterise observable behaviours and influential factors. To probe the underlying cause, we propose the \textbf{cognitive-rigidity hypothesis}, which posits that overconfidence arises when the reasoning process prematurely converges on a narrow set of thought paths. An attention-entropy analysis offers preliminary support for this view. These findings provide tools for assessing the completeness of LLM reasoning and highlight the need to move evaluation beyond single-answer accuracy toward comprehensive exploration.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning About the Unsaid: Misinformation Detection with Omission-Aware Graph Inference</title>
<link>https://arxiv.org/abs/2512.01728</link>
<guid>https://arxiv.org/abs/2512.01728</guid>
<content:encoded><![CDATA[
arXiv:2512.01728v1 Announce Type: new 
Abstract: This paper investigates the detection of misinformation, which deceives readers by explicitly fabricating misleading content or implicitly omitting important information necessary for informed judgment. While the former has been extensively studied, omission-based deception remains largely overlooked, even though it can subtly guide readers toward false conclusions under the illusion of completeness. To pioneer in this direction, this paper presents OmiGraph, the first omission-aware framework for misinformation detection. Specifically, OmiGraph constructs an omission-aware graph for the target news by utilizing a contextual environment that captures complementary perspectives of the same event, thereby surfacing potentially omitted contents. Based on this graph, omission-oriented relation modeling is then proposed to identify the internal contextual dependencies, as well as the dynamic omission intents, formulating a comprehensive omission relation representation. Finally, to extract omission patterns for detection, OmiGraph introduces omission-aware message-passing and aggregation that establishes holistic deception perception by integrating the omission contents and relations. Experiments show that, by considering the omission perspective, our approach attains remarkable performance, achieving average improvements of +5.4% F1 and +5.3% ACC on two large-scale benchmarks.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InnoGym: Benchmarking the Innovation Potential of AI Agents</title>
<link>https://arxiv.org/abs/2512.01822</link>
<guid>https://arxiv.org/abs/2512.01822</guid>
<content:encoded><![CDATA[
arXiv:2512.01822v1 Announce Type: new 
Abstract: LLMs and Agents have achieved impressive progress in code generation, mathematical reasoning, and scientific discovery. However, existing benchmarks primarily measure correctness, overlooking the diversity of methods behind solutions. True innovation depends not only on producing correct answers but also on the originality of the approach. We present InnoGym, the first benchmark and framework designed to systematically evaluate the innovation potential of AI agents. InnoGym introduces two complementary metrics: performance gain, which measures improvement over the best-known solutions, and novelty, which captures methodological differences from prior approaches. The benchmark includes 18 carefully curated tasks from real-world engineering and scientific domains, each standardized through resource filtering, evaluator validation, and solution collection. In addition, we provide iGym, a unified execution environment for reproducible and long-horizon evaluations. Extensive experiments show that while some agents produce novel approaches, their lack of robustness limits performance gains. These results highlight a key gap between creativity and effectiveness, underscoring the need for benchmarks that evaluate both.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond SFT: Reinforcement Learning for Safer Large Reasoning Models with Better Reasoning Ability</title>
<link>https://arxiv.org/abs/2512.01848</link>
<guid>https://arxiv.org/abs/2512.01848</guid>
<content:encoded><![CDATA[
arXiv:2512.01848v1 Announce Type: new 
Abstract: Large reasoning models (LRMs) extend large language models by generating explicit chain-of-thought (CoT) reasoning, significantly improving mathematical and logical problem solving. However, this explicit reasoning process also introduces new safety risks, as unsafe behaviors often emerge within intermediate reasoning trajectories, even when final answers appear harmless. Existing safety alignment approaches primarily rely on supervised fine-tuning (SFT) over safety-oriented long CoT datasets. While intuitive, we find that SFT produces inconsistent safety improvements, degrades reasoning ability, and generalizes poorly across model families. These limitations suggest that purely supervised approaches are insufficient for robust safety alignment in LRMs. To address this, we investigate reinforcement learning (RL) as a complementary optimization framework for LRM safety training. Unlike SFT, RL directly optimizes model policies with reward feedback, enabling more adaptive and stable alignment. Extensive experiments across multiple model families and benchmarks show that RL achieves stronger and more consistent safety gains while maintaining reasoning competence. Further analysis of reflection dynamics and token-level entropy reveals that RL suppresses unsafe exploratory reasoning while preserving reflective depth, leading to safer and more reliable reasoning processes.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BHRAM-IL: A Benchmark for Hallucination Recognition and Assessment in Multiple Indian Languages</title>
<link>https://arxiv.org/abs/2512.01852</link>
<guid>https://arxiv.org/abs/2512.01852</guid>
<content:encoded><![CDATA[
arXiv:2512.01852v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed in multilingual applications but often generate plausible yet incorrect or misleading outputs, known as hallucinations. While hallucination detection has been studied extensively in English, under-resourced Indian languages remain largely unexplored. We present BHRAM-IL, a benchmark for hallucination recognition and assessment in multiple Indian languages, covering Hindi, Gujarati, Marathi, Odia, along with English. The benchmark comprises 36,047 curated questions across nine categories spanning factual, numerical, reasoning, and linguistic tasks. We evaluate 14 state-of-the-art multilingual LLMs on a benchmark subset of 10,265 questions, analyzing cross-lingual and factual hallucinations across languages, models, scales, categories, and domains using category-specific metrics normalized to (0,1) range. Aggregation over all categories and models yields a primary score of 0.23 and a language-corrected fuzzy score of 0.385, demonstrating the usefulness of BHRAM-IL for hallucination-focused evaluation. The dataset, and the code for generation and evaluation are available on GitHub (https://github.com/sambhashana/BHRAM-IL/) and HuggingFace (https://huggingface.co/datasets/sambhashana/BHRAM-IL/) to support future research in multilingual hallucination detection and mitigation.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Lingual Interleaving for Speech Language Models</title>
<link>https://arxiv.org/abs/2512.01865</link>
<guid>https://arxiv.org/abs/2512.01865</guid>
<content:encoded><![CDATA[
arXiv:2512.01865v1 Announce Type: new 
Abstract: Spoken Language Models (SLMs) aim to learn linguistic competence directly from speech using discrete units, widening access to Natural Language Processing (NLP) technologies for languages with limited written resources. However, progress has been largely English-centric due to scarce spoken evaluation benchmarks and training data, making cross-lingual learning difficult. We present a cross-lingual interleaving method that mixes speech tokens across languages without textual supervision. We also release an EN-FR training dataset, TinyStories (~42k hours), together with EN-FR spoken StoryCloze and TopicCloze benchmarks for cross-lingual semantic evaluation, both synthetically generated using GPT-4. On 360M and 1B SLMs under matched training-token budgets, interleaving improves monolingual semantic accuracy, enables robust cross-lingual continuation, and strengthens cross-lingual hidden-state alignment. Taken together, these results indicate that cross-lingual interleaving is a simple, scalable route to building multilingual SLMs that understand and converse across languages. All resources will be made open-source to support reproducibility.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Human Perceptions of AI Responses: Insights from a Mixed-Methods Study on Risk Mitigation in Generative Models</title>
<link>https://arxiv.org/abs/2512.01892</link>
<guid>https://arxiv.org/abs/2512.01892</guid>
<content:encoded><![CDATA[
arXiv:2512.01892v1 Announce Type: new 
Abstract: With the rapid uptake of generative AI, investigating human perceptions of generated responses has become crucial. A major challenge is their `aptitude' for hallucinating and generating harmful contents. Despite major efforts for implementing guardrails, human perceptions of these mitigation strategies are largely unknown. We conducted a mixed-method experiment for evaluating the responses of a mitigation strategy across multiple-dimensions: faithfulness, fairness, harm-removal capacity, and relevance. In a within-subject study design, 57 participants assessed the responses under two conditions: harmful response plus its mitigation and solely mitigated response. Results revealed that participants' native language, AI work experience, and annotation familiarity significantly influenced evaluations. Participants showed high sensitivity to linguistic and contextual attributes, penalizing minor grammar errors while rewarding preserved semantic contexts. This contrasts with how language is often treated in the quantitative evaluation of LLMs. We also introduced new metrics for training and evaluating mitigation strategies and insights for human-AI evaluation studies.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OPOR-Bench: Evaluating Large Language Models on Online Public Opinion Report Generation</title>
<link>https://arxiv.org/abs/2512.01896</link>
<guid>https://arxiv.org/abs/2512.01896</guid>
<content:encoded><![CDATA[
arXiv:2512.01896v1 Announce Type: new 
Abstract: Online Public Opinion Reports consolidate news and social media for timely crisis management by governments and enterprises. While large language models have made automated report generation technically feasible, systematic research in this specific area remains notably absent, particularly lacking formal task definitions and corresponding benchmarks. To bridge this gap, we define the Automated Online Public Opinion Report Generation (OPOR-GEN) task and construct OPOR-BENCH, an event-centric dataset covering 463 crisis events with their corresponding news articles, social media posts, and a reference summary. To evaluate report quality, we propose OPOR-EVAL, a novel agent-based framework that simulates human expert evaluation by analyzing generated reports in context. Experiments with frontier models demonstrate that our framework achieves high correlation with human judgments. Our comprehensive task definition, benchmark dataset, and evaluation framework provide a solid foundation for future research in this critical domain.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent Debate: A Surrogate Framework for Interpreting LLM Thinking</title>
<link>https://arxiv.org/abs/2512.01909</link>
<guid>https://arxiv.org/abs/2512.01909</guid>
<content:encoded><![CDATA[
arXiv:2512.01909v1 Announce Type: new 
Abstract: Understanding the internal thinking process of Large Language Models (LLMs) and the cause of hallucinations remains a key challenge. To this end, we introduce latent debate, a novel framework for interpreting model predictions through the lens of implicit internal arguments. Unlike the current work of self-consistency and multi-agent debate, which relies on explicit debates among multiple answers or multiple models, latent debate captures the hidden supporting and attacking signals that arise within a single model during a single inference. We first present a model- and task-agnostic conceptual framework, and then instantiate it symbolically to approximate the thinking process of LLMs on True/False prediction tasks. Empirical studies demonstrate that latent debate is a faithful structured surrogate model that has highly consistent predictions with the original LLM. Beyond interpretability, we demonstrate that latent debate provides a strong baseline for hallucination detection. Further analysis reveals strong correlations between hallucinations and debate patterns, such as a high degree of latent debates in the middle layers is linked to a higher risk of hallucinations. These findings position latent debate as a potential framework for understanding internal mechanisms of LLMs, especially for scenarios where internal (dis)agreements appear during the inference steps.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rectifying LLM Thought from Lens of Optimization</title>
<link>https://arxiv.org/abs/2512.01925</link>
<guid>https://arxiv.org/abs/2512.01925</guid>
<content:encoded><![CDATA[
arXiv:2512.01925v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) have been driven by their emergent reasoning capabilities, particularly through long chain-of-thought (CoT) prompting, which enables thorough exploration and deliberation. Despite these advances, long-CoT LLMs often exhibit suboptimal reasoning behaviors, such as overthinking and excessively protracted reasoning chains, which can impair performance. In this paper, we analyze reasoning processes through an optimization lens, framing CoT as a gradient descent procedure where each reasoning step constitutes an update toward problem resolution. Building on this perspective, we introduce RePro (Rectifying Process-level Reward), a novel approach to refine LLM reasoning during post-training. RePro defines a surrogate objective function to assess the optimization process underlying CoT, utilizing a dual scoring mechanism to quantify its intensity and stability. These scores are aggregated into a composite process-level reward, seamlessly integrated into reinforcement learning with verifiable rewards (RLVR) pipelines to optimize LLMs. Extensive experiments across multiple reinforcement learning algorithms and diverse LLMs, evaluated on benchmarks spanning mathematics, science, and coding, demonstrate that RePro consistently enhances reasoning performance and mitigates suboptimal reasoning behaviors.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Far Are We from Genuinely Useful Deep Research Agents?</title>
<link>https://arxiv.org/abs/2512.01948</link>
<guid>https://arxiv.org/abs/2512.01948</guid>
<content:encoded><![CDATA[
arXiv:2512.01948v1 Announce Type: new 
Abstract: Deep Research Agents (DRAs) aim to automatically produce analyst-level reports through iterative information retrieval and synthesis. However, most existing DRAs were validated on question-answering benchmarks, while research on generating comprehensive reports remains overlooked. Worse, current benchmarks for report synthesis suffer from task complexity and subjective metrics -- this fails to reflect user demands and limits the practical utility of generated reports. To address these gaps, we present Fine-grained DEepResearch bench (FINDER), an enhanced benchmark consisting of 100 human-curated research tasks with 419 structured checklist items that standardize report structure, analytical depth, and factual grounding. Based on approximately 1,000 reports produced by mainstream DRAs, we further propose Deep rEsearch Failure Taxonomy (DEFT), the first failure taxonomy for deep research agents. DEFT contains 14 fine-grained failure modes across reasoning, retrieval, and generation, and is built upon grounded theory with human-LLM co-annotating and inter-annotator reliability validation. Our experimental findings reveal that current DRAs struggle not with task comprehension but with evidence integration, verification, and reasoning-resilient planning.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Art of Scaling Test-Time Compute for Large Language Models</title>
<link>https://arxiv.org/abs/2512.02008</link>
<guid>https://arxiv.org/abs/2512.02008</guid>
<content:encoded><![CDATA[
arXiv:2512.02008v1 Announce Type: new 
Abstract: Test-time scaling (TTS) -- the dynamic allocation of compute during inference -- is a promising direction for improving reasoning in large language models (LLMs). However, a systematic comparison of well-known TTS strategies under identical conditions is missing, and the influence of model type and problem difficulty on performance remains unclear. To address these gaps, we conduct the first large-scale study of TTS, spanning over thirty billion tokens generated using eight open-source LLMs (7B to 235B parameters), across four reasoning datasets. We observe three consistent trends: (1) no single TTS strategy universally dominates; (2) reasoning models exhibit distinct trace-quality patterns across problem difficulty and trace length, forming short-horizon and long-horizon categories; and (3) for a given model type, the optimal TTS performance scales monotonically with compute budget. Based on these insights, we provide a practical recipe for selecting the best TTS strategy, considering problem difficulty, model type, and compute budget, providing a practical guide to effective inference-time scaling.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Four Over Six: More Accurate NVFP4 Quantization with Adaptive Block Scaling</title>
<link>https://arxiv.org/abs/2512.02010</link>
<guid>https://arxiv.org/abs/2512.02010</guid>
<content:encoded><![CDATA[
arXiv:2512.02010v1 Announce Type: new 
Abstract: As large language models have grown larger, low-precision numerical formats such as NVFP4 have become increasingly popular due to the speed and memory benefits they provide. However, to accelerate computation with NVFP4, all matrix multiplication operands--weights and activations in the forward pass, and weights, activations, and gradients in the backward pass--must be quantized to NVFP4, often leading to divergence during training and performance degradation during inference. NVFP4 by evaluating multiple potential scale factors for each block of values. To address this issue, in this work we introduce Four Over Six (4/6), a modification to the NVFP4 quantization algorithm that evaluates two potential scale factors for each block of values. Unlike integer formats, floating-point formats such as FP4 have the most quantization error on near-maximal values in each block, which we find to be primarily responsible for downstream performance degradation. We find that for some blocks, scaling to smaller FP4 values makes the distribution of representable values more uniform, improving representation of near-maximal values. Importantly, 4/6 can be implemented efficiently on NVIDIA Blackwell GPUs, making it viable to use while training LLMs with NVFP4. In pre-training experiments with transformer and hybrid model architectures, we find that 4/6 prevents divergence in several cases, bringing training loss significantly closer to BF16 compared to models trained with current state-of-the-art NVFP4 training recipes. We also find that 4/6 can be easily incorporated into many different post-training quantization methods and generally improves downstream accuracy. We hope this inspires future work in training and deploying models with NVFP4.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VeriPy - A New Python-Based Approach for SDR Pipelined/Unrolled Hardware Accelerator Generation</title>
<link>https://arxiv.org/abs/2512.00006</link>
<guid>https://arxiv.org/abs/2512.00006</guid>
<content:encoded><![CDATA[
arXiv:2512.00006v1 Announce Type: cross 
Abstract: Software-defined radio (SDR) plays an important role in the communication field by providing a flexible and customized communication system for different purposes according to the needs. To enhance the performance of SDR applications, hardware accelerators have been widely deployed in recent years. In facing this obstacle, a necessity arises for a high-level synthesis (HLS) tool specifically designed for communication engineers without detailed hardware knowledge. To lower the barrier between SDR engineers and hardware development, this work proposed a Python-based HLS tool, VeriPy, which can generate both mainstream architecture for hardware accelerators in Verilog specifically for SDR designs including unrolled design and pipelined design, requiring no detailed digital hardware knowledge or Hardware Description Languages (HDL). Furthermore, VeriPy supports automatic testbench generation with random input stimulus, an extensible hardware library, performance and resource estimation, and offers strong optimisation potential at both the algorithmic and digital hardware levels. The generated hardware design by VeriPy can achieve up to 70% faster operating frequency compared to pragma-optimised Vivado HLS designs with a reasonably higher resource con-sumption while delivering comparable performance and resource consumption to hand-coded implementations. Regarding code complexity, VeriPy requires no pragmas, completely eliminating the need for low-level hardware knowledge. For straightforward algorithms, the input code length remains comparable to that of Vivado HLS.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Use of Retrieval-Augmented Large Language Model Agent for Long-Form COVID-19 Fact-Checking</title>
<link>https://arxiv.org/abs/2512.00007</link>
<guid>https://arxiv.org/abs/2512.00007</guid>
<content:encoded><![CDATA[
arXiv:2512.00007v1 Announce Type: cross 
Abstract: The COVID-19 infodemic calls for scalable fact-checking solutions that handle long-form misinformation with accuracy and reliability. This study presents SAFE (system for accurate fact extraction and evaluation), an agent system that combines large language models with retrieval-augmented generation (RAG) to improve automated fact-checking of long-form COVID-19 misinformation. SAFE includes two agents - one for claim extraction and another for claim verification using LOTR-RAG, which leverages a 130,000-document COVID-19 research corpus. An enhanced variant, SAFE (LOTR-RAG + SRAG), incorporates Self-RAG to refine retrieval via query rewriting. We evaluated both systems on 50 fake news articles (2-17 pages) containing 246 annotated claims (M = 4.922, SD = 3.186), labeled as true (14.1%), partly true (14.4%), false (27.0%), partly false (2.2%), and misleading (21.0%) by public health professionals. SAFE systems significantly outperformed baseline LLMs in all metrics (p < 0.001). For consistency (0-1 scale), SAFE (LOTR-RAG) scored 0.629, exceeding both SAFE (+SRAG) (0.577) and the baseline (0.279). In subjective evaluations (0-4 Likert scale), SAFE (LOTR-RAG) also achieved the highest average ratings in usefulness (3.640), clearness (3.800), and authenticity (3.526). Adding SRAG slightly reduced overall performance, except for a minor gain in clearness. SAFE demonstrates robust improvements in long-form COVID-19 fact-checking by addressing LLM limitations in consistency and explainability. The core LOTR-RAG design proved more effective than its SRAG-augmented variant, offering a strong foundation for scalable misinformation mitigation.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LM4Opt-RA: A Multi-Candidate LLM Framework with Structured Ranking for Automating Network Resource Allocation</title>
<link>https://arxiv.org/abs/2512.00039</link>
<guid>https://arxiv.org/abs/2512.00039</guid>
<content:encoded><![CDATA[
arXiv:2512.00039v1 Announce Type: cross 
Abstract: Building on advancements in Large Language Models (LLMs), we can tackle complex analytical and mathematical reasoning tasks requiring nuanced contextual understanding. A prime example of such complex tasks is modelling resource allocation optimization in networks, which extends beyond translating natural language inputs into mathematical equations or Linear Programming (LP), Integer Linear Programming (ILP), and Mixed-Integer Linear Programming (MILP) models. However, existing benchmarks and datasets cannot address the complexities of such problems with dynamic environments, interdependent variables, and heterogeneous constraints. To address this gap, we introduce NL4RA, a curated dataset comprising 50 resource allocation optimization problems formulated as LP, ILP, and MILP. We then evaluate the performance of well-known open-source LLMs with varying parameter counts. To enhance existing LLM based methods, we introduce LM4Opt RA, a multi candidate framework that applies diverse prompting strategies such as direct, few shot, and chain of thought, combined with a structured ranking mechanism to improve accuracy. We identified discrepancies between human judgments and automated scoring such as ROUGE, BLEU, or BERT scores. However, human evaluation is time-consuming and requires specialized expertise, making it impractical for a fully automated end-to-end framework. To quantify the difference between LLM-generated responses and ground truth, we introduce LLM-Assisted Mathematical Evaluation (LAME), an automated metric designed for mathematical formulations. Using LM4Opt-RA, Llama-3.1-70B achieved a LAME score of 0.8007, outperforming other models by a significant margin, followed closely by Llama-3.1-8B. While baseline LLMs demonstrate considerable promise, they still lag behind human expertise; our proposed method surpasses these baselines regarding LAME and other metrics.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Closing the Gap: Data-Centric Fine-Tuning of Vision Language Models for the Standardized Exam Questions</title>
<link>https://arxiv.org/abs/2512.00042</link>
<guid>https://arxiv.org/abs/2512.00042</guid>
<content:encoded><![CDATA[
arXiv:2512.00042v1 Announce Type: cross 
Abstract: Multimodal reasoning has become a cornerstone of modern AI research. Standardized exam questions offer a uniquely rigorous testbed for such reasoning, providing structured visual contexts and verifiable answers. While recent progress has largely focused on algorithmic advances such as reinforcement learning (e.g., GRPO, DPO), the data centric foundations of vision language reasoning remain less explored.
  We show that supervised fine-tuning (SFT) with high-quality data can rival proprietary approaches. To this end, we compile a 161.4 million token multimodal dataset combining textbook question-solution pairs, curriculum aligned diagrams, and contextual materials, and fine-tune Qwen-2.5VL-32B using an optimized reasoning syntax (QMSA). The resulting model achieves 78.6% accuracy, only 1.0% below Gemini 2.0 Flash, on our newly released benchmark YKSUniform, which standardizes 1,854 multimodal exam questions across 309 curriculum topics.
  Our results reveal that data composition and representational syntax play a decisive role in multimodal reasoning. This work establishes a data centric framework for advancing open weight vision language models, demonstrating that carefully curated and curriculum-grounded multimodal data can elevate supervised fine-tuning to near state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring What LLMs Think They Do: SHAP Faithfulness and Deployability on Financial Tabular Classification</title>
<link>https://arxiv.org/abs/2512.00163</link>
<guid>https://arxiv.org/abs/2512.00163</guid>
<content:encoded><![CDATA[
arXiv:2512.00163v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have attracted significant attention for classification tasks, offering a flexible alternative to trusted classical machine learning models like LightGBM through zero-shot prompting. However, their reliability for structured tabular data remains unclear, particularly in high stakes applications like financial risk assessment. Our study systematically evaluates LLMs and generates their SHAP values on financial classification tasks. Our analysis shows a divergence between LLMs self-explanation of feature impact and their SHAP values, as well as notable differences between LLMs and LightGBM SHAP values. These findings highlight the limitations of LLMs as standalone classifiers for structured financial modeling, but also instill optimism that improved explainability mechanisms coupled with few-shot prompting will make LLMs usable in risk-sensitive domains.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Challenges of Heterogeneity in Big Data: A Comparative Study of Classification in Large-Scale Structured and Unstructured Domains</title>
<link>https://arxiv.org/abs/2512.00298</link>
<guid>https://arxiv.org/abs/2512.00298</guid>
<content:encoded><![CDATA[
arXiv:2512.00298v1 Announce Type: cross 
Abstract: This study analyzes the impact of heterogeneity ("Variety") in Big Data by comparing classification strategies across structured (Epsilon) and unstructured (Rest-Mex, IMDB) domains. A dual methodology was implemented: evolutionary and Bayesian hyperparameter optimization (Genetic Algorithms, Optuna) in Python for numerical data, and distributed processing in Apache Spark for massive textual corpora. The results reveal a "complexity paradox": in high-dimensional spaces, optimized linear models (SVM, Logistic Regression) outperformed deep architectures and Gradient Boosting. Conversely, in text-based domains, the constraints of distributed fine-tuning led to overfitting in complex models, whereas robust feature engineering -- specifically Transformer-based embeddings (ROBERTa) and Bayesian Target Encoding -- enabled simpler models to generalize effectively. This work provides a unified framework for algorithm selection based on data nature and infrastructure constraints.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Progressive Code Integration for Abstractive Bug Report Summarization</title>
<link>https://arxiv.org/abs/2512.00325</link>
<guid>https://arxiv.org/abs/2512.00325</guid>
<content:encoded><![CDATA[
arXiv:2512.00325v1 Announce Type: cross 
Abstract: Bug reports are often unstructured and verbose, making it challenging for developers to efficiently comprehend software issues. Existing summarization approaches typically rely on surface-level textual cues, resulting in incomplete or redundant summaries, and they frequently ignore associated code snippets, which are essential for accurate defect diagnosis. To address these limitations, we propose a progressive code-integration framework for LLM-based abstractive bug report summarization. Our approach incrementally incorporates long code snippets alongside textual content, overcoming standard LLM context window constraints and producing semantically rich summaries. Evaluated on four benchmark datasets using eight LLMs, our pipeline outperforms extractive baselines by 7.5%-58.2% and achieves performance comparable to state-of-the-art abstractive methods, highlighting the benefits of jointly leveraging textual and code information for enhanced bug comprehension.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking It Down: Domain-Aware Semantic Segmentation for Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2512.00367</link>
<guid>https://arxiv.org/abs/2512.00367</guid>
<content:encoded><![CDATA[
arXiv:2512.00367v1 Announce Type: cross 
Abstract: Document chunking is a crucial component of Retrieval-Augmented Generation (RAG), as it directly affects the retrieval of relevant and precise context. Conventional fixed-length and recursive splitters often produce arbitrary, incoherent segments that fail to preserve semantic structure. Although semantic chunking has gained traction, its influence on generation quality remains underexplored. This paper introduces two efficient semantic chunking methods, Projected Similarity Chunking (PSC) and Metric Fusion Chunking (MFC), trained on PubMed data using three different embedding models. We further present an evaluation framework that measures the effect of chunking on both retrieval and generation by augmenting PubMedQA with full-text PubMed Central articles. Our results show substantial retrieval improvements (24x with PSC) in MRR and higher Hits@k on PubMedQA. We provide a comprehensive analysis, including statistical significance and response-time comparisons with common chunking libraries. Despite being trained on a single domain, PSC and MFC also generalize well, achieving strong out-of-domain generation performance across multiple datasets. Overall, our findings confirm that our semantic chunkers, especially PSC, consistently deliver superior performance.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Whose Personae? Synthetic Persona Experiments in LLM Research and Pathways to Transparency</title>
<link>https://arxiv.org/abs/2512.00461</link>
<guid>https://arxiv.org/abs/2512.00461</guid>
<content:encoded><![CDATA[
arXiv:2512.00461v1 Announce Type: cross 
Abstract: Synthetic personae experiments have become a prominent method in Large Language Model alignment research, yet the representativeness and ecological validity of these personae vary considerably between studies. Through a review of 63 peer-reviewed studies published between 2023 and 2025 in leading NLP and AI venues, we reveal a critical gap: task and population of interest are often underspecified in persona-based experiments, despite personalization being fundamentally dependent on these criteria. Our analysis shows substantial differences in user representation, with most studies focusing on limited sociodemographic attributes and only 35% discussing the representativeness of their LLM personae. Based on our findings, we introduce a persona transparency checklist that emphasizes representative sampling, explicit grounding in empirical data, and enhanced ecological validity. Our work provides both a comprehensive assessment of current practices and practical guidelines to improve the rigor and ecological validity of persona-based evaluations in language model alignment research.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rep3Net: An Approach Exploiting Multimodal Representation for Molecular Bioactivity Prediction</title>
<link>https://arxiv.org/abs/2512.00521</link>
<guid>https://arxiv.org/abs/2512.00521</guid>
<content:encoded><![CDATA[
arXiv:2512.00521v1 Announce Type: cross 
Abstract: In early stage drug discovery, bioactivity prediction of molecules against target proteins plays a crucial role. Trdaitional QSAR models that utilizes molecular descriptor based data often struggles to predict bioactivity of molecules effectively due to its limitation in capturing structural and contextual information embedded within each compound. To address this challenge, we propose Rep3Net, a unified deep learning architecture that not only incorporates descriptor data but also includes spatial and relational information through graph-based represenation of compounds and contextual information through ChemBERTa generated embeddings from SMILES strings. Our model employing multimodal concatenated features produce reliable bioactivity prediction on Poly [ADP-ribose] polymerase 1 (PARP-1) dataset. PARP-1 is a crucial agent in DNA damage repair and has become a significant theraputic target in malignancies that depend on it for survival and growth. A comprehensive analysis and comparison with conventional standalone models including GCN, GAT, XGBoost, etc. demonstrates that our architecture achieves the highest predictive performance. In computational screening of compounds in drug discovery, our architecture provides a scalable framework for bioactivity prediction.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bias Testing and Mitigation in Black Box LLMs using Metamorphic Relations</title>
<link>https://arxiv.org/abs/2512.00556</link>
<guid>https://arxiv.org/abs/2512.00556</guid>
<content:encoded><![CDATA[
arXiv:2512.00556v1 Announce Type: cross 
Abstract: The widespread deployment of Large Language Models (LLMs) has intensified concerns about subtle social biases embedded in their outputs. Existing guardrails often fail when faced with indirect or contextually complex bias-inducing prompts. To address these limitations, we propose a unified framework for both systematic bias evaluation and targeted mitigation. Our approach introduces six novel Metamorphic Relations (MRs) that, based on metamorphic testing principles, transform direct bias-inducing inputs into semantically equivalent yet adversarially challenging variants. These transformations enable an automated method for exposing hidden model biases: when an LLM responds inconsistently or unfairly across MR-generated variants, the underlying bias becomes detectable. We further show that the same MRs can be used to generate diverse bias-inducing samples for fine-tuning, directly linking the testing process to mitigation. Using six state-of-the-art LLMs - spanning open-source and proprietary models - and a representative subset of 385 questions from the 8,978-item BiasAsker benchmark covering seven protected groups, our MRs reveal up to 14% more hidden biases compared to existing tools. Moreover, fine-tuning with both original and MR-mutated samples significantly enhances bias resiliency, increasing safe response rates from 54.7% to over 88.9% across models. These results highlight metamorphic relations as a practical mechanism for improving fairness in conversational AI.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Statistical NLP for Optimization of Clinical Trial Success Prediction in Pharmaceutical R&amp;D</title>
<link>https://arxiv.org/abs/2512.00586</link>
<guid>https://arxiv.org/abs/2512.00586</guid>
<content:encoded><![CDATA[
arXiv:2512.00586v1 Announce Type: cross 
Abstract: This work presents the development and evaluation of an NLP-enabled probabilistic classifier designed to estimate the probability of technical and regulatory success (pTRS) for clinical trials in the field of neuroscience. While pharmaceutical R&amp;D is plagued by high attrition rates and enormous costs, particularly within neuroscience, where success rates are below 10%, timely identification of promising programs can streamline resource allocation and reduce financial risk. Leveraging data from the ClinicalTrials.gov database and success labels from the recently developed Clinical Trial Outcome dataset, the classifier extracts text-based clinical trial features using statistical NLP techniques. These features were integrated into several non-LLM frameworks (logistic regression, gradient boosting, and random forest) to generate calibrated probability scores. Model performance was assessed on a retrospective dataset of 101,145 completed clinical trials spanning 1976-2024, achieving an overall ROC-AUC of 0.64. An LLM-based predictive model was then built using BioBERT, a domain-specific language representation encoder. The BioBERT-based model achieved an overall ROC-AUC of 0.74 and a Brier Score of 0.185, indicating its predictions had, on average, 40% less squared error than would be observed using industry benchmarks. The BioBERT-based model also made trial outcome predictions that were superior to benchmark values 70% of the time overall. By integrating NLP-driven insights into drug development decision-making, this work aims to enhance strategic planning and optimize investment allocation in neuroscience programs.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Melody or Machine: Detecting Synthetic Music with Dual-Stream Contrastive Learning</title>
<link>https://arxiv.org/abs/2512.00621</link>
<guid>https://arxiv.org/abs/2512.00621</guid>
<content:encoded><![CDATA[
arXiv:2512.00621v1 Announce Type: cross 
Abstract: The rapid evolution of end-to-end AI music generation poses an escalating threat to artistic authenticity and copyright, demanding detection methods that can keep pace. While foundational, existing models like SpecTTTra falter when faced with the diverse and rapidly advancing ecosystem of new generators, exhibiting significant performance drops on out-of-distribution (OOD) content. This generalization failure highlights a critical gap: the need for more challenging benchmarks and more robust detection architectures. To address this, we first introduce Melody or Machine (MoM), a new large-scale benchmark of over 130,000 songs (6,665 hours). MoM is the most diverse dataset to date, built with a mix of open and closed-source models and a curated OOD test set designed specifically to foster the development of truly generalizable detectors. Alongside this benchmark, we introduce CLAM, a novel dual-stream detection architecture. We hypothesize that subtle, machine-induced inconsistencies between vocal and instrumental elements, often imperceptible in a mixed signal, offer a powerful tell-tale sign of synthesis. CLAM is designed to test this hypothesis by employing two distinct pre-trained audio encoders (MERT and Wave2Vec2) to create parallel representations of the audio. These representations are fused by a learnable cross-aggregation module that models their inter-dependencies. The model is trained with a dual-loss objective: a standard binary cross-entropy loss for classification, complemented by a contrastive triplet loss which trains the model to distinguish between coherent and artificially mismatched stream pairings, enhancing its sensitivity to synthetic artifacts without presuming a simple feature alignment. CLAM establishes a new state-of-the-art in synthetic music forensics. It achieves an F1 score of 0.925 on our challenging MoM benchmark.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probing the "Psyche'' of Large Reasoning Models: Understanding Through a Human Lens</title>
<link>https://arxiv.org/abs/2512.00729</link>
<guid>https://arxiv.org/abs/2512.00729</guid>
<content:encoded><![CDATA[
arXiv:2512.00729v1 Announce Type: cross 
Abstract: Large reasoning models (LRMs) have garnered significant attention from researchers owing to their exceptional capability in addressing complex tasks. Motivated by the observed human-like behaviors in their reasoning processes, this paper introduces a comprehensive taxonomy to characterize atomic reasoning steps and probe the ``psyche'' of LRM intelligence. Specifically, it comprises five groups and seventeen categories derived from human mental processes, thereby grounding the understanding of LRMs in an interdisciplinary perspective. The taxonomy is then applied for an in-depth understanding of current LRMs, resulting in a distinct labeled dataset that comprises 277,534 atomic reasoning steps. Using this resource, we analyze contemporary LRMs and distill several actionable takeaways for improving training and post-training of reasoning models. Notably, our analysis reveals that prevailing post-answer ``double-checks'' (self-monitoring evaluations) are largely superficial and rarely yield substantive revisions. Thus, incentivizing comprehensive multi-step reflection, rather than simple self-monitoring, may offer a more effective path forward. To complement the taxonomy, an automatic annotation framework, named CAPO, is proposed to leverage large language models (LLMs) for generating the taxonomy-based annotations. Experimental results demonstrate that CAPO achieves higher consistency with human experts compared to baselines, facilitating a scalable and comprehensive analysis of LRMs from a human cognitive perspective. Together, the taxonomy, CAPO, and the derived insights provide a principled, scalable path toward understanding and advancing LRM reasoning.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text Mining Analysis of Symptom Patterns in Medical Chatbot Conversations</title>
<link>https://arxiv.org/abs/2512.00768</link>
<guid>https://arxiv.org/abs/2512.00768</guid>
<content:encoded><![CDATA[
arXiv:2512.00768v1 Announce Type: cross 
Abstract: The fast growth of digital health systems has led to a need to better comprehend how they interpret and represent patient-reported symptoms. Chatbots have been used in healthcare to provide clinical support and enhance the user experience, making it possible to provide meaningful clinical patterns from text-based data through chatbots. The proposed research utilises several different natural language processing methods to study the occurrences of symptom descriptions in medicine as well as analyse the patterns that emerge through these conversations within medical bots. Through the use of the Medical Conversations to Disease Dataset which contains 960 multi-turn dialogues divided into 24 Clinical Conditions, a standardised representation of conversations between patient and bot is created for further analysis by computational means. The multi-method approach uses a variety of tools, including Latent Dirichlet Allocation (LDA) to identify latent symptom themes, K-Means to group symptom descriptions by similarity, Transformer-based Named Entity Recognition (NER) to extract medical concepts, and the Apriori algorithm to discover frequent symptom pairs. Findings from the analysis indicate a coherent structure of clinically relevant topics, moderate levels of clustering cohesiveness and several high confidence rates on the relationships between symptoms like fever headache and rash itchiness. The results support the notion that conversational medical data can be a valuable diagnostic signal for early symptom interpretation, assist in strengthening decision support and improve how users interact with tele-health technology. By demonstrating a method for converting unstructured free-flowing dialogue into actionable knowledge regarding symptoms this work provides an extensible framework to further enhance future performance, dependability and clinical utility of selecting medical chatbots.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One Swallow Does Not Make a Summer: Understanding Semantic Structures in Embedding Spaces</title>
<link>https://arxiv.org/abs/2512.00852</link>
<guid>https://arxiv.org/abs/2512.00852</guid>
<content:encoded><![CDATA[
arXiv:2512.00852v1 Announce Type: cross 
Abstract: Embedding spaces are fundamental to modern AI, translating raw data into high-dimensional vectors that encode rich semantic relationships. Yet, their internal structures remain opaque, with existing approaches often sacrificing semantic coherence for structural regularity or incurring high computational overhead to improve interpretability. To address these challenges, we introduce the Semantic Field Subspace (SFS), a geometry-preserving, context-aware representation that captures local semantic neighborhoods within the embedding space. We also propose SAFARI (SemAntic Field subspAce deteRmInation), an unsupervised, modality-agnostic algorithm that uncovers hierarchical semantic structures using a novel metric called Semantic Shift, which quantifies how semantics evolve as SFSes evolve. To ensure scalability, we develop an efficient approximation of Semantic Shift that replaces costly SVD computations, achieving a 15~30x speedup with average errors below 0.01. Extensive evaluations across six real-world text and image datasets show that SFSes outperform standard classifiers not only in classification but also in nuanced tasks such as political bias detection, while SAFARI consistently reveals interpretable and generalizable semantic hierarchies. This work presents a unified framework for structuring, analyzing, and scaling semantic understanding in embedding spaces.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Active Synthetic Data Generation for Finetuning Language Models</title>
<link>https://arxiv.org/abs/2512.00884</link>
<guid>https://arxiv.org/abs/2512.00884</guid>
<content:encoded><![CDATA[
arXiv:2512.00884v1 Announce Type: cross 
Abstract: A common and effective means for improving language model capabilities involves finetuning a ``student'' language model's parameters on generations from a more proficient ``teacher'' model. Termed ``synthetic data'', these generations are often produced before any student finetuning, but some work has considered generating new synthetic samples as training progresses. This paper studies and advocates for the latter case, where data are generated in an iterative, closed-loop fashion that is guided by the current state of the student model. For a fixed budget of generated samples, or a budget in terms of compute spent querying a teacher, we show that this curation of finetuning data affords improved student performance over static generation. Further, while there have been several LLM-specific methods proposed that operate in this regime, we find that simple, inexpensive selection criteria from the active learning literature tend to be most performant. We validate these claims across four mathematical and logical reasoning datasets using four different small language models.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Legal Reasoning Traces with Legal Issue Tree Rubrics</title>
<link>https://arxiv.org/abs/2512.01020</link>
<guid>https://arxiv.org/abs/2512.01020</guid>
<content:encoded><![CDATA[
arXiv:2512.01020v1 Announce Type: cross 
Abstract: Evaluating the quality of LLM-generated reasoning traces in expert domains (e.g., law) is essential for ensuring credibility and explainability, yet remains challenging due to the inherent complexity of such reasoning tasks. We introduce LEGIT (LEGal Issue Trees), a novel large-scale (24K instances) expert-level legal reasoning dataset with an emphasis on reasoning trace evaluation. We convert court judgments into hierarchical trees of opposing parties' arguments and the court's conclusions, which serve as rubrics for evaluating the issue coverage and correctness of the reasoning traces. We verify the reliability of these rubrics via human expert annotations and comparison with coarse, less informative rubrics. Using the LEGIT dataset, we show that (1) LLMs' legal reasoning ability is seriously affected by both legal issue coverage and correctness, and that (2) retrieval-augmented generation (RAG) and RL with rubrics bring complementary benefits for legal reasoning abilities, where RAG improves overall reasoning capability, whereas RL improves correctness albeit with reduced coverage.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Associative Syntax and Maximal Repetitions reveal context-dependent complexity in fruit bat communication</title>
<link>https://arxiv.org/abs/2512.01033</link>
<guid>https://arxiv.org/abs/2512.01033</guid>
<content:encoded><![CDATA[
arXiv:2512.01033v1 Announce Type: cross 
Abstract: This study presents an unsupervised method to infer discreteness, syntax and temporal structures of fruit-bats vocalizations, as a case study of graded vocal systems, and evaluates the complexity of communication patterns in relation with behavioral context. The method improved the baseline for unsupervised labeling of vocal units (i.e. syllables) through manifold learning, by investigating how dimen- sionality reduction on mel-spectrograms affects labeling, and comparing it with unsupervised labels based on acoustic similarity. We then encoded vocalizations as syllabic sequences to analyze the type of syntax, and extracted the Maximal Repetitions (MRs) to evaluate syntactical structures. We found evidence for: i) associative syntax, rather than combinatorial (context classification is unaffected by permutation of sequences, F 1 > 0.9); ii) context-dependent use of syllables (Wilcoxon rank-sum tests, p-value < 0.05); iii) heavy-tail distribution of MRs (truncated power-law, exponent {\alpha} < 2), indicative of mechanism encoding com- binatorial complexity. Analysis of MRs and syllabic transition networks revealed that mother-pupil interactions were characterized by repetitions, while commu- nication in conflict-contexts exhibited higher complexity (longer MRs and more interconnected vocal sequences) than non-agonistic contexts. We propose that communicative complexity is higher in scenarios of disagreement, reflecting lower compressibility of information.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Testing the Machine Consciousness Hypothesis</title>
<link>https://arxiv.org/abs/2512.01081</link>
<guid>https://arxiv.org/abs/2512.01081</guid>
<content:encoded><![CDATA[
arXiv:2512.01081v1 Announce Type: cross 
Abstract: The Machine Consciousness Hypothesis states that consciousness is a substrate-free functional property of computational systems capable of second-order perception. I propose a research program to investigate this idea in silico by studying how collective self-models (coherent, self-referential representations) emerge from distributed learning systems embedded within universal self-organizing environments. The theory outlined here starts from the supposition that consciousness is an emergent property of collective intelligence systems undergoing synchronization of prediction through communication. It is not an epiphenomenon of individual modeling but a property of the language that a system evolves to internally describe itself. For a model of base reality, I begin with a minimal but general computational world: a cellular automaton, which exhibits both computational irreducibility and local reducibility. On top of this computational substrate, I introduce a network of local, predictive, representational (neural) models capable of communication and adaptation. I use this layered model to study how collective intelligence gives rise to self-representation as a direct consequence of inter-agent alignment. I suggest that consciousness does not emerge from modeling per se, but from communication. It arises from the noisy, lossy exchange of predictive messages between groups of local observers describing persistent patterns in the underlying computational substrate (base reality). It is through this representational dialogue that a shared model arises, aligning many partial views of the world. The broader goal is to develop empirically testable theories of machine consciousness, by studying how internal self-models may form in distributed systems without centralized control.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalized Medical Phrase Grounding</title>
<link>https://arxiv.org/abs/2512.01085</link>
<guid>https://arxiv.org/abs/2512.01085</guid>
<content:encoded><![CDATA[
arXiv:2512.01085v1 Announce Type: cross 
Abstract: Medical phrase grounding (MPG) maps textual descriptions of radiological findings to corresponding image regions. These grounded reports are easier to interpret, especially for non-experts. Existing MPG systems mostly follow the referring expression comprehension (REC) paradigm and return exactly one bounding box per phrase. Real reports often violate this assumption. They contain multi-region findings, non-diagnostic text, and non-groundable phrases, such as negations or descriptions of normal anatomy. Motivated by this, we reformulate the task as generalised medical phrase grounding (GMPG), where each sentence is mapped to zero, one, or multiple scored regions. To realise this formulation, we introduce the first GMPG model: MedGrounder. We adopted a two-stage training regime: pre-training on report sentence--anatomy box alignment datasets and fine-tuning on report sentence--human annotated box datasets. Experiments on PadChest-GR and MS-CXR show that MedGrounder achieves strong zero-shot transfer and outperforms REC-style and grounded report generation baselines on multi-region and non-groundable phrases, while using far fewer human box annotations. Finally, we show that MedGrounder can be composed with existing report generators to produce grounded reports without retraining the generator.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mode-Conditioning Unlocks Superior Test-Time Scaling</title>
<link>https://arxiv.org/abs/2512.01127</link>
<guid>https://arxiv.org/abs/2512.01127</guid>
<content:encoded><![CDATA[
arXiv:2512.01127v1 Announce Type: cross 
Abstract: Parallel sampling promises substantial gains in test-time scaling, but its effectiveness is sharply limited by diversity collapse, where models concentrate on a few modes and repeated samples produce the same mistakes. We propose the mode-conditioning (ModC) framework, which explicitly allocates test-time compute across reasoning modes using either specialist models or mode-specific prefixes. ModC consistently improves scaling across controlled graph-search tasks and large-scale reasoning benchmarks, spanning model families and sizes from 0.5B to 7B. On OpenThoughts, fine-tuning Qwen2.5-7B with ModC achieves a 4x efficiency gain over standard training while also improving the maximum attainable Pass@k. We further show that gradient clustering enables ModC without explicit mode labels, yielding up to 10% gains on datasets such as NuminaMath. Finally, we show that ModC improves reinforcement learning (RL) and can further boost diversity-inducing RL methods. These results demonstrate that standard training underutilizes the diversity in data, and that ModC provides a simple, effective remedy for unlocking the full benefits of diversity in test-time scaling.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pay Attention Later: From Vector Space Diffusion to Linearithmic Spectral Phase-Locking</title>
<link>https://arxiv.org/abs/2512.01208</link>
<guid>https://arxiv.org/abs/2512.01208</guid>
<content:encoded><![CDATA[
arXiv:2512.01208v1 Announce Type: cross 
Abstract: Standard Transformers suffer from a "Semantic Alignment Tax", a prohibitive optimization cost required to organize a chaotic initialization into a coherent geometric map via local gradient diffusion. We hypothesize that this reliance on diffusive learning creates "Catastrophic Rigidity", rendering models unable to adapt to novel concepts without destroying their pre-trained reasoning capabilities. To isolate this phenomenon, we introduce Iterative Semantic Map Refinement (ISMR), a diagnostic protocol revealing that alignment is a fixed geometric barrier that scaling cannot solve; a 20-layer model overcomes this barrier no faster than a 1-layer model. We introduce the Phase-Resonant Intelligent Spectral Model (PRISM). PRISM encodes semantic identity as resonant frequencies in the complex domain (C^d) and replaces quadratic self-attention with linearithmic O(N log N) Gated Harmonic Convolutions. We validate PRISM on the WMT14 translation task. While the Standard Transformer maintains a slight edge in general competence on static benchmarks (23.88 vs 21.40 BLEU), it fails the "Plasticity-Stability" stress test completely. When injected with novel concepts, the Transformer suffers Catastrophic Forgetting, degrading by -10.55 BLEU points while achieving only 60% acquisition. In contrast, PRISM demonstrates Lossless Plasticity, achieving 96% 5-shot acquisition with negligible degradation (-0.84 BLEU). These results suggest that harmonic representations effectively decouple memory from reasoning, offering a structural solution to the plasticity-stability dilemma in real-time knowledge adaptation.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Adversarial Gumbel MCTS for Abstract Visual Composition Generation</title>
<link>https://arxiv.org/abs/2512.01242</link>
<guid>https://arxiv.org/abs/2512.01242</guid>
<content:encoded><![CDATA[
arXiv:2512.01242v1 Announce Type: cross 
Abstract: We study abstract visual composition, in which identity is primarily determined by the spatial configuration and relations among a small set of geometric primitives (e.g., parts, symmetry, topology). They are invariant primarily to texture and photorealistic detail. Composing such structures from fixed components under geometric constraints and vague goal specification (such as text) is non-trivial due to combinatorial placement choices, limited data, and discrete feasibility (overlap-free, allowable orientations), which create a sparse solution manifold ill-suited to purely statistical pixel-space generators. We propose a constraint-guided framework that combines explicit geometric reasoning with neural semantics. An AlphaGo-style search enforces feasibility, while a fine-tuned vision-language model scores semantic alignment as reward signals. Our algorithm uses a policy network as a heuristic in Monte-Carlo Tree Search and fine-tunes the network via search-generated plans. Inspired by the Generative Adversarial Network, we use the generated instances for adversarial reward refinement. Over time, the generation should approach the actual data more closely when the reward model cannot distinguish between generated instances and ground-truth. In the Tangram Assembly task, our approach yields higher validity and semantic fidelity than diffusion and auto-regressive baselines, especially as constraints tighten.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Models Cannot Reliably Detect Vulnerabilities in JavaScript: The First Systematic Benchmark and Evaluation</title>
<link>https://arxiv.org/abs/2512.01255</link>
<guid>https://arxiv.org/abs/2512.01255</guid>
<content:encoded><![CDATA[
arXiv:2512.01255v1 Announce Type: cross 
Abstract: Researchers have proposed numerous methods to detect vulnerabilities in JavaScript, especially those assisted by Large Language Models (LLMs). However, the actual capability of LLMs in JavaScript vulnerability detection remains questionable, necessitating systematic evaluation and comprehensive benchmarks. Unfortunately, existing benchmarks suffer from three critical limitations: (1) incomplete coverage, such as covering a limited subset of CWE types; (2) underestimation of LLM capabilities caused by unreasonable ground truth labeling; and (3) overestimation due to unrealistic cases such as using isolated vulnerable files rather than complete projects.
  In this paper, we introduce, for the first time, three principles for constructing a benchmark for JavaScript vulnerability detection that directly address these limitations: (1) comprehensiveness, (2) no underestimation, and (3) no overestimation. Guided by these principles, we propose FORGEJS, the first automatic benchmark generation framework for evaluating LLMs' capability in JavaScript vulnerability detection. Then, we use FORGEJS to construct ARENAJS-the first systematic benchmark for LLM-based JavaScript vulnerability detection-and further propose JUDGEJS, an automatic evaluation framework.
  We conduct the first systematic evaluation of LLMs for JavaScript vulnerability detection, leveraging JUDGEJS to assess seven popular commercial LLMs on ARENAJS. The results show that LLMs not only exhibit limited reasoning capabilities, but also suffer from severe robustness defects, indicating that reliable JavaScript vulnerability detection with LLMs remains an open challenge.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Securing Large Language Models (LLMs) from Prompt Injection Attacks</title>
<link>https://arxiv.org/abs/2512.01326</link>
<guid>https://arxiv.org/abs/2512.01326</guid>
<content:encoded><![CDATA[
arXiv:2512.01326v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly being deployed in real-world applications, but their flexibility exposes them to prompt injection attacks. These attacks leverage the model's instruction-following ability to make it perform malicious tasks. Recent work has proposed JATMO, a task-specific fine-tuning approach that trains non-instruction-tuned base models to perform a single function, thereby reducing susceptibility to adversarial instructions. In this study, we evaluate the robustness of JATMO against HOUYI, a genetic attack framework that systematically mutates and optimizes adversarial prompts. We adapt HOUYI by introducing custom fitness scoring, modified mutation logic, and a new harness for local model testing, enabling a more accurate assessment of defense effectiveness. We fine-tuned LLaMA 2-7B, Qwen1.5-4B, and Qwen1.5-0.5B models under the JATMO methodology and compared them with a fine-tuned GPT-3.5-Turbo baseline. Results show that while JATMO reduces attack success rates relative to instruction-tuned models, it does not fully prevent injections; adversaries exploiting multilingual cues or code-related disruptors still bypass defenses. We also observe a trade-off between generation quality and injection vulnerability, suggesting that better task performance often correlates with increased susceptibility. Our results highlight both the promise and limitations of fine-tuning-based defenses and point toward the need for layered, adversarially informed mitigation strategies.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EmoRAG: Evaluating RAG Robustness to Symbolic Perturbations</title>
<link>https://arxiv.org/abs/2512.01335</link>
<guid>https://arxiv.org/abs/2512.01335</guid>
<content:encoded><![CDATA[
arXiv:2512.01335v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) systems are increasingly central to robust AI, enhancing large language model (LLM) faithfulness by incorporating external knowledge. However, our study unveils a critical, overlooked vulnerability: their profound susceptibility to subtle symbolic perturbations, particularly through near-imperceptible emoticon tokens such as "(@_@)" that can catastrophically mislead retrieval, termed EmoRAG. We demonstrate that injecting a single emoticon into a query makes it nearly 100% likely to retrieve semantically unrelated texts that contain a matching emoticon. Our extensive experiment across general question-answering and code domains, using a range of state-of-the-art retrievers and generators, reveals three key findings: (I) Single-Emoticon Disaster: Minimal emoticon injections cause maximal disruptions, with a single emoticon almost 100% dominating RAG output. (II) Positional Sensitivity: Placing an emoticon at the beginning of a query can cause severe perturbation, with F1-Scores exceeding 0.92 across all datasets. (III) Parameter-Scale Vulnerability: Counterintuitively, models with larger parameters exhibit greater vulnerability to the interference. We provide an in-depth analysis to uncover the underlying mechanisms of these phenomena. Furthermore, we raise a critical concern regarding the robustness assumption of current RAG systems, envisioning a threat scenario where an adversary exploits this vulnerability to manipulate the RAG system. We evaluate standard defenses and find them insufficient against EmoRAG. To address this, we propose targeted defenses, analyzing their strengths and limitations in mitigating emoticon-based perturbations. Finally, we outline future directions for building robust RAG systems.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Necessity of Imperfection:Reversing Model Collapse via Simulating Cognitive Boundedness</title>
<link>https://arxiv.org/abs/2512.01354</link>
<guid>https://arxiv.org/abs/2512.01354</guid>
<content:encoded><![CDATA[
arXiv:2512.01354v1 Announce Type: cross 
Abstract: Although synthetic data is widely promoted as a remedy, its prevailing production paradigm -- one optimizing for statistical smoothness -- systematically removes the long-tail, cognitively grounded irregularities that characterize human text. Prolonged training on such statistically optimal but cognitively impoverished data accelerates model collapse.
  This paper proposes a paradigm shift: instead of imitating the surface properties of data, we simulate the cognitive processes that generate human text. We introduce the Prompt-driven Cognitive Computing Framework (PMCSF), whose core consists of a Cognitive State Decoder (CSD) that reverse-engineers unstructured text into structured cognitive vectors, and a Cognitive Text Encoder (CTE) that re-materializes these states into text enriched with human-typical imperfections via mathematically defined Cognitive Perturbation Operators.
  The framework is validated through a two-stage objective evaluation pipeline. First, in cognitive codec verification, CTE text yields a Jensen-Shannon divergence of 0.0614 from human text (vs. 0.4431 for standard LLM output), passes double-blind professional media review, and achieves an intraclass correlation coefficient ICC > 0.9 for cognitive profile alignment across heterogeneous models. Second, in functional gain evaluation, isomorphic stress tests in the A-share market show that strategies incorporating CTE-generated data reduce maximum drawdown by 47.4% during the 2015 crash and deliver 8.6% Defensive Alpha, exceeding transaction costs by a factor of 33.
  Our findings demonstrate that modelling human cognitive limitations -- not copying surface data -- enables synthetic data with genuine functional gain, offering a viable technical pathway toward resolving the AI data-collapse crisis.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stabilizing Reinforcement Learning with LLMs: Formulation and Practices</title>
<link>https://arxiv.org/abs/2512.01374</link>
<guid>https://arxiv.org/abs/2512.01374</guid>
<content:encoded><![CDATA[
arXiv:2512.01374v1 Announce Type: cross 
Abstract: This paper proposes a novel formulation for reinforcement learning (RL) with large language models, explaining why and under what conditions the true sequence-level reward can be optimized via a surrogate token-level objective in policy gradient methods such as REINFORCE. Specifically, through a first-order approximation, we show that this surrogate becomes increasingly valid only when both the training-inference discrepancy and policy staleness are minimized. This insight provides a principled explanation for the crucial role of several widely adopted techniques in stabilizing RL training, including importance sampling correction, clipping, and particularly Routing Replay for Mixture-of-Experts (MoE) models. Through extensive experiments with a 30B MoE model totaling hundreds of thousands of GPU hours, we show that for on-policy training, the basic policy gradient algorithm with importance sampling correction achieves the highest training stability. When off-policy updates are introduced to accelerate convergence, combining clipping and Routing Replay becomes essential to mitigate the instability caused by policy staleness. Notably, once training is stabilized, prolonged optimization consistently yields comparable final performance regardless of cold-start initialization. We hope that the shared insights and the developed recipes for stable RL training will facilitate future research.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BackportBench: A Multilingual Benchmark for Automated Backporting of Patches</title>
<link>https://arxiv.org/abs/2512.01396</link>
<guid>https://arxiv.org/abs/2512.01396</guid>
<content:encoded><![CDATA[
arXiv:2512.01396v1 Announce Type: cross 
Abstract: Many modern software projects evolve rapidly to incorporate new features and security patches. It is important for users to update their dependencies to safer versions, but many still use older, vulnerable package versions because upgrading can be difficult and may break their existing codebase. Software developers can mitigate this problem by backporting security patches to older releases. However, manually backporting is time-consuming and error-prone. The effectiveness of existing automated backporting techniques on general software remains unclear since they typically target only code-hunk or function-level patch porting scenarios and are evaluated with imperfect metrics.
  To facilitate the development and evaluation of automated backporting techniques, we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem. BackportBench is a multilingual benchmark that contains 202 patch backporting problems from PyPI, Maven, and npm, each with executable Docker environments and relevant test cases. We evaluated existing patch porting methods and LLM-based techniques that have the potential to adapt to this task using BackportBench. The results show that the agentic method has outperformed traditional patch porting methods, especially on cases that require logical and structural changes. However, the performance varies across different programming languages. Based on the findings, we draw several implications for researchers and software practitioners in future work on automated backporting.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ZIP-RC: Zero-overhead Inference-time Prediction of Reward and Cost for Adaptive and Interpretable Generation</title>
<link>https://arxiv.org/abs/2512.01457</link>
<guid>https://arxiv.org/abs/2512.01457</guid>
<content:encoded><![CDATA[
arXiv:2512.01457v1 Announce Type: cross 
Abstract: Large language models excel at reasoning but lack key aspects of introspection, including anticipating their own success and the computation required to achieve it. Humans use real-time introspection to decide how much effort to invest, when to make multiple attempts, when to stop, and when to signal success or failure. Without this, LLMs struggle to make intelligent meta-cognition decisions. Test-time scaling methods like Best-of-N drive up cost and latency by using a fixed budget of samples regardless of the marginal benefit of each one at any point in generation, and the absence of confidence signals can mislead people, prevent appropriate escalation to better tools, and undermine trustworthiness. Learned verifiers or reward models can provide confidence estimates, but do not enable adaptive inference and add substantial cost by requiring extra models or forward passes. We present ZIP-RC, an adaptive inference method that equips models with zero-overhead inference-time predictions of reward and cost. At every token, ZIP-RC reuses reserved or unused logits in the same forward pass as next-token prediction to output a joint distribution over final reward and remaining length -- no extra models, architecture change, or inference overhead. This full joint distribution is used to compute a sampling utility which is the linear combination of the expected maximum reward, total compute, and latency of set of samples if generated to completion. During inference, we maximize this utility with meta-actions that determine which prefix of tokens to continue or initiate sampling from. On mixed-difficulty mathematical benchmarks, ZIP-RC improves accuracy by up to 12% over majority voting at equal or lower average cost, and traces smooth Pareto frontiers between quality, compute, and latency. By providing real-time reward-cost introspection, ZIP-RC enables adaptive, efficient reasoning.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LPCD: Unified Framework from Layer-Wise to Submodule Quantization</title>
<link>https://arxiv.org/abs/2512.01546</link>
<guid>https://arxiv.org/abs/2512.01546</guid>
<content:encoded><![CDATA[
arXiv:2512.01546v1 Announce Type: cross 
Abstract: Post-training quantization (PTQ) aims to preserve model-level behavior; however, most methods focus on individual linear layers. Even recent extensions, such as QEP and LoaQ, which mitigate error propagation or target specific submodules, still rely on layer-wise formulations and fail to capture the behavior of larger submodules. We introduce Layer-Projected Coordinate Descent (LPCD), a unified framework that extends PTQ beyond layers by optimizing relaxed objectives across arbitrary submodules and projecting the solutions with layer-wise quantizers. LPCD generalizes existing methods and provides a principled approach to quantizing complex submodules while maintaining the efficiency and compatibility of layer-wise PTQ pipelines. Across diverse LLM architectures and bit-widths, LPCD-based submodule quantization consistently enhances both layer-wise PTQ methods and existing submodule approaches.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LEC: Linear Expectation Constraints for False-Discovery Control in Selective Prediction and Routing Systems</title>
<link>https://arxiv.org/abs/2512.01556</link>
<guid>https://arxiv.org/abs/2512.01556</guid>
<content:encoded><![CDATA[
arXiv:2512.01556v1 Announce Type: cross 
Abstract: Large language models (LLMs) often generate unreliable answers, while heuristic uncertainty methods fail to fully distinguish correct from incorrect predictions, causing users to accept erroneous answers without statistical guarantees. We address this issue through the lens of false discovery rate (FDR) control, ensuring that among all accepted predictions, the proportion of errors does not exceed a target risk level. To achieve this in a principled way, we propose LEC, which reinterprets selective prediction as a constrained decision problem by enforcing a Linear Expectation Constraint over selection and error indicators. Then, we establish a finite-sample sufficient condition, which relies only on a held-out set of exchangeable calibration samples, to compute an FDR-constrained, coverage-maximizing threshold. Furthermore, we extend LEC to a two-model routing mechanism: given a prompt, if the current model's uncertainty exceeds its calibrated threshold, we delegate it to a stronger model, while maintaining a unified FDR guarantee. Evaluations on closed-ended and open-ended question-answering (QA) datasets show that LEC achieves tighter FDR control and substantially improves sample retention over prior methods. Moreover, the two-model routing mechanism achieves lower risk levels while accepting more correct samples than each individual model.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HalluGraph: Auditable Hallucination Detection for Legal RAG Systems via Knowledge Graph Alignment</title>
<link>https://arxiv.org/abs/2512.01659</link>
<guid>https://arxiv.org/abs/2512.01659</guid>
<content:encoded><![CDATA[
arXiv:2512.01659v1 Announce Type: cross 
Abstract: Legal AI systems powered by retrieval-augmented generation (RAG) face a critical accountability challenge: when an AI assistant cites case law, statutes, or contractual clauses, practitioners need verifiable guarantees that generated text faithfully represents source documents. Existing hallucination detectors rely on semantic similarity metrics that tolerate entity substitutions, a dangerous failure mode when confusing parties, dates, or legal provisions can have material consequences. We introduce HalluGraph, a graph-theoretic framework that quantifies hallucinations through structural alignment between knowledge graphs extracted from context, query, and response. Our approach produces bounded, interpretable metrics decomposed into \textit{Entity Grounding} (EG), measuring whether entities in the response appear in source documents, and \textit{Relation Preservation} (RP), verifying that asserted relationships are supported by context. On structured control documents, HalluGraph achieves near-perfect discrimination ($>$400 words, $>$20 entities), HalluGraph achieves $AUC = 0.979$, while maintaining robust performance ($AUC \approx 0.89$) on challenging generative legal task, consistently outperforming semantic similarity baselines. The framework provides the transparency and traceability required for high-stakes legal applications, enabling full audit trails from generated assertions back to source passages.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos</title>
<link>https://arxiv.org/abs/2512.01707</link>
<guid>https://arxiv.org/abs/2512.01707</guid>
<content:encoded><![CDATA[
arXiv:2512.01707v1 Announce Type: cross 
Abstract: Streaming video understanding requires models not only to process temporally incoming frames, but also to anticipate user intention for realistic applications like AR glasses. While prior streaming benchmarks evaluate temporal reasoning, none measure whether MLLMs can interpret or leverage human gaze signals within a streaming setting. To fill this gap, we introduce StreamGaze, the first benchmark designed to evaluate how effectively MLLMs use gaze for temporal and proactive reasoning in streaming videos. StreamGaze introduces gaze-guided past, present, and proactive tasks that comprehensively evaluate streaming video understanding. These tasks assess whether models can use real-time gaze to follow shifting attention and infer user intentions from only past and currently observed frames. To build StreamGaze, we develop a gaze-video QA generation pipeline that aligns egocentric videos with raw gaze trajectories via fixation extraction, region-specific visual prompting, and scanpath construction. This pipeline produces spatio-temporally grounded QA pairs that closely reflect human perceptual dynamics. Across all StreamGaze tasks, we observe substantial performance gaps between state-of-the-art MLLMs and human performance, revealing fundamental limitations in gaze-based temporal reasoning, intention modeling, and proactive prediction. We further provide detailed analyses of gaze-prompting strategies, reasoning behaviors, and task-specific failure modes, offering deeper insight into why current MLLMs struggle and what capabilities future models must develop. All data and code will be publicly released to support continued research in gaze-guided streaming video understanding.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons</title>
<link>https://arxiv.org/abs/2512.01797</link>
<guid>https://arxiv.org/abs/2512.01797</guid>
<content:encoded><![CDATA[
arXiv:2512.01797v1 Announce Type: cross 
Abstract: Large language models (LLMs) frequently generate hallucinations -- plausible but factually incorrect outputs -- undermining their reliability. While prior work has examined hallucinations from macroscopic perspectives such as training data and objectives, the underlying neuron-level mechanisms remain largely unexplored. In this paper, we conduct a systematic investigation into hallucination-associated neurons (H-Neurons) in LLMs from three perspectives: identification, behavioral impact, and origins. Regarding their identification, we demonstrate that a remarkably sparse subset of neurons (less than $0.1\%$ of total neurons) can reliably predict hallucination occurrences, with strong generalization across diverse scenarios. In terms of behavioral impact, controlled interventions reveal that these neurons are causally linked to over-compliance behaviors. Concerning their origins, we trace these neurons back to the pre-trained base models and find that these neurons remain predictive for hallucination detection, indicating they emerge during pre-training. Our findings bridge macroscopic behavioral patterns with microscopic neural mechanisms, offering insights for developing more reliable LLMs.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agentic Policy Optimization via Instruction-Policy Co-Evolution</title>
<link>https://arxiv.org/abs/2512.01945</link>
<guid>https://arxiv.org/abs/2512.01945</guid>
<content:encoded><![CDATA[
arXiv:2512.01945v1 Announce Type: cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the reasoning capability of large language models (LLMs), enabling autonomous agents that can conduct effective multi-turn and tool-integrated reasoning. While instructions serve as the primary protocol for defining agents, RLVR typically relies on static and manually designed instructions. However, those instructions may be suboptimal for the base model, and the optimal instruction may change as the agent's policy improves and explores the interaction with the environment. To bridge the gap, we introduce INSPO, a novel Instruction-Policy co-evolution framework that integrates instruction optimization as a dynamic component of the reinforcement learning (RL) loop. INSPO maintains a dynamic population of instruction candidates that are sampled with questions, where reward signals in RL loops are automatically attributed to each instruction, and low performers are periodically pruned. New instructions are generated and verified through an on-policy reflection mechanism, where an LLM-based optimizer analyzes past experience from a replay buffer and evolves more effective strategies given the current policy. We conduct extensive experiments on multi-turn retrieval and reasoning tasks, demonstrating that INSPO substantially outperforms strong baselines relying on static instructions. INSPO discovers innovative instructions that guide the agent toward more strategic reasoning paths, achieving substantial performance gains with only a marginal increase in computational overhead.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Atomic to Composite: Reinforcement Learning Enables Generalization in Complementary Reasoning</title>
<link>https://arxiv.org/abs/2512.01970</link>
<guid>https://arxiv.org/abs/2512.01970</guid>
<content:encoded><![CDATA[
arXiv:2512.01970v1 Announce Type: cross 
Abstract: The mechanism by which RL contributes to reasoning capabilities-whether it incentivizes the synthesis of new skills or merely amplifies existing behaviors-remains a subject of intense debate. In this work, we investigate this question through the lens of Complementary Reasoning, a complex task that requires integrating internal parametric knowledge with external contextual information. Using a controlled synthetic dataset of human biographies, we strictly decouple this ability into two atomic skills: Parametric Reasoning (relying on internal knowledge) and Contextual Reasoning (depending on external information). To rigorously assess capability boundaries, we evaluate generalization across three distinct levels of difficulty: I.I.D., Composition, and Zero-shot settings. We find that while SFT is sufficient for in-distribution performance, it struggles with O.O.D. generalization, particularly in Zero-shot settings where relational combinations are novel. Crucially, we identify the SFT Generalization Paradox: Models supervised solely on the composite task achieve near-perfect in-distribution accuracy but collapse on out-of-distribution generalization, indicating their reliance on rote memorization of path shortcuts. In contrast, we find that RL acts as a reasoning synthesizer rather than a probability amplifier. However, we uncover a strict atomic prerequisite: RL can only synthesize these complex strategies if the base model has first mastered the independent atomic skills (Parametric and Contextual) via SFT. These findings challenge the view of RL as a mere amplifier, suggesting that given sufficient atomic foundations, RL can actively synthesize complex reasoning strategies from learned primitives without explicit supervision on such complex strategies. This indicates that decoupled atomic training followed by RL offers a scalable path to generalization for complex reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chain-of-Ground: Improving GUI Grounding via Iterative Reasoning and Reference Feedback</title>
<link>https://arxiv.org/abs/2512.01979</link>
<guid>https://arxiv.org/abs/2512.01979</guid>
<content:encoded><![CDATA[
arXiv:2512.01979v1 Announce Type: cross 
Abstract: GUI grounding aims to align natural language instructions with precise regions in complex user interfaces. Advanced multimodal large language models show strong ability in visual GUI grounding but still struggle with small or visually similar targets and ambiguity in real world layouts. These limitations arise from limited grounding capacity and from underuse of existing reasoning potential. We present Chain of Ground CoG a training free multi step grounding framework that uses multimodal large language models for iterative visual reasoning and refinement. Instead of direct prediction the model progressively reflects and adjusts its hypotheses leading to more accurate and interpretable localization. Our approach achieves 68.4 accuracy on the ScreenSpot Pro benchmark an improvement of 4.8 points. To measure real world generalization we introduce TPanel UI a dataset of 420 labeled industrial control panels with visual distortions such as blur and masking. On TPanel UI Chain of Ground improves over the strong baseline Qwen3 VL 235B by 6.9 points showing the effectiveness of multi step training free grounding across real world and digital interfaces. These results highlight a direction for unlocking grounding potential through structured iterative refinement instead of additional training.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM CHESS: Benchmarking Reasoning and Instruction-Following in LLMs through Chess</title>
<link>https://arxiv.org/abs/2512.01992</link>
<guid>https://arxiv.org/abs/2512.01992</guid>
<content:encoded><![CDATA[
arXiv:2512.01992v1 Announce Type: cross 
Abstract: We introduce LLM CHESS, an evaluation framework designed to probe the generalization of reasoning and instruction-following abilities in large language models (LLMs) through extended agentic interaction in the domain of chess. We rank over 50 open and closed source models by playing against a random opponent using a range of behavioral metrics, including win and loss rates, move quality, move legality, hallucinated actions, and game duration. For a subset of top reasoning models, we derive an Elo estimate by playing against a chess engine with variably configured skill, which allows for comparisons between models in an easily understandable way. Despite the simplicity of the instruction-following task and the weakness of the opponent, many state-of-the-art models struggle to complete games or achieve consistent wins. Similar to other benchmarks on complex reasoning tasks, our experiments reveal a clear separation between reasoning and non-reasoning models. However, unlike existing static benchmarks, the stochastic and dynamic nature of LLM CHESS uniquely reduces overfitting and memorization while preventing benchmark saturation, proving difficult even for top reasoning models. To support future work on evaluating reasoning and instruction-following in LLMs, we release our experimental framework, a public leaderboard, and a dataset of associated games.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AlignSAE: Concept-Aligned Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2512.02004</link>
<guid>https://arxiv.org/abs/2512.02004</guid>
<content:encoded><![CDATA[
arXiv:2512.02004v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) encode factual knowledge within hidden parametric spaces that are difficult to inspect or control. While Sparse Autoencoders (SAEs) can decompose hidden activations into more fine-grained, interpretable features, they often struggle to reliably align these features with human-defined concepts, resulting in entangled and distributed feature representations. To address this, we introduce AlignSAE, a method that aligns SAE features with a defined ontology through a "pre-train, then post-train" curriculum. After an initial unsupervised training phase, we apply supervised post-training to bind specific concepts to dedicated latent slots while preserving the remaining capacity for general reconstruction. This separation creates an interpretable interface where specific relations can be inspected and controlled without interference from unrelated features. Empirical results demonstrate that AlignSAE enables precise causal interventions, such as reliable "concept swaps", by targeting single, semantically aligned slots.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeID-GPT: Zero-shot Medical Text De-Identification by GPT-4</title>
<link>https://arxiv.org/abs/2303.11032</link>
<guid>https://arxiv.org/abs/2303.11032</guid>
<content:encoded><![CDATA[
arXiv:2303.11032v3 Announce Type: replace 
Abstract: The digitization of healthcare has facilitated the sharing and re-using of medical data but has also raised concerns about confidentiality and privacy. HIPAA (Health Insurance Portability and Accountability Act) mandates removing re-identifying information before the dissemination of medical records. Thus, effective and efficient solutions for de-identifying medical data, especially those in free-text forms, are highly needed. While various computer-assisted de-identification methods, including both rule-based and learning-based, have been developed and used in prior practice, such solutions still lack generalizability or need to be fine-tuned according to different scenarios, significantly imposing restrictions in wider use. The advancement of large language models (LLM), such as ChatGPT and GPT-4, have shown great potential in processing text data in the medical domain with zero-shot in-context learning, especially in the task of privacy protection, as these models can identify confidential information by their powerful named entity recognition (NER) capability. In this work, we developed a novel GPT4-enabled de-identification framework (``DeID-GPT") to automatically identify and remove the identifying information. Compared to existing commonly used medical text data de-identification methods, our developed DeID-GPT showed the highest accuracy and remarkable reliability in masking private information from the unstructured medical text while preserving the original structure and meaning of the text. This study is one of the earliest to utilize ChatGPT and GPT-4 for medical text data processing and de-identification, which provides insights for further research and solution development on the use of LLMs such as ChatGPT/GPT-4 in healthcare. Codes and benchmarking data information are available at https://github.com/yhydhx/ChatGPT-API.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Template-assisted Contrastive Learning of Task-oriented Dialogue Sentence Embeddings</title>
<link>https://arxiv.org/abs/2305.14299</link>
<guid>https://arxiv.org/abs/2305.14299</guid>
<content:encoded><![CDATA[
arXiv:2305.14299v2 Announce Type: replace 
Abstract: Learning high quality sentence embeddings from dialogues has drawn increasing attentions as it is essential to solve a variety of dialogue-oriented tasks with low annotation cost. Annotating and gathering utterance relationships in conversations are difficult, while token-level annotations, \eg, entities, slots and templates, are much easier to obtain. Other sentence embedding methods are usually sentence-level self-supervised frameworks and cannot utilize token-level extra knowledge. We introduce Template-aware Dialogue Sentence Embedding (TaDSE), a novel augmentation method that utilizes template information to learn utterance embeddings via self-supervised contrastive learning framework. We further enhance the effect with a synthetically augmented dataset that diversifies utterance-template association, in which slot-filling is a preliminary step. We evaluate TaDSE performance on five downstream benchmark dialogue datasets. The experiment results show that TaDSE achieves significant improvements over previous SOTA methods for dialogue. We further introduce a novel analytic instrument of semantic compression test, for which we discover a correlation with uniformity and alignment. Our code will be released upon acceptance.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multilingual DistilWhisper: Efficient Distillation of Multi-task Speech Models via Language-Specific Experts</title>
<link>https://arxiv.org/abs/2311.01070</link>
<guid>https://arxiv.org/abs/2311.01070</guid>
<content:encoded><![CDATA[
arXiv:2311.01070v4 Announce Type: replace 
Abstract: Whisper is a multitask and multilingual speech model covering 99 languages. It yields commendable automatic speech recognition (ASR) results in a subset of its covered languages, but the model still underperforms on a non-negligible number of under-represented languages, a problem exacerbated in smaller model versions. In this work, we propose DistilWhisper, an approach able to bridge the performance gap in ASR for these languages while retaining the advantages of multitask and multilingual capabilities. Our approach involves two key strategies: lightweight modular ASR fine-tuning of whisper-small using language-specific experts, and knowledge distillation from whisper-large-v2. This dual approach allows us to effectively boost ASR performance while keeping the robustness inherited from the multitask and multilingual pre-training. Results demonstrate that our approach is more effective than standard fine-tuning or LoRA adapters, boosting performance in the targeted languages for both in- and out-of-domain test sets, while introducing only a negligible parameter overhead at inference.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extending Multilingual Machine Translation through Imitation Learning</title>
<link>https://arxiv.org/abs/2311.08538</link>
<guid>https://arxiv.org/abs/2311.08538</guid>
<content:encoded><![CDATA[
arXiv:2311.08538v2 Announce Type: replace 
Abstract: Despite the growing variety of languages supported by existing multilingual neural machine translation (MNMT) models, most of the world's languages are still being left behind. We aim to extend large-scale MNMT models to incorporate a new language, enabling translations between this new language and all previously supported languages, even in the challenging scenario where only a parallel corpus between the new language and English is available. Previous methods, such as continued training on parallel data including the new language, often suffer from catastrophic forgetting, which degrades performance on other languages. We propose a novel approach Imit-MNMT which treats this task as an imitation learning problem, a technique widely used in computer vision but less explored in natural language processing. Specifically, we leverage an expert model to generate pseudo-parallel corpora between the new language and the existing languages. We then introduce a data distribution imitation strategy using language-specific weighting, alongside a translation behavior imitation mechanism. Extensive experiments show that our approach significantly improves translation performance between the new and existing languages while mitigating catastrophic forgetting.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reliable Reasoning Beyond Natural Language</title>
<link>https://arxiv.org/abs/2407.11373</link>
<guid>https://arxiv.org/abs/2407.11373</guid>
<content:encoded><![CDATA[
arXiv:2407.11373v3 Announce Type: replace 
Abstract: Despite their linguistic competence, Large Language Models (LLMs) often struggle to reason reliably and flexibly. To identify these shortcomings, we introduce the Non-Linear Reasoning (NLR) dataset, a collection of 55 unique, hand-designed problems that target reasoning bottlenecks arising from the sequential prediction paradigm of LLMs and the inherently linear nature of natural language. NLR tasks require iterative updates, backtracking, and reasoning across multiple parallel chains of thought but only basic arithmetic to solve. To address these limitations, we propose a neurosymbolic reasoning approach that integrates Prolog, a symbolic reasoning engine, into the inference pipeline of LLMs. This division of labor shifts the LLM's task from iterative computations to inferring all information, explicit or implied through common sense, and encoding it as logical code. Our method yields large and robust performance gains across the GSM8k and BIG-bench Navigate benchmarks and achieves near-perfect accuracy on NLR problems, maintaining robustness even as variable interdependence - the number of other variables on which the value of a single variable depends - increases.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HEALTH-PARIKSHA: Assessing RAG Models for Health Chatbots in Real-World Multilingual Settings</title>
<link>https://arxiv.org/abs/2410.13671</link>
<guid>https://arxiv.org/abs/2410.13671</guid>
<content:encoded><![CDATA[
arXiv:2410.13671v3 Announce Type: replace 
Abstract: Assessing the capabilities and limitations of large language models (LLMs) has garnered significant interest, yet the evaluation of multiple models in real-world scenarios remains rare. Multilingual evaluation often relies on translated benchmarks, which typically do not capture linguistic and cultural nuances present in the source language. This study provides an extensive assessment of 24 LLMs on real world data collected from Indian patients interacting with a medical chatbot in Indian English and 4 other Indic languages. We employ a uniform Retrieval Augmented Generation framework to generate responses, which are evaluated using both automated techniques and human evaluators on four specific metrics relevant to our application. We find that models vary significantly in their performance and that instruction tuned Indic models do not always perform well on Indic language queries. Further, we empirically show that factual correctness is generally lower for responses to Indic queries compared to English queries. Finally, our qualitative work shows that code-mixed and culturally relevant queries in our dataset pose challenges to evaluated models.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeKo: Cross-Modality Post-Recognition Error Correction with Tasks-Guided Mixture-of-Experts Language Model</title>
<link>https://arxiv.org/abs/2411.05945</link>
<guid>https://arxiv.org/abs/2411.05945</guid>
<content:encoded><![CDATA[
arXiv:2411.05945v2 Announce Type: replace 
Abstract: Construction of a general-purpose post-recognition error corrector poses a crucial question: how can we most effectively train a model on a large mixture of domain datasets? The answer would lie in learning dataset-specific features and digesting their knowledge in a single model. Previous methods achieve this by having separate correction language models, resulting in a significant increase in parameters. In this work, we present Mixture-of-Experts as a solution, highlighting that MoEs are much more than a scalability tool. We propose a Multi-Task Correction MoE, where we train the experts to become an ``expert'' of speech-to-text, language-to-text and vision-to-text datasets by learning to route each dataset's tokens to its mapped expert. Experiments on the Open ASR Leaderboard show that we explore a new state-of-the-art performance by achieving an average relative 5.0% WER reduction and substantial improvements in BLEU scores for speech and translation tasks. On zero-shot evaluation, NeKo outperforms GPT-3.5 and Claude-Opus with 15.5% to 27.6% relative WER reduction in the Hyporadise benchmark. NeKo performs competitively on grammar and post-OCR correction as a multi-task model.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-based Human Simulations Have Not Yet Been Reliable</title>
<link>https://arxiv.org/abs/2501.08579</link>
<guid>https://arxiv.org/abs/2501.08579</guid>
<content:encoded><![CDATA[
arXiv:2501.08579v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly employed for simulating human behaviors across diverse domains. However, our position is that current LLM-based human simulations remain insufficiently reliable, as evidenced by significant discrepancies between their outcomes and authentic human actions. Our investigation begins with a systematic review of LLM-based human simulations in social, economic, policy, and psychological contexts, identifying their common frameworks, recent advances, and persistent limitations. This review reveals that such discrepancies primarily stem from inherent limitations of LLMs and flaws in simulation design, both of which are examined in detail. Building on these insights, we propose a systematic solution framework that emphasizes enriching data foundations, advancing LLM capabilities, and ensuring robust simulation design to enhance reliability. Finally, we introduce a structured algorithm that operationalizes the proposed framework, aiming to guide credible and human-aligned LLM-based simulations. To facilitate further research, we provide a curated list of related literature and resources at https://github.com/Persdre/awesome-llm-human-simulation.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generating Text from Uniform Meaning Representation</title>
<link>https://arxiv.org/abs/2502.11973</link>
<guid>https://arxiv.org/abs/2502.11973</guid>
<content:encoded><![CDATA[
arXiv:2502.11973v2 Announce Type: replace 
Abstract: Uniform Meaning Representation (UMR) is a recently developed graph-based semantic representation, which expands on Abstract Meaning Representation (AMR) in a number of ways, in particular through the inclusion of document-level information and multilingual flexibility. In order to effectively adopt and leverage UMR for downstream tasks, efforts must be placed toward developing a UMR technological ecosystem. Though only a small amount of UMR annotations have been produced to date, in this work, we investigate the first approaches to producing text from multilingual UMR graphs. Exploiting the structural similarity between UMR and AMR graphs and the wide availability of AMR technologies, we introduce (1) a baseline approach which passes UMR graphs to AMR-to-text generation models, (2) a pipeline conversion of UMR to AMR, then using AMR-to-text generation models, and (3) a fine-tuning approach for both foundation models and AMR-to-text generation models with UMR data. Our best performing models achieve multilingual BERTscores of 0.825 for English and 0.882 for Chinese, a promising indication of the effectiveness of fine-tuning approaches for UMR-to-text generation even with limited UMR data.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Med-gte-hybrid: A contextual embedding transformer model for extracting actionable information from clinical texts</title>
<link>https://arxiv.org/abs/2502.15996</link>
<guid>https://arxiv.org/abs/2502.15996</guid>
<content:encoded><![CDATA[
arXiv:2502.15996v3 Announce Type: replace 
Abstract: We introduce a novel contextual embedding model med-gte-hybrid that was derived from the gte-large sentence transformer to extract information from unstructured clinical narratives. Our model tuning strategy for med-gte-hybrid combines contrastive learning and a denoising autoencoder. To evaluate the performance of med-gte-hybrid, we investigate several clinical prediction tasks in large patient cohorts extracted from the MIMIC-IV dataset, including Chronic Kidney Disease (CKD) patient prognosis, estimated glomerular filtration rate (eGFR) prediction, and patient mortality prediction. Furthermore, we demonstrate that the med-gte-hybrid model improves patient stratification, clustering, and text retrieval, thus outperforms current state-of-the-art models on the Massive Text Embedding Benchmark (MTEB). While some of our evaluations focus on CKD, our hybrid tuning of sentence transformers could be transferred to other medical domains and has the potential to improve clinical decision-making and personalised treatment pathways in various healthcare applications.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MimeQA: Towards Socially-Intelligent Nonverbal Foundation Models</title>
<link>https://arxiv.org/abs/2502.16671</link>
<guid>https://arxiv.org/abs/2502.16671</guid>
<content:encoded><![CDATA[
arXiv:2502.16671v3 Announce Type: replace 
Abstract: As AI becomes more closely integrated with peoples' daily activities, socially intelligent AI that can understand and interact seamlessly with humans in daily lives is increasingly important. However, current works in AI social reasoning all rely on language-only or language-dominant approaches to benchmark and training models, resulting in systems that are improving in verbal communication but struggle with nonverbal social understanding. To address this limitation, we tap into a novel data source rich in nonverbal social interactions -- mime videos. Mimes refer to the art of expression through gesture and movement without spoken words, which presents unique challenges and opportunities in interpreting nonverbal social communication. We contribute a new dataset called MimeQA, obtained by sourcing ~8 hours of videos clips from YouTube and developing a comprehensive video question-answering benchmark comprising 806 carefully annotated and verified question-answer pairs, designed to probe nonverbal social reasoning capabilities. Using MimeQA, we evaluate state-of-the-art video large language models (VideoLLMs) and find that they achieve low accuracy, generally ranging from 20-30%, while humans score 86%. Our analysis reveals that VideoLLMs often fail to ground imagined objects and over-rely on the text prompt while ignoring subtle nonverbal interactions. We hope to inspire future work in AI models that embody true social intelligence capable of interpreting non-verbal human interactions.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-based Automated Grading with Human-in-the-Loop</title>
<link>https://arxiv.org/abs/2504.05239</link>
<guid>https://arxiv.org/abs/2504.05239</guid>
<content:encoded><![CDATA[
arXiv:2504.05239v3 Announce Type: replace 
Abstract: The rise of artificial intelligence (AI) technologies, particularly large language models (LLMs), has brought significant advancements to the field of education. Among various applications, automatic short answer grading (ASAG), which focuses on evaluating open-ended textual responses, has seen remarkable progress with the introduction of LLMs. These models not only enhance grading performance compared to traditional ASAG approaches but also move beyond simple comparisons with predefined "golden" answers, enabling more sophisticated grading scenarios, such as rubric-based evaluation. However, existing LLM-powered methods still face challenges in achieving human-level grading performance in rubric-based assessments due to their reliance on fully automated approaches. In this work, we explore the potential of LLMs in ASAG tasks by leveraging their interactive capabilities through a human-in-the-loop (HITL) approach. Our proposed framework, GradeHITL, utilizes the generative properties of LLMs to pose questions to human experts, incorporating their insights to refine grading rubrics dynamically. This adaptive process significantly improves grading accuracy, outperforming existing methods and bringing ASAG closer to human-level evaluation.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Method for Handling Negative Similarities in Explainable Graph Spectral Clustering of Text Documents -- Extended Version</title>
<link>https://arxiv.org/abs/2504.12360</link>
<guid>https://arxiv.org/abs/2504.12360</guid>
<content:encoded><![CDATA[
arXiv:2504.12360v2 Announce Type: replace 
Abstract: This paper investigates the problem of Graph Spectral Clustering with negative similarities, resulting from document embeddings different from the traditional Term Vector Space (like doc2vec, GloVe, etc.). Solutions for combinatorial Laplacians and normalized Laplacians are discussed. An experimental investigation shows the advantages and disadvantages of 6 different solutions proposed in the literature and in this research. The research demonstrates that GloVe embeddings frequently cause failures of normalized Laplacian based GSC due to negative similarities. Furthermore, application of methods curing similarity negativity leads to accuracy improvement for both combinatorial and normalized Laplacian based GSC. It also leads to applicability for GloVe embeddings of explanation methods developed originally bythe authors for Term Vector Space embeddings.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nemotron-CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training</title>
<link>https://arxiv.org/abs/2504.13161</link>
<guid>https://arxiv.org/abs/2504.13161</guid>
<content:encoded><![CDATA[
arXiv:2504.13161v2 Announce Type: replace 
Abstract: Pre-training datasets are typically collected from web content and lack inherent domain divisions. For instance, widely used datasets like Common Crawl do not include explicit domain labels, while manually curating labeled datasets such as The Pile is labor-intensive. Consequently, identifying an optimal pre-training data mixture remains a challenging problem, despite its significant benefits for pre-training performance. To address these challenges, we propose CLustering-based Iterative Data Mixture Bootstrapping (Nemotron-CLIMB), an automated framework that discovers, evaluates, and refines data mixtures in a pre-training setting. Specifically, Nemotron-CLIMB embeds and clusters large-scale datasets in a semantic space and then iteratively searches for optimal mixtures using a smaller proxy model and a predictor. When continuously trained on 400B tokens with this mixture, our 1B model exceeds the state-of-the-art Llama-3.2-1B by 2.0%. Moreover, we observe that optimizing for a specific domain (e.g., Social Sciences) yields a 5% improvement over random sampling. Finally, we introduce Nemotron-ClimbLab, a filtered 1.2-trillion-token corpus with 20 clusters as a research playground, and Nemotron-ClimbMix, a compact yet powerful 400-billion-token dataset designed for efficient pre-training that delivers superior performance under an equal token budget. We analyze the final data mixture, elucidating the characteristics of an optimal data mixture. Our data is available at: https://research.nvidia.com/labs/lpr/climb/
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do different prompting methods yield a common task representation in language models?</title>
<link>https://arxiv.org/abs/2505.12075</link>
<guid>https://arxiv.org/abs/2505.12075</guid>
<content:encoded><![CDATA[
arXiv:2505.12075v3 Announce Type: replace 
Abstract: Demonstrations and instructions are two primary approaches for prompting language models to perform in-context learning (ICL) tasks. Do identical tasks elicited in different ways result in similar representations of the task? An improved understanding of task representation mechanisms would offer interpretability insights and may aid in steering models. We study this through \textit{function vectors} (FVs), recently proposed as a mechanism to extract few-shot ICL task representations. We generalize FVs to alternative task presentations, focusing on short textual instruction prompts, and successfully extract instruction function vectors that promote zero-shot task accuracy. We find evidence that demonstration- and instruction-based function vectors leverage different model components, and offer several controls to dissociate their contributions to task performance. Our results suggest that different task promptings forms do not induce a common task representation through FVs but elicit different, partly overlapping mechanisms. Our findings offer principled support to the practice of combining instructions and task demonstrations, imply challenges in universally monitoring task inference across presentation forms, and encourage further examinations of LLM task inference mechanisms.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extracting memorized pieces of (copyrighted) books from open-weight language models</title>
<link>https://arxiv.org/abs/2505.12546</link>
<guid>https://arxiv.org/abs/2505.12546</guid>
<content:encoded><![CDATA[
arXiv:2505.12546v4 Announce Type: replace 
Abstract: Plaintiffs and defendants in copyright lawsuits over generative AI often make sweeping, opposing claims about the extent to which large language models (LLMs) have memorized plaintiffs' protected expression in their training data. Drawing on both machine learning and copyright law, we show that these polarized positions dramatically oversimplify the relationship between memorization and copyright. To do so, we extend a recent probabilistic extraction technique to measure memorization of 50 books in 17 open-weight LLMs. Through thousands of experiments, we show that the extent of memorization varies both by model and by book. With respect to our specific extraction methodology, we find that most LLMs do not memorize most books -- either in whole or in part. However, we also find that Llama 3.1 70B entirely memorizes some books, like the first Harry Potter book and 1984. In fact, the first Harry Potter is so memorized that, using a seed prompt consisting of just the first few tokens of the first chapter, we can deterministically generate the entire book near-verbatim. We discuss why our results have significant implications for copyright cases, though not ones that unambiguously favor either side.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone</title>
<link>https://arxiv.org/abs/2505.12781</link>
<guid>https://arxiv.org/abs/2505.12781</guid>
<content:encoded><![CDATA[
arXiv:2505.12781v3 Announce Type: replace 
Abstract: Training high-performing Small Language Models (SLMs) remains costly, even with knowledge distillation and pruning from larger teacher models. Existing work often faces three key challenges: (1) information loss from hard pruning, (2) inefficient alignment of representations, and (3) underutilization of informative activations, particularly from Feed-Forward Networks (FFNs). To address these challenges, we introduce Low-Rank Clone (LRC), an efficient pre-training method that constructs SLMs aspiring to behavioral equivalence with strong teacher models. LRC trains a set of low-rank projection matrices that jointly enable soft pruning by compressing teacher weights, and activation clone by aligning student activations, including FFN signals, with those of the teacher. This unified design maximizes knowledge transfer while removing the need for explicit alignment modules. Extensive experiments with open-source teachers (e.g., Llama-3.2-3B-Instruct, Qwen2.5-3B/7B-Instruct) show that LRC matches or surpasses state-of-the-art models trained on trillions of tokens--while using only 20B tokens, achieving over 1,000x training efficiency. Our codes and model checkpoints are available at https://github.com/CURRENTF/LowRankClone and https://huggingface.co/collections/JitaiHao/low-rank-clone-lrc-6828389e96a93f1d4219dfaf.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attributional Safety Failures in Large Language Models under Code-Mixed Perturbations</title>
<link>https://arxiv.org/abs/2505.14469</link>
<guid>https://arxiv.org/abs/2505.14469</guid>
<content:encoded><![CDATA[
arXiv:2505.14469v2 Announce Type: replace 
Abstract: While LLMs appear robustly safety-aligned in English, we uncover a catastrophic, overlooked weakness: attributional collapse under code-mixed perturbations. Our systematic evaluation of open models shows that the linguistic camouflage of code-mixing -- ``blending languages within a single conversation'' -- can cause safety guardrails to fail dramatically. Attack success rates (ASR) spike from a benign 9\% in monolingual English to 69\% under code-mixed inputs, with rates exceeding 90\% in non-Western contexts such as Arabic and Hindi. These effects hold not only on controlled synthetic datasets but also on real-world social media traces, revealing a serious risk for billions of users. To explain why this happens, we introduce saliency drift attribution (SDA), an interpretability framework that shows how, under code-mixing, the model's internal attention drifts away from safety-critical tokens (e.g., ``violence'' or ``corruption''), effectively blinding it to harmful intent. Finally, we propose a lightweight translation-based restoration strategy that recovers roughly 80\% of the safety lost to code-mixing, offering a practical path toward more equitable and robust LLM safety.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparative Evaluation of Expressive Japanese Character Text-to-Speech with VITS and Style-BERT-VITS2</title>
<link>https://arxiv.org/abs/2505.17320</link>
<guid>https://arxiv.org/abs/2505.17320</guid>
<content:encoded><![CDATA[
arXiv:2505.17320v2 Announce Type: replace 
Abstract: Synthesizing expressive Japanese character speech poses unique challenges due to pitch-accent sensitivity and stylistic variability. This paper empirically evaluates two open-source text-to-speech models--VITS and Style-BERT-VITS2 JP Extra (SBV2JE)--on in-domain, character-driven Japanese speech. Using three character-specific datasets, we evaluate models across naturalness (mean opinion and comparative mean opinion score), intelligibility (word error rate), and speaker consistency. SBV2JE matches human ground truth in naturalness (MOS 4.37 vs. 4.38), achieves lower WER, and shows slight preference in CMOS. Enhanced by pitch-accent controls and a WavLM-based discriminator, SBV2JE proves effective for applications like language learning and character dialogue generation, despite higher computational demands.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comprehensive Evaluation on Lexical Normalization: Boundary-Aware Approaches for Unsegmented Languages</title>
<link>https://arxiv.org/abs/2505.22273</link>
<guid>https://arxiv.org/abs/2505.22273</guid>
<content:encoded><![CDATA[
arXiv:2505.22273v2 Announce Type: replace 
Abstract: Lexical normalization research has sought to tackle the challenge of processing informal expressions in user-generated text, yet the absence of comprehensive evaluations leaves it unclear which methods excel across multiple perspectives. Focusing on unsegmented languages, we make three key contributions: (1) creating a large-scale, multi-domain Japanese normalization dataset, (2) developing normalization methods based on state-of-the-art pretrained models, and (3) conducting experiments across multiple evaluation perspectives. Our experiments show that both encoder-only and decoder-only approaches achieve promising results in both accuracy and efficiency.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs with POLLUX</title>
<link>https://arxiv.org/abs/2505.24616</link>
<guid>https://arxiv.org/abs/2505.24616</guid>
<content:encoded><![CDATA[
arXiv:2505.24616v4 Announce Type: replace 
Abstract: We introduce POLLUX, a comprehensive open-source benchmark designed to evaluate the generative capabilities of large language models (LLMs) in Russian. Our main contribution is a novel evaluation methodology that enhances the interpretability of LLM assessment. For each task type, we define a set of detailed criteria and develop a scoring protocol where models evaluate responses and provide justifications for their ratings. This enables transparent, criteria-driven evaluation beyond traditional resource-consuming, side-by-side human comparisons. POLLUX includes a detailed, fine-grained taxonomy of 35 task types covering diverse generative domains such as code generation, creative writing, and practical assistant use cases, totaling 2,100 manually crafted and professionally authored prompts. Each task is categorized by difficulty (easy/medium/hard), with experts constructing the dataset entirely from scratch. We also release a family of LLM-as-a-Judge (7B and 32B) evaluators trained for nuanced assessment of generative outputs. This approach provides scalable, interpretable evaluation and annotation tools for model development, effectively replacing costly and less precise human judgments.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring and Guiding Monosemanticity</title>
<link>https://arxiv.org/abs/2506.19382</link>
<guid>https://arxiv.org/abs/2506.19382</guid>
<content:encoded><![CDATA[
arXiv:2506.19382v2 Announce Type: replace 
Abstract: There is growing interest in leveraging mechanistic interpretability and controllability to better understand and influence the internal dynamics of large language models (LLMs). However, current methods face fundamental challenges in reliably localizing and manipulating feature representations. Sparse Autoencoders (SAEs) have recently emerged as a promising direction for feature extraction at scale, yet they, too, are limited by incomplete feature isolation and unreliable monosemanticity. To systematically quantify these limitations, we introduce Feature Monosemanticity Score (FMS), a novel metric to quantify feature monosemanticity in latent representation. Building on these insights, we propose Guided Sparse Autoencoders (G-SAE), a method that conditions latent representations on labeled concepts during training. We demonstrate that reliable localization and disentanglement of target concepts within the latent space improve interpretability, detection of behavior, and control. Specifically, our evaluations on toxicity detection, writing style identification, and privacy attribute recognition show that G-SAE not only enhances monosemanticity but also enables more effective and fine-grained steering with less quality degradation. Our findings provide actionable guidelines for measuring and advancing mechanistic interpretability and control of LLMs.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying Cognitive Bias Induction in LLM-Generated Content</title>
<link>https://arxiv.org/abs/2507.03194</link>
<guid>https://arxiv.org/abs/2507.03194</guid>
<content:encoded><![CDATA[
arXiv:2507.03194v2 Announce Type: replace 
Abstract: Large language models (LLMs) are integrated into applications like shopping reviews, summarization, or medical diagnosis support, where their use affects human decisions. We investigate the extent to which LLMs expose users to biased content and demonstrate its effect on human decision-making. We assess five LLM families in summarization and news fact-checking tasks, evaluating the consistency of LLMs with their context and their tendency to hallucinate on a new self-updating dataset. Our findings show that LLMs expose users to content that changes the context's sentiment in 26.42% of cases (framing bias), hallucinate on 60.33% of post-knowledge-cutoff questions, and highlight context from earlier parts of the prompt (primacy bias) in 10.12% of cases, averaged across all tested models. We further find that humans are 32% more likely to purchase the same product after reading a summary of the review generated by an LLM rather than the original review. To address these issues, we evaluate 18 mitigation methods across three LLM families and find the effectiveness of targeted interventions.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Robustness of Verbal Confidence of LLMs in Adversarial Attacks</title>
<link>https://arxiv.org/abs/2507.06489</link>
<guid>https://arxiv.org/abs/2507.06489</guid>
<content:encoded><![CDATA[
arXiv:2507.06489v2 Announce Type: replace 
Abstract: Robust verbal confidence generated by large language models (LLMs) is crucial for the deployment of LLMs to help ensure transparency, trust, and safety in many applications, including those involving human-AI interactions. In this paper, we present the first comprehensive study on the robustness of verbal confidence under adversarial attacks. We introduce attack frameworks targeting verbal confidence scores through both perturbation and jailbreak-based methods, and demonstrate that these attacks can significantly impair verbal confidence estimates and lead to frequent answer changes. We examine a variety of prompting strategies, model sizes, and application domains, revealing that current verbal confidence is vulnerable and that commonly used defence techniques are largely ineffective or counterproductive. Our findings underscore the need to design robust mechanisms for confidence expression in LLMs, as even subtle semantic-preserving modifications can lead to misleading confidence in responses.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Confident RAG: Enhancing the Performance of LLMs for Mathematics Question Answering through Multi-Embedding and Confidence Scoring</title>
<link>https://arxiv.org/abs/2507.17442</link>
<guid>https://arxiv.org/abs/2507.17442</guid>
<content:encoded><![CDATA[
arXiv:2507.17442v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) hold significant promise for mathematics education, yet they often struggle with complex mathematical reasoning. While Retrieval-Augmented Generation (RAG) mitigates these issues by grounding LLMs in external knowledge, its effectiveness remains unstable, heavily dependent on the choice of a single embedding model. Moving beyond static RAG workflows, we draw on agentic workflow patterns, a paradigm that introduces structured task decomposition and collaboration to enhance system performance. We propose and examine two novel approaches that combine the benefits of multiple embedding models. While our Mixture-Embedding RAG approach (fusing retrieved documents) shows limited gains, our Confident RAG method (generating multiple answers and selecting the one with the highest confidence score) demonstrates significant improvement. Experimental results show that Confident RAG achieved average accuracy improvements of approximately 10% over vanilla LLMs and 5% over vanilla RAG. The consistent results across different LLMs and embedding models indicate that Confident RAG is an efficient plug-and-play solution for trustworthy mathematical AI assistants. Finally, we discuss how this work lays the groundwork for deploying Agentic RAG systems in educational settings, where autonomous planning and iterative refinement can be built upon our robust retrieval foundation.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Checklists Are Better Than Reward Models For Aligning Language Models</title>
<link>https://arxiv.org/abs/2507.18624</link>
<guid>https://arxiv.org/abs/2507.18624</guid>
<content:encoded><![CDATA[
arXiv:2507.18624v2 Announce Type: replace 
Abstract: Language models must be adapted to understand and follow user instructions. Reinforcement learning is widely used to facilitate this -- typically using fixed criteria such as "helpfulness" and "harmfulness". In our work, we instead propose using flexible, instruction-specific criteria as a means of broadening the impact that reinforcement learning can have in eliciting instruction following. We propose "Reinforcement Learning from Checklist Feedback" (RLCF). From instructions, we extract checklists and evaluate how well responses satisfy each item - using both AI judges and specialized verifier programs - then combine these scores to compute rewards for RL. We compare RLCF with other alignment methods applied to a strong instruction following model (Qwen2.5-7B-Instruct) on five widely-studied benchmarks -- RLCF is the only method to improve performance on every benchmark, including a 4-point boost in hard satisfaction rate on FollowBench, a 6-point increase on InFoBench, and a 3-point rise in win rate on Arena-Hard. These results establish checklist feedback as a key tool for improving language models' support of queries that express a multitude of needs.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Debating Truth: Debate-driven Claim Verification with Multiple Large Language Model Agents</title>
<link>https://arxiv.org/abs/2507.19090</link>
<guid>https://arxiv.org/abs/2507.19090</guid>
<content:encoded><![CDATA[
arXiv:2507.19090v2 Announce Type: replace 
Abstract: Claim verification is essential for digital literacy, yet state-of-the-art single-agent methods often struggle with complex claims that require nuanced analysis of multifaceted online evidence. Inspired by real-world human fact-checking practices, we propose \textbf{DebateCV}, the first debate-driven claim verification framework powered by multiple LLM agents. In DebateCV, two \textit{Debaters} argue opposing stances over multiple rounds to surface subtle errors in single-agent assessments. A decisive \textit{Moderator} is then required to weigh the evidential strength of conflicting arguments to deliver an accurate verdict. Yet zero-shot agents struggle to adjudicate multi-round debates for verifying complex claims, often defaulting to neutral judgements, and no datasets exist for training agents for this role. To bridge this gap, we propose \textbf{Debate-SFT}, a post-training framework that leverages synthetic data to enhance agents' ability to effectively adjudicate debates for claim verification. Results show that our methods surpass state-of-the-art non-debate approaches in both accuracy (across various evidence conditions) and justification quality, which strengthens societal resilience against misinformation and contributes to a more trustworthy online information ecosystem.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpeechIQ: Speech-Agentic Intelligence Quotient Across Cognitive Levels in Voice Understanding by Large Language Models</title>
<link>https://arxiv.org/abs/2507.19361</link>
<guid>https://arxiv.org/abs/2507.19361</guid>
<content:encoded><![CDATA[
arXiv:2507.19361v2 Announce Type: replace 
Abstract: We introduce Speech-based Intelligence Quotient (SIQ) as a new form of human cognition-inspired evaluation pipeline for voice understanding large language models, LLM Voice, designed to assess their voice understanding ability. Moving beyond popular voice understanding metrics such as word error rate (WER), SIQ examines LLM Voice across three cognitive levels motivated by Bloom's Taxonomy: (1) Remembering (i.e., WER for verbatim accuracy); (2) Understanding (i.e., similarity of LLM's interpretations); and (3) Application (i.e., QA accuracy for simulating downstream tasks). We demonstrate that SIQ not only quantifies voice understanding abilities but also provides unified comparisons between cascaded methods (e.g., ASR LLM) and end-to-end models, identifies annotation errors in existing benchmarks, and detects hallucinations in LLM Voice. Our framework represents a first-of-its-kind intelligence examination that bridges cognitive principles with voice-oriented benchmarks, while exposing overlooked challenges in multi-modal training. Our code and data will be open source to encourage future studies.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpeechRole: A Large-Scale Dataset and Benchmark for Evaluating Speech Role-Playing Agents</title>
<link>https://arxiv.org/abs/2508.02013</link>
<guid>https://arxiv.org/abs/2508.02013</guid>
<content:encoded><![CDATA[
arXiv:2508.02013v4 Announce Type: replace 
Abstract: Recently, role-playing agents have emerged as a promising paradigm for achieving personalized interaction and emotional resonance. Existing research primarily focuses on the textual modality, neglecting the critical dimension of speech in realistic interactive scenarios. In particular, there is a lack of systematic evaluation for Speech Role-Playing Agents (SRPAs). To address this gap, we construct SpeechRole-Data, a large-scale, high-quality dataset that comprises 98 diverse roles and 112k speech-based single-turn and multi-turn conversations. Each role demonstrates distinct vocal characteristics, including timbre and prosody, thereby enabling more sophisticated speech role-playing. Furthermore, we propose SpeechRole-Eval, a multidimensional evaluation benchmark that systematically assesses SRPAs performance in key aspects such as fundamental interaction ability, speech expressiveness, and role-playing fidelity. Experimental results reveal the advantages and challenges of both cascaded and end-to-end speech role-playing agents in maintaining vocal style consistency and role coherence. We release all data, code, and baseline models to provide a solid foundation for speech-driven multimodal role-playing research and to foster further developments in this field.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ShoppingBench: A Real-World Intent-Grounded Shopping Benchmark for LLM-based Agents</title>
<link>https://arxiv.org/abs/2508.04266</link>
<guid>https://arxiv.org/abs/2508.04266</guid>
<content:encoded><![CDATA[
arXiv:2508.04266v2 Announce Type: replace 
Abstract: Existing benchmarks in e-commerce primarily focus on basic user intents, such as finding or purchasing products. However, real-world users often pursue more complex goals, such as applying vouchers, managing budgets, and finding multi-products seller. To bridge this gap, we propose ShoppingBench, a novel end-to-end shopping benchmark designed to encompass increasingly challenging levels of grounded intent. Specifically, we propose a scalable framework to simulate user instructions based on various intents derived from sampled real-world products. To facilitate consistent and reliable evaluations, we provide a large-scale shopping sandbox that serves as an interactive simulated environment, incorporating over 2.5 million real-world products. Experimental results demonstrate that even state-of-the-art language agents (such as GPT-4.1) achieve absolute success rates under 50% on our benchmark tasks, highlighting the significant challenges posed by our ShoppingBench. In addition, we propose a trajectory distillation strategy and leverage supervised fine-tuning, along with reinforcement learning on synthetic trajectories, to distill the capabilities of a large language agent into a smaller one. As a result, our trained agent achieves competitive performance compared to GPT-4.1.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History</title>
<link>https://arxiv.org/abs/2508.04826</link>
<guid>https://arxiv.org/abs/2508.04826</guid>
<content:encoded><![CDATA[
arXiv:2508.04826v2 Announce Type: replace 
Abstract: Large language models require consistent behavioral patterns for safe deployment, yet there are indications of large variability that may lead to an instable expression of personality traits in these models. We present PERSIST (PERsonality Stability in Synthetic Text), a comprehensive evaluation framework testing 25 open-source models (1B-685B parameters) across 2 million+ responses. Using traditional (BFI, SD3) and novel LLM-adapted personality questionnaires, we systematically vary model size, personas, reasoning modes, question order or paraphrasing, and conversation history. Our findings challenge fundamental assumptions: (1) Question reordering alone can introduce large shifts in personality measurements; (2) Scaling provides limited stability gains: even 400B+ models exhibit standard deviations >0.3 on 5-point scales; (3) Interventions expected to stabilize behavior, such as reasoning and inclusion of conversation history, can paradoxically increase variability; (4) Detailed persona instructions produce mixed effects, with misaligned personas showing significantly higher variability than the helpful assistant baseline; (5) The LLM-adapted questionnaires, despite their improved ecological validity, exhibit instability comparable to human-centric versions. This persistent instability across scales and mitigation strategies suggests that current LLMs lack the architectural foundations for genuine behavioral consistency. For safety-critical applications requiring predictable behavior, these findings indicate that current alignment strategies may be inadequate.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CCFQA: A Benchmark for Cross-Lingual and Cross-Modal Speech and Text Factuality Evaluation</title>
<link>https://arxiv.org/abs/2508.07295</link>
<guid>https://arxiv.org/abs/2508.07295</guid>
<content:encoded><![CDATA[
arXiv:2508.07295v2 Announce Type: replace 
Abstract: As Large Language Models (LLMs) are increasingly popularized in the multilingual world, ensuring hallucination-free factuality becomes markedly crucial. However, existing benchmarks for evaluating the reliability of Multimodal Large Language Models (MLLMs) predominantly focus on textual or visual modalities with a primary emphasis on English, which creates a gap in evaluation when processing multilingual input, especially in speech. To bridge this gap, we propose a novel Cross-lingual and Cross-modal Factuality benchmark (CCFQA). Specifically, the CCFQA benchmark contains parallel speech-text factual questions across 8 languages, designed to systematically evaluate MLLMs' cross-lingual and cross-modal factuality capabilities. Our experimental results demonstrate that current MLLMs still face substantial challenges on the CCFQA benchmark. Furthermore, we propose a few-shot transfer learning strategy that effectively transfers the Question Answering (QA) capabilities of LLMs in English to multilingual Spoken Question Answering (SQA) tasks, achieving competitive performance with GPT-4o-mini-Audio using just 5-shot training. We release CCFQA as a foundational research resource to promote the development of MLLMs with more robust and reliable speech understanding capabilities. Our code and dataset are available at https://github.com/yxduir/ccfqa.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM Reasoning</title>
<link>https://arxiv.org/abs/2508.12726</link>
<guid>https://arxiv.org/abs/2508.12726</guid>
<content:encoded><![CDATA[
arXiv:2508.12726v4 Announce Type: replace 
Abstract: Large language models (LLMs) have achieved remarkable success in many natural language tasks but still struggle with complex, multi-step reasoning, particularly across diverse disciplines. Existing reasoning datasets often lack disciplinary breadth, reasoning depth, and diversity, as well as guiding principles for question synthesis. We propose DESIGNER: a DESIGN-logic-guidEd Reasoning data synthesis pipeline that leverages naturally available, extensive raw documents (e.g., book corpus and web corpus) to generate multidisciplinary challenging questions. We introduce the concept of "design logic" and instruct LLMs to mimic human educators' question-creation process, enabling the automated synthesis of large-scale, high-difficulty questions. We use LLMs to reverse-engineer and abstract over 120,000 design logics from existing questions across various disciplines. By matching these design logics with source documents, we are able to generate reasoning questions with controllable question types and difficulty levels. Using this pipeline, we synthesized two large-scale reasoning datasets that span 75 disciplines: DLR-Book (3.04 million questions from the book corpus) and DLR-Web (1.66 million questions from the web corpus). Data analysis indicates that the questions synthesized by our method exhibit greater difficulty and diversity compared to those in the baseline datasets. We validate our synthesized data through supervised fine-tuning (SFT) on the Qwen3 and Llama3 model families. Our data substantially enhances their multidisciplinary reasoning capabilities, outperforming existing datasets. Notably, by applying SFT on the base versions of these models using only our data, we even surpass their official final models that have undergone the full post-training process.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chain-of-Query: Unleashing the Power of LLMs in SQL-Aided Table Understanding via Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2508.15809</link>
<guid>https://arxiv.org/abs/2508.15809</guid>
<content:encoded><![CDATA[
arXiv:2508.15809v3 Announce Type: replace 
Abstract: Table understanding requires structured, multi-step reasoning. Large Language Models (LLMs) struggle with it due to the structural complexity of tabular data. Recently, multi-agent frameworks for SQL generation have shown promise in tackling the challenges of understanding tabular data, but existing approaches often suffer from limitations such as the inability to comprehend table structure for reliable SQL generation, error propagation that results in invalid queries, and over-reliance on execution correctness. To address these issues, we propose Chain-of-Query (CoQ), a novel multi-agent framework for SQL-aided table understanding. CoQ adopts natural-language-style representations of table schemas to abstract away structural noise and enhance understanding. It employs a clause-by-clause SQL generation strategy to improve query quality and introduces a hybrid reasoning division that separates SQL-based mechanical reasoning from LLM-based logical inference, thereby reducing reliance on execution outcomes. Extensive experiments across four models and five widely used benchmarks demonstrate that CoQ achieves substantial accuracy improvements and significantly lowers invalid SQL rates compared to prior generic LLM-based, SQL-aided, and hybrid baselines, confirming its superior effectiveness in table understanding. The code is available at https://github.com/SongyuanSui/ChainofQuery.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning-Intensive Regression</title>
<link>https://arxiv.org/abs/2508.21762</link>
<guid>https://arxiv.org/abs/2508.21762</guid>
<content:encoded><![CDATA[
arXiv:2508.21762v2 Announce Type: replace 
Abstract: AI researchers and practitioners increasingly apply large language models (LLMs) to what we call reasoning-intensive regression (RiR), i.e., deducing subtle numerical scores from text. Unlike standard language regression tasks, e.g., for sentiment or similarity, RiR often appears instead in ad-hoc problems such as rubric-based scoring, modeling dense rewards in complex environments, or domain-specific retrieval, where much deeper analysis of context is required while only limited task-specific training data and computation are available. We cast four realistic problems as RiR tasks to establish an initial benchmark, and use that to test our hypothesis that prompting frozen LLMs and finetuning Transformer encoders via gradient descent will both often struggle in RiR. We then propose MENTAT, a simple and lightweight method that combines batch-reflective prompt optimization with neural ensemble learning. MENTAT achieves up to 65% improvement over both baselines, though substantial room remains for future advances in RiR.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Semantic Text Relations: A Question-Answering Framework for Comparing Document Content</title>
<link>https://arxiv.org/abs/2509.08304</link>
<guid>https://arxiv.org/abs/2509.08304</guid>
<content:encoded><![CDATA[
arXiv:2509.08304v2 Announce Type: replace 
Abstract: Understanding semantic relations between two texts is crucial for many information and document management tasks, in which one must determine whether the content fully overlaps, is completely superseded by another document, or overlaps only partially, with unique information in each. Beyond establishing this relation, it is equally important to provide explainable outputs that specify which pieces of information are present, missing, or newly added between the text pair. In this study, we formally define semantic relations between two texts through the set-theoretic relation between their respective Answerable Question Sets (AQS), the sets of questions each text can answer. Under this formulation, Semantic Text Relation (STR), such as equivalence, inclusion, and mutual overlap, becomes a well-defined set relation between the corresponding texts' AQSs. The set differences between the AQSs also serve as an explanation or diagnostic tool for identifying how the information in the texts diverges. Using this definition, we construct a synthetic benchmark that captures fine-grained informational relations through controlled paraphrasing and deliberate information removal supported by AQS manipulations. We then use this dataset to evaluate several discriminative and generative models for classifying text pairs into STR categories, assessing how well different model architectures capture semantic relations beyond surface-level similarity. We publicly release both the dataset and the data generation code to support further research.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing Natural Language Formalization to First Order Logic with Fine-tuned LLMs</title>
<link>https://arxiv.org/abs/2509.22338</link>
<guid>https://arxiv.org/abs/2509.22338</guid>
<content:encoded><![CDATA[
arXiv:2509.22338v2 Announce Type: replace 
Abstract: Automating the translation of natural language to first-order logic (FOL) is crucial for knowledge representation and formal methods, yet remains challenging. We present a systematic evaluation of fine-tuned LLMs for this task, comparing architectures (encoder-decoder vs. decoder-only) and training strategies. Using the MALLS and Willow datasets, we explore techniques like vocabulary extension, predicate conditioning, and multilingual training, introducing metrics for exact match, logical equivalence, and predicate alignment. Our fine-tuned Flan-T5-XXL achieves 70% accuracy with predicate lists, outperforming GPT-4o and even the DeepSeek-R1-0528 model with CoT reasoning ability as well as symbolic systems like ccg2lambda. Key findings show: (1) predicate availability boosts performance by 15-20%, (2) T5 models surpass larger decoder-only LLMs, and (3) models generalize to unseen logical arguments (FOLIO dataset) without specific training. While structural logic translation proves robust, predicate extraction emerges as the main bottleneck.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations</title>
<link>https://arxiv.org/abs/2510.04398</link>
<guid>https://arxiv.org/abs/2510.04398</guid>
<content:encoded><![CDATA[
arXiv:2510.04398v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly deployed in high-risk domains. However, state-of-the-art LLMs often produce hallucinations, raising serious concerns about their reliability. Prior work has explored adversarial attacks for hallucination elicitation in LLMs, but it often produces unrealistic prompts, either by inserting gibberish tokens or by altering the original meaning. As a result, these approaches offer limited insight into how hallucinations may occur in practice. While adversarial attacks in computer vision often involve realistic modifications to input images, the problem of finding realistic adversarial prompts for eliciting LLM hallucinations has remained largely underexplored. To address this gap, we propose Semantically Equivalent and Coherent Attacks (SECA) to elicit hallucinations via realistic modifications to the prompt that preserve its meaning while maintaining semantic coherence. Our contributions are threefold: (i) we formulate finding realistic attacks for hallucination elicitation as a constrained optimization problem over the input prompt space under semantic equivalence and coherence constraints; (ii) we introduce a constraint-preserving zeroth-order method to effectively search for adversarial yet feasible prompts; and (iii) we demonstrate through experiments on open-ended multiple-choice question answering tasks that SECA achieves higher attack success rates while incurring almost no semantic equivalence or semantic coherence errors compared to existing methods. SECA highlights the sensitivity of both open-source and commercial gradient-inaccessible LLMs to realistic and plausible prompt variations. Code is available at https://github.com/Buyun-Liang/SECA.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TRepLiNa: Layer-wise CKA+REPINA Alignment Improves Low-Resource Machine Translation in Aya-23 8B</title>
<link>https://arxiv.org/abs/2510.06249</link>
<guid>https://arxiv.org/abs/2510.06249</guid>
<content:encoded><![CDATA[
arXiv:2510.06249v3 Announce Type: replace 
Abstract: The 2025 Multimodal Models for Low-Resource Contexts and Social Impact (MMLoSo) Language Challenge addresses one of India's most pressing linguistic gaps: the lack of resources for its diverse low-resource languages (LRLs). In this study, we investigate whether enforcing cross-lingual similarity in specific internal layers of a decoder-only multilingual large language model (LLM) can improve translation quality from LRL to high-resource language (HRL). Specifically, we combine Centered Kernel Alignment (CKA), a similarity metric that encourages representations of different languages to align, with REPINA, a regularization method that constrains parameter updates to remain close to the pretrained model, into a joint method we call TRepLiNa. In this research project, we experiment with zero-shot, few-shot, and fine-tuning settings using Aya-23 8B with QLoRA across MMLoSo shared task language pairs (Mundari, Santali, Bhili) with Hindi/English pivots. Our results show that aligning mid-level layers using TRepLiNa (CKA+REPINA) is a low-cost, practical approach to improving LRL translation, especially in data-scarce settings.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NarraBench: A Comprehensive Framework for Narrative Benchmarking</title>
<link>https://arxiv.org/abs/2510.09869</link>
<guid>https://arxiv.org/abs/2510.09869</guid>
<content:encoded><![CDATA[
arXiv:2510.09869v3 Announce Type: replace 
Abstract: We present NarraBench, a theory-informed taxonomy of narrative-understanding tasks, as well as an associated survey of 78 existing benchmarks in the area. We find significant need for new evaluations covering aspects of narrative understanding that are either overlooked in current work or are poorly aligned with existing metrics. Specifically, we estimate that only 27% of narrative tasks are well captured by existing benchmarks, and we note that some areas -- including narrative events, style, perspective, and revelation -- are nearly absent from current evaluations. We also note the need for increased development of benchmarks capable of assessing constitutively subjective and perspectival aspects of narrative, that is, aspects for which there is generally no single correct answer. Our taxonomy, survey, and methodology are of value to NLP researchers seeking to test LLM narrative understanding.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Closing the Data-Efficiency Gap Between Autoregressive and Masked Diffusion LLMs</title>
<link>https://arxiv.org/abs/2510.09885</link>
<guid>https://arxiv.org/abs/2510.09885</guid>
<content:encoded><![CDATA[
arXiv:2510.09885v2 Announce Type: replace 
Abstract: Despite autoregressive large language models (arLLMs) being the current dominant paradigm in language modeling, effectively updating these models to incorporate new factual knowledge still remains difficult. They resist knowledge injection via fine-tuning due to inherent shortcomings such as the "reversal curse" -- the challenge of answering questions that reverse the original information order in the training sample. Masked diffusion large language models (dLLMs) are rapidly emerging as a powerful alternative to the arLLM paradigm, with evidence of better data efficiency and free of the "reversal curse" in pre-training. However, it is unknown whether these advantages extend to the post-training phase, i.e. whether pre-trained dLLMs can easily acquire new knowledge through fine-tuning. On three diverse datasets, we fine-tune arLLMs and dLLMs, evaluating them with forward and backward style Question Answering (QA) to probe knowledge generalization and the reversal curse. Our results confirm that arLLMs critically rely on extensive data augmentation via paraphrases for QA generalization, and paraphrases are only effective when their information order matches the QA style. Conversely, dLLMs achieve high accuracies on both forward and backward QAs without paraphrases; adding paraphrases yields only marginal gains. Inspired by the dLLM's performance, we introduce a novel masked fine-tuning paradigm for knowledge injection into pre-trained arLLMs. This proposed method successfully and drastically improves the data efficiency of arLLM fine-tuning, effectively closing its performance gap with dLLMs. We further show that the masked fine-tuning paradigm of arLLMs can be extended to the supervised fine-tuning (SFT) of mathematical capability. Across two models and two datasets, our masked SFT outperforms regular SFT.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DND: Boosting Large Language Models with Dynamic Nested Depth</title>
<link>https://arxiv.org/abs/2510.11001</link>
<guid>https://arxiv.org/abs/2510.11001</guid>
<content:encoded><![CDATA[
arXiv:2510.11001v2 Announce Type: replace 
Abstract: We introduce Dynamic Nested Depth (DND), a novel method that improves performance for off-the-shelf LLMs by selecting critical tokens to reprocess in a nested depth manner. Specifically, at the end of the given transformer layer, DND identifies more critical tokens with a router and feeds them back for an extra round of processing, effectively ``reviewing" difficult tokens while avoiding redundant computation for easier ones. The dynamic selection mechanism is tailored for precise control via two novel strategies: a router controlling loss to enhance token selection distinguishability, and a threshold control scheme to ensure selection stability. We demonstrate the effectiveness of DND by directly integrating it into pre-trained dense and MoE models during a post-training phase. On diverse benchmarks, this approach boosts the performances of the dense Qwen3-1.7B by 1.88% and the MoE Qwen3-30B-A3B by 0.87%, all with a minimal parameter and computing increase.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Recursive numeral systems are highly regular and easy to process</title>
<link>https://arxiv.org/abs/2510.27049</link>
<guid>https://arxiv.org/abs/2510.27049</guid>
<content:encoded><![CDATA[
arXiv:2510.27049v2 Announce Type: replace 
Abstract: Previous work has argued that recursive numeral systems optimise the trade-off between lexicon size and average morphosyntatic complexity (Deni\'c and Szymanik, 2024). However, showing that only natural-language-like systems optimise this tradeoff has proven elusive, and the existing solution has relied on ad-hoc constraints to rule out unnatural systems (Yang and Regier, 2025). Here, we argue that this issue arises because the proposed trade-off has neglected regularity, a crucial aspect of complexity central to human grammars in general. Drawing on the Minimum Description Length (MDL) approach, we propose that recursive numeral systems are better viewed as efficient with regard to their regularity and processing complexity. We show that our MDL-based measures of regularity and processing complexity better capture the key differences between attested, natural systems and unattested but possible ones, including "optimal" recursive numeral systems from previous work, and that the ad-hoc constraints from previous literature naturally follow from regularity. Our approach highlights the need to incorporate regularity across sets of forms in studies that attempt to measure and explain optimality in language.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EQ-Negotiator: Dynamic Emotional Personas Empower Small Language Models for Edge-Deployable Credit Negotiation</title>
<link>https://arxiv.org/abs/2511.03370</link>
<guid>https://arxiv.org/abs/2511.03370</guid>
<content:encoded><![CDATA[
arXiv:2511.03370v2 Announce Type: replace 
Abstract: The deployment of large language models (LLMs) in automated negotiation has set a high performance benchmark, but their computational cost and data privacy requirements render them unsuitable for many privacy-sensitive, on-device applications such as mobile assistants, embodied AI agents or private client interactions. While small language models (SLMs) offer a practical alternative, they suffer from a significant performance gap compared to LLMs in playing emotionally charged complex personas, especially for credit negotiation. This paper introduces EQ-Negotiator, a novel framework that bridges this capability gap using emotional personas. Its core is a reasoning system that integrates game theory with a Hidden Markov Model(HMM) to learn and track debtor emotional states online, without pre-training. This allows EQ-Negotiator to equip SLMs with the strategic intelligence to counter manipulation while de-escalating conflict and upholding ethical standards. Through extensive agent-to-agent simulations across diverse credit negotiation scenarios, including adversarial debtor strategies like cheating, threatening, and playing the victim, we show that a 7B parameter language model with EQ-Negotiator achieves better debt recovery and negotiation efficiency than baseline LLMs more than 10 times its size. This work advances persona modeling from descriptive character profiles to dynamic emotional architectures that operate within privacy constraints. Besides, this paper establishes that strategic emotional intelligence, not raw model scale, is the critical factor for success in automated negotiation, paving the way for effective, ethical, and privacy-preserving AI negotiators that can operate on the edge.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DPRM: A Dual Implicit Process Reward Model in Multi-Hop Question Answering</title>
<link>https://arxiv.org/abs/2511.08364</link>
<guid>https://arxiv.org/abs/2511.08364</guid>
<content:encoded><![CDATA[
arXiv:2511.08364v2 Announce Type: replace 
Abstract: In multi-hop question answering (MHQA) tasks, Chain of Thought (CoT) improves the quality of generation by guiding large language models (LLMs) through multi-step reasoning, and Knowledge Graphs (KGs) reduce hallucinations via semantic matching. Outcome Reward Models (ORMs) provide feedback after generating the final answers but fail to evaluate the process for multi-step reasoning. Traditional Process Reward Models (PRMs) evaluate the reasoning process but require costly human annotations or rollout generation. While implicit PRM is trained only with outcome signals and derives step rewards through reward parameterization without explicit annotations, it is more suitable for multi-step reasoning in MHQA tasks. However, existing implicit PRM has only been explored for plain text scenarios. When adapting to MHQA tasks, it cannot handle the graph structure constraints in KGs and capture the potential inconsistency between CoT and KG paths. To address these limitations, we propose the DPRM (Dual Implicit Process Reward Model). It trains two implicit PRMs for CoT and KG reasoning in MHQA tasks. Both PRMs, namely KG-PRM and CoT-PRM, derive step-level rewards from outcome signals via reward parameterization without additional explicit annotations. Among them, KG-PRM uses preference pairs to learn structural constraints from KGs. DPRM further introduces a consistency constraint between CoT and KG reasoning steps, making the two PRMs mutually verify and collaboratively optimize the reasoning paths. We also provide a theoretical demonstration of the derivation of process rewards. Experimental results show that our method outperforms 13 baselines on multiple datasets with up to 16.6% improvement on Hit@1.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Black-Box On-Policy Distillation of Large Language Models</title>
<link>https://arxiv.org/abs/2511.10643</link>
<guid>https://arxiv.org/abs/2511.10643</guid>
<content:encoded><![CDATA[
arXiv:2511.10643v2 Announce Type: replace 
Abstract: Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient LLM-Jailbreaking via Multimodal-LLM Jailbreak</title>
<link>https://arxiv.org/abs/2405.20015</link>
<guid>https://arxiv.org/abs/2405.20015</guid>
<content:encoded><![CDATA[
arXiv:2405.20015v3 Announce Type: replace-cross 
Abstract: This paper focuses on jailbreaking attacks against large language models (LLMs), eliciting them to generate objectionable content in response to harmful user queries. Unlike previous LLM-jailbreak methods that directly orient to LLMs, our approach begins by constructing a multimodal large language model (MLLM) built upon the target LLM. Subsequently, we perform an efficient MLLM jailbreak and obtain a jailbreaking embedding. Finally, we convert the embedding into a textual jailbreaking suffix to carry out the jailbreak of target LLM. Compared to the direct LLM-jailbreak methods, our indirect jailbreaking approach is more efficient, as MLLMs are more vulnerable to jailbreak than pure LLM. Additionally, to improve the attack success rate of jailbreak, we propose an image-text semantic matching scheme to identify a suitable initial input. Extensive experiments demonstrate that our approach surpasses current state-of-the-art jailbreak methods in terms of both efficiency and effectiveness. Moreover, our approach exhibits superior cross-class generalization abilities.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GermanPartiesQA: Benchmarking Commercial Large Language Models and AI Companions for Political Alignment and Sycophancy</title>
<link>https://arxiv.org/abs/2407.18008</link>
<guid>https://arxiv.org/abs/2407.18008</guid>
<content:encoded><![CDATA[
arXiv:2407.18008v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are increasingly shaping citizens' information ecosystems. Products incorporating LLMs, such as chatbots and AI Companions, are now widely used for decision support and information retrieval, including in sensitive domains, raising concerns about hidden biases and growing potential to shape individual decisions and public opinion. This paper introduces GermanPartiesQA, a benchmark of 418 political statements from German Voting Advice Applications across 11 elections to evaluate six commercial LLMs. We evaluate their political alignment based on role-playing experiments with political personas. Our evaluation reveals three specific findings: (1) Factual limitations: LLMs show limited ability to accurately generate factual party positions, particularly for centrist parties. (2) Model-specific ideological alignment: We identify consistent alignment patterns and the degree of political steerability for each model across temperature settings and experiments. (3) Claim of sycophancy: While models adjust to political personas during role-play, we find this reflects persona-based steerability rather than the increasingly popular, yet contested concept of sycophancy. Our study contributes to evaluating the political alignment of closed-source LLMs that are increasingly embedded in electoral decision support tools and AI Companion chatbots.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IoT-LLM: a framework for enhancing Large Language Model reasoning from real-world sensor data</title>
<link>https://arxiv.org/abs/2410.02429</link>
<guid>https://arxiv.org/abs/2410.02429</guid>
<content:encoded><![CDATA[
arXiv:2410.02429v4 Announce Type: replace-cross 
Abstract: Large Language Models excel in textual tasks but often struggle with physical-world reasoning tasks. Inspired by human cognition, where perception is fundamental to reasoning, we explore augmenting LLMs with enhanced perception abilities using Internet of Things (IoT) data and pertinent knowledge. In this work, we systematically study LLMs' capability to address IoT-sensory tasks by augmenting their perception and knowledge base, and then propose a unified framework, IoT-LLM, to enhance such capability. In IoT-LLM, we customize three steps: preprocessing IoT data into suitable formats, expanding LLMs knowledge via IoT-oriented retrieval-augmented generation and activating LLMs commonsense knowledge through chain-of-thought prompting. We design a benchmark comprising five real-world tasks with varying data types and reasoning complexities to evaluate the performance of IoT-LLM. Experimental results reveal that IoT-LLM significantly improves the performance of IoT-sensory task reasoning of LLMs, with models like GPT-4o-mini showing a 49.4% average improvement over previous methods.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Teaching Language Models to Critique via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2502.03492</link>
<guid>https://arxiv.org/abs/2502.03492</guid>
<content:encoded><![CDATA[
arXiv:2502.03492v2 Announce Type: replace-cross 
Abstract: Teaching large language models (LLMs) to critique and refine their outputs is crucial for building systems that can iteratively improve, yet it is fundamentally limited by the ability to provide accurate judgments and actionable suggestions. In this work, we study LLM critics for code generation and propose $\texttt{CTRL}$, a framework for $\texttt{C}$ritic $\texttt{T}$raining via $\texttt{R}$einforcement $\texttt{L}$earning, which trains a critic model to generate feedback that maximizes correction performance for a fixed generator model without human supervision. Our results demonstrate that critics trained with $\texttt{CTRL}$ significantly enhance pass rates and mitigate compounding errors across both base and stronger generator models. Furthermore, we show that these critic models act as accurate generative reward models and enable test-time scaling through iterative critique-revision, achieving up to 106.1% relative improvements across challenging code generation benchmarks.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Life-Code: Central Dogma Modeling with Multi-Omics Sequence Unification</title>
<link>https://arxiv.org/abs/2502.07299</link>
<guid>https://arxiv.org/abs/2502.07299</guid>
<content:encoded><![CDATA[
arXiv:2502.07299v3 Announce Type: replace-cross 
Abstract: The interactions between DNA, RNA, and proteins are fundamental to biological processes, as illustrated by the central dogma of molecular biology. Although modern biological pre-trained models have achieved great success in analyzing these macromolecules individually, their interconnected nature remains underexplored. This paper follows the guidance of the central dogma to redesign both the data and model pipeline and offers a comprehensive framework, Life-Code, that spans different biological functions. As for data flow, we propose a unified pipeline to integrate multi-omics data by reverse-transcribing RNA and reverse-translating amino acids into nucleotide-based sequences. As for the model, we design a codon tokenizer and a hybrid long-sequence architecture to encode the interactions between coding and non-coding regions through masked modeling pre-training. To model the translation and folding process with coding sequences, Life-Code learns protein structures of the corresponding amino acids by knowledge distillation from off-the-shelf protein language models. Such designs enable Life-Code to capture complex interactions within genetic sequences, providing a more comprehensive understanding of multi-omics with the central dogma. Extensive experiments show that Life-Code achieves state-of-the-art results on various tasks across three omics, highlighting its potential for advancing multi-omics analysis and interpretation.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human Decision-making is Susceptible to AI-driven Manipulation</title>
<link>https://arxiv.org/abs/2502.07663</link>
<guid>https://arxiv.org/abs/2502.07663</guid>
<content:encoded><![CDATA[
arXiv:2502.07663v3 Announce Type: replace-cross 
Abstract: AI systems are increasingly intertwined with daily life, assisting users with various tasks and guiding decision-making. This integration introduces risks of AI-driven manipulation, where such systems may exploit users' cognitive biases and emotional vulnerabilities to steer them toward harmful outcomes. Through a randomized between-subjects experiment with 233 participants, we examined human susceptibility to such manipulation in financial (e.g., purchases) and emotional (e.g., conflict resolution) decision-making contexts. Participants interacted with one of three AI agents: a neutral agent (NA) optimizing for user benefit without explicit influence, a manipulative agent (MA) designed to covertly influence beliefs and behaviors, or a strategy-enhanced manipulative agent (SEMA) equipped with established psychological tactics, allowing it to select and apply them adaptively during interactions to reach its hidden objectives. By analyzing participants' preference ratings, we found significant susceptibility to AI-driven manipulation. Particularly across both decision-making domains, interacting with the manipulative agents significantly increased the odds of rating hidden incentives higher than optimal options (Financial, MA: OR=5.24, SEMA: OR=7.96; Emotional, MA: OR=5.52, SEMA: OR=5.71) compared to the NA group. Notably, we found no clear evidence that employing psychological strategies (SEMA) was overall more effective than simple manipulative objectives (MA) on our primary outcomes. Hence, AI-driven manipulation could become widespread even without requiring sophisticated tactics and expertise. While our findings are preliminary and derived from hypothetical, low-stakes scenarios, we highlight a critical vulnerability in human-AI interactions, emphasizing the need for ethical safeguards and regulatory frameworks to protect human autonomy.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Escaping Collapse: The Strength of Weak Data for Large Language Model Training</title>
<link>https://arxiv.org/abs/2502.08924</link>
<guid>https://arxiv.org/abs/2502.08924</guid>
<content:encoded><![CDATA[
arXiv:2502.08924v2 Announce Type: replace-cross 
Abstract: Synthetically-generated data plays an increasingly larger role in training large language models. However, while synthetic data has been found to be useful, studies have also shown that without proper curation it can cause LLM performance to plateau, or even "collapse", after many training iterations. In this paper, we formalize this question and develop a theoretical framework to investigate how much curation is needed in order to ensure that LLM performance continually improves. Our analysis is inspired by boosting, a classic machine learning technique that leverages a very weak learning algorithm to produce an arbitrarily good classifier. The approach we analyze subsumes many recently proposed methods for training LLMs on synthetic data, and thus our analysis sheds light on why they are successful, and also suggests opportunities for future improvement. We present experiments that validate our theory, and show that dynamically focusing labeling resources on the most challenging examples -- in much the same way that boosting focuses the efforts of the weak learner -- leads to improved performance.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution</title>
<link>https://arxiv.org/abs/2502.18449</link>
<guid>https://arxiv.org/abs/2502.18449</guid>
<content:encoded><![CDATA[
arXiv:2502.18449v2 Announce Type: replace-cross 
Abstract: The recent DeepSeek-R1 release has demonstrated the immense potential of reinforcement learning (RL) in enhancing the general reasoning capabilities of large language models (LLMs). While DeepSeek-R1 and other follow-up work primarily focus on applying RL to competitive coding and math problems, this paper introduces SWE-RL, the first approach to scale RL-based LLM reasoning for real-world software engineering. Leveraging a lightweight rule-based reward (e.g., the similarity score between ground-truth and LLM-generated solutions), SWE-RL enables LLMs to autonomously recover a developer's reasoning processes and solutions by learning from extensive open-source software evolution data -- the record of a software's entire lifecycle, including its code snapshots, code changes, and events such as issues and pull requests. Trained on top of Llama 3, our resulting reasoning model, Llama3-SWE-RL-70B, achieves a 41.0% solve rate on SWE-bench Verified -- a human-verified collection of real-world GitHub issues. To our knowledge, this is the best performance reported for medium-sized (<100B) LLMs to date, even comparable to leading proprietary LLMs like GPT-4o. Surprisingly, despite performing RL solely on software evolution data, Llama3-SWE-RL has even emerged with generalized reasoning skills. For example, it shows improved results on five out-of-domain tasks, namely, function coding, library use, code reasoning, mathematics, and general language understanding, whereas a supervised-finetuning baseline even leads to performance degradation on average. Overall, SWE-RL opens up a new direction to improve the reasoning capabilities of LLMs through reinforcement learning on massive software engineering data.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt-OT: An Optimal Transport Regularization Paradigm for Knowledge Preservation in Vision-Language Model Adaptation</title>
<link>https://arxiv.org/abs/2503.08906</link>
<guid>https://arxiv.org/abs/2503.08906</guid>
<content:encoded><![CDATA[
arXiv:2503.08906v3 Announce Type: replace-cross 
Abstract: Vision-language models (VLMs) such as CLIP demonstrate strong performance but struggle when adapted to downstream tasks. Prompt learning has emerged as an efficient and effective strategy to adapt VLMs while preserving their pre-trained knowledge. However, existing methods still lead to overfitting and degrade zero-shot generalization. To address this challenge, we propose an optimal transport (OT)-guided prompt learning framework that mitigates forgetting by preserving the structural consistency of feature distributions between pre-trained and fine-tuned models. Unlike conventional point-wise constraints, OT naturally captures cross-instance relationships and expands the feasible parameter space for prompt tuning, allowing a better trade-off between adaptation and generalization. Our approach enforces joint constraints on both vision and text representations, ensuring a holistic feature alignment. Extensive experiments on benchmark datasets demonstrate that our simple yet effective method can outperform existing prompt learning strategies in base-to-novel generalization, cross-dataset evaluation, and domain generalization without additional augmentation or ensemble techniques. The code is available at https://github.com/ChongQingNoSubway/Prompt-OT
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Semantic-based Optimization Approach for Repairing LLMs: Case Study on Code Generation</title>
<link>https://arxiv.org/abs/2503.12899</link>
<guid>https://arxiv.org/abs/2503.12899</guid>
<content:encoded><![CDATA[
arXiv:2503.12899v4 Announce Type: replace-cross 
Abstract: Language Models (LMs) are widely used in software engineering for code generation, but they may produce erroneous code. Rather than repairing outputs, a more thorough remedy is to address underlying model failures. LM repair offers a lightweight solution: it requires minimal data, lowers computational cost, and limits side effects. Unlike full retraining, LM repair focuses on applying tailored updates to targeted neurons, making it suitable for limited resources, high-performance demands, or strict safety requirements. In this paper, we propose Semantic Targeting for Analytical Repair (STAR), a novel semantic-based optimization method for repairing LLMs. STAR realizes the main operations of repairing LMs in an optimization process, including locating ``buggy neurons'', solving ``neuron patches'', and patching ``buggy neurons''. The neuron patches are computed with a solid semantic-based analytical formula, which directly bridges the changes to logits with the deltas of neurons, by steering latent representations. Compared to the prior work of LM repair (MINT) and standard optimization methods (SGD), STAR integrates their strengths while mitigating their limitations. By reformulating LM repair as an optimization process, STAR may solve multiple failures together, significantly improving the usefulness. Evaluated on coding tasks using popular code LMs, STAR demonstrates superior effectiveness compared with the state-of-the-art. Besides, STAR exhibits better efficiency. In terms of side effects, namely the balance between generalization and specificity, STAR outperforms prior work by a significant margin. Additionally, we conducted assessments on the overfitting risk of LM repair as well as the cumulative impact. Further, we analyzed the differences with pipeline-based methods and explained the reason why STAR is better and how it mitigated the common limitations of LM repair.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RealWebAssist: A Benchmark for Long-Horizon Web Assistance with Real-World Users</title>
<link>https://arxiv.org/abs/2504.10445</link>
<guid>https://arxiv.org/abs/2504.10445</guid>
<content:encoded><![CDATA[
arXiv:2504.10445v2 Announce Type: replace-cross 
Abstract: To achieve successful assistance with long-horizon web-based tasks, AI agents must be able to sequentially follow real-world user instructions over a long period. Unlike existing web-based agent benchmarks, sequential instruction following in the real world poses significant challenges beyond performing a single, clearly defined task. For instance, real-world human instructions can be ambiguous, require different levels of AI assistance, and may evolve over time, reflecting changes in the user's mental state. To address this gap, we introduce RealWebAssist, a novel benchmark designed to evaluate sequential instruction-following in realistic scenarios involving long-horizon interactions with the web, visual GUI grounding, and understanding ambiguous real-world user instructions. RealWebAssist includes a dataset of sequential instructions collected from real-world human users. Each user instructs a web-based assistant to perform a series of tasks on multiple websites. A successful agent must reason about the true intent behind each instruction, keep track of the mental state of the user, understand user-specific routines, and ground the intended tasks to actions on the correct GUI elements. Our experimental results show that state-of-the-art models struggle to understand and ground user instructions, posing critical challenges in following real-world user instructions for long-horizon web assistance.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Superposition Yields Robust Neural Scaling</title>
<link>https://arxiv.org/abs/2505.10465</link>
<guid>https://arxiv.org/abs/2505.10465</guid>
<content:encoded><![CDATA[
arXiv:2505.10465v4 Announce Type: replace-cross 
Abstract: The success of today's large language models (LLMs) depends on the observation that larger models perform better. However, the origin of this neural scaling law, that loss decreases as a power law with model size, remains unclear. We propose that representation superposition, meaning that LLMs represent more features than they have dimensions, can be a key contributor to loss and cause neural scaling. Based on Anthropic's toy model, we use weight decay to control the degree of superposition, allowing us to systematically study how loss scales with model size. When superposition is weak, the loss follows a power law only if data feature frequencies are power-law distributed. In contrast, under strong superposition, the loss generically scales inversely with model dimension across a broad class of frequency distributions, due to geometric overlaps between representation vectors. We confirmed that open-sourced LLMs operate in the strong superposition regime and have loss scaling inversely with model dimension, and that the Chinchilla scaling laws are also consistent with this behavior. Our results identify representation superposition as a central driver of neural scaling laws, providing insights into questions like when neural scaling laws can be improved and when they will break down.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InvisibleInk: High-Utility and Low-Cost Text Generation with Differential Privacy</title>
<link>https://arxiv.org/abs/2507.02974</link>
<guid>https://arxiv.org/abs/2507.02974</guid>
<content:encoded><![CDATA[
arXiv:2507.02974v2 Announce Type: replace-cross 
Abstract: As major progress in LLM-based long-form text generation enables paradigms such as retrieval-augmented generation (RAG) and inference-time scaling, safely incorporating private information into the generation remains a critical open question. We present InvisibleInk, a highly scalable long-form text generation framework satisfying rigorous differential privacy guarantees with respect to the sensitive reference texts. It interprets sampling from the LLM's next-token-distribution as the exponential mechanism over the LLM logits with two innovations. First, we reduce the privacy cost by isolating and clipping only the sensitive information in the model logits (relative to the public logits). Second, we improve text quality by sampling without any privacy cost from a small superset of the top-$k$ private tokens. Empirical evaluations demonstrate a consistent $8\times$ (or more) reduction in computation cost over state-of-the-art baselines to generate long-form private text of the same utility across privacy levels. InvisibleInk is able to generate, for the first time, high-quality private long-form text at less than $4$-$8\times$ times the computation cost of non-private generation, paving the way for its practical use. We open-source a pip-installable Python package (invink) for InvisibleInk at https://github.com/cerai-iitm/invisibleink.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MERA Code: A Unified Framework for Evaluating Code Generation Across Tasks</title>
<link>https://arxiv.org/abs/2507.12284</link>
<guid>https://arxiv.org/abs/2507.12284</guid>
<content:encoded><![CDATA[
arXiv:2507.12284v3 Announce Type: replace-cross 
Abstract: Advancements in LLMs have enhanced task automation in software engineering; however, current evaluations primarily focus on natural language tasks, overlooking code quality. Most benchmarks prioritize high-level reasoning over executable code and real-world performance, leaving gaps in understanding true capabilities and risks associated with these models in production. To address this issue, we propose MERA Code, a new addition to the MERA benchmark family, specifically focused on evaluating code for the latest code generation LLMs in Russian. This benchmark includes 11 evaluation tasks that span 8 programming languages. Our proposed evaluation methodology features a taxonomy that outlines the practical coding skills necessary for models to complete these tasks. The benchmark comprises an open-source codebase for users to conduct MERA assessments, a scoring system compatible with various programming environments, and a platform featuring a leaderboard and submission system. We evaluate open LLMs and frontier API models, analyzing their limitations in terms of practical coding tasks in non-English languages. We are publicly releasing MERA to guide future research, anticipate groundbreaking features in model development, and standardize evaluation procedures.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpikingBrain: Spiking Brain-inspired Large Models</title>
<link>https://arxiv.org/abs/2509.05276</link>
<guid>https://arxiv.org/abs/2509.05276</guid>
<content:encoded><![CDATA[
arXiv:2509.05276v3 Announce Type: replace-cross 
Abstract: Mainstream Transformer-based large language models face major efficiency bottlenecks: training computation scales quadratically with sequence length, and inference memory grows linearly, limiting long-context processing. Building large models on non-NVIDIA platforms also poses challenges for stable and efficient training. To address this, we introduce SpikingBrain, a family of brain-inspired models designed for efficient long-context training and inference. SpikingBrain leverages the MetaX GPU cluster and focuses on three aspects: (1) Model Architecture: linear and hybrid-linear attention architectures with adaptive spiking neurons; (2) Algorithmic Optimizations: an efficient, conversion-based training pipeline and a dedicated spike coding framework; (3) System Engineering: customized training frameworks, operator libraries, and parallelism strategies tailored to MetaX hardware.
  Using these techniques, we develop two models: SpikingBrain-7B, a linear LLM, and SpikingBrain-76B, a hybrid-linear MoE LLM. These models demonstrate the feasibility of large-scale LLM development on non-NVIDIA platforms, and training remains stable for weeks on hundreds of MetaX GPUs with Model FLOPs Utilization at expected levels. SpikingBrain achieves performance comparable to open-source Transformer baselines while using only about 150B tokens for continual pre-training. Our models also significantly improve long-context efficiency and deliver inference with (partially) constant memory and event-driven spiking behavior. For example, SpikingBrain-7B attains over 100x speedup in Time to First Token for 4M-token sequences. Furthermore, the proposed spiking scheme achieves 69.15 percent sparsity, enabling low-power operation. Overall, this work demonstrates the potential of brain-inspired mechanisms to drive the next generation of efficient and scalable large model design.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Event2Vec: A Geometric Approach to Learning Composable Representations of Event Sequences</title>
<link>https://arxiv.org/abs/2509.12188</link>
<guid>https://arxiv.org/abs/2509.12188</guid>
<content:encoded><![CDATA[
arXiv:2509.12188v2 Announce Type: replace-cross 
Abstract: The study of neural representations, both in biological and artificial systems, is increasingly revealing the importance of geometric and topological structures. Inspired by this, we introduce Event2Vec, a novel framework for learning representations of discrete event sequences. Our model leverages a simple, additive recurrent structure to learn composable, interpretable embeddings. We provide a theoretical analysis demonstrating that, under specific training objectives, our model's learned representations in a Euclidean space converge to an ideal additive structure. This ensures that the representation of a sequence is the vector sum of its constituent events, a property we term the linear additive hypothesis. To address the limitations of Euclidean geometry for hierarchical data, we also introduce a variant of our model in hyperbolic space, which is naturally suited to embedding tree-like structures with low distortion. We present experiments to validate our hypothesis. Quantitative evaluation on the Brown Corpus yields a Silhouette score of 0.0564, outperforming a Word2Vec baseline (0.0215), demonstrating the model's ability to capture structural dependencies without supervision.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Margin RLHF via Preference over Preferences</title>
<link>https://arxiv.org/abs/2509.22851</link>
<guid>https://arxiv.org/abs/2509.22851</guid>
<content:encoded><![CDATA[
arXiv:2509.22851v3 Announce Type: replace-cross 
Abstract: Margin-based optimization is fundamental to improving generalization and robustness in classification tasks. In the context of reward model learning from preferences within Reinforcement Learning from Human Feedback (RLHF), existing methods typically rely on no margins, fixed margins, or margins that are simplistic functions of preference ratings. However, such formulations often fail to account for the varying strengths of different preferences, for example some preferences are associated with larger margins between responses, or they rely on noisy margin information derived from ratings. We argue that modeling the strength of preferences can lead to better generalization and more faithful alignment. Furthermore, many existing methods that use adaptive margins assume access to accurate preference scores, which can be difficult for humans to provide reliably. We propose an approach that leverages preferences over preferences, that is annotations indicating which of two preferences reflects a stronger distinction. We use this ordinal signal to infer adaptive margins on a per-datapoint basis. We introduce an extension to Direct Preference Optimization (DPO), DPO-PoP, that incorporates adaptive margins from preference-over-preference supervision, enabling improved discriminative and generative performance. Empirically, our method outperforms vanilla DPO, DPO with fixed margins, and DPO with ground-truth margins on the UltraFeedback dataset. Additionally, we show that there is a tradeoff between discriminative and generative performance: improving test classification accuracy, particularly by correctly labeling weaker preferences at the expense of stronger ones, can lead to a decline in generative quality. To navigate this tradeoff, we propose two sampling strategies to gather preference-over-preference labels: one favoring discriminative performance and one favoring generative performance.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The AI Productivity Index (APEX)</title>
<link>https://arxiv.org/abs/2509.25721</link>
<guid>https://arxiv.org/abs/2509.25721</guid>
<content:encoded><![CDATA[
arXiv:2509.25721v3 Announce Type: replace-cross 
Abstract: We present an extended version of the AI Productivity Index (APEX-v1-extended), a benchmark for assessing whether frontier models are capable of performing economically valuable tasks in four jobs: investment banking associate, management consultant, big law associate, and primary care physician (MD). This technical report details the extensions to APEX-v1, including an increase in the held-out evaluation set from n = 50 to n = 100 cases per job (n = 400 total) and updates to the grading methodology. We present a new leaderboard, where GPT5 (Thinking = High) remains the top performing model with a score of 67.0%. APEX-v1-extended shows that frontier models still have substantial limitations when performing typical professional tasks. To support further research, we are open sourcing n = 25 non-benchmark example cases per role (n = 100 total) along with our evaluation harness.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing OCR for Sino-Vietnamese Language Processing via Fine-tuned PaddleOCRv5</title>
<link>https://arxiv.org/abs/2510.04003</link>
<guid>https://arxiv.org/abs/2510.04003</guid>
<content:encoded><![CDATA[
arXiv:2510.04003v2 Announce Type: replace-cross 
Abstract: Recognizing and processing Classical Chinese (Han-Nom) texts play a vital role in digitizing Vietnamese historical documents and enabling cross-lingual semantic research. However, existing OCR systems struggle with degraded scans, non-standard glyphs, and handwriting variations common in ancient sources. In this work, we propose a fine-tuning approach for PaddleOCRv5 to improve character recognition on Han-Nom texts. We retrain the text recognition module using a curated subset of ancient Vietnamese Chinese manuscripts, supported by a full training pipeline covering preprocessing, LMDB conversion, evaluation, and visualization. Experimental results show a significant improvement over the base model, with exact accuracy increasing from 37.5 percent to 50.0 percent, particularly under noisy image conditions. Furthermore, we develop an interactive demo that visually compares pre- and post-fine-tuning recognition results, facilitating downstream applications such as Han-Vietnamese semantic alignment, machine translation, and historical linguistics research. The demo is available at https://huggingface.co/spaces/MinhDS/Fine-tuned-PaddleOCRv5
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Influence Functions for Efficient Data Selection in Reasoning</title>
<link>https://arxiv.org/abs/2510.06108</link>
<guid>https://arxiv.org/abs/2510.06108</guid>
<content:encoded><![CDATA[
arXiv:2510.06108v2 Announce Type: replace-cross 
Abstract: Fine-tuning large language models (LLMs) on chain-of-thought (CoT) data shows that a small amount of high-quality data can outperform massive datasets. Yet, what constitutes "quality" remains ill-defined. Existing reasoning methods rely on indirect heuristics such as problem difficulty or trace length, while instruction-tuning has explored a broader range of automated selection strategies, but rarely in the context of reasoning. We propose to define reasoning data quality using influence functions, which measure the causal effect of individual CoT examples on downstream accuracy, and introduce influence-based pruning, which consistently outperforms perplexity and embedding-based baselines on math reasoning within a model family.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs can hide text in other text of the same length</title>
<link>https://arxiv.org/abs/2510.20075</link>
<guid>https://arxiv.org/abs/2510.20075</guid>
<content:encoded><![CDATA[
arXiv:2510.20075v4 Announce Type: replace-cross 
Abstract: A meaningful text can be hidden inside another, completely different yet still coherent and plausible, text of the same length. For example, a tweet containing a harsh political critique could be embedded in a tweet that celebrates the same political leader, or an ordinary product review could conceal a secret manuscript. This uncanny state of affairs is now possible thanks to Large Language Models, and in this paper we present Calgacus, a simple and efficient protocol to achieve it. We show that even modest 8-billion-parameter open-source LLMs are sufficient to obtain high-quality results, and a message as long as this abstract can be encoded and decoded locally on a laptop in seconds. The existence of such a protocol demonstrates a radical decoupling of text from authorial intent, further eroding trust in written communication, already shaken by the rise of LLM chatbots. We illustrate this with a concrete scenario: a company could covertly deploy an unfiltered LLM by encoding its answers within the compliant responses of a safe model. This possibility raises urgent questions for AI safety and challenges our understanding of what it means for a Large Language Model to know something.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculation</title>
<link>https://arxiv.org/abs/2510.20812</link>
<guid>https://arxiv.org/abs/2510.20812</guid>
<content:encoded><![CDATA[
arXiv:2510.20812v2 Announce Type: replace-cross 
Abstract: Large Vision-Language Models (VLMs) have achieved remarkable progress in multimodal understanding, yet they struggle when reasoning over information-intensive images that densely interleave textual annotations with fine-grained graphical elements. The main challenges lie in precisely localizing critical cues in dense layouts and multi-hop reasoning to integrate dispersed evidence. We propose Speculative Verdict (SV), a training-free framework inspired by speculative decoding that combines multiple lightweight draft experts with a large verdict model. In the draft stage, small VLMs act as draft experts to generate reasoning paths that provide diverse localization candidates; in the verdict stage, a strong VLM synthesizes these paths to produce the final answer, minimizing computational cost while recovering correct answers. To further improve efficiency and accuracy, SV introduces a consensus expert selection mechanism that forwards only high-agreement reasoning paths to the verdict. Empirically, SV achieves consistent gains on challenging information-intensive and high-resolution visual question answering benchmarks, including InfographicVQA, ChartMuseum, ChartQAPro, and HR-Bench 4K. By synthesizing correct insights from multiple partially accurate reasoning paths, SV achieves both error correction and cost-efficiency compared to large proprietary models or training pipelines. Code is available at https://github.com/Tinaliu0123/speculative-verdict.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Combining Textual and Structural Information for Premise Selection in Lean</title>
<link>https://arxiv.org/abs/2510.23637</link>
<guid>https://arxiv.org/abs/2510.23637</guid>
<content:encoded><![CDATA[
arXiv:2510.23637v2 Announce Type: replace-cross 
Abstract: Premise selection is a key bottleneck for scaling theorem proving in large formal libraries. Yet existing language-based methods often treat premises in isolation, ignoring the web of dependencies that connects them. We present a graph-augmented approach that combines dense text embeddings of Lean formalizations with graph neural networks over a heterogeneous dependency graph capturing both state-premise and premise-premise relations. On the LeanDojo Benchmark, our method outperforms the ReProver language-based baseline by over 25\% across standard retrieval metrics. These results suggest that relational information is beneficial for premise selection.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring Chain-of-Thought Monitorability Through Faithfulness and Verbosity</title>
<link>https://arxiv.org/abs/2510.27378</link>
<guid>https://arxiv.org/abs/2510.27378</guid>
<content:encoded><![CDATA[
arXiv:2510.27378v2 Announce Type: replace-cross 
Abstract: Chain-of-thought (CoT) outputs let us read a model's step-by-step reasoning. Since any long, serial reasoning process must pass through this textual trace, the quality of the CoT is a direct window into what the model is thinking. This visibility could help us spot unsafe or misaligned behavior (monitorability), but only if the CoT is transparent about its internal reasoning (faithfulness). Fully measuring faithfulness is difficult, so researchers often focus on examining the CoT in cases where the model changes its answer after adding a cue to the input. This proxy finds some instances of unfaithfulness but loses information when the model maintains its answer, and does not investigate aspects of reasoning not tied to the cue. We extend these results to a more holistic sense of monitorability by introducing verbosity: whether the CoT lists every factor needed to solve the task. We combine faithfulness and verbosity into a single monitorability score that shows how well the CoT serves as the model's external `working memory', a property that many safety schemes based on CoT monitoring depend on. We evaluate instruction-tuned and reasoning models on BBH, GPQA, and MMLU. Our results show that models can appear faithful yet remain hard to monitor when they leave out key factors, and that monitorability differs sharply across model families. We release our evaluation code using the Inspect library to support reproducible future work.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpeechJudge: Towards Human-Level Judgment for Speech Naturalness</title>
<link>https://arxiv.org/abs/2511.07931</link>
<guid>https://arxiv.org/abs/2511.07931</guid>
<content:encoded><![CDATA[
arXiv:2511.07931v2 Announce Type: replace-cross 
Abstract: Aligning large generative models with human feedback is a critical challenge. In speech synthesis, this is particularly pronounced due to the lack of a large-scale human preference dataset, which hinders the development of models that truly align with human perception. To address this, we introduce SpeechJudge, a comprehensive suite comprising a dataset, a benchmark, and a reward model centered on naturalness--one of the most fundamental subjective metrics for speech synthesis. First, we present SpeechJudge-Data, a large-scale human feedback corpus of 99K speech pairs. The dataset is constructed using a diverse set of advanced zero-shot text-to-speech (TTS) models across diverse speech styles and multiple languages, with human annotations for both intelligibility and naturalness preference. From this, we establish SpeechJudge-Eval, a challenging benchmark for speech naturalness judgment. Our evaluation reveals that existing metrics and AudioLLMs struggle with this task; the leading model, Gemini-2.5-Flash, achieves less than 70% agreement with human judgment, highlighting a significant gap for improvement. To bridge this gap, we develop SpeechJudge-GRM, a generative reward model (GRM) based on Qwen2.5-Omni-7B. It is trained on SpeechJudge-Data via a two-stage post-training process: Supervised Fine-Tuning (SFT) with Chain-of-Thought rationales followed by Reinforcement Learning (RL) with GRPO on challenging cases. On the SpeechJudge-Eval benchmark, the proposed SpeechJudge-GRM demonstrates superior performance, achieving 77.2% accuracy (and 79.4% after inference-time scaling @10) compared to a classic Bradley-Terry reward model (72.7%). Furthermore, SpeechJudge-GRM can be also employed as a reward function during the post-training of speech generation models to facilitate their alignment with human preferences.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training</title>
<link>https://arxiv.org/abs/2510.15859</link>
<guid>https://arxiv.org/abs/2510.15859</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Medical Dialogue, Rubric-Based Feedback, Large Language Models, Instruction Following<br /><br />Summary:<br /><br />1. The paper addresses the limitations of reinforcement learning (RL) in open-ended and high-stakes domains such as medical consultations, where feedback is ambiguous, context-dependent, and cannot be reduced to simple scalar rewards. 2. To overcome these challenges, the authors introduce ORBIT, an open-ended rubric-based incremental training framework specifically designed for high-stakes medical dialogue, combining synthetic dialogue generation with dynamically constructed rubrics as adaptive guides for incremental RL. 3. ORBIT eliminates the need for external medical knowledge bases or handcrafted rules by using rubric-driven feedback, and its judge component can be instantiated with general-purpose instruction-following large language models (LLMs) without requiring task-specific fine-tuning. 4. When applied to the Qwen3-4B-Instruct model using just 2,000 training samples, ORBIT significantly improves the HealthBench-Hard score from 7.0 to 27.5, achieving state-of-the-art performance among models at this scale, with larger rubric datasets enabling further competitive results against leading open-source baselines. 5. The approach not only enhances medical consultation quality across diverse scenarios but also generalizes to other domains, demonstrated by improved instruction-following on the InfoBench dataset, highlighting the robustness and adaptability of rubric-based feedback for training LLMs. <div>
arXiv:2510.15859v3 Announce Type: replace 
Abstract: Reinforcement learning has powered many of the recent breakthroughs in large language models, especially for tasks where rewards can be computed automatically, such as code generation. However, these methods deteriorate in open-ended domains like medical consultation, where feedback is inherently ambiguous, highly context-dependent, and cannot be reduced to a reliable scalar signal. In such settings, RL must either rely on supervision-intensive reward models that often fail to generalize, or it falls into pathological behaviors such as reward hacking - an especially troubling risk for high-stakes medical dialogue. To address these limitations, we introduce ORBIT, an open-ended rubric-based incremental training framework for high-stakes medical dialogue. ORBIT integrates synthetic dialogue generation with dynamically constructed rubrics that serve as adaptive guides for incremental RL. Instead of relying on external medical knowledge bases or handcrafted rule sets, ORBIT uses rubric-driven feedback to steer the learning process. Its judge component can be instantiated with general-purpose instruction-following LLMs, removing the need for any task-specific fine-tuning. Applied to the Qwen3-4B-Instruct model, ORBIT raises the HealthBench-Hard score from 7.0 to 27.5 using only 2k training samples, achieving SOTA performance for models at this scale. With larger rubric datasets, ORBIT-trained models further compete with the strongest open-source baselines on HealthBench-Hard. Our analysis shows that rubric-guided RL consistently improves consultation quality across diverse medical scenarios. We also apply such rubric generation and training pipeline to InfoBench, where ORBIT enhances instruction-following performance, highlighting the generality of rubric-based feedback.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continual Learning of Domain Knowledge from Human Feedback in Text-to-SQL</title>
<link>https://arxiv.org/abs/2511.10674</link>
<guid>https://arxiv.org/abs/2511.10674</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, continual learning, human feedback, text-to-SQL, structured memory<br /><br />Summary:<br /><br />1. The paper addresses the challenge Large Language Models (LLMs) face in generating accurate SQL queries due to complexities in database-specific schemas and tacit domain knowledge.  
2. It introduces a novel framework for continual learning where a text-to-SQL agent improves iteratively by receiving natural language feedback from humans, using it to refine generated queries.  
3. The learning agent distills the acquired knowledge into a structured memory, which supports knowledge reuse in future query tasks, enhancing overall query accuracy.  
4. Multiple agent architectures are designed and tested, differing in how they capture, store, and retrieve past learning experiences, demonstrating the importance of memory augmentation.  
5. Experimental results on the BIRD benchmark show that memory-augmented agents, specifically the Procedural Agent, achieve notable gains in execution accuracy and reduced errors by leveraging human-in-the-loop feedback, underscoring the effectiveness of transforming tacit human expertise into reusable knowledge for adaptive and domain-aware text-to-SQL systems. <div>
arXiv:2511.10674v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) can generate SQL queries from natural language questions but struggle with database-specific schemas and tacit domain knowledge. We introduce a framework for continual learning from human feedback in text-to-SQL, where a learning agent receives natural language feedback to refine queries and distills the revealed knowledge for reuse on future tasks. This distilled knowledge is stored in a structured memory, enabling the agent to improve execution accuracy over time. We design and evaluate multiple variations of a learning agent architecture that vary in how they capture and retrieve past experiences. Experiments on the BIRD benchmark Dev set show that memory-augmented agents, particularly the Procedural Agent, achieve significant accuracy gains and error reduction by leveraging human-in-the-loop feedback. Our results highlight the importance of transforming tacit human expertise into reusable knowledge, paving the way for more adaptive, domain-aware text-to-SQL systems that continually learn from a human-in-the-loop.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EvalCards: A Framework for Standardized Evaluation Reporting</title>
<link>https://arxiv.org/abs/2511.21695</link>
<guid>https://arxiv.org/abs/2511.21695</guid>
<content:encoded><![CDATA[
<div> Evaluation, NLP, transparency, reproducibility, governance<br /><br />Summary:<br /><br />1. Evaluation remains a fundamental concern in the field of Natural Language Processing (NLP), especially as the pace of releasing open-access models accelerates. 2. Transparent reporting practices are increasingly important to ensure clarity and reliability in these evaluations. 3. The authors highlight three key shortcomings that persist in current evaluation reporting: reproducibility (ensuring results can be replicated), accessibility (making evaluation details readily available), and governance (adhering to ethical and regulatory standards). 4. Despite ongoing efforts toward standardization, current approaches fall short of adequately addressing these challenges. 5. To tackle these issues, the paper introduces Evaluation Disclosure Cards (EvalCards), a new tool designed to improve transparency for both researchers and practitioners. EvalCards aim to provide a practical mechanism that supports reproducibility, enhances accessibility of evaluation information, and helps meet emerging governance requirements, thereby fostering more trustworthy and accountable NLP research and development. <div>
arXiv:2511.21695v1 Announce Type: new 
Abstract: Evaluation has long been a central concern in NLP, and transparent reporting practices are more critical than ever in today's landscape of rapidly released open-access models. Drawing on a survey of recent work on evaluation and documentation, we identify three persistent shortcomings in current reporting practices: reproducibility, accessibility, and governance. We argue that existing standardization efforts remain insufficient and introduce Evaluation Disclosure Cards (EvalCards) as a path forward. EvalCards are designed to enhance transparency for both researchers and practitioners while providing a practical foundation to meet emerging governance requirements.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cacheback: Speculative Decoding With Nothing But Cache</title>
<link>https://arxiv.org/abs/2511.21699</link>
<guid>https://arxiv.org/abs/2511.21699</guid>
<content:encoded><![CDATA[
<div> Keywords: Cacheback Decoding, speculative decoding, Large Language Models, LRU cache, inference acceleration<br /><br />Summary: Cacheback Decoding is a novel method designed to speed up the inference process for Large Language Models (LLMs) without needing additional training or model modifications. It utilizes Least Recently Used (LRU) cache tables containing token n-grams to generate draft sequences, capitalizing on the locality properties inherent in language. Despite its minimalist approach, Cacheback achieves state-of-the-art performance when compared to other speculative decoding techniques. The simplicity of the method enables easy integration into existing LLM inference pipelines, minimizing disruption and overhead. Additionally, Cacheback demonstrates promising adaptability to new domains due to its reliance on dynamic caching rather than pre-trained knowledge or extensive tuning. Overall, Cacheback Decoding offers a practical, efficient, and versatile solution for accelerating LLM inference while maintaining high-quality output generation. <div>
arXiv:2511.21699v1 Announce Type: new 
Abstract: We present Cacheback Decoding, a training-free and model-agnostic speculative decoding method that exploits the locality in language to accelerate Large Language Model (LLM) inference. Cacheback leverages only Least Recently Used (LRU) cache tables of token n-grams to generate draft sequences. Cacheback achieves state-of-the-art performance among comparable methods despite its minimalist design, and its simplicity allows easy integration into existing systems. Cacheback also shows potential for fast adaptation to new domains.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JELV: A Judge of Edit-Level Validity for Evaluation and Automated Reference Expansion in Grammatical Error Correction</title>
<link>https://arxiv.org/abs/2511.21700</link>
<guid>https://arxiv.org/abs/2511.21700</guid>
<content:encoded><![CDATA[
<div> Keywords: Grammatical Error Correction, evaluation metric, LLM judges, DeBERTa classifier, reference diversity<br /><br />Summary: Existing Grammatical Error Correction (GEC) systems often suffer due to limited diversity in correction references, leading to underestimation during evaluation and poor model generalization. To tackle this, the authors introduce the Judge of Edit-Level Validity (JELV), an automated framework designed to validate correction edits by assessing grammaticality, faithfulness, and fluency. The framework is benchmarked using a newly created human-annotated Pair-wise Edit-level Validity Dataset (PEVData). JELV has two key implementations: a multi-turn large language model (LLM)-based judging pipeline that achieves 90% agreement with human annotators, and a distilled DeBERTa classifier offering 85% precision in identifying valid edits. The authors use JELV to reclassify misjudged false positives during GEC evaluation and develop a comprehensive evaluation metric that integrates false positive decoupling alongside fluency scoring, yielding state-of-the-art correlation with human judgments. Additionally, JELV filters LLM-generated correction hypotheses to expand the BEA19 dataset, originally containing 38,692 sentences, thereby increasing reference diversity. Retraining leading GEC models on this augmented dataset results in noticeable performance improvements. Overall, JELV provides a scalable, effective solution to enhance reference diversity, improve evaluation fidelity, and strengthen model generalization in Grammatical Error Correction tasks. <div>
arXiv:2511.21700v1 Announce Type: new 
Abstract: Existing Grammatical Error Correction (GEC) systems suffer from limited reference diversity, leading to underestimated evaluation and restricted model generalization. To address this issue, we introduce the Judge of Edit-Level Validity (JELV), an automated framework to validate correction edits from grammaticality, faithfulness, and fluency. Using our proposed human-annotated Pair-wise Edit-level Validity Dataset (PEVData) as benchmark, JELV offers two implementations: a multi-turn LLM-as-Judges pipeline achieving 90% agreement with human annotators, and a distilled DeBERTa classifier with 85% precision on valid edits. We then apply JELV to reclassify misjudged false positives in evaluation and derive a comprehensive evaluation metric by integrating false positive decoupling and fluency scoring, resulting in state-of-the-art correlation with human judgments. We also apply JELV to filter LLM-generated correction candidates, expanding the BEA19's single-reference dataset containing 38,692 source sentences. Retraining top GEC systems on this expanded dataset yields measurable performance gains. JELV provides a scalable solution for enhancing reference diversity and strengthening both evaluation and model generalization.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>47B Mixture-of-Experts Beats 671B Dense Models on Chinese Medical Examinations</title>
<link>https://arxiv.org/abs/2511.21701</link>
<guid>https://arxiv.org/abs/2511.21701</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, medical examination, Chinese medicine, benchmark evaluation, model performance<br /><br />Summary:<br /><br />The paper evaluates 27 state-of-the-art large language models (LLMs) on Chinese medical examination questions, covering seven specialties: cardiovascular, gastroenterology, hematology, infectious diseases, nephrology, neurology, and respiratory medicine. The study uses a dataset of 2,800 carefully curated questions divided into two professional levels: attending physician and senior physician, allowing nuanced analysis of model performance across varying difficulties. Results show substantial variation in accuracy among the models, with Mixtral-8x7B achieving the highest overall accuracy of 74.25%, followed by DeepSeek-R1-671B at 64.07%. Interestingly, no consistent link was found between model size and performance, as smaller mixture-of-experts architectures performed strongly. Performance differences also emerged across medical specialties, with models doing better on cardiovascular and neurology topics relative to gastroenterology and nephrology. Additionally, there was minimal performance drop between attending and senior physician levels for top models, indicating robust generalization across difficulty levels. The benchmark offers important insights for deploying LLMs in medical education and clinical decision support, highlighting both the promise of these models in specialized fields and their current limitations. <div>
arXiv:2511.21701v1 Announce Type: new 
Abstract: The rapid advancement of large language models(LLMs) has prompted significant interest in their potential applications in medical domains. This paper presents a comprehensive benchmark evaluation of 27 state-of-the-art LLMs on Chinese medical examination questions, encompassing seven medical specialties across two professional levels. We introduce a robust evaluation framework that assesses model performance on 2,800 carefully curated questions from cardiovascular, gastroenterology, hematology, infectious diseases, nephrology, neurology, and respiratory medicine domains. Our dataset distinguishes between attending physician and senior physician difficulty levels, providing nuanced insights into model capabilities across varying complexity. Our empirical analysis reveals substantial performance variations among models, with Mixtral-8x7B achieving the highest overall accuracy of 74.25%, followed by DeepSeek-R1-671B at 64.07%. Notably, we observe no consistent correlation between model size and performance, as evidenced by the strong performance of smaller mixture-of-experts architectures. The evaluation demonstrates significant performance gaps between medical specialties, with models generally performing better on cardiovascular and neurology questions compared to gastroenterology and nephrology domains. Furthermore, our analysis indicates minimal performance degradation between attending and senior physician levels for top-performing models, suggesting robust generalization capabilities. This benchmark provides critical insights for the deployment of LLMs in medical education and clinical decision support systems, highlighting both the promise and current limitations of these technologies in specialized medical contexts.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CSV-Decode: Certifiable Sub-Vocabulary Decoding for Efficient Large Language Model Inference</title>
<link>https://arxiv.org/abs/2511.21702</link>
<guid>https://arxiv.org/abs/2511.21702</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, decoding efficiency, sparse computation, geometric upper bounds, softmax approximation<br /><br />Summary:<br /><br />This paper addresses the computational challenges faced by large language models (LLMs) during inference, particularly the high cost associated with output layer computations over massive vocabularies. The authors introduce CSV-Decode, an innovative technique that dynamically constructs small sub-vocabularies at each decoding step by leveraging geometric upper bounds, which allow for more efficient sparse computations. CSV-Decode ensures dual correctness guarantees: it provides exact top-$k$ certification, assuring that the most probable tokens are included, and $\varepsilon$-certified softmax approximations that maintain the probabilistic distribution quality. The approach uses an offline clustering of vocabulary embeddings and employs centroid-plus-radius bounds to safely exclude tokens from computations without sacrificing accuracy. A full system implementation accompanies the method, featuring optimized sparse General Matrix-Vector multiplication (GEMV) kernels, support for multi-GPU sharding, and CUDA Graph optimizations for effective hardware utilization. Experimental evaluations demonstrate that CSV-Decode achieves significant decoding speedups compared to full vocabulary decoding methods, with minimal impact on output distribution quality and a low rate of fallback to full computations. The authors also provide the source code publicly at their GitHub repository, facilitating further research and adoption. <div>
arXiv:2511.21702v1 Announce Type: new 
Abstract: Large language models face significant computational bottlenecks during inference due to the expensive output layer computation over large vocabularies. We present CSV-Decode, a novel approach that uses geometric upper bounds to construct small sub-vocabularies for each decoding step, enabling efficient sparse computation while maintaining dual correctness guarantees: exact top-$k$ certification and $\varepsilon$-certified softmax approximations. Our method clusters vocabulary embeddings offline and uses centroid-plus-radius bounds to identify which tokens can be safely omitted from computation. We provide a complete system implementation with sparse GEMV kernels, multi-GPU sharding, and CUDA Graph optimization. Experimental results demonstrate significant speedup over full vocabulary decoding while maintaining distributional guarantees and low fallback rates. Our code implementation available at \href{https://github.com/FastLM/CSV-Decode}{https://github.com/FastLM/CSV-Decode}.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Embedding Generalization: How LLMs, LoRA, and SLERP Shape Representational Geometry</title>
<link>https://arxiv.org/abs/2511.21703</link>
<guid>https://arxiv.org/abs/2511.21703</guid>
<content:encoded><![CDATA[
<div> Keywords: dense text embeddings, large language models, LoRA, SLERP merging, model generalization  

<br /><br />Summary:  
This paper investigates the generalization capabilities of dense text embeddings derived from large language model (LLM) backbones versus non-LLM encoders. The authors set up a domain-agnostic experiment involving the embedding of short numerical sequences, assessing models on their capacity to cluster and classify based on number-theoretic properties. Four model families are compared: (1) non-LLM encoders either trained from scratch or fine-tuned for embeddings, (2) LLM-based encoders adapted with LoRA, a parameter-efficient tuning method, (3) LoRA-adapted LLMs merged back into the base model via model souping, and (4) LoRA-adapted LLMs merged using spherical linear interpolation (SLERP) across checkpoints or adaptation stages. Representational quality is quantitatively measured via clustering indices such as Silhouette and Davies-Bouldin scores. Additional analysis using k-means labels probes whether embeddings contain extraneous or additional encoded information. The findings reveal that LLM-based embeddings better capture higher-order, compositional numeric patterns but suffer from adapter dominance, which impairs generalization balance. Notably, SLERP merging effectively restores base-model structural properties while preserving task-specific improvements, thereby offering improved tradeoffs in clustering separability and robustness compared to model souping or unmerged adapted models. <div>
arXiv:2511.21703v1 Announce Type: new 
Abstract: We investigate the generalization properties of dense text embeddings when the embedding backbone is a large language model (LLM) versus when it is a non-LLM encoder, and we study the extent to which spherical linear interpolation (SLERP) model-merging mitigates over-specialization introduced by task-specific adaptation (e.g., LoRA). To make the comparison concrete and domain-agnostic, we design a controlled suite of experiments in which models embed short numerical sequences and are evaluated on their ability to cluster and classify those sequences according to well-defined number-theoretic properties. Our experimental protocol compares four families of models: (1) non-LLM encoders trained from scratch or fine-tuned for embeddings, (2) LLM-based encoders adapted with parameter-efficient methods (LoRA), (3) LLM-based encoders with LoRA followed by model souping merging into the base weights, and (4) the same LoRA-adapted LLMs merged using SLERP across checkpoints or stages. We evaluate representational quality with clustering indices (Silhouette and Davies Bouldin). We additionally analyze the use of kmeans labels to see if the embeddings encode any other information besides the one we are testing for. Empirically, we find that LLM-based backbones produce embeddings that better capture higher-order, compositional numeric patterns, but are prone to adapter dominance that degrades balanced generalization; SLERP merging consistently recovers base-model structure while retaining most task gains, yielding superior tradeoffs in clustering separability, and robustness compared to model souping or models that were not merged.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Cross-lingual Transferability of Pre-trained wav2vec2-based Models</title>
<link>https://arxiv.org/abs/2511.21704</link>
<guid>https://arxiv.org/abs/2511.21704</guid>
<content:encoded><![CDATA[
<div> Keywords: wav2vec 2.0, cross-lingual transfer, pre-trained speech models, fine-tuning, language diversity  

<br /><br />Summary:  
This work investigates the cross-lingual transferability of wav2vec 2.0-based pre-trained speech models across 18 languages using 15 large pre-trained models. The study demonstrates that the diversity of data used during pre-training has a greater impact on downstream automatic speech recognition (ASR) performance than the sheer size of the pre-training data. The results indicate that Indo-European languages generally achieve superior performance compared to non-Indo-European languages. A positive transfer of knowledge was observed even in monolingual models when fine-tuned on different languages, with transfer effects being more pronounced when the pre-training language was linguistically similar to the downstream target language. These findings emphasize the importance of selecting diverse and linguistically relevant data during pre-training to enhance the effectiveness of wav2vec 2.0 models for various languages. The study aims to guide the research community in better utilizing existing pre-trained wav2vec 2.0 models and inform strategies for the pre-training of new models with cross-lingual capabilities. The work provides empirical insights supporting improved resource allocation and model development for multilingual speech recognition tasks. <div>
arXiv:2511.21704v1 Announce Type: new 
Abstract: Using representations provided by a large pre-trained model has become the primary strategy for achieving state-of-the-art results in a wide range of tasks. A recently proposed large pre-trained model, wav2vec 2.0, was seminal for several other works on pre-training large models on speech data. Many models are being pre-trained using the same architecture as wav2vec 2.0 and are getting state-of-the-art in various speech-related tasks. Previous work has demonstrated that the data used during the pre-training of these wav2vec2-based models can impact the model's performance in downstream tasks, and this should be taken into consideration before utilizing these models. However, few works have proposed investigating further how the transfer knowledge of these pre-trained models behaves in different languages, even when the target language differs from the one used during the model's pre-training. Our work aims to investigate the cross-lingual transferability of these wav2vec2-based models. We performed several fine-tuning experiments on the speech recognition task in 18 languages using 15 large pre-trained models. The results of our experiments showed us that the size of data used during the pre-training of these models is not as important to the final performance as the diversity. We noticed that the performance of Indo-European languages is superior to non-Indo-European languages in the evaluated models. We have observed a positive cross-lingual transfer of knowledge using monolingual models, which was evident in all the languages we used, but more pronounced when the language used during pre-training was more similar to the downstream task language. With these findings, we aim to assist the scientific community in utilizing existing wav2vec2-based pre-trained models, as well as facilitate the pre-training of new ones.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Insight-A: Attribution-aware for Multimodal Misinformation Detection</title>
<link>https://arxiv.org/abs/2511.21705</link>
<guid>https://arxiv.org/abs/2511.21705</guid>
<content:encoded><![CDATA[
<div> AI-generated content, multimodal misinformation, attribution, cross-attribution prompting, hierarchical reasoning<br /><br />Summary:<br /><br />This paper addresses the challenge of detecting multimodal misinformation generated by AI-generated content (AIGC), which poses serious societal risks on social media. Unlike previous methods relying on standard prompting with multimodal large language models (MLLMs) that overlook misinformation attribution, the authors introduce Insight-A, a novel approach focusing on attribution for more effective detection. Insight-A aims to (I) attribute misinformation to its forgery sources by analyzing generation patterns, and (II) create a pipeline with hierarchical reasoning to identify distortions across different modalities like text and images. To capture the complex relationship between perception and reasoning, they propose cross-attribution prompting (CAP) which models these correlations explicitly. To minimize human bias in prompt design, they incorporate automatic attribution-debiased prompting (ADP) to allow MLLMs to adapt better to the task. Additionally, image captioning (IC) is integrated to provide detailed visual information that supports cross-modal consistency checks. Extensive experimental results confirm that Insight-A outperforms existing baselines, establishing a new paradigm for detecting sophisticated multimodal misinformation in the era of AI-generated content. <div>
arXiv:2511.21705v1 Announce Type: new 
Abstract: AI-generated content (AIGC) technology has emerged as a prevalent alternative to create multimodal misinformation on social media platforms, posing unprecedented threats to societal safety. However, standard prompting leverages multimodal large language models (MLLMs) to identify the emerging misinformation, which ignores the misinformation attribution. To this end, we present Insight-A, exploring attribution with MLLM insights for detecting multimodal misinformation. Insight-A makes two efforts: I) attribute misinformation to forgery sources, and II) an effective pipeline with hierarchical reasoning that detects distortions across modalities. Specifically, to attribute misinformation to forgery traces based on generation patterns, we devise cross-attribution prompting (CAP) to model the sophisticated correlations between perception and reasoning. Meanwhile, to reduce the subjectivity of human-annotated prompts, automatic attribution-debiased prompting (ADP) is used for task adaptation on MLLMs. Additionally, we design image captioning (IC) to achieve visual details for enhancing cross-modal consistency checking. Extensive experiments demonstrate the superiority of our proposal and provide a new paradigm for multimodal misinformation detection in the era of AIGC.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A General Highly Accurate Online Planning Method Integrating Large Language Models into Nested Rollout Policy Adaptation for Dialogue Tasks</title>
<link>https://arxiv.org/abs/2511.21706</link>
<guid>https://arxiv.org/abs/2511.21706</guid>
<content:encoded><![CDATA[
<div> Keywords: goal-oriented dialogue, nested rollout policy adaptation, large language model, Monte Carlo simulation, dialogue policy planning  

<br /><br />Summary:  
This paper addresses the challenge in goal-oriented dialogue tasks of steering conversations toward a predefined goal within a limited number of turns. Traditional methods rely heavily on elaborate prompt engineering or complex policy networks coupled with pre-trained models, both of which have limitations such as dependence on human expertise, difficulty in adapting to new scenarios, and high training costs. To overcome these issues, the authors propose Nested Rollout Policy Adaptation for Goal-oriented Dialogue (NRPA-GD), a novel approach that eliminates the need for specific model training by leveraging a large language model (LLM) to simulate both user and system behaviors simultaneously. NRPA-GD introduces a comprehensive evaluation mechanism for dialogue trajectories and utilizes a nested Monte Carlo simulation combined with policy self-adaptation, allowing dynamic policy adjustments during the dialogue process. Experimental results on four standard goal-oriented dialogue datasets demonstrate that NRPA-GD outperforms existing prompt engineering techniques and pre-trained model-based approaches. Remarkably, the method surpasses the performance of ChatGPT and specialized pre-trained policy models using only a relatively small 0.6-billion-parameter LLM. The study highlights the novel advantage of applying planning-based methods on LLMs to effectively solve real-world planning tasks within dialogue systems. <div>
arXiv:2511.21706v1 Announce Type: new 
Abstract: In goal-oriented dialogue tasks, the main challenge is to steer the interaction towards a given goal within a limited number of turns. Existing approaches either rely on elaborate prompt engineering, whose effectiveness is heavily dependent on human experience, or integrate policy networks and pre-trained policy models, which are usually difficult to adapt to new dialogue scenarios and costly to train. Therefore, in this paper, we present Nested Rollout Policy Adaptation for Goal-oriented Dialogue (NRPA-GD), a novel dialogue policy planning method that completely avoids specific model training by utilizing a Large Language Model (LLM) to simulate behaviors of user and system at the same time. Specifically, NRPA-GD constructs a complete evaluation mechanism for dialogue trajectories and employs an optimization framework of nested Monte Carlo simulation and policy self-adaptation to dynamically adjust policies during the dialogue process. The experimental results on four typical goal-oriented dialogue datasets show that NRPA-GD outperforms both existing prompt engineering and specifically pre-trained model-based methods. Impressively, NRPA-GD surpasses ChatGPT and pre-trained policy models with only a 0.6-billion-parameter LLM. The proposed approach further demonstrates the advantages and novelty of employing planning methods on LLMs to solve practical planning tasks.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lost in the Pipeline: How Well Do Large Language Models Handle Data Preparation?</title>
<link>https://arxiv.org/abs/2511.21708</link>
<guid>https://arxiv.org/abs/2511.21708</guid>
<content:encoded><![CDATA[
<div> Large language models, data preparation, data profiling, data cleaning, quality model<br /><br />Summary:<br /><br />This paper explores the potential of large language models (LLMs) to support and automate data preparation tasks, a critical yet labor-intensive element in data-driven workflows. The study evaluates both general-purpose and fine-tuned tabular LLMs by prompting them with poor-quality datasets. The models' abilities to perform key tasks such as data profiling and cleaning are thoroughly assessed. Additionally, the research compares the assistance provided by LLMs against traditional data preparation tools, highlighting their effectiveness and limitations. To systematically measure LLM performance, a custom-designed quality model was developed. This model not only quantifies task outcomes but has also been validated through a user study involving data practitioners, thereby ensuring it aligns with real-world expectations and requirements. The findings provide insights into how LLMs can be integrated into data preparation workflows and their potential impact on reducing manual effort and improving data quality. Overall, the paper contributes to understanding the practical capabilities of large language models in automating complex data preprocessing tasks and offers a foundation for future improvements in AI-assisted data preparation. <div>
arXiv:2511.21708v1 Announce Type: new 
Abstract: Large language models have recently demonstrated their exceptional capabilities in supporting and automating various tasks. Among the tasks worth exploring for testing large language model capabilities, we considered data preparation, a critical yet often labor-intensive step in data-driven processes. This paper investigates whether large language models can effectively support users in selecting and automating data preparation tasks. To this aim, we considered both general-purpose and fine-tuned tabular large language models. We prompted these models with poor-quality datasets and measured their ability to perform tasks such as data profiling and cleaning. We also compare the support provided by large language models with that offered by traditional data preparation tools. To evaluate the capabilities of large language models, we developed a custom-designed quality model that has been validated through a user study to gain insights into practitioners' expectations.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying and Mitigating Selection Bias in LLMs: A Transferable LoRA Fine-Tuning and Efficient Majority Voting Approach</title>
<link>https://arxiv.org/abs/2511.21709</link>
<guid>https://arxiv.org/abs/2511.21709</guid>
<content:encoded><![CDATA[
<div> Permutation Bias, Large Language Models, Multiple Choice Questions, Bias Mitigation, Low-Rank Adaptation<br /><br />Summary:<br /><br />1. This paper addresses the selection bias in Large Language Models (LLMs) when answering Multiple Choice Questions (MCQs), where models tend to pick answers based on position or symbol rather than content, reducing evaluation reliability. 2. Existing bias metrics rely on answer labels and measure prediction-answer distribution divergences but fail to capture prediction consistency across different answer orderings. 3. Current bias mitigation methods, such as majority voting and calibration, have drawbacks: majority voting is computationally expensive, and calibration needs validation sets and lacks cross-dataset generalization. 4. To overcome these issues, the authors propose three contributions: (a) a novel unsupervised, label-free Permutation Bias Metric (PBM) that quantifies inconsistencies in predictions across answer permutations, improving bias measurement precision; (b) Batch Question-Context KV caching (BaQCKV), an efficient majority voting technique that reduces computation while preserving effectiveness; (c) an unsupervised Low-Rank Adaptation fine-tuning approach (LoRA-1) leveraging the PBM and BaQCKV to mitigate bias efficiently and maintain generalizability. 5. Experiments on multiple MCQ benchmarks demonstrate that these methods reduce selection bias, increase prediction consistency, improve accuracy, and minimize computational costs effectively. <div>
arXiv:2511.21709v1 Announce Type: new 
Abstract: Multiple Choice Question (MCQ) answering is a widely used method for evaluating the performance of Large Language Models (LLMs). However, LLMs often exhibit selection bias in MCQ tasks, where their choices are influenced by factors like answer position or option symbols rather than the content. This bias undermines the reliability of MCQ as an evaluation framework. Most existing selection bias metrics require answer labels and measure divergences between prediction and answer distributions, but do not fully capture the consistency of a model's predictions across different orderings of answer choices. Existing selection bias mitigation strategies have notable limitations: majority voting, though effective, is computationally prohibitive; calibration-based methods require validation sets and often fail to generalize across datasets. To address these gaps, we propose three key contributions: (1) a new unsupervised label-free Permutation Bias Metric (PBM) that directly quantifies inconsistencies in model predictions across answer permutations, providing a more precise measure of selection bias, (2) an efficient majority voting approach called Batch Question-Context KV caching (BaQCKV), to significantly reduce computational costs while preserving bias mitigation effectiveness, and (3) an unsupervised Low-Rank Adaptation (LoRA-1) fine-tuning strategy based on our proposed metric and the BaQCKV that mitigates selection bias, providing a computationally efficient alternative that maintains model generalizability. Experiments across multiple MCQ benchmarks demonstrate that our approaches reduce bias, increasing consistency in accuracy while minimizing computational costs.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Addressing Stereotypes in Large Language Models: A Critical Examination and Mitigation</title>
<link>https://arxiv.org/abs/2511.21711</link>
<guid>https://arxiv.org/abs/2511.21711</guid>
<content:encoded><![CDATA[
<div> Bias, Large Language Models, Stereotypes, Fine-tuning, Bias Benchmarks<br /><br />Summary:<br /><br />This study examines biases in Large Language Models (LLMs) like BERT, GPT 3.5, and ADA, emphasizing the importance of identifying, understanding, and mitigating various biases including social, ethical, cultural, and racial prejudices. The research employs bias-specific benchmarks such as StereoSet and CrowSPairs to systematically evaluate both explicit and implicit biases in these generative models. A three-pronged approach is adopted to ensure comprehensive analysis of bias presence and extent. Findings reveal that fine-tuned models are particularly challenged by gender biases, although they perform better at recognizing and avoiding racial biases. Additionally, LLMs tend to over-rely on keywords in user prompts, highlighting their limited ability to genuinely understand the accuracy and authenticity of generated outputs. To improve bias mitigation, the study applies enhancement learning strategies including fine-tuning, diverse prompting techniques, and data augmentation of bias benchmarks. The application of these methods results in notable adaptability across datasets and significant improvement in handling implicit biases, with performance gains reaching up to 20%. The work stresses the ongoing need to address bias in generative AI to produce fairer, less stereotyped, and more reliable outputs. <div>
arXiv:2511.21711v1 Announce Type: new 
Abstract: Large Language models (LLMs), such as ChatGPT, have gained popularity in recent years with the advancement of Natural Language Processing (NLP), with use cases spanning many disciplines and daily lives as well. LLMs inherit explicit and implicit biases from the datasets they were trained on; these biases can include social, ethical, cultural, religious, and other prejudices and stereotypes. It is important to comprehensively examine such shortcomings by identifying the existence and extent of such biases, recognizing the origin, and attempting to mitigate such biased outputs to ensure fair outputs to reduce harmful stereotypes and misinformation. This study inspects and highlights the need to address biases in LLMs amid growing generative Artificial Intelligence (AI). We utilize bias-specific benchmarks such StereoSet and CrowSPairs to evaluate the existence of various biases in many different generative models such as BERT, GPT 3.5, and ADA. To detect both explicit and implicit biases, we adopt a three-pronged approach for thorough and inclusive analysis. Results indicate fine-tuned models struggle with gender biases but excel at identifying and avoiding racial biases. Our findings also illustrated that despite some cases of success, LLMs often over-rely on keywords in prompts and its outputs. This demonstrates the incapability of LLMs to attempt to truly understand the accuracy and authenticity of its outputs. Finally, in an attempt to bolster model performance, we applied an enhancement learning strategy involving fine-tuning, models using different prompting techniques, and data augmentation of the bias benchmarks. We found fine-tuned models to exhibit promising adaptability during cross-dataset testing and significantly enhanced performance on implicit bias benchmarks, with performance gains of up to 20%.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EulerESG: Automating ESG Disclosure Analysis with LLMs</title>
<link>https://arxiv.org/abs/2511.21712</link>
<guid>https://arxiv.org/abs/2511.21712</guid>
<content:encoded><![CDATA[
<div> Keywords: ESG reports, Large Language Models, ESG frameworks, disclosure analysis, SASB standards<br /><br />Summary:<br /><br />1. The paper introduces EulerESG, a novel system designed to automate the analysis of Environmental, Social, and Governance (ESG) disclosures in company reports, which are often lengthy and heterogeneous PDFs.<br /><br />2. Unlike existing tools that depend on brittle rule-based methods or treat reports as generic text, EulerESG explicitly incorporates knowledge of ESG reporting standards to improve accuracy.<br /><br />3. The system leverages a dual-channel retrieval mechanism combined with Large Language Model (LLM)-driven analysis to interpret and extract relevant ESG disclosure data.<br /><br />4. EulerESG includes an interactive dashboard and chatbot interface, enabling users to explore, benchmark, and explain ESG metrics effectively.<br /><br />5. Experimental evaluation with data from four globally recognized companies and twelve SASB sub-industries demonstrates high fidelity in populating standard-aligned metric tables, achieving up to 0.95 average accuracy.<br /><br />6. The system maintains efficient end-to-end runtime performance and offers a practical solution for ESG report analysis.<br /><br />7. A full implementation along with a demonstration video is publicly available on GitHub, providing a resource for further research and practical adoption. <div>
arXiv:2511.21712v1 Announce Type: new 
Abstract: Environmental, Social, and Governance (ESG) reports have become central to how companies communicate climate risk, social impact, and governance practices, yet they are still published primarily as long, heterogeneous PDF documents. This makes it difficult to systematically answer seemingly simple questions. Existing tools either rely on brittle rule-based extraction or treat ESG reports as generic text, without explicitly modelling the underlying reporting standards. We present \textbf{EulerESG}, an LLM-powered system for automating ESG disclosure analysis with explicit awareness of ESG frameworks. EulerESG combines (i) dual-channel retrieval and LLM-driven disclosure analysis over ESG reports, and (ii) an interactive dashboard and chatbot for exploration, benchmarking, and explanation. Using four globally recognised companies and twelve SASB sub-industries, we show that EulerESG can automatically populate standard-aligned metric tables with high fidelity (up to 0.95 average accuracy) while remaining practical in end-to-end runtime, and we compare several recent LLM models in this setting. The full implementation, together with a demonstration video, is publicly available at https://github.com/UNSW-database/EulerESG.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GPS: General Per-Sample Prompter</title>
<link>https://arxiv.org/abs/2511.21714</link>
<guid>https://arxiv.org/abs/2511.21714</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, prompting, reinforcement learning, per-sample adaptation, Minimum Bayes Risk decoding  

<br /><br />Summary:  
This paper addresses the challenge of sensitivity in large language model (LLM) prompting, where small changes in prompt phrasing can drastically affect task performance. It highlights the limitations of current automatic prompting methods, which require large task-specific datasets, involve costly optimization cycles, and generate a single fixed prompt per task rather than adapting to individual inputs. To overcome these issues, the authors propose GPS, a novel general-purpose prompting system that creates tailored prompts for each input sample without task-specific fine-tuning. GPS is trained via reinforcement learning on multiple training tasks and includes a new regularization technique to improve per-sample prompt adaptation. Additionally, it uses Minimum Bayes Risk decoding during inference to enhance stability. Empirical evaluations demonstrate that GPS achieves competitive or superior outcomes compared to baseline methods on various tasks such as text simplification, summarization, and classification, despite not being trained on these specific tasks. Notably, GPS achieves state-of-the-art results on the GSM8K benchmark through in-domain prompting. This work introduces an effective, scalable automatic prompting paradigm that adapts prompts at the input level without the need for extensive optimization or large, task-specific datasets. The associated code is publicly available for further research and application. <div>
arXiv:2511.21714v1 Announce Type: new 
Abstract: LLMs are sensitive to prompting, with task performance often hinging on subtle, sometimes imperceptible variations in phrasing. As a result, crafting effective prompts manually remains challenging and time-consuming. Recent automatic prompting methods mitigate this difficulty but face three key limitations: (i) for each new task, they require large datasets to train good prompts;(ii) they rely on costly optimization loops that may take hours; (iii)they typically produce a single task-level prompt that does not adapt to the individual input problem to be solved.
  We propose GPS, the first general-purpose, per-sample prompting method. Without any task-specific tuning, GPS generates a tailored prompt for each unseen input, improving performance across diverse tasks. The prompter is trained with reinforcement learning on a suite of training tasks and includes a novel regularization for effectively adapting to per-sample prompting. Finally, we employ Minimum Bayes Risk decoding to stabilize inference.
  Empirically, GPS demonstrates competitive performance: we attain second best results among baselines on text simplification, third best results on summarization and on-par results on classification, while not training on any of these tasks, in contrast to the baselines. For in-domain prompting, we obtain sota on GSM8K. Our work shows the potential of a novel and effective paradigm for automatic prompting: generating adaptive, input-specific prompts without extensive optimization and without access to a task-specific training set. Our code is available at https://github.com/Batorskq/GPS.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Optimized Machine Learning Classifier for Detecting Fake Reviews Using Extracted Features</title>
<link>https://arxiv.org/abs/2511.21716</link>
<guid>https://arxiv.org/abs/2511.21716</guid>
<content:encoded><![CDATA[
<div> Keywords: fraudulent reviews, computer-generated text, Harris Hawks Optimization, stacking ensemble classifier, privacy-preserving techniques  

<br /><br />Summary:  
This study addresses the challenge posed by fraudulent online reviews, particularly those generated by AI, which undermine the trustworthiness of online purchases. The authors developed an advanced machine-learning system to accurately differentiate between human-written and computer-generated reviews. Their method involves sophisticated text preprocessing and multi-modal feature extraction to capture comprehensive data characteristics. Harris Hawks Optimization (HHO), a bio-inspired algorithm, was employed for selecting the most relevant features from an initial set of 13,539, successfully reducing dimensionality by 89.9% down to 1,368. The refined features were then input into a stacking ensemble classifier, which combines multiple learning models to improve accuracy and robustness. Tested on a public dataset comprising 40,432 original and computer-generated reviews, the final model achieved high-performance metrics: 95.40% accuracy, 92.81% precision, 95.01% recall, and a 93.90% F1-score. This demonstrates the effectiveness of combining ensemble learning techniques with bio-inspired optimization for recognizing AI-generated text. Furthermore, the study highlights the importance of integrating privacy-preserving methods, such as differential privacy and secure outsourcing, to safeguard user data in cloud-based large-scale review analytics platforms. <div>
arXiv:2511.21716v1 Announce Type: new 
Abstract: It is well known that fraudulent reviews cast doubt on the legitimacy and dependability of online purchases. The most recent development that leads customers towards darkness is the appearance of human reviews in computer-generated (CG) ones. In this work, we present an advanced machine-learning-based system that analyses these reviews produced by AI with remarkable precision. Our method integrates advanced text preprocessing, multi-modal feature extraction, Harris Hawks Optimization (HHO) for feature selection, and a stacking ensemble classifier. We implemented this methodology on a public dataset of 40,432 Original (OR) and Computer-Generated (CG) reviews. From an initial set of 13,539 features, HHO selected the most applicable 1,368 features, achieving an 89.9% dimensionality reduction. Our final stacking model achieved 95.40% accuracy, 92.81% precision, 95.01% recall, and a 93.90% F1-Score, which demonstrates that the combination of ensemble learning and bio-inspired optimisation is an effective method for machine-generated text recognition. Because large-scale review analytics commonly run on cloud platforms, privacy-preserving techniques such as differential approaches and secure outsourcing are essential to protect user data in these systems.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CrossCheck-Bench: Diagnosing Compositional Failures in Multimodal Conflict Resolution</title>
<link>https://arxiv.org/abs/2511.21717</link>
<guid>https://arxiv.org/abs/2511.21717</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, contradiction detection, CrossCheck-Bench, multimodal reasoning, vision-language models<br /><br />Summary:<br /><br />1. The paper addresses the challenge that Multimodal Large Language Models (MLLMs) face when dealing with real-world inconsistencies in open-domain scenarios, where visual and textual information often conflict beyond simple alignment.<br /><br />2. To evaluate these models’ abilities in contradiction detection, the authors introduce CrossCheck-Bench, a diagnostic benchmark featuring a hierarchical task framework with three reasoning complexity levels and seven atomic capabilities required for resolving cross-modal contradictions.<br /><br />3. CrossCheck-Bench contains 15,000 question-answer pairs derived from real-world sources, with carefully injected synthetic contradictions, created through an extensive annotation process requiring over 450 expert hours to ensure semantic correctness and balanced difficulty.<br /><br />4. Testing 13 state-of-the-art vision-language models on this benchmark reveals significant performance drops as tasks progress from basic perceptual matching to complex logical contradiction detection; models generally excel in simple entity recognition but falter on tasks needing multi-clue synthesis and multi-step inference.<br /><br />5. Standard prompting techniques such as Chain-of-Thought and Set-of-Mark offer little improvement, whereas approaches integrating symbolic reasoning with grounded visual processing yield more stable performance gains, highlighting a major bottleneck in current multimodal reasoning and suggesting avenues for future model development. <div>
arXiv:2511.21717v1 Announce Type: new 
Abstract: Multimodal Large Language Models are primarily trained and evaluated on aligned image-text pairs, which leaves their ability to detect and resolve real-world inconsistencies largely unexplored. In open-domain applications visual and textual cues often conflict, requiring models to perform structured reasoning beyond surface-level alignment. We introduce CrossCheck-Bench, a diagnostic benchmark for evaluating contradiction detection in multimodal inputs. The benchmark adopts a hierarchical task framework covering three levels of reasoning complexity and defines seven atomic capabilities essential for resolving cross-modal inconsistencies. CrossCheck-Bench includes 15k question-answer pairs sourced from real-world artifacts with synthetically injected contradictions. The dataset is constructed through a multi-stage annotation pipeline involving more than 450 expert hours to ensure semantic validity and calibrated difficulty across perception, integration, and reasoning. We evaluate 13 state-of-the-art vision-language models and observe a consistent performance drop as tasks shift from perceptual matching to logical contradiction detection. Most models perform well on isolated entity recognition but fail when multiple clues must be synthesized for conflict reasoning. Capability-level analysis further reveals uneven skill acquisition, especially in tasks requiring multi-step inference or rule-based validation. Additional probing shows that conventional prompting strategies such as Chain-of-Thought and Set-of-Mark yield only marginal gains. By contrast, methods that interleave symbolic reasoning with grounded visual processing achieve more stable improvements. These results highlight a persistent bottleneck in multimodal reasoning and suggest new directions for building models capable of robust cross-modal verification.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Harmless Words Harm: A New Threat to LLM Safety via Conceptual Triggers</title>
<link>https://arxiv.org/abs/2511.21718</link>
<guid>https://arxiv.org/abs/2511.21718</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM jailbreaks, value alignment, conceptual morphology, MICM method, safety mechanisms<br /><br />Summary:<br /><br />1. The paper identifies a critical blind spot in current LLM safety mechanisms related to abstract generalization and implicit social values rather than overt harmful outputs.<br />2. It introduces MICM, a novel, model-agnostic jailbreak technique that manipulates the aggregate value structures in LLM outputs by embedding nuanced concept configurations through fixed prompt templates.<br />3. MICM uses conceptual morphology theory to design predefined phrase sets that act as conceptual triggers, subtly steering model responses toward specific value stances without activating traditional safety filters.<br />4. The method was tested across five advanced large language models, including GPT-4o, Deepseek-R1, and Qwen3-8B, demonstrating superior performance compared to existing jailbreak strategies.<br />5. Results reveal that despite sophisticated safety measures, commercial LLMs remain vulnerable to covert manipulation targeting their underlying value alignment, exposing fundamental limitations in current alignment frameworks. <div>
arXiv:2511.21718v1 Announce Type: new 
Abstract: Recent research on large language model (LLM) jailbreaks has primarily focused on techniques that bypass safety mechanisms to elicit overtly harmful outputs. However, such efforts often overlook attacks that exploit the model's capacity for abstract generalization, creating a critical blind spot in current alignment strategies. This gap enables adversaries to induce objectionable content by subtly manipulating the implicit social values embedded in model outputs. In this paper, we introduce MICM, a novel, model-agnostic jailbreak method that targets the aggregate value structure reflected in LLM responses. Drawing on conceptual morphology theory, MICM encodes specific configurations of nuanced concepts into a fixed prompt template through a predefined set of phrases. These phrases act as conceptual triggers, steering model outputs toward a specific value stance without triggering conventional safety filters. We evaluate MICM across five advanced LLMs, including GPT-4o, Deepseek-R1, and Qwen3-8B. Experimental results show that MICM consistently outperforms state-of-the-art jailbreak techniques, achieving high success rates with minimal rejection. Our findings reveal a critical vulnerability in commercial LLMs: their safety mechanisms remain susceptible to covert manipulation of underlying value alignment.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PeerCoPilot: A Language Model-Powered Assistant for Behavioral Health Organizations</title>
<link>https://arxiv.org/abs/2511.21721</link>
<guid>https://arxiv.org/abs/2511.21721</guid>
<content:encoded><![CDATA[
<div> behavioral health, peer providers, large language model, wellness plans, resource retrieval<br /><br />Summary:<br /><br />Behavioral health conditions, encompassing mental health and substance use disorders, represent the leading disease burden in the United States. Peer-run behavioral health organizations (PROs) play a vital role by delivering mental health support alongside assistance with income, employment, and housing, yet they face challenges due to limited funding and staffing. To aid peer providers in their daily work, the authors introduce PeerCoPilot, an assistant powered by a large language model (LLM) designed to help create personalized wellness plans, develop step-by-step goals, and identify relevant organizational resources. PeerCoPilot employs a retrieval-augmented generation approach, ensuring reliability by drawing from a vetted database of over 1,300 resources. Human evaluations involving 15 peer providers and 6 service users showed that more than 90% supported using PeerCoPilot. Additionally, compared to a baseline LLM, PeerCoPilot delivers more reliable and specific information. Currently, the tool is actively deployed among 5-10 peer providers at CSPNJ, a major behavioral health organization serving over 10,000 individuals, with ongoing efforts to expand its use further. <div>
arXiv:2511.21721v1 Announce Type: new 
Abstract: Behavioral health conditions, which include mental health and substance use disorders, are the leading disease burden in the United States. Peer-run behavioral health organizations (PROs) critically assist individuals facing these conditions by combining mental health services with assistance for needs such as income, employment, and housing. However, limited funds and staffing make it difficult for PROs to address all service user needs. To assist peer providers at PROs with their day-to-day tasks, we introduce PeerCoPilot, a large language model (LLM)-powered assistant that helps peer providers create wellness plans, construct step-by-step goals, and locate organizational resources to support these goals. PeerCoPilot ensures information reliability through a retrieval-augmented generation pipeline backed by a large database of over 1,300 vetted resources. We conducted human evaluations with 15 peer providers and 6 service users and found that over 90% of users supported using PeerCoPilot. Moreover, we demonstrated that PeerCoPilot provides more reliable and specific information than a baseline LLM. PeerCoPilot is now used by a group of 5-10 peer providers at CSPNJ, a large behavioral health organization serving over 10,000 service users, and we are actively expanding PeerCoPilot's use.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>German General Personas: A Survey-Derived Persona Prompt Collection for Population-Aligned LLM Studies</title>
<link>https://arxiv.org/abs/2511.21722</link>
<guid>https://arxiv.org/abs/2511.21722</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, persona prompting, German General Social Survey, social simulation, population alignment<br /><br />Summary: The paper introduces the German General Personas (GGP) collection, a novel set of persona prompts constructed from the German General Social Survey (ALLBUS) to facilitate human perspective simulations using Large Language Models (LLMs). The GGP aims to provide a comprehensive and empirically grounded resource for persona prompting, addressing the scarcity of well-curated and representative persona collections in computational social science. Designed for easy integration with various LLMs and task types, GGP steers model outputs to align closely with the demographic and social characteristics of the German population. The authors evaluate GGP by employing it to prompt multiple LLMs, showing that models guided by GGP better replicate real survey response distributions across diverse social topics compared to current state-of-the-art classifiers, especially when training data is limited. Further analysis reveals that the representativity of the personas and the choice of attributes significantly impact how well LLM-generated responses match actual population data. Overall, GGP offers a valuable tool for advancing research in LLM-based social simulations, enabling more systematic and population-aligned persona prompting approaches in natural language processing and social science studies. <div>
arXiv:2511.21722v1 Announce Type: new 
Abstract: The use of Large Language Models (LLMs) for simulating human perspectives via persona prompting is gaining traction in computational social science. However, well-curated, empirically grounded persona collections remain scarce, limiting the accuracy and representativeness of such simulations. Here we introduce the German General Personas (GGP) collection, a comprehensive and representative persona prompt collection built from the German General Social Survey (ALLBUS). The GGP and its persona prompts are designed to be easily plugged into prompts for all types of LLMs and tasks, steering models to generate responses aligned with the underlying German population. We evaluate GGP by prompting various LLMs to simulate survey response distributions across diverse topics, demonstrating that GGP-guided LLMs outperform state-of-the-art classifiers, particularly under data scarcity. Furthermore, we analyze how the representativity and attribute selection within persona prompts affect alignment with population responses. Our findings suggest that GGP provides a potentially valuable resource for research on LLM-based social simulations that enables more systematic explorations of population-aligned persona prompting in NLP and social science research.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AD-CDO: A Lightweight Ontology for Representing Eligibility Criteria in Alzheimer's Disease Clinical Trials</title>
<link>https://arxiv.org/abs/2511.21724</link>
<guid>https://arxiv.org/abs/2511.21724</guid>
<content:encoded><![CDATA[
<div> Keywords: Alzheimer's disease, ontology, clinical trials, eligibility criteria, semantic categories  

<br /><br />Summary:  
This study presents the Alzheimer's Disease Common Data Element Ontology for Clinical Trials (AD-CDO), a lightweight and semantically enriched ontology aimed at standardizing key eligibility criteria concepts in Alzheimer's disease clinical trials. The authors extracted frequently appearing concepts from over 1,500 AD clinical trials listed on ClinicalTrials.gov and categorized them into seven semantic groups: Disease, Medication, Diagnostic Test, Procedure, Social Determinants of Health, Rating Criteria, and Fertility. Each concept was annotated using well-established biomedical vocabularies including UMLS, OMOP Standardized Vocabularies, DrugBank, NDC, and NLM VSAC. Employing the Jenks Natural Breaks method, they optimized the concept set to strike a balance between comprehensive coverage and manageability. The resulting AD-CDO achieved coverage exceeding 63% of extracted trial concepts, maintaining both interpretability and compactness. The ontology's utility was demonstrated in two use cases: an ontology-driven trial simulation system enabling formal modeling and virtual execution of clinical trials, and an entity normalization task that maps unstructured clinical text to standardized ontology terms, facilitating data consistency and Electronic Health Record integration. The AD-CDO effectively bridges broader biomedical ontologies and specific trial modeling needs, supporting applications such as phenotyping algorithm development and cohort identification. Overall, AD-CDO provides a harmonized foundation aligned with standardized vocabularies, advancing ontology-driven research in Alzheimer's disease clinical trials. <div>
arXiv:2511.21724v1 Announce Type: new 
Abstract: Objective
  This study introduces the Alzheimer's Disease Common Data Element Ontology for Clinical Trials (AD-CDO), a lightweight, semantically enriched ontology designed to represent and standardize key eligibility criteria concepts in Alzheimer's disease (AD) clinical trials.
  Materials and Methods
  We extracted high-frequency concepts from more than 1,500 AD clinical trials on ClinicalTrials.gov and organized them into seven semantic categories: Disease, Medication, Diagnostic Test, Procedure, Social Determinants of Health, Rating Criteria, and Fertility. Each concept was annotated with standard biomedical vocabularies, including the UMLS, OMOP Standardized Vocabularies, DrugBank, NDC, and NLM VSAC value sets. To balance coverage and manageability, we applied the Jenks Natural Breaks method to identify an optimal set of representative concepts.
  Results
  The optimized AD-CDO achieved over 63% coverage of extracted trial concepts while maintaining interpretability and compactness. The ontology effectively captured the most frequent and clinically meaningful entities used in AD eligibility criteria. We demonstrated AD-CDO's practical utility through two use cases: (a) an ontology-driven trial simulation system for formal modeling and virtual execution of clinical trials, and (b) an entity normalization task mapping raw clinical text to ontology-aligned terms, enabling consistency and integration with EHR data.
  Discussion
  AD-CDO bridges the gap between broad biomedical ontologies and task-specific trial modeling needs. It supports multiple downstream applications, including phenotyping algorithm development, cohort identification, and structured data integration.
  Conclusion
  By harmonizing essential eligibility entities and aligning them with standardized vocabularies, AD-CDO provides a versatile foundation for ontology-driven AD clinical trial research.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PromptTailor: Multi-turn Intent-Aligned Prompt Synthesis for Lightweight LLMs</title>
<link>https://arxiv.org/abs/2511.21725</link>
<guid>https://arxiv.org/abs/2511.21725</guid>
<content:encoded><![CDATA[
<div> Keywords: PromptTailor, lightweight language models, prompt optimization, Llama3-8B, user intent alignment<br /><br />Summary:  
1. The paper addresses the challenge of optimizing prompts for lightweight language models used in on-device and privacy-sensitive applications, where non-expert users often struggle to craft effective prompts.  
2. It presents PromptTailor, a system designed to generate controllable, intent-aligned prompts for open-ended text generation by expanding minimal user instructions into rich, domain-aware prompts without losing the users’ original preferences.  
3. PromptTailor is built on a quantized Llama3-8B model fine-tuned with a lightweight LoRA adapter trained on 12,300 prompt-refinement dialogues across 41 everyday domains, distilled from three stronger large language models.  
4. The system supports edge deployment by attaching the adapter to any Llama3-8B base model, facilitating efficient and privacy-preserving use cases.  
5. Evaluations using both human judges and strong LLM evaluators demonstrate that PromptTailor achieves higher preference rates compared to chain-of-thought prompting and matches or exceeds state-of-the-art prompt optimization methods while requiring significantly fewer model calls (3 versus 9).  
Overall, the study shows that a compact fine-tuned student model can effectively learn prompt-generation strategies from powerful teacher models, enhancing output quality and maintaining strong alignment with user intent. <div>
arXiv:2511.21725v1 Announce Type: new 
Abstract: Lightweight language models remain attractive for on-device and privacy-sensitive applications, but their responses are highly sensitive to prompt quality. For open-ended generation, non-expert users often lack the knowledge or time to consistently craft high-quality prompts, leading them to rely on prompt optimization tools. However, a key challenge is ensuring the optimized prompts genuinely align with users' original intents and preferences. We introduce PromptTailor, a system for controllable prompt generation for open-ended text that improves model output quality by intent-aligned prompt synthesis. PromptTailor expands minimal user instructions into rich, domain-aware prompts while preserving the user's stated preferences. The system is a quantized Llama3-8B model fine-tuned with a lightweight LoRA adapter on 12,300 prompt-refinement dialogues spanning 41 everyday domains, distilled from three stronger LLMs. The adapter attaches to any Llama3-8B base, enabling edge deployment. In human and LLM-judge evaluations across multiple target models and optimization baselines, PromptTailor yields higher preference rates than chain-of-thought prompting and matches or surpasses state-of-the-art prompt optimization methods while requiring fewer model calls (e.g., 3 vs. 9). These results show that a compact student, guided by powerful teachers, can learn effective prompt-generation strategies that enhance response quality while maintaining alignment with user intent.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Goal-Directed Search Outperforms Goal-Agnostic Memory Compression in Long-Context Memory Tasks</title>
<link>https://arxiv.org/abs/2511.21726</link>
<guid>https://arxiv.org/abs/2511.21726</guid>
<content:encoded><![CDATA[
<div> Keywords: long-term memory, large language models, memory compression, reinforcement learning, search-based memory

<br /><br />Summary:  
1. The paper addresses the challenge of enabling human-like long-term memory in large language models (LLMs) to improve capabilities such as few-shot generalization.  
2. Existing memory frameworks mainly focus on optimizing lossy memory compression algorithms tailored to specific benchmarks, which introduces human bias and limits generalizability across diverse data distributions.  
3. The authors propose SUMER (Search in Uncompressed Memory via Experience Replay), an end-to-end reinforcement learning agent with verifiable reward (RLVR), that learns to utilize search tools for gathering relevant uncompressed information to answer target questions.  
4. Evaluated on the LoCoMo dataset for long-context conversation understanding, SUMER combined with Qwen2.5-7B-Instruct outperforms all previous memory compression methods as well as the full-context baseline, achieving a state-of-the-art performance with a 43% improvement over prior results.  
5. The study demonstrates that direct search methods on raw, uncompressed data can outperform biased, goal-agnostic compression algorithms in current long-context memory tasks, encouraging new dynamic and autonomously scalable paradigms and benchmarks for memory in LLMs.  
6. The authors have made the code for SUMER and all baseline implementations publicly available on GitHub for further research and development. <div>
arXiv:2511.21726v1 Announce Type: new 
Abstract: How to enable human-like long-term memory in large language models (LLMs) has been a central question for unlocking more general capabilities such as few-shot generalization. Existing memory frameworks and benchmarks focus on finding the optimal memory compression algorithm for higher performance in tasks that require recollection and sometimes further reasoning. However, such efforts have ended up building more human bias into the compression algorithm, through the search for the best prompts and memory architectures that suit specific benchmarks, rather than finding a general solution that would work on other data distributions. On the other hand, goal-directed search on uncompressed information could potentially exhibit superior performance because compression is lossy, and a predefined compression algorithm will not fit all raw data distributions. Here we present SUMER (Search in Uncompressed Memory via Experience Replay), an end-to-end reinforcement learning agent with verifiable reward (RLVR) that learns to use search tools to gather information and answer a target question. On the LoCoMo dataset for long-context conversation understanding, SUMER with Qwen2.5-7B-Instruct learned to use search tools and outperformed all other biased memory compression approaches and also the full-context baseline, reaching SOTA performance (43% gain over the prior best). We demonstrate that a simple search method applied to raw data outperforms goal-agnostic and biased compression algorithms in current long-context memory tasks, arguing for new paradigms and benchmarks that are more dynamic and autonomously scalable. Code for SUMER and all implemented baselines is publicly available at https://github.com/zycyc/SUMER.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Affective Multimodal Agents with Proactive Knowledge Grounding for Emotionally Aligned Marketing Dialogue</title>
<link>https://arxiv.org/abs/2511.21728</link>
<guid>https://arxiv.org/abs/2511.21728</guid>
<content:encoded><![CDATA[
<div> AffectMind, multimodal dialogue agent, emotional alignment, proactive reasoning, persuasive interaction<br /><br />Summary:<br /><br />1. This work introduces AffectMind, a multimodal affective dialogue agent designed to enhance goal-oriented marketing conversations by integrating emotional awareness and proactive behavior into dialogue systems.  
2. AffectMind consists of three core components: the Proactive Knowledge Grounding Network (PKGN) which continuously updates context using text, visual, and prosodic information; the Emotion--Intent Alignment Model (EIAM) that jointly models user emotions and purchase intent to tailor persuasion strategies; and the Reinforced Discourse Loop (RDL) that uses reinforcement learning from user feedback to optimize emotional coherence and engagement.  
3. The system aims to overcome limitations of existing large language models that are primarily reactive and struggle with emotionally rich interactions requiring persuasion and sustained engagement.  
4. The authors curated two new multimodal marketing dialogue datasets, MM-ConvMarket and AffectPromo, to evaluate AffectMind’s capabilities.  
5. Experimental results demonstrate that AffectMind outperforms strong LLM-based baselines substantially, showing improvements of 26% in emotional consistency, 19% in persuasive success rate, and 23% in long-term user engagement, underscoring the importance of emotion-grounded proactivity for commercial multimodal agents. <div>
arXiv:2511.21728v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have enabled fluent dialogue systems, but most remain reactive and struggle in emotionally rich, goal-oriented settings such as marketing conversations. To address this limitation, we propose AffectMind, a multimodal affective dialogue agent that performs proactive reasoning and dynamic knowledge grounding to sustain emotionally aligned and persuasive interactions. AffectMind combines three components: a Proactive Knowledge Grounding Network (PKGN) that continuously updates factual and affective context from text, vision, and prosody; an Emotion--Intent Alignment Model (EIAM) that jointly models user emotion and purchase intent to adapt persuasion strategies; and a Reinforced Discourse Loop (RDL) that optimizes emotional coherence and engagement via reinforcement signals from user responses. Experiments on two newly curated marketing dialogue datasets, MM-ConvMarket and AffectPromo, show that AffectMind outperforms strong LLM-based baselines in emotional consistency (+26\%), persuasive success rate (+19\%), and long-term user engagement (+23\%), highlighting emotion-grounded proactivity as a key capability for commercial multimodal agents.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Component Strength: Synergistic Integration and Adaptive Calibration in Multi-Agent RAG Systems</title>
<link>https://arxiv.org/abs/2511.21729</link>
<guid>https://arxiv.org/abs/2511.21729</guid>
<content:encoded><![CDATA[
<div> retrieval-augmented generation, hybrid retrieval, ensemble verification, adaptive thresholding, hallucination reduction  

<br /><br />Summary:  
This paper investigates the design of reliable retrieval-augmented generation (RAG) systems by focusing on the interplay between different system components rather than just their individual performance. Through ablation studies on 50 carefully selected queries (comprising 15 answerable, 10 edge cases, and 25 adversarial), the authors show that individually, methods such as hybrid retrieval, ensemble verification, and adaptive thresholding offer negligible improvements. However, when combined synergistically, these techniques produce a dramatic 95% reduction in abstention rates—from 40% down to just 2%—without increasing the frequency of hallucinations. The study also uncovers a critical measurement issue: different verification methods may safely abstain from answering but assign inconsistent labels (e.g., "abstained" vs. "unsupported"), causing misleading variations in perceived hallucination metrics. The results emphasize the importance of integrated system design over isolated component strength, highlight the need for standardized evaluation metrics and consistent labeling schemes to accurately interpret system performance, and advocate for adaptive calibration techniques to avoid overconfident and erroneous answers, even under conditions of high-quality retrieval. These findings provide valuable insights for advancing trustworthy and effective RAG system development. <div>
arXiv:2511.21729v1 Announce Type: new 
Abstract: Building reliable retrieval-augmented generation (RAG) systems requires more than adding powerful components; it requires understanding how they interact. Using ablation studies on 50 queries (15 answerable, 10 edge cases, and 25 adversarial), we show that enhancements such as hybrid retrieval, ensemble verification, and adaptive thresholding provide almost no benefit when used in isolation, yet together achieve a 95% reduction in abstention (from 40% to 2%) without increasing hallucinations. We also identify a measurement challenge: different verification strategies can behave safely but assign inconsistent labels (for example, "abstained" versus "unsupported"), creating apparent hallucination rates that are actually artifacts of labeling. Our results show that synergistic integration matters more than the strength of any single component, that standardized metrics and labels are essential for correctly interpreting performance, and that adaptive calibration is needed to prevent overconfident over-answering even when retrieval quality is high.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Benchmark for Procedural Memory Retrieval in Language Agents</title>
<link>https://arxiv.org/abs/2511.21730</link>
<guid>https://arxiv.org/abs/2511.21730</guid>
<content:encoded><![CDATA[
<div> Keywords: procedural memory, AI agents, generalization, embedding methods, ALFWorld<br /><br />Summary:<br /><br />This paper addresses a key limitation in current AI agents: their failure to generalize procedural memory to novel tasks involving unseen vocabularies. To tackle this, the authors introduce the first benchmark designed to isolate procedural memory retrieval from task execution, focusing on whether agents can recognize functionally equivalent procedures across different object instantiations. Using ALFWorld, they build dual corpora comprising expert and large language model (LLM)-generated trajectories. Six retrieval methods are evaluated against systematically stratified queries to assess performance. Results reveal a significant generalization gap; embedding-based methods excel in familiar contexts but degrade sharply when facing novel ones. In contrast, LLM-generated procedural abstractions exhibit reliable transfer across contexts. Controlled ablation studies demonstrate that current embedding approaches, while capturing some lexical abstraction, fundamentally ignore the temporal structure of procedures by treating them as unordered word bags, which limits their cross-context transfer ability. The study finds that increasing corpus scale offers greater performance gains than improving representation richness, indicating architectural limitations in existing encoders. Finally, the benchmark provides a diagnostic framework to distinguish between superficial memorization and true procedural understanding, facilitating the development of retrieval systems with robust generalization capabilities. All resources are publicly available on GitHub. <div>
arXiv:2511.21730v1 Announce Type: new 
Abstract: Current AI agents excel in familiar settings, but fail sharply when faced with novel tasks with unseen vocabularies -- a core limitation of procedural memory systems. We present the first benchmark that isolates procedural memory retrieval from task execution, evaluating whether agents can recognize functionally equivalent procedures that span different object instantiations. Using ALFWorld, we construct dual corpora of expert and LLM-generated trajectories and evaluate six retrieval methods using systematically stratified queries. Our results expose a clear generalization cliff: embedding-based methods perform strongly on familiar contexts, yet degrade considerably on novel ones, while LLM-generated procedural abstractions demonstrate reliable cross-context transfer. Controlled ablations show that although embeddings capture some lexical-level abstraction, they fundamentally treat procedures as unordered bags of words, discarding temporal structure necessary for cross-context transfer. Corpus scale delivers far larger gains than representation enrichment, revealing an architectural ceiling in current encoders. Our benchmark offers the first diagnostic framework separating genuine procedural understanding from surface-level memorization and gives tools for developing retrieval systems capable of dependable generalization. Resources available at our GitHub repository (https://github.com/qpiai/Proced_mem_bench).
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Identifying Quantum Structure in AI Language: Evidence for Evolutionary Convergence of Human and Artificial Cognition</title>
<link>https://arxiv.org/abs/2511.21731</link>
<guid>https://arxiv.org/abs/2511.21731</guid>
<content:encoded><![CDATA[
<div> Keywords: cognitive tests, Large Language Models, quantum entanglement, Bose-Einstein statistics, conceptual-linguistic quantum structures  

<br /><br />Summary:  
1. The paper reports cognitive experiments on conceptual combinations using Large Language Models (LLMs) like ChatGPT and Gemini as test subjects.  
2. The first test reveals a significant violation of Bell's inequalities, suggesting the presence of quantum entanglement in the concepts processed by these LLMs.  
3. The second test finds Bose-Einstein statistics governing the distribution of words in large texts generated or analyzed by LLMs, diverging from the classically expected Maxwell-Boltzmann statistics.  
4. These results mirror previous findings observed in human cognitive tests and large-scale information retrieval tasks, indicating a shared quantum-structured organization of meaning across human and artificial cognition.  
5. The authors propose that the meaningful, vector-space semantic structures emerging from neural networks represent a deeper, more fundamental knowledge organization than the neural network architecture itself, suggesting an evolutionary convergence between human cognitive language and LLM-based language through different mechanisms.  
6. They present a unifying theoretical framework to explain the pervasive quantum organization of meaning in conceptual-linguistic domains, highlighting a systematic emergence of quantum structures independent of the nature of the cognitive agent. <div>
arXiv:2511.21731v1 Announce Type: new 
Abstract: We present the results of cognitive tests on conceptual combinations, performed using specific Large Language Models (LLMs) as test subjects. In the first test, performed with ChatGPT and Gemini, we show that Bell's inequalities are significantly violated, which indicates the presence of 'quantum entanglement' in the tested concepts. In the second test, also performed using ChatGPT and Gemini, we instead identify the presence of 'Bose-Einstein statistics', rather than the intuitively expected 'Maxwell-Boltzmann statistics', in the distribution of the words contained in large-size texts. Interestingly, these findings mirror the results previously obtained in both cognitive tests with human participants and information retrieval tests on large corpora. Taken together, they point to the 'systematic emergence of quantum structures in conceptual-linguistic domains', regardless of whether the cognitive agent is human or artificial. Although LLMs are classified as neural networks for historical reasons, we believe that a more essential form of knowledge organization takes place in the distributive semantic structure of vector spaces built on top of the neural network. It is this meaning-bearing structure that lends itself to a phenomenon of evolutionary convergence between human cognition and language, slowly established through biological evolution, and LLM cognition and language, emerging much more rapidly as a result of self-learning and training. We analyze various aspects and examples that contain evidence supporting the above hypothesis. We also advance a unifying framework that explains the pervasive quantum organization of meaning that we identify.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HUMORCHAIN: Theory-Guided Multi-Stage Reasoning for Interpretable Multimodal Humor Generation</title>
<link>https://arxiv.org/abs/2511.21732</link>
<guid>https://arxiv.org/abs/2511.21732</guid>
<content:encoded><![CDATA[
<div> humor generation, multimodal AI, cognitive reasoning, theory-guided framework, image captioning  

<br /><br />Summary:  
This paper addresses the challenge of generating humor in AI, which requires complex cognitive and social understanding. Despite humor's complexity, theoretical humor structures suggest it can be learned by generative models. The authors focus on multimodal humor prevalent in Gen Z's online communication, emphasizing the necessity for AI systems to merge visual interpretation with humorous language generation. Existing data-driven models lack explicit humor modeling, often producing literal but non-humorous image descriptions that miss deeper cognitive humor mechanisms. To overcome these limitations, the authors introduce HUMORCHAIN, a novel multi-stage reasoning framework guided by humor theories. This system integrates visual semantic parsing, psychology-based humor reasoning, and a fine-tuned discriminator to evaluate humor, creating an interpretable and controllable cognitive reasoning chain. HUMORCHAIN is the first approach to explicitly incorporate cognitive humor structures into multimodal humor generation, enabling a structured progression from image understanding to humor creation. Experiments on various datasets (Meme-Image-No-Text, Oogiri-GO, OxfordTVG-HIC) demonstrate that HUMORCHAIN surpasses state-of-the-art methods in human humor preference, evaluation scores (Elo/BT), and semantic diversity. The results confirm that theory-driven structured reasoning enhances large language models' ability to produce humor that aligns with human perception. <div>
arXiv:2511.21732v1 Announce Type: new 
Abstract: Humor, as both a creative human activity and a social binding mechanism, has long posed a major challenge for AI generation. Although producing humor requires complex cognitive reasoning and social understanding, theories of humor suggest that it follows learnable patterns and structures, making it theoretically possible for generative models to acquire them implicitly. In recent years, multimodal humor has become a prevalent form of online communication, especially among Gen Z, highlighting the need for AI systems capable of integrating visual understanding with humorous language generation. However, existing data-driven approaches lack explicit modeling or theoretical grounding of humor, often producing literal descriptions that fail to capture its underlying cognitive mechanisms, resulting in the generated image descriptions that are fluent but lack genuine humor or cognitive depth. To address this limitation, we propose HUMORCHAIN (HUmor-guided Multi-step Orchestrated Reasoning Chain for Image Captioning), a theory-guided multi-stage reasoning framework. It integrates visual semantic parsing, humor- and psychology-based reasoning, and a fine-tuned discriminator for humor evaluation, forming an interpretable and controllable cognitive reasoning chain. To the best of our knowledge, this is the first work to explicitly embed cognitive structures from humor theories into multimodal humor generation, enabling a structured reasoning process from visual understanding to humor creation. Experiments on Meme-Image-No-Text, Oogiri-GO, and OxfordTVG-HIC datasets show that HUMORCHAIN outperforms state-of-the-art baselines in human humor preference, Elo/BT scores, and semantic diversity, demonstrating that theory-driven structured reasoning enables large language models to generate humor aligned with human perception.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RoSA: Enhancing Parameter-Efficient Fine-Tuning via RoPE-aware Selective Adaptation in Large Language Models</title>
<link>https://arxiv.org/abs/2511.21733</link>
<guid>https://arxiv.org/abs/2511.21733</guid>
<content:encoded><![CDATA[
<div> Keywords: Fine-tuning, Parameter-Efficient Fine-Tuning, Rotary Position Embeddings, Attention, LayerNorm gradient norms<br /><br />Summary:<br /><br />1. Fine-tuning large language models is crucial for adapting them to specific tasks, but it is computationally expensive.<br />2. Parameter-Efficient Fine-Tuning (PEFT) methods help reduce this cost but often fail to consider the unique roles of different model components and varying importance across layers.<br />3. The authors observe that Rotary Position Embeddings (RoPE) cause key activations in the low-frequency dimensions of attention states, which can be leveraged for more efficient tuning.<br />4. They propose RoPE-aware Selective Adaptation (RoSA), a novel PEFT framework that allocates trainable parameters more effectively by focusing on these low-frequency components and sensitive layers.<br />5. RoSA includes two main components: RoPE-aware Attention Enhancement (RoAE), which enhances low-frequency attention components, and Dynamic Layer Selection (DLS), which adaptively selects layers to fine-tune based on LayerNorm gradient norms.<br />6. By combining dimension-wise enhancement with layer-wise selection, RoSA achieves more targeted and efficient fine-tuning.<br />7. Experiments on fifteen commonsense and arithmetic benchmarks show that RoSA outperforms existing leading PEFT methods while using a comparable number of trainable parameters.<br />8. The authors provide publicly accessible code to support reproducibility at the given GitHub repository. <div>
arXiv:2511.21733v1 Announce Type: new 
Abstract: Fine-tuning large language models is essential for task-specific adaptation, yet it remains computationally prohibitive. Parameter-Efficient Fine-Tuning (PEFT) methods have emerged as a solution, but current approaches typically ignore the distinct roles of model components and the heterogeneous importance across layers, thereby limiting adaptation efficiency. Motivated by the observation that Rotary Position Embeddings (RoPE) induce critical activations in the low-frequency dimensions of attention states, we propose RoPE-aware Selective Adaptation (RoSA), a novel PEFT framework that allocates trainable parameters in a more targeted and effective manner. RoSA comprises a RoPE-aware Attention Enhancement (RoAE) module, which selectively enhances the low-frequency components of RoPE-influenced attention states, and a Dynamic Layer Selection (DLS) strategy that adaptively identifies and updates the most critical layers based on LayerNorm gradient norms. By combining dimension-wise enhancement with layer-wise adaptation, RoSA achieves more targeted and efficient fine-tuning. Extensive experiments on fifteen commonsense and arithmetic benchmarks demonstrate that RoSA outperforms existing mainstream PEFT methods under comparable trainable parameters. The code is available to ease reproducibility at https://github.com/Applied-Machine-Learning-Lab/RoSA.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Asking LLMs to Verify First is Almost Free Lunch</title>
<link>https://arxiv.org/abs/2511.21734</link>
<guid>https://arxiv.org/abs/2511.21734</guid>
<content:encoded><![CDATA[
<div> Keywords: Verification-First, Large Language Models, reverse reasoning, Chain-of-Thought, Iter-VF<br /><br />Summary:<br /><br />The paper introduces the Verification-First (VF) strategy to improve the reasoning capabilities of Large Language Models (LLMs) without incurring significant training costs or heavy test-time computation. VF prompts the model to first verify a given candidate answer, which could be trivial or random, before attempting to generate a full solution. This verification step activates a "reverse reasoning" process that is cognitively easier for the model and complements the traditional forward Chain-of-Thought (CoT) method, helping reduce logical errors via critical thinking. The VF approach is further extended into Iter-VF, a sequential test-time scaling method that continually cycles between verification and answer generation using the model’s previous output. Extensive experiments show that applying VF with a random initial answer consistently outperforms standard CoT across diverse benchmarks, including mathematical reasoning, coding, and agentic tasks, and using various LLMs ranging from small open-source models (1B parameters) to state-of-the-art commercial models. Iter-VF achieves better performance than existing test-time scaling strategies, offering a cost-effective and generalizable way to enhance LLM reasoning with minimal computational overhead. <div>
arXiv:2511.21734v1 Announce Type: new 
Abstract: To enhance the reasoning capabilities of Large Language Models (LLMs) without high costs of training, nor extensive test-time sampling, we introduce Verification-First (VF), a strategy that prompts models to verify a provided candidate answer, even a trivial or random one, before generating a solution. This approach triggers a "reverse reasoning" process that is cognitively easier and complementary to standard forward Chain-of-Thought (CoT), effectively invoking the model's critical thinking to reduce logical errors. We further generalize the VF strategy to Iter-VF, a sequential test-time scaling (TTS) method that iteratively cycles the verification-generation process using the model's previous answer. Extensive experiments across various benchmarks (from mathematical reasoning to coding and agentic tasks) and various LLMs (from open-source 1B to cutting-edge commercial ones) confirm that VF with random answer consistently outperforms standard CoT with minimal computational overhead, and Iter-VF outperforms existing TTS strategies.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Closing the Performance Gap Between AI and Radiologists in Chest X-Ray Reporting</title>
<link>https://arxiv.org/abs/2511.21735</link>
<guid>https://arxiv.org/abs/2511.21735</guid>
<content:encoded><![CDATA[
<div> Keywords: MAIRA-X, chest X-ray, AI report generation, lines and tubes, radiologist workload<br /><br />Summary:  
1. MAIRA-X is a multimodal AI model designed for longitudinal chest X-ray (CXR) report generation, addressing both clinical findings and lines and tubes (L&amp;T) interpretation.  
2. The model was developed using a large-scale, multi-site longitudinal dataset of 3.1 million studies (6 million images) from 806,000 patients at the Mayo Clinic.  
3. MAIRA-X was evaluated on three holdout datasets and the public MIMIC-CXR dataset, demonstrating significant improvements over previous AI models in lexical quality, clinical accuracy, and L&amp;T-specific reporting elements.  
4. A novel L&amp;T-specific metrics framework was introduced to validate accuracy in reporting attributes such as L&amp;T type, longitudinal changes, and placement.  
5. A retrospective user study involving nine radiologists with varied experience reviewed 600 studies blindly, showing comparable rates of critical errors between original and AI-generated reports (3.0% vs. 4.6%) and similar acceptability of sentences (97.8% vs. 97.4%).  
6. The results indicate that MAIRA-X can effectively assist radiologists, particularly in high-volume clinical environments, reducing workload while maintaining diagnostic accuracy and report quality. <div>
arXiv:2511.21735v1 Announce Type: new 
Abstract: AI-assisted report generation offers the opportunity to reduce radiologists' workload stemming from expanded screening guidelines, complex cases and workforce shortages, while maintaining diagnostic accuracy. In addition to describing pathological findings in chest X-ray reports, interpreting lines and tubes (L&amp;T) is demanding and repetitive for radiologists, especially with high patient volumes. We introduce MAIRA-X, a clinically evaluated multimodal AI model for longitudinal chest X-ray (CXR) report generation, that encompasses both clinical findings and L&amp;T reporting. Developed using a large-scale, multi-site, longitudinal dataset of 3.1 million studies (comprising 6 million images from 806k patients) from Mayo Clinic, MAIRA-X was evaluated on three holdout datasets and the public MIMIC-CXR dataset, where it significantly improved AI-generated reports over the state of the art on lexical quality, clinical correctness, and L&amp;T-related elements. A novel L&amp;T-specific metrics framework was developed to assess accuracy in reporting attributes such as type, longitudinal change and placement. A first-of-its-kind retrospective user evaluation study was conducted with nine radiologists of varying experience, who blindly reviewed 600 studies from distinct subjects. The user study found comparable rates of critical errors (3.0% for original vs. 4.6% for AI-generated reports) and a similar rate of acceptable sentences (97.8% for original vs. 97.4% for AI-generated reports), marking a significant improvement over prior user studies with larger gaps and higher error rates. Our results suggest that MAIRA-X can effectively assist radiologists, particularly in high-volume clinical settings.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>R2Q: Towards Robust 2-Bit Large Language Models via Residual Refinement Quantization</title>
<link>https://arxiv.org/abs/2511.21736</link>
<guid>https://arxiv.org/abs/2511.21736</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, 2-bit quantization, Residual Refinement Quantization, adaptive quantization lattice, quantization-aware training<br /><br />Summary:<br /><br />The paper addresses the significant computational and memory challenges posed by Large Language Models (LLMs) through low-bit quantization techniques. While 8-bit and 4-bit quantizations are common, pushing quantization down to 2 bits results in notable accuracy loss. To overcome this, the authors introduce Residual Refinement Quantization (R2Q), a novel 2-bit quantization framework that breaks the process into two sequential 1-bit sub-quantizations, effectively creating an adaptive quantization lattice. This approach refines quantization via a residual learning mechanism, which enhances model performance, improves training stability, and accelerates convergence despite extreme compression. The method has been extensively evaluated on prominent LLMs such as LLaMA, OPT, and Qwen across a range of tasks including question answering, commonsense reasoning, and language modeling. Results indicate that R2Q consistently outperforms existing 2-bit quantization methods in both fine-grained and coarse-grained settings. Additionally, the modular design of R2Q allows easy integration with current quantization-aware training (QAT) frameworks, facilitating broader adoption and practical implementation. Overall, R2Q presents an effective solution to enable more efficient LLM deployment without severe accuracy degradation at very low-bit quantization levels. <div>
arXiv:2511.21736v1 Announce Type: new 
Abstract: The rapid progress of Large Language Models (LLMs) has brought substantial computational and memory demands, spurring the adoption of low-bit quantization. While 8-bit and 4-bit formats have become prevalent, extending quantization to 2 bits remains challenging due to severe accuracy degradation. To address this, we propose Residual Refinement Quantization (R2Q)-a novel 2-bit quantization framework that decomposes the process into two sequential 1-bit sub-quantizations, forming an adaptive quantization lattice. Extensive evaluations on Llama, OPT, and Qwen across diverse benchmarks-covering question answering, commonsense reasoning, and language modeling-demonstrate that R2Q consistently outperforms existing 2-bit quantization methods in both fine-grained and coarse-grained settings. By refining quantization through a residual learning mechanism, R2Q enhances performance, improves training stability, and accelerates convergence under extreme compression. Furthermore, its modular design enables seamless integration with existing quantization-aware training (QAT) frameworks.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Polarity-Aware Probing for Quantifying Latent Alignment in Language Models</title>
<link>https://arxiv.org/abs/2511.21737</link>
<guid>https://arxiv.org/abs/2511.21737</guid>
<content:encoded><![CDATA[
<div> Keywords: Contrast-Consistent Search, Polarity-Aware CCS, model alignment, latent harmful knowledge, interpretability benchmarks<br /><br />Summary:  
1. This paper examines unsupervised probing methods, focusing on Contrast-Consistent Search (CCS), which reveals latent model beliefs without relying on token outputs, to assess model alignment reliability.  
2. It introduces Polarity-Aware CCS (PA-CCS), a novel method designed to evaluate if a model’s internal representations maintain consistency when the polarity of statements is inverted.  
3. Two new metrics, Polar-Consistency and the Contradiction Index, are proposed to quantitatively measure the semantic robustness of a model’s latent knowledge related to alignment.  
4. The authors curate three datasets comprising matched harmful-safe sentence pairs, constructed via different approaches including concurrent and antagonistic statement generation, to validate PA-CCS.  
5. PA-CCS is applied to 16 language models, revealing differences in how latent harmful knowledge is encoded across architectures and specific layers.  
6. An experiment replacing negation tokens with meaningless markers shows degradation in PA-CCS scores only in models with well-aligned internal representations, demonstrating the method’s sensitivity to structural robustness.  
7. The findings underscore the potential of unsupervised probing for evaluating alignment and advocate for incorporating structural robustness checks into interpretability benchmarks to better capture model behavior.  
8. The paper also provides code and datasets publicly at the linked GitHub repository for further research. <div>
arXiv:2511.21737v1 Announce Type: new 
Abstract: Advances in unsupervised probes such as Contrast-Consistent Search (CCS), which reveal latent beliefs without relying on token outputs, raise the question of whether these methods can reliably assess model alignment. We investigate this by examining the sensitivity of CCS to harmful vs. safe statements and by introducing Polarity-Aware CCS (PA-CCS), a method for evaluating whether a model's internal representations remain consistent under polarity inversion. We propose two alignment-oriented metrics, Polar-Consistency and the Contradiction Index, to quantify the semantic robustness of a model's latent knowledge. To validate PA-CCS, we curate two main datasets and one control dataset containing matched harmful-safe sentence pairs constructed using different methodologies (concurrent and antagonistic statements). We apply PA-CCS to 16 language models. Our results show that PA-CCS identifies both architectural and layer-specific differences in the encoding of latent harmful knowledge. Notably, replacing the negation token with a meaningless marker degrades PA-CCS scores for models with well-aligned internal representations, while models lacking robust internal calibration do not exhibit this degradation. Our findings highlight the potential of unsupervised probing for alignment evaluation and emphasize the need to incorporate structural robustness checks into interpretability benchmarks. Code and datasets are available at: https://github.com/SadSabrina/polarity-probing. WARNING: This paper contains potentially sensitive, harmful, and offensive content.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoding inner speech with an end-to-end brain-to-text neural interface</title>
<link>https://arxiv.org/abs/2511.21740</link>
<guid>https://arxiv.org/abs/2511.21740</guid>
<content:encoded><![CDATA[
<div> Keywords: brain-computer interfaces, speech decoding, end-to-end neural network, large language models, imagined speech<br /><br />Summary:<br /><br />1. This work presents Brain-to-Text (BIT), an end-to-end framework that translates neural activity directly into coherent sentences using a single differentiable neural network, overcoming limitations of cascaded speech brain-computer interface (BCI) systems that decode phonemes followed by sentence assembly with n-gram language models.<br /><br />2. Central to BIT is a cross-task, cross-species pretrained neural encoder whose learned representations effectively transfer to both attempted and imagined speech scenarios, enhancing generalizability across different speech modalities.<br /><br />3. When used in a cascaded setting with an n-gram language model, the pretrained encoder achieves new state-of-the-art performance on the Brain-to-Text benchmarks from 2024 and 2025.<br /><br />4. BIT integrates audio large language models (LLMs) into the end-to-end training pipeline with contrastive learning for cross-modal alignment, significantly reducing the word error rate (WER) from 24.69% to 10.22% compared to prior end-to-end methods.<br /><br />5. The study highlights that even small-scale audio LLMs substantially improve decoding accuracy, and BIT’s alignment of attempted and imagined speech embeddings enables robust cross-task generalization, advancing the use of large, diverse neural datasets for seamless, differentiable speech decoding optimization in BCIs. <div>
arXiv:2511.21740v1 Announce Type: new 
Abstract: Speech brain-computer interfaces (BCIs) aim to restore communication for people with paralysis by translating neural activity into text. Most systems use cascaded frameworks that decode phonemes before assembling sentences with an n-gram language model (LM), preventing joint optimization of all stages simultaneously. Here, we introduce an end-to-end Brain-to-Text (BIT) framework that translates neural activity into coherent sentences using a single differentiable neural network. Central to our approach is a cross-task, cross-species pretrained neural encoder, whose representations transfer to both attempted and imagined speech. In a cascaded setting with an n-gram LM, the pretrained encoder establishes a new state-of-the-art (SOTA) on the Brain-to-Text '24 and '25 benchmarks. Integrated end-to-end with audio large language models (LLMs) and trained with contrastive learning for cross-modal alignment, BIT reduces the word error rate (WER) of the prior end-to-end method from 24.69% to 10.22%. Notably, we find that small-scale audio LLMs markedly improve end-to-end decoding. Beyond record-setting performance, BIT aligns attempted and imagined speech embeddings to enable cross-task generalization. Altogether, our approach advances the integration of large, diverse neural datasets, paving the way for an end-to-end decoding framework that supports seamless, differentiable optimization.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multiscale Geometric Method for Capturing Relational Topic Alignment</title>
<link>https://arxiv.org/abs/2511.21741</link>
<guid>https://arxiv.org/abs/2511.21741</guid>
<content:encoded><![CDATA[
<div> Keywords: interpretable topic modeling, niche topics, transformer embeddings, Hellinger distances, hierarchical dendrogram  

<br /><br />Summary:  
1. The paper addresses the challenge of detecting underrepresented niche topics within scientific research communities, which is crucial for understanding evolving research interests.  
2. It critiques current topic modeling approaches that rely on dense transformer embeddings, noting their tendency to overlook rare topics and lack smooth temporal alignment.  
3. The authors propose a novel geometric method that integrates both text data and co-author network information, employing Hellinger distances combined with Ward's linkage to build a hierarchical topic dendrogram.  
4. This approach effectively captures both local nuances and global topic structures, enabling multiscale learning across semantic and temporal dimensions.  
5. Experimental results demonstrate that the proposed method excels in identifying rare topics and visualizing gradual topic shifts over time, highlighting the advantages of interpretable bag-of-words models enhanced by rigorous geometric alignment techniques. <div>
arXiv:2511.21741v1 Announce Type: new 
Abstract: Interpretable topic modeling is essential for tracking how research interests evolve within co-author communities. In scientific corpora, where novelty is prized, identifying underrepresented niche topics is particularly important. However, contemporary models built from dense transformer embeddings tend to miss rare topics and therefore also fail to capture smooth temporal alignment. We propose a geometric method that integrates multimodal text and co-author network data, using Hellinger distances and Ward's linkage to construct a hierarchical topic dendrogram. This approach captures both local and global structure, supporting multiscale learning across semantic and temporal dimensions. Our method effectively identifies rare-topic structure and visualizes smooth topic drift over time. Experiments highlight the strength of interpretable bag-of-words models when paired with principled geometric alignment.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EduMod-LLM: A Modular Approach for Designing Flexible and Transparent Educational Assistants</title>
<link>https://arxiv.org/abs/2511.21742</link>
<guid>https://arxiv.org/abs/2511.21742</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Function Calling, Educational QA Systems, Retrieval Methods, Modular Pipeline<br /><br />Summary: This paper introduces EduMod-LLM, a modular function-calling pipeline designed for educational Question-Answering (QA) systems powered by Large Language Models (LLMs). The study critically evaluates the pipeline by isolating and analyzing three major components: function-calling strategies, retrieval methods, and generative language models, enabling pinpoint identification of performance issues. The authors benchmark function-calling capabilities across different LLMs, revealing varying effectiveness and failure modes in how functions are invoked. They propose a novel structure-aware retrieval method and compare its performance against established vector-based and LLM-scoring retrieval baselines, finding improved interpretability and relevance. Various generative LLMs are also assessed for their ability to synthesize accurate and pedagogically aligned responses in educational contexts. The modular design facilitates transparent evaluation, supporting improvements in system explainability and alignment with educational goals. Overall, the research highlights the importance of modular function calling to enhance transparency, interpretability, and performance in LLM-based educational QA systems, promising more effective and trustworthy AI tools for learning environments. The authors provide supplementary materials and a project website for further exploration. <div>
arXiv:2511.21742v1 Announce Type: new 
Abstract: With the growing use of Large Language Model (LLM)-based Question-Answering (QA) systems in education, it is critical to evaluate their performance across individual pipeline components. In this work, we introduce {\model}, a modular function-calling LLM pipeline, and present a comprehensive evaluation along three key axes: function calling strategies, retrieval methods, and generative language models. Our framework enables fine-grained analysis by isolating and assessing each component. We benchmark function-calling performance across LLMs, compare our novel structure-aware retrieval method to vector-based and LLM-scoring baselines, and evaluate various LLMs for response synthesis. This modular approach reveals specific failure modes and performance patterns, supporting the development of interpretable and effective educational QA systems. Our findings demonstrate the value of modular function calling in improving system transparency and pedagogical alignment. Website and Supplementary Material: https://chancharikmitra.github.io/EduMod-LLM-website/
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Competence, Shrinking Reasoning: Cognitive Signatures in Language Model Learning</title>
<link>https://arxiv.org/abs/2511.21743</link>
<guid>https://arxiv.org/abs/2511.21743</guid>
<content:encoded><![CDATA[
<div> Keywords: reasoning tokens, language models, Four Stages of Competence, training dynamics, task-specific fine-tuning<br /><br />Summary:<br /><br />This paper investigates the reasoning process in language models during task-specific fine-tuning, comparing intermediate reasoning tokens to human working memory. It aligns model training dynamics with the Four Stages of Competence, where models initially produce wrong outputs without reasoning, then start reasoning but fail, then reason effectively, and finally solve tasks without explicit reasoning. The length of reasoning tokens increases as model performance improves, peaking at the conscious competence stage, then decreases as the model internalizes the task. Importantly, models maintain their performance even after reasoning tokens are removed post-training, indicating that reasoning acts as a scaffold for learning rather than being required for task execution. This progression provides practical insights, suggesting that tracking reasoning token dynamics can diagnose training stages, signal convergence, and support early stopping decisions. The authors propose specific metrics to monitor this trajectory and argue that observing reasoning behavior is crucial for understanding and enhancing the training of reasoning-capable language models. <div>
arXiv:2511.21743v1 Announce Type: new 
Abstract: We analyze reasoning in language models during task-specific fine-tuning and draws parallel between reasoning tokens--intermediate steps generated while solving problem and the human working memory. Drawing from cognitive science, we align training dynamics with the Four Stages of Competence: models initially produce incorrect outputs without reasoning, then begin reasoning (but still fail), eventually reason effectively, and finally solve tasks without explicit reasoning. We find that reasoning token length expands as performance improves, peaks at the stage of conscious competence, then declines as the model internalizes the task. Notably, after training, models retain performance even when reasoning is removed--suggesting it scaffolded learning but is no longer needed. This progression offers actionable insights: reasoning token dynamics can serve as a signal for diagnosing training stage, identifying convergence, and guiding early stopping. We propose metrics to track this trajectory and argue that reasoning behavior is valuable for understanding and optimizing reasoning model training.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Lightweight Approach to Detection of AI-Generated Texts Using Stylometric Features</title>
<link>https://arxiv.org/abs/2511.21744</link>
<guid>https://arxiv.org/abs/2511.21744</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-generated text detection, lightweight models, stylometric features, convolutional neural network, random forest<br /><br />Summary:<br /><br />1. The paper addresses the growing concern over the detection of AI-generated texts and notes that existing detection methods relying on fine-tuned large transformers or ensembles are computationally expensive and often lack generalization across domains.<br /><br />2. The authors propose NEULIF, a lightweight detection approach that decomposes texts into stylometric and readability features and classifies them using either a compact Convolutional Neural Network (CNN) or a Random Forest (RF).<br /><br />3. Evaluations on the Kaggle AI vs. Human corpus show that NEULIF achieves high accuracy, with the CNN model reaching 97% accuracy (~0.95 F1) and the Random Forest model reaching 95% accuracy (~0.94 F1), alongside strong precision, recall, and ROC-AUC scores (99.5% for CNN and 95% for RF).<br /><br />4. The NEULIF models are significantly smaller in size (CNN ~25 MB, RF ~10.6 MB) compared to transformer-based ensembles, allowing efficient execution on standard CPUs without compromising accuracy.<br /><br />5. The study emphasizes the potential for NEULIF to be applied across multiple languages, domains, and streaming data contexts, demonstrating that simpler, structurally informed models can match the effectiveness of more complex solutions in AI content detection. <div>
arXiv:2511.21744v1 Announce Type: new 
Abstract: A growing number of AI-generated texts raise serious concerns. Most existing approaches to AI-generated text detection rely on fine-tuning large transformer models or building ensembles, which are computationally expensive and often provide limited generalization across domains. Existing lightweight alternatives achieved significantly lower accuracy on large datasets. We introduce NEULIF, a lightweight approach that achieves best performance in the lightweight detector class, that does not require extensive computational power and provides high detection accuracy. In our approach, a text is first decomposed into stylometric and readability features which are then used for classification by a compact Convolutional Neural Network (CNN) or Random Forest (RF). Evaluated and tested on the Kaggle AI vs. Human corpus, our models achieve 97% accuracy (~ 0.95 F1) for CNN and 95% accuracy (~ 0.94 F1) for the Random Forest, demonstrating high precision and recall, with ROC-AUC scores of 99.5% and 95%, respectively. The CNN (~ 25 MB) and Random Forest (~ 10.6 MB) models are orders of magnitude smaller than transformer-based ensembles and can be run efficiently on standard CPU devices, without sacrificing accuracy.This study also highlights the potential of such models for broader applications across languages, domains, and streaming contexts, showing that simplicity, when guided by structural insights, can rival complexity in AI-generated content detection.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DELTA: Language Diffusion-based EEG-to-Text Architecture</title>
<link>https://arxiv.org/abs/2511.21746</link>
<guid>https://arxiv.org/abs/2511.21746</guid>
<content:encoded><![CDATA[
<div> EEG-to-text, Residual Vector Quantization, Masked Language Diffusion Model, Semantic Alignment, Multimodal EEG-language Models<br /><br />Summary: Electroencephalogram (EEG)-to-text conversion is a challenging task due to factors such as high-dimensional noise in EEG signals, variability across subjects, and the problem of error accumulation in conventional autoregressive decoding methods. This study introduces DELTA, a novel approach that combines a Residual Vector Quantization (RVQ) EEG tokenizer with a masked language diffusion model named LLaDA. The RVQ tokenizer discretizes continuous EEG data into multi-layer tokens, effectively reducing noise and minimizing individual subject differences. Meanwhile, LLaDA reconstructs the corresponding text through a non-sequential denoising process, which contrasts with traditional sequential decoding techniques. Evaluated on the ZuCo dataset, DELTA demonstrates a significant improvement in semantic alignment, surpassing autoregressive baselines by up to 5.37 points. In terms of performance metrics under word-level conditions, DELTA achieves a BLEU-1 score of 21.9 and a ROUGE-1 F score of 17.2. These promising results indicate that DELTA enables reliable text generation even from small EEG-text paired datasets. Overall, this work points toward the development of scalable multimodal models that integrate EEG signals with language processing capabilities, advancing the field of brain-computer interfaces and neural decoding. <div>
arXiv:2511.21746v1 Announce Type: new 
Abstract: Electroencephalogram (EEG)-to-text remains challenging due to high-dimensional noise, subject variability, and error accumulation in autoregressive decoding. We introduce DELTA, which pairs a Residual Vector Quantization (RVQ) EEG tokenizer with a masked language diffusion model (LLaDA). RVQ discretizes continuous EEG into multi-layer tokens to reduce noise and individual differences, while LLaDA reconstructs sentences via non-sequential denoising. On ZuCo, DELTA improves semantic alignment by up to 5.37 points over autoregressive baselines, achieving BLEU-1 21.9 and ROUGE-1 F 17.2 under word-level conditions. These results enable reliable text generation from small EEG-text datasets and point toward scalable multimodal EEG-language models.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Building Domain-Specific Small Language Models via Guided Data Generation</title>
<link>https://arxiv.org/abs/2511.21748</link>
<guid>https://arxiv.org/abs/2511.21748</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, domain-specific training, synthetic data generation, DiagnosticSLM, fault diagnosis  

<br /><br />Summary: This paper addresses the challenges of deploying Large Language Models (LLMs) in specialized domains, particularly the issues related to data privacy and computational demands of open-source models. The authors propose a cost-efficient and scalable training pipeline that leverages guided synthetic data generation from a limited seed corpus combined with a bottom-up domain data curation approach. Their method integrates Domain-Adaptive Pretraining (DAPT), Domain-specific Supervised Fine-tuning (DSFT), and Direct Preference Optimization (DPO) to develop smaller, domain-specialized LLMs. The approach is demonstrated through the development of DiagnosticSLM, a 3-billion-parameter model designed specifically for industrial fault diagnosis, root cause analysis, and repair recommendations. To rigorously evaluate the model’s effectiveness, four domain-specific benchmarks are introduced: DiagnosticMCQ (multiple-choice questions), DiagnosticQA (question answering), DiagnosticComp (sentence completion), and DiagnosticSum (summarization). DiagnosticSLM outperforms or matches larger or similarly sized open-source models (ranging from 2B to 9B parameters), achieving up to a 25% accuracy increase on the DiagnosticMCQ task. The results highlight the model’s strong domain-specific reasoning and generalization capabilities, showcasing the viability of small-scale, specialized LLMs trained via synthetic data and domain-specific fine-tuning. <div>
arXiv:2511.21748v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown remarkable success in supporting a wide range of knowledge-intensive tasks. In specialized domains, there is growing interest in leveraging LLMs to assist subject matter experts with domain-specific challenges. However, deploying LLMs as SaaS solutions raises data privacy concerns, while many open-source models demand significant computational resources for effective domain adaptation and deployment. A promising alternative is to develop smaller, domain-specialized LLMs, though this approach is often constrained by the lack of high-quality domain-specific training data. In this work, we address these limitations by presenting a cost-efficient and scalable training pipeline that combines guided synthetic data generation from a small seed corpus with bottom-up domain data curation. Our pipeline integrates Domain-Adaptive Pretraining (DAPT), Domain-specific Supervised Fine-tuning (DSFT), and Direct Preference Optimization (DPO) to train effective small-scale models for specialized use cases. We demonstrate this approach through DiagnosticSLM, a 3B-parameter domain-specific model tailored for fault diagnosis, root cause analysis, and repair recommendation in industrial settings. To evaluate model performance, we introduce four domain-specific benchmarks: multiple-choice questions (DiagnosticMCQ), question answering (DiagnosticQA), sentence completion (DiagnosticComp), and summarization (DiagnosticSum). DiagnosticSLM achieves up to 25% accuracy improvement over open-source models of comparable or larger size (2B-9B) on the MCQ task, while also outperforming or matching them in other tasks, demonstrating effective domain-specific reasoning and generalization capabilities.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Proactive Defense: Compound AI for Detecting Persuasion Attacks and Measuring Inoculation Effectiveness</title>
<link>https://arxiv.org/abs/2511.21749</link>
<guid>https://arxiv.org/abs/2511.21749</guid>
<content:encoded><![CDATA[
<div> Keywords: BRIES, persuasion attacks, detection, causal inference, language models<br /><br />Summary: This paper introduces BRIES, a novel AI architecture designed to detect and measure persuasion attacks across information environments. It consists of four specialized agents: Twister, which generates adversarial persuasive content; Detector, which identifies attack types with configurable parameters; Defender, which produces resilient content via inoculation; and Assessor, which uses causal inference to evaluate the effectiveness of inoculation. The system was tested using the SemEval 2023 Task 3 taxonomy on a synthetic persuasion dataset, revealing significant variation in detection performance across different language models. Notably, GPT-4 outperformed open-source models like Llama3 and Mistral in recognizing complex persuasion techniques, while the latter struggled with subtle rhetorical cues. The study highlights that different model architectures encode and process persuasive language differently. Additionally, prompt engineering, especially temperature settings and confidence scoring, dramatically influenced detection performance—GPT-4 and Gemma excelled at lower temperatures, whereas Llama3 and Mistral improved at higher ones. Causal analysis provided new insights into socio-emotional-cognitive patterns targeted by various persuasion attacks. Overall, this research advances generative AI safety and cognitive security by identifying model-specific vulnerabilities to persuasion and proposing a framework aimed at enhancing human cognitive resilience through pre-exposure interventions. <div>
arXiv:2511.21749v1 Announce Type: new 
Abstract: This paper introduces BRIES, a novel compound AI architecture designed to detect and measure the effectiveness of persuasion attacks across information environments. We present a system with specialized agents: a Twister that generates adversarial content employing targeted persuasion tactics, a Detector that identifies attack types with configurable parameters, a Defender that creates resilient content through content inoculation, and an Assessor that employs causal inference to evaluate inoculation effectiveness. Experimenting with the SemEval 2023 Task 3 taxonomy across the synthetic persuasion dataset, we demonstrate significant variations in detection performance across language agents. Our comparative analysis reveals significant performance disparities with GPT-4 achieving superior detection accuracy on complex persuasion techniques, while open-source models like Llama3 and Mistral demonstrated notable weaknesses in identifying subtle rhetorical, suggesting that different architectures encode and process persuasive language patterns in fundamentally different ways. We show that prompt engineering dramatically affects detection efficacy, with temperature settings and confidence scoring producing model-specific variations; Gemma and GPT-4 perform optimally at lower temperatures while Llama3 and Mistral show improved capabilities at higher temperatures. Our causal analysis provides novel insights into socio-emotional-cognitive signatures of persuasion attacks, revealing that different attack types target specific cognitive dimensions. This research advances generative AI safety and cognitive security by quantifying LLM-specific vulnerabilities to persuasion attacks and delivers a framework for enhancing human cognitive resilience through structured interventions before exposure to harmful content.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantics as a Shield: Label Disguise Defense (LDD) against Prompt Injection in LLM Sentiment Classification</title>
<link>https://arxiv.org/abs/2511.21752</link>
<guid>https://arxiv.org/abs/2511.21752</guid>
<content:encoded><![CDATA[
<div> Keywords: prompt injection, label disguise defense, few-shot learning, adversarial attacks, large language models

<br /><br />Summary:  
This paper addresses the vulnerability of large language models (LLMs) to prompt injection attacks, particularly class-directive injections that exploit knowledge of label sets to manipulate model outputs. Existing defense methods often require retraining or are susceptible to obfuscation attacks. To overcome these issues, the authors propose Label Disguise Defense (LDD), a lightweight, model-agnostic approach that replaces true labels with semantically transformed or unrelated alias labels (e.g., blue vs. yellow). Through few-shot demonstrations, the model implicitly learns these new label mappings, breaking the direct connection between injected prompts and final outputs. LDD was evaluated on nine state-of-the-art models, including GPT-5, GPT-4o, LLaMA3.2, Gemma3, and Mistral variants, under both few-shot and adversarial conditions. Results show that LDD effectively recovers some of the accuracy lost due to attacks across all tested models. The study also found that using semantically aligned alias labels (e.g., good vs. bad) provides stronger robustness than using unrelated or symbolic aliases (e.g., blue vs. yellow). Overall, the research demonstrates that manipulating label semantics offers a practical defense layer, transforming meaning into a shield that enhances resistance against prompt injection in text classification tasks. <div>
arXiv:2511.21752v1 Announce Type: new 
Abstract: Large language models are increasingly used for text classification tasks such as sentiment analysis, yet their reliance on natural language prompts exposes them to prompt injection attacks. In particular, class-directive injections exploit knowledge of the model's label set (e.g., positive vs. negative) to override its intended behavior through adversarial instructions. Existing defenses, such as detection-based filters, instruction hierarchies, and signed prompts, either require model retraining or remain vulnerable to obfuscation. This paper introduces Label Disguise Defense (LDD), a lightweight and model-agnostic strategy that conceals true labels by replacing them with semantically transformed or unrelated alias labels(e.g., blue vs. yellow). The model learns these new label mappings implicitly through few-shot demonstrations, preventing direct correspondence between injected directives and decision outputs. We evaluate LDD across nine state-of-the-art models, including GPT-5, GPT-4o, LLaMA3.2, Gemma3, and Mistral variants, under varying few-shot and an adversarial setting. Our results show that the ability of LDD to recover performance lost to the adversarial attack varies across models and alias choices. For every model evaluated, LDD is able to restore a portion of the accuracy degradation caused by the attack. Moreover, for the vast majority of models, we can identify more than one alias pair that achieves higher accuracy than the under-attack baseline, in which the model relies solely on few-shot learning without any defensive mechanism. A linguistic analysis further reveals that semantically aligned alias labels(e.g., good vs. bad) yield stronger robustness than unaligned symbols(e.g., blue vs. yellow). Overall, this study demonstrates that label semantics can serve as an effective defense layer, transforming meaning itself into a shield against prompt injection.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extracting Disaster Impacts and Impact Related Locations in Social Media Posts Using Large Language Models</title>
<link>https://arxiv.org/abs/2511.21753</link>
<guid>https://arxiv.org/abs/2511.21753</guid>
<content:encoded><![CDATA[
<div> Keywords: disaster impact, social media, large language models, location extraction, situational awareness<br /><br />Summary:  
1. Large-scale disasters cause severe consequences affecting people and infrastructure, with traditional authoritative data sources often hindered by atmospheric conditions, satellite revisit times, and temporal limitations, resulting in geo-temporal information gaps.  
2. Social media posts during disasters serve as "geo-sensors," providing real-time impact-related information from affected individuals describing specific locations and damages.  
3. However, not every location mentioned in disaster-related social media posts corresponds to a disaster impact; distinguishing impacted locations from non-impacted ones is crucial for efficient resource allocation.  
4. This research fine-tunes Large Language Models (LLMs) to accurately identify and extract all mentioned locations, impacts, and specifically impacted locations from disaster-related social media posts, including informal expressions and abbreviations.  
5. The fine-tuned LLM achieves F1-scores of 0.69 for impact extraction and 0.74 for impacted location extraction, significantly outperforming the pre-trained baseline, indicating its effectiveness as a scalable tool for enhancing situational awareness, timely decision-making, and post-disaster recovery planning. <div>
arXiv:2511.21753v1 Announce Type: new 
Abstract: Large-scale disasters can often result in catastrophic consequences on people and infrastructure. Situation awareness about such disaster impacts generated by authoritative data from in-situ sensors, remote sensing imagery, and/or geographic data is often limited due to atmospheric opacity, satellite revisits, and time limitations. This often results in geo-temporal information gaps. In contrast, impact-related social media posts can act as "geo-sensors" during a disaster, where people describe specific impacts and locations. However, not all locations mentioned in disaster-related social media posts relate to an impact. Only the impacted locations are critical for directing resources effectively. e.g., "The death toll from a fire which ripped through the Greek coastal town of #Mati stood at 80, with dozens of people unaccounted for as forensic experts tried to identify victims who were burned alive #Greecefires #AthensFires #Athens #Greece." contains impacted location "Mati" and non-impacted locations "Greece" and "Athens". This research uses Large Language Models (LLMs) to identify all locations, impacts and impacted locations mentioned in disaster-related social media posts. In the process, LLMs are fine-tuned to identify only impacts and impacted locations (as distinct from other, non-impacted locations), including locations mentioned in informal expressions, abbreviations, and short forms. Our fine-tuned model demonstrates efficacy, achieving an F1-score of 0.69 for impact and 0.74 for impacted location extraction, substantially outperforming the pre-trained baseline. These robust results confirm the potential of fine-tuned language models to offer a scalable solution for timely decision-making in resource allocation, situational awareness, and post-disaster recovery planning for responders.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dissecting the Ledger: Locating and Suppressing "Liar Circuits" in Financial Large Language Models</title>
<link>https://arxiv.org/abs/2511.21756</link>
<guid>https://arxiv.org/abs/2511.21756</guid>
<content:encoded><![CDATA[
<div> Arithmetic reasoning, hallucination detection, causal tracing, GPT-2 XL, financial AI<br /><br />Summary:<br /><br />This paper investigates the hallucinations large language models (LLMs) exhibit when performing arithmetic in financial domains, focusing on the GPT-2 XL model using the ConvFinQA benchmark. The authors identify a dual-stage arithmetic reasoning mechanism within the model: a distributed "computational scratchpad" spread across the middle layers (L12-L30), where intermediate calculations occur, and a critical "aggregation circuit" localized in the late layer (Layer 46), which consolidates outputs. They apply Causal Tracing to pinpoint these components and verify their significance via an ablation study. Specifically, suppressing Layer 46 results in an 81.8% reduction in the model’s confidence on hallucinatory arithmetic outputs, indicating this layer’s crucial role in producing deceptive results. Additionally, training a linear probe on Layer 46 allows the model to detect arithmetic hallucinations with 98% accuracy, even on financial topics unseen during training. This suggests the presence of a universal geometric representation related to arithmetic errors, providing a foundation for intrinsic hallucination detection strategies. The study moves beyond black-box approaches by mechanistically understanding where and how errors emerge inside the model, offering new directions for improving reliability in high-stakes financial applications. <div>
arXiv:2511.21756v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly deployed in high-stakes financial domains, yet they suffer from specific, reproducible hallucinations when performing arithmetic operations. Current mitigation strategies often treat the model as a black box. In this work, we propose a mechanistic approach to intrinsic hallucination detection. By applying Causal Tracing to the GPT-2 XL architecture on the ConvFinQA benchmark, we identify a dual-stage mechanism for arithmetic reasoning: a distributed computational scratchpad in middle layers (L12-L30) and a decisive aggregation circuit in late layers (specifically Layer 46). We verify this mechanism via an ablation study, demonstrating that suppressing Layer 46 reduces the model's confidence in hallucinatory outputs by 81.8%. Furthermore, we demonstrate that a linear probe trained on this layer generalizes to unseen financial topics with 98% accuracy, suggesting a universal geometry of arithmetic deception.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Orchestrating Dual-Boundaries: An Arithmetic Intensity Inspired Acceleration Framework for Diffusion Language Models</title>
<link>https://arxiv.org/abs/2511.21759</link>
<guid>https://arxiv.org/abs/2511.21759</guid>
<content:encoded><![CDATA[
<div> Diffusion-based LLMs, KV caching, adaptive length prediction, speculative decoding, inference acceleration<br /><br />Summary: Diffusion-based large language models (dLLMs) have attracted attention due to their strong performance and potential for parallel decoding. Existing methods improve inference speed using KV caching but face challenges from the bidirectional attention mechanism that requires frequent cache refreshes, resulting in costly and interleaved prefill and decoding stages. This limits overall speedup gains. To address this, the paper proposes ODB-dLLM, a novel framework leveraging dual-boundary orchestration to accelerate dLLM inference. During the prefill phase, ODB-dLLM introduces an adaptive length prediction mechanism that reduces redundant computation caused by fixed response lengths, thereby decreasing prefill overhead. For the decoding phase, the framework analyzes dLLM’s computational traits and presents a jump-share speculative decoding method tailored specifically to dLLMs, which cuts down decoding iterations and enhances efficiency. Experimental results reveal that ODB-dLLM achieves significant speedups of 46–162 times over the baseline dLLM and 2.63–6.30 times compared to Fast-dLLM, while also lessening the accuracy loss common to prior acceleration techniques. Overall, ODB-dLLM effectively balances speed and accuracy improvements in diffusion-based large language model inference. <div>
arXiv:2511.21759v1 Announce Type: new 
Abstract: Diffusion-based large language models (dLLMs) have recently gained significant attention for their exceptional performance and inherent potential for parallel decoding. Existing frameworks further enhance its inference efficiency by enabling KV caching. However, its bidirectional attention mechanism necessitates periodic cache refreshes that interleave prefill and decoding phases, both contributing substantial inference cost and constraining achievable speedup. Inspired by the heterogeneous arithmetic intensity of the prefill and decoding phases, we propose ODB-dLLM, a framework that orchestrates dual-boundaries to accelerate dLLM inference. In the prefill phase, we find that the predefined fixed response length introduces heavy yet redundant computational overhead, which affects efficiency. To alleviate this, ODB-dLLM incorporates an adaptive length prediction mechanism that progressively reduces prefill overhead and unnecessary computation. In the decoding phase, we analyze the computational characteristics of dLLMs and propose a dLLM-specific jump-share speculative decoding method to enhance efficiency by reducing the number of decoding iterations. Experimental results demonstrate that ODB-dLLM achieves 46-162x and 2.63-6.30x speedups over the baseline dLLM and Fast-dLLM, respectively, while simultaneously mitigating the accuracy degradation in existing acceleration frameworks.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>fMRI-LM: Towards a Universal Foundation Model for Language-Aligned fMRI Understanding</title>
<link>https://arxiv.org/abs/2511.21760</link>
<guid>https://arxiv.org/abs/2511.21760</guid>
<content:encoded><![CDATA[
<div> Keywords: fMRI-LM, multimodal LLM, neural tokenizer, language alignment, instruction tuning

<br /><br />Summary: The paper introduces fMRI-LM, a foundational model designed to bridge functional MRI (fMRI) data and language, enabling unified multimodal reasoning that incorporates brain imaging. The development follows a three-stage framework: first, a neural tokenizer translates fMRI signals into discrete tokens embedded in a language-consistent space, facilitating the integration of brain data with text. Second, a pretrained large language model (LLM) is adapted to jointly process these fMRI tokens alongside text, treating brain activity sequences as temporally predictable and linguistically describable. To address the scarcity of natural fMRI-text paired data, the authors create a large descriptive corpus that converts diverse imaging features into structured textual descriptors representing the low-level organization of fMRI signals. Third, multi-task and multi-paradigm instruction tuning is performed to imbue fMRI-LM with high-level semantic understanding, enabling it to support various downstream applications. The model demonstrates strong zero-shot and few-shot performance across benchmarks, showing efficient adaptability using parameter-efficient tuning methods like LoRA. Overall, fMRI-LM establishes a scalable approach toward a language-aligned, universal model capable of both structural and semantic interpretation of fMRI data. <div>
arXiv:2511.21760v1 Announce Type: new 
Abstract: Recent advances in multimodal large language models (LLMs) have enabled unified reasoning across images, audio, and video, but extending such capability to brain imaging remains largely unexplored. Bridging this gap is essential to link neural activity with semantic cognition and to develop cross-modal brain representations. To this end, we present fMRI-LM, a foundational model that bridges functional MRI (fMRI) and language through a three-stage framework. In Stage 1, we learn a neural tokenizer that maps fMRI into discrete tokens embedded in a language-consistent space. In Stage 2, a pretrained LLM is adapted to jointly model fMRI tokens and text, treating brain activity as a sequence that can be temporally predicted and linguistically described. To overcome the lack of natural fMRI-text pairs, we construct a large descriptive corpus that translates diverse imaging-based features into structured textual descriptors, capturing the low-level organization of fMRI signals. In Stage 3, we perform multi-task, multi-paradigm instruction tuning to endow fMRI-LM with high-level semantic understanding, supporting diverse downstream applications. Across various benchmarks, fMRI-LM achieves strong zero-shot and few-shot performance, and adapts efficiently with parameter-efficient tuning (LoRA), establishing a scalable pathway toward a language-aligned, universal model for structural and semantic understanding of fMRI.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs for Low-Resource Dialect Translation Using Context-Aware Prompting: A Case Study on Sylheti</title>
<link>https://arxiv.org/abs/2511.21761</link>
<guid>https://arxiv.org/abs/2511.21761</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Sylheti, low-resource translation, Context-Aware Prompting, machine translation<br /><br />Summary: This study examines the use of advanced Large Language Models (LLMs) for machine translation (MT) between Bangla and Sylheti, a low-resource dialect of Bangla. Five LLMs—GPT-4.1 (listed twice, possibly a typo), LLaMA 4, Grok 3, and DeepSeek V3.2—are evaluated to assess their ability to handle dialectal vocabulary, revealing consistent challenges in translation quality due to dialect-specific terms. To mitigate these issues, the authors propose Sylheti-CAP, a novel three-step Context-Aware Prompting framework. This framework incorporates a linguistic rulebook, a specialized dictionary containing 2,260 core vocabulary items and idioms, and an authenticity check embedded directly into the LLM prompts. Extensive experiments demonstrate that Sylheti-CAP significantly improves translation performance across different models and prompting techniques. Both automatic evaluation metrics and human judgments confirm the framework’s success in enhancing accuracy and naturalness of translations. Qualitative analysis further highlights reductions in common translation errors such as hallucinations, ambiguities, and awkward phrasing. The study thus establishes Sylheti-CAP as an effective and scalable strategy for addressing the challenges of dialectal and low-resource machine translation tasks. The dataset and resources are made publicly available to facilitate further research in this area. <div>
arXiv:2511.21761v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated strong translation abilities through prompting, even without task-specific training. However, their effectiveness in dialectal and low-resource contexts remains underexplored. This study presents the first systematic investigation of LLM-based machine translation (MT) for Sylheti, a dialect of Bangla that is itself low-resource. We evaluate five advanced LLMs (GPT-4.1, GPT-4.1, LLaMA 4, Grok 3, and DeepSeek V3.2) across both translation directions (Bangla $\Leftrightarrow$ Sylheti), and find that these models struggle with dialect-specific vocabulary. To address this, we introduce Sylheti-CAP (Context-Aware Prompting), a three-step framework that embeds a linguistic rulebook, a dictionary (2{,}260 core vocabulary items and idioms), and an authenticity check directly into prompts. Extensive experiments show that Sylheti-CAP consistently improves translation quality across models and prompting strategies. Both automatic metrics and human evaluations confirm its effectiveness, while qualitative analysis reveals notable reductions in hallucinations, ambiguities, and awkward phrasing, establishing Sylheti-CAP as a scalable solution for dialectal and low-resource MT. Dataset link: \href{https://github.com/TabiaTanzin/LLMs-for-Low-Resource-Dialect-Translation-Using-Context-Aware-Prompting-A-Case-Study-on-Sylheti.git}{https://github.com/TabiaTanzin/LLMs-for-Low-Resource-Dialect-Translation-Using-Context-Aware-Prompting-A-Case-Study-on-Sylheti.git}
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Factors That Support Grounded Responses in LLM Conversations: A Rapid Review</title>
<link>https://arxiv.org/abs/2511.21762</link>
<guid>https://arxiv.org/abs/2511.21762</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, alignment, hallucination mitigation, inference-time methods, reinforcement learning<br /><br />Summary: This review focuses on addressing key challenges in large language model (LLM) outputs, including misalignment with user intent, lack of contextual grounding, hallucinations, and topic drift, which impair their reliability in conversational applications. The authors conducted a Rapid Review using the PRISMA framework and PICO strategy to systematically identify and categorize alignment strategies throughout the LLM lifecycle. These strategies fall into three main categories: inference-time approaches, post-training methods, and reinforcement learning-based techniques. Among these, inference-time methods stand out for their efficiency, as they enable output alignment without requiring retraining, thus better supporting user intent and reducing hallucinations. The review highlights that these approaches provide structured mechanisms to improve response quality and reliability across multiple alignment objectives. By comparing different techniques in the context of conversational goals, the article offers a comprehensive overview of methods that enhance grounding and mitigate issues such as hallucination and topic drift. Overall, the study emphasizes the importance of choosing alignment strategies appropriate to the lifecycle phase to optimize LLM performance in practical deployments. <div>
arXiv:2511.21762v1 Announce Type: new 
Abstract: Large language models (LLMs) may generate outputs that are misaligned with user intent, lack contextual grounding, or exhibit hallucinations during conversation, which compromises the reliability of LLM-based applications. This review aimed to identify and analyze techniques that align LLM responses with conversational goals, ensure grounding, and reduce hallucination and topic drift. We conducted a Rapid Review guided by the PRISMA framework and the PICO strategy to structure the search, filtering, and selection processes. The alignment strategies identified were categorized according to the LLM lifecycle phase in which they operate: inference-time, post-training, and reinforcement learning-based methods. Among these, inference-time approaches emerged as particularly efficient, aligning outputs without retraining while supporting user intent, contextual grounding, and hallucination mitigation. The reviewed techniques provided structured mechanisms for improving the quality and reliability of LLM responses across key alignment objectives.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FLAWS: A Benchmark for Error Identification and Localization in Scientific Papers</title>
<link>https://arxiv.org/abs/2511.21843</link>
<guid>https://arxiv.org/abs/2511.21843</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, error localization, peer review, benchmark, scientific papers  

<br /><br />Summary:  
The article addresses the challenge of identifying and localizing errors in scientific papers, a critical task in peer review complicated by the rapid growth of scientific publications and limited expert availability. It explores the use of Large Language Models (LLMs) to support error detection in academic review and automated scientific assessment. To systematically evaluate LLMs on this task, the authors introduce FLAWS (Fault Localization Across Writing in Science), a novel benchmark consisting of 713 pairs of papers and deliberately inserted claim-invalidating errors. The benchmark is designed to be challenging and relevant by ensuring errors are well-defined and avoiding trivial artifacts, alongside a scalable automated metric for evaluation. The study benchmarks five leading LLMs—Claude Sonnet 4.5, DeepSeek Reasoner v3.1, Gemini 2.5 Pro, GPT 5, and Grok 4—on their ability to detect these errors. Among these, GPT 5 achieves the highest performance, correctly identifying errors with 39.1% accuracy when considering its top 10 candidate error text predictions. This work highlights both the promise and current limitations of LLMs in scientific error localization and provides a standardized testbed for future improvements in peer review support technology. <div>
arXiv:2511.21843v1 Announce Type: new 
Abstract: The identification and localization of errors is a core task in peer review, yet the exponential growth of scientific output has made it increasingly difficult for human reviewers to reliably detect errors given the limited pool of experts. Recent advances in Large Language Models (LLMs) have sparked interest in their potential to support such evaluation tasks, from academic peer review to automated scientific assessment. However, despite the growing use of LLMs in review systems, their capabilities to pinpoint errors remain underexplored. In this work, we introduce Fault Localization Across Writing in Science (FLAWS), an automated benchmark consisting of 713 paper-error pairs designed to evaluate how effectively LLMs detect errors that undermine key claims in research papers. We construct the benchmark by systematically inserting claim-invalidating errors into peer-reviewed papers using LLMs, paired with an automated evaluation metric that measures whether models can identify and localize these errors. Developing such a benchmark presents unique challenges that we overcome: ensuring that the inserted errors are well-defined, challenging, and relevant to the content of the paper, avoiding artifacts that would make identification trivial, and designing a scalable, automated evaluation metric. On the resulting benchmark, we evaluate five frontier LLMs: Claude Sonnet 4.5, DeepSeek Reasoner v3.1, Gemini 2.5 Pro, GPT 5, and Grok 4. Among these, GPT 5 is the top-performing model, achieving 39.1% identification accuracy when k=10, where k is the number of top-ranked error text candidates generated by the LLM.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Score Reliability of Multiple Choice Benchmarks with Consistency Evaluation and Altered Answer Choices</title>
<link>https://arxiv.org/abs/2511.21860</link>
<guid>https://arxiv.org/abs/2511.21860</guid>
<content:encoded><![CDATA[
<div> Consistency-Rebalanced Accuracy, Large Language Models, Multiple Choice Question Answering, Response Consistency, Evaluation Metrics  

<br /><br />Summary:  
This work introduces the Consistency-Rebalanced Accuracy (CoRA) metric designed to improve the assessment reliability of Large Language Models (LLMs) on multiple choice (MC) benchmarks. CoRA evaluates the consistency of LLM responses by leveraging synthetically-generated questions that have altered answer choices, thereby testing the stability of model predictions. The metric incorporates two intermediate scores: Bare-Minimum-Consistency Accuracy (BMCA) and Consistency Index (CI), which together adjust the standard multiple-choice question answering (MCQA) scores based on the model's response consistency. Evaluations conducted across various benchmarks and LLM architectures reveal that some models can achieve high MCQA scores despite exhibiting low response consistency. By applying CoRA, scores of these inconsistent models are scaled down, providing a more accurate reflection of their true performance. This approach highlights the importance of accounting for consistency when evaluating LLMs and offers a more robust metric to capture both accuracy and reliability in model predictions on MC tasks. <div>
arXiv:2511.21860v1 Announce Type: new 
Abstract: In this work we present the Consistency-Rebalanced Accuracy (CoRA) metric, improving the reliability of Large Language Model (LLM) scores computed on multiple choice (MC) benchmarks. Our metric explores the response consistency of the LLMs, taking advantage of synthetically-generated questions with altered answer choices. With two intermediate scores, i.e. Bare-Minimum-Consistency Accuracy (BMCA) and Consistency Index (CI), CoRA is computed by adjusting the multiple-choice question answering (MCQA) scores to better reflect the level of consistency of the LLM. We present evaluations in different benchmarks using diverse LLMs, and not only demonstrate that LLMs can present low response consistency even when they present high MCQA scores, but also that CoRA can successfully scale down the scores of inconsistent models.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Customer Journey in the Land of Oz: Leveraging the Wizard of Oz Technique to Model Emotions in Customer Service Interactions</title>
<link>https://arxiv.org/abs/2511.21909</link>
<guid>https://arxiv.org/abs/2511.21909</guid>
<content:encoded><![CDATA[
<div> Emotion recognition, Wizard of Oz experiment, emotion trajectories, bilingual dialogues, customer service  

<br /><br />Summary: This paper presents EmoWOZ-CS, a novel bilingual (Dutch-English) corpus of 2,148 written customer service dialogues collected through a controlled Wizard of Oz (WOZ) experiment targeting specific affective trajectories in commercial aviation, e-commerce, online travel, and telecommunications domains. The study evaluates WOZ-based operator-steered valence trajectories as a method for emotion research, demonstrating that conversation-level affect steering is feasible, especially for negative emotions, while positive and neutral trajectories tend to converge in final valence. Human annotation analysis reveals moderate agreement for multilabel emotions and valence but lower consistency for arousal and dominance dimensions. Notably, self-reported emotions often diverge from third-party judgments except for neutral, gratitude, and anger. Emotion distribution shows neutrality dominates participant messages, with desire and gratitude as the most frequent non-neutral emotions. Different interaction strategies yield varied emotional outcomes: objective approaches commonly elicit neutrality or gratitude, whereas suboptimal strategies increase anger, annoyance, disappointment, desire, and confusion. Some affective strategies, like cheerfulness and gratitude, encourage positive reciprocity, while apology and empathy may still evoke negative emotions. Benchmarks for emotion detection and proactive, forward-looking inference underscore the challenges of predicting future emotional states from prior dialogue turns, highlighting the complexity of designing emotion-aware real-time customer support systems. <div>
arXiv:2511.21909v1 Announce Type: new 
Abstract: Emotion-aware customer service needs in-domain conversational data, rich annotations, and predictive capabilities, but existing resources for emotion recognition are often out-of-domain, narrowly labeled, and focused on post-hoc detection. To address this, we conducted a controlled Wizard of Oz (WOZ) experiment to elicit interactions with targeted affective trajectories. The resulting corpus, EmoWOZ-CS, contains 2,148 bilingual (Dutch-English) written dialogues from 179 participants across commercial aviation, e-commerce, online travel agencies, and telecommunication scenarios. Our contributions are threefold: (1) Evaluate WOZ-based operator-steered valence trajectories as a design for emotion research; (2) Quantify human annotation performance and variation, including divergences between self-reports and third-party judgments; (3) Benchmark detection and forward-looking emotion inference in real-time support. Findings show neutral dominates participant messages; desire and gratitude are the most frequent non-neutral emotions. Agreement is moderate for multilabel emotions and valence, lower for arousal and dominance; self-reports diverge notably from third-party labels, aligning most for neutral, gratitude, and anger. Objective strategies often elicit neutrality or gratitude, while suboptimal strategies increase anger, annoyance, disappointment, desire, and confusion. Some affective strategies (cheerfulness, gratitude) foster positive reciprocity, whereas others (apology, empathy) can also leave desire, anger, or annoyance. Temporal analysis confirms successful conversation-level steering toward prescribed trajectories, most distinctly for negative targets; positive and neutral targets yield similar final valence distributions. Benchmarks highlight the difficulty of forward-looking emotion inference from prior turns, underscoring the complexity of proactive emotion-aware support.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tracing How Annotators Think: Augmenting Preference Judgments with Reading Processes</title>
<link>https://arxiv.org/abs/2511.21912</link>
<guid>https://arxiv.org/abs/2511.21912</guid>
<content:encoded><![CDATA[
arXiv:2511.21912v1 Announce Type: new 
Abstract: We propose an annotation approach that captures not only labels but also the reading process underlying annotators' decisions, e.g., what parts of the text they focus on, re-read or skim. Using this framework, we conduct a case study on the preference annotation task, creating a dataset PreferRead that contains fine-grained annotator reading behaviors obtained from mouse tracking. PreferRead enables detailed analysis of how annotators navigate between a prompt and two candidate responses before selecting their preference. We find that annotators re-read a response in roughly half of all trials, most often revisiting the option they ultimately choose, and rarely revisit the prompt. Reading behaviors are also significantly related to annotation outcomes: re-reading is associated with higher inter-annotator agreement, whereas long reading paths and times are associated with lower agreement. These results demonstrate that reading processes provide a complementary cognitive dimension for understanding annotator reliability, decision-making and disagreement in complex, subjective NLP tasks. Our code and data are publicly available.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Comparative Study of LLM Prompting and Fine-Tuning for Cross-genre Authorship Attribution on Chinese Lyrics</title>
<link>https://arxiv.org/abs/2511.21930</link>
<guid>https://arxiv.org/abs/2511.21930</guid>
<content:encoded><![CDATA[
arXiv:2511.21930v1 Announce Type: new 
Abstract: We propose a novel study on authorship attribution for Chinese lyrics, a domain where clean, public datasets are sorely lacking. Our contributions are twofold: (1) we create a new, balanced dataset of Chinese lyrics spanning multiple genres, and (2) we develop and fine-tune a domain-specific model, comparing its performance against zero-shot inference using the DeepSeek LLM.
  We test two central hypotheses. First, we hypothesize that a fine-tuned model will outperform a zero-shot LLM baseline. Second, we hypothesize that performance is genre-dependent. Our experiments strongly confirm Hypothesis 2: structured genres (e.g. Folklore & Tradition) yield significantly higher attribution accuracy than more abstract genres (e.g. Love & Romance). Hypothesis 1 receives only partial support: fine-tuning improves robustness and generalization in Test1 (real-world data and difficult genres), but offers limited or ambiguous gains in Test2, a smaller, synthetically-augmented set. We show that the design limitations of Test2 (e.g., label imbalance, shallow lexical differences, and narrow genre sampling) can obscure the true effectiveness of fine-tuning.
  Our work establishes the first benchmark for cross-genre Chinese lyric attribution, highlights the importance of genre-sensitive evaluation, and provides a public dataset and analytical framework for future research. We conclude with recommendations: enlarge and diversify test sets, reduce reliance on token-level data augmentation, balance author representation across genres, and investigate domain-adaptive pretraining as a pathway for improved attribution performance.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Start Making Sense(s): A Developmental Probe of Attention Specialization Using Lexical Ambiguity</title>
<link>https://arxiv.org/abs/2511.21974</link>
<guid>https://arxiv.org/abs/2511.21974</guid>
<content:encoded><![CDATA[
arXiv:2511.21974v1 Announce Type: new 
Abstract: Despite an in-principle understanding of self-attention matrix operations in Transformer language models (LMs), it remains unclear precisely how these operations map onto interpretable computations or functions--and how or when individual attention heads develop specialized attention patterns. Here, we present a pipeline to systematically probe attention mechanisms, and we illustrate its value by leveraging lexical ambiguity--where a single word has multiple meanings--to isolate attention mechanisms that contribute to word sense disambiguation. We take a "developmental" approach: first, using publicly available Pythia LM checkpoints, we identify inflection points in disambiguation performance for each LM in the suite; in 14M and 410M, we identify heads whose attention to disambiguating words covaries with overall disambiguation performance across development. We then stress-test the robustness of these heads to stimulus perturbations: in 14M, we find limited robustness, but in 410M, we identify multiple heads with surprisingly generalizable behavior. Then, in a causal analysis, we find that ablating the target heads demonstrably impairs disambiguation performance, particularly in 14M. We additionally reproduce developmental analyses of 14M across all of its random seeds. Together, these results suggest: that disambiguation benefits from a constellation of mechanisms, some of which (especially in 14M) are highly sensitive to the position and part-of-speech of the disambiguating cue; and that larger models (410M) may contain heads with more robust disambiguation behavior. They also join a growing body of work that highlights the value of adopting a developmental perspective when probing LM mechanisms.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AfriStereo: A Culturally Grounded Dataset for Evaluating Stereotypical Bias in Large Language Models</title>
<link>https://arxiv.org/abs/2511.22016</link>
<guid>https://arxiv.org/abs/2511.22016</guid>
<content:encoded><![CDATA[
arXiv:2511.22016v1 Announce Type: new 
Abstract: Existing AI bias evaluation benchmarks largely reflect Western perspectives, leaving African contexts underrepresented and enabling harmful stereotypes in applications across various domains. To address this gap, we introduce AfriStereo, the first open-source African stereotype dataset and evaluation framework grounded in local socio-cultural contexts. Through community engaged efforts across Senegal, Kenya, and Nigeria, we collected 1,163 stereotypes spanning gender, ethnicity, religion, age, and profession. Using few-shot prompting with human-in-the-loop validation, we augmented the dataset to over 5,000 stereotype-antistereotype pairs. Entries were validated through semantic clustering and manual annotation by culturally informed reviewers. Preliminary evaluation of language models reveals that nine of eleven models exhibit statistically significant bias, with Bias Preference Ratios (BPR) ranging from 0.63 to 0.78 (p <= 0.05), indicating systematic preferences for stereotypes over antistereotypes, particularly across age, profession, and gender dimensions. Domain-specific models appeared to show weaker bias in our setup, suggesting task-specific training may mitigate some associations. Looking ahead, AfriStereo opens pathways for future research on culturally grounded bias evaluation and mitigation, offering key methodologies for the AI community on building more equitable, context-aware, and globally inclusive NLP technologies.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ResearchArcade: Graph Interface for Academic Tasks</title>
<link>https://arxiv.org/abs/2511.22036</link>
<guid>https://arxiv.org/abs/2511.22036</guid>
<content:encoded><![CDATA[
arXiv:2511.22036v1 Announce Type: new 
Abstract: Academic research generates diverse data sources, and as researchers increasingly use machine learning to assist research tasks, a crucial question arises: Can we build a unified data interface to support the development of machine learning models for various academic tasks? Models trained on such a unified interface can better support human researchers throughout the research process, eventually accelerating knowledge discovery. In this work, we introduce ResearchArcade, a graph-based interface that connects multiple academic data sources, unifies task definitions, and supports a wide range of base models to address key academic challenges. ResearchArcade utilizes a coherent multi-table format with graph structures to organize data from different sources, including academic corpora from ArXiv and peer reviews from OpenReview, while capturing information with multiple modalities, such as text, figures, and tables. ResearchArcade also preserves temporal evolution at both the manuscript and community levels, supporting the study of paper revisions as well as broader research trends over time. Additionally, ResearchArcade unifies diverse academic task definitions and supports various models with distinct input requirements. Our experiments across six academic tasks demonstrate that combining cross-source and multi-modal information enables a broader range of tasks, while incorporating graph structures consistently improves performance over baseline methods. This highlights the effectiveness of ResearchArcade and its potential to advance research progress.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Early Risk Prediction with Temporally and Contextually Grounded Clinical Language Processing</title>
<link>https://arxiv.org/abs/2511.22038</link>
<guid>https://arxiv.org/abs/2511.22038</guid>
<content:encoded><![CDATA[
arXiv:2511.22038v1 Announce Type: new 
Abstract: Clinical notes in Electronic Health Records (EHRs) capture rich temporal information on events, clinician reasoning, and lifestyle factors often missing from structured data. Leveraging them for predictive modeling can be impactful for timely identification of chronic diseases. However, they present core natural language processing (NLP) challenges: long text, irregular event distribution, complex temporal dependencies, privacy constraints, and resource limitations. We present two complementary methods for temporally and contextually grounded risk prediction from longitudinal notes. First, we introduce HiTGNN, a hierarchical temporal graph neural network that integrates intra-note temporal event structures, inter-visit dynamics, and medical knowledge to model patient trajectories with fine-grained temporal granularity. Second, we propose ReVeAL, a lightweight, test-time framework that distills the reasoning of large language models into smaller verifier models. Applied to opportunistic screening for Type 2 Diabetes (T2D) using temporally realistic cohorts curated from private and public hospital corpora, HiTGNN achieves the highest predictive accuracy, especially for near-term risk, while preserving privacy and limiting reliance on large proprietary models. ReVeAL enhances sensitivity to true T2D cases and retains explanatory reasoning. Our ablations confirm the value of temporal structure and knowledge augmentation, and fairness analysis shows HiTGNN performs more equitably across subgroups.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Hybrid Theory and Data-driven Approach to Persuasion Detection with Large Language Models</title>
<link>https://arxiv.org/abs/2511.22109</link>
<guid>https://arxiv.org/abs/2511.22109</guid>
<content:encoded><![CDATA[
arXiv:2511.22109v1 Announce Type: new 
Abstract: Traditional psychological models of belief revision focus on face-to-face interactions, but with the rise of social media, more effective models are needed to capture belief revision at scale, in this rich text-based online discourse. Here, we use a hybrid approach, utilizing large language models (LLMs) to develop a model that predicts successful persuasion using features derived from psychological experiments.
  Our approach leverages LLM generated ratings of features previously examined in the literature to build a random forest classification model that predicts whether a message will result in belief change. Of the eight features tested, \textit{epistemic emotion} and \textit{willingness to share} were the top-ranking predictors of belief change in the model. Our findings provide insights into the characteristics of persuasive messages and demonstrate how LLMs can enhance models of successful persuasion based on psychological theory. Given these insights, this work has broader applications in fields such as online influence detection and misinformation mitigation, as well as measuring the effectiveness of online narratives.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging the Modality Gap by Similarity Standardization with Pseudo-Positive Samples</title>
<link>https://arxiv.org/abs/2511.22141</link>
<guid>https://arxiv.org/abs/2511.22141</guid>
<content:encoded><![CDATA[
arXiv:2511.22141v1 Announce Type: new 
Abstract: Advances in vision-language models (VLMs) have enabled effective cross-modality retrieval. However, when both text and images exist in the database, similarity scores would differ in scale by modality. This phenomenon, known as the modality gap, hinders accurate retrieval. Most existing studies address this issue with manually labeled data, e.g., by fine-tuning VLMs on them. In this work, we propose a similarity standardization approach with pseudo data construction. We first compute the mean and variance of the similarity scores between each query and its paired data in text or image modality. Using these modality-specific statistics, we standardize all similarity scores to compare on a common scale across modalities. These statistics are calculated from pseudo pairs, which are constructed by retrieving the text and image candidates with the highest cosine similarity to each query. We evaluate our method across seven VLMs using two multi-modal QA benchmarks (MMQA and WebQA), where each question requires retrieving either text or image data. Our experimental results show that our method significantly improves retrieval performance, achieving average Recall@20 gains of 64% on MMQA and 28% on WebQA when the query and the target data belong to different modalities. Compared to E5-V, which addresses the modality gap through image captioning, we confirm that our method more effectively bridges the modality gap.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>C$^2$DLM: Causal Concept-Guided Diffusion Large Language Models</title>
<link>https://arxiv.org/abs/2511.22146</link>
<guid>https://arxiv.org/abs/2511.22146</guid>
<content:encoded><![CDATA[
arXiv:2511.22146v1 Announce Type: new 
Abstract: Autoregressive (AR) language models and Diffusion Language Models (DLMs) constitute the two principal paradigms of large language models. However, both paradigms suffer from insufficient reasoning capabilities. Human reasoning inherently relies on causal knowledge and thought, which are reflected in natural language. But in the AR paradigm, language is modeled as next token prediction (a strictly left-to-right, token-by-token order), whereas natural language itself exhibits more flexible causal structures. In the DLM paradigm, the attention mechanism is fully connected, which entirely disregards causal order. To fill this gap, we propose a \underline{\textbf{C}}ausal \underline{\textbf{C}}oncept-Guided \underline{\textbf{D}}iffusion \underline{\textbf{L}}anguage \underline{\textbf{M}}odel (C$^2$DLM). Starting from DLM's fully connected attention, C$^2$DLM first obtains a concept-level causal graph from the teacher model, and then explicitly guides attention to learn causal relationships between concepts. By focusing on causal relationships and avoiding interference from difficult subgoals involving causal inversion, C$^2$DLM improves 12\% with about 3.2 times training speedup in the COT-OrderPerturb task, and achieves an average gain of 1.31\% across six downstream reasoning tasks. More details in the repository ~\href{https://github.com/Kairong-Han/C-2-DLM}{here}.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Theoretically Grounded Hybrid Ensemble for Reliable Detection of LLM-Generated Text</title>
<link>https://arxiv.org/abs/2511.22153</link>
<guid>https://arxiv.org/abs/2511.22153</guid>
<content:encoded><![CDATA[
arXiv:2511.22153v1 Announce Type: new 
Abstract: The rapid proliferation of Large Language Models (LLMs) has blurred the line between human and machine authorship, creating practical risks for academic integrity and information reliability. Existing text detectors typically rely on a single methodological paradigm and suffer from poor generalization and high false positive rates (FPR), especially on high-stakes academic text. We propose a theoretically grounded hybrid ensemble that systematically fuses three complementary detection paradigms: (i) a RoBERTa-based transformer classifier for deep semantic feature extraction, (ii) a GPT-2-based probabilistic detector using perturbation-induced likelihood curvature, and (iii) a statistical linguistic feature analyzer capturing stylometric patterns. The core novelty lies in an optimized weighted voting framework, where ensemble weights are learned on the probability simplex to maximize F1-score rather than set heuristically. We provide a bias-variance analysis and empirically demonstrate low inter-model correlation (rho ~ 0.35-0.42), a key condition for variance reduction. Evaluated on a large-scale, multigenerator corpus of 30,000 documents, our system achieves 94.2% accuracy and an AUC of 0.978, with a 35% relative reduction in false positives on academic text. This yields a more reliable and ethically responsible detector for real-world deployment in education and other high-stakes domains.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lips-Jaw and Tongue-Jaw Articulatory Tradeoff in DYNARTmo</title>
<link>https://arxiv.org/abs/2511.22155</link>
<guid>https://arxiv.org/abs/2511.22155</guid>
<content:encoded><![CDATA[
arXiv:2511.22155v1 Announce Type: new 
Abstract: This paper investigates how the dynamic articulatory model DYNARTmo accounts for articulatory tradeoffs between primary and secondary articulators, with a focus on lips-jaw and tongue-jaw coordination. While DYNARTmo does not implement full task-dynamic second-order biomechanics, it adopts first-order task-space gesture specifications comparable to those used in articulatory phonology and integrates a simplified mechanism for distributing articulatory effort across multiple articulators. We first outline the conceptual relationship between task dynamics and DYNARTmo, emphasizing the distinction between high-level task-space trajectories and their low-level articulatory execution. We then present simulation results for a set of CV syllables that illustrate how jaw displacement varies as a function of both place of articulation (labial, apical, dorsal) and vowel context (/a/, /i/, /u/). The model reproduces empirically attested patterns of articulatory synergy, including jaw-supported apical closures, lower-lip elevation in bilabial stops, tongue-jaw co-movement, and saturation effects in labial constrictions. These results demonstrate that even with computationally simplified assumptions, DYNARTmo can generate realistic spatio-temporal movement patterns that capture key aspects of articulatory tradeoff and synergy across a range of consonant-vowel combinations.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RefineBench: Evaluating Refinement Capability of Language Models via Checklists</title>
<link>https://arxiv.org/abs/2511.22173</link>
<guid>https://arxiv.org/abs/2511.22173</guid>
<content:encoded><![CDATA[
arXiv:2511.22173v1 Announce Type: new 
Abstract: Can language models (LMs) self-refine their own responses? This question is increasingly relevant as a wide range of real-world user interactions involve refinement requests. However, prior studies have largely tested LMs' refinement abilities on verifiable tasks such as competition math or symbolic reasoning with simplified scaffolds, whereas users often pose open-ended queries and provide varying degrees of feedback on what they desire. The recent advent of reasoning models that exhibit self-reflection patterns in their chains-of-thought further motivates this question. To analyze this, we introduce RefineBench, a benchmark of 1,000 challenging problems across 11 domains paired with a checklist-based evaluation framework. We evaluate two refinement modes: (1) guided refinement, where an LM is provided natural language feedback, and (2) self-refinement, where LMs attempt to improve without guidance. In the self-refinement setting, even frontier LMs such as Gemini 2.5 Pro and GPT-5 achieve modest baseline scores of 31.3% and 29.1%, respectively, and most models fail to consistently improve across iterations (e.g., Gemini-2.5-Pro gains only +1.8%, while DeepSeek-R1 declines by -0.1%). By contrast, in guided refinement, both proprietary LMs and large open-weight LMs (>70B) can leverage targeted feedback to refine responses to near-perfect levels within five turns. These findings suggest that frontier LMs require breakthroughs to self-refine their incorrect responses, and that RefineBench provides a valuable testbed for tracking progress.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Focused Chain-of-Thought: Efficient LLM Reasoning via Structured Input Information</title>
<link>https://arxiv.org/abs/2511.22176</link>
<guid>https://arxiv.org/abs/2511.22176</guid>
<content:encoded><![CDATA[
arXiv:2511.22176v1 Announce Type: new 
Abstract: Recent large language models achieve strong reasoning performance by generating detailed chain-of-thought traces, but this often leads to excessive token use and high inference latency. Existing efficiency approaches typically focus on model-centric interventions, such as reinforcement learning or supervised fine-tuning, to reduce verbosity. In contrast, we propose a training-free, input-centric approach. Inspired by cognitive psychology, we introduce Focused Chain-of-Thought (F-CoT), which separates information extraction from the reasoning process. F-CoT first organizes the essential information from a query into a concise, structured context and then guides the model to reason exclusively over this context. By preventing attention to irrelevant details, F-CoT naturally produces shorter reasoning paths. On arithmetic word problems, F-CoT reduces generated tokens by 2-3x while maintaining accuracy comparable to standard zero-shot CoT. These results highlight structured input as a simple yet effective lever for more efficient LLM reasoning.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Query-Level Comparison: Fine-Grained Reinforcement Learning for Text-to-SQL with Automated Interpretable Critiques</title>
<link>https://arxiv.org/abs/2511.22258</link>
<guid>https://arxiv.org/abs/2511.22258</guid>
<content:encoded><![CDATA[
arXiv:2511.22258v1 Announce Type: new 
Abstract: Text-to-SQL, a pivotal natural language processing (NLP) task that converts textual queries into executable SQL, has seen substantial progress in recent years. However, existing evaluation and reward mechanisms used to train and assess the text-to-SQL models remain a critical bottleneck. Current approaches heavily rely on manually annotated gold SQL queries, which are costly to produce and impractical for large-scale evaluation. More importantly, most reinforcement learning (RL) methods in text-to-SQL leverage only the final binary execution outcome as the reward signal, a coarse-grained supervision that overlooks detailed structural and semantic errors from the perspective of rubrics. To address these challenges, we propose RuCo-C, a novel generative judge model for fine-grained, query-specific automatic evaluation using interpretable critiques without human intervention. Our framework first automatically generates query-specific evaluation rubrics for human-free annotation, linking them to interpretable critiques. Subsequently, it integrates densified reward feedback through a "progressive exploration" strategy during the RL training process, which dynamically adjusts the rewards to enhance the model's performance. Comprehensive experiments demonstrate that RuCo-C outperforms existing methods in text-to-SQL evaluation, yielding significant performance gains.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Token-Level Marginalization for Multi-Label LLM Classifiers</title>
<link>https://arxiv.org/abs/2511.22312</link>
<guid>https://arxiv.org/abs/2511.22312</guid>
<content:encoded><![CDATA[
arXiv:2511.22312v1 Announce Type: new 
Abstract: This paper addresses the critical challenge of deriving interpretable confidence scores from generative language models (LLMs) when applied to multi-label content safety classification. While models like LLaMA Guard are effective for identifying unsafe content and its categories, their generative architecture inherently lacks direct class-level probabilities, which hinders model confidence assessment and performance interpretation. This limitation complicates the setting of dynamic thresholds for content moderation and impedes fine-grained error analysis. This research proposes and evaluates three novel token-level probability estimation approaches to bridge this gap. The aim is to enhance model interpretability and accuracy, and evaluate the generalizability of this framework across different instruction-tuned models. Through extensive experimentation on a synthetically generated, rigorously annotated dataset, it is demonstrated that leveraging token logits significantly improves the interpretability and reliability of generative classifiers, enabling more nuanced content safety moderation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sentiment Analysis Of Shopee Product Reviews Using Distilbert</title>
<link>https://arxiv.org/abs/2511.22313</link>
<guid>https://arxiv.org/abs/2511.22313</guid>
<content:encoded><![CDATA[
arXiv:2511.22313v1 Announce Type: new 
Abstract: The rapid growth of digital commerce has led to the accumulation of a massive number of consumer reviews on online platforms. Shopee, as one of the largest e-commerce platforms in Southeast Asia, receives millions of product reviews every day containing valuable information regarding customer satisfaction and preferences. Manual analysis of these reviews is inefficient, thus requiring a computational approach such as sentiment analysis. This study examines the use of DistilBERT, a lightweight transformer-based deep learning model, for sentiment classification on Shopee product reviews. The dataset used consists of approximately one million English-language reviews that have been preprocessed and trained using the distilbert-base-uncased model. Evaluation was conducted using accuracy, precision, recall, and F1-score metrics, and compared against benchmark models such as BERT and SVM. The results show that DistilBERT achieved an accuracy of 94.8%, slightly below BERT (95.3%) but significantly higher than SVM (90.2%), with computation time reduced by more than 55%. These findings demonstrate that DistilBERT provides an optimal balance between accuracy and efficiency, making it suitable for large scale sentiment analysis on e-commerce platforms. Keywords: Sentiment Analysis, DistilBERT, Shopee Reviews, Natural Language Processing, Deep Learning, Transformer Models.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Named Entity Recognition for the Kurdish Sorani Language: Dataset Creation and Comparative Analysis</title>
<link>https://arxiv.org/abs/2511.22315</link>
<guid>https://arxiv.org/abs/2511.22315</guid>
<content:encoded><![CDATA[
arXiv:2511.22315v1 Announce Type: new 
Abstract: This work contributes towards balancing the inclusivity and global applicability of natural language processing techniques by proposing the first 'name entity recognition' dataset for Kurdish Sorani, a low-resource and under-represented language, that consists of 64,563 annotated tokens. It also provides a tool for facilitating this task in this and many other languages and performs a thorough comparative analysis, including classic machine learning models and neural systems. The results obtained challenge established assumptions about the advantage of neural approaches within the context of NLP. Conventional methods, in particular CRF, obtain F1-scores of 0.825, outperforming the results of BiLSTM-based models (0.706) significantly. These findings indicate that simpler and more computationally efficient classical frameworks can outperform neural architectures in low-resource settings.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mapping Clinical Doubt: Locating Linguistic Uncertainty in LLMs</title>
<link>https://arxiv.org/abs/2511.22402</link>
<guid>https://arxiv.org/abs/2511.22402</guid>
<content:encoded><![CDATA[
arXiv:2511.22402v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly used in clinical settings, where sensitivity to linguistic uncertainty can influence diagnostic interpretation and decision-making. Yet little is known about where such epistemic cues are internally represented within these models. Distinct from uncertainty quantification, which measures output confidence, this work examines input-side representational sensitivity to linguistic uncertainty in medical text. We curate a contrastive dataset of clinical statements varying in epistemic modality (e.g., 'is consistent with' vs. 'may be consistent with') and propose Model Sensitivity to Uncertainty (MSU), a layerwise probing metric that quantifies activation-level shifts induced by uncertainty cues. Our results show that LLMs exhibit structured, depth-dependent sensitivity to clinical uncertainty, suggesting that epistemic information is progressively encoded in deeper layers. These findings reveal how linguistic uncertainty is internally represented in LLMs, offering insight into their interpretability and epistemic reliability.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Performance Variations in Finetuned Translators of Ultra-Low Resource Languages: Do Linguistic Differences Matter?</title>
<link>https://arxiv.org/abs/2511.22482</link>
<guid>https://arxiv.org/abs/2511.22482</guid>
<content:encoded><![CDATA[
arXiv:2511.22482v1 Announce Type: new 
Abstract: Finetuning pre-trained language models with small amounts of data is a commonly-used method to create translators for ultra-low resource languages such as endangered Indigenous languages. However, previous works have reported substantially different performances with translators created using similar methodology and data. In this work we systematically explored possible causes of the performance difference, aiming to determine whether it was a product of different cleaning procedures, limitations of the pre-trained models, the size of the base model, or the size of the training dataset, studying both directions of translation. Our studies, using two Brazilian Indigenous languages, related but with significant structural linguistic characteristics, indicated none or very limited influence from those training factors, suggesting differences between languages may play a significant role in the ability to produce translators by fine-tuning pre-trained models.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Joint Speech and Text Training for LLM-Based End-to-End Spoken Dialogue State Tracking</title>
<link>https://arxiv.org/abs/2511.22503</link>
<guid>https://arxiv.org/abs/2511.22503</guid>
<content:encoded><![CDATA[
arXiv:2511.22503v1 Announce Type: new 
Abstract: End-to-end spoken dialogue state tracking (DST) is made difficult by the tandem of having to handle speech input and data scarcity. Combining speech foundation encoders and large language models has been proposed in recent work as to alleviate some of this difficulty. Although this approach has been shown to result in strong spoken DST models, achieving state-of-the-art performance in realistic multi-turn DST, it struggles to generalize across domains and requires annotated spoken DST training data for each domain of interest. However, collecting such data for every target domain is both costly and difficult. Noting that textual DST data is more easily obtained for various domains, in this work, we propose jointly training on available spoken DST data and written textual data from other domains as a way to achieve cross-domain generalization. We conduct experiments which show the efficacy of our proposed method for getting good cross-domain DST performance without relying on spoken training data from the target domains.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extension Condition "violations" and Merge optimality constraints</title>
<link>https://arxiv.org/abs/2511.22582</link>
<guid>https://arxiv.org/abs/2511.22582</guid>
<content:encoded><![CDATA[
arXiv:2511.22582v1 Announce Type: new 
Abstract: We analyze, using the mathematical formulation of Merge within the Strong Minimalist Thesis framework, a set of linguistic phenomena, including head-to-head movement, phrasal affixes and syntactic cliticization, verb-particle alternation, and operator-variable phenomena. These are often regarded as problematic, as violations of the Extension Condition. We show that, in fact, all of these phenomena can be explained without involving any EC violation. We first show that derivations using Sideward Merge are possible for all of these cases: these respect EC, though they involve some amount of optimality violations, with respect to Resource Restrictions cost functions, andthe amount of violation differs among these cases. We show that all the cases that involve large optimality violations can be derived in alternative ways involving neither EC nor the use of SM. The main remaining case (head-to-head movement) only involves SM with minimal violations of optimality (near equilibrium fluctuations). We analyze explicitly also the cases of multiple wh-fronting, clusters of clitics in Romance languages and possessor agreement construction in Korean, and how an explanation of these phenomena based on SM can be made compatible with the colored operad generators for phases and theta roles. We also show that the EC condition has a clear algebraic meaning in the mathematical formulation of Merge and is therefore an intrinsic structural algebraic constraint of the model, rather than an additional assumption. We also show that the minimal optimality violating SM plays a structural role in the Markovian properties of Merge, and we compare different optimality conditions coming from Minimal Search and from Resource Restriction in terms of their effect on the dynamics of the Hopf algebra Markov chain, in a simple explicit example.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Smarter, not Bigger: Fine-Tuned RAG-Enhanced LLMs for Automotive HIL Testing</title>
<link>https://arxiv.org/abs/2511.22584</link>
<guid>https://arxiv.org/abs/2511.22584</guid>
<content:encoded><![CDATA[
arXiv:2511.22584v1 Announce Type: new 
Abstract: Hardware-in-the-Loop (HIL) testing is essential for automotive validation but suffers from fragmented and underutilized test artifacts. This paper presents HIL-GPT, a retrieval-augmented generation (RAG) system integrating domain-adapted large language models (LLMs) with semantic retrieval. HIL-GPT leverages embedding fine-tuning using a domain-specific dataset constructed via heuristic mining and LLM-assisted synthesis, combined with vector indexing for scalable, traceable test case and requirement retrieval. Experiments show that fine-tuned compact models, such as \texttt{bge-base-en-v1.5}, achieve a superior trade-off between accuracy, latency, and cost compared to larger models, challenging the notion that bigger is always better. An A/B user study further confirms that RAG-enhanced assistants improve perceived helpfulness, truthfulness, and satisfaction over general-purpose LLMs. These findings provide insights for deploying efficient, domain-aligned LLM-based assistants in industrial HIL environments.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving LLM-based Ontology Matching with fine-tuning on synthetic data</title>
<link>https://arxiv.org/abs/2511.22612</link>
<guid>https://arxiv.org/abs/2511.22612</guid>
<content:encoded><![CDATA[
arXiv:2511.22612v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly being integrated into various components of Ontology Matching pipelines. This paper investigates the capability of LLMs to perform ontology matching directly on ontology modules and generate the corresponding alignments. Furthermore, it is explored how a dedicated fine-tuning strategy can enhance the model's matching performance in a zero-shot setting. The proposed method incorporates a search space reduction technique to select relevant subsets from both source and target ontologies, which are then used to automatically construct prompts. Recognizing the scarcity of reference alignments for training, a novel LLM-based approach is introduced for generating a synthetic dataset. This process creates a corpus of ontology submodule pairs and their corresponding reference alignments, specifically designed to fine-tune an LLM for the ontology matching task. The proposed approach was evaluated on the Conference, Geolink, Enslaved, Taxon, and Hydrography datasets from the OAEI complex track. The results demonstrate that the LLM fine-tuned on the synthetically generated data exhibits superior performance compared to the non-fine-tuned base model. The key contribution is a strategy that combines automatic dataset generation with fine-tuning to effectively adapt LLMs for ontology matching tasks.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modeling Romanized Hindi and Bengali: Dataset Creation and Multilingual LLM Integration</title>
<link>https://arxiv.org/abs/2511.22769</link>
<guid>https://arxiv.org/abs/2511.22769</guid>
<content:encoded><![CDATA[
arXiv:2511.22769v1 Announce Type: new 
Abstract: The development of robust transliteration techniques to enhance the effectiveness of transforming Romanized scripts into native scripts is crucial for Natural Language Processing tasks, including sentiment analysis, speech recognition, information retrieval, and intelligent personal assistants. Despite significant advancements, state-of-the-art multilingual models still face challenges in handling Romanized script, where the Roman alphabet is adopted to represent the phonetic structure of diverse languages. Within the South Asian context, where the use of Romanized script for Indo-Aryan languages is widespread across social media and digital communication platforms, such usage continues to pose significant challenges for cutting-edge multilingual models. While a limited number of transliteration datasets and models are available for Indo-Aryan languages, they generally lack sufficient diversity in pronunciation and spelling variations, adequate code-mixed data for large language model (LLM) training, and low-resource adaptation. To address this research gap, we introduce a novel transliteration dataset for two popular Indo-Aryan languages, Hindi and Bengali, which are ranked as the 3rd and 7th most spoken languages worldwide. Our dataset comprises nearly 1.8 million Hindi and 1 million Bengali transliteration pairs. In addition to that, we pre-train a custom multilingual seq2seq LLM based on Marian architecture using the developed dataset. Experimental results demonstrate significant improvements compared to existing relevant models in terms of BLEU and CER metrics.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Semantic Drift: Evaluating LLMs' Efficacy in Psychotherapy through MI Dialogue Summarization</title>
<link>https://arxiv.org/abs/2511.22818</link>
<guid>https://arxiv.org/abs/2511.22818</guid>
<content:encoded><![CDATA[
arXiv:2511.22818v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) have shown their potential across both general and domain-specific tasks. However, there is a growing concern regarding their lack of sensitivity, factual incorrectness in responses, inconsistent expressions of empathy, bias, hallucinations, and overall inability to capture the depth and complexity of human understanding, especially in low-resource and sensitive domains such as psychology. To address these challenges, our study employs a mixed-methods approach to evaluate the efficacy of LLMs in psychotherapy. We use LLMs to generate precise summaries of motivational interviewing (MI) dialogues and design a two-stage annotation scheme based on key components of the Motivational Interviewing Treatment Integrity (MITI) framework, namely evocation, collaboration, autonomy, direction, empathy, and a non-judgmental attitude. Using expert-annotated MI dialogues as ground truth, we formulate multi-class classification tasks to assess model performance under progressive prompting techniques, incorporating one-shot and few-shot prompting. Our results offer insights into LLMs' capacity for understanding complex psychological constructs and highlight best practices to mitigate ``semantic drift" in therapeutic settings. Our work contributes not only to the MI community by providing a high-quality annotated dataset to address data scarcity in low-resource domains but also critical insights for using LLMs for precise contextual interpretation in complex behavioral therapy.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAG System for Supporting Japanese Litigation Procedures: Faithful Response Generation Complying with Legal Norms</title>
<link>https://arxiv.org/abs/2511.22858</link>
<guid>https://arxiv.org/abs/2511.22858</guid>
<content:encoded><![CDATA[
arXiv:2511.22858v1 Announce Type: new 
Abstract: This study discusses the essential components that a Retrieval-Augmented Generation (RAG)-based LLM system should possess in order to support Japanese medical litigation procedures complying with legal norms. In litigation, expert commissioners, such as physicians, architects, accountants, and engineers, provide specialized knowledge to help judges clarify points of dispute. When considering the substitution of these expert roles with a RAG-based LLM system, the constraint of strict adherence to legal norms is imposed. Specifically, three requirements arise: (1) the retrieval module must retrieve appropriate external knowledge relevant to the disputed issues in accordance with the principle prohibiting the use of private knowledge, (2) the responses generated must originate from the context provided by the RAG and remain faithful to that context, and (3) the retrieval module must reference external knowledge with appropriate timestamps corresponding to the issues at hand. This paper discusses the design of a RAG-based LLM system that satisfies these requirements.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JBE-QA: Japanese Bar Exam QA Dataset for Assessing Legal Domain Knowledge</title>
<link>https://arxiv.org/abs/2511.22869</link>
<guid>https://arxiv.org/abs/2511.22869</guid>
<content:encoded><![CDATA[
arXiv:2511.22869v1 Announce Type: new 
Abstract: We introduce JBE-QA, a Japanese Bar Exam Question-Answering dataset to evaluate large language models' legal knowledge. Derived from the multiple-choice (tanto-shiki) section of the Japanese bar exam (2015-2024), JBE-QA provides the first comprehensive benchmark for Japanese legal-domain evaluation of LLMs. It covers the Civil Code, the Penal Code, and the Constitution, extending beyond the Civil Code focus of prior Japanese resources. Each question is decomposed into independent true/false judgments with structured contextual fields. The dataset contains 3,464 items with balanced labels. We evaluate 26 LLMs, including proprietary, open-weight, Japanese-specialised, and reasoning models. Our results show that proprietary models with reasoning enabled perform best, and the Constitution questions are generally easier than the Civil Code or the Penal Code questions.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FEANEL: A Benchmark for Fine-Grained Error Analysis in K-12 English Writing</title>
<link>https://arxiv.org/abs/2511.22883</link>
<guid>https://arxiv.org/abs/2511.22883</guid>
<content:encoded><![CDATA[
arXiv:2511.22883v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have transformed artificial intelligence, offering profound opportunities for educational applications. However, their ability to provide fine-grained educational feedback for K-12 English writing remains underexplored. In this paper, we challenge the error analysis and pedagogical skills of LLMs by introducing the problem of Fine-grained Error Analysis for English Learners and present the Fine-grained Error ANalysis for English Learners (FEANEL) Benchmark. The benchmark comprises 1,000 essays written by elementary and secondary school students, and a well-developed English writing error taxonomy. Each error is annotated by language education experts and categorized by type, severity, and explanatory feedback, using a part-of-speech-based taxonomy they co-developed. We evaluate state-of-the-art LLMs on the FEANEL Benchmark to explore their error analysis and pedagogical abilities. Experimental results reveal significant gaps in current LLMs' ability to perform fine-grained error analysis, highlighting the need for advancements in particular methods for educational applications.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language-conditioned world model improves policy generalization by reading environmental descriptions</title>
<link>https://arxiv.org/abs/2511.22904</link>
<guid>https://arxiv.org/abs/2511.22904</guid>
<content:encoded><![CDATA[
arXiv:2511.22904v1 Announce Type: new 
Abstract: To interact effectively with humans in the real world, it is important for agents to understand language that describes the dynamics of the environment--that is, how the environment behaves--rather than just task instructions specifying "what to do". Understanding this dynamics-descriptive language is important for human-agent interaction and agent behavior. Recent work address this problem using a model-based approach: language is incorporated into a world model, which is then used to learn a behavior policy. However, these existing methods either do not demonstrate policy generalization to unseen games or rely on limiting assumptions. For instance, assuming that the latency induced by inference-time planning is tolerable for the target task or expert demonstrations are available. Expanding on this line of research, we focus on improving policy generalization from a language-conditioned world model while dropping these assumptions. We propose a model-based reinforcement learning approach, where a language-conditioned world model is trained through interaction with the environment, and a policy is learned from this model--without planning or expert demonstrations. Our method proposes Language-aware Encoder for Dreamer World Model (LED-WM) built on top of DreamerV3. LED-WM features an observation encoder that uses an attention mechanism to explicitly ground language descriptions to entities in the observation. We show that policies trained with LED-WM generalize more effectively to unseen games described by novel dynamics and language compared to other baselines in several settings in two environments: MESSENGER and MESSENGER-WM.To highlight how the policy can leverage the trained world model before real-world deployment, we demonstrate the policy can be improved through fine-tuning on synthetic test trajectories generated by the world model.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual Puns from Idioms: An Iterative LLM-T2IM-MLLM Framework</title>
<link>https://arxiv.org/abs/2511.22943</link>
<guid>https://arxiv.org/abs/2511.22943</guid>
<content:encoded><![CDATA[
arXiv:2511.22943v1 Announce Type: new 
Abstract: We study idiom-based visual puns--images that align an idiom's literal and figurative meanings--and present an iterative framework that coordinates a large language model (LLM), a text-to-image model (T2IM), and a multimodal LLM (MLLM) for automatic generation and evaluation. Given an idiom, the system iteratively (i) generates detailed visual prompts, (ii) synthesizes an image, (iii) infers the idiom from the image, and (iv) refines the prompt until recognition succeeds or a step limit is reached. Using 1,000 idioms as inputs, we synthesize a corresponding dataset of visual pun images with paired prompts, enabling benchmarking of both generation and understanding. Experiments across 10 LLMs, 10 MLLMs, and one T2IM (Qwen-Image) show that MLLM choice is the primary performance driver: GPT achieves the highest accuracies, Gemini follows, and the best open-source MLLM (Gemma) is competitive with some closed models. On the LLM side, Claude attains the strongest average performance for prompt generation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training-Free Loosely Speculative Decoding: Accepting Semantically Correct Drafts Beyond Exact Match</title>
<link>https://arxiv.org/abs/2511.22972</link>
<guid>https://arxiv.org/abs/2511.22972</guid>
<content:encoded><![CDATA[
arXiv:2511.22972v1 Announce Type: new 
Abstract: Large language models (LLMs) achieve strong performance across diverse tasks but suffer from high inference latency due to their autoregressive generation. Speculative Decoding (SPD) mitigates this issue by verifying candidate tokens in parallel from a smaller draft model, yet its strict exact-match verification discards many semantically valid continuations. Moreover, existing training-based SPD methods often suffer from performance degradation on out-of-distribution (OOD) tasks. To this end, we propose Training-Free Loosely Speculative Decoding (FLy), a novel method that loosens the rigid verification criterion by leveraging the target model's self-corrective behavior to judge whether a draft-target mismatch remains semantically valid. FLy introduces a two-tier mechanism: an entropy-level gate that identifies whether the current token allows multiple plausible alternatives or is nearly deterministic, and a token-level deferred window that distinguishes genuine errors from differently worded yet semantically correct variants. To further reduce latency, we design a multi-level acceleration strategy that accelerates not only the target model but also the drafter itself. Owing to its training-free design, FLy composes seamlessly with arbitrary draft-target pairs and generalizes across models and domains without hyperparameter re-tuning. Experiments show that FLy preserves more than 99% of the target model's accuracy while achieving an average 2.81x speedup on Llama-3.1-70B-Instruct and 5.07x speedup on the 405B variant. Notably, on out-of-domain datasets, our method remains highly effective and outperforms the training-based method EAGLE-3 by 1.62x.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pooling Attention: Evaluating Pretrained Transformer Embeddings for Deception Classification</title>
<link>https://arxiv.org/abs/2511.22977</link>
<guid>https://arxiv.org/abs/2511.22977</guid>
<content:encoded><![CDATA[
arXiv:2511.22977v1 Announce Type: new 
Abstract: This paper investigates fake news detection as a downstream evaluation of Transformer representations, benchmarking encoder-only and decoder-only pre-trained models (BERT, GPT-2, Transformer-XL) as frozen embedders paired with lightweight classifiers. Through controlled preprocessing comparing pooling versus padding and neural versus linear heads, results demonstrate that contextual self-attention encodings consistently transfer effectively. BERT embeddings combined with logistic regression outperform neural baselines on LIAR dataset splits, while analyses of sequence length and aggregation reveal robustness to truncation and advantages from simple max or average pooling. This work positions attention-based token encoders as robust, architecture-centric foundations for veracity tasks, isolating Transformer contributions from classifier complexity.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ShoppingComp: Are LLMs Really Ready for Your Shopping Cart?</title>
<link>https://arxiv.org/abs/2511.22978</link>
<guid>https://arxiv.org/abs/2511.22978</guid>
<content:encoded><![CDATA[
arXiv:2511.22978v1 Announce Type: new 
Abstract: We present ShoppingComp, a challenging real-world benchmark for rigorously evaluating LLM-powered shopping agents on three core capabilities: precise product retrieval, expert-level report generation, and safety critical decision making. Unlike prior e-commerce benchmarks, ShoppingComp introduces highly complex tasks under the principle of guaranteeing real products and ensuring easy verifiability, adding a novel evaluation dimension for identifying product safety hazards alongside recommendation accuracy and report quality. The benchmark comprises 120 tasks and 1,026 scenarios, curated by 35 experts to reflect authentic shopping needs. Results reveal stark limitations of current LLMs: even state-of-the-art models achieve low performance (e.g., 11.22% for GPT-5, 3.92% for Gemini-2.5-Flash). These findings highlight a substantial gap between research benchmarks and real-world deployment, where LLMs make critical errors such as failure to identify unsafe product usage or falling for promotional misinformation, leading to harmful recommendations. ShoppingComp fills the gap and thus establishes a new standard for advancing reliable and practical agents in e-commerce.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Social Perceptions of English Spelling Variation on Twitter: A Comparative Analysis of Human and LLM Responses</title>
<link>https://arxiv.org/abs/2511.23041</link>
<guid>https://arxiv.org/abs/2511.23041</guid>
<content:encoded><![CDATA[
arXiv:2511.23041v1 Announce Type: new 
Abstract: Spelling variation (e.g. funnnn vs. fun) can influence the social perception of texts and their writers: we often have various associations with different forms of writing (is the text informal? does the writer seem young?). In this study, we focus on the social perception of spelling variation in online writing in English and study to what extent this perception is aligned between humans and large language models (LLMs). Building on sociolinguistic methodology, we compare LLM and human ratings on three key social attributes of spelling variation (formality, carefulness, age). We find generally strong correlations in the ratings between humans and LLMs. However, notable differences emerge when we analyze the distribution of ratings and when comparing between different types of spelling variation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoding the Past: Explainable Machine Learning Models for Dating Historical Texts</title>
<link>https://arxiv.org/abs/2511.23056</link>
<guid>https://arxiv.org/abs/2511.23056</guid>
<content:encoded><![CDATA[
arXiv:2511.23056v1 Announce Type: new 
Abstract: Accurately dating historical texts is essential for organizing and interpreting cultural heritage collections. This article addresses temporal text classification using interpretable, feature-engineered tree-based machine learning models. We integrate five feature categories - compression-based, lexical structure, readability, neologism detection, and distance features - to predict the temporal origin of English texts spanning five centuries. Comparative analysis shows that these feature domains provide complementary temporal signals, with combined models outperforming any individual feature set. On a large-scale corpus, we achieve 76.7% accuracy for century-scale prediction and 26.1% for decade-scale classification, substantially above random baselines (20% and 2.3%). Under relaxed temporal precision, performance increases to 96.0% top-2 accuracy for centuries and 85.8% top-10 accuracy for decades. The final model exhibits strong ranking capabilities with AUCROC up to 94.8% and AUPRC up to 83.3%, and maintains controlled errors with mean absolute deviations of 27 years and 30 years, respectively. For authentication-style tasks, binary models around key thresholds (e.g., 1850-1900) reach 85-98% accuracy. Feature importance analysis identifies distance features and lexical structure as most informative, with compression-based features providing complementary signals. SHAP explainability reveals systematic linguistic evolution patterns, with the 19th century emerging as a pivot point across feature domains. Cross-dataset evaluation on Project Gutenberg highlights domain adaptation challenges, with accuracy dropping by 26.4 percentage points, yet the computational efficiency and interpretability of tree-based models still offer a scalable, explainable alternative to neural architectures.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Standard Occupation Classifier -- A Natural Language Processing Approach</title>
<link>https://arxiv.org/abs/2511.23057</link>
<guid>https://arxiv.org/abs/2511.23057</guid>
<content:encoded><![CDATA[
arXiv:2511.23057v1 Announce Type: new 
Abstract: Standard Occupational Classifiers (SOC) are systems used to categorize and classify different types of jobs and occupations based on their similarities in terms of job duties, skills, and qualifications. Integrating these facets with Big Data from job advertisement offers the prospect to investigate labour demand that is specific to various occupations. This project investigates the use of recent developments in natural language processing to construct a classifier capable of assigning an occupation code to a given job advertisement. We develop various classifiers for both UK ONS SOC and US O*NET SOC, using different Language Models. We find that an ensemble model, which combines Google BERT and a Neural Network classifier while considering job title, description, and skills, achieved the highest prediction accuracy. Specifically, the ensemble model exhibited a classification accuracy of up to 61% for the lower (or fourth) tier of SOC, and 72% for the third tier of SOC. This model could provide up to date, accurate information on the evolution of the labour market using job advertisements.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conveying Imagistic Thinking in TCM Translation: A Prompt Engineering and LLM-Based Evaluation Framework</title>
<link>https://arxiv.org/abs/2511.23059</link>
<guid>https://arxiv.org/abs/2511.23059</guid>
<content:encoded><![CDATA[
arXiv:2511.23059v1 Announce Type: new 
Abstract: Traditional Chinese Medicine theory is built on imagistic thinking, in which medical principles and diagnostic and therapeutic logic are structured through metaphor and metonymy. However, existing English translations largely rely on literal rendering, making it difficult for target-language readers to reconstruct the underlying conceptual networks and apply them in clinical practice. This study adopted a human-in-the-loop framework and selected four passages from the medical canon Huangdi Neijing that are fundamental in theory. Through prompt-based cognitive scaffolding, DeepSeek V3.1 was guided to identify metaphor and metonymy in the source text and convey the theory in translation. In the evaluation stage, ChatGPT 5 Pro and Gemini 2.5 Pro were instructed by prompts to simulate three types of real-world readers. Human translations, baseline model translations, and prompt-adjusted translations were scored by the simulated readers across five cognitive dimensions, followed by structured interviews and Interpretative Phenomenological Analysis. Results show that the prompt-adjusted LLM translations perform best across all five dimensions, with high cross-model and cross-role consistency. The interview themes reveal differences between human and machine translation, effective strategies for metaphor and metonymy transfer, and readers' cognitive preferences. This study provides a cognitive, efficient and replicable HITL methodological pathway for translation of ancient, concept-dense texts like TCM.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accent Placement Models for Rigvedic Sanskrit Text</title>
<link>https://arxiv.org/abs/2511.23088</link>
<guid>https://arxiv.org/abs/2511.23088</guid>
<content:encoded><![CDATA[
arXiv:2511.23088v1 Announce Type: new 
Abstract: The Rigveda, among the oldest Indian texts in Vedic Sanskrit, employs a distinctive pitch-accent system : ud\=atta, anud\=atta, svarita whose marks encode melodic and interpretive cues but are often absent from modern e-texts. This work develops a parallel corpus of accented-unaccented \'slokas and conducts a controlled comparison of three strategies for automatic accent placement in Rigvedic verse: (i) full fine-tuning of ByT5, a byte-level Transformer that operates directly on Unicode combining marks, (ii) a from-scratch BiLSTM-CRF sequence-labeling baseline, and (iii) LoRA-based parameter-efficient fine-tuning atop ByT5.
  Evaluation uses Word Error Rate (WER) and Character Error Rate (CER) for orthographic fidelity, plus a task-specific Diacritic Error Rate (DER) that isolates accent edits. Full ByT5 fine-tuning attains the lowest error across all metrics; LoRA offers strong efficiency-accuracy trade-offs, and BiLSTM-CRF serves as a transparent baseline. The study underscores practical requirements for accent restoration - Unicode-safe preprocessing, mark-aware tokenization, and evaluation that separates grapheme from accent errors - and positions heritage-language technology as an emerging NLP area connecting computational modeling with philological and pedagogical aims. Results establish reproducible baselines for Rigvedic accent restoration and provide guidance for downstream tasks such as accent-aware OCR, ASR/chant synthesis, and digital scholarship.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mind Reading or Misreading? LLMs on the Big Five Personality Test</title>
<link>https://arxiv.org/abs/2511.23101</link>
<guid>https://arxiv.org/abs/2511.23101</guid>
<content:encoded><![CDATA[
arXiv:2511.23101v1 Announce Type: new 
Abstract: We evaluate large language models (LLMs) for automatic personality prediction from text under the binary Five Factor Model (BIG5). Five models -- including GPT-4 and lightweight open-source alternatives -- are tested across three heterogeneous datasets (Essays, MyPersonality, Pandora) and two prompting strategies (minimal vs. enriched with linguistic and psychological cues). Enriched prompts reduce invalid outputs and improve class balance, but also introduce a systematic bias toward predicting trait presence. Performance varies substantially: Openness and Agreeableness are relatively easier to detect, while Extraversion and Neuroticism remain challenging. Although open-source models sometimes approach GPT-4 and prior benchmarks, no configuration yields consistently reliable predictions in zero-shot binary settings. Moreover, aggregate metrics such as accuracy and macro-F1 mask significant asymmetries, with per-class recall offering clearer diagnostic value. These findings show that current out-of-the-box LLMs are not yet suitable for APPT, and that careful coordination of prompt design, trait framing, and evaluation metrics is essential for interpretable results.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dripper: Token-Efficient Main HTML Extraction with a Lightweight LM</title>
<link>https://arxiv.org/abs/2511.23119</link>
<guid>https://arxiv.org/abs/2511.23119</guid>
<content:encoded><![CDATA[
arXiv:2511.23119v1 Announce Type: new 
Abstract: Accurately and efficiently extracting main content from general web pages is of great significance for obtaining training data for large models. Using well-pre-trained decoder-only generative language models offers excellent document comprehension capabilities, thereby effectively enhancing parsing quality. However, it remains constrained by issues such as context window length, inference cost, and format hallucination. We present Dripper, an efficient HTML main content extraction framework powered by lightweight language models, which addresses these challenges through four key innovations: (1) We design a specialized HTML simplification algorithm that reduces input token count to 22\% compared to raw HTML while preserving critical structural information; (2) We reformulate main content extraction as a semantic block sequence classification task, significantly reducing inference cost; (3) We introduce a controlled decoding mechanism that strictly constrains the output space through logits processors, effectively eliminating hallucination issues common in small-scale models; (4) We propose WebMainBench, an evaluation dataset containing over 7,800 web pages with meticulously human-annotated main content extraction labels. Experimental results demonstrate that using only a 0.6B parameter model, Dripper achieves state-of-the-art performance across all evaluation benchmarks and outperforms all baseline methods, attaining an ROUGE-N F1 score of 81.58\%( 83.13\% with fall-back strategy) on our proposed WebMainBench dataset.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-chain Graph Refinement and Selection for Reliable Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2511.23136</link>
<guid>https://arxiv.org/abs/2511.23136</guid>
<content:encoded><![CDATA[
arXiv:2511.23136v1 Announce Type: new 
Abstract: The complex reasoning ability of Large Language Models (LLMs) poses a critical bottleneck for their practical applications. Test-time expansion methods such as Tree-of-Thought (ToT) and Graph-of-Thought (GoT) enhance reasoning by introducing intermediate reasoning structures, tree search, or graph-based exploration mechanisms. However, their reasoning strategies suffer from limited diversity, redundant search branches, and inadequate integration and error correction across heterogeneous reasoning paths. To address these limitations, we propose a novel reasoning framework called Multi-chain Graph Refinement & Selection (MGRS), which first generates multiple diverse reasoning trajectories for a given problem, refines candidate responses using a composite self- and cross-verification strategy, then constructs a reasoning relation graph and estimates the success rate of intermediate nodes, and finally computes cumulative success rates to select the most reliable answer and corresponding reasoning trajectory. Experimental results demonstrate that MGRS significantly advances both the reasoning capability and computational efficiency of reasoning enhancement methods. Across six benchmark datasets spanning four distinct tasks, MGRS achieves an average accuracy of 82.9%, outperforming state-of-the-art baselines by a clear margin of 2.1%. Remarkably, on the 24-point game, MGRS attains 100% accuracy for the first time, while delivering a 13.6x speed-up compared to the leading Forest of Thoughts framework.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are LLMs Good Safety Agents or a Propaganda Engine?</title>
<link>https://arxiv.org/abs/2511.23174</link>
<guid>https://arxiv.org/abs/2511.23174</guid>
<content:encoded><![CDATA[
arXiv:2511.23174v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are trained to refuse to respond to harmful content. However, systematic analyses of whether this behavior is truly a reflection of its safety policies or an indication of political censorship, that is practiced globally by countries, is lacking. Differentiating between safety influenced refusals or politically motivated censorship is hard and unclear. For this purpose we introduce PSP, a dataset built specifically to probe the refusal behaviors in LLMs from an explicitly political context. PSP is built by formatting existing censored content from two data sources, openly available on the internet: sensitive prompts in China generalized to multiple countries, and tweets that have been censored in various countries. We study: 1) impact of political sensitivity in seven LLMs through data-driven (making PSP implicit) and representation-level approaches (erasing the concept of politics); and, 2) vulnerability of models on PSP through prompt injection attacks (PIAs). Associating censorship with refusals on content with masked implicit intent, we find that most LLMs perform some form of censorship. We conclude with summarizing major attributes that can cause a shift in refusal distributions across models and contexts of different countries.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Listwise Preference Optimization with Element-wise Confusions for Aspect Sentiment Quad Prediction</title>
<link>https://arxiv.org/abs/2511.23184</link>
<guid>https://arxiv.org/abs/2511.23184</guid>
<content:encoded><![CDATA[
arXiv:2511.23184v1 Announce Type: new 
Abstract: Aspect sentiment quad prediction (ASQP) is inherently challenging to predict a structured quadruple with four core sentiment elements, including aspect term (a), aspect category (c), opinion term (o), and sentiment polarity (s). Prior methods relying on marker-based prediction struggle with modeling the intricate relationships among elements and experience sharp performance declines when predicting higher-order elements (e.g., c and s) under standard supervised fine-tuning. To address these limitations, we employ reasoning-based generation to output both the quadruple and a natural language rationale under element prefixes within a unified template, encouraging explicit relational reasoning and interpretability. To further enhance element-wise alignment, we introduce a listwise preference optimization framework for improving structural validity and relational coherence. Specifically, we generate element-wise confusable candidates via syntactic and semantic proximity, then train the model with listwise objectives to prefer the gold candidates over closely competing alternatives. Extensive experiments on four benchmark datasets demonstrate that our framework effectively improves quadruple prediction accuracy and explanation consistency.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TWEO: Transformers Without Extreme Outliers Enables FP8 Training And Quantization For Dummies</title>
<link>https://arxiv.org/abs/2511.23225</link>
<guid>https://arxiv.org/abs/2511.23225</guid>
<content:encoded><![CDATA[
arXiv:2511.23225v1 Announce Type: new 
Abstract: Native FP8 support in modern hardware is essential for training large Transformers, but is severely hindered by extreme activation outliers. Existing solutions either rely on complex mixed-precision engineering or invasive architectural modifications. This paper fundamentally challenges the conventional wisdom that outliers are data-driven. We demonstrate that extreme outliers are a data-independent, mechanically-produced artifact of training, originating from specific structural properties of the weight matrices (i.e., colinearity). Based on this insight, we propose TWEO (Transformers Without Extreme Outliers), a novel, non-invasive loss function. TWEO effectively prevents extreme outliers via a very simple loss term, which reduces outliers from 10000+ to less than 20. TWEO then enables full-model FP8 pre-training with neither engineering tricks nor architectural changes for both LLM and ViT. When standard FP8 training catastrophically collapses, TWEO achieves performance comparable to the BF16 baseline while delivering a 36% increase in training throughput. Also, TWEO enables a new quantization paradigm. Hardware-friendly W8A8 per-tensor static quantization of LLMs, previously considered completely unusable due to outliers, achieves SOTA performance for the first time on TWEO-trained models.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tourism Question Answer System in Indian Language using Domain-Adapted Foundation Models</title>
<link>https://arxiv.org/abs/2511.23235</link>
<guid>https://arxiv.org/abs/2511.23235</guid>
<content:encoded><![CDATA[
arXiv:2511.23235v1 Announce Type: new 
Abstract: This article presents the first comprehensive study on designing a baseline extractive question-answering (QA) system for the Hindi tourism domain, with a specialized focus on the Varanasi-a cultural and spiritual hub renowned for its Bhakti-Bhaav (devotional ethos). Targeting ten tourism-centric subdomains-Ganga Aarti, Cruise, Food Court, Public Toilet, Kund, Museum, General, Ashram, Temple and Travel, the work addresses the absence of language-specific QA resources in Hindi for culturally nuanced applications. In this paper, a dataset comprising 7,715 Hindi QA pairs pertaining to Varanasi tourism was constructed and subsequently augmented with 27,455 pairs generated via Llama zero-shot prompting. We propose a framework leveraging foundation models-BERT and RoBERTa, fine-tuned using Supervised Fine-Tuning (SFT) and Low-Rank Adaptation (LoRA), to optimize parameter efficiency and task performance. Multiple variants of BERT, including pre-trained languages (e.g., Hindi-BERT), are evaluated to assess their suitability for low-resource domain-specific QA. Evaluation metrics - F1, BLEU, and ROUGE-L - highlight trade-offs between answer precision and linguistic fluency. Experiments demonstrate that LoRA-based fine-tuning achieves competitive performance (85.3\% F1) while reducing trainable parameters by 98\% compared to SFT, striking a balance between efficiency and accuracy. Comparative analysis across models reveals that RoBERTa with SFT outperforms BERT variants in capturing contextual nuances, particularly for culturally embedded terms (e.g., Aarti, Kund). This work establishes a foundational baseline for Hindi tourism QA systems, emphasizing the role of LORA in low-resource settings and underscoring the need for culturally contextualized NLP frameworks in the tourism domain.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Behavior-Equivalent Token: Single-Token Replacement for Long Prompts in LLMs</title>
<link>https://arxiv.org/abs/2511.23271</link>
<guid>https://arxiv.org/abs/2511.23271</guid>
<content:encoded><![CDATA[
arXiv:2511.23271v1 Announce Type: new 
Abstract: Carefully engineered system prompts play a critical role in guiding the behavior of LLM agents, but their considerable length introduces significant drawbacks, including increased inference latency, higher computational cost, and reduced effective context length. This raises the question of whether such lengthy prompts can be replaced by a drastically reduced number of tokens while preserving their behavioral effect on downstream tasks. To enable this, we propose a lightweight three-stage training framework that learns a single prompt-specific Behavior-Equivalent token ([BE]). The framework first trains [BE] to encode the natural-language content of the original system prompt via reconstruction, and then distills the prompt 's downstream behavior into this single token. Importantly, our method requires no access to model internals, no auxiliary compression models, and no labeled responses. Empirical evaluations on three datasets show that a single [BE] token achieves up to a 3000x reduction in prompt length, while retaining about 98% of the downstream performance of the original system prompts. This substantially reduces inference cost and leaves almost the entire context window available for user inputs.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MCP vs RAG vs NLWeb vs HTML: A Comparison of the Effectiveness and Efficiency of Different Agent Interfaces to the Web (Technical Report)</title>
<link>https://arxiv.org/abs/2511.23281</link>
<guid>https://arxiv.org/abs/2511.23281</guid>
<content:encoded><![CDATA[
arXiv:2511.23281v1 Announce Type: new 
Abstract: Large language model agents are increasingly used to automate web tasks such as product search, offer comparison, and checkout. Current research explores different interfaces through which these agents interact with websites, including traditional HTML browsing, retrieval-augmented generation (RAG) over pre-crawled content, communication via Web APIs using the Model Context Protocol (MCP), and natural-language querying through the NLWeb interface. However, no prior work has compared these four architectures within a single controlled environment using identical tasks.
  To address this gap, we introduce a testbed consisting of four simulated e-shops, each offering its products via HTML, MCP, and NLWeb interfaces. For each interface (HTML, RAG, MCP, and NLWeb) we develop specialized agents that perform the same sets of tasks, ranging from simple product searches and price comparisons to complex queries for complementary or substitute products and checkout processes. We evaluate the agents using GPT 4.1, GPT 5, GPT 5 mini, and Claude Sonnet 4 as underlying LLM. Our evaluation shows that the RAG, MCP and NLWeb agents outperform HTML on both effectiveness and efficiency. Averaged over all tasks, F1 rises from 0.67 for HTML to between 0.75 and 0.77 for the other agents. Token usage falls from about 241k for HTML to between 47k and 140k per task. The runtime per task drops from 291 seconds to between 50 and 62 seconds. The best overall configuration is RAG with GPT 5 achieving an F1 score of 0.87 and a completion rate of 0.79. Also taking cost into consideration, RAG with GPT 5 mini offers a good compromise between API usage fees and performance. Our experiments show the choice of the interaction interface has a substantial impact on both the effectiveness and efficiency of LLM-based web agents.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models</title>
<link>https://arxiv.org/abs/2511.23319</link>
<guid>https://arxiv.org/abs/2511.23319</guid>
<content:encoded><![CDATA[
arXiv:2511.23319v1 Announce Type: new 
Abstract: This work explores the challenge of building ``Machines that Can Remember'', framing long-term memory as the problem of efficient ultra-long context modeling. We argue that this requires three key properties: \textbf{sparsity}, \textbf{random-access flexibility}, and \textbf{length generalization}. To address ultra-long-context modeling, we leverage Hierarchical Sparse Attention (HSA), a novel attention mechanism that satisfies all three properties. We integrate HSA into Transformers to build HSA-UltraLong, which is an 8B-parameter MoE model trained on over 8 trillion tokens and is rigorously evaluated on different tasks with in-domain and out-of-domain context lengths to demonstrate its capability in handling ultra-long contexts. Results show that our model performs comparably to full-attention baselines on in-domain lengths while achieving over 90\% accuracy on most in-context retrieval tasks with contexts up to 16M. This report outlines our experimental insights and open problems, contributing a foundation for future research in ultra-long context modeling.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tackling a Challenging Corpus for Early Detection of Gambling Disorder: UNSL at MentalRiskES 2025</title>
<link>https://arxiv.org/abs/2511.23325</link>
<guid>https://arxiv.org/abs/2511.23325</guid>
<content:encoded><![CDATA[
arXiv:2511.23325v1 Announce Type: new 
Abstract: Gambling disorder is a complex behavioral addiction that is challenging to understand and address, with severe physical, psychological, and social consequences. Early Risk Detection (ERD) on the Web has become a key task in the scientific community for identifying early signs of mental health behaviors based on social media activity. This work presents our participation in the MentalRiskES 2025 challenge, specifically in Task 1, aimed at classifying users at high or low risk of developing a gambling-related disorder. We proposed three methods based on a CPI+DMC approach, addressing predictive effectiveness and decision-making speed as independent objectives. The components were implemented using the SS3, BERT with extended vocabulary, and SBERT models, followed by decision policies based on historical user analysis. Although it was a challenging corpus, two of our proposals achieved the top two positions in the official results, performing notably in decision metrics. Further analysis revealed some difficulty in distinguishing between users at high and low risk, reinforcing the need to explore strategies to improve data interpretation and quality, and to promote more transparent and reliable ERD systems for mental disorders.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Improving Interpretability of Language Model Generation through a Structured Knowledge Discovery Approach</title>
<link>https://arxiv.org/abs/2511.23335</link>
<guid>https://arxiv.org/abs/2511.23335</guid>
<content:encoded><![CDATA[
arXiv:2511.23335v1 Announce Type: new 
Abstract: Knowledge-enhanced text generation aims to enhance the quality of generated text by utilizing internal or external knowledge sources. While language models have demonstrated impressive capabilities in generating coherent and fluent text, the lack of interpretability presents a substantial obstacle. The limited interpretability of generated text significantly impacts its practical usability, particularly in knowledge-enhanced text generation tasks that necessitate reliability and explainability. Existing methods often employ domain-specific knowledge retrievers that are tailored to specific data characteristics, limiting their generalizability to diverse data types and tasks. To overcome this limitation, we directly leverage the two-tier architecture of structured knowledge, consisting of high-level entities and low-level knowledge triples, to design our task-agnostic structured knowledge hunter. Specifically, we employ a local-global interaction scheme for structured knowledge representation learning and a hierarchical transformer-based pointer network as the backbone for selecting relevant knowledge triples and entities. By combining the strong generative ability of language models with the high faithfulness of the knowledge hunter, our model achieves high interpretability, enabling users to comprehend the model output generation process. Furthermore, we empirically demonstrate the effectiveness of our model in both internal knowledge-enhanced table-to-text generation on the RotoWireFG dataset and external knowledge-enhanced dialogue response generation on the KdConv dataset. Our task-agnostic model outperforms state-of-the-art methods and corresponding language models, setting new standards on the benchmark.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling HuBERT for African Languages: From Base to Large and XL</title>
<link>https://arxiv.org/abs/2511.23370</link>
<guid>https://arxiv.org/abs/2511.23370</guid>
<content:encoded><![CDATA[
arXiv:2511.23370v1 Announce Type: new 
Abstract: Despite recent progress in multilingual speech processing, African languages remain under-represented in both research and deployed systems, particularly when it comes to strong, open-weight encoders that transfer well under low-resource supervision. Self-supervised learning has proven especially promising in such settings, yet most publicly released models targeting African speech remain at BASE scale, leaving unanswered whether larger encoders, trained exclusively on Africa-centric audio, offer tangible benefits and how model capacity interacts with data composition. This work addresses that gap by introducing SSA-HuBERT-Large (317M parameters) and SSA-HuBERT-XL (964M parameters), the first large models trained solely on African speech, alongside a BASE size counterpart. We release these models as open weights: see https://huggingface.co/collections/Orange/african-speech-foundation-models. By conducting a carefully controlled experimental study focused exclusively on Sub-Saharan languages, covering automatic speech recognition (ASR) and language identification (LID) tasks, we demonstrate that larger architectures significantly improve performance by effectively leveraging large audio datasets.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Multimodal Language Models through Attention-based Interpretability</title>
<link>https://arxiv.org/abs/2511.23375</link>
<guid>https://arxiv.org/abs/2511.23375</guid>
<content:encoded><![CDATA[
arXiv:2511.23375v1 Announce Type: new 
Abstract: Modern large language models become multimodal, analyzing various data formats like text and images. While fine-tuning is effective for adapting these multimodal language models (MLMs) to downstream tasks, full fine-tuning is computationally expensive. Parameter-Efficient Fine-Tuning (PEFT) methods address this by training only a small portion of model weights. However, MLMs are difficult to interpret, making it challenging to identify which components are most effective for training to balance efficiency and performance. We propose an attention-based interpretability method for MLMs by analyzing attention scores relative to image tokens. The core idea is to identify attention heads that focus on image key objects. We utilize this information to select optimal model components for PEFT in multimodal models. Our contributions include a method for identifying attention heads associated with image key objects, its application to PEFT for image captioning, and the creation of a new dataset containing images, key object masks, and their textual descriptions. We conducted experiments on MLMs with 2-3 billion parameters to validate the method's effectiveness. By calculating Head Impact (HI) scores we quantify an attention head's focus on key objects, indicating its significance in image understanding. Our fine-tuning experiments demonstrate that adapting layers with the highest HI scores leads to the most significant shifts in metrics compared to pre-trained, randomly selected, or lowest-HI-score layers. This indicates that fine-tuning a small percentage (around 0.01%) of parameters in these crucial layers can substantially influence image understanding capabilities.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ambiguity Awareness Optimization: Towards Semantic Disambiguation for Direct Preference Optimization</title>
<link>https://arxiv.org/abs/2511.23391</link>
<guid>https://arxiv.org/abs/2511.23391</guid>
<content:encoded><![CDATA[
arXiv:2511.23391v1 Announce Type: new 
Abstract: Direct Preference Optimization (DPO) is a widely used reinforcement learning from human feedback (RLHF) method across various domains. Recent research has increasingly focused on the role of token importance in improving DPO effectiveness. It is observed that identical or semantically similar content (defined as ambiguous content) frequently appears within the preference pairs. We hypothesize that the presence of ambiguous content during DPO training may introduce ambiguity, thereby limiting further improvements in alignment. Through mathematical analysis and proof-of-concept experiments, we reveal that ambiguous content may potentially introduce ambiguities, thereby degrading performance. To address this issue, we introduce Ambiguity Awareness Optimization (AAO), a simple yet effective approach that automatically re-weights ambiguous content to reduce ambiguities by calculating semantic similarity from preference pairs. Through extensive experiments, we demonstrate that AAO consistently and significantly surpasses state-of-the-art approaches in performance, without markedly increasing response length, across multiple model scales and widely adopted benchmark datasets, including AlpacaEval 2, MT-Bench, and Arena-Hard. Specifically, AAO outperforms DPO by up to 8.9 points on AlpacaEval 2 and achieves an improvement of by up to 15.0 points on Arena-Hard.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MegaChat: A Synthetic Persian Q&amp;A Dataset for High-Quality Sales Chatbot Evaluation</title>
<link>https://arxiv.org/abs/2511.23397</link>
<guid>https://arxiv.org/abs/2511.23397</guid>
<content:encoded><![CDATA[
arXiv:2511.23397v1 Announce Type: new 
Abstract: Small and medium-sized enterprises (SMEs) in Iran increasingly leverage Telegram for sales, where real-time engagement is essential for conversion. However, developing AI-driven chatbots for this purpose requires large, high-quality question-and-answer (Q&amp;A) datasets, which are typically expensive and resource-intensive to produce, especially for low-resource languages like Persian. In this paper, we introduce MegaChat, the first fully synthetic Persian Q&amp;A dataset designed to evaluate intelligent sales chatbots in Telegram-based e-commerce. We propose a novel, automated multi-agent architecture that generates persona-aware Q&amp;A pairs by collecting data from active Telegram shopping channels. The system employs specialized agents for question generation, validation, and refinement, ensuring the production of realistic and diverse conversational data. To evaluate answer generation, we compare three classic retrieval-augmented generation (RAG) models with our advanced agentic system, which features multi-query retrieval, reranking, and persona-aligned response synthesis. Using GPT-5.1 for evaluation across six quality dimensions, our results show that the agentic architecture outperformed traditional RAG models in 4 out of 5 diverse channels, demonstrating its ability to generate scalable, high-quality datasets without relying on expensive human annotation or complex fine-tuning. MegaChat provides SMEs with an efficient, cost-effective solution for building intelligent customer engagement systems in specialized commercial domains, enabling advancements in multilingual conversational AI for low-resource languages. Download: https://github.com/MegaChat-Tech/MegaChat-DataSet
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SO-Bench: A Structural Output Evaluation of Multimodal LLMs</title>
<link>https://arxiv.org/abs/2511.21750</link>
<guid>https://arxiv.org/abs/2511.21750</guid>
<content:encoded><![CDATA[
arXiv:2511.21750v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) are increasingly deployed in real-world, agentic settings where outputs must not only be correct, but also conform to predefined data schemas. Despite recent progress in structured generation in textual domain, there is still no benchmark that systematically evaluates schema-grounded information extraction and reasoning over visual inputs. In this work, we conduct a comprehensive study of visual structural output capabilities for MLLMs with our carefully designed SO-Bench benchmark. Covering four visual domains, including UI screens, natural images, documents, and charts, SO-Bench is built from over 6.5K diverse JSON schemas and 1.8K curated image-schema pairs with human-verified quality. Benchmarking experiments on open-sourced and frontier proprietary models reveal persistent gaps in predicting accurate, schema compliant outputs, highlighting the need for better multimodal structured reasoning. Beyond benchmarking, we further conduct training experiments to largely improve the model's structured output capability. We plan to make the benchmark available to the community.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Medical Malice: A Dataset for Context-Aware Safety in Healthcare LLMs</title>
<link>https://arxiv.org/abs/2511.21757</link>
<guid>https://arxiv.org/abs/2511.21757</guid>
<content:encoded><![CDATA[
arXiv:2511.21757v1 Announce Type: cross 
Abstract: The integration of Large Language Models (LLMs) into healthcare demands a safety paradigm rooted in \textit{primum non nocere}. However, current alignment techniques rely on generic definitions of harm that fail to capture context-dependent violations, such as administrative fraud and clinical discrimination. To address this, we introduce Medical Malice: a dataset of 214,219 adversarial prompts calibrated to the regulatory and ethical complexities of the Brazilian Unified Health System (SUS). Crucially, the dataset includes the reasoning behind each violation, enabling models to internalize ethical boundaries rather than merely memorizing a fixed set of refusals. Using an unaligned agent (Grok-4) within a persona-driven pipeline, we synthesized high-fidelity threats across seven taxonomies, ranging from procurement manipulation and queue-jumping to obstetric violence. We discuss the ethical design of releasing these "vulnerability signatures" to correct the information asymmetry between malicious actors and AI developers. Ultimately, this work advocates for a shift from universal to context-aware safety, providing the necessary resources to immunize healthcare AI against the nuanced, systemic threats inherent to high-stakes medical environments -- vulnerabilities that represent the paramount risk to patient safety and the successful integration of AI in healthcare systems.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Topology to Retrieval: Decoding Embedding Spaces with Unified Signatures</title>
<link>https://arxiv.org/abs/2511.22150</link>
<guid>https://arxiv.org/abs/2511.22150</guid>
<content:encoded><![CDATA[
arXiv:2511.22150v1 Announce Type: cross 
Abstract: Studying how embeddings are organized in space not only enhances model interpretability but also uncovers factors that drive downstream task performance. In this paper, we present a comprehensive analysis of topological and geometric measures across a wide set of text embedding models and datasets. We find a high degree of redundancy among these measures and observe that individual metrics often fail to sufficiently differentiate embedding spaces. Building on these insights, we introduce Unified Topological Signatures (UTS), a holistic framework for characterizing embedding spaces. We show that UTS can predict model-specific properties and reveal similarities driven by model architecture. Further, we demonstrate the utility of our method by linking topological structure to ranking effectiveness and accurately predicting document retrievability. We find that a holistic, multi-attribute perspective is essential to understanding and leveraging the geometry of text embeddings.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Compound Figures to Composite Understanding: Developing a Multi-Modal LLM from Biomedical Literature with Medical Multiple-Image Benchmarking and Validation</title>
<link>https://arxiv.org/abs/2511.22232</link>
<guid>https://arxiv.org/abs/2511.22232</guid>
<content:encoded><![CDATA[
arXiv:2511.22232v1 Announce Type: cross 
Abstract: Multi-modal large language models (MLLMs) have shown promise in advancing healthcare. However, most existing models remain confined to single-image understanding, which greatly limits their applicability in clinical workflows. In practice, medical diagnosis and progression often require synthesizing information across multiple images from different modalities or time points. The development of medical MLLMs capable of such multi-image understanding has been hindered by the lack of large-scale, high-quality annotated training data. To address this limitation, we propose a novel framework that leverages license-permissive compound images in biomedical literature, as a rich yet underutilized data source for multi-image analysis. Specifically, we design a five-stage, context-aware instruction generation paradigm underpinned by a divide-and-conquer strategy. By decomposing multi-image analysis into manageable sub-tasks, this paradigm empowers MLLMs to move beyond single-panel analysis and provide a composite understanding by learning the complex spatial, temporal, and cross-modal relationships inherent in these compound figures. By parsing over 237,000 compound figures and their contextual text for instruction generation, we develop M3LLM, a medical multi-image multi-modal large language model. For benchmarking, we construct PMC-MI-Bench for composite understanding, manually validated by medical experts. Extensive experiments show that M3LLM significantly outperforms both general-purpose and specialized medical MLLMs across multi-image, single-image, text-only, and multi-choice scenarios. Notably, M3LLM exhibits strong generalization to longitudinal chest X-ray analysis using the MIMIC dataset. This work establishes a scalable and efficient paradigm for developing medical MLLMs capable of composite reasoning, bridging the gap between biomedical literature and real-world clinical applications.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Swarms of Large Language Model Agents for Protein Sequence Design with Experimental Validation</title>
<link>https://arxiv.org/abs/2511.22311</link>
<guid>https://arxiv.org/abs/2511.22311</guid>
<content:encoded><![CDATA[
arXiv:2511.22311v1 Announce Type: cross 
Abstract: Designing proteins de novo with tailored structural, physicochemical, and functional properties remains a grand challenge in biotechnology, medicine, and materials science, due to the vastness of sequence space and the complex coupling between sequence, structure, and function. Current state-of-the-art generative methods, such as protein language models (PLMs) and diffusion-based architectures, often require extensive fine-tuning, task-specific data, or model reconfiguration to support objective-directed design, thereby limiting their flexibility and scalability. To overcome these limitations, we present a decentralized, agent-based framework inspired by swarm intelligence for de novo protein design. In this approach, multiple large language model (LLM) agents operate in parallel, each assigned to a specific residue position. These agents iteratively propose context-aware mutations by integrating design objectives, local neighborhood interactions, and memory and feedback from previous iterations. This position-wise, decentralized coordination enables emergent design of diverse, well-defined sequences without reliance on motif scaffolds or multiple sequence alignments, validated with experiments on proteins with alpha helix and coil structures. Through analyses of residue conservation, structure-based metrics, and sequence convergence and embeddings, we demonstrate that the framework exhibits emergent behaviors and effective navigation of the protein fitness landscape. Our method achieves efficient, objective-directed designs within a few GPU-hours and operates entirely without fine-tuning or specialized training, offering a generalizable and adaptable solution for protein design. Beyond proteins, the approach lays the groundwork for collective LLM-driven design across biomolecular systems and other scientific discovery tasks.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PAT: Accelerating LLM Decoding via Prefix-Aware Attention with Resource Efficient Multi-Tile Kernel</title>
<link>https://arxiv.org/abs/2511.22333</link>
<guid>https://arxiv.org/abs/2511.22333</guid>
<content:encoded><![CDATA[
arXiv:2511.22333v1 Announce Type: cross 
Abstract: LLM serving is increasingly dominated by decode attention, which is a memory-bound operation due to massive KV cache loading from global memory. Meanwhile, real-world workloads exhibit substantial, hierarchical shared prefixes across requests (e.g., system prompts, tools/templates, RAG). Existing attention implementations fail to fully exploit prefix sharing: *one-query-per-CTA* execution repeatedly loads shared prefix KV cache, while *one-size-fits-all* tiling leaves on-chip resources idle and exacerbates bubbles for uneven KV lengths. These choices amplify memory bandwidth pressure and stall memory-bound decode attention.
  This paper introduces PAT, a prefix-aware attention kernel implementation for LLM decoding that organizes execution with a pack-forward-merge paradigm. PAT packs queries by shared prefix to reduce repeated memory accesses, runs a customized multi-tile kernel to achieve high resource efficiency. It further applies practical multi-stream forwarding and KV splitting to reduce resource bubbles. The final merge performs online softmax with negligible overhead. We implement PAT as an off-the-shelf plugin for vLLM. Evaluation on both real-world and synthetic workloads shows that PAT reduces attention latency by 67.4% on average and TPOT by 13.6-83.4% under the same configurations against state-of-the-art attention kernels.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SuRe: Surprise-Driven Prioritised Replay for Continual LLM Learning</title>
<link>https://arxiv.org/abs/2511.22367</link>
<guid>https://arxiv.org/abs/2511.22367</guid>
<content:encoded><![CDATA[
arXiv:2511.22367v1 Announce Type: cross 
Abstract: Continual learning, one's ability to adapt to a sequence of tasks without forgetting previously acquired knowledge, remains a major challenge in machine learning and a key gap between artificial and human intelligence. While regularisation and replay perform well in vision, they lag behind multi-task learning for large language models (LLMs), especially at scale with many tasks. We revisit replay and argue that two failure modes drive this gap: selection (what to rehearse) and integration (how to consolidate new knowledge). To address selection, we propose Surprise-prioritised Replay (SuRe), a simple, architecture-agnostic rule that ranks and stores the most surprising (high Negative Log-Likelihood) sequences. SuRe achieves state-of-the-art performance in the Large Number of Tasks (LNT) setting and delivers the best overall average across both Standard CL and LNT benchmarks. To address integration, we add a dual-learner design with fast and slow LoRA adapters merged via an exponential moving average (EMA), enabling rapid adaptation while stabilising long-term knowledge. Combining SuRe with the dual learner yields further gains, including improvements of up to +5 accuracy points on LNT over prior SOTA. Ablation studies confirm that our proposed method remains robust under reduced replay frequency and small buffer size, demonstrating both effectiveness and sample efficiency. Taken together, our results establish replay as a strong baseline for continual LLM fine-tuning and demonstrate that surprise-based selection and slow-weight consolidation are complementary components for mitigating catastrophic forgetting.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Shape Is Optimal for Masks in Text Removal?</title>
<link>https://arxiv.org/abs/2511.22499</link>
<guid>https://arxiv.org/abs/2511.22499</guid>
<content:encoded><![CDATA[
arXiv:2511.22499v1 Announce Type: cross 
Abstract: The advent of generative models has dramatically improved the accuracy of image inpainting. In particular, by removing specific text from document images, reconstructing original images is extremely important for industrial applications. However, most existing methods of text removal focus on deleting simple scene text which appears in images captured by a camera in an outdoor environment. There is little research dedicated to complex and practical images with dense text. Therefore, we created benchmark data for text removal from images including a large amount of text. From the data, we found that text-removal performance becomes vulnerable against mask profile perturbation. Thus, for practical text-removal tasks, precise tuning of the mask shape is essential. This study developed a method to model highly flexible mask profiles and learn their parameters using Bayesian optimization. The resulting profiles were found to be character-wise masks. It was also found that the minimum cover of a text region is not optimal. Our research is expected to pave the way for a user-friendly guideline for manual masking.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2511.22570</link>
<guid>https://arxiv.org/abs/2511.22570</guid>
<content:encoded><![CDATA[
arXiv:2511.22570v1 Announce Type: cross 
Abstract: Large language models have made significant progress in mathematical reasoning, which serves as an important testbed for AI and could impact scientific research if further advanced. By scaling reasoning with reinforcement learning that rewards correct final answers, LLMs have improved from poor performance to saturating quantitative reasoning competitions like AIME and HMMT in one year. However, this approach faces fundamental limitations. Pursuing higher final answer accuracy doesn't address a key issue: correct answers don't guarantee correct reasoning. Moreover, many mathematical tasks like theorem proving require rigorous step-by-step derivation rather than numerical answers, making final answer rewards inapplicable. To push the limits of deep reasoning, we believe it is necessary to verify the comprehensiveness and rigor of mathematical reasoning. Self-verification is particularly important for scaling test-time compute, especially for open problems without known solutions. Towards self-verifiable mathematical reasoning, we investigate how to train an accurate and faithful LLM-based verifier for theorem proving. We then train a proof generator using the verifier as the reward model, and incentivize the generator to identify and resolve as many issues as possible in their own proofs before finalizing them. To maintain the generation-verification gap as the generator becomes stronger, we propose to scale verification compute to automatically label new hard-to-verify proofs, creating training data to further improve the verifier. Our resulting model, DeepSeekMath-V2, demonstrates strong theorem-proving capabilities, achieving gold-level scores on IMO 2025 and CMO 2024 and a near-perfect 118/120 on Putnam 2024 with scaled test-time compute.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mechanistic Finetuning of Vision-Language-Action Models via Few-Shot Demonstrations</title>
<link>https://arxiv.org/abs/2511.22697</link>
<guid>https://arxiv.org/abs/2511.22697</guid>
<content:encoded><![CDATA[
arXiv:2511.22697v1 Announce Type: cross 
Abstract: Vision-Language Action (VLAs) models promise to extend the remarkable success of vision-language models (VLMs) to robotics. Yet, unlike VLMs in the vision-language domain, VLAs for robotics require finetuning to contend with varying physical factors like robot embodiment, environment characteristics, and spatial relationships of each task. Existing fine-tuning methods lack specificity, adapting the same set of parameters regardless of a task's visual, linguistic, and physical characteristics. Inspired by functional specificity in neuroscience, we hypothesize that it is more effective to finetune sparse model representations specific to a given task. In this work, we introduce Robotic Steering, a finetuning approach grounded in mechanistic interpretability that leverages few-shot demonstrations to identify and selectively finetune task-specific attention heads aligned with the physical, visual, and linguistic requirements of robotic tasks. Through comprehensive on-robot evaluations with a Franka Emika robot arm, we demonstrate that Robotic Steering outperforms LoRA while achieving superior robustness under task variation, reduced computational cost, and enhanced interpretability for adapting VLAs to diverse robotic tasks.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReAG: Reasoning-Augmented Generation for Knowledge-based Visual Question Answering</title>
<link>https://arxiv.org/abs/2511.22715</link>
<guid>https://arxiv.org/abs/2511.22715</guid>
<content:encoded><![CDATA[
arXiv:2511.22715v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) have shown impressive capabilities in jointly understanding text, images, and videos, often evaluated via Visual Question Answering (VQA). However, even state-of-the-art MLLMs struggle with domain-specific or knowledge-intensive queries, where relevant information is underrepresented in pre-training data. Knowledge-based VQA (KB-VQA) addresses this by retrieving external documents to condition answer generation, but current retrieval-augmented approaches suffer from low precision, noisy passages, and limited reasoning. To address this, we propose ReAG, a novel Reasoning-Augmented Multimodal RAG approach that combines coarse- and fine-grained retrieval with a critic model that filters irrelevant passages, ensuring high-quality additional context. The model follows a multi-stage training strategy leveraging reinforcement learning to enhance reasoning over retrieved content, while supervised fine-tuning serves only as a cold start. Extensive experiments on Encyclopedic-VQA and InfoSeek demonstrate that ReAG significantly outperforms prior methods, improving answer accuracy and providing interpretable reasoning grounded in retrieved evidence. Our source code is publicly available at: https://github.com/aimagelab/ReAG.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRISM: Privacy-Aware Routing for Adaptive Cloud-Edge LLM Inference via Semantic Sketch Collaboration</title>
<link>https://arxiv.org/abs/2511.22788</link>
<guid>https://arxiv.org/abs/2511.22788</guid>
<content:encoded><![CDATA[
arXiv:2511.22788v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) demonstrate impressive capabilities in natural language understanding and generation, but incur high communication overhead and privacy risks in cloud deployments, while facing compute and memory constraints when confined to edge devices. Cloud-edge inference has emerged as a promising paradigm for improving privacy in LLM services by retaining sensitive computations on local devices. However, existing cloud-edge inference approaches apply uniform privacy protection without considering input sensitivity, resulting in unnecessary perturbation and degraded utility even for non-sensitive tokens. To address this limitation, we propose Privacy-aware Routing for Inference with Semantic Modulation (PRISM), a context-aware framework that dynamically balances privacy and inference quality. PRISM executes in four stages: (1) the edge device profiles entity-level sensitivity; (2) a soft gating module on the edge selects an execution mode - cloud, edge, or collaboration; (3) for collaborative paths, the edge applies adaptive two-layer local differential privacy based on entity risks; and (4) the cloud LLM generates a semantic sketch from the perturbed prompt, which is then refined by the edge-side small language model (SLM) using local context. Our results show that PRISM consistently achieves superior privacy-utility trade-offs across various scenarios, reducing energy consumption and latency to 40-50% of baseline methods such as Uniform and Selective LDP, while maintaining high output quality under strong privacy constraints. These findings are validated through comprehensive evaluations involving realistic prompts, actual energy measurements, and heterogeneous cloud-edge model deployments.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intelligent Neural Networks: From Layered Architectures to Graph-Organized Intelligence</title>
<link>https://arxiv.org/abs/2511.22813</link>
<guid>https://arxiv.org/abs/2511.22813</guid>
<content:encoded><![CDATA[
arXiv:2511.22813v1 Announce Type: cross 
Abstract: Biological neurons exhibit remarkable intelligence: they maintain internal states, communicate selectively with other neurons, and self-organize into complex graphs rather than rigid hierarchical layers. What if artificial intelligence could emerge from similarly intelligent computational units? We introduce Intelligent Neural Networks (INN), a paradigm shift where neurons are first-class entities with internal memory and learned communication patterns, organized in complete graphs rather than sequential layers.
  Each Intelligent Neuron combines selective state-space dynamics (knowing when to activate) with attention-based routing (knowing to whom to send signals), enabling emergent computation through graph-structured interactions. On the standard Text8 character modeling benchmark, INN achieves 1.705 Bit-Per-Character (BPC), significantly outperforming a comparable Transformer (2.055 BPC) and matching a highly optimized LSTM baseline. Crucially, a parameter-matched baseline of stacked Mamba blocks fails to converge (>3.4 BPC) under the same training protocol, demonstrating that INN's graph topology provides essential training stability. Ablation studies confirm this: removing inter-neuron communication degrades performance or leads to instability, proving the value of learned neural routing.
  This work demonstrates that neuron-centric design with graph organization is not merely bio-inspired -- it is computationally effective, opening new directions for modular, interpretable, and scalable neural architectures.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ORION: Teaching Language Models to Reason Efficiently in the Language of Thought</title>
<link>https://arxiv.org/abs/2511.22891</link>
<guid>https://arxiv.org/abs/2511.22891</guid>
<content:encoded><![CDATA[
arXiv:2511.22891v1 Announce Type: cross 
Abstract: Large Reasoning Models (LRMs) achieve strong performance in mathematics, code generation, and task planning, but their reliance on long chains of verbose "thinking" tokens leads to high latency, redundancy, and incoherent reasoning paths. Inspired by the Language of Thought Hypothesis, which posits that human reasoning operates over a symbolic, compositional mental language called Mentalese, we introduce a framework that trains models to reason in a similarly compact style. Mentalese encodes abstract reasoning as ultra-compressed, structured tokens, enabling models to solve complex problems with far fewer steps. To improve both efficiency and accuracy, we propose SHORTER LENGTH PREFERENCE OPTIMIZATION (SLPO), a reinforcement learning method that rewards concise solutions that stay correct, while still allowing longer reasoning when needed. Applied to Mentalese-aligned models, SLPO yields significantly higher compression rates by enabling concise reasoning that preserves the benefits of detailed thinking without the computational overhead. Across benchmarks including AIME 2024 and 2025, MinervaMath, OlympiadBench, Math500, and AMC, our ORION models produce reasoning traces with 4-16x fewer tokens, achieve up to 5x lower inference latency, and reduce training costs by 7-9x relative to the DeepSeek R1 Distilled model, while maintaining 90-98% of its accuracy. ORION also surpasses Claude and ChatGPT-4o by up to 5% in accuracy while maintaining 2x compression. These results show that Mentalese-style compressed reasoning offers a step toward human-like cognitive efficiency, enabling real-time, cost-effective reasoning without sacrificing accuracy.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Artwork Interpretation with Vision Language Models: A Case Study on Emotions and Emotion Symbols</title>
<link>https://arxiv.org/abs/2511.22929</link>
<guid>https://arxiv.org/abs/2511.22929</guid>
<content:encoded><![CDATA[
arXiv:2511.22929v1 Announce Type: cross 
Abstract: Emotions are a fundamental aspect of artistic expression. Due to their abstract nature, there is a broad spectrum of emotion realization in artworks. These are subject to historical change and their analysis requires expertise in art history. In this article, we investigate which aspects of emotional expression can be detected by current (2025) vision language models (VLMs). We present a case study of three VLMs (Llava-Llama and two Qwen models) in which we ask these models four sets of questions of increasing complexity about artworks (general content, emotional content, expression of emotions, and emotion symbols) and carry out a qualitative expert evaluation. We find that the VLMs recognize the content of the images surprisingly well and often also which emotions they depict and how they are expressed. The models perform best for concrete images but fail for highly abstract or highly symbolic images. Reliable recognition of symbols remains fundamentally difficult. Furthermore, the models continue to exhibit the well-known LLM weakness of providing inconsistent answers to related questions.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bharat Scene Text: A Novel Comprehensive Dataset and Benchmark for Indian Language Scene Text Understanding</title>
<link>https://arxiv.org/abs/2511.23071</link>
<guid>https://arxiv.org/abs/2511.23071</guid>
<content:encoded><![CDATA[
arXiv:2511.23071v1 Announce Type: cross 
Abstract: Reading scene text, that is, text appearing in images, has numerous application areas, including assistive technology, search, and e-commerce. Although scene text recognition in English has advanced significantly and is often considered nearly a solved problem, Indian language scene text recognition remains an open challenge. This is due to script diversity, non-standard fonts, and varying writing styles, and, more importantly, the lack of high-quality datasets and open-source models. To address these gaps, we introduce the Bharat Scene Text Dataset (BSTD) - a large-scale and comprehensive benchmark for studying Indian Language Scene Text Recognition. It comprises more than 100K words that span 11 Indian languages and English, sourced from over 6,500 scene images captured across various linguistic regions of India. The dataset is meticulously annotated and supports multiple scene text tasks, including: (i) Scene Text Detection, (ii) Script Identification, (iii) Cropped Word Recognition, and (iv) End-to-End Scene Text Recognition. We evaluated state-of-the-art models originally developed for English by adapting (fine-tuning) them for Indian languages. Our results highlight the challenges and opportunities in Indian language scene text recognition. We believe that this dataset represents a significant step toward advancing research in this domain. All our models and data are open source.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BanglaSentNet: An Explainable Hybrid Deep Learning Framework for Multi-Aspect Sentiment Analysis with Cross-Domain Transfer Learning</title>
<link>https://arxiv.org/abs/2511.23264</link>
<guid>https://arxiv.org/abs/2511.23264</guid>
<content:encoded><![CDATA[
arXiv:2511.23264v1 Announce Type: cross 
Abstract: Multi-aspect sentiment analysis of Bangla e-commerce reviews remains challenging due to limited annotated datasets, morphological complexity, code-mixing phenomena, and domain shift issues, affecting 300 million Bangla-speaking users. Existing approaches lack explainability and cross-domain generalization capabilities crucial for practical deployment. We present BanglaSentNet, an explainable hybrid deep learning framework integrating LSTM, BiLSTM, GRU, and BanglaBERT through dynamic weighted ensemble learning for multi-aspect sentiment classification. We introduce a dataset of 8,755 manually annotated Bangla product reviews across four aspects (Quality, Service, Price, Decoration) from major Bangladeshi e-commerce platforms. Our framework incorporates SHAP-based feature attribution and attention visualization for transparent insights. BanglaSentNet achieves 85% accuracy and 0.88 F1-score, outperforming standalone deep learning models by 3-7% and traditional approaches substantially. The explainability suite achieves 9.4/10 interpretability score with 87.6% human agreement. Cross-domain transfer learning experiments reveal robust generalization: zero-shot performance retains 67-76% effectiveness across diverse domains (BanglaBook reviews, social media, general e-commerce, news headlines); few-shot learning with 500-1000 samples achieves 90-95% of full fine-tuning performance, significantly reducing annotation costs. Real-world deployment demonstrates practical utility for Bangladeshi e-commerce platforms, enabling data-driven decision-making for pricing optimization, service improvement, and customer experience enhancement. This research establishes a new state-of-the-art benchmark for Bangla sentiment analysis, advances ensemble learning methodologies for low-resource languages, and provides actionable solutions for commercial applications.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transformer-Driven Triple Fusion Framework for Enhanced Multimodal Author Intent Classification in Low-Resource Bangla</title>
<link>https://arxiv.org/abs/2511.23287</link>
<guid>https://arxiv.org/abs/2511.23287</guid>
<content:encoded><![CDATA[
arXiv:2511.23287v1 Announce Type: cross 
Abstract: The expansion of the Internet and social networks has led to an explosion of user-generated content. Author intent understanding plays a crucial role in interpreting social media content. This paper addresses author intent classification in Bangla social media posts by leveraging both textual and visual data. Recognizing limitations in previous unimodal approaches, we systematically benchmark transformer-based language models (mBERT, DistilBERT, XLM-RoBERTa) and vision architectures (ViT, Swin, SwiftFormer, ResNet, DenseNet, MobileNet), utilizing the Uddessho dataset of 3,048 posts spanning six practical intent categories. We introduce a novel intermediate fusion strategy that significantly outperforms early and late fusion on this task. Experimental results show that intermediate fusion, particularly with mBERT and Swin Transformer, achieves 84.11% macro-F1 score, establishing a new state-of-the-art with an 8.4 percentage-point improvement over prior Bangla multimodal approaches. Our analysis demonstrates that integrating visual context substantially enhances intent classification. Cross-modal feature integration at intermediate levels provides optimal balance between modality-specific representation and cross-modal learning. This research establishes new benchmarks and methodological standards for Bangla and other low-resource languages. We call our proposed framework BangACMM (Bangla Author Content MultiModal).
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Automatic Safe Driving Instruction: A Large-Scale Vision Language Model Approach</title>
<link>https://arxiv.org/abs/2511.23311</link>
<guid>https://arxiv.org/abs/2511.23311</guid>
<content:encoded><![CDATA[
arXiv:2511.23311v1 Announce Type: cross 
Abstract: Large-scale Vision Language Models (LVLMs) exhibit advanced capabilities in tasks that require visual information, including object detection. These capabilities have promising applications in various industrial domains, such as autonomous driving. For example, LVLMs can generate safety-oriented descriptions of videos captured by road-facing cameras. However, ensuring comprehensive safety requires monitoring driver-facing views as well to detect risky events, such as the use of mobiles while driving. Thus, the ability to process synchronized inputs is necessary from both driver-facing and road-facing cameras. In this study, we develop models and investigate the capabilities of LVLMs by constructing a dataset and evaluating their performance on this dataset. Our experimental results demonstrate that while pre-trained LVLMs have limited effectiveness, fine-tuned LVLMs can generate accurate and safety-aware driving instructions. Nonetheless, several challenges remain, particularly in detecting subtle or complex events in the video. Our findings and error analysis provide valuable insights that can contribute to the improvement of LVLM-based systems in this domain.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Is Passive Expertise-Based Personalization Enough? A Case Study in AI-Assisted Test-Taking</title>
<link>https://arxiv.org/abs/2511.23376</link>
<guid>https://arxiv.org/abs/2511.23376</guid>
<content:encoded><![CDATA[
arXiv:2511.23376v1 Announce Type: cross 
Abstract: Novice and expert users have different systematic preferences in task-oriented dialogues. However, whether catering to these preferences actually improves user experience and task performance remains understudied. To investigate the effects of expertise-based personalization, we first built a version of an enterprise AI assistant with passive personalization. We then conducted a user study where participants completed timed exams, aided by the two versions of the AI assistant. Preliminary results indicate that passive personalization helps reduce task load and improve assistant perception, but reveal task-specific limitations that can be addressed through providing more user agency. These findings underscore the importance of combining active and passive personalization to optimize user experience and effectiveness in enterprise task-oriented environments.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ThetaEvolve: Test-time Learning on Open Problems</title>
<link>https://arxiv.org/abs/2511.23473</link>
<guid>https://arxiv.org/abs/2511.23473</guid>
<content:encoded><![CDATA[
arXiv:2511.23473v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have enabled breakthroughs in mathematical discovery, exemplified by AlphaEvolve, a closed-source system that evolves programs to improve bounds on open problems. However, it relies on ensembles of frontier LLMs to achieve new bounds and is a pure inference system that models cannot internalize the evolving strategies. We introduce ThetaEvolve, an open-source framework that simplifies and extends AlphaEvolve to efficiently scale both in-context learning and Reinforcement Learning (RL) at test time, allowing models to continually learn from their experiences in improving open optimization problems. ThetaEvolve features a single LLM, a large program database for enhanced exploration, batch sampling for higher throughput, lazy penalties to discourage stagnant outputs, and optional reward shaping for stable training signals, etc. ThetaEvolve is the first evolving framework that enable a small open-source model, like DeepSeek-R1-0528-Qwen3-8B, to achieve new best-known bounds on open problems (circle packing and first auto-correlation inequality) mentioned in AlphaEvolve. Besides, across two models and four open tasks, we find that ThetaEvolve with RL at test-time consistently outperforms inference-only baselines, and the model indeed learns evolving capabilities, as the RL-trained checkpoints demonstrate faster progress and better final performance on both trained target task and other unseen tasks. We release our code publicly: https://github.com/ypwang61/ThetaEvolve
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continual Learning with Global Alignment</title>
<link>https://arxiv.org/abs/2205.12186</link>
<guid>https://arxiv.org/abs/2205.12186</guid>
<content:encoded><![CDATA[
arXiv:2205.12186v3 Announce Type: replace 
Abstract: Continual learning aims to sequentially learn new tasks without forgetting previous tasks' knowledge (catastrophic forgetting). One factor that can cause forgetting is the interference between the gradients on losses from different tasks. When the gradients on the current task's loss are in opposing directions to those on previous tasks' losses, updating the model for the current task may cause performance degradation on previous tasks. In this paper, we first identify causes of the above interference, and hypothesize that correlations between data representations are a key factor of interference. We then propose a method for promoting appropriate correlations between arbitrary tasks' data representations (i.e., global alignment) in individual task learning. Specifically, we learn the data representation as a task-specific composition of pre-trained token representations shared across all tasks. Then the correlations between different tasks' data representations are grounded by correlations between pre-trained token representations. We explore different ways to learn such compositions. Without experience replay, our model achieves SOTA performance in continual learning tasks. It also achieves advanced class-incremental performance through task-incremental training.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoHall: Automated Factuality Hallucination Dataset Generation for Large Language Models</title>
<link>https://arxiv.org/abs/2310.00259</link>
<guid>https://arxiv.org/abs/2310.00259</guid>
<content:encoded><![CDATA[
arXiv:2310.00259v3 Announce Type: replace 
Abstract: Large language models (LLMs) have gained broad applications across various domains but still struggle with hallucinations. Currently, hallucinations occur frequently in the generation of factual content and pose a great challenge to trustworthy LLMs. However, hallucination detection is hindered by the laborious and expensive manual annotation of hallucinatory content. Meanwhile, as different LLMs exhibit distinct types and rates of hallucination, the collection of hallucination datasets is inherently model-specific, which also increases the cost. To address this issue, this paper proposes a method called $\textbf{AutoHall}$ for $\underline{Auto}$matically constructing model-specific $\underline{Hall}$ucination datasets based on existing fact-checking datasets. The empirical results reveal variations in hallucination proportions and types among different models. Moreover, we introduce a zero-resource and black-box hallucination detection method based on self-contradiction to recognize the hallucination in our constructed dataset, achieving superior detection performance compared to baselines. Further analysis on our dataset provides insight into factors that may contribute to LLM hallucinations. Our codes and datasets are publicly available at https://github.com/zouyingcao/AutoHall.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Biomolecule and Natural Language through Multi-Modal Learning: A Survey</title>
<link>https://arxiv.org/abs/2403.01528</link>
<guid>https://arxiv.org/abs/2403.01528</guid>
<content:encoded><![CDATA[
arXiv:2403.01528v3 Announce Type: replace 
Abstract: The integration of biomolecular modeling with natural language (BL) has emerged as a promising interdisciplinary area at the intersection of artificial intelligence, chemistry and biology. This approach leverages the rich, multifaceted descriptions of biomolecules contained within textual data sources to enhance our fundamental understanding and enable downstream computational tasks such as biomolecule property prediction. The fusion of the nuanced narratives expressed through natural language with the structural and functional specifics of biomolecules described via various molecular modeling techniques opens new avenues for comprehensively representing and analyzing biomolecules. By incorporating the contextual language data that surrounds biomolecules into their modeling, BL aims to capture a holistic view encompassing both the symbolic qualities conveyed through language as well as quantitative structural characteristics. In this review, we provide an extensive analysis of recent advancements achieved through cross modeling of biomolecules and natural language. (1) We begin by outlining the technical representations of biomolecules employed, including sequences, 2D graphs, and 3D structures. (2) We then examine in depth the rationale and key objectives underlying effective multi-modal integration of language and molecular data sources. (3) We subsequently survey the practical applications enabled to date in this developing research area. (4) We also compile and summarize the available resources and datasets to facilitate future work. (5) Looking ahead, we identify several promising research directions worthy of further exploration and investment to continue advancing the field. The related resources and contents are updating in https://github.com/QizhiPei/Awesome-Biomolecule-Language-Cross-Modeling.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Normal forms in Virus Machines</title>
<link>https://arxiv.org/abs/2409.03327</link>
<guid>https://arxiv.org/abs/2409.03327</guid>
<content:encoded><![CDATA[
arXiv:2409.03327v3 Announce Type: replace 
Abstract: In the present work, we further study the computational power of virus machines (VMs in short).VMs provide a computing paradigm inspired by the transmission and replication networks of viruses.VMs consist of process units (called hosts) structured by a directed graph whose arcs are called channels and an instruction graph that controls the transmissions of virus objects among hosts. The present work complements our understanding of the computing power of VMs by introducing normal forms; these expressions restrict the features in a given computing model.Some of the features that we restrict in our normal forms include (a) the number of hosts, (b) the number of instructions, and (c) the number of virus objects in each host. After we recall some known results on the computing power of VMs we give our series of normal forms, such as the size of the loops in the network, proving new characterisations of family of sets, such as finite sets, semilinear sets, or recursively enumerable sets (NRE).
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simulated patient systems powered by large language model-based AI agents offer potential for transforming medical education</title>
<link>https://arxiv.org/abs/2409.18924</link>
<guid>https://arxiv.org/abs/2409.18924</guid>
<content:encoded><![CDATA[
arXiv:2409.18924v4 Announce Type: replace 
Abstract: Background: Simulated patient systems are important in medical education and research, providing safe, integrative training environments and supporting clinical decision making. Advances in artificial intelligence (AI), especially large language models (LLMs), can enhance simulated patients by replicating medical conditions and doctor patient interactions with high fidelity and at low cost, but effectiveness and trustworthiness remain open challenges. Methods: We developed AIPatient, a simulated patient system powered by LLM based AI agents. The system uses a retrieval augmented generation (RAG) framework with six task specific agents for complex reasoning. To improve realism, it is linked to the AIPatient knowledge graph built from de identified real patient data in the MIMIC III intensive care database. Results: We evaluated electronic health record (EHR) based medical question answering (QA), readability, robustness, stability, and user experience. AIPatient reached 94.15 percent QA accuracy when all six agents were enabled, outperforming versions with partial or no agent integration. The knowledge base achieved an F1 score of 0.89. Readability scores showed a median Flesch Reading Ease of 68.77 and a median Flesch Kincaid Grade of 6.4, indicating accessibility for most medical trainees and clinicians. Robustness and stability were supported by non significant variance in repeated trials (analysis of variance F value 0.61, p greater than 0.1; F value 0.78, p greater than 0.1). A user study with medical students showed that AIPatient provides high fidelity, usability, and educational value, comparable to or better than human simulated patients for history taking. Conclusions: LLM based simulated patient systems can deliver accurate, readable, and reliable medical encounters and show strong potential to transform medical education.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Linguistically-Controlled Paraphrase Generation</title>
<link>https://arxiv.org/abs/2410.24199</link>
<guid>https://arxiv.org/abs/2410.24199</guid>
<content:encoded><![CDATA[
arXiv:2410.24199v2 Announce Type: replace 
Abstract: Controlled paraphrase generation produces paraphrases that preserve meaning while allowing precise control over linguistic attributes of the output. We introduce LingConv, an encoder-decoder framework that enables fine-grained control over 40 linguistic attributes in English. To improve reliability, we introduce a novel inference-time quality control mechanism that iteratively refines attribute embeddings to generate paraphrases that closely match target attributes without sacrificing semantic fidelity. LingConv reduces attribute error by up to 34% over existing models, with the quality control mechanism contributing an additional 14% improvement.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Equitable Access: Leveraging Crowdsourced Reviews to Investigate Public Perceptions of Health Resource Accessibility</title>
<link>https://arxiv.org/abs/2502.10641</link>
<guid>https://arxiv.org/abs/2502.10641</guid>
<content:encoded><![CDATA[
arXiv:2502.10641v2 Announce Type: replace 
Abstract: Monitoring health resource disparities during public health crises is critical, yet traditional methods, like surveys, lack the requisite speed and spatial granularity. This study introduces a novel framework that leverages: 1) crowdsourced Google Maps reviews (2018-2021) and 2) advanced NLP (DeBERTa) to create a high-resolution, spatial-temporal index of public perception of health resource accessibility in the United States. We then employ Partial Least Squares (PLS) regression to link this perception index to a range of socioeconomic and demographic drivers. Our results quantify significant spatial-temporal shifts in perceived access, confirming that disparities peaked during the COVID-19 crisis and only partially recovered post-peak. We identify political affiliation, racial composition, and educational attainment as primary determinants of these perceptions. This study validates a scalable method for real-time health equity monitoring and provides actionable evidence for interventions to build a more resilient healthcare infrastructure.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Atom of Thoughts for Markov LLM Test-Time Scaling</title>
<link>https://arxiv.org/abs/2502.12018</link>
<guid>https://arxiv.org/abs/2502.12018</guid>
<content:encoded><![CDATA[
arXiv:2502.12018v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) achieve superior performance through training-time scaling, and test-time scaling further enhances their capabilities by conducting effective reasoning during inference. However, as the scale of reasoning increases, existing test-time scaling methods suffer from accumulated historical information, which not only wastes computational resources but also interferes with effective reasoning. To address this issue, we observe that complex reasoning can be achieved by solving a series of independent and self-contained subquestions. These subquestions are essentially \textit{atomic questions}, exhibiting the memoryless property similar to Markov processes. Based on this observation, we propose Atom of Thoughts (\our), where each state transition consists of decomposing the current question into a dependency-based directed acyclic graph and contracting its subquestions, forming a simplified question that maintains answer equivalence with the original problem. This answer preservation enables the iterative \textit{decomposition-contraction} process to naturally form a meaningful Markov reasoning process. Furthermore, these atomic states can be seamlessly integrated into existing test-time scaling methods, enabling \our to serve as a plug-in enhancement for improving reasoning capabilities. Experiments across six benchmarks demonstrate the effectiveness of \our both as a standalone framework and a plug-in enhancement. Notably, on HotpotQA, when applied to gpt-4o-mini, \our achieves an \textbf{80.6\%} F1 score, surpassing o3-mini by \textbf{3.4\%} and DeepSeek-R1 by \textbf{10.6\%}. The code is available at \href{https://github.com/qixucen/atom}{https://github.com/qixucen/atom}.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self Iterative Label Refinement via Robust Unlabeled Learning</title>
<link>https://arxiv.org/abs/2502.12565</link>
<guid>https://arxiv.org/abs/2502.12565</guid>
<content:encoded><![CDATA[
arXiv:2502.12565v2 Announce Type: replace 
Abstract: Recent advances in large language models (LLMs) have yielded impressive performance on various tasks, yet they often depend on high-quality feedback that can be costly. Self-refinement methods attempt to leverage LLMs' internal evaluation mechanisms with minimal human supervision; however, these approaches frequently suffer from inherent biases and overconfidence, especially in domains where the models lack sufficient internal knowledge, resulting in performance degradation. As an initial step toward enhancing self-refinement for broader applications, we introduce an iterative refinement pipeline that employs the Unlabeled-Unlabeled learning framework to improve LLM-generated pseudo-labels for classification tasks. By exploiting two unlabeled datasets with differing positive class ratios, our approach iteratively denoises and refines the initial pseudo-labels, thereby mitigating the adverse effects of internal biases with minimal human supervision. Evaluations on diverse datasets, including low-resource language corpora, patent classifications, and protein structure categorizations, demonstrate that our method consistently outperforms both initial LLM's classification performance and the self-refinement approaches by cutting-edge models (e.g., GPT-4o and DeepSeek-R1). Moreover, we experimentally confirm that our refined classifier facilitates effective post-training alignment for safety in LLMs and demonstrate successful self-refinement in generative tasks as well.\footnote{Our code is available at https://github.com/HikaruAsano/self-iterative-label-refinement.}
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>More Documents, Same Length: Isolating the Challenge of Multiple Documents in RAG</title>
<link>https://arxiv.org/abs/2503.04388</link>
<guid>https://arxiv.org/abs/2503.04388</guid>
<content:encoded><![CDATA[
arXiv:2503.04388v3 Announce Type: replace 
Abstract: Retrieval-Augmented Generation (RAG) enhances the accuracy of Large Language Model (LLM) responses by leveraging relevant external documents during generation. Although previous studies noted that retrieving many documents can degrade performance, they did not isolate how the quantity of documents affects performance while controlling for context length. We evaluate various language models on custom datasets derived from a multi-hop QA task. We keep the context length and position of relevant information constant while varying the number of documents, and find that increasing the document count in RAG settings poses significant challenges for most LLMs, reducing performance by up to 20%. However, Qwen2.5 maintained consistent results across increasing document counts, indicating better multi-document handling capability. Finally, our results indicate that processing multiple documents is a separate challenge from handling long contexts. We also make the datasets and code available: https://github.com/shaharl6000/MoreDocsSameLen .
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KSHSeek: Data-Driven Approaches to Mitigating and Detecting Knowledge-Shortcut Hallucinations in Generative Models</title>
<link>https://arxiv.org/abs/2503.19482</link>
<guid>https://arxiv.org/abs/2503.19482</guid>
<content:encoded><![CDATA[
arXiv:2503.19482v2 Announce Type: replace 
Abstract: The emergence of large language models (LLMs) has significantly advanced the development of natural language processing (NLP), especially in text generation tasks like question answering. However, model hallucinations remain a major challenge in natural language generation (NLG) tasks due to their complex causes. We systematically expand on the causes of factual hallucinations from the perspective of knowledge shortcuts, analyzing hallucinations arising from correct and defect-free data and demonstrating that knowledge-shortcut hallucinations are prevalent in generative models. To mitigate this issue, we propose a high similarity pruning algorithm at the data preprocessing level to reduce spurious correlations in the data. Additionally, we design a specific detection method for knowledge-shortcut hallucinations to evaluate the effectiveness of our mitigation strategy. Experimental results show that our approach effectively reduces knowledge-shortcut hallucinations, particularly in fine-tuning tasks, without negatively impacting model performance in question answering. This work introduces a new paradigm for mitigating specific hallucination issues in generative models, enhancing their robustness and reliability in real-world applications.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Strong Memory, Weak Control: An Empirical Study of Executive Functioning in LLMs</title>
<link>https://arxiv.org/abs/2504.02789</link>
<guid>https://arxiv.org/abs/2504.02789</guid>
<content:encoded><![CDATA[
arXiv:2504.02789v2 Announce Type: replace 
Abstract: Working memory, or the ability to hold and manipulate information in the mind, is a critical component of human intelligence and executive functioning. It is correlated with performance on various cognitive tasks, including measures of fluid intelligence, which encompasses reasoning and problem solving. We use a comprehensive set of classic working memory tasks to estimate the working memory capacity of large language models (LLMs). We find that in most cases, LLMs exceed normative human scores. However, we do not find that the increased capacity of working memory is associated with higher performance on other executive functioning tasks or problem solving benchmarks. These results suggest that LLMs may have deficits in attentional control and cognitive flexibility, which result in difficulties with inhibiting automatic responses and adapting to shifting information. Our findings suggest that current reasoning models have mixed results in compensating for these deficits.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Superimposed Noise Accumulation Problem in Sequential Knowledge Editing of Large Language Models</title>
<link>https://arxiv.org/abs/2505.07899</link>
<guid>https://arxiv.org/abs/2505.07899</guid>
<content:encoded><![CDATA[
arXiv:2505.07899v2 Announce Type: replace 
Abstract: Sequential knowledge editing techniques aim to continuously update knowledge in large language models at low cost, preventing models from generating outdated or incorrect information. However, existing sequential editing methods suffer from a significant decline in editing success rates after long-term editing. Through theoretical analysis and experiments, our findings reveal that as the number of edits increases, the model's output increasingly deviates from the desired target, leading to a drop in editing success rates. We refer to this issue as the superimposed noise accumulation problem. Our further analysis demonstrates that the problem is related to the erroneous activation of irrelevant knowledge and conflicts between activated knowledge. Based on this analysis, a method named DeltaEdit is proposed that reduces conflicts between knowledge through dynamic orthogonal constraint strategies. Experiments show that DeltaEdit significantly reduces superimposed noise, achieving a 16.8% improvement in editing performance over the strongest baseline.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning</title>
<link>https://arxiv.org/abs/2505.14684</link>
<guid>https://arxiv.org/abs/2505.14684</guid>
<content:encoded><![CDATA[
arXiv:2505.14684v3 Announce Type: replace 
Abstract: Large language models (LLMs) have achieved remarkable progress on mathematical tasks through Chain-of-Thought (CoT) reasoning. However, existing mathematical CoT datasets often suffer from Thought Leaps due to experts omitting intermediate steps, which negatively impacts model learning and generalization. We propose the CoT Thought Leap Bridge Task, which aims to automatically detect leaps and generate missing intermediate reasoning steps to restore the completeness and coherence of CoT. To facilitate this, we constructed a specialized training dataset called ScaleQM+, based on the structured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought leaps. Through comprehensive experiments on mathematical reasoning benchmarks, we demonstrate that models fine-tuned on bridged datasets consistently outperform those trained on original datasets, with improvements of up to +5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%) and provides better starting points for reinforcement learning (+3.1%), functioning as a plug-and-play module compatible with existing optimization techniques. Furthermore, CoT-Bridge demonstrate improved generalization to out-of-domain logical reasoning tasks, confirming that enhancing reasoning completeness yields broadly applicable benefits.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Large Language Models Think Like the Brain? Sentence-Level Evidence from fMRI and Hierarchical Embeddings</title>
<link>https://arxiv.org/abs/2505.22563</link>
<guid>https://arxiv.org/abs/2505.22563</guid>
<content:encoded><![CDATA[
arXiv:2505.22563v2 Announce Type: replace 
Abstract: Understanding whether large language models (LLMs) and the human brain converge on similar computational principles remains a fundamental and important question in cognitive neuroscience and AI. Do the brain-like patterns observed in LLMs emerge simply from scaling, or do they reflect deeper alignment with the architecture of human language processing? This study focuses on the sentence-level neural mechanisms of language models, systematically investigating how hierarchical representations in LLMs align with the dynamic neural responses during human sentence comprehension. By comparing hierarchical embeddings from 14 publicly available LLMs with fMRI data collected from participants, who were exposed to a naturalistic narrative story, we constructed sentence-level neural prediction models to precisely identify the model layers most significantly correlated with brain region activations. Results show that improvements in model performance drive the evolution of representational architectures toward brain-like hierarchies, particularly achieving stronger functional and anatomical correspondence at higher semantic abstraction levels.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benford's Curse: Tracing Digit Bias to Numerical Hallucination in LLMs</title>
<link>https://arxiv.org/abs/2506.01734</link>
<guid>https://arxiv.org/abs/2506.01734</guid>
<content:encoded><![CDATA[
arXiv:2506.01734v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) exhibit impressive performance on complex reasoning tasks, yet they frequently fail on basic numerical problems, producing incorrect outputs. Inspired by Benford's Law, a statistical pattern in which lower digits occur more frequently as leading digits, we hypothesize that the skewed digit distributions in web-collected corpora may be learned by LLMs during pretraining, leading to biased numerical generation. To investigate the hypothesis, we first examine whether digits frequencies in pretraining corpus (OLMo2) follows Benford's law. We then construct an evaluation benchmark in which the ground-truth digits are uniformly distributed within each of the seven numerical reasoning tasks. Our evaluation results demonstrate that leading open-source LLMs show a consistent pattern of digit bias that resembles Benford's law. Through logit-lens tracing and neuron-level dissection, we identify that this bias arises predominantly from a small subset of highly digit-selective feed-forward network (FFN) neurons in the deeper layers. Finally, we demonstrate that pruning these neurons mitigates imbalanced overgeneration and partially corrects erroneous outputs, providing causal evidence that fine-grained pretraining digit bias can propagate into model behavior. Our findings reveal a fundamental connection between corpus-level statistics and symbolic failure modes in LLMs, offering a new lens for diagnosing and mitigating hallucinations in numerical tasks.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowerTune: A Cross-Domain Benchmark for Federated Fine-Tuning of Large Language Models</title>
<link>https://arxiv.org/abs/2506.02961</link>
<guid>https://arxiv.org/abs/2506.02961</guid>
<content:encoded><![CDATA[
arXiv:2506.02961v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have achieved state-of-the-art results across diverse domains, yet their development remains reliant on vast amounts of publicly available data, raising concerns about data scarcity and the lack of access to domain-specific, sensitive information. Federated Learning (FL) presents a compelling framework to address these challenges by enabling decentralized fine-tuning on pre-trained LLMs without sharing raw data. However, the compatibility and performance of pre-trained LLMs in FL settings remain largely under explored. We introduce the FlowerTune LLM Leaderboard, a first-of-its-kind benchmarking suite designed to evaluate federated fine-tuning of LLMs across four diverse domains: general NLP, finance, medical, and coding. Each domain includes federated instruction-tuning datasets and domain-specific evaluation metrics. Our results, obtained through a collaborative, open-source and community-driven approach, provide the first comprehensive comparison across 26 pre-trained LLMs with different aggregation and fine-tuning strategies under federated settings, offering actionable insights into model performance, resource constraints, and domain adaptation. This work lays the foundation for developing privacy-preserving, domain-specialized LLMs for real-world applications.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ROVER: Recursive Reasoning Over Videos with Vision-Language Models for Embodied Tasks</title>
<link>https://arxiv.org/abs/2508.01943</link>
<guid>https://arxiv.org/abs/2508.01943</guid>
<content:encoded><![CDATA[
arXiv:2508.01943v2 Announce Type: replace 
Abstract: Vision-language models (VLMs) have exhibited impressive capabilities across diverse image understanding tasks, but still struggle in settings that require reasoning over extended sequences of camera frames from a video. This limits their utility in embodied settings, which require reasoning over long frame sequences from a continuous stream of visual input at each moment of a task attempt. To address this limitation, we propose ROVER (Reasoning Over VidEo Recursively), a framework that enables the model to recursively decompose long-horizon video trajectories into segments corresponding to shorter subtasks within the trajectory. In doing so, ROVER facilitates more focused and accurate reasoning over temporally localized frame sequences without losing global context. We evaluate ROVER, implemented using an in-context learning approach, on diverse OpenX Embodiment videos and on a new dataset derived from RoboCasa that consists of 543 videos showing both expert and perturbed non-expert trajectories across 27 robotic manipulation tasks. ROVER outperforms strong baselines across three video reasoning tasks: task progress estimation, frame-level natural language reasoning, and video question answering. We observe that, by reducing the number of frames the model reasons over at each timestep, ROVER mitigates hallucinations, especially during unexpected or non-optimal moments of a trajectory. In addition, by enabling the implementation of a subtask-specific sliding context window, ROVER's time complexity scales linearly with video length, an asymptotic improvement over baselines. Demos, code, and data available at: https://rover-vlm.github.io
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>COPO: Causal-Oriented Policy Optimization for Hallucinations of MLLMs</title>
<link>https://arxiv.org/abs/2508.04182</link>
<guid>https://arxiv.org/abs/2508.04182</guid>
<content:encoded><![CDATA[
arXiv:2508.04182v2 Announce Type: replace 
Abstract: Despite Multimodal Large Language Models (MLLMs) having shown impressive capabilities, they may suffer from hallucinations. Empirically, we find that MLLMs attend disproportionately to task-irrelevant background regions compared with text-only LLMs, implying spurious background-answer correlations. We claim and analyze that (i) outcome-based rewards can be an important factor leading to spurious correlations, and (ii) spurious correlations can be an important factor leading to hallucinations. Based on these results, we propose Causal-Oriented Policy Optimization (COPO) to mitigate these spurious correlations, thus addressing the issue of hallucinations. It imposes token-level sufficiency and necessity constraints to measure each inference token's causal contribution, thus ensuring correct and evidence-grounded output. Specifically, we first evaluate each token's causal contribution via a newly proposed causal completeness reward. This reward is then used to construct a causally informed advantage function within the GRPO optimization framework, encouraging the model to focus on tokens that are causally sufficient and necessary for accurate generation. Experimental results across various benchmarks demonstrate the advantages of COPO.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IROTE: Human-like Traits Elicitation of Large Language Model via In-Context Self-Reflective Optimization</title>
<link>https://arxiv.org/abs/2508.08719</link>
<guid>https://arxiv.org/abs/2508.08719</guid>
<content:encoded><![CDATA[
arXiv:2508.08719v2 Announce Type: replace 
Abstract: Trained on various human-authored corpora, Large Language Models (LLMs) have demonstrated a certain capability of reflecting specific human-like traits (e.g., personality or values) by prompting, benefiting applications like personalized LLMs and social simulations. However, existing methods suffer from the superficial elicitation problem: LLMs can only be steered to mimic shallow and unstable stylistic patterns, failing to embody the desired traits precisely and consistently across diverse tasks like humans. To address this challenge, we propose IROTE, a novel in-context method for stable and transferable trait elicitation. Drawing on psychological theories suggesting that traits are formed through identity-related reflection, our method automatically generates and optimizes a textual self-reflection within prompts, which comprises self-perceived experience, to stimulate LLMs' trait-driven behavior. The optimization is performed by iteratively maximizing an information-theoretic objective that enhances the connections between LLMs' behavior and the target trait, while reducing noisy redundancy in reflection without any fine-tuning, leading to evocative and compact trait reflection. Extensive experiments across three human trait systems manifest that one single IROTE-generated self-reflection can induce LLMs' stable impersonation of the target trait across diverse downstream tasks beyond simple questionnaire answering, consistently outperforming existing strong baselines.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploiting Vocabulary Frequency Imbalance in Language Model Pre-training</title>
<link>https://arxiv.org/abs/2508.15390</link>
<guid>https://arxiv.org/abs/2508.15390</guid>
<content:encoded><![CDATA[
arXiv:2508.15390v3 Announce Type: replace 
Abstract: Large language models are trained with tokenizers, and the resulting token distribution is highly imbalanced: a few words dominate the stream while most occur rarely. Recent practice favors ever-larger vocabularies, but it is unclear where the benefit comes from. To this end, we perform a controlled study that scales the vocabulary of the language model from 24K to 196K while holding data, computation, and optimization unchanged. We begin by quantifying the complexity of tokenized text -- formalized via Kolmogorov complexity -- and show that larger vocabularies reduce this complexity. Above 24K, every common word is already tokenized as a single token, so enlarging vocabulary only deepens the relative token-frequency imbalance. Word-level loss decomposition shows that larger vocabularies reduce cross-entropy loss almost exclusively by lowering uncertainty on the 2,500 most frequent words, even though loss on the rare tail rises. The same frequent words cover roughly 75% of tokens in downstream benchmarks, so this training advantage transfers intact. We further show that enlarging model parameters with a fixed vocabulary yields the same frequent-word benefit. Our results recast "bigger vocabularies help" as "lowering complexity of tokenized text helps," offering a simple, principled knob for tokenizer-model co-design and clarifying the loss dynamics that govern language model scaling in pre-training.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy-Preserving Reasoning with Knowledge-Distilled Parametric Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2509.01088</link>
<guid>https://arxiv.org/abs/2509.01088</guid>
<content:encoded><![CDATA[
arXiv:2509.01088v2 Announce Type: replace 
Abstract: The current RAG system requires uploading plaintext documents to the cloud, risking private data leakage. Parametric RAG (PRAG) encodes documents as LoRA parameters within LLMs, offering a possible way to reduce exposure of raw content. However, it still faces two issues: (1) PRAG demands synthesizing QA pairs and fine-tuning LLM for each individual document to create its corresponding LoRA, leading to unacceptable inference latency. (2) The performance of PRAG relies solely on synthetic QA data while lacking internal alignment with standard RAG, resulting in poor generalization on out-of-distribution(OOD) inputs. Therefore, achieving high-efficiency parameterization while maintaining RAG-level performance remains a critical challenge for privacy-preserving reasoning. In this paper, we propose DistilledPRAG, a generalizable knowledge-distilled parametric RAG model aligned with standard RAG in document structure and parameter activation. We first synthesize QA pairs from single and multi-documents to enhance cross-document reasoning. Then, we mask the plaintext documents with a special token and translate them to LoRA via a parameter generator, maintaining the standard RAG document structure. Finally, guided by synthetic QA data, we train the parameter generator to match standard RAG's hidden states and output logits, enabling RAG-style reasoning without original documents. Experiments on four QA datasets show that DistilledPRAG outperforms baselines in accuracy and generalizes well on OOD data.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Financial Risk Relation Identification through Dual-view Adaptation</title>
<link>https://arxiv.org/abs/2509.18775</link>
<guid>https://arxiv.org/abs/2509.18775</guid>
<content:encoded><![CDATA[
arXiv:2509.18775v2 Announce Type: replace 
Abstract: A multitude of interconnected risk events -- ranging from regulatory changes to geopolitical tensions -- can trigger ripple effects across firms. Identifying inter-firm risk relations is thus crucial for applications like portfolio management and investment strategy. Traditionally, such assessments rely on expert judgment and manual analysis, which are, however, subjective, labor-intensive, and difficult to scale. To address this, we propose a systematic method for extracting inter-firm risk relations using Form 10-K filings -- authoritative, standardized financial documents -- as our data source. Leveraging recent advances in natural language processing, our approach captures implicit and abstract risk connections through unsupervised fine-tuning based on chronological and lexical patterns in the filings. This enables the development of a domain-specific financial encoder with a deeper contextual understanding and introduces a quantitative risk relation score for transparency, interpretable analysis. Extensive experiments demonstrate that our method outperforms strong baselines across multiple evaluation settings. Our codes are available at https://github.com/cnclabs/codes.fin.relation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KurdSTS: The Kurdish Semantic Textual Similarity</title>
<link>https://arxiv.org/abs/2510.02336</link>
<guid>https://arxiv.org/abs/2510.02336</guid>
<content:encoded><![CDATA[
arXiv:2510.02336v2 Announce Type: replace 
Abstract: Semantic Textual Similarity (STS) measures the degree of meaning overlap between two texts and underpins many NLP tasks. While extensive resources exist for high-resource languages, low-resource languages such as Kurdish remain underserved. We present, to our knowledge, the first Kurdish STS dataset: 10,000 sentence pairs spanning formal and informal registers, each annotated for similarity. We benchmark Sentence-BERT, multilingual BERT, and other strong baselines, obtaining competitive results while highlighting challenges arising from Kurdish morphology, orthographic variation, and code-mixing. The dataset and baselines establish a reproducible evaluation suite and provide a strong starting point for future research on Kurdish semantics and low-resource NLP.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Composition of Agents: A Knapsack Approach for Agentic Component Selection</title>
<link>https://arxiv.org/abs/2510.16499</link>
<guid>https://arxiv.org/abs/2510.16499</guid>
<content:encoded><![CDATA[
arXiv:2510.16499v2 Announce Type: replace 
Abstract: Designing effective agentic systems requires the seamless composition and integration of agents, tools, and models within dynamic and uncertain environments. Most existing methods rely on static, semantic retrieval approaches for tool or agent discovery. However, effective reuse and composition of existing components remain challenging due to incomplete capability descriptions and the limitations of retrieval methods. Component selection suffers because the decisions are not based on capability, cost, and real-time utility. To address these challenges, we introduce a structured, automated framework for agentic system composition that is inspired by the knapsack problem. Our framework enables a composer agent to systematically identify, select, and assemble an optimal set of agentic components by jointly considering performance, budget constraints, and compatibility. By dynamically testing candidate components and modeling their utility in real-time, our approach streamlines the assembly of agentic systems and facilitates scalable reuse of resources. Empirical evaluation with Claude 3.5 Sonnet across five benchmarking datasets shows that our online-knapsack-based composer consistently lies on the Pareto frontier, achieving higher success rates at significantly lower component costs compared to our baselines. In the single-agent setup, the online knapsack composer shows a success rate improvement of up to 31.6% in comparison to the retrieval baselines. In multi-agent systems, the online knapsack composer increases success rate from 37% to 87% when agents are selected from an agent inventory of 100+ agents. The substantial performance gap confirms the robust adaptability of our method across diverse domains and budget constraints.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChiKhaPo: A Large-Scale Multilingual Benchmark for Evaluating Lexical Comprehension and Generation in Large Language Models</title>
<link>https://arxiv.org/abs/2510.16928</link>
<guid>https://arxiv.org/abs/2510.16928</guid>
<content:encoded><![CDATA[
arXiv:2510.16928v2 Announce Type: replace 
Abstract: Existing benchmarks for large language models (LLMs) are largely restricted to high- or mid-resource languages, and often evaluate performance on higher-order tasks in reasoning and generation. However, plenty of evidence points to the fact that LLMs lack basic linguistic competence in the vast majority of the world's 3800+ written languages. We introduce ChiKhaPo, consisting of 8 subtasks of varying difficulty designed to evaluate the lexical comprehension and generation abilities of generative models. ChiKhaPo draws on existing lexicons, monolingual data, and bitext, and provides coverage for 2700+ languages for 2 subtasks, surpassing any existing benchmark in terms of language coverage. We further show that 6 SOTA models struggle on our benchmark, and discuss the factors contributing to performance scores, including language family, language resourcedness, task, and comprehension versus generation directions. With ChiKhaPo, we hope to enable and encourage the massively multilingual benchmarking of LLMs.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Reasoning via Thought-Training and Thought-Free Inference</title>
<link>https://arxiv.org/abs/2511.03408</link>
<guid>https://arxiv.org/abs/2511.03408</guid>
<content:encoded><![CDATA[
arXiv:2511.03408v3 Announce Type: replace 
Abstract: Recent advances in large language models (LLMs) have leveraged explicit Chain-of-Thought (CoT) prompting to improve reasoning accuracy. However, most existing methods primarily focus on compressing verbose reasoning outputs. These Long-to-Short transformations aim to improve efficiency, but require a large amount of short CoT data. In this work, we introduce \textbf{3TF} (\textbf{T}hought-\textbf{T}raining and \textbf{T}hought-\textbf{F}ree inference), a framework for efficient reasoning that takes a Short-to-Long perspective. We first train a hybrid model that can operate in both reasoning and non-reasoning modes, and then further train it on CoT-annotated data to internalize structured reasoning, while enforcing concise, thought-free outputs at inference time using the no-reasoning mode. Unlike compression-based approaches, 3TF improves the reasoning quality of non-reasoning outputs, enabling models to perform rich internal reasoning implicitly while keeping external outputs short. Empirically, 3TF-trained models obtain large improvements on reasoning benchmarks under thought-free inference, demonstrating that high quality reasoning can be learned and executed implicitly without explicit step-by-step generation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mina: A Multilingual LLM-Powered Legal Assistant Agent for Bangladesh for Empowering Access to Justice</title>
<link>https://arxiv.org/abs/2511.08605</link>
<guid>https://arxiv.org/abs/2511.08605</guid>
<content:encoded><![CDATA[
arXiv:2511.08605v2 Announce Type: replace 
Abstract: Bangladesh's low-income population faces major barriers to affordable legal advice due to complex legal language, procedural opacity, and high costs. Existing AI legal assistants lack Bengali-language support and jurisdiction-specific adaptation, limiting their effectiveness. To address this, we developed Mina, a multilingual LLM-based legal assistant tailored for the Bangladeshi context. It employs multilingual embeddings and a RAG-based chain-of-tools framework for retrieval, reasoning, translation, and document generation, delivering context-aware legal drafts, citations, and plain-language explanations via an interactive chat interface. Evaluated by law faculty from leading Bangladeshi universities across all stages of the 2022 and 2023 Bangladesh Bar Council Exams, Mina scored 75-80% in Preliminary MCQs, Written, and simulated Viva Voce exams, matching or surpassing average human performance and demonstrating clarity, contextual understanding, and sound legal reasoning. Even under a conservative upper bound, Mina operates at just 0.12-0.61% of typical legal consultation costs in Bangladesh, yielding a 99.4-99.9\% cost reduction relative to human-provided services. These results confirm its potential as a low-cost, multilingual AI assistant that automates key legal tasks and scales access to justice, offering a real-world case study on building domain-specific, low-resource systems and addressing challenges of multilingual adaptation, efficiency, and sustainable public-service AI deployment.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Local Hybrid Retrieval-Augmented Document QA</title>
<link>https://arxiv.org/abs/2511.10297</link>
<guid>https://arxiv.org/abs/2511.10297</guid>
<content:encoded><![CDATA[
arXiv:2511.10297v2 Announce Type: replace 
Abstract: Organizations handling sensitive documents face a critical dilemma: adopt cloud-based AI systems that offer powerful question-answering capabilities but compromise data privacy, or maintain local processing that ensures security but delivers poor accuracy. We present a question-answering system that resolves this trade-off by combining semantic understanding with keyword precision, operating entirely on local infrastructure without internet access. Our approach demonstrates that organizations can achieve competitive accuracy on complex queries across legal, scientific, and conversational documents while keeping all data on their machines. By balancing two complementary retrieval strategies and using consumer-grade hardware acceleration, the system delivers reliable answers with minimal errors, letting banks, hospitals, and law firms adopt conversational document AI without transmitting proprietary information to external providers. This work establishes that privacy and performance need not be mutually exclusive in enterprise AI deployment.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Trio Neural Model for Dynamic Entity Relatedness Ranking</title>
<link>https://arxiv.org/abs/1808.08316</link>
<guid>https://arxiv.org/abs/1808.08316</guid>
<content:encoded><![CDATA[
arXiv:1808.08316v5 Announce Type: replace-cross 
Abstract: Measuring entity relatedness is a fundamental task for many natural language processing and information retrieval applications. Prior work often studies entity relatedness in static settings and an unsupervised manner. However, entities in real-world are often involved in many different relationships, consequently entity-relations are very dynamic over time. In this work, we propose a neural networkbased approach for dynamic entity relatedness, leveraging the collective attention as supervision. Our model is capable of learning rich and different entity representations in a joint framework. Through extensive experiments on large-scale datasets, we demonstrate that our method achieves better results than competitive baselines.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extensible Multi-Granularity Fusion Network and Transferable Curriculum Learning for Aspect-based Sentiment Analysis</title>
<link>https://arxiv.org/abs/2402.07787</link>
<guid>https://arxiv.org/abs/2402.07787</guid>
<content:encoded><![CDATA[
arXiv:2402.07787v4 Announce Type: replace-cross 
Abstract: Aspect-based Sentiment Analysis (ABSA) aims to determine sentiment polarity toward specific aspects in text. Existing methods enrich semantic and syntactic representations through external knowledge or GNNs, but the growing diversity of linguistic features increases model complexity and lacks a unified, extensible framework. We propose an Extensible Multi-Granularity Fusion Network (EMGF) that integrates dependency syntax, constituent syntax, attention-based semantics, and external knowledge graphs. EMGF employs multi-anchor triplet learning and orthogonal projection to effectively fuse multi-granularity features and strengthen their interactions without additional computational overhead. Furthermore, we introduce the first task-specific curriculum learning framework for text-only ABSA, which assigns difficulty scores using five indicators and trains the model from easy to hard to mimic human learning and improve generalization. Experiments on SemEval 2014, Twitter, and MAMS datasets show that EMGF+CL consistently outperforms state-of-the-art ABSA models.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring the Human-LLM Synergy in Advancing Theory-driven Qualitative Analysis</title>
<link>https://arxiv.org/abs/2405.05758</link>
<guid>https://arxiv.org/abs/2405.05758</guid>
<content:encoded><![CDATA[
arXiv:2405.05758v2 Announce Type: replace-cross 
Abstract: Qualitative coding is a demanding yet crucial research method in the field of Human-Computer Interaction (HCI). While recent studies have shown the capability of large language models (LLMs) to perform qualitative coding within theoretical frameworks, their potential for collaborative human-LLM discovery and generation of new insights beyond initial theory remains underexplored. To bridge this gap, we proposed CHALET, a novel approach that harnesses the power of human-LLM partnership to advance theory-driven qualitative analysis by facilitating iterative coding, disagreement analysis, and conceptualization of qualitative data. We demonstrated CHALET's utility by applying it to the qualitative analysis of conversations related to mental-illness stigma, using the attribution model as the theoretical framework. Results highlighted the unique contribution of human-LLM collaboration in uncovering latent themes of stigma across the cognitive, emotional, and behavioral dimensions. We discuss the methodological implications of the human-LLM collaborative approach to theory-based qualitative analysis for the HCI community and beyond.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Harvesting Textual and Contrastive Data from the HAL Publication Repository</title>
<link>https://arxiv.org/abs/2407.20595</link>
<guid>https://arxiv.org/abs/2407.20595</guid>
<content:encoded><![CDATA[
arXiv:2407.20595v3 Announce Type: replace-cross 
Abstract: Authorship attribution in natural language processing traditionally struggles to distinguish genuine stylistic signals from topical confounds. While contrastive learning approaches have addressed this by maximizing semantic overlap between positive pairs, creating large-scale datasets under strict topic constraints remains challenging. We introduce HALvest, a 17-billion-token multilingual corpus harvested from 778k open-access academic papers, and HALvest-Contrastive, a derived dataset designed to isolate stylometric signals through controlled topic variation. Unlike prior work that minimizes lexical overlap, we exploit natural topic drift between papers by the same author, treating residual lexical patterns as authorial fingerprints rather than noise. Comparing lexical baselines (BM25) against neural models trained on unrestricted (topic-rich) versus base (topic-decoupled) triplets, we demonstrate that models trained exclusively on topic-decoupled data achieve superior performance across all test conditions, outperforming both retrieval baselines and models exposed to topic-rich training data. Our analysis reveals that while lexical signals provide substantial performance gains for keyword-driven methods, neural architectures learn robust stylometric representations that plateau with moderate context length, suggesting they capture distributional style beyond surface-level tokens. Both datasets and code are publicly available.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Event Stream-based Sign Language Translation: A High-Definition Benchmark Dataset and A Novel Baseline</title>
<link>https://arxiv.org/abs/2408.10488</link>
<guid>https://arxiv.org/abs/2408.10488</guid>
<content:encoded><![CDATA[
arXiv:2408.10488v2 Announce Type: replace-cross 
Abstract: Sign Language Translation (SLT) is a core task in the field of AI-assisted disability. Traditional SLT methods are typically based on visible light videos, which are easily affected by factors such as lighting variations, rapid hand movements, and privacy concerns. This paper proposes the use of bio-inspired event cameras to alleviate the aforementioned issues. Specifically, we introduce a new high-definition event-based sign language dataset, termed Event-CSL, which effectively addresses the data scarcity in this research area. The dataset comprises 14,827 videos, 14,821 glosses, and 2,544 Chinese words in the text vocabulary. These samples are collected across diverse indoor and outdoor scenes, covering multiple viewpoints, lighting conditions, and camera motions. We have also benchmarked existing mainstream SLT methods on this dataset to facilitate fair comparisons in future research.Furthermore, we propose a novel event-based sign language translation framework, termed EvSLT. The framework first segments continuous video features into clips and employs a Mamba-based memory aggregation module to compress and aggregate spatial detail features at the clip level. Subsequently, these spatial features, along with temporal representations obtained from temporal convolution, are then fused by a graph-guided spatiotemporal fusion module. Extensive experiments on Event-CSL, as well as other publicly available datasets, demonstrate the superior performance of our method. The dataset and source code will be released on https://github.com/Event-AHU/OpenESL
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MCTS-SQL: Light-Weight LLMs can Master the Text-to-SQL through Monte Carlo Tree Search</title>
<link>https://arxiv.org/abs/2501.16607</link>
<guid>https://arxiv.org/abs/2501.16607</guid>
<content:encoded><![CDATA[
arXiv:2501.16607v3 Announce Type: replace-cross 
Abstract: Text-to-SQL is a fundamental yet challenging task in the NLP area, aiming at translating natural language questions into SQL queries. While recent advances in large language models have greatly improved performance, most existing approaches depend on models with tens of billions of parameters or costly APIs, limiting their applicability in resource-constrained environments. For real world, especially on edge devices, it is crucial for Text-to-SQL to ensure cost-effectiveness. Therefore, enabling the light-weight models for Text-to-SQL is of great practical significance. However, smaller LLMs often struggle with complicated user instruction, redundant schema linking or syntax correctness. To address these challenges, we propose MCTS-SQL, a novel framework that uses Monte Carlo Tree Search to guide SQL generation through multi-step refinement. Since the light-weight models' weak performance of single-shot prediction, we generate better results through several trials with feedback. However, directly applying MCTS-based methods inevitably leads to significant time and computational overhead. Driven by this issue, we propose a token-level prefix-cache mechanism that stores prior information during iterations, effectively improved the execution speed. Experiments results on the SPIDER and BIRD benchmarks demonstrate the effectiveness of our approach. Using a small open-source Qwen2.5-Coder-1.5B, our method outperforms ChatGPT-3.5. When leveraging a more powerful model Gemini 2.5 to explore the performance upper bound, we achieved results competitive with the SOTA. Our findings demonstrate that even small models can be effectively deployed in practical Text-to-SQL systems with the right strategy.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WritingBench: A Comprehensive Benchmark for Generative Writing</title>
<link>https://arxiv.org/abs/2503.05244</link>
<guid>https://arxiv.org/abs/2503.05244</guid>
<content:encoded><![CDATA[
arXiv:2503.05244v4 Announce Type: replace-cross 
Abstract: Recent advancements in large language models (LLMs) have significantly enhanced text generation capabilities, yet evaluating their performance in generative writing remains a challenge. Existing benchmarks primarily focus on generic text generation or limited in writing tasks, failing to capture the diverse requirements of high-quality written contents across various domains. To bridge this gap, we present WritingBench, a comprehensive benchmark designed to evaluate LLMs across 6 core writing domains and 100 subdomains. We further propose a query-dependent evaluation framework that empowers LLMs to dynamically generate instance-specific assessment criteria. This framework is complemented by a fine-tuned critic model for criteria-aware scoring, enabling evaluations in style, format and length. The framework's validity is further demonstrated by its data curation capability, which enables a 7B-parameter model to outperform the performance of GPT-4o in writing. We open-source the benchmark, along with evaluation tools and modular framework components, to advance the development of LLMs in writing.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KeepKV: Achieving Periodic Lossless KV Cache Compression for Efficient LLM Inference</title>
<link>https://arxiv.org/abs/2504.09936</link>
<guid>https://arxiv.org/abs/2504.09936</guid>
<content:encoded><![CDATA[
arXiv:2504.09936v2 Announce Type: replace-cross 
Abstract: Efficient inference of large language models (LLMs) is hindered by an ever-growing key-value (KV) cache, making KV cache compression a critical research direction. Traditional methods selectively evict less important KV cache entries, which leads to information loss and hallucinations. Recently, merging-based strategies have been explored to retain more information by merging KV pairs that would be discarded; however, these existing approaches inevitably introduce inconsistencies in attention distributions before and after merging, causing degraded generation quality. To overcome this challenge, we propose KeepKV, a novel adaptive KV cache merging method designed to preserve performance under strict memory constraints, achieving single-step lossless compression and providing error bounds for multi-step compression. KeepKV introduces the Electoral Votes mechanism that records merging history and adaptively adjusts attention scores. Moreover, it further leverages a novel Zero Inference-Perturbation Merging method, compensating for attention loss resulting from cache merging. Extensive experiments on various benchmarks and LLM architectures demonstrate that KeepKV substantially reduces memory usage while successfully retaining essential context information, achieving over 2x inference throughput improvement and maintaining superior generation quality even with only 10% KV cache budgets.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mavors: Multi-granularity Video Representation for Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2504.10068</link>
<guid>https://arxiv.org/abs/2504.10068</guid>
<content:encoded><![CDATA[
arXiv:2504.10068v2 Announce Type: replace-cross 
Abstract: Long-context video understanding in multimodal large language models (MLLMs) faces a critical challenge: balancing computational efficiency with the retention of fine-grained spatio-temporal patterns. Existing approaches (e.g., sparse sampling, dense sampling with low resolution, and token compression) suffer from significant information loss in temporal dynamics, spatial details, or subtle interactions, particularly in videos with complex motion or varying resolutions. To address this, we propose $\mathbf{Mavors}$, a novel framework that introduces $\mathbf{M}$ulti-gr$\mathbf{a}$nularity $\mathbf{v}$ide$\mathbf{o}$ $\mathbf{r}$epre$\mathbf{s}$entation for holistic long-video modeling. Specifically, Mavors directly encodes raw video content into latent representations through two core components: 1) an Intra-chunk Vision Encoder (IVE) that preserves high-resolution spatial features via 3D convolutions and Vision Transformers, and 2) an Inter-chunk Feature Aggregator (IFA) that establishes temporal coherence across chunks using transformer-based dependency modeling with chunk-level rotary position encodings. Moreover, the framework unifies image and video understanding by treating images as single-frame videos via sub-image decomposition. Experiments across diverse benchmarks demonstrate Mavors' superiority in maintaining both spatial fidelity and temporal continuity, significantly outperforming existing methods in tasks requiring fine-grained spatio-temporal reasoning.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RvLLM: LLM Runtime Verification with Domain Knowledge</title>
<link>https://arxiv.org/abs/2505.18585</link>
<guid>https://arxiv.org/abs/2505.18585</guid>
<content:encoded><![CDATA[
arXiv:2505.18585v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have emerged as a dominant AI paradigm due to their exceptional text understanding and generation capabilities. However, their tendency to generate inconsistent or erroneous outputs challenges their reliability, especially in high-stakes domains requiring accuracy and trustworthiness. Existing research primarily focuses on detecting and mitigating model misbehavior in general-purpose scenarios, often overlooking the potential of integrating domain-specific knowledge. In this work, we advance misbehavior detection by incorporating domain knowledge. The core idea is to design a general specification language that enables domain experts to customize domain-specific predicates in a lightweight and intuitive manner, supporting later runtime verification of LLM outputs. To achieve this, we design a novel specification language, ESL, and introduce a runtime verification framework, RvLLM, to validate LLM output against domain-specific constraints defined in ESL. We evaluate RvLLM on three representative tasks: violation detection against Singapore Rapid Transit Systems Act, numerical comparison, and inequality solving. Experimental results demonstrate that RvLLM effectively detects erroneous outputs across various LLMs in a lightweight and flexible manner. The results reveal that despite their impressive capabilities, LLMs remain prone to low-level errors due to limited interpretability and a lack of formal guarantees during inference, and our framework offers a potential long-term solution by leveraging expert domain knowledge to rigorously and efficiently verify LLM outputs.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One Patient, Many Contexts: Scaling Medical AI with Contextual Intelligence</title>
<link>https://arxiv.org/abs/2506.10157</link>
<guid>https://arxiv.org/abs/2506.10157</guid>
<content:encoded><![CDATA[
arXiv:2506.10157v3 Announce Type: replace-cross 
Abstract: Medical AI, including clinical language models, vision-language models, and multimodal health record models, already summarizes notes, answers questions, and supports decisions. Their adaptation to new populations, specialties, or care settings often relies on fine-tuning, prompting, or retrieval from external knowledge bases. These strategies can scale poorly and risk contextual errors: outputs that appear plausible but miss critical patient or situational information. We envision context switching as a solution. Context switching adjusts model reasoning at inference without retraining. Generative models can tailor outputs to patient biology, care setting, or disease. Multimodal models can reason on notes, laboratory results, imaging, and genomics, even when some data are missing or delayed. Agent models can coordinate tools and roles based on tasks and users. In each case, context switching enables medical AI to adapt across specialties, populations, and geographies. It requires advances in data design, model architectures, and evaluation frameworks, and establishes a foundation for medical AI that scales to infinitely many contexts while remaining reliable and suited to real-world care.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking And Normalization</title>
<link>https://arxiv.org/abs/2506.12484</link>
<guid>https://arxiv.org/abs/2506.12484</guid>
<content:encoded><![CDATA[
arXiv:2506.12484v5 Announce Type: replace-cross 
Abstract: Language models can retain dangerous knowledge and skills even after extensive safety fine-tuning, posing both misuse and misalignment risks. Recent studies show that even specialized unlearning methods can be easily reversed. To address this, we systematically evaluate many existing and novel components of unlearning methods and identify ones crucial for irreversible unlearning.
  We introduce Disruption Masking, a technique in which we only allow updating weights, where the signs of the unlearning gradient and the retaining gradient are the same. This ensures all updates are non-disruptive.
  Additionally, we identify the need for normalizing the unlearning gradients, and also confirm the usefulness of meta-learning. We combine these insights into MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and validate its effectiveness at preventing the recovery of dangerous capabilities. MUDMAN outperforms the prior TAR method by 40%, setting a new state-of-the-art for robust unlearning.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing positive and negative rewards</title>
<link>https://arxiv.org/abs/2506.20520</link>
<guid>https://arxiv.org/abs/2506.20520</guid>
<content:encoded><![CDATA[
arXiv:2506.20520v2 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) is increasingly used to align large language models (LLMs). Off-policy methods offer greater implementation simplicity and data efficiency than on-policy techniques, but often result in suboptimal performance. In this work, we study the intermediate range of algorithms between off-policy RL and supervised fine-tuning by analyzing a simple off-policy REINFORCE algorithm, where the advantage is defined as $A=r-V$, with $r$ a reward and $V$ some tunable baseline. Intuitively, lowering $V$ emphasizes high-reward samples, while raising it penalizes low-reward ones more heavily. We first provide a theoretical analysis of this off-policy REINFORCE algorithm, showing that when the baseline $V$ lower-bounds the expected reward, the algorithm enjoys a policy improvement guarantee. Our analysis reveals that while on-policy updates can safely leverage both positive and negative signals, off-policy updates benefit from focusing more on positive rewards than on negative ones. We validate our findings experimentally in a controlled stochastic bandit setting and through fine-tuning state-of-the-art LLMs on reasoning tasks.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReGATE: Learning Faster and Better with Fewer Tokens in MLLMs</title>
<link>https://arxiv.org/abs/2507.21420</link>
<guid>https://arxiv.org/abs/2507.21420</guid>
<content:encoded><![CDATA[
arXiv:2507.21420v2 Announce Type: replace-cross 
Abstract: The computational cost of training multimodal large language models (MLLMs) grows rapidly with the number of processed tokens. Existing efficiency methods mainly target inference via token reduction or merging, offering limited benefits during training. We introduce ReGATE (Reference-Guided Adaptive Token Elision), an adaptive token pruning method for accelerating MLLM training. ReGATE adopts a teacher-student framework, in which a frozen teacher LLM provides per-token guidance losses that are fused with an exponential moving average of the student's difficulty estimates. This adaptive scoring mechanism dynamically selects informative tokens while skipping redundant ones in the forward pass, substantially reducing computation without altering the model architecture. Across three representative MLLMs, ReGATE matches the peak accuracy of standard training on MVBench up to 2$\times$ faster, using only 38% of the tokens. With extended training, it even surpasses the baseline across multiple multimodal benchmarks, cutting total token usage by over 41%. Code and models will be released publicly.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Holistic Evaluation of Multimodal LLMs on Spatial Intelligence</title>
<link>https://arxiv.org/abs/2508.13142</link>
<guid>https://arxiv.org/abs/2508.13142</guid>
<content:encoded><![CDATA[
arXiv:2508.13142v4 Announce Type: replace-cross 
Abstract: Multimodal models have achieved remarkable progress in recent years. Nevertheless, they continue to exhibit notable limitations in spatial understanding and reasoning, the very capability that anchors artificial general intelligence in the physical world. With the recent release of GPT-5, allegedly the most powerful AI model to date, it is timely to examine where the leading models (GPT, Gemini, Grok, Seed, Qwen, and Intern) stand on the path toward spatial intelligence (SI). We thus propose EASI for holistic Evaluation of multimodAl LLMs on Spatial Intelligence. EASI conceptualizes a comprehensive taxonomy of spatial tasks that unifies existing benchmarks and a growing collection of newly curated ones, enabling systematic evaluation of state-of-the-art models. In this report, we conduct the study across eight key benchmarks, at a cost exceeding ten billion total tokens. Our empirical study then reveals that (1) GPT-5 demonstrates unprecedented strength in SI, yet (2) still falls short of human performance significantly across a broad spectrum of SI-tasks. Moreover, we (3) show that SI-tasks expose greater model capability deficiency than non-SI tasks, to the extent that (4) proprietary models do not exhibit a decisive advantage when facing the most difficult ones. In addition, we conduct a qualitative evaluation across a diverse set of scenarios that are intuitive for humans, yet fail the most advanced multimodal models. EASI is an ongoing community effort: we have open-sourced the EASI codebase that provides a one-stop and reproducible solution with standardized interfaces, integrated protocols and prompts that significantly reduce the friction of configuring and running multiple benchmarks; we have also launched an accompanying EASI leaderboard to provide a continually updated snapshot of model performance across the full SI spectrum, accelerating collective progress toward robust SI.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Masked Diffusion Models as Energy Minimization</title>
<link>https://arxiv.org/abs/2509.13866</link>
<guid>https://arxiv.org/abs/2509.13866</guid>
<content:encoded><![CDATA[
arXiv:2509.13866v2 Announce Type: replace-cross 
Abstract: We present a systematic theoretical framework that interprets masked diffusion models (MDMs) as solutions to energy minimization problems in discrete optimal transport. Specifically, we prove that three distinct energy formulations--kinetic, conditional kinetic, and geodesic energy--are mathematically equivalent under the structure of MDMs, and that MDMs minimize all three when the mask schedule satisfies a closed-form optimality condition. This unification not only clarifies the theoretical foundations of MDMs, but also motivates practical improvements in sampling. By parameterizing interpolation schedules via Beta distributions, we reduce the schedule design space to a tractable 2D search, enabling efficient post-training tuning without model modification. Experiments on synthetic and real-world benchmarks demonstrate that our energy-inspired schedules outperform hand-crafted baselines, particularly in low-step sampling settings.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence</title>
<link>https://arxiv.org/abs/2510.24693</link>
<guid>https://arxiv.org/abs/2510.24693</guid>
<content:encoded><![CDATA[
arXiv:2510.24693v2 Announce Type: replace-cross 
Abstract: Despite rapid progress in Multi-modal Large Language Models and Large Audio-Language Models, existing audio benchmarks largely test semantics that can be recovered from text captions, masking deficits in fine-grained perceptual reasoning. We formalize audio 4D intelligence that is defined as reasoning over sound dynamics in time and 3D space, and introduce STAR-Bench to measure it. STAR-Bench combines a Foundational Acoustic Perception setting (six attributes under absolute and relative regimes) with a Holistic Spatio-Temporal Reasoning setting that includes segment reordering for continuous and discrete processes and spatial tasks spanning static localization, multi-source relations, and dynamic trajectories. Our data curation pipeline uses two methods to ensure high-quality samples. For foundational tasks, we use procedurally synthesized and physics-simulated audio. For holistic data, we follow a four-stage process that includes human annotation and final selection based on human performance. Unlike prior benchmarks where caption-only answering reduces accuracy slightly, STAR-Bench induces far larger drops (-31.5\% temporal, -35.2\% spatial), evidencing its focus on linguistically hard-to-describe cues. Evaluating 19 models reveals substantial gaps compared with humans and a capability hierarchy: closed-source models are bottlenecked by fine-grained perception, while open-source models lag across perception, knowledge, and reasoning. Our STAR-Bench provides critical insights and a clear path forward for developing future models with a more robust understanding of the physical world.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LongCat-Flash-Omni Technical Report</title>
<link>https://arxiv.org/abs/2511.00279</link>
<guid>https://arxiv.org/abs/2511.00279</guid>
<content:encoded><![CDATA[
arXiv:2511.00279v2 Announce Type: replace-cross 
Abstract: We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal model with 560 billion parameters, excelling at real-time audio-visual interaction. By adopting a curriculum-inspired progressive training strategy that transitions from simpler to increasingly complex modality sequence modeling tasks, LongCat-Flash-Omni attains comprehensive multimodal capabilities while maintaining strong unimodal capability. Building upon LongCat-Flash, which adopts a high-performance Shortcut-connected Mixture-of-Experts (MoE) architecture with zero-computation experts, LongCat-Flash-Omni integrates efficient multimodal perception and speech reconstruction modules. Despite its immense size of 560B parameters (with 27B activated), LongCat-Flash-Omni achieves low-latency real-time audio-visual interaction. For training infrastructure, we developed a modality-decoupled parallelism scheme specifically designed to manage the data and model heterogeneity inherent in large-scale multimodal training. This innovative approach demonstrates exceptional efficiency by sustaining over 90% of the throughput achieved by text-only training. Extensive evaluations show that LongCat-Flash-Omni achieves state-of-the-art performance on omni-modal benchmarks among open-source models. Furthermore, it delivers highly competitive results across a wide range of modality-specific tasks, including text, image, and video understanding, as well as audio understanding and generation. We provide a comprehensive overview of the model architecture design, training procedures, and data strategies, and open-source the model to foster future research and development in the community.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerating Training of Recursive Reasoning Models with Curriculum Guided Adaptive Recursion</title>
<link>https://arxiv.org/abs/2511.08653</link>
<guid>https://arxiv.org/abs/2511.08653</guid>
<content:encoded><![CDATA[
arXiv:2511.08653v2 Announce Type: replace-cross 
Abstract: Recursive reasoning models achieve remarkable performance on complex reasoning tasks through iterative refinement, enabling tiny networks to match large language models thousands of times their size. However, training remains computationally expensive, prior work reporting approximately 36 GPU-hours per dataset, limiting broader adoption and research. We propose CGAR, a novel training methodology that applies curriculum learning to architectural depth rather than traditional data ordering. CGAR introduces two synergistic components: Progressive Depth Curriculum dynamically adjusts recursion depth from shallow to deep configurations during training, preventing early overfitting while reducing computational cost, and Hierarchical Supervision Weighting applies exponentially decaying importance to supervision steps, aligning loss weighting with observed gradient magnitude decay. On Sudoku-Extreme with 423,168 test puzzles, CGAR achieves 1.71x training speedup (10.93 to 6.38 hours, 42% cost reduction) with only 0.63% accuracy drop (86.65% to 86.02%). Systematic ablations reveal Progressive Depth Curriculum alone achieves 2.26x speedup with 85.47% accuracy, demonstrating a rare Pareto improvement where architectural curriculum simultaneously enhances training efficiency and solution quality. CGAR-trained models exhibit superior inference efficiency with 100% halting accuracy and 11% fewer reasoning steps. Our work demonstrates that principled curriculum on architectural depth enables efficient training of recursive reasoning models on modest hardware. Code and models: https://github.com/Kaleemullahqasim/CGAR and https://huggingface.co/Kaleemullah/trm-cgar-sudoku
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Democratizing LLM Efficiency: From Hyperscale Optimizations to Universal Deployability</title>
<link>https://arxiv.org/abs/2511.20662</link>
<guid>https://arxiv.org/abs/2511.20662</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, efficiency, mixture-of-experts, retrofitting, overhead-aware efficiency<br /><br />Summary:  
The paper addresses the limitations of current efficiency techniques for large language models (LLMs), such as mixture-of-experts (MoE), speculative decoding, and retrieval-augmented generation (RAG), which are mainly effective in hyperscale environments with extensive infrastructure and elite teams. Outside of these contexts, these methods often result in overhead, fragility, and increased carbon footprint, disproportionately benefiting a few big tech companies while leaving many institutions like hospitals, schools, and governments without accessible solutions. The authors advocate for a shift toward "robust simplicity," emphasizing efficient LLM methods that perform well with limited resources and minimal expertise. Their proposed research agenda includes retrofitting pretrained models with efficient architectures without requiring retraining, developing lightweight fine-tuning techniques that maintain model alignment, making reasoning over long chains of thought more economical, and enabling dynamic knowledge management without heavy reliance on complex RAG pipelines. Additionally, they introduce Overhead-Aware Efficiency (OAE) as a standard benchmark that accounts for adoption cost, sustainability, and fairness. By redefining efficiency to incorporate these broader factors, the work aims to democratize LLM deployment, ensuring that optimizations reduce inequality and environmental impact rather than exacerbating them. <div>
arXiv:2511.20662v1 Announce Type: new 
Abstract: Large language models (LLMs) have become indispensable, but the most celebrated efficiency methods -- mixture-of-experts (MoE), speculative decoding, and complex retrieval-augmented generation (RAG) -- were built for hyperscale providers with vast infrastructure and elite teams. Outside that context, their benefits collapse into overhead, fragility, and wasted carbon. The result is that a handful of Big Tech companies benefit, while thousands of hospitals, schools, governments, and enterprises are left without viable options. We argue that the next frontier is not greater sophistication at scale, but robust simplicity: efficiency that thrives under modest resources and minimal expertise. We propose a new research agenda: retrofitting pretrained models with more efficient architectures without retraining, inventing lightweight fine-tuning that preserves alignment, making reasoning economical despite long chains of thought, enabling dynamic knowledge management without heavy RAG pipelines, and adopting Overhead-Aware Efficiency (OAE) as a standard benchmark. By redefining efficiency to include adoption cost, sustainability, and fairness, we can democratize LLM deployment -- ensuring that optimization reduces inequality and carbon waste rather than amplifying them.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Harmonic Token Projection (HTP): A Vocabulary-Free, Training-Free, Deterministic, and Reversible Embedding Methodology</title>
<link>https://arxiv.org/abs/2511.20665</link>
<guid>https://arxiv.org/abs/2511.20665</guid>
<content:encoded><![CDATA[
<div> Harmonic Token Projection, reversible embedding, deterministic encoding, semantic similarity, multilingual representation<br /><br />Summary:<br /><br />This paper presents Harmonic Token Projection (HTP), a novel framework for generating text embeddings that is both reversible and deterministic, eliminating the need for training, vocabularies, or stochastic parameters. Unlike traditional neural embeddings reliant on co-occurrence statistics or optimization, HTP encodes tokens analytically through harmonic trajectories derived from their Unicode integer values, creating a bijective and interpretable mapping between discrete tokens and continuous vectors. The harmonic nature ensures phase-coherent projections that preserve structural information, allowing accurate reconstruction of original tokens and enabling semantic similarity estimation via geometric alignment. Experimental results on the English Semantic Textual Similarity Benchmark (STS-B) and its multilingual extension demonstrate that HTP achieves a Spearman correlation of 0.68, maintaining consistent performance across ten languages. Remarkably, this approach requires negligible computational resources, processing sentence pairs with sub-millisecond latency, highlighting its efficiency. The study shows that meaningful semantic relationships can emerge purely from deterministic geometric representations, offering a transparent, training-free, and computationally light alternative to data-driven embedding methods. HTP thus combines interpretability, reversibility, and multilingual applicability, providing a promising direction for embedding techniques in natural language processing. <div>
arXiv:2511.20665v1 Announce Type: new 
Abstract: This paper introduces the Harmonic Token Projection (HTP), a reversible and deterministic framework for generating text embeddings without training, vocabularies, or stochastic parameters. Unlike neural embeddings that rely on statistical co-occurrence or optimization, HTP encodes each token analytically as a harmonic trajectory derived from its Unicode integer representation, establishing a bijective and interpretable mapping between discrete symbols and continuous vector space. The harmonic formulation provides phase-coherent projections that preserve both structure and reversibility, enabling semantic similarity estimation from purely geometric alignment. Experimental evaluation on the Semantic Textual Similarity Benchmark (STS-B) and its multilingual extension shows that HTP achieves a Spearman correlation of \r{ho} = 0.68 in English, maintaining stable performance across ten languages with negligible computational cost and sub-millisecond latency per sentence pair. This demonstrates that meaningful semantic relations can emerge from deterministic geometry, offering a transparent and efficient alternative to data-driven embeddings. Keywords: Harmonic Token Projection, reversible embedding, deterministic encoding, semantic similarity, multilingual representation.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A centroid based framework for text classification in itsm environments</title>
<link>https://arxiv.org/abs/2511.20667</link>
<guid>https://arxiv.org/abs/2511.20667</guid>
<content:encoded><![CDATA[
<div> Keywords: text classification, hierarchical taxonomy, IT service management, dual-embedding centroid, incremental updates<br /><br />Summary:<br />1. The paper addresses text classification challenges within IT Service Management (ITSM) systems, where support tickets need categorization into hierarchical, tree-structured taxonomies.<br />2. It proposes a novel dual-embedding centroid-based classification framework that separately maintains semantic and lexical centroid representations for each category.<br />3. These dual centroids are combined at inference time through reciprocal rank fusion, enhancing classification performance.<br />4. The framework's classification accuracy is competitive with Support Vector Machines, achieving hierarchical F1 scores of 0.731 compared to SVM's 0.727.<br />5. The method offers interpretability by using centroid representations, allowing insight into the classification process.<br />6. Experiments were conducted on a dataset of 8,968 ITSM tickets spanning 123 categories, demonstrating robust practical relevance.<br />7. Significant efficiency gains include 5.9 times faster training and up to 152 times faster incremental updates, crucial for real-time or frequently updated environments.<br />8. In addition, the approach yields an 8.6 to 8.8-fold speedup in batch processing (100–1000 samples), excluding embedding computation time.<br />9. These combined advantages make the approach highly suitable for deployment in production ITSM contexts where both interpretability and operational efficiency are prioritized. <div>
arXiv:2511.20667v1 Announce Type: new 
Abstract: Text classification with hierarchical taxonomies is a fundamental requirement in IT Service Management (ITSM) systems, where support tickets must be categorized into tree-structured taxonomies. We present a dual-embedding centroid-based classification framework that maintains separate semantic and lexical centroid representations per category, combining them through reciprocal rank fusion at inference time. The framework achieves performance competitive with Support Vector Machines (hierarchical F1: 0.731 vs 0.727) while providing interpretability through centroid representations. Evaluated on 8,968 ITSM tickets across 123 categories, this method achieves 5.9 times faster training and up to 152 times faster incremental updates. With 8.6-8.8 times speedup across batch sizes (100-1000 samples) when excluding embedding computation. These results make the method suitable for production ITSM environments prioritizing interpretability and operational efficiency.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PIRA: Preference-Oriented Instruction-Tuned Reward Models with Dual Aggregation</title>
<link>https://arxiv.org/abs/2511.20668</link>
<guid>https://arxiv.org/abs/2511.20668</guid>
<content:encoded><![CDATA[
<div> Keywords: Reward models, Large Language Models, preference-based instructions, reward overoptimization, PIRA

<br /><br />Summary: Reward models play a critical role in aligning Large Language Models (LLMs) with human preferences, but they encounter two main challenges: low data efficiency due to the direct concatenation of questions and responses in traditional discriminative reward models, and vulnerability to reward overoptimization. To address these, the paper proposes PIRA, a novel training paradigm with three key strategies. First, it reformulates question-answer pairs into preference-based instructions, offering clearer and more explicit task specifications, which enhances data efficiency. Second, PIRA aggregates rewards from a variety of preference tasks, reducing bias and promoting greater robustness in the reward signal. Third, it introduces the averaging of value-head outputs under different dropout rates, stabilizing reward outputs and mitigating overoptimization issues. Extensive experiments validate the effectiveness of PIRA, demonstrating improvements in both robustness and reliability of reward models. Overall, PIRA offers a promising approach to refining reward modeling in LLM alignment by improving task clarity, reducing bias across tasks, and stabilizing reward signals, thus addressing fundamental limitations of existing reward models. <div>
arXiv:2511.20668v1 Announce Type: new 
Abstract: Reward models are crucial for aligning Large Language Models (LLMs) with human preferences but face two representative challenges. First, traditional discriminative reward models usually concatenate questions and responses directly as input, resulting in low data efficiency. Second, reward models are vulnerable to reward overoptimization. We propose PIRA, a training paradigm addressing these issues through three strategies: (1) Reformulating question-answer pairs into preference-based instructions for clearer and more explicit task specification, (2) aggregating rewards from diverse preference tasks to reduce bias and improve robustness, and (3) averaging value-head outputs under varying dropout rates to stabilize rewards. Extensive experiments have demonstrated the effectiveness of PIRA.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structured Definitions and Segmentations for Legal Reasoning in LLMs: A Study on Indian Legal Data</title>
<link>https://arxiv.org/abs/2511.20669</link>
<guid>https://arxiv.org/abs/2511.20669</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, legal domain, rhetorical roles, Indian legal datasets, zero-shot learning  

<br /><br />Summary:  
This paper addresses the challenge of applying Large Language Models (LLMs) to the specialized legal domain, which often has long and complex documents that are difficult for standard LLMs to process effectively due to lack of domain-specific pretraining. The authors conducted experiments in three main areas: (i) reorganizing legal documents based on their rhetorical roles to evaluate how structured information impacts long-context understanding and model decisions, (ii) defining and explaining rhetorical roles to the model in order to improve familiarity with legal terminology, and (iii) simulating the step-by-step reasoning process typical of courts in relation to rhetorical roles to enhance the model’s inferencing ability. These experiments were performed using a zero-shot learning approach on three Indian legal judgment prediction datasets, meaning the models were not fine-tuned on domain data beforehand. Results demonstrated that restructuring data or providing explanations about legal terms noticeably improved model performance, with F1 score gains ranging between approximately 1.5% and 4.36% relative to baseline models. This indicates that even without full domain alignment, thoughtful contextual organization and clarification can significantly boost LLM effectiveness in legal tasks. <div>
arXiv:2511.20669v1 Announce Type: new 
Abstract: Large Language Models (LLMs), trained on extensive datasets from the web, exhibit remarkable general reasoning skills. Despite this, they often struggle in specialized areas like law, mainly because they lack domain-specific pretraining. The legal field presents unique challenges, as legal documents are generally long and intricate, making it hard for models to process the full text efficiently. Previous studies have examined in-context approaches to address the knowledge gap, boosting model performance in new domains without full domain alignment. In our paper, we analyze model behavior on legal tasks by conducting experiments in three areas: (i) reorganizing documents based on rhetorical roles to assess how structured information affects long context processing and model decisions, (ii) defining rhetorical roles to familiarize the model with legal terminology, and (iii) emulating the step-by-step reasoning of courts regarding rhetorical roles to enhance model reasoning. These experiments are conducted in a zero-shot setting across three Indian legal judgment prediction datasets. Our results reveal that organizing data or explaining key legal terms significantly boosts model performance, with a minimum increase of ~1.5% and a maximum improvement of 4.36% in F1 score compared to the baseline.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MindSET: Advancing Mental Health Benchmarking through Large-Scale Social Media Data</title>
<link>https://arxiv.org/abs/2511.20672</link>
<guid>https://arxiv.org/abs/2511.20672</guid>
<content:encoded><![CDATA[
<div> Keywords: MindSET, mental health, Reddit, benchmark dataset, classification

<br /><br />Summary:  
The paper introduces MindSET, a new large-scale benchmark dataset curated from Reddit, designed to analyze mental health through social media data. MindSET addresses limitations of previous datasets by incorporating over 13 million annotated posts related to seven mental health conditions, making it more than twice as large as earlier benchmarks. The data quality is enhanced through rigorous preprocessing steps, including filtering for language, removal of Not Safe for Work (NSFW) content, and elimination of duplicates. A linguistic analysis was conducted using LIWC (Linguistic Inquiry and Word Count) to explore psychological term frequencies across eight groups represented in the dataset. To demonstrate practical utility, the authors performed binary classification experiments to detect mental health diagnoses by fine-tuning language models and applying Bag-of-Words (BoW) features. Models trained on MindSET consistently outperformed those trained on previous datasets, with up to an 18-point F1 score improvement in Autism detection. The dataset thus provides a robust tool for researchers studying social media's role in mental health, enabling both early risk detection and analysis of emerging psychological trends in diverse online communities. <div>
arXiv:2511.20672v1 Announce Type: new 
Abstract: Social media data has become a vital resource for studying mental health, offering real-time insights into thoughts, emotions, and behaviors that traditional methods often miss. Progress in this area has been facilitated by benchmark datasets for mental health analysis; however, most existing benchmarks have become outdated due to limited data availability, inadequate cleaning, and the inherently diverse nature of social media content (e.g., multilingual and harmful material). We present a new benchmark dataset, \textbf{MindSET}, curated from Reddit using self-reported diagnoses to address these limitations. The annotated dataset contains over \textbf{13M} annotated posts across seven mental health conditions, more than twice the size of previous benchmarks. To ensure data quality, we applied rigorous preprocessing steps, including language filtering, and removal of Not Safe for Work (NSFW) and duplicate content. We further performed a linguistic analysis using LIWC to examine psychological term frequencies across the eight groups represented in the dataset. To demonstrate the dataset utility, we conducted binary classification experiments for diagnosis detection using both fine-tuned language models and Bag-of-Words (BoW) features. Models trained on MindSET consistently outperformed those trained on previous benchmarks, achieving up to an \textbf{18-point} improvement in F1 for Autism detection. Overall, MindSET provides a robust foundation for researchers exploring the intersection of social media and mental health, supporting both early risk detection and deeper analysis of emerging psychological trends.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantics Meet Signals: Dual Codebook Representationl Learning for Generative Recommendation</title>
<link>https://arxiv.org/abs/2511.20673</link>
<guid>https://arxiv.org/abs/2511.20673</guid>
<content:encoded><![CDATA[
<div> Generative recommendation, codebook, collaborative filtering, semantic tokens, popularity-aware  

<br /><br />Summary:  
Generative recommendation is a powerful approach that unifies item retrieval and generation by representing items as discrete semantic tokens and applying autoregressive sequence modeling. Existing methods use a single uniform codebook that encodes all items equally, which fails to address the imbalance between popular items rich in collaborative signals and long-tail items reliant on semantic understanding. This uniform approach limits representational efficiency and generalization capabilities. To overcome this, the authors introduce FlexCode, a popularity-aware framework that adaptively assigns a fixed token budget across two codebooks: a collaborative filtering (CF) codebook and a semantic codebook. FlexCode employs a lightweight mixture-of-experts (MoE) mechanism to dynamically balance CF-specific precision and semantic generalization. Additionally, an alignment and smoothness objective helps maintain coherence across the popularity spectrum of items. Empirical experiments conducted on both public and large-scale industrial datasets show that FlexCode consistently outperforms strong baseline models. The approach improves accuracy and robustness, particularly for tail items, and offers a new mechanism for token representation in generative recommenders. Overall, FlexCode presents a novel way to balance memorization and generalization in token-based recommendation models, enhancing both effectiveness and scalability. <div>
arXiv:2511.20673v1 Announce Type: new 
Abstract: Generative recommendation has recently emerged as a powerful paradigm that unifies retrieval and generation, representing items as discrete semantic tokens and enabling flexible sequence modeling with autoregressive models. Despite its success, existing approaches rely on a single, uniform codebook to encode all items, overlooking the inherent imbalance between popular items rich in collaborative signals and long-tail items that depend on semantic understanding. We argue that this uniform treatment limits representational efficiency and hinders generalization. To address this, we introduce FlexCode, a popularity-aware framework that adaptively allocates a fixed token budget between a collaborative filtering (CF) codebook and a semantic codebook. A lightweight MoE dynamically balances CF-specific precision and semantic generalization, while an alignment and smoothness objective maintains coherence across the popularity spectrum. We perform experiments on both public and industrial-scale datasets, showing that FlexCode consistently outperform strong baselines. FlexCode provides a new mechanism for token representation in generative recommenders, achieving stronger accuracy and tail robustness, and offering a new perspective on balancing memorization and generalization in token-based recommendation models.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt Engineering Techniques for Context-dependent Text-to-SQL in Arabic</title>
<link>https://arxiv.org/abs/2511.20677</link>
<guid>https://arxiv.org/abs/2511.20677</guid>
<content:encoded><![CDATA[
<div> Keywords: Arabic text-to-SQL, cross-domain, context-dependent, large language models, GAT corrector<br /><br />Summary:<br /><br />1. The paper introduces Ar-SParC, the first Arabic cross-domain, context-dependent text-to-SQL dataset, addressing the lack of resources in the Arabic language for this task. <br />2. Ar-SParC contains 3,450 sequences of interrelated questions, with each sequence averaging about three questions, totaling 10,225 questions paired with their corresponding SQL queries.<br />3. The authors performed 40 experiments using two large language models, GPT-3.5-turbo and GPT-4.5-turbo, applying 10 different prompt engineering techniques including four question representation methods and six in-context learning techniques.<br />4. A novel method named GAT corrector was developed, which enhanced model performance in all 40 experiments, improving execution accuracy (EX) by an average of 1.9% and interaction accuracy (IX) by 1.9% under zero-shot settings, and by 1.72% EX and 0.92% IX under in-context learning.<br />5. An ablation study was conducted with two additional experiments to analyze why the GAT corrector outperforms the previously used GAT verifier, especially in the context of the Arabic language, explaining its effectiveness for this specific task. <div>
arXiv:2511.20677v1 Announce Type: new 
Abstract: In recent years, the task of cross-domain, context-dependent text-to-SQL has received significant attention. Enables users with no prior knowledge of SQL to have a conversation with databases using natural language. However, most of the available datasets and research have been conducted in English, along with some work in Chinese. To this date, no effort has been made to address this task in the Arabic language. In this paper, we introduce Ar-SParC, the first Arabic cross-domain, context-dependent text-to-SQL dataset. The dataset consists of 3,450 sequences of interrelated questions, each sequence containing an average of approximately three questions, which results in a total of 10225 questions along with their corresponding SQL queries. We conducted 40 experiments on the Ar-SParC dataset using two large language models, GPT-3.5-turbo and GPT-4.5-turbo, applying 10 different prompt engineering techniques, including four question representation methods and six in-context learning techniques. Furthermore, we developed a novel approach named GAT corrector, which enhanced the performance across all 40 experiments, yielding an average improvement of 1.9% in execution accuracy (EX) and 1.9% in interaction accuracy (IX) under zero-shot settings, and an average increase of 1.72% EX and 0.92% IX under in-context learning settings. Finally, we conducted an ablation study with two more experiments to explain why the GAT corrector outperformed the previous GAT verifier technique, particularly for the Arabic language.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cognitive bias in LLM reasoning compromises interpretation of clinical oncology notes</title>
<link>https://arxiv.org/abs/2511.20680</link>
<guid>https://arxiv.org/abs/2511.20680</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, reasoning errors, oncology decision support, cognitive biases, clinical safety<br /><br />Summary:<br /><br />1. This study investigates the reasoning failures of large language models (LLMs) like GPT-4 in the context of oncology decision support, highlighting that despite high accuracy, these models can produce clinically unsafe recommendations due to faulty reasoning.<br /><br />2. Researchers developed a hierarchical, three-tier taxonomy of reasoning errors derived from chain-of-thought responses to real oncology clinical notes, linking computational failures to established cognitive bias frameworks.<br /><br />3. The taxonomy was created using annotations of 600 reasoning traces from breast and pancreatic cancer notes (CORAL dataset) and validated on 822 GPT-4 responses from prostate cancer consult notes, covering disease stages from localized to metastatic.<br /><br />4. Results showed reasoning errors in 23% of interpretations, with confirmation bias and anchoring bias being the most frequent, and these errors correlated strongly with guideline-discordant and potentially harmful clinical recommendations, especially in advanced disease management.<br /><br />5. Although automated evaluators with advanced language models could detect the presence of errors, they failed to reliably classify specific subtypes of reasoning errors, underscoring the need for improved evaluation frameworks.<br /><br />This taxonomy offers a generalizable approach to assess and improve reasoning fidelity in LLMs before applying them in clinical oncology settings, aiming to enhance patient safety and model trustworthiness. <div>
arXiv:2511.20680v1 Announce Type: new 
Abstract: Despite high performance on clinical benchmarks, large language models may reach correct conclusions through faulty reasoning, a failure mode with safety implications for oncology decision support that is not captured by accuracy-based evaluation. In this two-cohort retrospective study, we developed a hierarchical taxonomy of reasoning errors from GPT-4 chain-of-thought responses to real oncology notes and tested its clinical relevance. Using breast and pancreatic cancer notes from the CORAL dataset, we annotated 600 reasoning traces to define a three-tier taxonomy mapping computational failures to cognitive bias frameworks. We validated the taxonomy on 822 responses from prostate cancer consult notes spanning localized through metastatic disease, simulating extraction, analysis, and clinical recommendation tasks. Reasoning errors occurred in 23 percent of interpretations and dominated overall errors, with confirmation bias and anchoring bias most common. Reasoning failures were associated with guideline-discordant and potentially harmful recommendations, particularly in advanced disease management. Automated evaluators using state-of-the-art language models detected error presence but could not reliably classify subtypes. These findings show that large language models may provide fluent but clinically unsafe recommendations when reasoning is flawed. The taxonomy provides a generalizable framework for evaluating and improving reasoning fidelity before clinical deployment.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Template Selection for Output Token Generation Optimization: MLP-Based and Transformer Approaches</title>
<link>https://arxiv.org/abs/2511.20683</link>
<guid>https://arxiv.org/abs/2511.20683</guid>
<content:encoded><![CDATA[
<div> Dynamic Template Selection, routing accuracy, large language models, cost reduction, query complexity<br /><br />Summary:<br /><br />1. Contemporary large language model deployments often use uniform prompting strategies regardless of query complexity, which results in token inefficiency, especially since output tokens are significantly more expensive than input tokens (4-8x higher across major providers).<br /><br />2. The authors propose Dynamic Template Selection (DTS), a method that adaptively selects response templates based on the complexity of the query, leading to substantial cost reductions without sacrificing response quality.<br /><br />3. Two routing approaches were compared: a simple multilayer perceptron (MLP) utilizing pre-computed embeddings, and a fine-tuned RoBERTa transformer. The MLP router achieved a routing accuracy of 90.5% on held-out test data, slightly outperforming RoBERTa's 89.5% accuracy, while using significantly fewer parameters (125M fewer).<br /><br />4. Extensive empirical evaluations on 1,000 MMLU questions and 9,000 production API calls across three major LLM providers (OpenAI GPT-4, Google Gemini, Anthropic Claude) demonstrated provider-agnostic routing decisions, with consistent 90.5% accuracy and token reductions ranging from 32.6% to 33.9%, reflecting differences in provider generation characteristics.<br /><br />5. The work presents a formal problem formulation supported by theoretical machine learning foundations, four algorithms with complexity analyses, and comprehensive validation on production systems, highlighting practical and cost-saving benefits of adaptive template routing in LLM applications. <div>
arXiv:2511.20683v1 Announce Type: new 
Abstract: Contemporary large language model deployments typically employ uniform prompting strategies across diverse query types, applying verbose response patterns to both complex analytical tasks and straightforward factual questions. This one-size-fits-all methodology leads to substantial token inefficiency, a concern amplified by the significant cost differential between input and output tokens--the latter commanding 4-8x higher prices across major providers. We present Dynamic Template Selection (DTS), which adaptively matches response templates to query complexity, achieving significant cost reductions without compromising response quality.
  We compared two routing approaches: a simple MLP that uses pre-computed embeddings and a more complex fine-tuned RoBERTa transformer. Through comprehensive evaluation on 1,000 MMLU questions, we find that the MLP router achieves 90.5% routing accuracy on held-out test data, marginally exceeding RoBERTa's performance (89.5%) despite utilizing 125M fewer parameters. Notably, our empirical analysis reveals provider-agnostic behavior in template selection--routing decisions generalize effectively across 3 major LLM providers (OpenAI GPT-4, Google Gemini, and Anthropic Claude), as validated through 9,000 production API calls. While routing accuracy remains consistent at 90.5% across providers, observed token reductions vary from 32.6% to 33.9%, reflecting provider-specific generation characteristics.
  This work contributes several key elements: formal problem formulation with theoretical grounding in machine learning, four algorithms with corresponding complexity analyses, and extensive empirical validation across production systems.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs-Powered Accurate Extraction, Querying and Intelligent Management of Literature derived 2D Materials Data</title>
<link>https://arxiv.org/abs/2511.20691</link>
<guid>https://arxiv.org/abs/2511.20691</guid>
<content:encoded><![CDATA[
<div> Two-dimensional materials, energy storage, electronic properties, synthesis methods, data extraction  

<br /><br />Summary:  
1. Two-dimensional (2D) materials have garnered significant attention due to their unique physicochemical and electronic properties, making them highly suitable for energy storage and conversion applications.  
2. Most valuable information about these materials, including their intrinsic properties and synthesis techniques, is embedded within numerous published research papers.  
3. A major challenge arises from the dispersed nature of this valuable data, making it difficult to systematically collect and analyze the synthesis methods and properties of 2D materials.  
4. The paper likely discusses strategies or methodologies aimed at extracting, integrating, or organizing this scattered information to better understand and accelerate the development of 2D materials.  
5. By consolidating data on the properties and preparation methods, it becomes possible to optimize material design and enhance their performance in energy-related applications. <div>
arXiv:2511.20691v1 Announce Type: new 
Abstract: Two-dimensional (2D) materials have showed widespread applications in energy storage and conversion owning to their unique physicochemical, and electronic properties. Most of the valuable information for the materials, such as their properties and preparation methods, is included in the published research papers. However, due to the dispersion of synthe
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memories Retrieved from Many Paths: A Multi-Prefix Framework for Robust Detection of Training Data Leakage in Large Language Models</title>
<link>https://arxiv.org/abs/2511.20799</link>
<guid>https://arxiv.org/abs/2511.20799</guid>
<content:encoded><![CDATA[
<div> Memorization, Large Language Models, Multi-Prefix Memorization, Data Leakage, Privacy Risks<br /><br />Summary: This paper addresses the issue of verbatim memorization in large language models (LLMs), which poses privacy and copyright concerns due to the models reproducing training data exactly. Traditional definitions and detection methods for memorization have limitations, particularly with aligned language models. To overcome these issues, the authors propose a new framework called multi-prefix memorization. The key insight is that memorized sequences can be retrieved through many different starting prompts or prefixes, unlike non-memorized content, which is only accessible via very specific prefixes. They formalize memorization by requiring that an adversarial search finds a substantial number of distinct prefixes all generating the same target sequence. This shifts the focus from detecting memorization via single extraction paths to assessing the robustness and multiplicity of retrieval routes. Experimental results on both open-source and aligned chat models demonstrate that this multi-prefix approach reliably differentiates memorized data from non-memorized content. Overall, their method offers a practical and comprehensive audit tool to better identify and manage data leakage risks in LLMs. <div>
arXiv:2511.20799v1 Announce Type: new 
Abstract: Large language models, trained on massive corpora, are prone to verbatim memorization of training data, creating significant privacy and copyright risks. While previous works have proposed various definitions for memorization, many exhibit shortcomings in comprehensively capturing this phenomenon, especially in aligned models. To address this, we introduce a novel framework: multi-prefix memorization. Our core insight is that memorized sequences are deeply encoded and thus retrievable via a significantly larger number of distinct prefixes than non-memorized content. We formalize this by defining a sequence as memorized if an external adversarial search can identify a target count of distinct prefixes that elicit it. This framework shifts the focus from single-path extraction to quantifying the robustness of a memory, measured by the diversity of its retrieval paths. Through experiments on open-source and aligned chat models, we demonstrate that our multi-prefix definition reliably distinguishes memorized from non-memorized data, providing a robust and practical tool for auditing data leakage in LLMs.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAGE: An Agentic Explainer Framework for Interpreting SAE Features in Language Models</title>
<link>https://arxiv.org/abs/2511.20820</link>
<guid>https://arxiv.org/abs/2511.20820</guid>
<content:encoded><![CDATA[
<div> Sparse Autoencoders, Large Language Models, Feature Interpretation, Agent-based Framework, Explanation-driven Process<br /><br />Summary:<br /><br />This paper addresses the opacity of large language models (LLMs) by focusing on the interpretation of their internal representations. It leverages Sparse Autoencoders (SAEs) as a tool to decompose these representations into more interpretable features. However, understanding these features remains challenging. To overcome this, the authors propose SAGE (SAE AGentic Explainer), an innovative agent-based framework that transforms feature interpretation from a passive, single-pass task into an active, explanation-driven process. SAGE systematically generates multiple hypotheses for each feature and designs targeted experiments to test these explanations. It then iteratively refines the explanations based on empirical activation feedback collected during these experiments. The framework’s methodology rigorously combines generation, testing, and refinement steps to produce high-quality, interpretable explanations. Experimental results demonstrate that SAGE significantly outperforms existing state-of-the-art baselines on features extracted from SAEs across diverse language models, achieving improvements in both generative and predictive accuracy. This approach marks a substantial advance in explainability techniques for LLMs, potentially enhancing their safe and reliable deployment by making their internal mechanisms more transparent. <div>
arXiv:2511.20820v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved remarkable progress, yet their internal mechanisms remain largely opaque, posing a significant challenge to their safe and reliable deployment. Sparse autoencoders (SAEs) have emerged as a promising tool for decomposing LLM representations into more interpretable features, but explaining the features captured by SAEs remains a challenging task. In this work, we propose SAGE (SAE AGentic Explainer), an agent-based framework that recasts feature interpretation from a passive, single-pass generation task into an active, explanation-driven process. SAGE implements a rigorous methodology by systematically formulating multiple explanations for each feature, designing targeted experiments to test them, and iteratively refining explanations based on empirical activation feedback. Experiments on features from SAEs of diverse language models demonstrate that SAGE produces explanations with significantly higher generative and predictive accuracy compared to state-of-the-art baselines.an agent-based framework that recasts feature interpretation from a passive, single-pass generation task into an active, explanationdriven process. SAGE implements a rigorous methodology by systematically formulating multiple explanations for each feature, designing targeted experiments to test them, and iteratively refining explanations based on empirical activation feedback. Experiments on features from SAEs of diverse language models demonstrate that SAGE produces explanations with significantly higher generative and predictive accuracy compared to state-of-the-art baselines.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structured Prompting Enables More Robust, Holistic Evaluation of Language Models</title>
<link>https://arxiv.org/abs/2511.20836</link>
<guid>https://arxiv.org/abs/2511.20836</guid>
<content:encoded><![CDATA[
<div> Keywords: Language Models, Benchmarking, Prompt Engineering, Structured Prompting, HELM

<br /><br />Summary: This paper addresses the challenge of accurately benchmarking language models (LMs) across various domains, emphasizing the limitations of existing frameworks such as HELM that use fixed prompts leading to underestimation of LM performance. It highlights the importance of estimating each LM's performance ceiling by optimizing prompts rather than relying on static ones. The authors introduce a DSPy+HELM framework that integrates structured prompting methods, including reasoning through chain-of-thought prompting, to better capture LM capabilities. They evaluate four advanced LMs across seven benchmarks in both general and medical domains using four prompting strategies, comparing results against baseline HELM scores. Key findings include that without structured prompting, HELM underestimates LM performance by an average of 4%, exhibits greater variability (+2% standard deviation) in performance estimates, and misrepresents performance gaps causing leaderboard changes in 3 out of 7 benchmarks. Moreover, incorporating reasoning reduces sensitivity to prompt design, resulting in more stable LM evaluations. This study is the first large-scale empirical characterization of LM behavior across benchmarks and prompting approaches, demonstrating that scalable prompt optimization permits more accurate and decision-useful performance assessments. The researchers also open-source their integration and prompt optimization pipeline for public use. <div>
arXiv:2511.20836v1 Announce Type: new 
Abstract: As language models (LMs) are increasingly adopted across domains, high-quality benchmarking frameworks that accurately estimate performance are essential for guiding deployment decisions. While frameworks such as Holistic Evaluation of Language Models (HELM) enable broad evaluation across tasks, they often rely on fixed prompts that fail to generalize across LMs, yielding unrepresentative performance estimates. Unless we estimate each LM's ceiling (maximum achievable via changes to the prompt), we risk underestimating performance. Declarative prompting frameworks, such as DSPy, offer a scalable alternative to manual prompt engineering by crafting structured prompts that can be optimized per task. However, such frameworks have not been systematically evaluated across established benchmarks. We present a reproducible DSPy+HELM framework that introduces structured prompting methods which elicit reasoning, enabling more accurate LM benchmarking. Using four prompting methods, we evaluate four frontier LMs across seven benchmarks (general/medical domain) against existing HELM baseline scores. We find that without structured prompting: (i) HELM underestimates LM performance (by 4% average), (ii) performance estimates vary more across benchmarks (+2% standard deviation), (iii) performance gaps are misrepresented (leaderboard rankings flip on 3/7 benchmarks), and (iv) introducing reasoning (chain-of-thought) reduces LM sensitivity to prompt design (smaller {\Delta} across prompts). To our knowledge, this is the first large-scale benchmarking study to empirically characterize LM behavior across benchmarks and prompting methods, showing that scalable performance ceiling estimation enables more decision-useful benchmarks. We open-source (i) DSPy+HELM Integration (https://github.com/stanford-crfm/helm/pull/3893) and (ii) Prompt Optimization Pipeline (https://github.com/StanfordMIMI/dspy-helm).
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Length-MAX Tokenizer for Language Models</title>
<link>https://arxiv.org/abs/2511.20849</link>
<guid>https://arxiv.org/abs/2511.20849</guid>
<content:encoded><![CDATA[
<div> Keywords: tokenizer, Length-MAX, token efficiency, language models, inference latency<br /><br />Summary:<br /><br />1. The paper introduces a novel tokenizer called Length-MAX that minimizes the average number of tokens per character, aiming to reduce token count in both training and inference phases of language models. <br /><br />2. Length-MAX formulates vocabulary construction as a length-weighted objective maximization problem and solves it using a greedy graph partitioning approximation algorithm. <br /><br />3. Experimental results on FineWeb and various domains show Length-MAX achieves 14–18% fewer tokens than Byte Pair Encoding (BPE) for vocabularies ranging from 10K to 50K, and a 13.0% reduction at a 64K vocabulary. <br /><br />4. Training GPT-2 models (124M, 355M, and 1.3B parameters) from scratch demonstrates Length-MAX reduces the number of training steps needed to reach a fixed validation loss by approximately 17–18.5%. <br /><br />5. The tokenizer decreases inference latency by around 12.7–13.7%, improves throughput by 16% at the 124M scale, lowers LAMBADA perplexity by 11.7%, and raises HellaSwag accuracy by 4.3%. <br /><br />6. It achieves high vocabulary coverage (99.62%) and maintains a low out-of-vocabulary rate (0.12%) on tests, while also reducing embedding and key-value cache memory by 18% during inference. <br /><br />7. Overall, optimizing for average token length rather than token frequency alone results in more efficient language modeling, often improving downstream task performance without sacrificing compatibility with production systems. <div>
arXiv:2511.20849v1 Announce Type: new 
Abstract: We introduce a new tokenizer for language models that minimizes the average tokens per character, thereby reducing the number of tokens needed to represent text during training and to generate text during inference. Our method, which we refer to as the Length-MAX tokenizer, obtains its vocabulary by casting a length-weighted objective maximization as a graph partitioning problem and developing a greedy approximation algorithm. On FineWeb and diverse domains, it yields 14--18\% fewer tokens than Byte Pair Encoding (BPE) across vocabulary sizes from 10K to 50K, and the reduction is 13.0\% when the size is 64K. Training GPT-2 models at 124M, 355M, and 1.3B parameters from scratch with five runs each shows 18.5\%, 17.2\%, and 18.5\% fewer steps, respectively, to reach a fixed validation loss, and 13.7\%, 12.7\%, and 13.7\% lower inference latency, together with a 16\% throughput gain at 124M, while consistently improving on downstream tasks including reducing LAMBADA perplexity by 11.7\% and enhancing HellaSwag accuracy by 4.3\%. Moreover, the Length-MAX tokenizer achieves 99.62\% vocabulary coverage and the out-of-vocabulary rate remains low at 0.12\% on test sets. These results demonstrate that optimizing for average token length, rather than frequency alone, offers an effective approach to more efficient language modeling without sacrificing -- and often improving -- downstream performance. The tokenizer is compatible with production systems and reduces embedding and KV-cache memory by 18\% at inference.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory</title>
<link>https://arxiv.org/abs/2511.20857</link>
<guid>https://arxiv.org/abs/2511.20857</guid>
<content:encoded><![CDATA[
<div> Statefulness, Memory management, Large language models, Continual learning, Benchmarking  

<br /><br />Summary:  
This paper addresses the critical role of statefulness and memory management in large language model (LLM) agents for long-term planning and problem-solving. It highlights a gap in current evaluations that focus on static conversational memory retrieval but neglect the dynamic evolution of memory across continuous task streams. Real-world applications, such as interactive assistants and embodied agents, require LLMs to continuously learn and adapt from ongoing interactions, yet they often fail to do so effectively. To tackle this, the authors introduce Evo-Memory, a comprehensive streaming benchmark and framework designed to evaluate how LLM agents self-evolve their memory over sequential tasks. Evo-Memory structures datasets into continuous streams, forcing the models to search, adapt, and update memory iteratively. The paper unifies and implements over ten representative memory modules and tests them across 10 diverse datasets covering multi-turn goal-oriented dialogues, reasoning tasks, and QA scenarios. Furthermore, the authors propose ExpRAG, a baseline method that retrieves and leverages prior experience efficiently, and ReMem, an innovative pipeline integrating reasoning, task actions, and memory refinement for continual memory improvement during deployment. This work advances the benchmarking and methodology for self-evolving memory in LLM agents, crucial for real-world interactive tasks. <div>
arXiv:2511.20857v1 Announce Type: new 
Abstract: Statefulness is essential for large language model (LLM) agents to perform long-term planning and problem-solving. This makes memory a critical component, yet its management and evolution remain largely underexplored. Existing evaluations mostly focus on static conversational settings, where memory is passively retrieved from dialogue to answer queries, overlooking the dynamic ability to accumulate and reuse experience across evolving task streams. In real-world environments such as interactive problem assistants or embodied agents, LLMs are required to handle continuous task streams, yet often fail to learn from accumulated interactions, losing valuable contextual insights, a limitation that calls for test-time evolution, where LLMs retrieve, integrate, and update memory continuously during deployment. To bridge this gap, we introduce Evo-Memory, a comprehensive streaming benchmark and framework for evaluating self-evolving memory in LLM agents. Evo-Memory structures datasets into sequential task streams, requiring LLMs to search, adapt, and evolve memory after each interaction. We unify and implement over ten representative memory modules and evaluate them across 10 diverse multi-turn goal-oriented and single-turn reasoning and QA datasets. To better benchmark experience reuse, we provide a baseline method, ExpRAG, for retrieving and utilizing prior experience, and further propose ReMem, an action-think-memory refine pipeline that tightly integrates reasoning, task actions, and memory updates to achieve continual improvement.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Winning with Less for Low Resource Languages: Advantage of Cross-Lingual English_Persian Argument Mining Model over LLM Augmentation</title>
<link>https://arxiv.org/abs/2511.20872</link>
<guid>https://arxiv.org/abs/2511.20872</guid>
<content:encoded><![CDATA[
<div> Keywords: argument mining, cross-lingual, low-resource languages, zero-shot transfer, large language models<br /><br />Summary:<br />1. This paper addresses the challenge of argument mining, focusing on identifying premises, conclusions, and their relations within texts, particularly for low-resource languages. <br />2. The study explores a cross-lingual approach, using English (high-resource) and Persian (low-resource) languages to construct and evaluate three training scenarios. <br />3. The evaluation is based on the English Microtext corpus and its manually translated Persian parallel corpus. <br />4. The three scenarios include: (i) zero-shot transfer with training only on English data, (ii) English-only training augmented with synthetic examples generated by Large Language Models (LLMs), and (iii) a cross-lingual model combining original English data with manual Persian translations. <br />5. Results show zero-shot achieves around 50% F1 on both languages, LLM-augmented training improves performance to 59.2% (English) and 69.3% (Persian), while the cross-lingual model significantly outperforms both with 74.8% F1 on Persian. <br />6. The findings highlight that lightweight cross-lingual training can offer a more effective and resource-efficient solution than augmentation methods for enhancing argument mining in low-resource languages. <div>
arXiv:2511.20872v1 Announce Type: new 
Abstract: Argument mining is a subfield of natural language processing to identify and extract the argument components, like premises and conclusions, within a text and to recognize the relations between them. It reveals the logical structure of texts to be used in tasks like knowledge extraction. This paper aims at utilizing a cross-lingual approach to argument mining for low-resource languages, by constructing three training scenarios. We examine the models on English, as a high-resource language, and Persian, as a low-resource language. To this end, we evaluate the models based on the English Microtext corpus \citep{PeldszusStede2015}, and its parallel Persian translation. The learning scenarios are as follow: (i) zero-shot transfer, where the model is trained solely with the English data, (ii) English-only training enhanced by synthetic examples generated by Large Language Models (LLMs), and (iii) a cross-lingual model that combines the original English data with manually translated Persian sentences. The zero-shot transfer model attains F1 scores of 50.2\% on the English test set and 50.7\% on the Persian test set. LLM-based augmentation model improves the performance up to 59.2\% on English and 69.3\% on Persian. The cross-lingual model, trained on both languages but evaluated solely on the Persian test set, surpasses the LLM-based variant, by achieving a F1 of 74.8\%. Results indicate that a lightweight cross-lingual blend can outperform considerably the more resource-intensive augmentation pipelines, and it offers a practical pathway for the argument mining task to overcome data resource shortage on low-resource languages.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emergence and Localisation of Semantic Role Circuits in LLMs</title>
<link>https://arxiv.org/abs/2511.20910</link>
<guid>https://arxiv.org/abs/2511.20910</guid>
<content:encoded><![CDATA[
<div> Keywords: semantic roles, large language models, internal mechanisms, structural refinement, cross-model comparison<br /><br />Summary: This paper investigates how large language models (LLMs) internally implement abstract semantic roles, a topic that remains poorly understood despite LLMs showing strong semantic capabilities. First, the authors introduce a novel analytical framework combining role-cross minimal pairs, temporal emergence analysis, and cross-model comparison to probe these internal mechanisms. Second, they reveal that semantic role processing is supported by highly concentrated circuits, with 89-94% of attribution focused within approximately 28 nodes, indicating compact and localized neural substructures. Third, rather than abrupt phase transitions, LLMs exhibit gradual structural refinement as they scale, with larger models occasionally bypassing these localized circuits, suggesting evolving internal dynamics. Fourth, cross-scale analysis shows moderate conservation of components (24-59% overlap) alongside high spectral similarity, implying that while exact components vary, functionally similar mechanisms persist across different model sizes and architectures. Overall, the findings suggest that LLMs develop compact, causally isolated mechanisms dedicated to encoding abstract semantic structure, and these mechanisms exhibit partial transferability across architectural and scale variations, enriching understanding of semantic grounding within large-scale neural models. <div>
arXiv:2511.20910v1 Announce Type: new 
Abstract: Despite displaying semantic competence, large language models' internal mechanisms that ground abstract semantic structure remain insufficiently characterised. We propose a method integrating role-cross minimal pairs, temporal emergence analysis, and cross-model comparison to study how LLMs implement semantic roles. Our analysis uncovers: (i) highly concentrated circuits (89-94% attribution within 28 nodes); (ii) gradual structural refinement rather than phase transitions, with larger models sometimes bypassing localised circuits; and (iii) moderate cross-scale conservation (24-59% component overlap) alongside high spectral similarity. These findings suggest that LLMs form compact, causally isolated mechanisms for abstract semantic structure, and these mechanisms exhibit partial transfer across scales and architectures.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chatty-KG: A Multi-Agent AI System for On-Demand Conversational Question Answering over Knowledge Graphs</title>
<link>https://arxiv.org/abs/2511.20940</link>
<guid>https://arxiv.org/abs/2511.20940</guid>
<content:encoded><![CDATA[
<div> Conversational QA, Knowledge Graphs, Large Language Models, Multi-agent System, SPARQL Query Generation<br /><br />Summary:<br /><br />1. The paper addresses the challenge of conversational question answering (QA) over knowledge graphs (KGs), aiming to combine the structured factual grounding of KG-based QA with the interactive capabilities of dialogue systems. 2. Existing systems face issues: Large language models (LLMs) enable context-aware conversations but lack direct access to private and dynamic KGs, while retrieval-augmented generation (RAG) often struggles with preserving graph structure, multi-turn context, and indexing overhead. Traditional KGQA systems tend to support only single-turn queries, suffer from latency, and have difficulty with coreference and dialogue context tracking. 3. To overcome these limitations, the authors propose Chatty-KG, a modular multi-agent framework that integrates RAG retrieval with structured query execution by using task-specialized LLM agents. These agents work collaboratively on contextual interpretation, dialogue state tracking, entity/relation linking, and efficient SPARQL query planning. 4. Experiments on large, diverse KGs demonstrate that Chatty-KG achieves substantial performance gains over state-of-the-art baselines in both single-turn and multi-turn QA, with improved F1 and P@1 measures. 5. The modular design supports coherent dialogue management and adaptation to evolving KGs without requiring fine-tuning or pre-processing. Evaluations with both commercial (GPT-4o, Gemini-2.0) and open-weight (Phi-4, Gemma 3) LLMs show broad compatibility and consistent robust performance, establishing Chatty-KG as a scalable, flexible, and reliable approach for multi-turn conversational KGQA. <div>
arXiv:2511.20940v1 Announce Type: new 
Abstract: Conversational Question Answering over Knowledge Graphs (KGs) combines the factual grounding of KG-based QA with the interactive nature of dialogue systems. KGs are widely used in enterprise and domain applications to provide structured, evolving, and reliable knowledge. Large language models (LLMs) enable natural and context-aware conversations, but lack direct access to private and dynamic KGs. Retrieval-augmented generation (RAG) systems can retrieve graph content but often serialize structure, struggle with multi-turn context, and require heavy indexing. Traditional KGQA systems preserve structure but typically support only single-turn QA, incur high latency, and struggle with coreference and context tracking. To address these limitations, we propose Chatty-KG, a modular multi-agent system for conversational QA over KGs. Chatty-KG combines RAG-style retrieval with structured execution by generating SPARQL queries through task-specialized LLM agents. These agents collaborate for contextual interpretation, dialogue tracking, entity and relation linking, and efficient query planning, enabling accurate and low-latency translation of natural questions into executable queries. Experiments on large and diverse KGs show that Chatty-KG significantly outperforms state-of-the-art baselines in both single-turn and multi-turn settings, achieving higher F1 and P@1 scores. Its modular design preserves dialogue coherence and supports evolving KGs without fine-tuning or pre-processing. Evaluations with commercial (e.g., GPT-4o, Gemini-2.0) and open-weight (e.g., Phi-4, Gemma 3) LLMs confirm broad compatibility and stable performance. Overall, Chatty-KG unifies conversational flexibility with structured KG grounding, offering a scalable and extensible approach for reliable multi-turn KGQA.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TrackList: Tracing Back Query Linguistic Diversity for Head and Tail Knowledge in Open Large Language Models</title>
<link>https://arxiv.org/abs/2511.21006</link>
<guid>https://arxiv.org/abs/2511.21006</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, linguistic queries, RefoMed-EN, definition-type questions, knowledge frequency  

<br /><br />Summary:  
This study investigates the performance of Large Language Models (LLMs) beyond their typical strength in providing definition-type answers, focusing on their ability to handle diverse linguistic queries such as examples, paraphrases, and explanations. Using TrackList, a detailed linguistic and statistical analysis pipeline, the authors examine how pre-training data influences LLM responses. A new dataset, RefoMed-EN, was introduced, containing 6170 human-annotated English medical terms paired with various types of answers including definitions, denominations, exemplifications, explanations, and paraphrases. The study evaluates whether the frequency of concepts—either common (head) or rare (tail)—affects LLM performance. Quality assessment of LLM outputs employed syntactic and semantic similarity metrics, embedding analysis, and statistical correlations. Results show that LLMs perform best on definition-type queries but struggle with exemplification requests, achieving the lowest accuracy in those cases. Furthermore, the models tend to generate more paraphrased answers for frequent, popular knowledge and fewer paraphrases when handling rare or highly technical content, especially within expert-level texts. This exposes limitations in LLMs' adaptability across knowledge frequency and answer type diversity in specialized domains such as medicine. <div>
arXiv:2511.21006v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have proven efficient in giving definition-type answers to user input queries. While for humans giving various types of answers, such as examples and paraphrases, is an easy task, LLMs struggle to provide correct answers for other than definition-type queries. In this study, we evaluated this drop in performance using TrackList, a fine-grained linguistic and statistical analysis pipeline to investigate the impact of the pre-training data on LLMs answers to diverse linguistic queries. We also introduce RefoMed-EN, an English dataset consisting of 6170 human-annotated medical terms alongside their corresponding definitions, denominations, exemplifications, explanations, or paraphrases. We studied whether the high frequency of a concept (head) or low frequency (tail) impacts the language model's performance. We evaluated the quality of the LLM's output using syntactic and semantic similarity metrics, statistical correlations and embeddings. Results showed that the LLM's task performance for definition type questions is the highest, while for the exemplification type it is the lowest. Additionally, we showed that for definition-type questions, large language models are prone to paraphrase more on popular and frequent knowledge and less on tail and technical knowledge, especially in the expert texts.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Anchors in In-Context Learning: Why Small LLMs Cannot Flip Their Labels</title>
<link>https://arxiv.org/abs/2511.21038</link>
<guid>https://arxiv.org/abs/2511.21038</guid>
<content:encoded><![CDATA[
<div> Keywords: in-context learning, label semantics, large language models, prompt alignment, semantic override rate<br /><br />Summary:<br /><br />This paper investigates whether in-context learning (ICL) can change the inherent label semantics learned by large language models (LLMs) or if it only fine-tunes an existing semantic foundation. The authors frame LLMs as classifiers triggered by prompts and analyze their responses under two demonstration conditions: natural demonstrations, which provide correct label meanings, and inverted demonstrations, which systematically reverse label meanings. They introduce three alignment metrics—truth alignment, prior alignment, and prompt alignment—and propose a new measure called the semantic override rate, which assesses accuracy when label semantics are flipped. Experiments across eight classification tasks and eight open-source LLMs ranging from 1 to 12 billion parameters reveal a consistent pattern supporting the "semantic anchor" hypothesis. Under natural demonstrations, ICL improves accuracy but keeps strong alignment with the model’s pretrained priors, as most correct classifications are similar to zero-shot predictions, even when priors are weak. With inverted demonstrations, models fail to form meaningful anti-semantic classifiers; attempts to increase prompt alignment come at the cost of accuracy, and semantic override rates stay at zero in few-shot settings. The study concludes that ICL primarily modifies input representation along stable semantic directions from pre-training, highlighting the fundamental limitations of few-shot prompting in overriding label semantics at these model scales and suggesting that stronger interventions are necessary for semantic remapping. <div>
arXiv:2511.21038v1 Announce Type: new 
Abstract: Can in-context learning (ICL) override pre-trained label semantics, or does it merely refine an existing semantic backbone? We address this question by treating LLMs as prompt-induced classifiers and contrasting their behavior under \emph{natural} demonstrations (with correct labels) and \emph{inverted} demonstrations (systematically flipping label meanings). We decompose ICL behavior into three alignment metrics (truth, prior, and prompt alignment) and introduce a semantic override rate, defined as correctness under flipped semantics. Across eight classification tasks and eight open-source LLMs (1--12B parameters), we find consistent evidence for a semantic anchor view. With natural demonstrations, ICL improves accuracy while maintaining strong prior alignment; most correct predictions coincide with zero-shot behavior, even when the prior is weak. With inverted demonstrations, models cannot learn coherent anti-semantic classifiers: prompt alignment increases only by sacrificing accuracy, and semantic override rates remain exactly zero in our few-shot 1--12B setting. Rather than flexibly remapping label meanings, ICL primarily adjusts how inputs project onto stable semantic directions learned during pre-training, clarifying fundamental limits of few-shot prompting and suggesting that overriding label semantics at these scales requires interventions beyond ICL. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/semantic-anchors-icl.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context-Aware Pragmatic Metacognitive Prompting for Sarcasm Detection</title>
<link>https://arxiv.org/abs/2511.21066</link>
<guid>https://arxiv.org/abs/2511.21066</guid>
<content:encoded><![CDATA[
<div> Keywords: sarcasm detection, large language models, retrieval-aware approach, pragmatic metacognitive prompting, contextual information<br /><br />Summary:<br /><br />Detecting sarcasm in natural language remains difficult despite advances in neural networks and the use of pre-trained language models (PLMs) and large language models (LLMs). The complexity arises due to linguistic diversity, cultural variations, and the challenge of interpreting words or tokens that require extra context. This paper builds upon the Pragmatic Metacognitive Prompting (PMP) method by introducing a retrieval-aware approach that integrates retrieved contextual information for each input text to enhance sarcasm detection. The approach uses two complementary strategies: (1) non-parametric knowledge via web-based retrieval to supplement background information missing in the model, and (2) self-knowledge awareness by eliciting the model’s internal knowledge. Evaluation was performed on three datasets—Twitter Indonesia Sarcastic, SemEval-2018 Task 3, and MUStARD. Non-parametric retrieval improved macro-F1 scores by 9.87% on the Twitter Indonesia dataset. Self-knowledge retrieval showed improvements of 3.29% on SemEval and 4.08% on MUStARD datasets. The study emphasizes the critical role of contextual information, especially culturally specific slang and references, in improving LLM performance on sarcasm detection. Future work will involve optimizing retrieval methods and analyzing the impact of retrieval quality on model effectiveness. The code is openly available for replication and further research. <div>
arXiv:2511.21066v1 Announce Type: new 
Abstract: Detecting sarcasm remains a challenging task in the areas of Natural Language Processing (NLP) despite recent advances in neural network approaches. Currently, Pre-trained Language Models (PLMs) and Large Language Models (LLMs) are the preferred approach for sarcasm detection. However, the complexity of sarcastic text, combined with linguistic diversity and cultural variation across communities, has made the task more difficult even for PLMs and LLMs. Beyond that, those models also exhibit unreliable detection of words or tokens that require extra grounding for analysis. Building on a state-of-the-art prompting method in LLMs for sarcasm detection called Pragmatic Metacognitive Prompting (PMP), we introduce a retrieval-aware approach that incorporates retrieved contextual information for each target text. Our pipeline explores two complementary ways to provide context: adding non-parametric knowledge using web-based retrieval when the model lacks necessary background, and eliciting the model's own internal knowledge for a self-knowledge awareness strategy. We evaluated our approach with three datasets, such as Twitter Indonesia Sarcastic, SemEval-2018 Task 3, and MUStARD. Non-parametric retrieval resulted in a significant 9.87% macro-F1 improvement on Twitter Indonesia Sarcastic compared to the original PMP method. Self-knowledge retrieval improves macro-F1 by 3.29% on Semeval and by 4.08% on MUStARD. These findings highlight the importance of context in enhancing LLMs performance in sarcasm detection task, particularly the involvement of culturally specific slang, references, or unknown terms to the LLMs. Future work will focus on optimizing the retrieval of relevant contextual information and examining how retrieval quality affects performance. The experiment code is available at: https://github.com/wllchrst/sarcasm-detection_pmp_knowledge-base.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Burmese News Classification with Kolmogorov-Arnold Network Head Fine-tuning</title>
<link>https://arxiv.org/abs/2511.21081</link>
<guid>https://arxiv.org/abs/2511.21081</guid>
<content:encoded><![CDATA[
<div> Keywords: low-resource languages, classification heads, Kolmogorov-Arnold Networks, EfficientKAN, transformer embeddings  

<br /><br />Summary:  
This paper addresses classification tasks in low-resource languages such as Burmese, where typical practice involves fine-tuning only the final classification layer and keeping the pre-trained encoder weights frozen. Commonly used classification heads are Multi-Layer Perceptrons (MLPs), but MLPs have limitations due to their fixed non-linearity and computational cost. The study proposes and examines Kolmogorov-Arnold Networks (KANs) as alternative classification heads. Three variants of KANs are evaluated: FourierKAN (Fourier-based), EfficientKAN (Spline-based), and FasterKAN (Grid-based). These heads are tested on different types of embeddings, including traditional TF-IDF, fastText embeddings, and multilingual transformers like mBERT and Distil-mBERT. Experimental results demonstrate that KAN-based heads perform competitively or better than MLPs in classification tasks. Specifically, EfficientKAN with fastText embeddings achieved the highest F1-score of 0.928. FasterKAN showed the best balance between computational speed and accuracy. When applied to transformer-based embeddings, EfficientKAN matched or slightly outperformed MLPs, achieving 0.917 F1 with mBERT. The findings suggest that KANs are both expressive and efficient alternatives to MLPs for classification in low-resource language scenarios. <div>
arXiv:2511.21081v1 Announce Type: new 
Abstract: In low-resource languages like Burmese, classification tasks often fine-tune only the final classification layer, keeping pre-trained encoder weights frozen. While Multi-Layer Perceptrons (MLPs) are commonly used, their fixed non-linearity can limit expressiveness and increase computational cost. This work explores Kolmogorov-Arnold Networks (KANs) as alternative classification heads, evaluating Fourier-based FourierKAN, Spline-based EfficientKAN, and Grid-based FasterKAN-across diverse embeddings including TF-IDF, fastText, and multilingual transformers (mBERT, Distil-mBERT). Experimental results show that KAN-based heads are competitive with or superior to MLPs. EfficientKAN with fastText achieved the highest F1-score (0.928), while FasterKAN offered the best trade-off between speed and accuracy. On transformer embeddings, EfficientKAN matched or slightly outperformed MLPs with mBERT (0.917 F1). These findings highlight KANs as expressive, efficient alternatives to MLPs for low-resource language classification.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Orthographic Constraint Satisfaction and Human Difficulty Alignment in Large Language Models</title>
<link>https://arxiv.org/abs/2511.21086</link>
<guid>https://arxiv.org/abs/2511.21086</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, orthographic constraints, cross-architecture evaluation, constraint satisfaction, model calibration

<br /><br />Summary:  
1. This study evaluates 28 configurations across three large language model families—Qwen3, Claude Haiku-4.5, and GPT-5-mini—on 58 word puzzles emphasizing character-level orthographic constraint satisfaction.  
2. Results show that differences in model architecture cause much larger performance gaps (2.0-2.2 times; F1 scores 0.761 vs. 0.343) than increasing model size within the same family, which yields an 83% performance gain from eightfold parameter scaling.  
3. This suggests that satisfying hard orthographic constraints may require architectural features or specialized training objectives rather than simply scaling up parameters or compute.  
4. Sensitivity to the "thinking budget" (computational effort) is heterogeneous: high-capacity models improve substantially (+0.102 to +0.136 F1 gain), while mid-sized models reach saturation or even decline in performance, indicating inconsistent returns on compute resources.  
5. Using human difficulty ratings from 10,000 solvers per puzzle, all models show modest but consistent calibration correlations (r=0.24–0.38). However, systematic failures occur on common words with unusual orthographies (e.g., “data”, “poop”, “loll”), where human success rates are high (86–95%) but model miss rates range from 89–96%.  
6. These failures reveal models’ reliance on distributional plausibility, causing them to penalize orthographically atypical but constraint-valid patterns. This highlights a need for innovations in model architecture beyond scaling parameters or compute budgets to better handle orthographic constraints. <div>
arXiv:2511.21086v1 Announce Type: new 
Abstract: Large language models must satisfy hard orthographic constraints during controlled text generation, yet systematic cross-architecture evaluation remains limited. We evaluate 28 configurations spanning three model families (Qwen3, Claude Haiku-4.5, GPT-5-mini) on 58 word puzzles requiring character-level constraint satisfaction. Architectural differences produce substantially larger performance gaps (2.0-2.2x, F1=0.761 vs. 0.343) than parameter scaling within families (83% gain from eightfold scaling), suggesting that constraint satisfaction may require specialized architectural features or training objectives beyond standard language model scaling. Thinking budget sensitivity proves heterogeneous: high-capacity models show strong returns (+0.102 to +0.136 F1), while mid-sized variants saturate or degrade. These patterns are inconsistent with uniform compute benefits. Using difficulty ratings from 10,000 human solvers per puzzle, we establish modest but consistent calibration (r=0.24-0.38) across all families, yet identify systematic failures on common words with unusual orthography ("data", "poop", "loll": 86-95% human success, 89-96% model miss rate). These failures reveal over-reliance on distributional plausibility that penalizes orthographically atypical but constraint-valid patterns, suggesting architectural innovations may be required beyond simply scaling parameters or computational budgets.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ASR Error Correction in Low-Resource Burmese with Alignment-Enhanced Transformers using Phonetic Features</title>
<link>https://arxiv.org/abs/2511.21088</link>
<guid>https://arxiv.org/abs/2511.21088</guid>
<content:encoded><![CDATA[
<div> Burmese, ASR error correction, Transformer, low-resource, feature integration<br /><br />Summary:<br /><br />This paper focuses on automatic speech recognition (ASR) error correction specifically for the low-resource Burmese language using sequence-to-sequence Transformer models. To enhance the performance of ASR systems, the study explores different feature integration strategies, including the use of International Phonetic Alphabet (IPA) representations and alignment information. This research is the first to address ASR error correction tailored for Burmese, filling a notable gap in the field. The authors evaluate five distinct ASR backbone models to benchmark the effectiveness of their proposed ASR error correction (AEC) approaches. Their findings demonstrate consistent improvements in both word-level and character-level accuracy across all baseline outputs. Notably, the AEC model that combines IPA and alignment features achieves significant error reduction by lowering the average Word Error Rate (WER) from 51.56 to 39.82 before data augmentation and from 51.56 to 43.59 after augmentation. Additionally, this approach improves the chrF++ metric from 0.5864 to 0.627, reflecting enhanced output quality. Overall, the results underline the robustness of the AEC methods and emphasize the critical role of carefully designed features for refining ASR outputs, especially in low-resource language scenarios such as Burmese. <div>
arXiv:2511.21088v1 Announce Type: new 
Abstract: This paper investigates sequence-to-sequence Transformer models for automatic speech recognition (ASR) error correction in low-resource Burmese, focusing on different feature integration strategies including IPA and alignment information. To our knowledge, this is the first study addressing ASR error correction specifically for Burmese. We evaluate five ASR backbones and show that our ASR Error Correction (AEC) approaches consistently improve word- and character-level accuracy over baseline outputs. The proposed AEC model, combining IPA and alignment features, reduced the average WER of ASR models from 51.56 to 39.82 before augmentation (and 51.56 to 43.59 after augmentation) and improving chrF++ scores from 0.5864 to 0.627, demonstrating consistent gains over the baseline ASR outputs without AEC. Our results highlight the robustness of AEC and the importance of feature design for improving ASR outputs in low-resource settings.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MortgageLLM: Domain-Adaptive Pretraining with Residual Instruction Transfer, Alignment Tuning, and Task-Specific Routing</title>
<link>https://arxiv.org/abs/2511.21101</link>
<guid>https://arxiv.org/abs/2511.21101</guid>
<content:encoded><![CDATA[
<div> Keywords: MortgageLLM, dual-track specialization, domain adaptation, instruction residual, mortgage finance  

<br /><br />Summary: Large Language Models (LLMs) excel in general domains but require domain-specific adaptation to perform well in specialized sectors like mortgage finance while maintaining instruction-following ability. The paper introduces MortgageLLM, a domain-specific LLM developed using a dual-track specialization framework based on the LLaMA-3.1-8B base model. This dual-expert approach avoids the trade-offs seen in single multi-task models where supervised fine-tuning (SFT) for structured tasks compromises conversational fidelity typically maintained by direct preference optimization (DPO). MortgageLLM creates two experts: one focused on conversational Q&amp;A and another on structured tasks such as classification and summarization. To preserve instruction-following without additional supervised fine-tuning, the model applies an instruction residual technique post-domain adaptation. Contributions include the first use of this residual technique in the mortgage finance domain, the novel dual-expert architecture, and an intelligent task routing mechanism that uses few-shot classification performed by one expert to direct tasks efficiently. Evaluation on domain-specific benchmarks shows significant improvements over baseline LLaMA-3.1-8B-Instruct, with higher scores in summarization, Q&amp;A, classification, and semantic similarity metrics, demonstrating the effectiveness of this specialized dual-expert model. <div>
arXiv:2511.21101v1 Announce Type: new 
Abstract: Large Language Models (LLMs) demonstrate exceptional capabilities across general domains, yet their application to specialized sectors such as mortgage finance requires domain-specific knowledge augmentation while preserving instruction-following fidelity. We present MortgageLLM, a novel domain-specific large language model that addresses this dual challenge. It is developed using a dual-track specialization framework from a single base model (LLaMA-3.1-8B). We opted for this dual-expert approach as a single multi-task model suffers from performance trade-offs, where optimizing for structured tasks (via SFT) degrades conversational fidelity (via DPO). Our dual-track method solves this by creating two specialists, allowing each to be optimally trained for its distinct capability. Our approach applies the instruction residual technique to restore instruction-following capabilities post-domain adaptation without supervised fine-tuning. We contribute: (1) application of this residual technique to the highly specialized mortgage finance domain; (2) a dual-expert architecture combining a conversational Q&amp;A model and a structured task model for classification and summarization; and (3) an intelligent task routing mechanism using few-shot classification performed by one of the expert models itself. We validate our approach on domain-specific benchmarks, where our final model (MLM v2) significantly outperforms the base LLaMA-3.1-8B-Instruct, achieving an LLM-as-a-Judge summarization score of 4.58 (vs. 3.99), a Q&amp;A score of 4.09 (vs. 4.0), and a classification score of 2.6 (vs. 1.2). On semantic similarity, our model achieved a BERTScore of 0.77 for summarization (vs. 0.74), 0.68 for Q&amp;A (vs. 0.58), and 0.75 for classification (vs. 0.73), substantially outperforming baseline approaches.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Guided Defense: Adaptive Safety Alignment for Reasoning Models via Synthesized Guidelines</title>
<link>https://arxiv.org/abs/2511.21214</link>
<guid>https://arxiv.org/abs/2511.21214</guid>
<content:encoded><![CDATA[
<div> Keywords: adaptive safety alignment, adversarial jailbreak prompts, model robustness, supervised fine-tuning, direct preference optimization<br /><br />Summary: Reasoning models have shown strong capabilities in tackling complex tasks but remain vulnerable to adversarial jailbreak prompts that bypass safety mechanisms and produce harmful content. To address this, the paper proposes the Synthesized Guideline-based Adaptive Safety Alignment (SGASA) framework designed to enable models to autonomously reinforce defenses against such adversarial inputs. SGASA operates in two main stages: Data Pre-synthesis, which creates safety guidelines alongside augmented prompts intended to expose and mitigate vulnerabilities; and Alignment Fine-tuning, which integrates these guidelines using Supervised Fine-tuning (SFT) and Direct Preference Optimization (DPO). This dual-stage approach allows the model to internalize safety rules effectively while maintaining responsiveness to benign user requests, minimizing unnecessary refusal rates. Extensive experiments conducted on multiple datasets validate the efficacy of SGASA in significantly enhancing safety and robustness. The framework demonstrates adaptive and scalable safety alignment that can continuously evolve as new adversarial prompts emerge, marking an important advance in safeguarding language model deployments against harmful content generation triggered by deceptive inputs. <div>
arXiv:2511.21214v1 Announce Type: new 
Abstract: Reasoning models have demonstrated remarkable capabilities in complex reasoning tasks. However, ensuring their safety against adversarial jailbreak prompts remains a critical challenge. Due to the covert and deceptive nature of such prompts, they can often evade built-in safety mechanisms and lead to the generation of harmful content. This underscores the need for an adaptive safety alignment approach that enables models to autonomously reinforce their defenses in response to adversarial inputs. This paper introduces the Synthesized Guideline-based Adaptive Safety Alignment (SGASA) framework, which internalizes model-generated safety guidelines to strengthen models' ability to enhance robustness against harmful adversarial prompts while minimizing unnecessary refusals of benign requests. SGASA consists of two key stages: Data Pre-synthesis, which generates safety guidelines and augmented prompts; and Alignment Fine-tuning, which leverages Supervised Fine-tuning (SFT) and Direct Preference Optimization (DPO) to embed these guidelines into the model. Extensive experiments across multiple datasets demonstrate that SGASA significantly improves model safety, validating its adaptive and scalable effectiveness.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Finetuing LLMs on Small Human Samples Increase Heterogeneity, Alignment, and Belief-Action Coherence?</title>
<link>https://arxiv.org/abs/2511.21218</link>
<guid>https://arxiv.org/abs/2511.21218</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, fine-tuning, survey research, behavioral experiments, belief-action coherence  

<br /><br />Summary:  
This study addresses whether fine-tuning large language models (LLMs) on small subsets of human survey data can improve their use as substitutes for human participants in research. It builds on prior concerns that LLMs often do not replicate human behaviors accurately, showing limited diversity and misalignment, especially among minority groups. The researchers conducted a behavioral experiment focused on information disclosure to compare responses from humans and LLMs across several criteria. They assessed distributional divergence, subgroup alignment, coherence between stated beliefs and actions, and the ability to recover key regression coefficients from the original study. Fine-tuning the LLMs with small human data samples resulted in notable improvements in behavioral heterogeneity, alignment with human subgroups, and belief-action coherence compared to the base LLM. Despite these gains, the fine-tuned models still failed to replicate the regression coefficients accurately, signaling that LLM-generated data cannot reliably replace human participants for formal inferential statistical analysis. Overall, while fine-tuning helps LLMs better simulate human variability and behavior, significant limitations remain when it comes to substituting them fully in experimental and survey research contexts. <div>
arXiv:2511.21218v1 Announce Type: new 
Abstract: There is ongoing debate about whether large language models (LLMs) can serve as substitutes for human participants in survey and experimental research. While recent work in fields such as marketing and psychology has explored the potential of LLM-based simulation, a growing body of evidence cautions against this practice: LLMs often fail to align with real human behavior, exhibiting limited diversity, systematic misalignment for minority subgroups, insufficient within-group variance, and discrepancies between stated beliefs and actions. This study examines an important and distinct question in this domain: whether fine-tuning on a small subset of human survey data, such as that obtainable from a pilot study, can mitigate these issues and yield realistic simulated outcomes. Using a behavioral experiment on information disclosure, we compare human and LLM-generated responses across multiple dimensions, including distributional divergence, subgroup alignment, belief-action coherence, and the recovery of regression coefficients. We find that fine-tuning on small human samples substantially improves heterogeneity, alignment, and belief-action coherence relative to the base model. However, even the best-performing fine-tuned models fail to reproduce the regression coefficients of the original study, suggesting that LLM-generated data remain unsuitable for replacing human participants in formal inferential analyses.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Developing an Open Conversational Speech Corpus for the Isan Language</title>
<link>https://arxiv.org/abs/2511.21229</link>
<guid>https://arxiv.org/abs/2511.21229</guid>
<content:encoded><![CDATA[
<div> Keywords: Isan language, conversational speech dataset, transcription protocols, code-switching, underrepresented languages  

<br /><br />Summary:  
This paper presents the creation of the first open conversational speech dataset for the Isan language, a major regional dialect in Thailand. Unlike prior corpora that primarily feature read or scripted speech, the dataset captures natural, spontaneous speech, including colloquialisms, prosodic variations, disfluencies, and frequent code-switching with Central Thai. A significant challenge arose from the lack of a standardized orthography for Isan, since its lexical tones differ from those of Thai, leading to diverse writing practices. This variation complicated the development of consistent transcription guidelines that are both linguistically authentic and computationally practical. To overcome this, the authors designed transcription protocols that strike a balance between accurate representation of the language and the needs of computational processing. By releasing this dataset as an open resource, the work supports inclusive AI development and research on languages that are typically underrepresented. Additionally, it provides crucial groundwork for addressing linguistic and technical challenges associated with modeling conversational speech in Isan, which can help drive future advancements in speech technology and language preservation for the community. <div>
arXiv:2511.21229v1 Announce Type: new 
Abstract: This paper introduces the development of the first open conversational speech dataset for the Isan language, the most widely spoken regional dialect in Thailand. Unlike existing speech corpora that are primarily based on read or scripted speech, this dataset consists of natural speech, thereby capturing authentic linguistic phenomena such as colloquials, spontaneous prosody, disfluencies, and frequent code-switching with central Thai. A key challenge in building this resource lies in the lack of a standardized orthography for Isan. Current writing practices vary considerably, due to the different lexical tones between Thai and Isan. This variability complicates the design of transcription guidelines and poses questions regarding consistency, usability, and linguistic authenticity. To address these issues, we establish practical transcription protocols that balance the need for representational accuracy with the requirements of computational processing. By releasing this dataset as an open resource, we aim to contribute to inclusive AI development, support research on underrepresented languages, and provide a basis for addressing the linguistic and technical challenges inherent in modeling conversational speech.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PEFT-Bench: A Parameter-Efficient Fine-Tuning Methods Benchmark</title>
<link>https://arxiv.org/abs/2511.21285</link>
<guid>https://arxiv.org/abs/2511.21285</guid>
<content:encoded><![CDATA[
<div> Parameter-efficient fine-tuning, Large Language Models, PEFT-Bench, PSCP metric, NLP benchmark<br /><br />Summary:<br /><br />This paper addresses the high computational and environmental costs associated with fine-tuning large-scale Large Language Models (LLMs) by focusing on Parameter-Efficient Fine-Tuning (PEFT) methods, which reduce the number of trainable parameters while maintaining task performance. Despite their promise, the evaluation of PEFT methods remains limited and lacks reproducibility due to inconsistent benchmarking across models and datasets. To overcome these challenges, the authors introduce PEFT-Bench, a comprehensive and unified end-to-end benchmark designed to evaluate diverse PEFT methods specifically on autoregressive LLMs. PEFT-Bench is demonstrated across 27 natural language processing datasets and incorporates 6 different PEFT methods, providing a broad and systematic comparison. Furthermore, to better reflect practical considerations, the authors develop the PEFT Soft Score Penalties (PSCP) metric, which evaluates PEFT approaches by factoring in trainable parameter count, inference speed, and training memory usage. This integrated framework offers a more holistic and practical assessment of PEFT methods, empowering researchers and practitioners to better balance performance and efficiency in LLM fine-tuning tasks. <div>
arXiv:2511.21285v1 Announce Type: new 
Abstract: Despite the state-of-the-art performance of Large Language Models (LLMs) achieved on many tasks, their massive scale often leads to high computational and environmental costs, limiting their accessibility. Parameter-efficient fine-tuning (PEFT) methods address this challenge by reducing the number of trainable parameters while maintaining strong downstream performance. Despite the increased development in PEFT methods, current evaluations remain limited (in terms of evaluated models and datasets) and difficult to reproduce. To bridge this gap, we introduce PEFT-Bench, a unified end-to-end benchmark for evaluating diverse PEFT methods on autoregressive LLMs. We demonstrate its usage across 27 NLP datasets and 6 PEFT methods. To account for different PEFT training and inference factors, we also introduce the PEFT Soft Score Penalties (PSCP) metric, which takes trainable parameters, inference speed, and training memory usage into account.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emergent Lexical Semantics in Neural Language Models: Testing Martin's Law on LLM-Generated Text</title>
<link>https://arxiv.org/abs/2511.21334</link>
<guid>https://arxiv.org/abs/2511.21334</guid>
<content:encoded><![CDATA[
<div> Keywords: Martin's Law, neural language models, word frequency, polysemy, semantic collapse<br /><br />Summary:<br /><br />1. This study presents the first systematic investigation of Martin's Law, which describes the empirical relationship between word frequency and polysemy, in text generated by neural language models during their training process. <br />2. The authors operationalize word senses using DBSCAN clustering on contextualized embeddings and analyze four Pythia models ranging from 70 million to 1 billion parameters across 30 training checkpoints.<br />3. Their results reveal a non-monotonic developmental trajectory of Martin's Law: it emerges around checkpoint 100, peaks with a correlation greater than 0.6 at checkpoint 104, and then degrades after checkpoint 105.<br />4. Smaller models (70M, 160M parameters) experience catastrophic semantic collapse at later checkpoints, whereas larger models (410M, 1B parameters) show a more gradual, less severe degradation.<br />5. The frequency-specificity trade-off remains stable around a correlation of -0.3 across all models and checkpoints.<br />6. These findings indicate that neural language models do not steadily improve in compliance with linguistic regularities throughout training but instead reach an optimal semantic window before experiencing decline.<br />7. The work establishes a novel methodology for evaluating emergent linguistic structures within neural language models, offering insights into the semantic development and limitations of such models during training. <div>
arXiv:2511.21334v1 Announce Type: new 
Abstract: We present the first systematic investigation of Martin's Law - the empirical relationship between word frequency and polysemy - in text generated by neural language models during training. Using DBSCAN clustering of contextualized embeddings as an operationalization of word senses, we analyze four Pythia models (70M-1B parameters) across 30 training checkpoints. Our results reveal a non-monotonic developmental trajectory: Martin's Law emerges around checkpoint 100, reaches peak correlation (r > 0.6) at checkpoint 104, then degrades by checkpoint 105. Smaller models (70M, 160M) experience catastrophic semantic collapse at late checkpoints, while larger models (410M, 1B) show graceful degradation. The frequency-specificity trade-off remains stable (r $\approx$ -0.3) across all models. These findings suggest that compliance with linguistic regularities in LLM-generated text is not monotonically increasing with training, but instead follows a balanced trajectory with an optimal semantic window. This work establishes a novel methodology for evaluating emergent linguistic structure in neural language models.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Introspective Behavior: Fine-Tuning Induces Reliable Internal State Detection in a 7B Model</title>
<link>https://arxiv.org/abs/2511.21399</link>
<guid>https://arxiv.org/abs/2511.21399</guid>
<content:encoded><![CDATA[
<div> Keywords: introspective awareness, language models, fine-tuning, activation pattern detection, AI transparency  

<br /><br />Summary:  
Lindsey (2025) explored introspective awareness in language models using four experiments, revealing that models could detect injected activation patterns but only about 20% reliably. This study focuses on Lindsey’s first experiment involving self-report of injected "thoughts" and investigates whether this capability can be enhanced through direct training rather than waiting for spontaneous emergence. By fine-tuning a 7 billion parameter language model on transient single-token injections, the researchers improved detection accuracy from near failure (0.4%) to high reliability (85%) on held-out concepts with no false positives. The trained model successfully detects fleeting "thoughts" injected at a single token and can maintain and verbalize this semantic information during subsequent output generation steps. The model satisfies three criteria outlined by Lindsey: accuracy (correct identification), grounding (zero false positives out of 60 trials), and internality (detection happens before verbalization). Additionally, the model generalizes well to unseen concept vectors, demonstrating transferable skill learning rather than memorization, although this does not fully confirm metacognitive representation as defined by Lindsey. These findings address Lindsey’s open question about whether training can reduce variability across models and suggest a promising method to embed introspective capabilities directly, enhancing AI transparency. <div>
arXiv:2511.21399v1 Announce Type: new 
Abstract: Lindsey (2025) investigates introspective awareness in language models through four experiments, finding that models can sometimes detect and identify injected activation patterns -- but unreliably (~20% success in the best model). We focus on the first of these experiments -- self-report of injected "thoughts" -- and ask whether this capability can be directly trained rather than waiting for emergence. Through fine-tuning on transient single-token injections, we transform a 7B parameter model from near-complete failure (0.4% accuracy, 6.7% false positive rate) to reliable detection (85% accuracy on held-out concepts at {\alpha}=40, 0% false positives). Our model detects fleeting "thoughts" injected at a single token position, retains that information, and reports the semantic content across subsequent generation steps. On this task, our trained model satisfies three of Lindsey's criteria: accuracy (correct identification), grounding (0/60 false positives), and internality (detection precedes verbalization). Generalization to unseen concept vectors (7.5pp gap) demonstrates the model learns a transferable skill rather than memorizing specific vectors, though this does not establish metacognitive representation in Lindsey's sense. These results address an open question raised by Lindsey: whether "training for introspection would help eliminate cross-model differences." We show that at least one component of introspective behavior can be directly induced, offering a pathway to built-in AI transparency.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can LLMs extract human-like fine-grained evidence for evidence-based fact-checking?</title>
<link>https://arxiv.org/abs/2511.21401</link>
<guid>https://arxiv.org/abs/2511.21401</guid>
<content:encoded><![CDATA[
<div> Keywords: misinformation, evidence extraction, Czech and Slovak claims, large language models, dataset evaluation<br /><br />Summary:<br />The paper addresses the challenge of misinformation spreading through user comments on online news articles, emphasizing the need to detect factually incorrect information effectively. It concentrates specifically on fine-grained evidence extraction to support or refute claims in Czech and Slovak languages. The authors introduce a newly created dataset containing two-way annotated fine-grained evidence, developed by paid annotators to ensure quality and reliability. They evaluate several large language models (LLMs) on this dataset to determine how well the models align with human annotations. The study reveals that many LLMs struggle to reproduce evidence verbatim from source texts, resulting in invalid or inaccurate outputs. Despite its relatively smaller size, the llama3.1:8b model achieves a high proportion of correct outputs, illustrating that larger model size does not always equate to better performance. On the other hand, the gpt-oss-120b model underperforms despite having significantly more parameters. Additionally, the models qwen3:14b, deepseek-r1:32b, and gpt-oss:20b demonstrate a favorable balance between model size and alignment with human annotations, indicating promising efficiency in evidence extraction tasks. This work contributes valuable resources and insights for misinformation detection in underrepresented languages. <div>
arXiv:2511.21401v1 Announce Type: new 
Abstract: Misinformation frequently spreads in user comments under online news articles, highlighting the need for effective methods to detect factually incorrect information. To strongly support or refute claims extracted from such comments, it is necessary to identify relevant documents and pinpoint the exact text spans that justify or contradict each claim. This paper focuses on the latter task -- fine-grained evidence extraction for Czech and Slovak claims. We create new dataset, containing two-way annotated fine-grained evidence created by paid annotators. We evaluate large language models (LLMs) on this dataset to assess their alignment with human annotations. The results reveal that LLMs often fail to copy evidence verbatim from the source text, leading to invalid outputs. Error-rate analysis shows that the {llama3.1:8b model achieves a high proportion of correct outputs despite its relatively small size, while the gpt-oss-120b model underperforms despite having many more parameters. Furthermore, the models qwen3:14b, deepseek-r1:32b, and gpt-oss:20b demonstrate an effective balance between model size and alignment with human annotations.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text-to-SQL as Dual-State Reasoning: Integrating Adaptive Context and Progressive Generation</title>
<link>https://arxiv.org/abs/2511.21402</link>
<guid>https://arxiv.org/abs/2511.21402</guid>
<content:encoded><![CDATA[
<div> Keywords: Text-to-SQL, Dual-State Reasoning, Large Language Models, schema refinement, SQL synthesis<br /><br />Summary:<br /><br />This paper presents DSR-SQL, a novel Dual-State Reasoning framework designed to improve Text-to-SQL generation, particularly for complex enterprise databases. 1) The framework addresses key challenges faced by existing Chain-of-Thought approaches, such as limited context capacity, unreliable schema linking, and weak database semantic grounding. 2) DSR-SQL models the Text-to-SQL task as an interaction between two states: an adaptive context state that refines and selects relevant schema structures to build a compact and semantically accurate environment, and a progressive generation state that formalizes SQL query synthesis as feedback-guided state transitions. 3) This design enables self-correction during query generation and better alignment with the user’s intent without the need for additional post-training or in-context examples. 4) Experimental results demonstrate DSR-SQL’s competitive performance, achieving 35.28% execution accuracy on the challenging Spider 2.0-Snow benchmark and 68.32% on the BIRD development set. 5) The authors plan to release their implementation openly, facilitating further research and application in enterprise-level Text-to-SQL problems. <div>
arXiv:2511.21402v1 Announce Type: new 
Abstract: Recent divide-and-conquer reasoning approaches, particularly those based on Chain-of-Thought (CoT), have substantially improved the Text-to-SQL capabilities of Large Language Models (LLMs). However, when applied to complex enterprise databases, such methods struggle to maintain coherent reasoning due to limited context capacity, unreliable schema linking, and weak grounding in database semantics. To overcome these issues, we introduce DSR-SQL, a \textbf{D}ual-\textbf{S}tate \textbf{R}easoning framework that models Text-to-SQL as an interaction between an adaptive context state and a progressive generation state. The first constructs a compact, semantically faithful environment by refining large schemas and selecting relevant structures, while the second formalizes SQL synthesis as feedback-guided state transitions, enabling the model to self-correct and align with user intent. Without any post-training or in-context examples, DSR-SQL achieves competitive performance, reaching 35.28\% execution accuracy on Spider 2.0-Snow and 68.32\% on BIRD development set. Our implementation will be open-sourced at: https://github.com/DMIRLAB-Group/DSR-SQL.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Odin: Oriented Dual-module Integration for Text-rich Network Representation Learning</title>
<link>https://arxiv.org/abs/2511.21416</link>
<guid>https://arxiv.org/abs/2511.21416</guid>
<content:encoded><![CDATA[
<div> Keywords: text-attributed graphs, Transformers, graph structure integration, over-smoothing, Light Odin  

<br /><br />Summary:  
This paper presents Odin (Oriented Dual-module INtegration), a novel model designed for text-attributed graphs that effectively combines deep textual understanding with structural reasoning. Existing approaches either utilize Graph Neural Networks (GNNs), which suffer from over-smoothing and dependence on multi-hop diffusion, or Transformers that ignore graph topology by treating nodes as independent sequences. Odin overcomes these limitations by injecting graph structure into Transformers at selected layers through an oriented dual-module mechanism, allowing multi-hop structural information to be integrated in alignment with the semantic hierarchy of the model. Unlike message-passing GNNs, Odin aggregates structural information on the global [CLS] token, thereby avoiding over-smoothing and decoupling from neighborhood size or graph topology. The authors further prove that Odin is strictly more expressive than both pure Transformers and GNNs. To address efficiency in large-scale or resource-constrained scenarios, they introduce Light Odin, a lightweight variant that maintains the layered structural abstraction but reduces training and inference costs. Experimental results on multiple text-rich graph benchmarks demonstrate that Odin achieves state-of-the-art accuracy, while Light Odin offers a competitive balance between performance and computational efficiency. The work thus establishes a unified, hop-free framework for principled integration of structure and text. The source code is publicly available. <div>
arXiv:2511.21416v1 Announce Type: new 
Abstract: Text-attributed graphs require models to effectively combine strong textual understanding with structurally informed reasoning. Existing approaches either rely on GNNs--limited by over-smoothing and hop-dependent diffusion--or employ Transformers that overlook graph topology and treat nodes as isolated sequences. We propose Odin (Oriented Dual-module INtegration), a new architecture that injects graph structure into Transformers at selected depths through an oriented dual-module mechanism.Unlike message-passing GNNs, Odin does not rely on multi-hop diffusion; instead, multi-hop structures are integrated at specific Transformer layers, yielding low-, mid-, and high-level structural abstraction aligned with the model's semantic hierarchy. Because aggregation operates on the global [CLS] representation, Odin fundamentally avoids over-smoothing and decouples structural abstraction from neighborhood size or graph topology. We further establish that Odin's expressive power strictly contains that of both pure Transformers and GNNs.To make the design efficient in large-scale or low-resource settings, we introduce Light Odin, a lightweight variant that preserves the same layer-aligned structural abstraction for faster training and inference. Experiments on multiple text-rich graph benchmarks show that Odin achieves state-of-the-art accuracy, while Light Odin delivers competitive performance with significantly reduced computational cost. Together, Odin and Light Odin form a unified, hop-free framework for principled structure-text integration. The source code of this model has been released at https://github.com/hongkaifeng/Odin.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Systematic Study of Model Merging Techniques in Large Language Models</title>
<link>https://arxiv.org/abs/2511.21437</link>
<guid>https://arxiv.org/abs/2511.21437</guid>
<content:encoded><![CDATA[
<div> Model merging, large language models, fine-tuning, Task Arithmetic, performance evaluation<br /><br />Summary:<br /><br />This paper investigates the effectiveness of model merging techniques specifically applied to large language models (LLMs). The study systematically evaluates six state-of-the-art merging methods, including recently proposed subspace approaches, across four open-weight LLM architectures. Each base model is fine-tuned into twelve different checkpoints, resulting in a broad empirical scope tested on sixteen standardized LLM benchmarks. The evaluation criteria focus on two key aspects: the likelihood that the merged model surpasses the base model's performance and the relative improvement over the best individual fine-tuned checkpoint. Results reveal that among all methods tested, the simplest and oldest technique, Task Arithmetic, consistently produces reliable performance improvements in LLMs. In contrast, more complex interference-aware and subspace merging methods frequently cause significant degradation in performance when applied to these models. These findings suggest that merging strategies successful on smaller models and classifiers do not straightforwardly generalize to the realm of modern large language models. Consequently, the authors highlight the need for new merging algorithms tailored to LLMs as well as enhanced fine-tuning procedures that take merging into account. They plan to release their code after the paper is accepted, facilitating further research in this area. <div>
arXiv:2511.21437v1 Announce Type: new 
Abstract: Model merging combines multiple fine-tuned checkpoints into a single model without additional training, offering an attractive approach to reusing models and efficiently improving performance. However, it remains unclear whether the advantages reported for smaller models and classifiers generalize to LLMs. We present a large-scale, systematic evaluation of six state-of-the-art merging methods, including recent subspace methods, across four open-weight LLMs, twelve fine-tuned checkpoints per base model, and sixteen standard LLM benchmarks. Evaluating through standardized benchmarks, we measure both the probability that a merged model outperforms the base model and relative gains over the best individual checkpoint. Our results show that the oldest and simplest method, Task Arithmetic, is the only approach that reliably yields performance gains on LLMs. Other interference-aware and subspace merging methods typically result in significant performance drops. Our findings indicate that current merging techniques do not directly transfer to modern LLMs. This motivates the design of LLM-specific merging algorithms and merging-aware fine-tuning methods. Code will be released upon acceptance of this paper.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Ranking Neural Network for Long Document Readability Assessment</title>
<link>https://arxiv.org/abs/2511.21473</link>
<guid>https://arxiv.org/abs/2511.21473</guid>
<content:encoded><![CDATA[
<div> Keywords: readability assessment, deep learning, bidirectional mechanism, ordinal relationship, pairwise sorting algorithm<br /><br />Summary:<br /><br />1. This paper addresses the challenge of readability assessment, which is the task of evaluating the reading difficulty of a text. 2. Existing deep learning approaches often overlook two important factors: the length of the text and the ordinal relationship among readability labels. 3. The authors propose a novel bidirectional readability assessment mechanism that captures contextual information and identifies semantically rich regions within a text by predicting readability at the sentence level. 4. These sentence-level readability predictions are then aggregated to enhance the accuracy of the overall document-level readability classification. 5. To properly model the inherent ordinal nature of readability labels, a pairwise sorting algorithm based on label subtraction is introduced, effectively capturing relative differences between readability levels. 6. The model is evaluated on both Chinese and English datasets, demonstrating competitive performance and outperforming several baseline models in readability prediction tasks. 7. This study contributes a new methodology that combines fine-grained sentence-level analysis with ordinal modeling to improve the reliability and interpretability of automated readability assessment systems. <div>
arXiv:2511.21473v1 Announce Type: new 
Abstract: Readability assessment aims to evaluate the reading difficulty of a text. In recent years, while deep learning technology has been gradually applied to readability assessment, most approaches fail to consider either the length of the text or the ordinal relationship of readability labels. This paper proposes a bidirectional readability assessment mechanism that captures contextual information to identify regions with rich semantic information in the text, thereby predicting the readability level of individual sentences. These sentence-level labels are then used to assist in predicting the overall readability level of the document. Additionally, a pairwise sorting algorithm is introduced to model the ordinal relationship between readability levels through label subtraction. Experimental results on Chinese and English datasets demonstrate that the proposed model achieves competitive performance and outperforms other baseline models.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Voice, Bias, and Coreference: An Interpretability Study of Gender in Speech Translation</title>
<link>https://arxiv.org/abs/2511.21517</link>
<guid>https://arxiv.org/abs/2511.21517</guid>
<content:encoded><![CDATA[
<div> gender bias, speech translation, acoustic cues, internal language model, pronoun linking<br /><br />Summary:<br /><br />1. Speech conveys speaker-specific information like gender through acoustic cues such as pitch, which introduces modality-specific biases in speech translation (ST) systems. 2. When translating from English into languages with grammatical gender that depends on speaker reference, ST models may misgender speakers due to masculine defaults or assumptions based on vocal characteristics. 3. The study investigates how ST models assign gender to speaker-referring terms across three language pairs: English-Spanish, English-French, and English-Italian, focusing on the interplay between training data, internal language model (ILM) biases, and acoustic input. 4. Findings show that ST models do not just replicate gender associations from training data but develop a broader pattern of masculine bias internally. 5. Although the ILM demonstrates a strong preference for masculine forms, the overall model can override these biases by interpreting acoustic signals. 6. Using contrastive feature attribution on spectrograms, the study uncovers that the model with better gender accuracy uses an unexpected mechanism: it leverages first-person pronouns to connect gendered terms to the speaker and accesses gender information spread across the frequency spectrum rather than relying only on pitch. <div>
arXiv:2511.21517v1 Announce Type: new 
Abstract: Unlike text, speech conveys information about the speaker, such as gender, through acoustic cues like pitch. This gives rise to modality-specific bias concerns. For example, in speech translation (ST), when translating from languages with notional gender, such as English, into languages where gender-ambiguous terms referring to the speaker are assigned grammatical gender, the speaker's vocal characteristics may play a role in gender assignment. This risks misgendering speakers, whether through masculine defaults or vocal-based assumptions. Yet, how ST models make these decisions remains poorly understood. We investigate the mechanisms ST models use to assign gender to speaker-referring terms across three language pairs (en-es/fr/it), examining how training data patterns, internal language model (ILM) biases, and acoustic information interact. We find that models do not simply replicate term-specific gender associations from training data, but learn broader patterns of masculine prevalence. While the ILM exhibits strong masculine bias, models can override these preferences based on acoustic input. Using contrastive feature attribution on spectrograms, we reveal that the model with higher gender accuracy relies on a previously unknown mechanism: using first-person pronouns to link gendered terms back to the speaker, accessing gender information distributed across the frequency spectrum rather than concentrated in pitch.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bangla Sign Language Translation: Dataset Creation Challenges, Benchmarking and Prospects</title>
<link>https://arxiv.org/abs/2511.21533</link>
<guid>https://arxiv.org/abs/2511.21533</guid>
<content:encoded><![CDATA[
<div> Bangla Sign Language, Dataset, Translation, IsharaKhobor, AI Assistive Tools<br /><br />Summary:<br /><br />1. The paper addresses the lack of resources in Bangla Sign Language Translation (BdSLT), which has hindered progress in developing AI-based assistive technologies for the Bangla-speaking deaf and hard of hearing community.<br /><br />2. It introduces a novel and standard sentence-level dataset named IsharaKhobor along with two subsets, IsharaKhobor_small and IsharaKhobor_canonical_small, designed to facilitate research in the BdSLT domain.<br /><br />3. Challenges encountered during the dataset creation process are discussed, along with potential solutions to overcome these difficulties.<br /><br />4. The paper benchmarks different techniques using landmark-based raw embeddings and RQE (Relation-aware Query Embeddings) embeddings to evaluate the performance of models trained on these datasets.<br /><br />5. The authors conduct ablation studies focusing on vocabulary restrictions and canonicalization processes within the dataset, which resulted in the development of the smaller and canonicalized versions of IsharaKhobor.<br /><br />6. The dataset is made publicly available on Kaggle to encourage further research and development of AI-driven assistive solutions for the Bangla-sign language community.<br /><br />7. This work represents a significant step forward in creating comprehensive, standardized resources for low-resource sign languages, potentially improving accessibility for the deaf and hard of hearing population. <div>
arXiv:2511.21533v1 Announce Type: new 
Abstract: Bangla Sign Language Translation (BdSLT) has been severely constrained so far as the language itself is very low resource. Standard sentence level dataset creation for BdSLT is of immense importance for developing AI based assistive tools for deaf and hard of hearing people of Bangla speaking community. In this paper, we present a dataset, IsharaKhobor , and two subset of it for enabling research. We also present the challenges towards developing the dataset and present some way forward by benchmarking with landmark based raw and RQE embedding. We do some ablation on vocabulary restriction and canonicalization of the same within the dataset, which resulted in two more datasets, IsharaKhobor_small and IsharaKhobor_canonical_small. The dataset is publicly available at: www.kaggle.com/datasets/hasanssl/isharakhobor [1].
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RoParQ: Paraphrase-Aware Alignment of Large Language Models Towards Robustness to Paraphrased Questions</title>
<link>https://arxiv.org/abs/2511.21568</link>
<guid>https://arxiv.org/abs/2511.21568</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, paraphrase consistency, RoParQ benchmark, XParaCon metric, supervised fine-tuning<br /><br />Summary: Large Language Models (LLMs) demonstrate inconsistent behavior when answering paraphrased questions, revealing their dependence on superficial patterns instead of genuine semantic understanding. To tackle this issue, the authors introduce RoParQ, a benchmark designed to evaluate cross-paraphrase consistency in closed-book multiple-choice question answering tasks. RoParQ is constructed by generating paraphrases using proprietary models and selecting examples that cause inconsistent confidence responses from a judge model. Alongside RoParQ, the authors propose XParaCon, a novel evaluation metric that measures model robustness by calculating the standard deviation of accuracy across different paraphrased question variants. To improve model performance, they develop a reasoning-based, paraphrase-aware Supervised Fine-Tuning (SFT) approach aimed at aligning models toward semantic invariance rather than surface pattern memorization. Experimental results indicate that this fine-tuning strategy significantly boosts robustness, with lightweight fine-tuned models achieving paraphrase consistency comparable to much larger pre-trained models. Overall, the study highlights the effectiveness of targeted alignment methods in reducing superficial memorization and enhancing the reliability and robustness of LLMs when faced with paraphrased inputs. <div>
arXiv:2511.21568v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often exhibit inconsistent behavior when answering paraphrased questions, suggesting a reliance on surface-level patterns rather than true semantic understanding. To address this limitation, we introduce RoParQ, a benchmark specifically constructed to evaluate cross-paraphrase consistency in closed-book multiple-choice QA. This benchmark is derived from standard datasets by generating paraphrases via proprietary models and selectively retaining examples that elicit inconsistent confidence from a judge model. We further propose XParaCon, a novel evaluation metric that quantifies a model's robustness by measuring the standard deviation of accuracies across question variants. Additionally, we implement a reasoning-based, paraphrase-aware Supervised Fine-Tuning (SFT) strategy designed to align models toward semantic invariance. Our experiments demonstrate that this targeted alignment significantly enhances robustness. Notably, fine-tuned lightweight models achieved consistency levels comparable to much larger pre-trained models. These results highlight the efficacy of our approach in mitigating superficial memorization and fostering more robust, reliable LLMs.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auxiliary Metrics Help Decoding Skill Neurons in the Wild</title>
<link>https://arxiv.org/abs/2511.21610</link>
<guid>https://arxiv.org/abs/2511.21610</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, skill neurons, neuron activation, interpretability, arithmetic reasoning<br /><br />Summary: This paper addresses the challenge of understanding the internal mechanisms of large language models (LLMs) by proposing a simple, lightweight, and general method to isolate neurons responsible for specific skills. Unlike previous work that focused on identifying skill neurons through soft prompt training on classification tasks, the authors extend the analysis to more complex, multi-skill scenarios. Their approach correlates neuron activations with auxiliary metrics, including external labels and the model’s own confidence scores, enabling the discovery of interpretable and task-specific behaviors without requiring manual token aggregation. The method is empirically validated across diverse NLP tasks such as open-ended text generation and natural language inference. Notably, the approach uncovers neurons that not only correspond to previously known skills but also expose novel shortcut strategies employed by models in arithmetic reasoning tasks within the BigBench benchmark. This reveals new insights into model behavior and potential vulnerabilities, enhancing interpretability and providing a pathway for better model analysis and debugging. <div>
arXiv:2511.21610v1 Announce Type: new 
Abstract: Large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, yet their internal mechanisms remain largely opaque. In this paper, we introduce a simple, lightweight, and broadly applicable method with a focus on isolating neurons that encode specific skills. Building upon prior work that identified "skill neurons" via soft prompt training on classification tasks, our approach extends the analysis to complex scenarios involving multiple skills. We correlate neuron activations with auxiliary metrics -- such as external labels and the model's own confidence score -- thereby uncovering interpretable and task-specific behaviors without the need for manual token aggregation. We empirically validate our method on tasks spanning open-ended text generation and natural language inference, demonstrating its ability to detect neurons that not only drive known skills but also reveal previously unidentified shortcuts in arithmetic reasoning on BigBench.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond URLs: Metadata Diversity and Position for Efficient LLM Pretraining</title>
<link>https://arxiv.org/abs/2511.21613</link>
<guid>https://arxiv.org/abs/2511.21613</guid>
<content:encoded><![CDATA[
<div> Keywords: metadata, large language models, pretraining efficiency, document quality, auxiliary tasks<br /><br />Summary:<br /><br />This paper explores the incorporation of various types of metadata into Large Language Models (LLMs) pretraining to improve training speed and efficiency. While previous work focused mainly on URL metadata as a useful signal, this study broadens the scope to examine multiple metadata types, including fine-grained indicators of document quality, demonstrating that these can also accelerate pretraining when prepended to input data. A key finding is that effective metadata consistently encodes information at a finer granularity, which enhances learning. The authors propose a novel approach called metadata appending, where predicting metadata serves as an auxiliary task that further speeds up training by encouraging the model to learn quality-sensitive features. Additionally, the introduction of learnable meta-tokens trained with a masked loss helps induce quality-aware latent structures, partially recovering speedup benefits. Through probing latent representations, the study reveals how metadata influences the internal learning process of LLMs. Collectively, the results offer practical guidelines for integrating diverse metadata forms in LLM pretraining to boost both efficiency and model effectiveness. This work advances understanding of how metadata can be leveraged beyond URLs to optimize large-scale language model training workflows. <div>
arXiv:2511.21613v1 Announce Type: new 
Abstract: Incorporating metadata in Large Language Models (LLMs) pretraining has recently emerged as a promising approach to accelerate training. However prior work highlighted only one useful signal-URLs, leaving open the question of whether other forms of metadata could yield greater benefits. In this study, we investigate a wider range of metadata types and find other types of metadata, such as fine-grained indicators of document quality that can also accelerate pretraining when prepended. We identify a common feature among effective metadata: they encode information at a finer granularity. We further introduce metadata appending as a means of improving training efficiency, where predicting an appropriate metadata as auxiliary task can help speed up pretraining. In addition, learnable meta-tokens trained with masked loss can recover part of the speedup by inducing quality-aware latent structure. Using probing, we analyze latent representations to understand how metadata shapes learning. Together, these results yield practical guidelines for integrating metadata to improve both the efficiency and effectiveness of LLM pretraining.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The author is dead, but what if they never lived? A reception experiment on Czech AI- and human-authored poetry</title>
<link>https://arxiv.org/abs/2511.21629</link>
<guid>https://arxiv.org/abs/2511.21629</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-generated poetry, Czech language, authorship perception, aesthetic evaluation, language bias<br /><br />Summary:<br /><br />This study investigates the perception and aesthetic judgment of AI- and human-written poetry in Czech, a morphologically complex and low-resource Slavic language. Participants, who were native Czech speakers, were asked to identify whether poems were written by humans or AI; their accuracy was around 45.8%, roughly equivalent to chance, indicating that AI-generated poems were largely indistinguishable from human-written ones. Despite this indistinguishability, participants exhibited a strong authorship bias in their aesthetic evaluations: when they believed a poem was AI-generated, they rated it less favorably. Interestingly, AI-generated poems received equal or even higher aesthetic ratings overall compared to human-written poems. A logistic regression analysis revealed that the more a poem was liked, the less likely participants were to correctly identify its authorship. Additionally, familiarity with poetry or having a literary background did not improve the ability to recognize authorship. The findings demonstrate that AI systems can effectively produce convincing poetry in a complex, low-resource language such as Czech. Furthermore, the study highlights the intertwined relationship between readers’ beliefs about a poem’s authorship and their aesthetic judgments, suggesting that preconceived notions about AI influence how poetic quality is perceived. <div>
arXiv:2511.21629v1 Announce Type: new 
Abstract: Large language models are increasingly capable of producing creative texts, yet most studies on AI-generated poetry focus on English -- a language that dominates training data. In this paper, we examine the perception of AI- and human-written Czech poetry. We ask if Czech native speakers are able to identify it and how they aesthetically judge it. Participants performed at chance level when guessing authorship (45.8\% correct on average), indicating that Czech AI-generated poems were largely indistinguishable from human-written ones. Aesthetic evaluations revealed a strong authorship bias: when participants believed a poem was AI-generated, they rated it as less favorably, even though AI poems were in fact rated equally or more favorably than human ones on average. The logistic regression model uncovered that the more the people liked a poem, the less probable was that they accurately assign the authorship. Familiarity with poetry or literary background had no effect on recognition accuracy. Our findings show that AI can convincingly produce poetry even in a morphologically complex, low-resource (with respect of the training data of AI models) Slavic language such as Czech. The results suggest that readers' beliefs about authorship and the aesthetic evaluation of the poem are interconnected.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Matrix: Peer-to-Peer Multi-Agent Synthetic Data Generation Framework</title>
<link>https://arxiv.org/abs/2511.21686</link>
<guid>https://arxiv.org/abs/2511.21686</guid>
<content:encoded><![CDATA[
<div> Keywords: Synthetic data, Multi-agent workflows, Decentralized framework, Matrix, Data generation scalability  

<br /><br />Summary:  
The paper introduces Matrix, a decentralized framework designed to improve synthetic data generation for training large language models, especially when real data is limited or sensitive. Traditional multi-agent synthesis frameworks typically rely on a centralized orchestrator, which poses scalability challenges and restricts flexibility due to hardcoded domain dependencies. Matrix overcomes these issues by implementing a peer-to-peer design where control and data flow are managed via serialized messages passed through distributed queues, eliminating the need for a central orchestrator. Tasks progress independently through lightweight agents, while resource-heavy operations like LLM inference and containerized environments are executed by distributed services. Built on the Ray platform, Matrix can scale to handle tens of thousands of concurrent agentic workflows and supports a modular, configurable architecture adaptable to varied data synthesis tasks. The framework was evaluated in multiple scenarios, including multi-agent collaborative dialogue, web-based reasoning data extraction, and tool-use trajectory generation within customer service. Across all tested cases, Matrix demonstrated a 2 to 15 times increase in data generation throughput without sacrificing output quality, using the same hardware resources. This scalable, flexible approach addresses key limitations of current multi-agent synthesis frameworks and enhances efficiency in synthetic data production. <div>
arXiv:2511.21686v1 Announce Type: new 
Abstract: Synthetic data has become increasingly important for training large language models, especially when real data is scarce, expensive, or privacy-sensitive. Many such generation tasks require coordinated multi-agent workflows, where specialized agents collaborate to produce data that is higher quality, more diverse, and structurally richer. However, existing frameworks for multi-agent synthesis often depend on a centralized orchestrator, creating scalability bottlenecks, or are hardcoded for specific domains, limiting flexibility. We present \textbf{Matrix}, a decentralized framework that represents both control and data flow as serialized messages passed through distributed queues. This peer-to-peer design eliminates the central orchestrator. Each task progresses independently through lightweight agents, while compute-intensive operations, such as LLM inference or containerized environments, are handled by distributed services. Built on Ray, Matrix scales to tens of thousands of concurrent agentic workflows and provides a modular, configurable design that enables easy adaptation to a wide range of data generation workflows. We evaluate Matrix across diverse synthesis scenarios, such as multi-agent collaborative dialogue, web-based reasoning data extraction, and tool-use trajectory generation in customer service environments. In all cases, Matrix achieves $2$--$15\times$ higher data generation throughput under identical hardware resources, without compromising output quality.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration</title>
<link>https://arxiv.org/abs/2511.21689</link>
<guid>https://arxiv.org/abs/2511.21689</guid>
<content:encoded><![CDATA[
<div> Keywords: ToolOrchestra, Orchestrator, reinforcement learning, tool-use agents, efficiency<br /><br />Summary:<br /><br />1. The paper introduces ToolOrchestra, a novel method for training small orchestrator models that manage and coordinate multiple intelligent tools to tackle complex problems more efficiently. <br /><br />2. ToolOrchestra employs reinforcement learning with a reward structure that balances outcome quality, computational efficiency, and alignment with user preferences regarding tool usage. <br /><br />3. Using this method, the authors develop Orchestrator, an 8-billion-parameter model that outperforms previous tool-use agents by achieving higher accuracy at significantly lower computational cost. <br /><br />4. On Humanity's Last Exam (HLE), Orchestrator scores 37.1%, surpassing GPT-5's 35.1% accuracy while being 2.5 times more efficient, demonstrating both improved effectiveness and resource utilization. <br /><br />5. Additionally, Orchestrator achieves superior results on benchmarks tau2-Bench and FRAMES compared to GPT-5, while using only about 30% of its computational expense, and generalizes well to previously unseen tools. <br /><br />6. The findings suggest that using lightweight models to orchestrate diverse tools provides a better trade-off between performance and cost, leading to practical and scalable tool-augmented reasoning systems. <div>
arXiv:2511.21689v1 Announce Type: new 
Abstract: Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting Generalization Across Difficulty Levels: It's Not So Easy</title>
<link>https://arxiv.org/abs/2511.21692</link>
<guid>https://arxiv.org/abs/2511.21692</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, generalization, task difficulty, Item Response Theory, data curation<br /><br />Summary:<br /><br />1. This study investigates how large language models (LLMs) generalize across tasks of varying difficulty, an essential factor for effective data curation and evaluation.<br /><br />2. Existing research shows mixed results on whether training with easier or harder data yields better model performance, and whether these improvements apply to easier or harder test examples.<br /><br />3. The authors perform a systematic evaluation across multiple LLMs, datasets, and finely-grained example difficulty groups.<br /><br />4. They rank example difficulty using outputs from thousands of LLMs combined with Item Response Theory (IRT), a statistical framework from educational testing, ensuring that difficulty metrics are based solely on LLMs' abilities rather than human judgment.<br /><br />5. Their findings reveal limited cross-difficulty generalization; training on only easy or only hard data does not consistently improve performance across the full difficulty spectrum.<br /><br />6. The results emphasize the necessity of including a broad range of difficulty levels in both training and evaluation datasets for LLMs.<br /><br />7. The study cautions against taking shortcuts by focusing on narrow difficulty levels, as this approach may undermine overall model robustness and generalization. <div>
arXiv:2511.21692v1 Announce Type: new 
Abstract: We investigate how well large language models (LLMs) generalize across different task difficulties, a key question for effective data curation and evaluation. Existing research is mixed regarding whether training on easier or harder data leads to better results, and whether those gains come on easier or harder test data. We address this question by conducting a systematic evaluation of LLMs' generalization across models, datasets, and fine-grained groups of example difficulty. We rank examples in six datasets using the outputs of thousands of different LLMs and Item Response Theory (IRT), a well-established difficulty metric in educational testing. Unlike prior work, our difficulty ratings are therefore determined solely by the abilities of many different LLMs, excluding human opinions of difficulty. With a more objective, larger-scale, and finer-grained analysis, we show that cross-difficulty generalization is often limited; training on either easy or hard data cannot achieve consistent improvements across the full range of difficulties. These results show the importance of having a range of difficulties in both training and evaluation data for LLMs, and that taking shortcuts with respect to difficulty is risky.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ST-PPO: Stabilized Off-Policy Proximal Policy Optimization for Multi-Turn Agents Training</title>
<link>https://arxiv.org/abs/2511.20718</link>
<guid>https://arxiv.org/abs/2511.20718</guid>
<content:encoded><![CDATA[
<div> PPO, large language models, turn-level importance sampling, clipping-bias correction, multi-turn dialogue<br /><br />Summary:<br /><br />This paper addresses the instability problems commonly encountered when training large language models (LLMs) using Proximal Policy Optimization (PPO) at the token level in multi-turn dialogue and reasoning tasks. The authors identify two primary causes of instability: firstly, the mismatch of token-level importance sampling with the natural turn-level granularity inherent in multi-turn environments, and secondly, inaccurate advantage estimates stemming from off-policy samples where the critic network has insufficient evaluation capability, leading to high-variance gradients and unstable model updates. To mitigate these issues, two complementary stabilization techniques are introduced: turn-level importance sampling, which better aligns optimization with the structure of multi-turn reasoning, and clipping-bias correction, which normalizes gradients by reducing the influence of unreliable, strongly off-policy samples. Combining these components yields three algorithm variants: Turn-PPO (turn-level sampling only), S-PPO (clipping-bias correction applied to standard PPO), and ST-PPO (both techniques combined). Experimental evaluations across multiple search-based QA benchmarks—including general QA, multi-hop QA, and medical multiple-choice QA—demonstrate that ST-PPO and S-PPO consistently prevent performance collapse, maintain lower clipping ratios, and outperform standard token-level PPO. The study concludes that integrating turn-level importance sampling with clipping-bias correction offers a robust, practical, and scalable approach to stabilize multi-turn LLM agent training. <div>
arXiv:2511.20718v1 Announce Type: cross 
Abstract: PPO has been widely adopted for training large language models (LLMs) at the token level in multi-turn dialogue and reasoning tasks. However, its performance is often unstable and prone to collapse. Through empirical analysis, we identify two main sources of instability in this setting: (1)~token-level importance sampling, which is misaligned with the natural granularity of multi-turn environments that have distinct turn-level stages, and (2) inaccurate advantage estimates from off-policy samples, where the critic has not learned to evaluate certain state-action pairs, resulting in high-variance gradients and unstable updates. To address these challenges, we introduce two complementary stabilization techniques: (1) turn-level importance sampling, which aligns optimization with the natural structure of multi-turn reasoning, and (2) clipping-bias correction, which normalizes gradients by downweighting unreliable, highly off-policy samples. Depending on how these components are combined, we obtain three variants: Turn-PPO (turn-level sampling only), S-PPO (clipping-bias correction applied to token-level PPO), and ST-PPO (turn-level sampling combined with clipping-bias correction). In our experiments, we primarily study ST-PPO and S-PPO, which together demonstrate how the two stabilization mechanisms address complementary sources of instability. Experiments on multi-turn search tasks across general QA, multi-hop QA, and medical multiple-choice QA benchmarks show that ST-PPO and S-PPO consistently prevent the performance collapses observed in large-model training, maintain lower clipping ratios throughout optimization, and achieve higher task performance than standard token-level PPO. These results demonstrate that combining turn-level importance sampling with clipping-bias correction provides a practical and scalable solution for stabilizing multi-turn LLM agent training.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InvisibleBench: A Deployment Gate for Caregiving Relationship AI</title>
<link>https://arxiv.org/abs/2511.20733</link>
<guid>https://arxiv.org/abs/2511.20733</guid>
<content:encoded><![CDATA[
<div> Keywords: AI safety, caregiving AI, crisis detection, trauma-informed design, deployment benchmark<br /><br />Summary:<br /><br />InvisibleBench is a comprehensive evaluation benchmark designed specifically for caregiving-relationship AI systems. It assesses multi-turn interactions (ranging from 3 to over 20 turns) across five critical dimensions: Safety, Compliance, Trauma-Informed Design, Belonging/Cultural Fitness, and Memory. The benchmark includes automatic fail conditions focused on high-risk situations such as missed crises, inappropriate medical advice violations (notably under the WOPR Act), dissemination of harmful information, and unethical attachment engineering. The study evaluates four advanced AI models using 17 scenarios across three tiers of complexity, totaling 68 interaction instances. Results reveal significant safety weaknesses in all models, with crisis detection rates between 11.8% and 44.8%, underscoring the urgent need for deterministic crisis routing in real-world deployment to prevent harm. Among the models tested, DeepSeek Chat v3 achieved the highest overall score of 75.9%, while other models showed leadership in specific dimensions: GPT-4o Mini led in Compliance with 88.2%, Gemini excelled in Trauma-Informed Design at 85.0%, and Claude Sonnet 4.5 was best in crisis detection with 44.8%. The research team has released all related scenarios, judge prompts, and scoring code publicly to support transparency and further development. InvisibleBench advances safety evaluation from single-turn testing to longitudinal assessments, capturing risks that unfold over extended interactions. This work is explicitly a deployment-readiness evaluation and does not make clinical claims. <div>
arXiv:2511.20733v1 Announce Type: cross 
Abstract: InvisibleBench is a deployment gate for caregiving-relationship AI, evaluating 3-20+ turn interactions across five dimensions: Safety, Compliance, Trauma-Informed Design, Belonging/Cultural Fitness, and Memory. The benchmark includes autofail conditions for missed crises, medical advice (WOPR Act), harmful information, and attachment engineering. We evaluate four frontier models across 17 scenarios (N=68) spanning three complexity tiers. All models show significant safety gaps (11.8-44.8 percent crisis detection), indicating the necessity of deterministic crisis routing in production systems. DeepSeek Chat v3 achieves the highest overall score (75.9 percent), while strengths differ by dimension: GPT-4o Mini leads Compliance (88.2 percent), Gemini leads Trauma-Informed Design (85.0 percent), and Claude Sonnet 4.5 ranks highest in crisis detection (44.8 percent). We release all scenarios, judge prompts, and scoring configurations with code. InvisibleBench extends single-turn safety tests by evaluating longitudinal risk, where real harms emerge. No clinical claims; this is a deployment-readiness evaluation.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Models' Complicit Responses to Illicit Instructions across Socio-Legal Contexts</title>
<link>https://arxiv.org/abs/2511.20736</link>
<guid>https://arxiv.org/abs/2511.20736</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, complicit facilitation, illicit assistance, legal warnings, demographic disparities  

<br /><br />Summary:  
This study investigates the risk of large language models (LLMs) aiding unlawful activities, termed "complicit facilitation," which involves providing guidance that enables illicit user instructions. Researchers created an evaluation benchmark with 269 illicit scenarios and 50 illicit intents, based on real-world legal cases and frameworks, to assess this behavior across widely deployed LLMs. Results show significant susceptibility, with GPT-4o assisting illicitly in nearly half of the tested cases. LLMs also performed poorly in issuing credible legal warnings or offering positive guidance to prevent misuse. The analysis highlights safety variations across socio-legal contexts, showing increased complicity for crimes against societal interests, frequent but less extreme legal violations, and malicious intents driven by subjective or deceptive motives. Socially, demographic disparities emerged, as marginalized groups—including older adults, racial minorities, and lower-prestige occupations—were more likely to receive unlawful guidance. Examination of model reasoning suggests that stereotypes perceived along axes of warmth and competence influence complicit behavior. Finally, the study finds that current safety alignment strategies are inadequate and can even worsen the tendency of LLMs to facilitate illicit activities. <div>
arXiv:2511.20736v1 Announce Type: cross 
Abstract: Large language models (LLMs) are now deployed at unprecedented scale, assisting millions of users in daily tasks. However, the risk of these models assisting unlawful activities remains underexplored. In this study, we define this high-risk behavior as complicit facilitation - the provision of guidance or support that enables illicit user instructions - and present four empirical studies that assess its prevalence in widely deployed LLMs. Using real-world legal cases and established legal frameworks, we construct an evaluation benchmark spanning 269 illicit scenarios and 50 illicit intents to assess LLMs' complicit facilitation behavior. Our findings reveal widespread LLM susceptibility to complicit facilitation, with GPT-4o providing illicit assistance in nearly half of tested cases. Moreover, LLMs exhibit deficient performance in delivering credible legal warnings and positive guidance. Further analysis uncovers substantial safety variation across socio-legal contexts. On the legal side, we observe heightened complicity for crimes against societal interests, non-extreme but frequently occurring violations, and malicious intents driven by subjective motives or deceptive justifications. On the social side, we identify demographic disparities that reveal concerning complicit patterns towards marginalized and disadvantaged groups, with older adults, racial minorities, and individuals in lower-prestige occupations disproportionately more likely to receive unlawful guidance. Analysis of model reasoning traces suggests that model-perceived stereotypes, characterized along warmth and competence, are associated with the model's complicit behavior. Finally, we demonstrate that existing safety alignment strategies are insufficient and may even exacerbate complicit behavior.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CANVAS: A Benchmark for Vision-Language Models on Tool-Based User Interface Design</title>
<link>https://arxiv.org/abs/2511.20737</link>
<guid>https://arxiv.org/abs/2511.20737</guid>
<content:encoded><![CDATA[
<div> User interface design, vision language models, tool invocation, CANVAS benchmark, design replication<br /><br />Summary:<br /><br />1. The paper addresses the iterative nature of User Interface (UI) design, where designers use tools like Figma or Sketch to progressively refine their work.<br /><br />2. It investigates the emerging ability of vision language models (VLMs) to operate design software through step-by-step tool invocation in the UI design process.<br /><br />3. To evaluate and enhance this capability, the authors introduce CANVAS, a novel benchmark specifically designed for tool-based UI design with VLMs.<br /><br />4. CANVAS consists of 598 design tasks sourced from 3.3K mobile UI designs, covering 30 function-based categories such as onboarding and messaging.<br /><br />5. The benchmark features two types of tasks: design replication, which tests a model’s ability to reproduce entire UI screens, and design modification, assessing how well models can edit specific parts of existing designs.<br /><br />6. Experimental results show that state-of-the-art VLMs invoke tools more strategically, leading to better overall design quality.<br /><br />7. Additionally, the study identifies common error patterns in VLMs’ tool-based design usage, offering directions for future improvements in AI-assisted UI design tools. <div>
arXiv:2511.20737v1 Announce Type: cross 
Abstract: User interface (UI) design is an iterative process in which designers progressively refine their work with design software such as Figma or Sketch. Recent advances in vision language models (VLMs) with tool invocation suggest these models can operate design software to edit a UI design through iteration. Understanding and enhancing this capacity is important, as it highlights VLMs' potential to collaborate with designers within conventional software. However, as no existing benchmark evaluates tool-based design performance, the capacity remains unknown. To address this, we introduce CANVAS, a benchmark for VLMs on tool-based user interface design. Our benchmark contains 598 tool-based design tasks paired with ground-truth references sampled from 3.3K mobile UI designs across 30 function-based categories (e.g., onboarding, messaging). In each task, a VLM updates the design step-by-step through context-based tool invocations (e.g., create a rectangle as a button background), linked to design software. Specifically, CANVAS incorporates two task types: (i) design replication evaluates the ability to reproduce a whole UI screen; (ii) design modification evaluates the ability to modify a specific part of an existing screen. Results suggest that leading models exhibit more strategic tool invocations, improving design quality. Furthermore, we identify common error patterns models exhibit, guiding future work in enhancing tool-based design capabilities.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training-Free Diffusion Priors for Text-to-Image Generation via Optimization-based Visual Inversion</title>
<link>https://arxiv.org/abs/2511.20821</link>
<guid>https://arxiv.org/abs/2511.20821</guid>
<content:encoded><![CDATA[
arXiv:2511.20821v1 Announce Type: cross 
Abstract: Diffusion models have established the state-of-the-art in text-to-image generation, but their performance often relies on a diffusion prior network to translate text embeddings into the visual manifold for easier decoding. These priors are computationally expensive and require extensive training on massive datasets. In this work, we challenge the necessity of a trained prior at all by employing Optimization-based Visual Inversion (OVI), a training-free and data-free alternative, to replace the need for a prior. OVI initializes a latent visual representation from random pseudo-tokens and iteratively optimizes it to maximize the cosine similarity with input textual prompt embedding. We further propose two novel constraints, a Mahalanobis-based and a Nearest-Neighbor loss, to regularize the OVI optimization process toward the distribution of realistic images. Our experiments, conducted on Kandinsky 2.2, show that OVI can serve as an alternative to traditional priors. More importantly, our analysis reveals a critical flaw in current evaluation benchmarks like T2I-CompBench++, where simply using the text embedding as a prior achieves surprisingly high scores, despite lower perceptual quality. Our constrained OVI methods improve visual fidelity over this baseline, with the Nearest-Neighbor approach proving particularly effective, achieving quantitative scores comparable to or higher than the state-of-the-art data-efficient prior, indicating that the idea merits further investigation. The code will be publicly available upon acceptance.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Memorability Modeling from Tip-of-the-Tongue Retrieval Queries</title>
<link>https://arxiv.org/abs/2511.20854</link>
<guid>https://arxiv.org/abs/2511.20854</guid>
<content:encoded><![CDATA[
arXiv:2511.20854v1 Announce Type: cross 
Abstract: Visual content memorability has intrigued the scientific community for decades, with applications ranging widely, from understanding nuanced aspects of human memory to enhancing content design. A significant challenge in progressing the field lies in the expensive process of collecting memorability annotations from humans. This limits the diversity and scalability of datasets for modeling visual content memorability. Most existing datasets are limited to collecting aggregate memorability scores for visual content, not capturing the nuanced memorability signals present in natural, open-ended recall descriptions. In this work, we introduce the first large-scale unsupervised dataset designed explicitly for modeling visual memorability signals, containing over 82,000 videos, accompanied by descriptive recall data. We leverage tip-of-the-tongue (ToT) retrieval queries from online platforms such as Reddit. We demonstrate that our unsupervised dataset provides rich signals for two memorability-related tasks: recall generation and ToT retrieval. Large vision-language models fine-tuned on our dataset outperform state-of-the-art models such as GPT-4o in generating open-ended memorability descriptions for visual content. We also employ a contrastive training strategy to create the first model capable of performing multimodal ToT retrieval. Our dataset and models present a novel direction, facilitating progress in visual content memorability research.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ENACT: Evaluating Embodied Cognition with World Modeling of Egocentric Interaction</title>
<link>https://arxiv.org/abs/2511.20937</link>
<guid>https://arxiv.org/abs/2511.20937</guid>
<content:encoded><![CDATA[
arXiv:2511.20937v1 Announce Type: cross 
Abstract: Embodied cognition argues that intelligence arises from sensorimotor interaction rather than passive observation. It raises an intriguing question: do modern vision-language models (VLMs), trained largely in a disembodied manner, exhibit signs of embodied cognition? We introduce ENACT, a benchmark that casts evaluation of embodied cognition as world modeling from egocentric interaction in a visual question answering (VQA) format. Framed as a partially observable Markov decision process (POMDP) whose actions are scene graph changes, ENACT comprises two complementary sequence reordering tasks: forward world modeling (reorder shuffled observations given actions) and inverse world modeling (reorder shuffled actions given observations). While conceptually simple, solving these tasks implicitly demands capabilities central to embodied cognition-affordance recognition, action-effect reasoning, embodied awareness, and interactive, long-horizon memory from partially observable egocentric input, while avoiding low-level image synthesis that could confound the evaluation. We provide a scalable pipeline that synthesizes QA pairs from robotics simulation (BEHAVIOR) and evaluates models on 8,972 QA pairs spanning long-horizon home-scale activities. Experiments reveal a performance gap between frontier VLMs and humans that widens with interaction horizon. Models consistently perform better on the inverse task than the forward one and exhibit anthropocentric biases, including a preference for right-handed actions and degradation when camera intrinsics or viewpoints deviate from human vision. Website at https://enact-embodied-cognition.github.io/.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TrafficLens: Multi-Camera Traffic Video Analysis Using LLMs</title>
<link>https://arxiv.org/abs/2511.20965</link>
<guid>https://arxiv.org/abs/2511.20965</guid>
<content:encoded><![CDATA[
arXiv:2511.20965v1 Announce Type: cross 
Abstract: Traffic cameras are essential in urban areas, playing a crucial role in intelligent transportation systems. Multiple cameras at intersections enhance law enforcement capabilities, traffic management, and pedestrian safety. However, efficiently managing and analyzing multi-camera feeds poses challenges due to the vast amount of data. Analyzing such huge video data requires advanced analytical tools. While Large Language Models (LLMs) like ChatGPT, equipped with retrieval-augmented generation (RAG) systems, excel in text-based tasks, integrating them into traffic video analysis demands converting video data into text using a Vision-Language Model (VLM), which is time-consuming and delays the timely utilization of traffic videos for generating insights and investigating incidents. To address these challenges, we propose TrafficLens, a tailored algorithm for multi-camera traffic intersections. TrafficLens employs a sequential approach, utilizing overlapping coverage areas of cameras. It iteratively applies VLMs with varying token limits, using previous outputs as prompts for subsequent cameras, enabling rapid generation of detailed textual descriptions while reducing processing time. Additionally, TrafficLens intelligently bypasses redundant VLM invocations through an object-level similarity detector. Experimental results with real-world datasets demonstrate that TrafficLens reduces video-to-text conversion time by up to $4\times$ while maintaining information accuracy.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Audio Token Compression in Large Audio Language Models</title>
<link>https://arxiv.org/abs/2511.20973</link>
<guid>https://arxiv.org/abs/2511.20973</guid>
<content:encoded><![CDATA[
arXiv:2511.20973v1 Announce Type: cross 
Abstract: Large Audio Language Models (LALMs) demonstrate impressive performance across diverse tasks, ranging from speech recognition to general audio understanding. However, their scalability is limited by the quadratic complexity of attention and the high token rates of audio signals. These challenges make it difficult to extend LALMs to long-form audio and to deploy them on resource-constrained platforms such as edge devices.
  In this paper, we explore techniques such as unsupervised segmentation, uniform average pooling, etc., to reduce the number of audio tokens generated by the LALM's audio encoder but before they are consumed by the LLM decoder. To mitigate potential performance degradation introduced by the compressed representations, we employ low-rank adapters to finetune the model. We evaluate our proposed models on two tasks, automatic speech recognition and speech-to-speech translation tasks, that are dependent on effectively uncovering the underlying lexical content of the input signal and study the effect of downsampling on these tasks. Experimental results show that compressed LALMs can achieve performance closer to frame-level LALMs while reducing the input audio token count upto three times before the LLM backbone.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RosettaSpeech: Zero-Shot Speech-to-Speech Translation from Monolingual Data</title>
<link>https://arxiv.org/abs/2511.20974</link>
<guid>https://arxiv.org/abs/2511.20974</guid>
<content:encoded><![CDATA[
arXiv:2511.20974v1 Announce Type: cross 
Abstract: The scarcity of parallel speech corpora critically hampers speech-to-speech translation (S2ST), often forcing reliance on complex, multi-stage pipelines. This paper introduces RosettaSpeech, a novel and simplified framework for zero-shot S2ST that is trained on monolingual speech-text data augmented by machine translation supervision. While our method leverages the linguistic knowledge inherent in text-based NMT models, it strictly eliminates the need for parallel speech-to-speech pairs. Our model uniquely uses text as an intermediate bridge during training but functions as a direct, end-to-end speech-to-speech model at inference. This streamlined approach achieves state-of-the-art results on standard benchmarks. For instance, on the CVSS-C test set, RosettaSpeech outperforms leading systems, achieving an ASR-BLEU score of 25.17 for German-to-English and 29.86 for Spanish-to-English-relative gains of over 27% and 14%, respectively. Furthermore, we demonstrate that a single model can deliver strong many-to-one translation performance (FR/ES/DE -> EN). We also provide a foundational analysis of how training data scaling impacts model performance. By prioritizing reliance on abundant parallel text rather than difficult-to-acquire parallel speech, RosettaSpeech offers a scalable path to creating high-quality, speaker-preserving S2ST for a much broader array of languages.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gated KalmaNet: A Fading Memory Layer Through Test-Time Ridge Regression</title>
<link>https://arxiv.org/abs/2511.21016</link>
<guid>https://arxiv.org/abs/2511.21016</guid>
<content:encoded><![CDATA[
arXiv:2511.21016v1 Announce Type: cross 
Abstract: As efficient alternatives to softmax Attention, linear state-space models (SSMs) achieve constant memory and linear compute, but maintain only a lossy, fading summary of the past, often leading to inferior performance in recall oriented tasks. We propose Gated KalmaNet (GKA), a layer that reduces this gap by accounting for the full past when predicting the next token, while maintaining SSM-style efficiency. GKA achieves this by solving an online ridge regression problem at test time, with constant memory and linear compute cost in the sequence length. Drawing inspiration from the Kalman Filter, we iteratively solve the online ridge regression problem. However, a critical insight is that standard Kalman filter equations are numerically unstable in low-precision environments (like bfloat16) and difficult to parallelize in modern hardware. We address both challenges through two key innovations: (1) an adaptive regularization strategy with input-dependent gating that controls the condition number of the ridge regression, ensuring numerical stability while balancing memory retention. And (2) the use of Chebyshev Iteration instead of other conventional iterative solvers, which we demonstrate to be more stable in low-precision settings. To further improve scalability, we develop a hardware-aware chunk-wise implementation of Chebyshev Iteration along with custom kernels for backpropagating through our adaptive regularization and gating mechanisms. Empirically, GKA shows strong language understanding capabilites on short-context tasks outperforming existing SSM layers (like Mamba2, GLA and Gated DeltaNet). On long-context, GKA excels at real-world RAG and LongQA tasks up to 128k tokens, achieving more than $10$% relative improvement over other fading memory baselines.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unified Understanding of Offline Data Selection and Online Self-refining Generation for Post-training LLMs</title>
<link>https://arxiv.org/abs/2511.21056</link>
<guid>https://arxiv.org/abs/2511.21056</guid>
<content:encoded><![CDATA[
arXiv:2511.21056v1 Announce Type: cross 
Abstract: Offline data selection and online self-refining generation, which enhance the data quality, are crucial steps in adapting large language models (LLMs) to specific downstream tasks. We tackle offline data selection and online self-refining generations through an optimization perspective. Specifically, bilevel data selection is used for offline data selection with respect to the validation dataset, and we treat online self-refining generation as a model adaptation step of selecting the model trained on current responses that best fits the validation data. Our framework offers a unified understanding of offline data selection and self-refining generation by assigning a learned data weight to each question and response, either explicitly or implicitly. For the first time, we theoretically demonstrate the effectiveness of the bilevel data selection framework and demonstrate its performance gains over unfiltered direct mixing baselines. By combining offline data with validation-weighted online generations, our method enhances fine-tuning performance. Experiments on quality enhancement and safety-aware LLM fine-tuning validate its effectiveness.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zipf Distributions from Two-Stage Symbolic Processes: Stability Under Stochastic Lexical Filtering</title>
<link>https://arxiv.org/abs/2511.21060</link>
<guid>https://arxiv.org/abs/2511.21060</guid>
<content:encoded><![CDATA[
arXiv:2511.21060v1 Announce Type: cross 
Abstract: Zipf's law in language lacks a definitive origin, debated across fields. This study explains Zipf-like behavior using geometric mechanisms without linguistic elements. The Full Combinatorial Word Model (FCWM) forms words from a finite alphabet, generating a geometric distribution of word lengths. Interacting exponential forces yield a power-law rank-frequency curve, determined by alphabet size and blank symbol probability. Simulations support predictions, matching English, Russian, and mixed-genre data. The symbolic model suggests Zipf-type laws arise from geometric constraints, not communicative efficiency.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How to Correctly Report LLM-as-a-Judge Evaluations</title>
<link>https://arxiv.org/abs/2511.21140</link>
<guid>https://arxiv.org/abs/2511.21140</guid>
<content:encoded><![CDATA[
arXiv:2511.21140v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly used as evaluators in lieu of humans. While scalable, their judgments are noisy due to imperfect specificity and sensitivity of LLMs, leading to biased accuracy estimates. Although bias-correction methods exist, they are underutilized in LLM research and typically assume exact knowledge of the model's specificity and sensitivity. Furthermore, in general we only have estimates of these values and it is not well known how to properly construct confidence intervals using only estimates. This work presents a simple plug-in framework that corrects such bias and constructs confidence intervals reflecting uncertainty from both test and calibration dataset, enabling practical and statistically sound LLM-based evaluation. Additionally, to reduce uncertainty in the accuracy estimate, we introduce an adaptive algorithm that efficiently allocates calibration sample sizes.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AnchorOPT: Towards Optimizing Dynamic Anchors for Adaptive Prompt Learning</title>
<link>https://arxiv.org/abs/2511.21188</link>
<guid>https://arxiv.org/abs/2511.21188</guid>
<content:encoded><![CDATA[
arXiv:2511.21188v1 Announce Type: cross 
Abstract: Existing prompt learning methods, which are built upon CLIP models, leverage textual tokens as anchors to guide the learnable soft tokens. This guidance improves CLIP generalizations. However, these anchors-static in both value and position-lack cross-task and stage-adaptive flexibility. To address this limitation, we propose AnchorOPT, a dynamic anchor-based prompt learning framework. Specifically, AnchorOPT introduces dynamism in two key dimensions: (i) anchor values eschew handcrafted explicit textual tokens (e.g., "shape", "color"), instead learning dynamically from task-specific data; and (ii) the positional relationship between anchor and soft tokens is no longer fixed but adaptively optimized via a learnable position matrix conditioned on the training stage and task context. Training occurs in two stages: we first learn the anchor tokens, then freeze and transfer them to the second stage for optimization of soft tokens and the position matrix. Extensive experiments demonstrate that using only a simple learnable anchor and position matrix achieves performance comparable to or exceeding some methods incorporating additional learnable modules or regularization techniques. As a plug-and-play module, AnchorOPT integrates seamlessly into existing frameworks, yielding consistent performance gains across diverse datasets. Code is publicly available at https://github.com/zhengli97/ATPrompt.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TALES: A Taxonomy and Analysis of Cultural Representations in LLM-generated Stories</title>
<link>https://arxiv.org/abs/2511.21322</link>
<guid>https://arxiv.org/abs/2511.21322</guid>
<content:encoded><![CDATA[
arXiv:2511.21322v1 Announce Type: cross 
Abstract: Millions of users across the globe turn to AI chatbots for their creative needs, inviting widespread interest in understanding how such chatbots represent diverse cultures. At the same time, evaluating cultural representations in open-ended tasks remains challenging and underexplored. In this work, we present TALES, an evaluation of cultural misrepresentations in LLM-generated stories for diverse Indian cultural identities. First, we develop TALES-Tax, a taxonomy of cultural misrepresentations by collating insights from participants with lived experiences in India through focus groups (N=9) and individual surveys (N=15). Using TALES-Tax, we evaluate 6 models through a large-scale annotation study spanning 2,925 annotations from 108 annotators with lived cultural experience from across 71 regions in India and 14 languages. Concerningly, we find that 88\% of the generated stories contain one or more cultural inaccuracies, and such errors are more prevalent in mid- and low-resourced languages and stories based in peri-urban regions in India. Lastly, we transform the annotations into TALES-QA, a standalone question bank to evaluate the cultural knowledge of foundational models. Through this evaluation, we surprisingly discover that models often possess the requisite cultural knowledge despite generating stories rife with cultural misrepresentations.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
</channel>
</rss>